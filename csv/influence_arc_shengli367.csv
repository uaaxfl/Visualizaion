2007.mtsummit-papers.71,J93-2003,0,0.0134483,"etter than Pharaoh, a state-of-the-art phrase-based SMT system, and other syntax-based methods, such as the synchronous CFG-based method on the small dataset. Keywords: statistical machine translation, syntax-based statistical machine translation, tree-to-tree alignment, synchronous tree-substitution grammar, elementary tree Motivation Phrase-based SMT Phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) to statistical machine translation (SMT) has recently achieved significant improvements in translation accuracy over the original IBM word-alignment-based model (Brown et al., 1993). In phrase-based models, a phrase can be any string of adjacent words without constraints imposed by any syntactic theory. These phrases allow a model to learn local reorderings, translations of multiword expressions, or insertions and deletions that are sensitive to local context. These make it a simple and powerful mechanism for machine translation. However, there exist many open issues to be resolved in phrase-based models. For examples, the handling of discontiguous phrases and modeling of global reordering, estimation of phrase translation probabilities and phrase partition probabilities"
2007.mtsummit-papers.71,W06-1628,0,0.0735041,"Missing"
2007.mtsummit-papers.71,P05-1067,0,0.215492,"Missing"
2007.mtsummit-papers.71,W04-3250,0,0.47186,"02). Hence, all knowledge sources, including source and target string and all hidden variables and any additional knowledge source, such as language model or additional dictionaries, are described as feature functions. In our implementation, we further simplify our model as follows: e1I (7) , PET1K Eq. (5) is the simplified model. Eq. (6) formalizes the modeling process based on log-linear framework. Eq. (7) formulizes the decoding, i.e., the translation process. Finally, for our experiments we use the following seven feature functions that are analogous to the default feature set of Pharaoh (Koehn, 2004a). 1) Bidirectional elementary tree mapping probability: φ (e |f ) = log ∏ k =1 K φ ( f |e) = log ∏ k =1 K N (ξ ek , ξ fk ) N (ξ fk ) N (ξ fk , ξ ek ) N (ξ ek ) 2) Bidirectional elementary tree lexical translation probability: lex ( f |e) and lex(e |f ) . Here, we only consider terminal translation probability and set the non-terminal translation probability to 1. 3) Language model (lm): log ∏ I i =1 p (ei |ei − 2 , ei −1 ) . 4) Number of elementary tree pairs used (pp): K. 5) Number of target words (wp): I. Rule Extraction Rules or PETs are extracted from word-aligned, bi-parsed sentence pai"
2007.mtsummit-papers.71,W04-3312,0,0.0310476,"rs while Huang et al (2006) and Liu et al (2006) work on tree-tostring alignment models. Our method, in terms of modeling, training and decoding algorithms are different from theirs at one or more points. In the rest of this paper, we elaborate our modeling, training and decoding methods and report our experimental results in detail. Tree-to-Tree Alignment-based Model In this section, we first introduce what STSG is and then based on which we define our tree-to-tree alignmentbased SMT model. Finally, we present the modeling process based on log-linear framework. Synchronous TSG (STSG) for SMT Shieber (2004) gives a formal and general definition of STSG. Here we give a more concrete definition of STSG with respect to its application in SMT. A STSG is a septet G =< Σ s, Σ t , Ns, Nt , Ss, St , P > , where: • Σ s and Σ t are source and target terminal alphabets (POSs or lexical words), respectively, and • Ns and Nt are source and target non-terminal alphabets (linguistic phrase tag, i.e., NP/VP…), respectively, and examples of elementary trees which belong to the English parse tree Tt shown in Figure 1. Obviously, a normal subtree (whose leaf nodes must be terminal symbols) is an elementary tree bu"
2007.mtsummit-papers.71,P01-1067,0,0.363922,"n probabilities and phrase partition probabilities are not yet effectively addressed in phrase-based models (Quirk and Menezes, 2006). Much research has been carried out to look into the above issues. One natural extension is to utilize syntax-based structure features for SMT. Syntax-based SMT Recent work in SMT has evolved from the word-based and phrase-based models to syntax-based models, that include hierarchical phrase models (Wu, 1997; Chiang, 2007), bilingual synchronous grammars (Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006;) and other syntax-based models (Yamada and Knight, 2001; Gildea, 2003; Och et al, 2004b; Liu et al., 2006). Wu (1997) and Chiang (2007)’s methods are formally syntax-based, i.e., their methods are not informed by any linguistically syntactic theory. Wu (1997) proposes Inversion Transduction Grammars (ITGs, an instance of synchronous CFGs), treating translation as a process of parallel parsing of the source and target languages via ITGs. Chiang (2007) uses a formal binary synchronous CFG to model hierarchical phrase structures. Yamada and Knight (2001) use noisy-channel model to transfer a target parse tree into a source sentence. Och et al (2004)"
2011.mtsummit-papers.65,W10-1756,0,0.0133859,"test set because of a small number of features (Li and Eisner, 2009b) or a sparse feature of the non-terminal rule which only includes language model probability. In total, MR&DA on HG outperform baseline (incremental IHMM) using Cube Prunning up to +1.47 in BLEU score. 5.3 Third-Pass HG Decoding Firstly, the n-gram features are extracted from incremental TER and IHMM model via second-pass decoding. Then, we mix both n-grams on one of HG. Finally, we can search the HG during thirdpass decoding. Model Incremental TER Incremental IHMM NIST06 39.99 40.50 A unified framework (Pauls et al., 2009, Arun et al., 2010) was employed in MBR training and decoding. However, their methods aren’t based on the HG. In this paper, we present a unified framework of training and decoding on HG. On the other hand, there are several research on HG based decoding (Li et al., 2009a; Kumar et al., 2009; Denero et al., 2009), which use the n-gram probability to further improve the performance of the single system. In this paper, we compare three n-gram probability. In the view of HG mixture, our method is most similar to the mixture model based on HG in SMT. Duan et al. (2010) proposed a two-pass parameter optimization: fir"
2011.mtsummit-papers.65,J07-2003,0,0.0504719,"pment and test set) than incremental TER. We compare each-pass decoding of system combination based on HG. We report performance using incremental IHMM (Li et al., 2009) during first two-pass decoding. The components of mixture model in last-pass decoding are incremental TER and incremental IHMM. 1 The input of system combination is the same as Li et al. (2009). 574 5.1 First-pass HG Decoding During first-pass decoding, HG decoding with stack size 500 outperform the baseline (incremental IHMM) by +0.28 and +0.57 BLEU point on development and test set. Incremental IHMM model use Cube Prunning (Chiang 2007) and HG decoding use Cube Growing (Huang and Chiang 2007). Mixing model mixes the output of incremental IHMM and HG decoding model. Model Inc IHMM HG Decoding Mixing NIST06 39.34 39.47 39.62 NIST08 32.82 33.02 33.39 n-gram model 1-5gram_1+wp 1-5gram_2+wp 1-5gram_3+wp NIST06 40.28 39.97 39.91 NIST08 33.98 33.99 33.95 Table 3: The quality of second-pass decoding on the development and test set Table 2: The result of first-pass HG decoding on the development and test set 5.2 formance of three types of n-gram probability can be obtained when the setting is Vi+1-5gram_1+wp. It obtains +1.19 and +1."
2011.mtsummit-papers.65,P09-1064,0,0.0342338,"oding Firstly, the n-gram features are extracted from incremental TER and IHMM model via second-pass decoding. Then, we mix both n-grams on one of HG. Finally, we can search the HG during thirdpass decoding. Model Incremental TER Incremental IHMM NIST06 39.99 40.50 A unified framework (Pauls et al., 2009, Arun et al., 2010) was employed in MBR training and decoding. However, their methods aren’t based on the HG. In this paper, we present a unified framework of training and decoding on HG. On the other hand, there are several research on HG based decoding (Li et al., 2009a; Kumar et al., 2009; Denero et al., 2009), which use the n-gram probability to further improve the performance of the single system. In this paper, we compare three n-gram probability. In the view of HG mixture, our method is most similar to the mixture model based on HG in SMT. Duan et al. (2010) proposed a two-pass parameter optimization: first for n-gram probability weight for each system; second for mixture model weight whose number is the same as the number of system. DeNero et al. (2010) employed one-pass training for tuning the weight of n-gram posterior and model score, and don’t achieve the best tuning effect of each search"
2011.mtsummit-papers.65,N10-1141,0,0.0622202,"r each incoming hyperedge 6: initialize the quantities b ( wn ) and b of hyperedge e 7: b = p(e)hIant(v) 8: b (wn ) p(e) u ( I ant (v)  I ant (v) (wn )) 9: 10: 11: 12: 13: 14: if wnęe c(wn) += p(e|H) c(h(wn)) += p(e|H) Iv(wn) += b else I v ( wn )  b  b ( wn ) 15: ሺݓ ሻ ൌ 16: ݍሺݓ ሻ ൌ ሺ௪ ሻ ሺሺ௪ ሻሻ ூ ሺ௪ ሻ ூ 3.2 17: return n-gram model score p(wn) 18: n-gram count expectation c(wn) 19: n-gram posterior probability q(wn) üüüüüüüüüüüüüüüüüüüü Figure 3: the Computation of N-gram Model/Count Expectation/Posterior 3.1 tion of n-gram count, n-gram posterior (Kumar et al. 2009; DeNero et al., 2010) that is the ratio of ngram inside score and regular inside score of root. This algorithm1 can in principle compute ngram model, count expectation and posterior probability on HG. For each hypernode, we track four quantities: (1) the regular inside scores Iv that sum the scores of all derivations rooted at v and can be computed by inside recursion procedure; (2) n-gram inside scores Iv (wn) that sum the scores of all derivations rooted at v that contain ngram wn; (3) soft count c(wn) and c(h(wn)) that sum the posterior probabilities of all hyperedges introducing wn or h(wn) into HG. For each h"
2011.mtsummit-papers.65,C10-1036,0,0.0189412,"40.50 A unified framework (Pauls et al., 2009, Arun et al., 2010) was employed in MBR training and decoding. However, their methods aren’t based on the HG. In this paper, we present a unified framework of training and decoding on HG. On the other hand, there are several research on HG based decoding (Li et al., 2009a; Kumar et al., 2009; Denero et al., 2009), which use the n-gram probability to further improve the performance of the single system. In this paper, we compare three n-gram probability. In the view of HG mixture, our method is most similar to the mixture model based on HG in SMT. Duan et al. (2010) proposed a two-pass parameter optimization: first for n-gram probability weight for each system; second for mixture model weight whose number is the same as the number of system. DeNero et al. (2010) employed one-pass training for tuning the weight of n-gram posterior and model score, and don’t achieve the best tuning effect of each search space on model score. There are two major differences between our approach and above two approaches. Firstly, our model has three-pass training phase. Secondly, we have many weights for every n-gram probability and Vitebi score in each involved component mo"
2011.mtsummit-papers.65,D09-1115,0,0.0164445,"ich can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construction and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment"
2011.mtsummit-papers.65,P08-2021,0,0.0132356,"Rosti et al. 2007) for wordlevel combination is a widely adopted approach for combining SMT output, which can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construction and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al"
2011.mtsummit-papers.65,P09-1019,0,0.329372,"., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many natural language processing areas, would the technology still be efficient in system combination? (2) Models in SMT exhibit"
2011.mtsummit-papers.65,D08-1011,0,0.0181817,"al systems (Matusov et. al., 2006; Rosti et. al., 2007). Confusion network (CN) (Matusov et al. 2006 and Rosti et al. 2007) for wordlevel combination is a widely adopted approach for combining SMT output, which can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construct"
2011.mtsummit-papers.65,D09-1125,0,0.0185308,"ed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construction and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as t"
2011.mtsummit-papers.65,W05-1506,0,0.272784,"Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many natural language processing areas, would the technology still be efficient in system combination? (2) Models in SMT exhibit spurious ambiguous (Li et al. 2009).We can resolve it by using the reestimation of n-gram probability o"
2011.mtsummit-papers.65,P07-1019,0,0.0196488,"mpare each-pass decoding of system combination based on HG. We report performance using incremental IHMM (Li et al., 2009) during first two-pass decoding. The components of mixture model in last-pass decoding are incremental TER and incremental IHMM. 1 The input of system combination is the same as Li et al. (2009). 574 5.1 First-pass HG Decoding During first-pass decoding, HG decoding with stack size 500 outperform the baseline (incremental IHMM) by +0.28 and +0.57 BLEU point on development and test set. Incremental IHMM model use Cube Prunning (Chiang 2007) and HG decoding use Cube Growing (Huang and Chiang 2007). Mixing model mixes the output of incremental IHMM and HG decoding model. Model Inc IHMM HG Decoding Mixing NIST06 39.34 39.47 39.62 NIST08 32.82 33.02 33.39 n-gram model 1-5gram_1+wp 1-5gram_2+wp 1-5gram_3+wp NIST06 40.28 39.97 39.91 NIST08 33.98 33.99 33.95 Table 3: The quality of second-pass decoding on the development and test set Table 2: The result of first-pass HG decoding on the development and test set 5.2 formance of three types of n-gram probability can be obtained when the setting is Vi+1-5gram_1+wp. It obtains +1.19 and +1.41 BLEU score on the development and test set respectivel"
2011.mtsummit-papers.65,P08-1067,0,0.0226696,"oding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many"
2011.mtsummit-papers.65,P09-1066,0,0.0992258,"e and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construction and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hy"
2011.mtsummit-papers.65,P09-1067,0,0.136782,"e and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second problem, joint optimization (He and Toutanova, 2009) integrated CN construction and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hy"
2011.mtsummit-papers.65,D09-1005,0,0.0705847,"; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many natural language processing areas, would the technology"
2011.mtsummit-papers.65,P09-1065,0,0.0179254,"in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many natural language processing areas, would the technology still be efficient in system"
2011.mtsummit-papers.65,E06-1005,0,0.195268,"re compare two training procedures: minimum error training (MERT) on n-best and MR&DA on HG. The unified training and decoding approaches of HG based system combination outperform baseline using conventional Cube Prunning on Chinese-toEnglish benchmark corpus NIST08 test set. 1 Figure 1: The pipeline of three-pass training and decoding for HG generation, HG reranking and HG model mixture Introduction System combination has been proven that consensus translations are usually better than the translations of individual systems (Matusov et. al., 2006; Rosti et. al., 2007). Confusion network (CN) (Matusov et al. 2006 and Rosti et al. 2007) for wordlevel combination is a widely adopted approach for combining SMT output, which can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignme"
2011.mtsummit-papers.65,D08-1022,0,0.0213444,"and decoding into a decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many"
2011.mtsummit-papers.65,D09-1147,0,0.0168134,"same performance on test set because of a small number of features (Li and Eisner, 2009b) or a sparse feature of the non-terminal rule which only includes language model probability. In total, MR&DA on HG outperform baseline (incremental IHMM) using Cube Prunning up to +1.47 in BLEU score. 5.3 Third-Pass HG Decoding Firstly, the n-gram features are extracted from incremental TER and IHMM model via second-pass decoding. Then, we mix both n-grams on one of HG. Finally, we can search the HG during thirdpass decoding. Model Incremental TER Incremental IHMM NIST06 39.99 40.50 A unified framework (Pauls et al., 2009, Arun et al., 2010) was employed in MBR training and decoding. However, their methods aren’t based on the HG. In this paper, we present a unified framework of training and decoding on HG. On the other hand, there are several research on HG based decoding (Li et al., 2009a; Kumar et al., 2009; Denero et al., 2009), which use the n-gram probability to further improve the performance of the single system. In this paper, we compare three n-gram probability. In the view of HG mixture, our method is most similar to the mixture model based on HG in SMT. Duan et al. (2010) proposed a two-pass paramet"
2011.mtsummit-papers.65,P07-1040,0,0.450899,"procedures: minimum error training (MERT) on n-best and MR&DA on HG. The unified training and decoding approaches of HG based system combination outperform baseline using conventional Cube Prunning on Chinese-toEnglish benchmark corpus NIST08 test set. 1 Figure 1: The pipeline of three-pass training and decoding for HG generation, HG reranking and HG model mixture Introduction System combination has been proven that consensus translations are usually better than the translations of individual systems (Matusov et. al., 2006; Rosti et. al., 2007). Confusion network (CN) (Matusov et al. 2006 and Rosti et al. 2007) for wordlevel combination is a widely adopted approach for combining SMT output, which can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 200"
2011.mtsummit-papers.65,2006.amta-papers.25,0,0.0230242,"at consensus translations are usually better than the translations of individual systems (Matusov et. al., 2006; Rosti et. al., 2007). Confusion network (CN) (Matusov et al. 2006 and Rosti et al. 2007) for wordlevel combination is a widely adopted approach for combining SMT output, which can significantly outperform sentence-level re-ranking methods and phrase-level combination (Rosti et. al., 2007). During constructing CN, word alignment between skeleton/backbone and hypothesis and skeleton selection are two key issues in this approach. To solve first issue, Translation Error Rate (TER) 570 (Snover et al., 2006) based alignment was proposed in Sim et al. (2007); IHMM (He et al., 2008) got the better alignment using source language as pivot language; ITG-based alignment (Karakos et al., 2008) uses the ITG constrain during constructing CN; lattice-based system combination (Feng et al., 2009) normalized the alignment between the skeleton and the hypothesis into the lattice without breaking the phrase structure; incremental strategy (Rosti et al., 2008; Li et al., 2009) was added into the monolingual alignment algorithm including TER and IHMM in order to avoid pairwise alignment error. To solve second pr"
2011.mtsummit-papers.65,P06-2101,0,0.029268,"alue and weight; length function ሺǡ ǡ ሻ is the length of a derivation of each model in system combination. I is the number of system, and N is n-gram number. If we have two models of system combination and use the 5-gram probability model, we have 2*5+2+1=13 mixture factor numbers. The optimization of mixture factor is at third-pass training and third-pass decoding. We initially construct the hypergraph bottom-up. After the construction, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations in three-pass decoding. 4 MR Training on HG To overcome the overfitting, Smith and Eisner (2006) smoothed the risk function by DA to ensure the search space is as large as possible before objective function achieves the optimal weight. The objective function can be defined as follow: L RH ( pO ,T )  T  EH ( pO ,T ) (3) 5 We use NIST MT06 data set including 1099 sentences as the development set and NIST MT08 data set including 1357 sentences from both newswire and web-data genres as the test set. To save computation effort, the result on the development and test set are reported in case-insensitive BLEU score. The above system generates the 10best of every sentence as input of system co"
2011.mtsummit-papers.65,C10-1123,0,0.015831,"decoder without skeleton selection; multiple CNs was first proposed in (Matusov et al.2006; Rosti et al., 2007), and was implemented via combining several different hypothesis alignment metrics (Du and Way, 2009). However, there are few works about training and decoding for system combination. Tranditional nbest training method train feature weights at limited hypothesis space and propogate the errors to target translation. These errors will severely hurt the translation quanlity. To alleviate such error, a HG was applied to many areas, such as translation rule extraction (Mi and Huang, 2008; Tu et al., 2010), model training (Li and Eisner, 2009b), decoding (Liu et al., 2009; Li et al., 2009a; Kumar et al., 2009; DeNero et al., 2009) in the field of machine translation, constituent parsing (Huang and Chiang, 2005). Overall, HG gives large search space for training and decoding which is expected to avoid the search error caused by the imprecision parameter estimation and early prunning. This paper introduces HG into system combination and explores to address three problems: (1) Since the HG technology gives better performance than conventional training and decoding method in many natural language p"
2020.ccl-1.102,D14-1179,0,0.0343802,"Missing"
2020.ccl-1.102,N19-1423,0,0.0409894,"Missing"
2020.ccl-1.102,W18-3002,0,0.102466,", u2 , ..., uN }, where N is the number of utterances in the dialogue, ui = {w1 , w2 , ..., wL } represents the ith(1 ≤ i ≤ N ) utterance in the dialogue that consists of L words, our goal is to analyze the emotion of each utterance in the dialogue. To solve this task, we propose a hierarchical model CAN-GRU and extend three variants, CAN-GRUA, CAN-biGRU and CANbiGRUA(illustrated in Fig. 1). 3.2 Text Feature Extraction In this section, we discuss the first layer of the model. Like (Poria et al., 2017), we use convolutional neural network to extract the features of the utterance. Inspired by (Gao et al., 2018), in order to capture the contextual information of long text effectively, we use convolutional self-attention network(CAN) instead of traditional CNN network. Proceedings of the 19th China National Conference on Computational Linguistics, pages 1101-1111, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China 02 0 Computational Linguistics L2 Figure 1: The architecture of our proposed CAN-biGRUA. In the first layer, convolutional neural network and self-attention mechanism are used to extract text fe"
2020.ccl-1.102,D19-1015,0,0.0262627,"erances by LSTM. Considering inter-speaker dependency relations, conversational memory network(CMN)(Hazarika et al., 2018b) has been proposed to model the speaker-based emotion using memory network and summarize task-specific details by attention mechanisms. ICON(Hazarika et al., 2018a) improves the CMN, it hierarchically models the self-speaker emotion and inter-speaker emotion into global memories. DialogueRNN(Majumder et al., 2019) uses emotion GRU and global GRU to model inter-party relation, and uses party GRU to model relation between two sequential states of the same party. DialogueGCN(Ghosal et al., 2019) improves DialogueRNN by graph convolutional network, and it can hold richer context relevant to emotion. However, these models may be too complex for small textual dialogue datasets. In this paper, we study on the EmotionX Challenge(Hsu and Ku, 2018), Dialogue Emotion Recognition Challenge, which aims to recognize the emotion of each utterance in dialogues. According to the overview of this task, the best team(Khosla, 2018) proposes a CNN-DCNN auto encoder based model, which includes a convolutional encoder and a deconvolutional decoder. The second place team(Luo et al., 2018) mainly uses BiL"
2020.ccl-1.102,D18-1280,0,0.0153324,"ng-range contextual information. Later, attention mechanism(Bahdanau et al., 2015) is proposed to solve this problem. Recently, self-attention(Vaswani et al., 2017) is widely used since it can solve the long-term dependence problem of text effectively. Recent years, more and more researchers focus on emotion recognition in conversation. This task aims to recognize the emotion of each utterance in dialogues. bcLSTM(Poria et al., 2017) extracts textual features by CNN and model the sequence of utterances by LSTM. Considering inter-speaker dependency relations, conversational memory network(CMN)(Hazarika et al., 2018b) has been proposed to model the speaker-based emotion using memory network and summarize task-specific details by attention mechanisms. ICON(Hazarika et al., 2018a) improves the CMN, it hierarchically models the self-speaker emotion and inter-speaker emotion into global memories. DialogueRNN(Majumder et al., 2019) uses emotion GRU and global GRU to model inter-party relation, and uses party GRU to model relation between two sequential states of the same party. DialogueGCN(Ghosal et al., 2019) improves DialogueRNN by graph convolutional network, and it can hold richer context relevant to emot"
2020.ccl-1.102,N18-1193,0,0.0208552,"ng-range contextual information. Later, attention mechanism(Bahdanau et al., 2015) is proposed to solve this problem. Recently, self-attention(Vaswani et al., 2017) is widely used since it can solve the long-term dependence problem of text effectively. Recent years, more and more researchers focus on emotion recognition in conversation. This task aims to recognize the emotion of each utterance in dialogues. bcLSTM(Poria et al., 2017) extracts textual features by CNN and model the sequence of utterances by LSTM. Considering inter-speaker dependency relations, conversational memory network(CMN)(Hazarika et al., 2018b) has been proposed to model the speaker-based emotion using memory network and summarize task-specific details by attention mechanisms. ICON(Hazarika et al., 2018a) improves the CMN, it hierarchically models the self-speaker emotion and inter-speaker emotion into global memories. DialogueRNN(Majumder et al., 2019) uses emotion GRU and global GRU to model inter-party relation, and uses party GRU to model relation between two sequential states of the same party. DialogueGCN(Ghosal et al., 2019) improves DialogueRNN by graph convolutional network, and it can hold richer context relevant to emot"
2020.ccl-1.102,W18-3507,0,0.304819,"19) uses emotion GRU and global GRU to model inter-party relation, and uses party GRU to model relation between two sequential states of the same party. DialogueGCN(Ghosal et al., 2019) improves DialogueRNN by graph convolutional network, and it can hold richer context relevant to emotion. However, these models may be too complex for small textual dialogue datasets. In this paper, we study on the EmotionX Challenge(Hsu and Ku, 2018), Dialogue Emotion Recognition Challenge, which aims to recognize the emotion of each utterance in dialogues. According to the overview of this task, the best team(Khosla, 2018) proposes a CNN-DCNN auto encoder based model, which includes a convolutional encoder and a deconvolutional decoder. The second place team(Luo et al., 2018) mainly uses BiLSTM with a self-attentive architecture on the top for the classiffication. The third place team(Saxena et al., 2018) proposes a hierarchical network based on attention models and conditional random fields(CRF). For a meaningful comparison, we use the same dataset and metric as the challenge in our study. 3 3.1 Method Task Definition Given a dialogue dia = {u1 , u2 , ..., uN }, where N is the number of utterances in the dialo"
2020.ccl-1.102,D14-1181,0,0.0034927,"mputational Linguistics speaker Phoebe Rachel Wayne Joey Gary Chandler Gary Chandler utterance Can I tell you a little secret? Yeah! Hey Joey, I want to talk to you. Yeah? Hey Chandler, what are you doing here? Gary, I’m here to report a crime. Yeah? It is a crime that you and I don’t spend more time together. emotion neutral joy neutral neutral suprise neutral suprise neutral Table 1: The word ’Yeah’ expresses different emotions in the different contexts. CC L2 02 0 work(Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit Network(Cho et al., 2014) and textual Convolutional Neural Network(Kim, 2014). However, these models don’t perform well when the texts are too long, because it’s hard to capture the long-range contextual information. Later, attention mechanism(Bahdanau et al., 2015) is proposed to solve this problem. Recently, self-attention(Vaswani et al., 2017) is widely used since it can solve the long-term dependence problem of text effectively. Recent years, more and more researchers focus on emotion recognition in conversation. This task aims to recognize the emotion of each utterance in dialogues. bcLSTM(Poria et al., 2017) extracts textual features by CNN and model the sequence"
2020.ccl-1.102,W18-3506,0,0.0713314,"DialogueGCN(Ghosal et al., 2019) improves DialogueRNN by graph convolutional network, and it can hold richer context relevant to emotion. However, these models may be too complex for small textual dialogue datasets. In this paper, we study on the EmotionX Challenge(Hsu and Ku, 2018), Dialogue Emotion Recognition Challenge, which aims to recognize the emotion of each utterance in dialogues. According to the overview of this task, the best team(Khosla, 2018) proposes a CNN-DCNN auto encoder based model, which includes a convolutional encoder and a deconvolutional decoder. The second place team(Luo et al., 2018) mainly uses BiLSTM with a self-attentive architecture on the top for the classiffication. The third place team(Saxena et al., 2018) proposes a hierarchical network based on attention models and conditional random fields(CRF). For a meaningful comparison, we use the same dataset and metric as the challenge in our study. 3 3.1 Method Task Definition Given a dialogue dia = {u1 , u2 , ..., uN }, where N is the number of utterances in the dialogue, ui = {w1 , w2 , ..., wL } represents the ith(1 ≤ i ≤ N ) utterance in the dialogue that consists of L words, our goal is to analyze the emotion of each"
2020.ccl-1.102,D14-1162,0,0.0833501,"tion sadness neutral 498 6530 514 9855 others 5006 2133 Table 2: Statistics of the datasets. 4.2 Evaluation Metric We use the unweighted accuracy(UWA) as the evaluation metric instead of the weighted accuracy(WA), the same as the challenge. This is because WA is easily influced by the large proportion of neutral emotion and UWA can help to make a meaningful comparision. c c X 1X ai , W A = weighti ai c (20) i=1 02 i=1 0 UWA = Where ai is the accuracy of class i and weighti is the percentage of the class i. 4.3 Experimental Setting Baselines CC 4.4 L2 We use 300-dimensional pre-trained GloVe2 (Pennington et al., 2014) word-embeddings which is trained from web data. We use three distinct convolution filters of sizes 3, 4, and 5 respectively, each having 100 feature maps. The dimension of the hidden states of the GRU is set to 300. We use adam(Kingma and Ba, 2015) optimizer and set the initial learning rate as 1.0 × 10−4 . The learning rate is halved every 20 epochs during training. Dropout probability is set to 0.3. In experiments, we compare our proposed model with the following models. CNN-DCNN: The winner of EmotionX Challenge(Khosla, 2018). The model contains a convolutional encoder and a deconvolutiona"
2020.ccl-1.102,D15-1303,0,0.0257801,"an important component of human intelligence, emotional intelligence is defined as the ability to perceive, integrate, understand and regulate emotions(Mayer et al., 2008). Emotion is the essential difference between human and machine, so emotion understanding is an important research direction of artificial intelligence. As the most common way for people to communicate in daily life, dialogue contains a wealth of emotions. Recognising the emotions in the conversation is of great significance in intelligent customer service, medical systems, education systems and other aspects. According to (Poria et al., 2015), textual features usually contain more emotional information than video or audio features, so we focus on the emotion analysis of dialogue text and aims to recognize the emotion of each utterrance in dialogues. There are some challenges in this task. First, the length of an utterance may be too long, making it difficult to capture contextual information. Furthermore, a dialogue usually contains lots of utterances, therefore, it’s hard to grasp long-term contextual relations between utterances. Second, the same word may express different emotions in different contexts. For example, in Table 1,"
2020.ccl-1.102,P17-1081,0,0.310515,"twork(Cho et al., 2014) and textual Convolutional Neural Network(Kim, 2014). However, these models don’t perform well when the texts are too long, because it’s hard to capture the long-range contextual information. Later, attention mechanism(Bahdanau et al., 2015) is proposed to solve this problem. Recently, self-attention(Vaswani et al., 2017) is widely used since it can solve the long-term dependence problem of text effectively. Recent years, more and more researchers focus on emotion recognition in conversation. This task aims to recognize the emotion of each utterance in dialogues. bcLSTM(Poria et al., 2017) extracts textual features by CNN and model the sequence of utterances by LSTM. Considering inter-speaker dependency relations, conversational memory network(CMN)(Hazarika et al., 2018b) has been proposed to model the speaker-based emotion using memory network and summarize task-specific details by attention mechanisms. ICON(Hazarika et al., 2018a) improves the CMN, it hierarchically models the self-speaker emotion and inter-speaker emotion into global memories. DialogueRNN(Majumder et al., 2019) uses emotion GRU and global GRU to model inter-party relation, and uses party GRU to model relatio"
2021.naacl-main.221,P84-1044,0,0.188303,"Missing"
2021.naacl-main.221,D18-1221,0,0.406475,"nd it is a challenge to generate low-dimensional representations from a graph with many missing edges. To mitigate this issue, auxil1 protective body arthropod iary texts that are easily accessible have been popularly exploited for enhancing the KG (as illustrated in Figure 1). More specifically, given that KG entities contain textual features, we can link them to an auxiliary source of knowledge, e.g., WordNet, and therefore enhance the existing feature space. With notable exceptions, the use of external textual properties for KG embedding has not been extensively explored before. Recently, (Kartsaklis et al., 2018) used entities of the KG to query BabelNet (Navigli and Ponzetto, 2012), added new nodes to the original KG based on co-occurrence of entities, and produced more meaningful embeddings using the enriched graph. However, this hard-coded, cooccurrence based KG enrichment strategy fails to make connections to other semantically related entities. As motivated in Figure 1, the newly added entities “wound&quot;, “arthropod&quot; and “protective body&quot;, are semantically close to some input KG entity nodes (marked in red). However, they cannot be directly retrieved from BabelNet using co-occurrence matching. In t"
2021.naacl-main.221,D18-1455,0,0.0276611,"(Kartsaklis et al., 2018) adds an edge (e, t) to KG per entity e based on co-occurrence and finds graph embeddings using random walks. However, there is no learning component in these approaches in constructing the new knowledge graph. And the enrichment procedure is solely based on occurrences (“hard&quot; matching) of entities in the external text. For graph completion task, (Malaviya et al., 2020) uses pre-trained language models to improve The rest of the paper is organized as follows. In the representations and for Question Answering the next section, we try to identify the gap in the task, (Sun et al., 2018) extracts a sub-graph Gq existing literature and motivate our work. Next, from KG and Wikipedia, which contains the an2768 GCN-based Decoder GCN-based Encoder graph alignment Legend: forward path Augment graph alignment GCN-based Decoder GCN-based Encoder locality preserving regularization locality preserving regularization existing entities newly added entities Figure 2: Our proposed framework for aligning two graphs in the embedding space. The graph alignment component, LJ , requires an additional matrix, R, that selects embeddings of KG entities from ZT , so the resulting matrix, RZT , woul"
2021.woah-1.1,D14-1162,0,0.0851051,"ark datasets, which demonstrate the remarkable performance of our approach on offensive language detection. 2 A Related Work In this section, we briefly review related work on offensive language detection and transformers. Offensive Language Detection. Offensive language detection (OLD) has become an active research topic in recent years (Araujo De Souza and Da Costa Abreu, 2020). Nikolov and Radivchev (2019) experimented with a variety of models and observe promising results with BERT and SVC based models. Han et al. (2019) employed a GRU based RNN with 100 dimensional glove word embeddings (Pennington et al., 2014). Additionally, they develop a Modified Sentence Offensiveness Calculation (MSOC) model which makes use of a dictionary of offensive words. Liu et al. (2019) evaluated three models on the OLID dataset, including logistic regression, LSTM and BERT, and results show that BERT achieves the best performance. The concept of transfer learning mentioned in (Liu et al., 2019) is closely related to our work, since the BERT model is also pretrained on external text corpus. However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we pro"
2021.woah-1.1,2021.naacl-main.221,1,0.707672,"ote the number of layers (including the predictive layer), NS denote the number of training samples in the source domain, and NT denote the number of training samples in the target domain. K is the set of layers that remain frozen during training in the target domain. We propose a simple yet effective domain adaptation approach to train an ALBERT model for offensive language detection, which fully exploits auxiliary information from source domain to assist the learning task in target domain. The effectiveness of using auxiliary text for language understanding has been discussed in literature (Rezayi et al., 2021). Both the source and target domains contain rich information about offensive contents, which makes 1 https://github.com/grantjenks/pythonwordsegment 2 https://github.com/carpedm20/emoji 3 https://www.kaggle.com/c/jigsawtoxic-comment-classification-challenge/ data 3 and 2 × 10−5 on the source data and target data, respectively. Following the standard evaluation protocol on the OLID dataset, the 9:1 training versus validation split is used. In each experiment (other than SVM), the models are trained for 3 epochs. The metric used here is macro F1 score, which is calculated by taking the unweight"
2021.woah-1.1,S19-2116,0,0.0259077,"Missing"
2021.woah-1.1,N16-2013,0,0.0790764,"Missing"
2021.woah-1.1,N19-1144,0,0.0283164,"nderstanding. Existing methods on OLD, such as (Davidson et al., 2017), mainly focus on detecting whether the content is offensive or not, but they can not identify the specific type and target of such content. Waseem and Hovy (2016) analyze a corpus of around 16k tweets for hate speech detection, make use of meta features (such as gender and location of the user), and employ a simple n-gram based model. Liu et al. (2019) evaluate the performance of some deep learning models, including BERT (Devlin et al., 2018), and achieve the state of the art results on a newly collected OLD dataset, OLID (Zampieri et al., 2019). Although promising progress on OLD has been observed in recent years, existing methods, especially the deep learning based ones, often rely on large-scale well-labeled data for model training. In practice, labeling offensive language data requires tremendous efforts, due to linguistic variety and human bias. In this paper, we propose to tackle the challenging issue of data/label scarcity in offensive language detection, by designing a simple yet effective domain adaptation approach based on bidirectional transformers. Domain adaptation aims to enhance the model capacity for a target domain b"
2021.woah-1.1,S19-2011,0,0.214741,"r Singh University of Georgia Athens, GA, USA sumer.singh@uga.edu Sheng Li University of Georgia Athens, GA, USA sheng.li@uga.edu Abstract years, which is an active topic in natural language understanding. Existing methods on OLD, such as (Davidson et al., 2017), mainly focus on detecting whether the content is offensive or not, but they can not identify the specific type and target of such content. Waseem and Hovy (2016) analyze a corpus of around 16k tweets for hate speech detection, make use of meta features (such as gender and location of the user), and employ a simple n-gram based model. Liu et al. (2019) evaluate the performance of some deep learning models, including BERT (Devlin et al., 2018), and achieve the state of the art results on a newly collected OLD dataset, OLID (Zampieri et al., 2019). Although promising progress on OLD has been observed in recent years, existing methods, especially the deep learning based ones, often rely on large-scale well-labeled data for model training. In practice, labeling offensive language data requires tremendous efforts, due to linguistic variety and human bias. In this paper, we propose to tackle the challenging issue of data/label scarcity in offensi"
C02-1003,P95-1033,0,0.346537,"the next section, a bilingual language model is introduced. Then, a bilingual parsing method supervised by English parsing is proposed in section 2. Based on the bilingual parsing, Chinese bracketing knowlege is extracted in section 3. The evaluation and discussion are given in section 4. We conclude with discussion of future work. 1 A bilingual language model – ITG Wu (1997) has proposed a bilingual language model called Inversion Transduction Grammar (ITG), which can be used to parse bilingual sentence pairs simultaneously. We will give a brief description here. For details please refer to (Wu 1995, Wu 1997). The Inversion Transduction Grammar is a bilingual context-free grammar that generates two matched output languages (referred to as L1 and L2). It also differs from standard context-free grammars in that the ITG allows right-hand side production in two directions: straight or inverted. The following examples are two ITG productions: C -> [A B] C -> <A B> Each nonterminal symbol stands for a pair of matched strings. For example, the nonterminal A stands for the string-pair (A1, A2). A1 is a sub-string in L1, and A2 is A1’s corresponding translation in L2. Similarly, (B1, B2) denotes"
C02-1003,J97-3002,0,0.898238,"results. The main idea of the method is that we may acquire knowledge for a language lacking a rich collection of resources and tools from a second language that is full of them. The rest of this paper is organized as follows : In the next section, a bilingual language model is introduced. Then, a bilingual parsing method supervised by English parsing is proposed in section 2. Based on the bilingual parsing, Chinese bracketing knowlege is extracted in section 3. The evaluation and discussion are given in section 4. We conclude with discussion of future work. 1 A bilingual language model – ITG Wu (1997) has proposed a bilingual language model called Inversion Transduction Grammar (ITG), which can be used to parse bilingual sentence pairs simultaneously. We will give a brief description here. For details please refer to (Wu 1995, Wu 1997). The Inversion Transduction Grammar is a bilingual context-free grammar that generates two matched output languages (referred to as L1 and L2). It also differs from standard context-free grammars in that the ITG allows right-hand side production in two directions: straight or inverted. The following examples are two ITG productions: C -> [A B] C -> <A B> Eac"
C02-1003,1993.iwpt-1.3,0,0.13968,"Missing"
C02-1003,O96-2006,0,\N,Missing
C02-1003,J93-2004,0,\N,Missing
C02-1003,P97-1003,0,\N,Missing
C02-1003,J00-2004,0,\N,Missing
C02-1003,O01-2004,1,\N,Missing
C02-1057,2001.mtsummit-papers.25,0,0.293303,"Missing"
C02-1057,2001.mtsummit-papers.67,0,0.0972107,"-occurrence and so on as indicators of translation quality. Brew C (1994) compares human rankings and automatic measures to decide the translation quality, whose criteria involve word frequency, POS tagging distribution and other text features. Another type of evaluation method involves comparison of the translation result with human translations. Yokoyama (2001) proposed a two-way MT based evaluation method, which compares output Japanese sentences with the original Japanese sentence for the word identification, the correctness of the modification, the syntactic dependency and the parataxis. Yasuda (2001) evaluates the translation output by measuring the similarity between the translation output and translation answer candidates from a parallel corpus. Akiba (2001) uses multiple edit distances to automatically rank machine translation output by translation examples. Another path of machine translation evaluation is based on test suites. Yu (1993) designs a test suite consisting of sentences with various test points. Guessoum (2001) proposes a semi-automatic evaluation method of the grammatical coverage machine translation systems via a database of unfolded grammatical structures. Koh (2001) de"
C02-1057,2001.mtsummit-papers.68,0,0.060773,"Missing"
C02-1057,2001.mtsummit-papers.3,0,\N,Missing
C02-1057,bohan-etal-2000-evaluating,0,\N,Missing
C02-1057,2001.mtsummit-papers.35,0,\N,Missing
C02-1057,H94-1019,0,\N,Missing
C02-1057,C00-1055,0,\N,Missing
C08-1138,koen-2004-pharaoh,0,0.150361,"rivations3 that could lead to the same target tree T (e1I ) , the mapping probability Pr (T (e1I ) |T ( f1J )) is obtained by summing over the probabilities of all derivations. The probability of each derivation θ is given by the product of the probabilities of all the rules p (ri ) used in the derivation (here we assume that a rule is applied independently in a derivation). Pr (e1I |f1J ) = Pr (T (e1I ) |T ( f1J )) = ∑∏ p (ri ) θ (1) ri ∈θ The model is implemented under log-linear framework. We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. Besides, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized rules and 2) the average tree height in a rule to balance the usage of hierarchical rules and more flat rules. 2) SCFG-based tree-to-tree model when α s = The overall training process is similar to the process in the phrase-based system (koehn et al., α"
C08-1138,J93-2003,0,0.00973221,"mainly de© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explai"
C08-1138,P07-2045,0,0.00915683,"s our development set and the NIST MT-2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ and the heuristics “grow-diagfinal” to generate m-to-n word alignments. For the MER training, we modified Koehn’s MER trainer (Koehn, 2004) for our STSSG-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We compared four SMT systems: Moses (Koehn et al., 2007), SCFG-based, STSG-based and STSSGbased tree-to-tree translation models. For Moses, we used its default settings. For the others, we implemented them on the STSSG platform by adopting the same settings as used in the synchronous parsing. We optimized the decoding parameters on the development sets empirically. 4.2 Experimental Results SCFG STSG a larger span than SCFG. It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al., 2004)4. 3) All the three models on the FBIS corpus show much lower performance th"
C08-1138,P06-1077,0,0.629179,"nported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has grea"
C08-1138,P07-1089,0,0.805014,"ttp://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both t"
C08-1138,J94-4004,0,0.194206,"Missing"
C08-1138,W02-1039,0,0.253408,"ous factors in this study, including the genera of corpora (newspaper domain via spoken domain), the accuracy of word alignments and syntax parsing (automatically vs. manually). We report our experimental settings, experimental results and our findings in detail in the rest of the paper, which is organized as follows: Section 2 reviews previous work. Section 3 elaborates the general framework while Section 4 reports the experimental results. Finally, we conclude our work in Section 5. 2 Previous Work There are only a few of previous work related to the study of translation grammar comparison. Fox (2002) is the first to look at how well proposed translation models fit actual translation data empirically. She examined the issue of phrasal cohesion between English and French and discovered that while there is less cohesion than one might desire, there is still a large amount of regularity in the constructions where breakdowns occur. This suggests that reordering words by phrasal movement is a reasonable strategy (Fox, 2002). She has also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures. Their experimental resul"
C08-1138,P06-1121,0,0.093015,"Missing"
C08-1138,P04-1083,0,0.0235041,"arget ones. To speed up the decoder, we utilize several thresholds to limit the search beams for each span, such as the number of rules used and the number of hypotheses generated. 3.4 Synchronous Parsing A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures. When a parser’s input can have fewer dimensions than the parser’s grammar, we call it a translator. When a parser’s grammar can have fewer dimensions than the parser’s input, we call it a synchronizer (Melamed, 2004). Therefore, synchronous parsing and MT are closed to each other. In this paper, we use synchronous parsing to compare the ability of different grammars in translational equivalence modeling. Given a bilingual sentence pair f1J and e1I , the synchronous parser is to find a derivation θ that generates &lt; T ( f1J ) , T (e1I ) >. Our synchronous parser is similar to the synchronous CKY parser presented at (Melamed, 2004). The difference is that we implement it based on our STSSG decoder. Therefore, in nature the parser is a standard synchronous chart parser but constrained by the rules of the STSS"
C08-1138,P05-1034,0,0.108909,"Missing"
C08-1138,N04-1035,0,0.716904,"cohesion between English and French and discovered that while there is less cohesion than one might desire, there is still a large amount of regularity in the constructions where breakdowns occur. This suggests that reordering words by phrasal movement is a reasonable strategy (Fox, 2002). She has also examined the differences in cohesion between Treebank-style parse trees, trees with flattened verb phrases, and dependency structures. Their experimental results indicate that the highest degree of cohesion is present in dependency structures. Motivated by the same problem raised by Fox (2002), Galley et al. (2004) study what rule can better explain human translation data. They first propose a theory that gives formal semantics to word-level alignments defined over parallel corpora, and then use the theory to introduce a linear algorithm that is used to derive from wordaligned, parallel corpora the minimal set of syntactically motivated transformation rules to explain human translation data. Their basic idea is to create transformation rules that condition on larger fragments of tree structure. Their experimental results suggest that their proposed rules provide a good, realistic indicator of the comple"
C08-1138,J97-3002,0,0.252765,"process to describe and build these alignments using mathematical models. Thus, the study of TEM is highly relevant to Statistical Machine Translation (SMT). Grammar is the most important infrastructure for TEM and SMT since translation models’ expressive and generative abilities are mainly de© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Co"
C08-1138,P01-1067,0,0.0407236,"ghts reserved. termined by the grammar. Many grammars, such as finite-state grammars (FSG), bracket/inversion transduction grammars (BTG/ITG) (Wu, 1997), context-free grammar (CFG), tree substitution grammar (TSG) (Comon et al., 2007) and their synchronous versions, have been explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both theory and practice to do comparison studies among these grammar"
C08-1138,2003.mtsummit-papers.22,0,0.0433645,"een explored in SMT. Based on these grammars, a great number of SMT models have been recently proposed, including string-to-string model (Synchronous FSG) (Brown et al., 1993; Koehn et al., 2003), tree-to-string model (TSG-string) (Huang et al., 2006; Liu et al., 2006; Liu et al., 2007), string-totree model (string-CFG/TSG) (Yamada and Knight, 2001; Galley et al., 2006; Marcu et al., 2006), tree-to-tree model (Synchronous CFG/TSG, Data-Oriented Translation) (Chiang, 2005; Cowan et al., 2006; Eisner, 2003; Ding and Palmer, 2005; Zhang et al., 2007; Bod, 2007; Quirk wt al., 2005; Poutsma, 2000; Hearne and Way, 2003) and so on. Although many achievements have been obtained by these advances, it is still unclear which of these important pursuits is able to best explain human translation data, as each has its advantages and disadvantages. Therefore, it has great meaning in both theory and practice to do comparison studies among these grammars and SMT models to see which of them are capable of better describing parallel translation data. This is a fundamental issue worth exploring in multilingual information processing. However, little effort in previous work has been put in this point. To address this issue"
C08-1138,zhang-etal-2004-interpreting,0,0.0967943,"less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT-2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ and the heuristics “grow-diagfinal” to generate m-to-n word alignments. For the MER training, we modified Koehn’s MER trainer (Koehn, 2004) for our STSSG-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We compared four SMT systems: Moses (Koehn et al., 2007), SCFG-based, STSG-based and STSSGbased tree-to-tree translation models. For Moses, we used its default settings. For the others, we implemented them on the STSSG platform by adopting the same settings as used in the synchronous parsing. We optimized the decoding parameters on the development sets empirically. 4.2 Experimental Results SCFG STSG a larger span than SCFG. It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al., 2004)4. 3) All the thre"
C08-1138,2006.amta-papers.8,0,\N,Missing
C08-1138,2007.mtsummit-papers.8,0,\N,Missing
C08-1138,P00-1057,0,\N,Missing
C08-1138,C00-2092,0,\N,Missing
C08-1138,P03-1054,0,\N,Missing
C08-1138,P02-1040,0,\N,Missing
C08-1138,P06-1123,0,\N,Missing
C08-1138,P05-1067,0,\N,Missing
C08-1138,P03-2041,0,\N,Missing
C08-1138,W06-1628,0,\N,Missing
C08-1138,W06-1606,0,\N,Missing
C08-1138,P05-1033,0,\N,Missing
C08-1138,N03-1017,0,\N,Missing
C10-2086,J97-3002,0,0.313212,"Missing"
C10-2086,J98-4004,0,0.399118,"d of phrasal/constituent level model. In our model, with the help of the alignment and the head-modifier dependency based relationship in the source side, the reordering type of each target word with alignment in source side is identified as one of pre-defined reordering types. With these reordering types, the reordering of phrase in translation is estimated on word level. 748 Coling 2010: Poster Volume, pages 748–756, Beijing, August 2010 Fig 1. An Constituent based Parse Tree 2 Baseline ing capacity of the translation model. Instead of directly employing the parse tree fragments (Bod, 1992; Johnson, 1998) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Kl"
C10-2086,2006.amta-papers.8,0,0.0359001,"Missing"
C10-2086,J07-2003,0,0.119531,"Missing"
C10-2086,P06-1077,0,0.0454593,"Missing"
C10-2086,P01-1067,0,0.0775597,"uccessfully applied to SMT to improve translation performance. Research in applying syntax information to SMT has been carried out in two aspects. On the one hand, the syntax knowledge is employed by directly integrating the syntactic structure into the translation rules i.e. syntactic translation rules. On this perspective, the word order of the target translation is modeled by the syntax structure explicitly. Chiang (2005), Wu (1997) and Xiong (2006) learn the syntax rules using the formal grammars. While more research is conducted to learn syntax rules with the help of linguistic analysis (Yamada and Knight, 2001; Graehl and Knight, 2004). However, there are some challenges to these models. Firstly, the linguistic analysis is far from perfect. Most of these methods require an off-the-shelf parser to generate syntactic structure, which makes the translation results sensitive to the parsing errors to some extent. To tackle this problem, n-best parse trees and parsing forest (Mi and Huang, 2008; Zhang, 2009) are proposed to relieve the error propagation brought by linguistic analysis. Secondly, some phrases which violate the boundary of linguistic analysis are also useful in these models ( DeNeefe et al."
C10-2086,P08-1114,0,0.0221605,"To tackle this problem, n-best parse trees and parsing forest (Mi and Huang, 2008; Zhang, 2009) are proposed to relieve the error propagation brought by linguistic analysis. Secondly, some phrases which violate the boundary of linguistic analysis are also useful in these models ( DeNeefe et al., 2007; Cowan et al. 2006). Thus, a tradeoff needs to be found between linguistic sense and formal sense. On the other hand, instead of using syntactic translation rules, some previous work attempts to learn the syntax knowledge separately and then integrated those knowledge to the original constraint. Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. By doing so, this approach addresses the challenges brought by linguistic analysis through the log-linear model in a soft way. Starting from the state-of-the-art phrase based model Moses ( Koehn e.t. al, 2007), we propose a head-modifier relation based reordering model and use the proposed model as a soft syntax constraint in the phrase-based translation framework. Compared with most of previous soft constraint models, we study the way to utilize the constituent based parse tre"
C10-2086,P08-1066,0,0.0281483,"Missing"
C10-2086,P03-1054,0,0.0259913,"8) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Klein and Manning (2003) and reordering-related components: language model, Collins (1999), there are two shortcomings in nlexical reordering model and distortion model. ary Treebank grammar. Firstly, the grammar is The language model can reorder the local target too coarse for parsing. The rules in different words within a fixed window in an implied way. context always have different distributions. SeThe lexical reordering model and distortion condly, the rules learned from training corpus reordering model tackle the reordering problem cannot cover the rules in testing set. Currently, the state-of-the-art parsing al"
C10-2086,P96-1025,0,0.203111,"ide is identified as one of pre-defined reordering types. With these reordering types, the reordering of phrase in translation is estimated on word level. 748 Coling 2010: Poster Volume, pages 748–756, Beijing, August 2010 Fig 1. An Constituent based Parse Tree 2 Baseline ing capacity of the translation model. Instead of directly employing the parse tree fragments (Bod, 1992; Johnson, 1998) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to ebest = argmaxe p ( e |f ) pLM ( e ) ωlength(e) (1) sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the where p(e|f) can be computed using phrase constituent based parse tree with the help of translation model, distortion model and lexical head rules ( Bikel, 2004 ). reordering model. pLM(e) can be computed using the language model. ωlength(e) is word penalty 3.1 Head-modifier Relation model. Among the above models, there are three According to Klein and Manning (2003) and reordering-related components: language model, Collins (1999), there are two shortcomings in nlexical reordering model and distortion model. ary Treebank grammar. Firstly, the grammar is The l"
C10-2086,P96-1021,0,0.0531225,"stituents in parse tree. Chiang(2005), Marton and Resnik(2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang(2009) added tree-kernel to BTG model. All these studies show promising results. Making soft constrain is an easy and 755 efficient way in adding linguistic analysis into formal sense SMT model. In modeling the reordering, most of previous studies are on phrase level. In Moses, the lexical reordering is modeled on adjacent phrases. In (Wu, 1996; Xiong, 2006), the reordering is also modeled on adjacent translated phrases. In hiero, the reordering is modeled on the segments of the unmotivated translation rules. The tree-tostring models (Yamada et al. 2001; Liu et al.2006) are model on phrases with syntax representations. All these studies show excellent performance, while there are few studies on word level model in recent years. It is because, we consider, the alignment in word level model is complex which limits the reordering capacity of word level models. However, our work exploits a new direction in reordering that, by utilizing"
C10-2086,P06-1066,0,0.0524097,"Missing"
C10-2086,P09-1036,1,0.886552,"Missing"
C10-2086,2009.mtsummit-posters.25,1,0.792708,"Missing"
C10-2086,P07-2045,0,0.00524484,"Missing"
C10-2086,N03-1017,0,0.0551136,"Missing"
C10-2086,W06-1628,0,0.0343592,"Missing"
C10-2086,N04-1014,0,\N,Missing
C10-2086,koen-2004-pharaoh,0,\N,Missing
C10-2086,D08-1022,0,\N,Missing
C10-2086,J02-1005,0,\N,Missing
C10-2086,J03-4003,0,\N,Missing
C10-2086,P09-1020,1,\N,Missing
C10-2086,P08-1064,1,\N,Missing
C10-2086,P08-2038,1,\N,Missing
C10-2086,P05-1033,0,\N,Missing
C10-2086,D07-1079,0,\N,Missing
C10-2086,P00-1056,0,\N,Missing
C10-2175,P07-1111,0,0.0287965,"Missing"
C10-2175,P09-1035,0,0.0879121,"Missing"
C10-2175,P06-2003,0,0.0335086,"Missing"
C10-2175,W05-0909,0,0.161374,"Missing"
C10-2175,E06-1032,0,0.0616036,"Missing"
C10-2175,W07-0738,0,0.0359518,"Missing"
C10-2175,I08-1042,0,0.050271,"Missing"
C10-2175,P03-1054,0,0.00555934,"Missing"
C10-2175,P04-1077,0,0.148674,"Missing"
C10-2175,niessen-etal-2000-evaluation,0,0.21254,"Missing"
C10-2175,W06-3112,0,0.0419574,"Missing"
C10-2175,W07-0714,0,0.462946,"Missing"
C10-2175,P02-1040,0,0.0818322,"Missing"
C10-2175,W07-0707,0,0.0315761,"Missing"
C10-2175,W09-0402,0,0.0540264,"Missing"
C10-2175,W05-0904,0,0.589594,"Missing"
C10-2175,2006.amta-papers.25,0,0.101418,"Missing"
C10-2175,N07-1006,0,0.0264202,"Missing"
C10-2175,2007.tmi-papers.15,0,0.564511,"Missing"
C10-2175,N03-2021,0,\N,Missing
C10-2175,P08-1007,0,\N,Missing
C10-2175,P05-1035,0,\N,Missing
C10-2175,C04-1046,0,\N,Missing
C18-1181,D15-1075,0,0.0167856,"ing semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the sentence vectors can be used for visualization, sentence clustering and many other purposes (Wang et al., 2016a). One of the first attempts at applying deep learning to answer selection was the bag-of-words model proposed by (Yu et al., 2014). The model generates the vector representation of a sentence by simply taking the average of all the word vectors in the sentence - having previously removed all the stop words. Integrating additional overlapping word coun"
C18-1181,D17-1070,0,0.0244326,"tentive architectures. Even though the boundaries are not crystal clear, separating the existing different neural architectures into the three categories can provide the big picture more easily. 2135 Figure 2: The general architecture of a Siamese model. The same encoder is used to generate the vector representations for the input sentences. 2.2.1 Siamese Architecture Siamese neural networks have been proposed for a number of sentence pair modeling tasks, including semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the"
C18-1181,N16-1108,0,0.0646547,"ean TrecQA) - TRAIN-ALL unigram+count (Yu et al., 2014) TRAIN-ALL bigram+count (Yu et al., 2014) QA-LSTM (Tan et al., 2015) QA-LSTM with attention (Tan et al., 2015) QA-LSTM/CNN (Tan et al., 2015) Attentive Pooling CNN (dos Santos et al., 2016) (Severyn and Moschitti, 2015) L.D.C Model (Wang et al., 2016b) Pointwise Pointwise Siamese 0.711 - Pairwise Pairwise Siamese Attentive - 0.682 0.690 Pairwise Pairwise Siamese Attentive - 0.706 0.753 Pointwise Pointwise 0.746 - 0.771 0.758 - Pointwise Pairwise Siamese CompareAggregate CompareAggregate Siamese Siamese Pairwise Word Interaction Modelling (He and Lin, 2016) Multi-Perspective CNN (He et al., 2015) HyperQA (Hyperbolic Embeddings) (Tay et al., 2018a) PairwiseRank+Multi-Perspective CNN (Rao et al., 2016) BiMPM (Shen et al., 2017) Pointwise 0.762 0.770 0.777 0.784 Pairwise Siamese 0.780 0.801 Pointwise CompareAggregate CompareAggregate CompareAggregate CompareAggregate CompareAggregate - 0.802 Dynamic-Clip Attention (Bian et al., 2017) IWAN (Shen et al., 2017) Listwise - 0.821 - 0.822 IWAN+CARNN (Tran et al., 2018) Pointwise - 0.829 MCAN (Tay et al., 2018b) Pointwise - 0.838 Pointwise Table 1: Overview of existing deep learning methods to answer sele"
C18-1181,D15-1181,0,0.304897,"et al., 2014) TRAIN-ALL bigram+count (Yu et al., 2014) QA-LSTM (Tan et al., 2015) QA-LSTM with attention (Tan et al., 2015) QA-LSTM/CNN (Tan et al., 2015) Attentive Pooling CNN (dos Santos et al., 2016) (Severyn and Moschitti, 2015) L.D.C Model (Wang et al., 2016b) Pointwise Pointwise Siamese 0.711 - Pairwise Pairwise Siamese Attentive - 0.682 0.690 Pairwise Pairwise Siamese Attentive - 0.706 0.753 Pointwise Pointwise 0.746 - 0.771 0.758 - Pointwise Pairwise Siamese CompareAggregate CompareAggregate Siamese Siamese Pairwise Word Interaction Modelling (He and Lin, 2016) Multi-Perspective CNN (He et al., 2015) HyperQA (Hyperbolic Embeddings) (Tay et al., 2018a) PairwiseRank+Multi-Perspective CNN (Rao et al., 2016) BiMPM (Shen et al., 2017) Pointwise 0.762 0.770 0.777 0.784 Pairwise Siamese 0.780 0.801 Pointwise CompareAggregate CompareAggregate CompareAggregate CompareAggregate CompareAggregate - 0.802 Dynamic-Clip Attention (Bian et al., 2017) IWAN (Shen et al., 2017) Listwise - 0.821 - 0.822 IWAN+CARNN (Tran et al., 2018) Pointwise - 0.829 MCAN (Tay et al., 2018b) Pointwise - 0.838 Pointwise Table 1: Overview of existing deep learning methods to answer selection whole candidate sentence space. An"
C18-1181,N10-1145,0,0.0586564,"iera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http"
C18-1181,P14-1062,0,0.0360067,"15) proposed the QA-LSTM model that employs a bidirectional long short-term memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) and a pooling layer to construct distributed vector representations of the input sentences independently. Then the model utilizes cosine similarity to measure the distance of the sentence representations. Severyn and Moschitti (2015) proposed a model that employs a convolutional neural network (CNN) to generate the representations of the input sentences. The CNN is based on an architecture that has previously been applied to many sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). In (He et al., 2015), each input sentence is modeled using a CNN that extracts features at multiple levels of granularity and uses multiple types of pooling. The representations of the input sentences are then compared at several granularities using multiple similarity metrics. Finally, the comparison results are fed into a fully connected layer to obtain the final relevance score. The proposed model outperforms many other Siamese models. Tay et al. (2018a) proposed a simple but novel deep learning architecture that models the relationship between a question and a candidate sente"
C18-1181,D14-1181,0,0.00553191,"del that employs a bidirectional long short-term memory (biLSTM) network (Hochreiter and Schmidhuber, 1997) and a pooling layer to construct distributed vector representations of the input sentences independently. Then the model utilizes cosine similarity to measure the distance of the sentence representations. Severyn and Moschitti (2015) proposed a model that employs a convolutional neural network (CNN) to generate the representations of the input sentences. The CNN is based on an architecture that has previously been applied to many sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). In (He et al., 2015), each input sentence is modeled using a CNN that extracts features at multiple levels of granularity and uses multiple types of pooling. The representations of the input sentences are then compared at several granularities using multiple similarity metrics. Finally, the comparison results are fed into a fully connected layer to obtain the final relevance score. The proposed model outperforms many other Siamese models. Tay et al. (2018a) proposed a simple but novel deep learning architecture that models the relationship between a question and a candidate sentence in Hyper"
C18-1181,W18-3105,1,0.871041,"Missing"
C18-1181,D15-1166,0,0.0275785,"iamese Architecture. In a Siamese architecture (Bromley et al., 1993), the same encoder (e.g., a CNN or a RNN) is used to build the vector representations for the input sentences (i.e., the candidate answer and the question) individually. After that, the relevance score is determined solely based on the encoded vectors. There is no explicit interaction between the input sentences during the encoding process. • Attentive Architecture. Instead of generating representations for the candidate answer and the question independently, attention mechanisms (Bahdanau et al., 2014; Hermann et al., 2015; Luong et al., 2015) can be used to allow the information from an input sentence to influence the computation of the other’s representation (Tan et al., 2015; dos Santos et al., 2016). Even though the weakness of the Siamese models is alleviated, the interaction between the input sentences during the encoding process is still minimal in most Attentive architectures. • Compare-Aggregate Architecture. The Compare-Aggregate architectures can capture more interactive features between input sentences than the Siamese architectures and the Attentive architectures, therefore typically have better performance when evalua"
C18-1181,P17-2081,0,0.100225,"e, and listwise) (ii) neural network architectures (Siamese architecture, Attentive architecture, and Compare-Aggregate architecture). In addition, we examine the most popular datasets and the evaluation metrics for answer selection. Below we discuss several promising future research directions. Transfer learning (Pan and Yang, 2010) has achieved success in domains such as speech recognition (Huang et al., 2013), computer vision (Razavian et al., 2014), and natural language processing (Zhang et al., 2017). Its applicability to question answering and answer selection has recently been studied (Min et al., 2017; Chung et al., 2017). Min et al. (2017) created SQuAD-T, a modification of the original large-scale SQuAD dataset (Rajpurkar et al., 2016) to allow for directly training and evaluating answer selection systems. Through a basic transfer learning technique from SQuAD-T, the state-of-the-art result in the WikiQA dataset can be improved. This demonstrates the potential of developing novel transfer learning techniques for the answer selection task. Many deep learning methods for answer selection are applicable to other sentence pair modeling tasks such as natural language inference (He and Lin, 20"
C18-1181,D14-1162,0,0.0805594,"k on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: A typical question answering pipeline architecture, adapted from (Sequiera et al., 2017) While previous work has recognized the increasing use of deep learning techniques in natural language processing (Young et al., 2017), no systematic survey of deep learning methods for answer selection to"
C18-1181,D17-1122,0,0.0688256,"most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018."
C18-1181,P15-1150,0,0.0284415,"sed for a number of sentence pair modeling tasks, including semantic similarity (Mueller and Thyagarajan, 2016), paraphrase identification (Hu et al., 2014), and natural language inference (Conneau et al., 2017). Figure 2 shows the general architecture of a Siamese model. The vector representations of the input sentences are built separately by the encoder. Two input sentences have no influence on the computation of each other’s representation. After that, the encoded vectors are compared using measures such as cosine similarity (Feng et al., 2015; Yang et al., 2015), element-wise operations (Tai et al., 2015), or neural network-based combination (Bowman et al., 2015). An advantage of this architecture is that applying the same encoder to each input sentence makes the model smaller. In addition, the sentence vectors can be used for visualization, sentence clustering and many other purposes (Wang et al., 2016a). One of the first attempts at applying deep learning to answer selection was the bag-of-words model proposed by (Yu et al., 2014). The model generates the vector representation of a sentence by simply taking the average of all the word vectors in the sentence - having previously removed all t"
C18-1181,N18-1115,1,0.531494,"he question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2132 Proceedings of the 27th International Conference on Computational Linguistics, pages 2132–2144 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: A typical"
C18-1181,C10-1131,0,0.0475597,"06; Ferrucci, 2012; Sequiera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4"
C18-1181,D07-1003,0,0.390006,"uestion (Prager, 2006; Ferrucci, 2012; Sequiera et al., 2017). Figure 1 depicts a typical question answering pipeline. In this setup, answer selection can be applied to identify the sentences that are most relevant to the question within the retrieved documents. Besides its application in open domain question answering, the techniques developed for answer selection can be potentially used to predict answer quality in community question answering (cQA) sites (Nakov et al., 2015). Previous work on answer selection typically relies on feature engineering, linguistic tools, or external resources (Wang et al., 2007; Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013; Yao et al., 2013). Recently, many deep learning based methods have been proposed for the task (Bian et al., 2017; Shen et al., 2017; Tran et al., 2018). They outperform traditional techniques. In addition, they do not need any feature-engineering effort or hand-coded resources beyond some large unlabeled corpus on which to learn the initial word embeddings, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativec"
C18-1181,K16-1004,0,0.360352,"lue indicating whether cij contains the correct answer to qi . It is enough to train a binary classifier: hθ (qi , cij ) → yˆij , where 0 ≤ yˆij ≤ 1. For example, in (Yu et al., 2014), the training objective is to minimize the cross entropy of all labelled questioncandidate pairs in the training set. During inference, given a question, the trained classifier hθ is used to rank every candidate sentence, and the top-ranked candidate is selected (i.e., argmaxcij hθ (qi , cij ) should be selected as the answer to qi ). Many work adopted this approach (Yu et al., 2014; Severyn and Moschitti, 2015; Wang et al., 2016b; Shen et al., 2017). The second approach to ranking is the pairwise approach, where the ranking function hθ is explicitly trained to score correct candidate sentences higher than incorrect sentences. Given a question, the approach takes a pair of candidate answer sentences and explicitly learns to predict which sentence is more − relevant to the question. For example, in (Feng et al., 2015), the training instances are triples (qi , c+ i , ci ), + − where qi is a question, ci is a correct sentence for qi , and ci is an incorrect sentence sampled from the 2133 Method Learning Approach Model Ar"
C18-1181,C16-1127,0,0.153924,"lue indicating whether cij contains the correct answer to qi . It is enough to train a binary classifier: hθ (qi , cij ) → yˆij , where 0 ≤ yˆij ≤ 1. For example, in (Yu et al., 2014), the training objective is to minimize the cross entropy of all labelled questioncandidate pairs in the training set. During inference, given a question, the trained classifier hθ is used to rank every candidate sentence, and the top-ranked candidate is selected (i.e., argmaxcij hθ (qi , cij ) should be selected as the answer to qi ). Many work adopted this approach (Yu et al., 2014; Severyn and Moschitti, 2015; Wang et al., 2016b; Shen et al., 2017). The second approach to ranking is the pairwise approach, where the ranking function hθ is explicitly trained to score correct candidate sentences higher than incorrect sentences. Given a question, the approach takes a pair of candidate answer sentences and explicitly learns to predict which sentence is more − relevant to the question. For example, in (Feng et al., 2015), the training instances are triples (qi , c+ i , ci ), + − where qi is a question, ci is a correct sentence for qi , and ci is an incorrect sentence sampled from the 2133 Method Learning Approach Model Ar"
C18-1181,D15-1237,0,0.653065,"n et al., 2017) adopted the pointwise approach, this approach is not close to the nature of ranking. The pairwise approach and the listwise approach exploit more information about the ground truth ordering of candidate sentences. (Rao et al., 2016) proposed a pairwise ranking approach that can directly exploit existing pointwise neural network models as base components. The approach outperforms many competitive pointwise baselines. (Bian et al., 2017) showed that the listwise approach performs better than the pointwise approach on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). In the next section, we describe various neural network architectures for modeling the ranking function hθ , which takes a question-candidate pair and returns a score indicating whether the candidate is relevant to the question. 2.2 Neural Network Architectures There are three main types of general architectures for measuring the relevance of a candidate sentence to a question. • Siamese Architecture. In a Siamese architecture (Bromley et al., 1993), the same encoder (e.g., a CNN or a RNN) is used to build the vector representations for the input sentences (i.e., the candidate answer and the"
C18-1181,N13-1106,0,0.0765149,"Missing"
C18-1181,Q17-1036,0,0.0412026,"deep learning methods for answer selection along two dimensions: (i) learning approaches (pointwise, pairwise, and listwise) (ii) neural network architectures (Siamese architecture, Attentive architecture, and Compare-Aggregate architecture). In addition, we examine the most popular datasets and the evaluation metrics for answer selection. Below we discuss several promising future research directions. Transfer learning (Pan and Yang, 2010) has achieved success in domains such as speech recognition (Huang et al., 2013), computer vision (Razavian et al., 2014), and natural language processing (Zhang et al., 2017). Its applicability to question answering and answer selection has recently been studied (Min et al., 2017; Chung et al., 2017). Min et al. (2017) created SQuAD-T, a modification of the original large-scale SQuAD dataset (Rajpurkar et al., 2016) to allow for directly training and evaluating answer selection systems. Through a basic transfer learning technique from SQuAD-T, the state-of-the-art result in the WikiQA dataset can be improved. This demonstrates the potential of developing novel transfer learning techniques for the answer selection task. Many deep learning methods for answer selecti"
D09-1051,P06-1120,0,0.174786,"w. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words i"
D09-1051,C04-1141,0,0.115627,"cations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocati"
D09-1051,J93-2003,0,0.0505828,"Missing"
D09-1051,J93-1003,0,0.0325009,"ns in this paper include phrasal verbs (e.g. &quot;put on&quot;), proper nouns (e.g. &quot;New York&quot;), idioms (e.g. &quot;dry run&quot;), compound nouns (e.g. &quot;ice cream&quot;), correlative conjunctions (e.g. &quot;either … or&quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &quot;break rules&quot;, &quot;strong tea&quot;, &quot;softly whisper&quot;, &quot;fully aware&quot;, and &quot;fond of&quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be part"
D09-1051,pearce-2002-comparative,0,\N,Missing
D09-1051,J90-1003,0,\N,Missing
D09-1051,J93-1007,0,\N,Missing
D13-1085,D07-1074,0,0.14966,"ot, and his opinion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to o"
D13-1085,N13-1122,0,0.171725,"ask is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL"
D13-1085,P11-1095,0,0.0370182,"Missing"
D13-1085,P11-1115,0,0.0346173,"EL can significantly improve the performance • We annotate a microblog entity linking corpus which is comparable to an existing long text corpus. 864 • We show the inefficiency of previous method on the microblog corpus and our method can significantly improve the results. 2 Task defination The microblog entity linking task is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Metho"
D13-1085,P13-1128,0,0.0311755,"mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL graph. p1 . . . p4 are"
D13-1085,N13-1039,0,0.0970558,"Missing"
D13-1085,P11-1138,0,0.021886,"Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proceedings of the 2013 Conference on Empirical Me"
D13-1085,P10-1149,0,0.0300152,"Missing"
D13-1085,N10-1072,0,0.0212542,"25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We 866 Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emoticons, discourse markers and URLs in the po"
D15-1106,J08-1001,0,0.0939993,"Missing"
D15-1106,J97-3002,0,0.0402534,"of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we"
D15-1106,D14-1218,0,0.181084,"the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural network"
D15-1106,P06-1066,0,0.0156601,"3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. Setting IWSLT Baseline SMT + Rerank tst2010 11.12 12.40 12.55 tst2011 13.34 15.09 15.23 tst2012 13.52 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we can find that the rerank system improves SMT performance consistently. For a single sentence without the"
D15-1106,P14-1140,1,0.746216,"Missing"
D15-1106,P13-1017,1,0.460566,"olov et al., 2010) uses a hidden layer which employs a real-valued vector recurrently as network’s input to keep as many history as possible. This makes RNNLM be able to extend for capturing history beyond a sentence. To prevent the potential exponential decay of the history, the history length in RNN can not be too long. Here we approximate the history information of previous sentences, p(Sk |S1 , S2 , ..., Sk−1 ), by the following: For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2 NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying"
D15-1106,P03-1021,0,0.0435395,"two Chinese sentence in one document together with their correct translation: 我 拍摄 过 的 冰山, 有些 冰 是 非常 年 轻 - - 几千 年 年龄 Some of the ice in the icebergs that I photograph is very young - - a couple thousand years old. 有些 冰 超过 十万 年 And some of the ice is over 100,000 years old. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document. Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank905 Chinese word “ 有 些” means “some” in English. But when it is used in parallelism sentences, it means “some of” instead of “some”. The traditional SMT system translates the italics part without considering the context. The translation result for this kind"
D15-1106,P02-1040,0,0.10845,"Missing"
D15-1106,D13-1170,0,0.00405147,"dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. Related work An attempt of introducing RNN into convolutional neural networ"
D15-1106,2012.eamt-1.60,0,\N,Missing
I05-2003,J93-2003,0,\N,Missing
I05-2003,C04-1069,0,\N,Missing
I05-5007,W04-3219,0,0.126464,"Missing"
I05-5007,W03-1604,0,0.0610289,"Missing"
I05-5007,W03-1610,0,0.0337717,"Missing"
I05-5007,P03-1016,0,0.0311076,"Missing"
I05-5007,W03-1605,0,0.0361287,"Missing"
I05-5007,P01-1008,0,0.159123,"Missing"
I05-5007,N03-1003,0,0.194012,"Missing"
I05-5007,W03-1609,0,0.0334474,"Missing"
I05-5007,C02-1056,0,0.0556653,"Missing"
I08-2109,P06-2010,1,0.900752,"hods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significant"
I08-2109,P02-1031,0,0.0363188,"verbs or nouns and some constituents of the sentence. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure"
I08-2109,P04-1043,0,0.078076,"ce. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate"
I08-2109,N06-2025,0,0.0686209,"= opt([d]) = 1, p = 3. Then according to Eq (14), ∆p (cn1 , cn2 ) can be calculated recursively as Eq. (15) (Please refer to the next page). Finally, we have ∆p (cn1 , cn2 ) = λ1 × ∆0 (a, a) × 0 ∆ (b, b) × ∆0 (c, c) By means of the above algorithm, we can compute the ∆0 (n1 , n2 ) in O(p|cn1 |· |cn2 |2 ) (Lodhi et al., 2002). This means that the worst case complexity of the FGTK-I is O(pρ3 |N1 |· |N2 |2 ), where ρ is the maximum branching factor of the two trees. 3.2 Fast Grammar-driven Convolution Tree Kernel II (FGTK-II) Our FGTK-II algorithm is motivated by the partial trees (PTs) kernel (Moschitti, 2006). The PT kernel algorithm uses the following recursive formulas to evaluate ∆p (cn1 , cn2 ): |cn1 ||cn2 | ∆p (cn1 , cn2 ) = X X ∆0p (cn1 [1 : i], cn2 [1 : j]) (16) i=1 j=1 where cn1 [1 : i] and cn2 [1 : j] are the child subsequences of cn1 and cn2 from 1 to i and from 1 to j, respectively. Given two child node sequences s1 a = cn1 [1 : i] and s2 b = cn2 [1 : j] (a and b are the last children), the PT kernel computes ∆0p (·, ·) as follows:  0 ∆p (s1 a, s2 b) = µ2 ∆0 (a, b)Dp (|s1 |, |s2 |) 0 if a = b else (17) where ∆0 (a, b) is defined in Eq. (7) and Dp is recursively defined as follows: Dp ("
I08-2109,W05-0639,0,0.0418752,"Missing"
I08-2109,P07-1026,1,0.892395,"in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significantly outperforms the TK (Zhang et al., 2007). Theoretically, the GTK method is applicable to any problem that uses syntax structure features and can be solved by the TK methods, such as parsing, relation extraction, and so on. In this paper, we use SRL as an application to test our proposed algorithms. Although the GTK shows promising results"
I08-2109,W05-0620,0,\N,Missing
I11-1113,E06-1002,0,0.0456043,"mental settings, results and analysis are presented in 1010 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to"
I11-1113,D07-1074,0,0.922986,"Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track eva"
I11-1113,C10-1032,0,0.285777,"ation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap"
I11-1113,J97-4004,0,0.0148976,": 6: 7: 8: 9: 10: Similarly, for the in-degree measure we build the graph in Algorithm 2, where Cn is the name node set of the candidate entities and Na is the article node set of the neighboring entities. Algorithm 2 In-degree measure based graph construction Require: Cn and Na Ensure: Graph G = (V, E) Disambiguation 1: To build a graph for the disambiguation, we need to extract names from the context of the query (either as the name node or the article node). We use a segmentation technique which is inspired from a Chinese word segmentation algorithm, the forward maximum matching algorithm (Guo, 1997) on the context to find all the names which are included in the Wikipedia title list (i.e. all the name phrases in our Wikipedia graph are the Wikipedia article titles). This algorithm prefers to find the longest names that match with the string. Here we 6 See http://en.wikipedia.org/wiki/Wikipedia:Redirect for detailed instructions V := Ca ∪ Nn E := ∅ for all c ∈ Ca do for all n ∈ Nn do if n ∈ Article(c) then E := E ∪ (c, n) end if end for end for return (V, E) 2: 3: 4: 5: 6: 7: 8: 9: 10: V := Cn ∪ Na E := ∅ for all c ∈ Cn do for all n ∈ Na do if c ∈ Article(n) then E := E ∪ (n, c) end if end"
I11-1113,P10-1138,0,0.0597374,"Missing"
I11-1113,zesch-etal-2008-extracting,0,0.027243,"Missing"
I11-1113,N10-1072,0,0.0186403,"the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap of the relevant Wikipedia entry (Milne and Witten, 2008; Kulkarni et al., 2009; Radford et al., 2010), which implies the co1 http://www.nist.gov/tac occurrence of"
I11-1113,E09-1073,0,\N,Missing
I11-1114,P08-1004,0,0.0235139,"dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synonym / comparable entities clustering (Lin, 1998; Pantel, 2003; Wang and Cohen, 2007). Lin (1998) popularized the automatic clustering of similar words using distributional"
I11-1114,W04-3205,0,0.0246191,"presents a more sophisticated clustering algorithm that first collects a small set of representative elements for each concept and then assigns words to their mostsimilar concept. Wang and Cohen (2007) alternatively investigate set expansion problem, that is, how to retrieve similar entities given a small number of seeds. With flexible matching patterns and random walk based ranking algorithm, their system outperforms Google SetsT M in terms of Mean Average Precision (MAP). There are also researches forcusing on the relationship between verbs or adjectives, such as (Turney et al., 2003) and (Chklovski and Pantel, 2004). In comparison to previous works on relation extraction, our work does not restrict itself to identifying pairs of entities whose relation is explicitly described by “infix”-like patterns, such as “Louis XVI was born in 1754”. Alternatively, we mine related entities from context similarity and cooccurrence points of views. Moreover, through empirical studies, we find that query log is an effective data source in the extraction of related entities. On the other hand, different from synonym / comparable entities mining, our work retrieves a more extensive scope of results. In addition to entiti"
I11-1114,eichler-etal-2008-unsupervised,0,0.0143331,"2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synon"
I11-1114,C10-1058,0,0.0242895,"o vectors, and v ′ = (vi + vj )/2. Note that the larger the JSD is, the less similar two vectors are. Thus the similarity between vectors vi and vj is computed as 1 − JSD(vi , vj ). 3.1.3 Query Text Co-occurrence Web search queries represent the demand of information from the users. Queries are known to be noisy and of little syntactic structure. However, previous works have demonstrated that there is sufficient knowledge encoded in the query texts to perform information extraction tasks (Paca, 2007), and that extracting information within query logs can better represent the users’ interests (Jain and Pennacchiotti, 2010). We found from Baidu query logs that related persons are often searched together for users’ curiosity about their relationships. Such pairs of persons consist of not only those with persistent relationships like couples and friends, but also those related in certain events, especially some hot news. In this spirit, we employ a Baidu query log containing approximately 9.08 billion raw queries to extract candidate related persons. For each query person q, we traverse the query log and extract persons that co-occur with q in the same queries. We filter out the persons that co-occur with q for le"
I11-1114,N07-1015,0,0.0572023,"Missing"
I11-1114,P99-1004,0,0.0481229,"lection. We use logarithm on the frequency tfi to reduce the influence of the words with extremely high frequency. We extract candidate related persons for each query person q via selecting top 10 persons from the collection according to the contextual similarity with q. We compute the similarity between two context vectors vi and vj using Jensen-Shannon divergence (JSD), as it performs better than some other similarity computation methods, such as cosine similarity, in our experiments. We first normalize the input vectors by the sum of their components, and calculate the JSD as described in (Lee, 1999): 1 JSD(vi , vj ) = [KL(vi ∥v ′ ) + KL(vj ∥v ′ )] (2) 2 where KL denotes the Kullback-Leibler divergence between two vectors, and v ′ = (vi + vj )/2. Note that the larger the JSD is, the less similar two vectors are. Thus the similarity between vectors vi and vj is computed as 1 − JSD(vi , vj ). 3.1.3 Query Text Co-occurrence Web search queries represent the demand of information from the users. Queries are known to be noisy and of little syntactic structure. However, previous works have demonstrated that there is sufficient knowledge encoded in the query texts to perform information extractio"
I11-1114,C10-1112,0,0.0177511,"a state-ofthe-art baseline. 1 Introduction Facilitating efficient navigation in the knowledge space is essential to satisfying the current Web search demands. Named entities are vital building blocks of such a space, and retrieving related entities provides an efficient way of navigation. Related entity extraction refers to mining from text resources named entities with certain relationships between them, e.g. personaffiliation and organization-location. To this end, great efforts have been made recently in both academic (Banko et al., 2007; Wu and Weld, 2010) and industry (Zhu et al., 2009; Shi et al., 2010) circles. A wide range of NLP applications could benefit from the high-quality repository of related entities. For query suggestion in Web search and ebusiness, given a query concerning some entity e, one can suggest entities related to e, in which users ∗ This work was done when the first author was visiting Baidu. • Persons with definite relationships. The relationships in this category can be explicitly represented with definite concepts, e.g. parent, friend, colleague, etc. Most previous literature focuses on such definite relationships between persons (Brin,1998; Etzioni et al.,2005; Bank"
I11-1114,N06-1039,0,0.0209414,"cation. On the other hand, semi-supervised approaches avoid the heavy human labor in providing training examples by using bootstrapping techniques. For instance, DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), and KNOWITALL (Etzioni et al., 2005) all adopt seed-pattern iterations to aggregate related entities. Besides the works on traditional relation extraction, studies on open information extraction (Open IE) have emerged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichl"
I11-1114,I08-2119,0,0.0224889,"Missing"
I11-1114,P10-1013,0,0.0171028,"no, 2000), and KNOWITALL (Etzioni et al., 2005) all adopt seed-pattern iterations to aggregate related entities. Besides the works on traditional relation extraction, studies on open information extraction (Open IE) have emerged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF co"
I11-1114,P98-2127,0,0.0476361,"erform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synonym / comparable entities clustering (Lin, 1998; Pantel, 2003; Wang and Cohen, 2007). Lin (1998) popularized the automatic clustering of similar words using distributional similarity. Pantel (2003) presents a more sophisticated clustering algorithm that first collects a small set of representative elements for each concept and then assigns words to their mostsimilar concept. Wang and Cohen (2007) alternatively investigate set expansion problem, that is, how to retrieve similar entities given a small number of seeds. With flexible matching patterns and random walk based ranking algorithm, their system outperforms Google SetsT M in terms of"
I11-1114,P09-1113,0,0.00781033,"ged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction probl"
I11-1114,J90-1003,0,\N,Missing
I11-1114,C98-2122,0,\N,Missing
I13-1068,N09-1003,0,0.0931124,"Missing"
I13-1068,J10-4006,0,0.171311,"we focus on the semantic similarity between words. Well-known implementations of word-level distributional similarity scheme (DSS) mainly fall ∗ This work was done when the first author was visiting Baidu. 596 International Joint Conference on Natural Language Processing, pages 596–604, Nagoya, Japan, 14-18 October 2013. which distributional similarity and semantic relations are available. Third, our DSS’s hierarchical nature allows us to individually replace each component with better implementations to adapt to specific applications or new languages. sibling / hypernym / hyponym relations (Baroni and Lenci, 2010; Bansal and Klein, 2012), concept properties (Baroni et al., 2010), and attribute information (Baroni and Lenci, 2010). Compared with these studies, our approach systematically exploits specific semantic relations instead of counting cooccurrence under surface patterns. We also develop a hierarchical similarity fusion architecture, rather than blending the heterogeneous evidences in a single distribution vector (Baroni and Lenci, 2010). It is also notable that sibling term extraction, in which various semantic evidences (e.g. hypernyms) also help, is not in the track of our study. Sibling ter"
I13-1068,P08-1028,0,0.0378182,". A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relati"
I13-1068,D08-1094,0,0.059866,"Missing"
I13-1068,J07-2002,0,0.0665187,"Missing"
I13-1068,P06-1015,0,0.0390744,"context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-windo"
I13-1068,W13-0112,0,0.0123148,"s have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 20"
I13-1068,C92-2082,0,0.129187,"ional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relat"
I13-1068,D09-1025,0,0.0142559,"us evidences in a single distribution vector (Baroni and Lenci, 2010). It is also notable that sibling term extraction, in which various semantic evidences (e.g. hypernyms) also help, is not in the track of our study. Sibling term extraction focuses on words sharing the same super concept, and does not quantify the pair-wise similarity between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thes"
I13-1068,2003.mtsummit-papers.42,0,0.0718911,"not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relation"
I13-1068,C10-1112,0,0.0130589,"and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper,"
I13-1068,C10-1058,0,0.012838,"us still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain"
I13-1068,C08-1051,0,0.0167289,"between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thesaurus is limited, and not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contex"
I13-1068,D11-1011,0,0.0145597,"e of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-window based distributional similarity. Indeed, previous studies did resort to a closed set of lexical patterns that indicate 3 Hierarchical Semantics-aware DSS Our proposed DSS has a four-layer structure, as shown i"
I13-1068,P97-1009,0,0.270765,"Missing"
I13-1068,P10-1097,0,0.0302936,"Missing"
I13-1068,P98-2127,0,0.240588,"th this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or au"
I13-1068,J06-3003,0,0.0392718,"e et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which ex"
I13-1068,P06-2075,0,0.0260698,"does not quantify the pair-wise similarity between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thesaurus is limited, and not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Rela"
I13-1068,P11-1116,1,0.832072,"Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-window based distributional similarity. Indeed, previous studies did resort to a closed set of lexical patterns that indicate 3 Hierarchical Semantics-aware DSS Our proposed DSS has a four-lay"
I13-1068,P12-1041,0,\N,Missing
I13-1068,C98-2122,0,\N,Missing
I13-1068,D10-1113,0,\N,Missing
I13-1128,P02-1040,0,0.087697,"Missing"
I13-1128,W08-0336,0,0.0291583,"Missing"
I13-1128,P11-2071,0,0.0454951,"Missing"
I13-1128,P10-1064,0,0.045945,"Missing"
I13-1128,P03-1021,0,0.0971049,"Missing"
I13-1128,W99-0604,0,0.293756,"Missing"
I13-1128,J82-2005,0,0.757727,"Missing"
I13-1128,P11-1124,0,0.0288371,"Missing"
I13-1128,2009.mtsummit-papers.14,0,0.106923,"Missing"
I13-1128,2010.jec-1.4,0,\N,Missing
J15-4007,C08-1048,0,0.0197514,"is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem that describe the content of the picture. In addition, I have heard recently that both Microsoft and Baidu have released their chatting robots, which are named Microsoft XiaoIce and Baidu Xiaodu, respectively. They both use translation techniques in the searching and generation of chatting responses. It is fair to say that machine translation has become more than a specif"
J15-4007,J10-3010,0,0.0240955,"]. This is a Chinese old saying from “《诗经》” [The Book of Songs], which was written 2,500 years ago. It suggests that one may benefit from other people’s opinions and methods for their task. Machine translation technology is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem that describe the content of the picture. In addition, I have heard recently that both Microsoft and Baidu have released their chatting robots, which are"
J15-4007,P06-2112,0,0.0659772,"Missing"
J15-4007,P07-1108,0,0.0324859,"anguage. He could not communicate with the waiters or even read the menu. These incidents told us that solving translation problems for these resource-poor languages is urgent. Therefore, we have successively released translation services between Chinese and over 20 foreign languages. Now, we have covered languages in eight of the top ten destinations for Chinese tourists, and all the top ten foreign cities where Chinese tourists spend the most money. On this basis, we took a further step. We built translation systems between any two languages using the pivot approach (Wang, Wu, and Liu 2006; Wu and Wang 2007). For resource-poor language pairs, we use English or Chinese as the pivot language. Translation models are trained for source-pivot and pivot-target, respectively, which are then combined to form the translation model from the source to the target language. Using this model, Baidu online translation services successfully realized pairwise translation between any two of 27 languages; in total, 702 translation directions. 9. MT Methodology for Other Areas “他山之石，可以攻玉” [Stones from other hills may serve to polish the jade at hand]. This is a Chinese old saying from “《诗经》” [The Book of Songs], whi"
J15-4007,P07-1026,1,0.784173,"tructure, which can handle a large number of specific grammatical phenomena in dependency structures. This treebank has been released by the Linguistic Data Consortium (LDC) (Che, Li, and Liu 2012). We hope that more researchers can benefit from it. Based on syntactic parsing, we hoped to further explore the semantic structure and the relationship of sentences. Therefore, we carried out research on semantic role labeling, and worked on the semantic role labeling methods based on the tree kernel, including the hybrid convolution tree kernel (Che et al. 2008) and the grammar-driven tree kernel (Zhang et al. 2007). In addition, we further broadened our mind and tried to analyze the semantics of Chinese directly. We proposed semantic dependency parsing tasks that directly establish semantic-level dependencies between content words, ignoring auxiliaries and prepositions. Meanwhile, we violated the tree structure constraints, allowing one word to depend on more than one parent node, so as to form semantic 711 Computational Linguistics Volume 41, Number 4 Figure 1 Example of syntactic dependency parsing, semantic role labeling, and semantic dependency parsing in LTP-Cloud. dependency graph structures. At t"
J15-4007,P09-1094,1,0.807601,"s; in total, 702 translation directions. 9. MT Methodology for Other Areas “他山之石，可以攻玉” [Stones from other hills may serve to polish the jade at hand]. This is a Chinese old saying from “《诗经》” [The Book of Songs], which was written 2,500 years ago. It suggests that one may benefit from other people’s opinions and methods for their task. Machine translation technology is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem t"
J15-4007,C10-1149,0,0.0610467,"Missing"
K15-2004,J93-2004,0,0.0496088,"ng determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe"
K15-2004,W13-3303,0,0.0770476,"Missing"
K15-2004,P09-2004,0,0.725356,"ts and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical te"
K15-2004,P05-1018,0,0.133453,"Missing"
K15-2004,prasad-etal-2008-penn,0,0.740449,"Missing"
K15-2004,W01-1605,0,0.336334,"11), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of sentences glued together in a systematic way to form a coherent discourse. 32 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 32–36,"
K15-2004,prasad-etal-2010-exploiting,0,0.058121,"role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of senten"
K15-2004,I11-1120,0,0.0144547,"age understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of sentences glued together i"
K15-2004,H05-1044,0,0.030634,"al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may, will, shall, must, need) in Arg1 and Arg2, and their cross-product. The intuition • Connective related features: connective itself, its syntactic category, its sense class.3 • Number of left/right siblings of the connective. • The context of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. When there is no parent or siblings, it"
K15-2004,kipper-etal-2006-extending,0,0.0392896,"on-explicit sense Classification Referring to the PDTB, the non-explicit relations4 are annotated for all adjacent sentence pairs within paragraphs. So non-explicit sense classification only considers the sense of every adjacent sentence pair within a paragraph without explicit discourse relations. Our non-explicit sense classifier includes seven traditional features: Verbs: Following the work of Pitler et al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may,"
K15-2004,K15-2001,0,0.0925518,"Missing"
K15-2004,D14-1008,1,0.843423,"Missing"
K15-2004,D09-1036,0,0.24317,"Missing"
K15-2004,P11-1100,0,0.13464,"Missing"
K15-2004,P12-1106,0,0.180206,"Missing"
K16-2009,P05-1018,0,0.0218917,"Missing"
K16-2009,W01-1605,0,0.340772,"Missing"
K16-2009,prasad-etal-2008-penn,0,0.383481,"Missing"
K16-2009,D09-1036,0,0.0918186,"Missing"
K16-2009,prasad-etal-2010-exploiting,0,0.0617725,"Missing"
K16-2009,K15-2002,0,0.202279,"Missing"
K16-2009,P11-1100,0,0.0291601,"Missing"
K16-2009,P12-1106,0,0.0282517,"Missing"
K16-2009,J93-2004,0,0.0617108,"Missing"
K16-2009,W13-3303,0,0.0314232,"Missing"
K16-2009,P09-2004,0,0.458186,"ts and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations In this paper, we describe the system submission from the NLP group of Soochow university (SoNLP-DP). Our shallow discourse parser consists of multiple components in a pipeline architecture, including a connective classifier, argument labeler, explicit classifier, non-explicit classifier. Our system is evaluated on the CoNLL-2016 Shared Task clo"
K16-2009,D12-1092,1,\N,Missing
K16-2009,P14-1080,0,\N,Missing
K16-2009,K15-2001,0,\N,Missing
K16-2009,K15-2004,1,\N,Missing
K16-2009,D14-1224,1,\N,Missing
K16-2009,P12-1008,0,\N,Missing
K16-2009,D14-1008,1,\N,Missing
K16-2009,K16-2001,0,\N,Missing
K16-2011,P05-1018,0,0.0470744,"Missing"
K16-2011,kipper-etal-2006-extending,0,0.014395,"oosing one word cluster from Arg1 and the other from Arg2. course relation by merging all the first EDUs of the EDU pairs as Arg1 of the connective, and merging all the second EDUs of the EDU pairs as Arg2. Non-explicit discourse relations. If a valid EDU pair is not linked to any explicit connective, we construct a non-explicit discourse relation by regarding the first EDU as Arg1 and the second as Arg2. 2.6 Besides the above features, the research on English sense classification for non-explicit discourse relations has explored other useful features about polarity, modality, and verb class (Karin et al., 2006). Unfortunately, the shared task on Chinese does not provide relevant resources to obtain those features. Sense Classification for Explicit discourse relations Once an explicit discourse relation is identified, the sense classifier is used to predict its sense. Due to the fact the connective themselves are strong hint for their sense, we follow (Lin et al., 2014) to define a few lexical features to train a sense classifier: the connective words themselves, their partof-speeches and the previous words of each connective word. 2.7 3 Experimentation We evaluate our system on the Chinese dataset p"
K16-2011,K15-2004,1,0.703846,"Missing"
K16-2011,D12-1092,1,0.889202,"Missing"
K16-2011,D14-1224,1,0.879909,"Missing"
K16-2011,D09-1036,0,0.0710323,"Missing"
K16-2011,P11-1100,0,0.0299653,"Missing"
K16-2011,prasad-etal-2008-penn,0,0.405095,"Missing"
K16-2011,P14-1080,0,0.0317992,"Missing"
K16-2011,K15-2001,0,0.0866757,"Missing"
K16-2011,K16-2001,0,0.0387758,"Missing"
K16-2011,P12-1008,0,0.261375,"Missing"
K16-2011,W01-1605,0,\N,Missing
K16-2011,D14-1008,1,\N,Missing
O01-2004,P91-1022,0,0.082527,"Missing"
O01-2004,P93-1001,0,0.0434082,"Missing"
O01-2004,C94-2119,0,0.0380266,"Missing"
O01-2004,W09-4205,0,0.062288,"Missing"
O01-2004,C92-2101,0,0.061944,"Missing"
O01-2004,C00-1075,0,0.0265163,"Missing"
O01-2004,J93-2004,0,0.0354613,"Missing"
O01-2004,P98-2139,0,0.0592392,"Missing"
O01-2004,1997.tmi-1.13,0,0.107408,"Missing"
O01-2004,C96-1040,0,0.0598061,"Missing"
O01-2004,P98-2212,0,0.0382456,"Missing"
O01-2004,1993.tmi-1.25,0,0.14908,"Missing"
O01-2004,C00-2131,0,0.0446209,"Missing"
O01-2004,P95-1033,0,0.55593,"Missing"
O01-2004,J97-3002,0,0.330471,"Missing"
O01-2004,W00-1211,1,0.841686,"Missing"
O01-2004,W97-0119,0,\N,Missing
O01-2004,J93-2003,0,\N,Missing
O01-2004,1995.tmi-1.28,0,\N,Missing
O01-2004,J97-2004,0,\N,Missing
O01-2004,P00-1050,0,\N,Missing
O01-2004,W95-0106,0,\N,Missing
P06-1058,P91-1034,0,0.166095,"s the value of EP for unsupervised WSD. 1 Introduction Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. It is an important technique for applications such as information retrieval, text mining, machine translation, text classification, automatic text summarization, and so on. Statistical solutions to WSD acquire linguistic knowledge from the training corpus using machine learning technologies, and apply the knowledge to disambiguation. The first statistical model of WSD was built by Brown et al. (1991). Since then, most machine learning methods have been applied to WSD, including decision tree, Bayesian model, neural network, SVM, maximum entropy, genetic algorithms, and so on. For different learning methods, supervised methods usually achieve good performance at a cost of human tagging of training corpus. The precision improves with larger size of training corpus. Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods. Thus, knowledge acquisition is critical to WSD methods. This paper propo"
P06-1058,P04-1039,0,0.0364432,"Missing"
P06-1058,W04-1609,0,0.0455949,"Missing"
P06-1058,P02-1033,0,0.0295354,"further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it st"
P06-1058,1992.tmi-1.9,0,0.418271,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,P92-1032,0,0.0812357,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,W02-0808,0,0.0785777,"Missing"
P06-1058,P02-1044,0,0.0136108,"l Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it still need human intervention and is not a completely unsupervised method. Large-scale parallel corpus; especially wordaligned corpus is highly unobtainable, which has limited the WSD methods based on parallel corpus. 3 Equivalent Pseudoword This section describes how to obtain equivalent pseudowords without a seed corpus. Monosemous words are unambiguous priori knowledge. According"
P06-1058,mihalcea-2002-bootstrapping,0,0.0531623,"involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment"
P06-1058,N03-2023,0,0.154789,"n be used as a knowledge source for WSD. 3.1 Definition of Equivalent Pseudoword If the ambiguous words in the corpus are replaced with its synonymous monosemous word, then is it convenient to acquire knowledge from raw corpus? For example in table 1, the ambiguous word &quot;把握&quot; has three senses, whose synonymous monosemous words are listed on the right column. These synonyms contain some information for disambiguation task. An artificial ambiguous word can be coined with the monosemous words in table 1. This process is similar to the use of general pseudowords (Gale et al., 1992b; Gaustad, 2001; Nakov and Hearst, 2003), but has some essential differences. This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does. Thus, we call it an equivalent pseudoword (EP) for its equivalence with the real ambiguous word. It's apparent that the equivalent pseudoword has provided a new way to unsupervised WSD. S1 信心/自信心 把握(ba3 wo4) S2 握住/在握/把住/抓住/控制 S3 领会/理解/领悟/深谙/体会 Table 1. Synonymous Monosemous Words for the Ambiguous Word &quot;把握&quot; The equivalence of the EP with the real ambiguous word is a kind of semantic synonym or similarit"
P06-1058,P03-1058,0,0.0859216,"Missing"
P06-1058,P05-1049,0,0.0167964,"solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervise"
P06-1058,J98-1004,0,0.0966695,"P-based WSD It is based on the following assumptions that EPs can substitute the original ambiguous word for knowledge acquisition in WSD model training. Assumption 1: Words of the same meaning play the same role in a language. The sense is an important attribute of a word. This plays as the basic assumption in this paper. Assumption 2: Words of the same meaning occur in similar context. This assumption is widely used in semantic analysis and plays as a basis for much related research. For example, some researchers cluster the contexts of ambiguous words for WSD, which shows good performance (Schutze, 1998). Because an EP has a higher similarity with the ambiguous word in syntax and semantics, it is a useful knowledge source for WSD. 3.4 Implementation of the EP-based Solution 3.3 Design and Construction of EPs Because of the special characteristics of EPs, it's more difficult to construct an EP than a general pseudo word. To ensure the maximum similarity between the EP and the original ambiguous word, the following principles should be followed. 1) Every EP should map to one and only one original ambiguous word. 2) The morphemes of an EP should map one by one to those of the original ambiguous"
P06-1058,P94-1013,0,0.0628952,"d corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab ("
P06-1058,P95-1026,0,0.70339,"Missing"
P06-1058,J04-1001,0,\N,Missing
P06-2010,W05-0627,1,0.831541,"based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc. The standard features with polynomial kernel gets the best performance. The reason is that the arbitrary binary combination among features implicated by the polynomial kernel is useful to SRL. We believe that combining the two methods can perform better. In order to make full use of the syntactic information and the standard flat features, we present a composite kernel between hybrid kernel (Khybrid ) and standard features with polynomial Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with t"
P06-2010,P98-1013,0,0.0108697,"l for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model th"
P06-2010,W04-2412,0,0.0431802,"Missing"
P06-2010,J93-2004,0,0.0306892,"hybrid convolution tree kernel, Khybrid . The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. Since the size of a parse tree is not constant, we normalize K(T1 , T2 ) by dividing it by p K(T1 , T1 ) · K(T2 , T2 ) 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. 4.3 Comparison with Previous Work It would be interesting to investigate the differences between our method and the feature-based methods. The basic"
P06-2010,W05-0620,0,0.262008,"Missing"
P06-2010,P04-1043,0,0.365021,"and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows"
P06-2010,A00-2018,0,0.0820962,"Missing"
P06-2010,W03-0423,0,0.026918,"sentation in our feature space is more robust than the Parse Tree Path feature in the flat feature set since the Path feature is sensitive to small changes of the parse trees and it also does not maintain the hierarchical information of a parse tree. Sentences Tokens Propositions Arguments Train 39,832 950,028 90,750 239,858 Devel 1,346 32,853 3,248 8,346 tWSJ 2,416 56,684 5,267 14,077 tBrown 426 7,159 804 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and Fβ=1 of the predicted arguments. P recision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Fβ=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: Fβ=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. It is also worth c"
P06-2010,J05-1004,0,0.316121,"d a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in"
P06-2010,P05-1072,0,0.24175,"tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each of them individually."
P06-2010,P04-1054,0,0.121072,"hod. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features"
P06-2010,C04-1197,0,0.0455194,"ing was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a highdimension space by directly calculating the simi"
P06-2010,W05-0639,0,0.113832,"Missing"
P06-2010,J02-3001,0,0.676474,"al body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each"
P06-2010,P02-1031,0,0.0644078,"cate-Constituent related features parse tree path from the predicate to the constituent the relative position of the constituent and the predicate, before or after the nodes number on the parse tree path some part on the parse tree path the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Firstly, a parse tree T can be represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): Φ(T ) = (# of sub-trees of type 1, . . . , # of sub-trees of type i, . . . , # of sub-trees of type n) This results in a very high dimension since the number of different subtrees is exponential to the tree’s size. Thus it is computationally infeasible to use the feature vector Φ(T ) directly. To solve this problem, we introduce the tree kernel function which is able to calculate the dot product between the above hi"
P06-2010,W04-3212,0,0.270004,"ent Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorit"
P06-2010,W04-3211,0,\N,Missing
P06-2010,C98-1013,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P08-1064,2007.mtsummit-papers.8,0,0.196123,"rase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed mod"
P08-1064,P05-1067,0,0.429194,"od statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine th"
P08-1064,P03-2041,0,0.821253,"that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is"
P08-1064,N04-1035,0,0.617925,"Missing"
P08-1064,P06-1121,0,0.542205,"ive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be comb"
P08-1064,N04-1014,0,0.102142,"Missing"
P08-1064,2003.mtsummit-papers.22,0,0.0978139,"Missing"
P08-1064,N03-1017,0,0.108044,"igned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al."
P08-1064,koen-2004-pharaoh,0,0.260163,"he proposed model works. First, the source sentence is parsed into a source parse tree. Next, the source parse tree is detached into two source tree sequences (the left hand side of rules in Fig. 3). Then the two rules in Fig. 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree. Finally, a target translation is yielded from the target tree. Our model is implemented under log-linear framework (Och and Ney, 2002). We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words. In addition, we define two new features: 1) the number of lexical words in a rule to control the model’s preference for lexicalized rules over un-lexicalized 562 rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules. Note that we do not distinguish between larger (taller) and shorter source side tree sequences, i.e. we let these rules compete directly"
P08-1064,P06-1077,0,0.798378,"al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic tran"
P08-1064,P07-1089,0,0.865288,"d Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1 as the basic translation unit and u"
P08-1064,W06-1606,0,0.778456,"hods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language mod560 el under the phrase-based translation framework, resulting in good performance improvement. However, neither source side syntactic knowledge nor reordering model is further explored. 2) Galley et al. (2006) handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached. This solution requires larger applicability contexts (Marcu et al., 2006). However, phrases are utilized independently in the phrase-based method without depending on any contexts. 3) Addressing the issues in Galley et al. (2006), Marcu et al. (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multiheaded syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees. The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule. The problem i"
P08-1064,P04-1083,0,0.0266683,"Missing"
P08-1064,P02-1038,0,0.183089,"Missing"
P08-1064,P03-1021,0,0.0106146,"s (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1 and h = 2 for the SCFG; d = 1 and h = 6 for the STSG; d = 4 and h = 6 for our model). We optimized these parameters on the training and develo"
P08-1064,J04-4002,0,0.538035,"pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al.,"
P08-1064,P02-1040,0,0.104502,"trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing. We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set. We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets. The evaluation metric is case-sensitive BLEU-4 (Papineni et al., 2002). We used GIZA++ (Och and Ney, 2004) and the heuristics “grow-diag-final” to generate m-to-n word alignments. For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system. For significance test, we used Zhang et al’s implementation (Zhang et al, 2004). We set three baseline systems: Moses (Koehn et al., 2007), and SCFG-based and STSG-based treeto-tree translation models (Zhang et al., 2007). For Moses, we used its default settings. For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h ( d = 1"
P08-1064,P05-1034,0,0.739531,"icantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phra"
P08-1064,P07-1090,1,0.843636,"07b) present a STSG-based tree-to-tree translation model. Bod (2007) reports that the unsupervised STSG-based translation model performs much better than the supervised one. The motivation behind all these work is to exploit linguistically syntactic structure features to model the translation process. However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al., 2003). The formally syntax-based model for SMT was first advocated by Wu (1997). Xiong et al. (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997) while Setiawan et al. (2007) propose a function word-based reordering model for BTG. Chiang (2005)’s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). In the last two years, many research efforts were devoted to integrating the strengths of phrasebased and syntax-based methods. In the following, we review four representatives of them. 1) Hassan et al. (2007) integrate supertags (a kind of lexicalized syntactic description) into the targe"
P08-1064,J97-3002,0,0.763933,"sh translation task show that our method statistically significantly outperforms the baseline systems. 1 Introduction Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tr"
P08-1064,P01-1067,0,0.813115,"modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well. However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006). Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Cowan et al., 2006; Zhang et al., 2007; Bod, 2007; Yamada and Knight, 2001; Liu et al., 2006; Liu et al., 2007; Gildea, 2003; Poutsma, 2000; Hearne and Way, 3 National University of Singapore tancl@comp.nus.edu.sg 2003). Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying. In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment. It is designed to combine the strengths of phrase-based and syntax-based methods. The proposed model adopts tree sequence 1"
P08-1064,zhang-etal-2004-interpreting,0,0.20435,"Missing"
P08-1064,2006.amta-papers.8,0,\N,Missing
P08-1064,C00-2092,0,\N,Missing
P08-1064,P03-1054,0,\N,Missing
P08-1064,P06-1123,0,\N,Missing
P08-1064,P06-1066,0,\N,Missing
P08-1064,P03-1011,0,\N,Missing
P08-1064,P07-2045,0,\N,Missing
P08-1064,N06-1032,0,\N,Missing
P08-1064,W06-1628,0,\N,Missing
P08-1064,J08-3004,0,\N,Missing
P08-1089,P05-1074,0,0.836647,"el, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involves three steps: (1) corpus preprocessing, including English monolingual dependency 780 Proceedings of ACL-08: HLT, pages 780–788, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics parsing and English-foreign language word alignment, (2) aligned patterns induction, which produces English patterns along with the aligned pivot patterns in the foreign language, (3) paraphrase patterns extraction, in which paraphrase patterns are extracted based on a log-linear model. Our contri"
P08-1089,N06-1003,0,0.288784,"Missing"
P08-1089,W03-1608,0,0.229068,"or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora."
P08-1089,N06-1058,0,0.313374,"Missing"
P08-1089,N03-1017,0,0.0127798,"hm 1 denotes the POS tag of wk . 783 weight. In this paper, 4 feature functions are used in our log-linear model, which include: h1 (e1 , e2 , c) = scoreM LE (c|e1 ) h2 (e1 , e2 , c) = scoreM LE (e2 |c) h3 (e1 , e2 , c) = scoreLW (c|e1 ) h4 (e1 , e2 , c) = scoreLW (e2 |c) Feature functions h1 (e1 , e2 , c) and h2 (e1 , e2 , c) are based on MLE. scoreM LE (c|e) is computed as: scoreM LE (c|e) = log pM LE (c|e) (4) scoreM LE (e|c) is computed in the same way. h3 (e1 , e2 , c) and h4 (e1 , e2 , c) are based on LW. LW was originally used to validate the quality of a phrase translation pair in MT (Koehn et al., 2003). It checks how well the words of the phrases translate to each other. This paper uses LW to measure the quality of aligned patterns. We define scoreLW (c|e) as the logarithm of the lexical weight3 : scoreLW (c|e) = n X 1X 1 log( w(ci |ej )) n |{j|(i, j) ∈ a}| i=1 4 (5) ∀(i,j)∈a where a denotes the word alignment between c and e. n is the number of words in c. ci and ej are words of c and e. w(ci |ej ) is computed as follows: count(ci , ej ) w(ci |ej ) = P 0 c0 count(ci , ej ) (6) i where count(ci , ej ) is the frequency count of the aligned word pair (ci , ej ) in the corpus. scoreLW (e|c) is"
P08-1089,W06-2931,1,0.814779,"Missing"
P08-1089,P00-1056,0,0.00494939,"p(e2 |e1 ) = X pM LE (c|e1 )pM LE (e2 |c) (1) c In Equation (1), pM LE (c|e1 ) and pM LE (e2 |c) are the probabilities of translating e1 to c and c to e2 , which are computed based on MLE: count(c, e1 ) pM LE (c|e1 ) = P 0 c0 count(c , e1 ) Figure 2: Examples of a subtree and a partial subtree. 3 Proposed Method 3.1 In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE , e a word of SE , we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE (e) is a particular connected subgraph of the tree TE , which is rooted at e and includes all the descendants of e. A partial subtree P STE (e) is a conn"
P08-1089,N03-1024,0,0.274941,"g the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involve"
P08-1089,W04-3219,0,0.6299,"Missing"
P08-1089,P02-1006,0,0.183123,". Hence the precision can be enhanced. In this experiment, we also estimated a threshold T 0 for MLE-Model using the development set (T 0 = −5.1). The pattern pairs whose score based on Equation (1) exceed T 0 were extracted as paraphrase patterns. 6 785 It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patterns appearing more than 10 times in the corpus for extracting paraphrase pat"
P08-1089,N03-1003,0,\N,Missing
P08-1089,2005.iwslt-1.8,0,\N,Missing
P08-1089,W04-3206,0,\N,Missing
P08-1096,P95-1017,0,0.282338,"plicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et"
P08-1096,N07-1011,0,0.682128,"n (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large. Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms. A solution seen in previous work (Luo et al., 2004; Culotta et al., 2007) is to design a set of first-order features summarizing the information of the mentions in an entity, for example, “whether the entity has any mention that is a name alias of the active mention?” or “whether most of the mentions in the entity have the same head word as the active mention?” These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity. In this paper, we present a more expressive entity843 Proceedings of ACL-08: HLT, pages 843–851, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Ling"
P08-1096,N07-1030,0,0.213821,"Missing"
P08-1096,P04-1018,0,0.879059,". Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all"
P08-1096,P02-1014,0,0.959589,"nd automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, th"
P08-1096,P05-1020,0,0.108939,"Missing"
P08-1096,P07-1068,0,0.0587945,"Missing"
P08-1096,J01-4004,0,0.901524,"ontained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional menti"
P08-1096,P07-1006,0,0.0425872,"ch other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n}, in which mj is the jth mention occurring in the document. Let ei be the ith entity in the document. We define P (L|ei , mj ), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of ei . By assuming that mentions occurring after mj have no influence on the decision of linking mj to an en"
P08-1096,M95-1005,0,0.818946,"Missing"
P08-1096,P07-1067,1,0.315086,"Missing"
P08-1096,P04-1017,1,0.929248,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,C04-1033,1,0.933005,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,W00-1309,1,0.821131,"Missing"
P08-1096,P02-1060,1,0.450274,"Missing"
P08-1116,P05-1074,0,0.349233,"paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (20"
P08-1116,N03-1003,0,0.929129,"es that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using biling"
P08-1116,P01-1008,0,0.869425,"e more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphras"
P08-1116,J93-2003,0,0.00852245,"raphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λT M i and λLM are the weights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T ."
P08-1116,N06-1003,0,0.198695,"f the method varies greatly on different test sets and it performs best on the test set of news sentences, which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (19"
P08-1116,I05-5003,0,0.0339384,", 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a pars"
P08-1116,N06-1058,0,0.367526,", which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resour"
P08-1116,koen-2004-pharaoh,0,0.038836,"ights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T . 4 Exploiting Multiple Resources This section describes the extraction of phrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be eas"
P08-1116,P98-2127,0,0.511715,"ts before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especial"
P08-1116,W07-0716,0,0.260551,"rns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λ"
P08-1116,P00-1056,0,0.124901,"m the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2 , if their similarity based on word overlapping rate is above a threshold T h3 , s1 and s2 are identified as parallel sentences. In this way, 872,330 parallel sentence pairs are extracted. 5 http://people.csail.mit.edu/mcollins/code.html The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7 The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. 6 1024 We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2 , we compute Score3 (p1 , p2 ) by relative frequency (Koehn et al., 2003): count(p2 , p1 ) Score3 (p1 , p2 ) = p(p2 |p1 ) = P 0 p0 count(p , p1 ) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot"
P08-1116,P02-1038,0,0.0245662,"hrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be easily combined in the loglinear model. Specifically, feature functions are derived from each paraphrase resource and then combined with the language model feature1 : ∗ T = arg max { T N X λT M i hT M i (T, S)+ i=1 λLM hLM (T, S)} (2) where N is the number of paraphrase tables. hT M i (T, S) is the feature function based on the ith paraphrase table P Ti . hLM (T, S) is the language 1 The reordering model is not considered in our model. 1023 Sim(e1 , e2 ) P = P (r,e)∈Tr (e1 )∩Tr (e2 ) (I(e1 , r, e) (r,e)∈Tr (e1 ) I(e1 , r, e) + P + I(e2 , r"
P08-1116,N03-1024,0,0.164374,"2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level"
P08-1116,I05-1011,0,0.0283694,"allison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang"
P08-1116,W04-3219,0,0.790425,"r contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora."
P08-1116,P02-1006,0,0.0452147,"ction 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been wide"
P08-1116,P03-1016,1,0.890346,"eanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation"
P08-1116,W97-0703,0,\N,Missing
P08-1116,N03-1017,0,\N,Missing
P08-1116,C98-2122,0,\N,Missing
P09-1094,N03-1003,0,0.101039,"el specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak"
P09-1094,W06-2932,0,0.0139591,"araphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other applications, only some very short sentences cannot be paraphrased. Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation. It also indicates that sentence similarity computation is more difficul"
P09-1094,W07-0718,0,0.0152141,". We compute the kappa statistic between the (E) raters. Kappa is defined as K = P (A)−P 1−P (E) (Carletta, 1996), where P (A) is the proportion of times that the labels agree, and P (E) is the proportion of times that they may agree by chance. We define P (E) = 31 , as the labeling is based on three point scales. The results show that the kappa statistics for adequacy and fluency are 0.6560 and 0.6500, which indicates a substantial agreement (K: 0.610.8) according to (Landis and Koch, 1977). The 4.2 Evaluation Metrics The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007). The generated paraphrases are manually evaluated based on three criteria, i.e., adequacy, fluency, and usability, each of which has three scales from 1 to 3. Here is a brief description of the different scales for the criteria: Adequacy 1: The meaning is evidently changed. 2: The meaning is generally preserved. 3: The meaning is completely preserved. Fluency 1: The paraphrase t is incomprehensible. 2: t is comprehensible. 3: t is a flawless sentence. Usability 1: t is opposite to the application purpose. 2: t does not achieve the application. 3: t achieves the application. 5 Results and Anal"
P09-1094,P79-1016,0,0.783926,"ummarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 200"
P09-1094,J96-2004,0,0.0854548,"Missing"
P09-1094,E99-1042,0,0.684738,"a conventional SMT-based PG method. Introduction 2 Related Work Paraphrases are alternative ways that convey the same meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either han"
P09-1094,P03-1021,0,0.00751664,"ed as the count of overlapping words. corpus and use a binary classifier to recognize if any two collocations are paraphrases. Due to the space limit, we cannot introduce the detail of the approach. We assign the score “1” for any pair of paraphrase collocations. PT-5 contains 238,882 pairs of paraphrase collocations. We combine the three sub-models based on a log-linear framework and get the SPG model: p(t|s) = K X (λk k=1 + λlm X 3.6 Parameter Estimation To estimate parameters λk (1 ≤ k ≤ K), λlm , and λum , we adopt the approach of minimum error rate training (MERT) that is popular in SMT (Och, 2003). In SMT, however, the optimization objective function in MERT is the MT evaluation criteria, such as BLEU. As we analyzed above, the BLEU-style criteria cannot be adapted in SPG. We therefore introduce a new optimization objective function in this paper. The basic assumption is that a paraphrase should contain as many correct unit replacements as possible. Accordingly, we design the following criteria: Replacement precision (rp): rp assesses the precision of the unit replacements, which is defined as rp = cdev (+r)/cdev (r), where cdev (r) is the total number of unit replacements in the gener"
P09-1094,C08-1018,0,0.0908022,"ble 2: The generated paraphrases of a source sentence for different applications. The target units after replacement are shown in blue and the pattern slot fillers are in cyan. [·]phr denotes that the unit is a phrase, while [·]pat denotes that the unit is a pattern. There is no collocation replacement in this example. reimplement the methods purposely designed for these applications. Thus here we just conduct an informal comparison with these methods. Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task. Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. Besides, they also used paraphrase patterns extracted from bilingual parallel corpora (like our PT-4) as a kind of rewriting resource. However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences. Sentence Simplification: Carroll et"
P09-1094,P02-1040,0,0.0860896,"n SMT, words of an input sentence should be totally translated, whereas in SPG, not all words of an input sentence need to be paraphrased. Therefore, a SPG model should be able to decide which part of a sentence needs to be paraphrased. 3. The bilingual parallel data for SMT are easy to collect. In contrast, the monolingual parallel data for SPG are not so common (Quirk et al., 2004). Thus the SPG model should be able to easily combine different resources and thereby solve the data shortage problem (Zhao et al., 2008a). 4. Methods have been proposed for automatic evaluation in MT (e.g., BLEU (Papineni et al., 2002)). The basic idea is that a translation should be scored based on their similarity to the human references. However, they cannot be adopted in SPG. The main reason is that it is more difficult to provide human references in SPG. Lin and Pantel (2001) have demonstrated that the overlapping between the automatically acquired paraphrases and handcrafted ones is very small. Thus the human references cannot properly assess the quality of the generated paraphrases. 3.2 Method Overview The SPG method proposed in this work contains three components, i.e., sentence preprocessing, paraphrase planning, a"
P09-1094,N06-2009,0,0.0694119,"osed method is promising, which generates useful paraphrases for the given applications. In addition, comparison experiments show that our method outperforms a conventional SMT-based PG method. Introduction 2 Related Work Paraphrases are alternative ways that convey the same meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase g"
P09-1094,I05-5010,0,0.372716,"NLP Kauchak and Barzilay, 2006). This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation. In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted. In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access. However, it cannot generate other types of paraphrases but only synonym substitution. NLG-based methods: NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005) generally involve two stages. In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc. In the second stage, a NLG system is employed to generate a sentence t from r. s and t are paraphrases as they are both derived from r. The NLG-based methods simulate human paraphrasing behavior, i.e., understanding a sentence and presenting the meaning in another way. However, deep analysis of sentences is a big challenge. Moreover, developing a NLG sys"
P09-1094,gimenez-marquez-2004-svmtool,0,0.0369816,"1,018,371 pairs of paraphrase patterns. Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other applications, only some very short sentences cannot be paraphrased. Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation. It also indicates that sentence s"
P09-1094,W04-3219,0,0.928563,"a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006). This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation. In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted. In the seco"
P09-1094,W08-0510,0,0.0131131,"pplied the approach proposed in (Zhao et al., 2008b). Its basic assumption is that if two English patterns e1 and e2 are aligned with the same foreign pattern f , then e1 and e2 are possible paraphrases. One can refer to (Zhao et al., 2008b) for the details. PT-4 contains 1,018,371 pairs of paraphrase patterns. Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other ap"
P09-1094,P08-1116,1,0.712766,"r paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006"
P09-1094,P08-1089,1,0.908289,"r paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006"
P09-1094,N06-1058,0,0.47651,"e meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually wri"
P09-1094,N06-1057,0,0.148086,"search of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001"
P09-1094,N03-1017,0,0.00997098,"s, respectively1 . Paraphrase Model: Paraphrase generation is a decoding process. The input sentence s is first segmented into a sequence of I units s¯I1 , which are then paraphrased to a sequence of units t¯I1 . Let (¯ si , t¯i ) be a pair of paraphrase units, their paraphrase likelihood is computed using a score function φpm (¯ si , t¯i ). Thus the paraphrase score I I ¯ ppm (¯ s1 , t1 ) between s and t is decomposed into: ppm (¯ sI1 , t¯I1 ) = I Y φpm (¯ si , t¯i )λpm (1) i=1 where λpm is the weight of the paraphrase model. Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003). In practice, the units of a sentence may be paraphrased using different PTs. Suppose we have K PTs, (¯ ski , t¯ki ) is a pair of paraphrase units from the k-th PT with the score function φk (¯ ski , t¯ki ), then Equation (1) can be rewritten as: ppm (¯ sI1 , t¯I1 ) = K Y Y ( φk (¯ ski , t¯ki )λk ) (2) k=1 ki 3.4 Paraphrase Generation where λk is the weight for φk (¯ ski , t¯ki ). Equation (2) assumes that a pair of paraphrase units is from only one paraphrase table. However, Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control th"
P09-1094,W03-1601,0,0.52605,"Missing"
P09-2032,J07-2003,0,0.332352,"ferent grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of"
P09-2032,P06-1121,0,0.0292268,"the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the"
P09-2032,N04-1022,0,0.0960631,"Missing"
P09-2032,P06-1077,0,0.0708956,"Missing"
P09-2032,J97-3002,0,0.0230068,"ted. Aiming at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalizatio"
P09-2032,zhang-etal-2004-interpreting,0,0.079825,"Missing"
P09-2032,P08-1064,1,0.919665,"s, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a synchronous context-free grammar (SCFG) and a synchronous tree sequence substitution grammar (STSSG) for statistical machine translation. The experimental results on NIST MT05 Chinese-to-English test set show that the SSG based translation system achieves significant improvement over three baseline systems. 1 Introduction The use of various synchronous grammar based formalisms has been a trend for statistical machine translation (SMT) (Wu, 1997; Eisner, 2003; Galley et al., 2006; Chiang, 2007; Zhang et al., 2008). The grammar formalism determines the intrinsic capacities and computational efficiency of the SMT systems. To evaluate the capacity of a grammar formalism, two factors, i.e. generative power and expressive power are usually considered (Su and Chang, 1990). The generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities. For the current synchronous grammars based SMT, to some extent, the generalization ability of the grammar rules (the usability of the rules for the new"
P09-2032,P03-2041,0,\N,Missing
P10-1085,ahrenberg-etal-2000-evaluation,0,0.0345317,"ng decision rule. Chinese words 6.3M To investigate the quality of the generated word alignments, we randomly selected a subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of t"
P10-1085,J93-2003,0,0.0187108,"Missing"
P10-1085,P03-1012,0,0.0613872,"Missing"
P10-1085,J07-2003,0,0.12447,"Missing"
P10-1085,P09-1105,0,0.0386071,"Missing"
P10-1085,W04-3250,0,0.0422656,"the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . problem . We must adopt effective measures can we avoid out of the question . Figure 3. Example of the translations generated by the baseline system and the system where the phrase colloc"
P10-1085,2005.iwslt-1.8,0,0.0194514,"we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given alignment sets As 2t and At 2 s . We can obtain the union As t  As 2t  At 2s . The source sentence f f m 1 Corpora Bilingual corpus Additional monolingual corpora can be se"
P10-1085,N03-1017,0,0.122261,"Missing"
P10-1085,P09-4007,0,0.0287087,"Missing"
P10-1085,P05-1057,0,0.0448216,"Missing"
P10-1085,D09-1051,1,0.637339,"f the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for various kinds of SMT systems and to improve phrase table for phrase-based SMT. To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. A cept is the set of source words that are connected to the same target word (Brown et al., 1993). An alignment between a source multi-word cept and a target wo"
P10-1085,W02-1018,0,0.0462638,"Missing"
P10-1085,P08-1114,0,0.0333448,"Missing"
P10-1085,P00-1056,0,0.0899882,"subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of the people of the world . 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 zhong-guo de china DE ke-xue-ji-shu yan-jiu science and"
P10-1085,P03-1021,0,0.0152963,"nces of Moses using the different bi-directional word alignments (Significantly better than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题"
P10-1085,J03-1002,0,0.00555258,"To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given"
P10-1085,P02-1040,0,0.0794477,"er than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . prob"
P10-1085,J97-3002,0,0.38215,"Missing"
P10-1085,P09-1036,0,0.0240816,"Missing"
P10-1085,P07-2045,0,\N,Missing
P11-1104,P06-1067,0,0.0460858,"Missing"
P11-1104,J93-2003,0,0.0367397,"Missing"
P11-1104,P05-1033,0,0.640525,"EU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source"
P11-1104,N10-1127,0,0.0615614,"n IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA)"
P11-1104,W04-3250,0,0.0191744,"toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that Reorder models Monotone model DBR model MSDR model (Baseline) DBR model SCBR Model 1 SCBR Model 2 MSDR+ SCBR Model 3 SCBR models (1+2) SCBR models (1+2+3) MT04 MT08 26.99 18.30 26.64"
P11-1104,N03-1017,0,0.0420923,"rce token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our metho"
P11-1104,2005.iwslt-1.8,0,0.0286277,"or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straig"
P11-1104,D09-1051,1,0.918596,"ence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases containing these collocated words. This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering mo"
P11-1104,P08-1114,0,0.0864168,"r SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignm"
P11-1104,P03-1021,0,0.00926147,"ic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost i"
P11-1104,J03-1002,0,0.00360503,"in (10). L Input: Input sentence F  f 1 Initialization: Score = 0 for each uncovered word f i do e4 e3 for each word f j ( j  ci or r ( f i , f j )   ) do e2 if f j is covered then e1 f1 f2 f3 if i > j then Score+= r ( f i , f j ) log p (o  straight |f i , f j ) f4 f5 else Score+= r ( f i , f j ) log p (o  inverted |f i , f j ) Figure 2. An example for reordering 4 4.1 else Score += arg max o r ( f i , f j ) log p (o |f i , f j ) Evaluation of Our Method Output: Score Implementation We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source p"
P11-1104,P02-1040,0,0.0825739,"d the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions t"
P11-1104,N04-4026,0,0.0295724,"led as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimat"
P11-1104,P05-1069,0,0.0191896,"though the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by con"
P11-1104,C10-1126,0,0.0841098,"ls (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing add"
P11-1104,J97-3002,0,0.675665,"ng decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the"
P11-1104,P06-1066,0,0.120655,"ted in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “金融 jin-rong „financial‟ 危 机 wei-ji „crisis‟” (financial crisis) whose English translation order is usually straight, and “ 法 律 fa-lv „law‟ 范 围 fan-wei „scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “潮流 chao-liu „trend‟” is collocated with “时代 shi-dai „times‟”, they are often translated into the “trend of times”; when collocated with “历史 li-shi „history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Th"
P11-1104,P03-1019,0,0.0237065,"n quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target"
P11-1104,W06-3108,0,0.0263897,"allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecu"
P11-1104,D07-1056,0,0.0844338,"uction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using"
P11-1104,P07-2045,0,\N,Missing
P18-1252,P16-1231,0,0.0506344,"Missing"
P18-1252,D12-1133,0,0.0569982,"Missing"
P18-1252,P12-1071,1,0.896274,"or boosting parsing performance. Though under different linguistic theories or annotation guidelines, the treebanks are painstakingly developed to capture the syntactic structures of the same language, thereby having a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treeb"
P18-1252,D14-1082,0,0.0998588,"we propose two simple yet effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre"
P18-1252,P16-1033,1,0.654377,"iSeqLSTM at wk , denoted as hseq k , is fed into two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task"
P18-1252,P07-1033,0,0.462705,"Missing"
P18-1252,P15-1033,0,0.104829,"t effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre (2012) by 0.97 (93."
P18-1252,C16-1002,0,0.486022,"ing a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1"
P18-1252,N13-1013,0,0.0187158,"of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1, the goal of this work is to convert the under tree that follows the HIT-CDT guideline (Che et al., 2012) into the upper one that follows our new guideline. However, due to the lack 2 Johansson (2013) applies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion"
P18-1252,P13-2105,0,0.0200075,"treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li et al. (2012) to th"
P18-1252,I17-1007,0,0.0141555,"two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task learning. We treat the source-side and target-side"
P18-1252,P16-1105,0,0.141086,"is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, which is consistent with Miwa and Bansal. We did not implement the in-between subtree treeLSTM. 2710 score(i ← j) consistent: i ← j h↑a wa grand: i ← k ← j sibling: i ← k → j reverse: i → j Biaffine wi h↓i reverse grand: i → k → j e"
P18-1252,P09-1006,0,0.036991,"pplies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofi"
P18-1252,D14-1162,0,0.079859,"Missing"
P18-1252,C14-1026,0,0.0706607,"Missing"
P18-1252,P15-1150,0,0.0608474,"←j rpat ⊕ eli ⊕ elj ⊕ ela i←j = e (3) Through rpat i←j , the extended word representaH tions, i.e., rD i,i←j and rj,i←j , now contain the structural information of wi and wj in dsrc . The remaining parts of the biaffine parser is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, w"
P18-1252,telljohann-etal-2004-tuba,0,0.122222,"Missing"
P18-1252,L16-1034,0,0.0229828,"Missing"
P18-1252,P15-1117,0,0.081965,"Missing"
P18-1252,P11-2126,0,0.0209418,"2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li e"
S07-1036,P01-1008,0,0.120893,"Missing"
S07-1036,P97-1067,0,0.0747512,"Missing"
S07-1036,P98-1116,0,0.00851545,"the given sentence. Then we construct queries by replacing the target word in the fragments with the candidate substitute. Finally, we search Google using the constructed queries and score each candidate based on the counts of retrieved snippets. The rest of this paper is organized as follows: Section 2 reviews some related work on lexical substitution. Section 3 describes our system, especially the web-based scoring method. Section 4 presents the results and analysis. 2 Related Work Synonyms defined in WordNet have been widely used in lexical substitution and expansion (Smeaton et al., 1994; Langkilde and Knight, 1998; Bol173 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173–176, c Prague, June 2007. 2007 Association for Computational Linguistics shakov and Gelbukh, 2004). In addition, a lot of methods have been proposed to automatically construct thesauri of synonyms. For example, Lin (1998) clustered words with similar meanings by calculating the dependency similarity. Barzilay and McKeown (2001) extracted paraphrases using multiple translations of literature works. Wu and Zhou (2003) extracted synonyms with multiple resources, including a monolingual diction"
S07-1036,P98-2127,0,0.0457621,"Missing"
S07-1036,C98-1112,0,\N,Missing
S07-1036,W03-1610,0,\N,Missing
S07-1036,W06-2907,0,\N,Missing
S07-1036,N06-1058,0,\N,Missing
S07-1036,P06-1057,0,\N,Missing
S07-1036,C98-2122,0,\N,Missing
S10-1091,P05-1074,0,0.0367893,"e can be translated to columna, columna vertebral, pilar and convicciones etc. in Spanish, and these words also have other relevant translations in English, such as vertebral column, column, pillar and convictions etc., which are semantically related to the target word backbone. We use a statistical machine translation system to calculate the translation probability from English to another language (called as pivot language) as well as the translation probability from that language to English. By multiplying these two probabilities, we get a paraphrase probability. This method was defined in (Bannard and Callison-Burch, 2005). In our system, we choose the top k paraphrases rs(w, nj ) = ∑ p(f |w)p(nj |f ), (4) f where f is the pivot language word. We use the English-Spanish parallel text from Europarl (Koehn, 2005). We choose Spanish as the pivot language because in the both directions the BLEU score of the translation between English and Spanish is relatively higher than other English and other languages (Koehn, 2005). 4 Data set and System Settings The organizers of the SemEval-2 specific domain WSD task provide no training data but raw background data in the environmental domain. The English background data is o"
S10-1091,2005.mtsummit-papers.11,0,0.0436287,"Missing"
S10-1091,H05-1053,0,0.0496578,"Missing"
S10-1091,J07-4005,0,0.106253,"ems when the training data is inadequate. In past evaluations, MFS from WordNet performed even better than most of the unsupervised systems (Snyder and Palmer, 2004; Navigli et al., 2007). MFS is usually obtained from a large scale sense tagged corpus, such as SemCor (Miller et al., 1994). However, some polysemous words have different MFS in different domains. For example, in the Koeling et al. (2005) corpus, target word coach means “manager” mostly in the S PORTS domain but means “bus” mostly in the F INANCE domain. So when the MFS is applied to specific domains, it needs to be re-estimated. McCarthy et al. (2007) proposed an unsupervised predominant word sense acquisition method which obtains domain specific MFS without sense tagged corpus. In their method, a thesaurus, in which words are connected with their distributional similarity, is constructed from the domain raw text. Word senses are ranked by their prevalence score which is calculated using the thesaurus and the sense inventory. In this paper, we propose another way to construct the thesaurus. We use statistical machine Figure 1: The architecture of HIT-CIR translation (SMT) techniques to extract paraphrase pairs from bilingual parallel text."
S10-1091,S07-1006,0,0.0286945,"Missing"
S10-1091,P07-1096,0,0.0354263,"Missing"
S10-1091,W04-0811,0,0.0259013,"Missing"
S10-1091,S10-1013,0,\N,Missing
S10-1091,H93-1061,0,\N,Missing
W04-1108,J96-1002,0,0.0274857,"wsky, 1992). These methods draw the support from the high-powered computers, get the statistics of large real-world corpus, find and acquire knowledge of linguistics automatically. They deal with all change by invariability, thus it is easy to trace the evaluation and development of natural language. So the statistic methods of NLP has attracted the attention of professional researchers and become the mainstream bit by bit. Corpus-based Statistical approaches are Decision Tree (Pedersen, 2001), Decision List, Genetic Algorithm, Naive-Bayesian Classifier (Escudero, 2000)、Maximum Entropy Model (Adam, 1996; Li, 1999), and so on. Corpus-based statistical approaches can be divided into supervised and unsupervised according to whether training corpus is sense-labeled text. Supervised learning methods have the good learning ability and can get better accuracy in WSD experiments (Schütze, 1998). Obviously the data sparseness problem is a bottleneck for supervised learning algorithm. If you want to get better learning and disambiguating effect, you can enlarge the size and smooth the data of training corpus. According to practical demand, it would spend much more time and manpower to enlarge the size"
W04-1108,J98-1004,0,0.0314309,"atural language. So the statistic methods of NLP has attracted the attention of professional researchers and become the mainstream bit by bit. Corpus-based Statistical approaches are Decision Tree (Pedersen, 2001), Decision List, Genetic Algorithm, Naive-Bayesian Classifier (Escudero, 2000)、Maximum Entropy Model (Adam, 1996; Li, 1999), and so on. Corpus-based statistical approaches can be divided into supervised and unsupervised according to whether training corpus is sense-labeled text. Supervised learning methods have the good learning ability and can get better accuracy in WSD experiments (Schütze, 1998). Obviously the data sparseness problem is a bottleneck for supervised learning algorithm. If you want to get better learning and disambiguating effect, you can enlarge the size and smooth the data of training corpus. According to practical demand, it would spend much more time and manpower to enlarge the size of training corpus. Smoothing data is merely a subsidiary measure. The sufficient large size of training corpus is still the foundation to get a satisfied effect in WSD experiment. Unsupervised WSD never depend on tagged corpus and could realize the training of large real corpus coming f"
W04-1120,C94-1032,0,0.0235379,"ayer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers. In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. Based on the mec"
W04-1121,P94-1012,0,0.501998,"Missing"
W04-1121,P93-1001,0,0.0925226,"Missing"
W04-1121,J93-1004,0,\N,Missing
W04-1121,P91-1022,0,\N,Missing
W04-1121,P96-1018,0,\N,Missing
W04-1121,J93-1006,0,\N,Missing
W04-1121,P93-1002,0,\N,Missing
W05-0627,J96-1002,0,0.0317627,"ntic interpretation is needed, such as question and answering, information extraction, machine translation, paraphrasing, and so on. ∗ This research was supported by National Natural Science Foundation of China via grant 60435020 Last year, CoNLL-2004 hold a semantic role labeling shared task (Carreras and M`arquez, 2004) to test the participant systems’ performance based on shallow syntactic parser results. In 2005, SRL shared task is continued (Carreras and M`arquez, 2005), because it is a complex task and now it is far from desired performance. In our SRL system, we select maximum entropy (Berger et al., 1996) as a classifier to implement the semantic role labeling system. Different from the best classifier reported in literatures (Pradhan et al., 2005) – support vector machines (SVMs) (Vapnik, 1995), it is much easier for maximum entropy classifier to handle the multi-class classification problem without additional post-processing steps. The classifier is much faster than training SVMs classifiers. In addition, maximum entropy classifier can be tuned to minimize over-fitting by adjusting gaussian prior. Xue and Palmer (2004; 2005) and Kwon et al. (2004) have applied the maximum entropy classifier"
W05-0627,W04-2412,0,0.0947866,"Missing"
W05-0627,W05-0620,0,0.112708,"Missing"
W05-0627,C04-1179,0,0.0311086,"Missing"
W05-0627,W04-3212,0,0.147832,"Missing"
W06-2931,I05-2044,0,0.0122772,"A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) is a blend of bottom-up and top-down processing. Its classifier is trained by memory-based learning. Deterministic parsing derives an analysis without redundancy or backtracking, and linear time can be achieved. But when searching the local optimum in the order of left-to-right, some wrong reduce may prevent next analysis with more possibility. (Jin et al., 2005) used a two-phase shift-reduce to decrease such errors, and improved the accuracy of long distance dependencies. In this paper a deterministic parsing based on dynamic local optimization is proposed. According to the probabilities of dependency arcs, the algorithm dynamically finds the one with the highest probabilities instead of dealing with the sentence in order. A procedure of constraint which can integrate more structure information is made to check the rationality of the reduce. Finally our results and error analysis are presented. 2 Dependency Probabilities An example of Chinese depende"
W06-2931,W04-2407,0,0.0657967,"lity because their GDs are stable and helpful to syntactic analysis. Other FTags with small probabilities are unstable in GDs and cannot provide efficient information for syntactic analysis. If their probabilities are less than 0.65 they will be ignored in our dependency parsing. 3 Dynamic local optimization Many previous methods are based on history-based models. Despite many obvious advantages, these methods can be awkward to encode some constrains within their framework (Collins, 2000). Classifiers are good at encoding more features in the deterministic parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004). However, such algorithm often make more probable dependencies be prevented by preceding errors. An example is showed in Figure 2. Arc a is a frequent dependency and b is an arc with more probability. Arc b will be prevented by a if the reduce is carried out in order. Figure 2: A common error in deterministic parsing 3.1 Our algorithm Our deterministic parsing is based on dynamic local optimization. The algorithm calculates the arc probabilities of two continuous nodes, and then reduces the most probable arc. The construction of dependency tree includes four actions: Check, Reduce, Delete, an"
W06-2931,C96-1058,0,0.0935554,"been achieved in some dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). With the availability of many dependency treebanks (van der Beek et al., 2002; Hajiˇc et al., 2004; B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (Brants et al., 2002; Nilsson et al., 2005; Chen et al., 2003; Kawata and Bartels, 2000), multi-lingual dependency parsing is proposed in CoNLL shared task (Buchholz et al., 2006). Many previous works focus on unlabeled parsing, in which exhaustive methods are often used (Eisner, 1996). Their global searching performs well in the unlabeled dependency parsing. But with the increase of parameters, efficiency has to be considered in labeled dependency parsing. Thus deterministic parsing was proposed as a robust and efficient method in recent years. Such method breaks the construction of dependency tree into a series of actions. A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) i"
W06-2931,W03-3023,0,\N,Missing
W06-2931,W06-2920,0,\N,Missing
W06-2931,dzeroski-etal-2006-towards,0,\N,Missing
W06-2931,W03-2405,0,\N,Missing
W06-2931,afonso-etal-2002-floresta,0,\N,Missing
W07-2418,W90-0110,0,0.0231843,"Missing"
W08-2134,W05-0627,1,0.833091,"o the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation"
W08-2134,C04-1197,0,0.0272563,"ost Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51)"
W08-2134,W08-2121,0,0.0848569,"Missing"
W09-2305,W05-0909,0,0.388154,"Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics fication (Lepage and Denoual, 2005; Lassner et al. 2005; Zhou et al. 2006; Kauchak and Barzilay, 2006; Owczarzak et al. 2006; Owczarzak et al. 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share. In short, all paraphrases of human-generated references should be considered “good” translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al. 2005). This is an alternation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact matching. Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references. But they still have two common deficiencies. The first is the demand of the external knowledge. Paraphrase based method need a mass of external corpus to extract paraphrases and syno"
W09-2305,N03-2021,0,0.0283808,"evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of"
W09-2305,N06-1058,0,0.0202515,"ighly desirable to extend the coverage of the references for the similarity based evaluation methods. To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations. The first strategy focuses on the extension based on paraphrase identi37 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37–44, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics fication (Lepage and Denoual, 2005; Lassner et al. 2005; Zhou et al. 2006; Kauchak and Barzilay, 2006; Owczarzak et al. 2006; Owczarzak et al. 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share. In short, all paraphrases of human-generated references should be considered “good” translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al. 2005). This is an alternation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, t"
W09-2305,P03-1054,0,0.00784953,"Missing"
W09-2305,I05-5008,0,0.0165819,"for any given foreign language sentence. Therefore, it would be highly desirable to extend the coverage of the references for the similarity based evaluation methods. To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations. The first strategy focuses on the extension based on paraphrase identi37 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37–44, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics fication (Lepage and Denoual, 2005; Lassner et al. 2005; Zhou et al. 2006; Kauchak and Barzilay, 2006; Owczarzak et al. 2006; Owczarzak et al. 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share. In short, all paraphrases of human-generated references should be considered “good” translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al. 2005). This is an alternation to obtain lexical variations with synonym"
W09-2305,W06-3112,0,0.0470017,"Missing"
W09-2305,W07-0411,0,0.03494,"Missing"
W09-2305,P02-1040,0,0.0829579,"h several popular automatic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in"
W09-2305,W06-1610,0,0.0291947,"Missing"
W09-2305,P04-1077,0,0.030851,"tion is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-b"
W09-2305,J03-4003,0,\N,Missing
W09-2306,2007.mtsummit-papers.8,0,0.09837,"2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based model"
W09-2306,J07-2003,0,0.153302,"research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinde"
W09-2306,P05-1067,0,0.0230261,"Missing"
W09-2306,D07-1079,0,0.0591386,"rred and which kinds of rules could be discard without too much decline in translation quality. However, one of the precondition for the investigations of these issues is what are the “rule categories”? In other words, some comprehensive rule classifications are necessary to make the rule analyses feasible. The motivation of this paper is to present such a rule classification. 2 Related Works A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories (Liu et al., 2007; Zhang et al., 2008a; DeNeefe et al., 2007). Liu et al. (2007) differentiated the rules in their tree-to-string model which integrated with forest1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR). The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And DPR refers to the rules having at least one non-terminal leaf node between two terminal lea"
W09-2306,N04-1035,0,0.195048,"nd Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by ex"
W09-2306,P06-1121,0,0.01405,"rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR). The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes. (DeNeefe et al., 2007) made an illuminating breakdown of the different kinds of rules. Firstly, they classify all the GHKM2 rules (Galley et al., 2004; Galley et al., 2006) into two categories: lexical rules and non-lexical rules. The former are the rules whose source side has no source words. In other words, a non-lexical rule is a purely ab1 A “forest” means a sub-tree sequence derived from a given parse tree 2 One reviewer asked about the acronym GHKM. We guess it is an acronym for the authors of (Galley et al., 2004): Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 46 把 钢笔 给 我 Figure 1: A syntax tree pair example. Dotted lines stands for the word alignments. stract rule. The latter is the complementary set of the former. And then lexical rules ar"
W09-2306,P03-2041,0,0.0376687,"ful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syn"
W09-2306,N03-1017,0,0.00830537,"model and syntax model, the mixture of diversified rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea,"
W09-2306,koen-2004-pharaoh,0,0.0220538,"diversified rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marc"
W09-2306,P07-2045,0,0.00446763,"rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod,"
W09-2306,P06-1077,0,0.0235576,"rresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax-based models to more general frameworks. For example, forest-to-string transformation rules have been integrated into the tree-to-string translation framework by (Liu et al., 2006; Liu et al., 2007). Zhang et al. (2008a) made it possible to utilize the non-syntactic rules and even the phrases which are used in phrase based model by advancing a general tree sequence to tree sequence framework based on the tree-to-tree model presented in (Zhang et al., 2007). In these models, various kinds of rules can be employed. For example, as shown in Figure 1 and Figure 2, Figure 1 shows a Chinese-to-English sentence pair with syntax parses on both sides and the word alignments (dotted lines). Figure 2 lists some of the rules which can be extracted from the sentence pair in Figure"
W09-2306,P07-1089,0,0.0180885,"d be discard without too much decline in translation quality. However, one of the precondition for the investigations of these issues is what are the “rule categories”? In other words, some comprehensive rule classifications are necessary to make the rule analyses feasible. The motivation of this paper is to present such a rule classification. 2 Related Works A few researches have made some exploratory investigations towards the effects of different rules by classifying the translation rules into different subcategories (Liu et al., 2007; Zhang et al., 2008a; DeNeefe et al., 2007). Liu et al. (2007) differentiated the rules in their tree-to-string model which integrated with forest1 -to-string into fully lexicalized rules, non-lexicalized rules and partial lexicalized rules according to the lexicalization levels. As an extension, Zhang et al. (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR). The SRR stands for the rules which have at least two non-terminal leaf nodes with inverted order in the source and target side. And DPR refers to the rules having at least one non-terminal leaf node between two terminal leaf nodes. (DeNeefe e"
W09-2306,W02-1018,0,0.0368681,"he conventional phrase model and syntax model, the mixture of diversified rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et"
W09-2306,W06-1606,0,0.0153397,"2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have attempted to make full use of the non-syntactic rules by extending their syntax"
W09-2306,P00-1056,0,0.0328612,"Missing"
W09-2306,J04-4002,0,0.0363686,"del, the mixture of diversified rules still leaves much room for study. In this paper, we present a refined rule classification system. Based on this classification system, the rules are classified according to different standards, such as lexicalization level and generalization. Especially, we refresh the concepts of the structure reordering rules and the discontiguous phrase rules. This novel classification system may supports the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al"
W09-2306,P05-1034,0,0.0300514,"Missing"
W09-2306,N06-1002,0,0.0178123,"(such as R5 in Figure 2) also can model the useful structure reorderings. Moreover, it is not uncommon that a rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal. The reason for our emphasis of content word terminal is that the reorderings between the non-terminals and function word are less meaningful. One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue (Simard et al., 2005; Quirk and Menezes, 2006; Wellington et al., 2006; Bod, 2007; Zhang et al., 2007). What seems to be lacking, however, is a explicit definition to the discontiguous translation. The definition of DPR in (Zhang et al., 2008a) is explicit but somewhat rough and not very accurate. For example, in Figure 3(a), non-terminal node pair ([0,‘爱’], [0,‘love’] ) is surrounded by lexical terminals. According to Definition 2, it is a DPR. However, obviously it is not a discontiguous phrase actually. This rule can be simulated by conjunctions of three phrases (‘我’, ‘I’; ‘爱’, ‘love’; ‘你’,‘you’). In contrast, the translation rule in"
W09-2306,P06-1123,0,0.0154268,"also can model the useful structure reorderings. Moreover, it is not uncommon that a rule demonstrates the reorderings between two non-terminals as well as the reorderings between one non-terminal and one content word terminal. The reason for our emphasis of content word terminal is that the reorderings between the non-terminals and function word are less meaningful. One of the theoretical problems with phrase based SMT models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue (Simard et al., 2005; Quirk and Menezes, 2006; Wellington et al., 2006; Bod, 2007; Zhang et al., 2007). What seems to be lacking, however, is a explicit definition to the discontiguous translation. The definition of DPR in (Zhang et al., 2008a) is explicit but somewhat rough and not very accurate. For example, in Figure 3(a), non-terminal node pair ([0,‘爱’], [0,‘love’] ) is surrounded by lexical terminals. According to Definition 2, it is a DPR. However, obviously it is not a discontiguous phrase actually. This rule can be simulated by conjunctions of three phrases (‘我’, ‘I’; ‘爱’, ‘love’; ‘你’,‘you’). In contrast, the translation rule in Figure 3(b) is an actual"
W09-2306,J97-3002,0,0.104504,"s the SMT research community with some helpful references. 1 Introduction Phrase-based statistical machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse)"
W09-2306,2007.mtsummit-papers.71,1,0.907841,"machine translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint,"
W09-2306,P08-1064,1,0.119349,"models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004; Koehn, 2004; Koehn et al., 2007) have achieved significant improvements in translation accuracy over the original IBM word-based model. However, there are still many limitations in phrase based models. The most frequently pointed limitation is its inefficacy to modeling the structure reordering and the discontiguous corresponding. To overcome these limitations, many syntaxbased SMT models have been proposed (Wu, 1997; Chiang, 2007; Ding et al., 2005; Eisner, 2003; Quirk 45 et al., 2005; Liu et al., 2007; Zhang et al., 2007; Zhang et al., 2008a; Zhang et al., 2008b; Gildea, 2003; Galley et al., 2004; Marcu et al., 2006; Bod, 2007). The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability. Although remarkable progresses have been reported, the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents. To alleviate this constraint, a few works have at"
W09-2306,C08-1138,1,\N,Missing
W09-2306,H05-1095,0,\N,Missing
W09-2306,P03-1011,0,\N,Missing
W10-4113,W03-0430,0,0.081773,"Missing"
W10-4113,W00-0731,0,0.0539404,"Missing"
W10-4113,P04-1007,0,0.0523101,"Missing"
W10-4113,W04-1221,0,0.0692151,"Missing"
W10-4113,N03-1028,0,0.103993,"Missing"
W18-3105,D17-1122,0,0.59262,"the performance of our model is shown to be comparable to a more complex state-of-the-art baseline. 1 2 2.1 Related Work Answer Selection Answer selection is an active research field and has drawn a lot of attention. Given a question and a set of candidate answers, the task is to identify which of the candidates contains the correct answer to the question. Two types of deep learning frameworks have been proposed for tackling the answer selection problem. One is the Siamese framework (Bromley et al., 1993) and the other is the Compare-Aggregate framework (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). In the Siamese framework, the same encoder (e.g., a CNN or a RNN) is used to map each input sentence to a vector representation individually. After that, the final output is determined solely based on the encoded vectors. There is no explicit interaction between the sentences during the encoding process. On the other hand, the CompareAggregate framework aims to capture more interactive features between sentences in consideration, therefore typically has better performance when evaluated on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). Introduction Custome"
W18-3105,P17-4017,0,0.295706,". We formalize the task as follows: Given a question Q about a product P and the list of specifications (s1 , s2 , ..., sM ) of P , the goal is to identify the specification that is most relevant to Q. M is the number of specifications of P , and si is the ith specification of P . In this formulation, the task is similar to the answer selection problem (Rao et al., 2016; Bian et al., 2017; Shen et al., 2017). ‘Answers’ shall be individual product specifications in this case. After identifying the most relevant specification, the final response sentence is generated using predefined templates (Cui et al., 2017). Figure 1 illus38 Proceedings of the First Workshop on Economics and Natural Language Processing, pages 38–43 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Figure 1: Answering questions regarding product facts and specifications 2.2 Customer Service Chatbot A common trait of a number of recent state-ofthe-art methods for answer selection is the use of the Compare-Aggregate architecture (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). Under this architecture, vector representations of smaller units (such as words) of the input sentences are com"
W18-3105,P15-1150,0,0.171296,"Missing"
W18-3105,D07-1003,0,0.798296,"ate framework (Wang et al., 2017; Bian et al., 2017; Shen et al., 2017). In the Siamese framework, the same encoder (e.g., a CNN or a RNN) is used to map each input sentence to a vector representation individually. After that, the final output is determined solely based on the encoded vectors. There is no explicit interaction between the sentences during the encoding process. On the other hand, the CompareAggregate framework aims to capture more interactive features between sentences in consideration, therefore typically has better performance when evaluated on public datasets such as TrecQA (Wang et al., 2007) and WikiQA (Yang et al., 2015). Introduction Customers ask many questions before buying products. Developing a general question answering system to assist customers is challenging, due to the diversity of questions. In this paper, we focus on the task of answering questions regarding product facts and specifications. We formalize the task as follows: Given a question Q about a product P and the list of specifications (s1 , s2 , ..., sM ) of P , the goal is to identify the specification that is most relevant to Q. M is the number of specifications of P , and si is the ith specification of P ."
W18-3105,D14-1162,0,0.0849795,"tecture Given a question and a set of candidate specifications, the goal is to identify the most relevant specification. We aim to train a classifier that takes a question and a specification name as input and predicts whether the specification is relevant to the question. During inference, given a question, the trained classifier is used to assign a score to every candidate specification based on how relevant the specification is. After that, the top-ranked specification is selected. 1 Word Representation Layer. Using word embeddings pre-trained with word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), we transform Q and S into two sequences Qe = Q Q S S S [eQ 1 , e2 , ..., em ] and Se = [e1 , e2 , ..., en ], where Q ei is the embedding of the ith word of the question and eSj is the embedding of the j th word of the specification name. m and n are the lengths of Q and S, respectively. BiLSTM Layer. https://shopbot.ebay.com 39 We use a bi-directional Figure 2: Architecture of our model LSTM (Hochreiter and Schmidhuber, 1997) to obtain a context-aware vector representation for each position of Q and S. We feed Qe and Se individually into a parameter shared bi-directional LSTM model. For the"
