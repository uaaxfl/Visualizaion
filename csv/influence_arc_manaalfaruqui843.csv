2020.emnlp-main.89,W13-2111,0,0.540524,") lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference"
2020.emnlp-main.89,2020.acl-main.708,0,0.215626,"nables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of annotator highlighted cells. Annotation Process There are various existing strategies to create the reference target y. One strategy employed by many datasets is to have annotators write targets from scratch given a representation of the source (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a). While this will result in a target that"
2020.emnlp-main.89,P19-1483,1,0.920838,"Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for mo"
2020.emnlp-main.89,N19-1246,0,0.0250175,"ailable at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of c"
2020.emnlp-main.89,P17-1017,0,0.0422605,"t to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). Wh"
2020.emnlp-main.89,W17-3518,0,0.484701,"t to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). Wh"
2020.emnlp-main.89,W18-6505,1,0.869327,"t h(D) be the set of 1178 header values for a given dataset D. We remove examples d from the training set where h(d) is both rare in the data as well as occurs in either the development or test sets. Specifically, Dtrain is defined as: pre-train a version of BERT on the Books corpus only, which we consider a more correct baseline. However, empirically we find that both models perform similarly in practice (Table 8). • Pointer-Generator (See et al., 2017): A Seq2Seq model with attention and copy mechanism. While originally designed for summarization it is commonly used in data-to-text as well (Gehrmann et al., 2018). • Puduppully et al. (2019): A Seq2Seq model with an explicit content selection and planning mechanism designed for data-to-text. Dtrain := {d : h(d) ∈ / (h(Ddev ) ∪ h(Dtest )) or  count h(d), Dorig-train &gt; α}. The count(h(d), Dorig-train ) function returns the number of examples in Dorig-train with header h(d). To choose the hyperparameter α we first split the test set as follows: Dtest-overlap := {d : h(d) ∈ h(Dtrain )} Dtest-nonoverlap := {d : h(d) ∈ / h(Dtrain )} The development set is analogously divided into Ddev-overlap and Ddev-nonoverlap . We then choose α = 5 so that Dtest-overlap"
2020.emnlp-main.89,N18-2017,0,0.136218,"´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1173–1186, c November 16–20, 2020. 2020 Association for Computationa"
2020.emnlp-main.89,Q18-1031,0,0.0326281,"s a summarization problem. However, summarization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantl"
2020.emnlp-main.89,W17-3204,0,0.0768419,"Missing"
2020.emnlp-main.89,D19-1051,0,0.0497763,"Missing"
2020.emnlp-main.89,P83-1022,0,0.499371,"obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al.,"
2020.emnlp-main.89,D16-1128,0,0.109807,"yses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is"
2020.emnlp-main.89,P09-1011,0,0.0375768,"hful to the source (see Table 4 and the Appendix for more complex examples). Our experiments demonstrate that state-of-the-art neural models struggle to generate faithful results, despite the high quality of the training data. These results suggest that our dataset could serve as a useful benchmark for controllable data-to-text generation. 2 Related Work T OTT O differs from existing datasets in both task design and annotation process as we describe below. A summary is given in Table 2. Task Design Most existing table-to-text datasets are restricted in topic and schema such as W EATH ER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), Rotowire (Wiseman et al., 2017, basketball), E2E (Novikova et al., 2016, 2017, restaurants), KBGen (Banik et al., 2013, biology), and Wikibio (Lebret et al., 2016, biographies). In contrast, T OTT O contains tables with various schema spanning various topical categories all over Wikipedia. Moreover, T OTT O takes a different view of content selection compared to 1174 Dataset Train Size Wikibio (Lebret et al., 2016) 583K Rotowire (Wiseman et al., 2017) 4.9K WebNLG (Gardent et al., 2017b) 25.3K E2E (Novikova et al., 2017) 50.6K LogicNLG (Chen et al., 2020) 28"
2020.emnlp-main.89,W19-5302,0,0.0175146,"odel achieves the highest BLEU. Red indicates model errors and blue denotes interesting reference language not in the model output. in the non-overlap case, where we see a moderate effect favoring the book model. model is unable to make these inferences from the simplistic source representation that we used. 9 Evaluation metrics Many of the above issues are difficult to capture with metrics like BLEU since the reference and prediction may only differ by a word but largely differ in terms of semantic meaning. This urges for better metrics possibly built on learned models (Wiseman et al., 2017; Ma et al., 2019; Sellam et al., 2020). Thus, while we have a task leaderboard, it should not be interpreted as the definitive measure of model performance. Model Errors and Challenges Table 11 shows predictions from the BERT-toBERT Books model to illustrate challenges existing models face. Hallucination The model sometimes outputs phrases such as first, winning that seem reasonable but are not faithful to the table. This hallucination phenomenon has been widely observed in other existing data-to-text datasets (Lebret et al., 2016; Wiseman et al., 2017). However, the noisy references in these datasets make it"
2020.emnlp-main.89,W17-5525,0,0.0636405,"Missing"
2020.emnlp-main.89,W16-6644,0,0.148876,"hat state-of-the-art neural models struggle to generate faithful results, despite the high quality of the training data. These results suggest that our dataset could serve as a useful benchmark for controllable data-to-text generation. 2 Related Work T OTT O differs from existing datasets in both task design and annotation process as we describe below. A summary is given in Table 2. Task Design Most existing table-to-text datasets are restricted in topic and schema such as W EATH ER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), Rotowire (Wiseman et al., 2017, basketball), E2E (Novikova et al., 2016, 2017, restaurants), KBGen (Banik et al., 2013, biology), and Wikibio (Lebret et al., 2016, biographies). In contrast, T OTT O contains tables with various schema spanning various topical categories all over Wikipedia. Moreover, T OTT O takes a different view of content selection compared to 1174 Dataset Train Size Wikibio (Lebret et al., 2016) 583K Rotowire (Wiseman et al., 2017) 4.9K WebNLG (Gardent et al., 2017b) 25.3K E2E (Novikova et al., 2017) 50.6K LogicNLG (Chen et al., 2020) 28.5K T OTT O 120K Domain Biographies Basketball 15 DBPedia categories Restaurants Wikipedia (open-domain) Wik"
2020.emnlp-main.89,P18-1123,0,0.0215634,"problem. However, summarization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled d"
2020.emnlp-main.89,P02-1040,0,0.108003,"aseline). • BERT-to-BERT (Rothe et al., 2020): A Transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both initialized with BERT (Devlin et al., 2018). The original BERT model is pre-trained with both Wikipedia and the Books corpus (Zhu et al., 2015), the former of which contains our (unrevised) test targets. Thus, we also In all cases, the cells are linearized with row and column separator tokens. We also experiment with prepending the table metadata to the source table.6 Evaluation metrics The model output is evaluated using two automatic metrics: BLEU (Papineni et al., 2002) and PARENT (Dhingra et al., 2019). PARENT is a metric recently proposed specifically for data-to-text evaluation that takes the table into account. We modify it to make it suitable for our dataset, described in the Appendix. Human evaluation is described in § 8.2. 8.1 Results Table 8 shows our results against multiple references with the subtable input format. Both the 6 The table section text is ignored, since it is usually missing or irrelevant. 1179 Overall Model Overlap Subset Nonoverlap Subset BLEU PARENT BLEU PARENT BLEU PARENT 44.0 43.9 41.6 19.2 52.6 52.6 51.6 29.2 52.7 52.7 50.6 24.5"
2020.emnlp-main.89,P15-1142,0,0.0470512,"uring an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulness of the generated text when the source content is structured (Wiseman et al., 2017; Dhingra et al., 2019). Moreover, structured data can also test a model’s ability for reasoning and numerical inference (Wiseman et al., 2017) and for building representations of structured objects (Liu et al., 2018), providing an interesting complement to tasks that test these aspects in the NLU setting (Pasupat and Liang, 2015; Chen et al., 2019; Dua et al., 2019). However, constructing a data-to-text dataset can be challenging on two axes: task design and annotation process. First, tasks with open-ended output like summarization (Mani, 1999; Lebret et al., 2016; Wiseman et al., 2017) lack explicit signals for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and"
2020.emnlp-main.89,N19-1263,1,0.844058,"marization is much more subjective, which can make the task underconstrained and difficult to evaluate (Kry´sci´nski et al., 2019). We place T OTT O as a middle-ground where the highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of an"
2020.emnlp-main.89,D15-1199,0,0.398205,"als for models on what to generate, which can lead to subjective content and evaluation challenges (Kry´sci´nski et al., 2019). On the other hand, data-to-text tasks that are limited to verbalizing a fully specified meaning representation (Gardent et al., 2017b) do not test a model’s ability to perform inference and thus remove a considerable amount of challenge from the task. Secondly, designing an annotation process to obtain natural but also clean targets is a significant challenge. One strategy employed by many datasets is to have annotators write targets from scratch (Banik et al., 2013; Wen et al., 2015; Gardent et al., 2017a) which can often lack variety in terms of structure and style (Gururangan et al., 2018; Poliak et al., 2018). An alternative is to pair naturally occurring text with tables (Lebret et al., 2016; Wiseman et al., 2017). While more diverse, naturally occurring targets are often noisy and contain information that cannot be inferred from the source. This can make it problematic to disentangle modeling weaknesses from data noise. In this work, we propose T OTT O, an opendomain table-to-text generation dataset that intro1173 Proceedings of the 2020 Conference on Empirical Meth"
2020.emnlp-main.89,D17-1239,0,0.113448,"phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.1 1 Introduction Data-to-text generation (Kukich, 1983; McKeown, 1992) is the task of generating a target textual description y conditioned on source content x in the form of structured data such as a table. Examples include generating sentences given biographical data (Lebret et al., 2016), textual descriptions of restaurants given meaning representations (Novikova et al., 2017), basketball game summaries given boxscore statistics (Wiseman et al., 2017), and generating fun facts from superlative tables in Wikipedia (Korn et al., 2019). Existing data-to-text tasks have provided an important test-bed for neural generation models (Sutskever et al., 2014; Bahdanau et al., 2014). Neural models are known to be prone to hallucination, i.e., generating text that is fluent but not faithful to the source (Vinyals and Le, 2015; Koehn ∗ Work done during an internship at Google. T OTT O is available at https://github.com/ google-research-datasets/totto. 1 and Knowles, 2017; Lee et al., 2018; Tian et al., 2019) and it is often easier to assess faithfulnes"
2020.emnlp-main.89,N18-1137,0,0.0181728,"highlighted cells provide some guidance on the topic of the target but still leave a considerable amount of content planning to be done by the model. the table. This enables T OTT O to maintain the varied language and structure found in natural sentences while producing cleaner targets. The technique of editing exemplar sentences has been used in semiparametric generation models (Guu et al., 2018; Pandey et al., 2018; Peng et al., 2019) and crowd-sourcing small, iterative changes to text has been shown to lead to higher-quality data and a more robust annotation process (Little et al., 2010). Perez-Beltrachini and Lapata (2018) also employed a revision strategy to construct a cleaner evaluation set for Wikibio (Lebret et al., 2016). Concurrent to this work, Chen et al. (2020) proposed LogicNLG which also uses Wikipedia tables, although omitting some of the more complex structured ones included in our dataset. Their target sentences are annotator-generated and their task is significantly more uncontrolled due to the lack of annotator highlighted cells. Annotation Process There are various existing strategies to create the reference target y. One strategy employed by many datasets is to have annotators write targets f"
2020.emnlp-main.89,S18-2023,0,0.0622913,"Missing"
2020.emnlp-main.89,2020.tacl-1.18,0,0.0606276,"refore, the task is more challenging, as the model must generate a new sentence instead of revising an existing one. 8 Details about hyperparameter settings are provided in the Appendix. Moreover, we explore different strategies of representing the source content that resemble standard linearization approaches in the literature (Lebret et al., 2016; Wiseman et al., 2017) Experiments We present baseline results on T OTT O by examining three existing state-of-the-art approaches (Note that since our tables do not have a fixed schema it is difficult to design a template baseline). • BERT-to-BERT (Rothe et al., 2020): A Transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both initialized with BERT (Devlin et al., 2018). The original BERT model is pre-trained with both Wikipedia and the Books corpus (Zhu et al., 2015), the former of which contains our (unrevised) test targets. Thus, we also In all cases, the cells are linearized with row and column separator tokens. We also experiment with prepending the table metadata to the source table.6 Evaluation metrics The model output is evaluated using two automatic metrics: BLEU (Papineni et al., 2002) and PARENT (Dhingra et"
2020.emnlp-main.89,2020.acl-main.704,1,0.851685,"highest BLEU. Red indicates model errors and blue denotes interesting reference language not in the model output. in the non-overlap case, where we see a moderate effect favoring the book model. model is unable to make these inferences from the simplistic source representation that we used. 9 Evaluation metrics Many of the above issues are difficult to capture with metrics like BLEU since the reference and prediction may only differ by a word but largely differ in terms of semantic meaning. This urges for better metrics possibly built on learned models (Wiseman et al., 2017; Ma et al., 2019; Sellam et al., 2020). Thus, while we have a task leaderboard, it should not be interpreted as the definitive measure of model performance. Model Errors and Challenges Table 11 shows predictions from the BERT-toBERT Books model to illustrate challenges existing models face. Hallucination The model sometimes outputs phrases such as first, winning that seem reasonable but are not faithful to the table. This hallucination phenomenon has been widely observed in other existing data-to-text datasets (Lebret et al., 2016; Wiseman et al., 2017). However, the noisy references in these datasets make it difficult to disentan"
2020.emnlp-main.89,P17-1099,0,\N,Missing
2020.emnlp-main.89,N19-1423,0,\N,Missing
2020.emnlp-main.89,D19-1609,0,\N,Missing
2021.acl-long.549,N12-1049,0,0.0804095,"Missing"
2021.acl-long.549,P07-2044,0,0.0743229,"ent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens bef"
2021.acl-long.549,chang-manning-2012-sutime,0,0.0461503,"ed to be spuriously correlated with the dialog context. For example, we include temporal spans in the dialog context as negative options, which will challenge models that rely primarily only on shallow pattern matching without correct temporal reasoning. We present more information in §3 about how the negative options were created by human annotators. 2 Temporal expression identification. Here, we select dialogs that are rich with temporal information, in order to focus on complex temporal reasoning that arises in natural dialogs. Temporal expressions are automatically identified with SUTime (Chang and Manning, 2012), an off-the-shelf Task: Temporal Reasoning in Dialog We formulate the dialog-based temporal commonsense reasoning problem as a cloze task (Taylor, 1953). Formally, given a multi-turn dialog context of n conversational turns between two speakers A 3 Dataset: T IME D IAL The T IME D IAL dataset is derived from DailyDialog data (Li et al., 2017), which is a multi-turn dialog corpus containing over 13K English dialogs. Dialogs in this dataset consist of turn-taking between two people on topics over 10 broad categories, ranging from daily lives to financial topics. 3.1 Data Collection Our data col"
2021.acl-long.549,D19-1109,0,0.02339,"soning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze question"
2021.acl-long.549,N19-1423,0,0.128778,"and outputs a score measuring how likely the candidate being a correct answer. Based on the prediction scores of all options, the model then chooses the top two positive candidates as the predicted answer for the instance. Each paradigm of models is finetuned using training data from different domains, as discussed in §4.3. 4.1.1 Binary Classification In this setting, we formulate the task as a binary classification problem, i.e., we use a classifier to measure the probability of the candidate in the (masked dialog context, candidate) pair being a correct answer. Any powerful LM — e.g., BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RO BERTA (Liu et al., 2019), etc. can be used to build the classifier. This method’s key challenge is the lack of annotated training data for direct supervision. We generate weak supervision training data as follows. In an unlabeled corpus, we use the SUTime tool 7069 Input: (2) Mask Filling (1) Classification Output: …… B: No, not all summer. Just for six weeks. Output: 1 A: I am afraid I can only rent it for two months. Classification Layer B: My holiday is only _______, but I think my brother and his family would take it for the other two weeks. BERT Options: a)"
2021.acl-long.549,D12-1062,0,0.0337886,"All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et a"
2021.acl-long.549,2020.acl-main.89,0,0.0217746,"these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog. The dataset is carefully curated to evaluate a models’ ability to do temporal commonsense/numerical reasoning over dialog context. In order to establish strong baselines and provide information on future model development, we conducted extensive experiments with state-of-the-a"
2021.acl-long.549,N18-2017,0,0.0200283,". and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options are selected to be spuriously correlated with the dialog context. For example, we include temporal spans in the dialog context as negative options, which will challenge models that rely pri"
2021.acl-long.549,2020.emnlp-main.88,0,0.0176383,"ns among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of larg"
2021.acl-long.549,N18-1077,0,0.0871066,"r machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous"
2021.acl-long.549,D16-1241,0,0.0207165,"e sufficient for robust temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them. and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options ar"
2021.acl-long.549,P06-1095,0,0.133773,"ypes. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions su"
2021.acl-long.549,P14-1135,0,0.0191428,"es. The performance CLS-IN CLS-OUT MF-IN MF-OUT GEN-IN GEN-OUT Figure 3: Percentage of errors on different reasoning types. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inf"
2021.acl-long.549,D18-1155,0,0.0983798,"ation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous works have studied temporal reasoning in natural language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in Eng"
2021.acl-long.549,D19-1509,1,0.849238,"uration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical"
2021.acl-long.549,P19-1539,1,0.800855,"uration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehension dataset where the model needs to answer questions such as “what happens before/after [event]”. Most related to our work is McTaco (Zhou et al., 2019), a dataset for evaluating temporal commonsense in the form of multiple-choice reading comprehension, where the context usually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical"
2021.acl-long.549,I17-1099,0,0.0227372,", 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et al., 2017). Given a dialog with one temporal span masked out, the model is asked to find all correct answers from a list of four options to fill in the blank (Table 1). The challenge set requires the models to demonstrate understanding of the context and use temporal commonsense to make right choices. Our final challenge set consists of 1.1K carefully curated dialog instances. We then study the performance of several stateof-the-art pre-trained language models on T IME D IAL along several dimensions including modeling paradigms (classification, mask filling, and generation), the scope of dialog contexts"
2021.acl-long.549,2020.emnlp-main.58,1,0.818348,"Missing"
2021.acl-long.549,2020.emnlp-main.557,0,0.0155892,"often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal comm"
2021.acl-long.549,2021.ccl-1.108,0,0.0602365,"Missing"
2021.acl-long.549,P18-1076,0,0.0242063,"st temporal reasoning in dialogs, and motivate future research toward modeling temporal concepts over diverse everyday events, and contextual reasoning about them. and B, where a temporal words span within the context is masked out, the task is to predict the suitable temporal expression(s) for the masked-out span from a list of options. That is, we want the conversation model to select all the correct answers from the options based on the dialog context. Following similar cloze-style challenge datasets, we use accuracy as the evaluation metric (Mostafazadeh et al., 2016; Onishi et al., 2016; Mihaylov and Frank, 2018). Having a non-trivial set of options is crucial to build a challenge set and to avoid accidental spurious biases (Geirhos et al., 2020; Gururangan et al., 2018; Le Bras et al., 2020). We ensure this via the following filtering process. (1) For each masked span, there is more than one correct answer in the options. This makes the task more challenging for models since more comprehensive understanding of the context is required to recognize all the correct choices. In our dataset (§3) we guarantee two correct answers for each masked span. (2) Some incorrect options are selected to be spuriously"
2021.acl-long.549,N16-1098,0,0.444559,"ral language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced cloze task with multiple choices based on dialogs in the DailyDialog dataset (Li et a"
2021.acl-long.549,P18-1212,0,0.0762013,"r machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference. Although previous"
2021.acl-long.549,setzer-gaizauskas-2000-annotating,0,0.256766,"Figure 3: Percentage of errors on different reasoning types. CLS, MF, and GEN represent classification, mask-filling, and generation models, respectively. All models are of large size. on comparison-based instances seems similar. 6 Related Work Temporal commonsense reasoning. Early studies related to temporal analysis define time in the context of sets and relations (Bruce, 1972; Allen, 1983). More recent works often associate time with events and focus on identifying time expressions (Chang and Manning, 2012; Angeli et al., 2012; Lee et al., 2014), extracting temporal relations among events (Setzer and Gaizauskas, 2000; Pustejovsky et al., 2005; Lapata and Lascarides, 2006; Chambers et al., 2007; Ning et al., 2018b), and timeline construction (Do et al., 2012; Leeuwenberg and Moens, 2018). Some recent work has focused on building challenging benchmarks for temporal commonsense reasoning. Story Cloze Test focuses on stereotypical causal temporal and causal relations between events (Mostafazadeh et al., 2016). Vashishtha et al. 1 (2020) recast temporal reasoning datasets for event duration and event ordering into the natural language inference (NLI) format. Turque (Ning et al., 2020) is an reading comprehensi"
2021.acl-long.549,S13-2001,0,0.038889,"as been challenging for machines (Kahn and Gorry, 1977; Kozareva and Hovy, 2011) since it requires both understanding the local temporal expressions and reasoning about their global contexts such as their relative ordering and relations Work done during an internship at Google. b) 45 days 3 d) two months 7 Table 1: Examples from our T IME D IAL challenge set, demonstrating the need for commonsense knowledge and arithmetic reasoning over the context to infer the correct answers. Key contextual information for reasoning success is highlighted. Introduction ∗ b) 30 years old 7 d) 18 years old 3 (UzZaman et al., 2013; Ning et al., 2018b; Pustejovsky, 2017). The problem becomes even more challenging in dialogs, where explicit and implicit inter-dependencies among temporal concepts can appear across conversation turns. For instance, for the first dialog in Table 1, one must understand the context, i.e., selling wine, and use world knowledge of minimum legal drinking age in order to reason about correct answers to fill in the blank. Similarly, in the second conversation, commonsense about the durations summer, month, week, day and their relations, plus numerical reasoning, are necessary to make the inference"
2021.acl-long.549,2020.findings-emnlp.363,0,0.0632093,"Missing"
2021.acl-long.549,D19-1332,0,0.335637,"e inference. Although previous works have studied temporal reasoning in natural language, they have either focused on specific time-related concepts in 7066 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7066–7076 August 1–6, 2021. ©2021 Association for Computational Linguistics isolation, such as temporal ordering and relation extraction (Leeuwenberg and Moens, 2018; Ning et al., 2018a), and/or dealt with limited context, such as single-sentence-based question answering (Zhou et al., 2019) and natural language inference (Vashishtha et al., 2020; Mostafazadeh et al., 2016). In this work, we make the first systematic study of temporal commonsense reasoning in a multi-turn dialog setting. The task involves complex reasoning that requires operations like comparison and arithmetic reasoning over temporal expressions and the need for commonsense and world knowledge. We design a new task for dialog-based temporal reasoning and present a new challenge set in English, called T IME D IAL, to evaluate language understanding models on the task. We formulate the problem as a crowd-sourced c"
2021.acl-long.549,2020.findings-emnlp.439,0,0.0227692,"sually consists of a single sentence. Our work instead studies temporal commonsense reasoning in dialogs which often require significant commonsense and world knowledge to reason over rich context (Qin et al., 2019b; Dinan et al., 2018). Commonsense reasoning with LMs. With the recent success of large pre-trained language models 7073 (LMs) (Devlin et al., 2019; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We"
2021.acl-long.549,2020.acl-main.678,0,0.0161864,"9; Brown et al., 2020), it is an open question whether these models, pretrained on large amounts of data, capture commonsense knowledge. Several works have been proposed to assess the ability of LMs for commonsense or numerical reasoning (Zhang et al., 2020; Bouraoui et al., 2020), or to mine commonsense knowledge from LMs (Davison et al., 2019). Lin et al. (2020) showed that state-of-the-art LMs such as BERT and RoBERTa performs poorly on numerical reasoning tasks without any finetuning. Works have also been proposed to improve language model’s commonsense reasoning (Qin et al., 2020, 2019a; Zhou et al., 2020) and numerical reasoning abilities (Geva et al., 2020). In our work, we study several modeling approaches and finetuning settings of large LMs, and establish strong baselines for temporal commonsense reasoning in dialogs. 7 Conclusions We introduced T IME D IAL, a challenge set consistting of 1.1K multiple-choice cloze questions for temporal commonsense reasoning in dialog. The dataset is carefully curated to evaluate a models’ ability to do temporal commonsense/numerical reasoning over dialog context. In order to establish strong baselines and provide information on future model development,"
2021.findings-acl.293,N01-1016,0,0.513969,"Missing"
2021.findings-acl.293,N19-1423,0,0.0277124,"RT and T5 variants on the following two data configurations: Table 2 summarizes the key differences between D ISFL -QA and the S WITCHBOARD dataset. 3 Fluent Q Experimental Setup Models to Compare We use two different modeling approaches to answer disfluent questions in D ISFL -QA. ALL where the model is trained on all of SQ UAD-v2, including the non-answerable questions. Evaluation is done against the entire test set. ANS where the model is trained only on answerable questions from SQ UAD-v1, without the capabilities of handling non-answerable questions. 3.3 Datasets LMs for QA. We use BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) as our QA models in the standard setup which has shown to achieve state-of-the-art performance for SQ UAD. We fine-tune BERT for a span selection task, whereby predicting start and end probabilities for all the tokens in the context. T5 is finetuned under the standard text2text formulation, when given (question, passage) as input the model generates the answer as the output. For predicting <no answer>, the model was trained to generate “unknown”. Human Annotated Datasets. We use 3 datasets in our experiments: SQ UAD-v1, SQ UAD-v2, and D ISFL -QA. We split the 11,"
2021.findings-acl.293,N15-1029,0,0.0221428,"f D ISFL -QA, with a simple DQ → Q and CDQ → Q T5 task formulation. With this pipelined approach, we get further improvements with an overall F1 of 87.19 (Table 7), however, still lacking by ≈2.4 F1 points compared to the fluent dataset. This shows that such complex cases require better modeling, preferably in an end-to-end setup. 5 5.1 Related Work Disfluency Correction The most popular approach in literature poses disfluency correction as a sequence tagging task, in which the fluent version of the utterance is obtained by identifying and removing the disfluent segments (Zayats et al., 2014; Ferguson et al., 2015; Zayats et al., 2016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques ha"
2021.findings-acl.293,D18-1490,0,0.0615598,"on of the utterance is obtained by identifying and removing the disfluent segments (Zayats et al., 2014; Ferguson et al., 2015; Zayats et al., 2016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques have been proposed (Yang et al., 2020; McDougall and Duckworth, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations. 5.2 Question Answering Under Noise In the QA literature, our work is related to two threads that aim to improve robustness of QA mod"
2021.findings-acl.293,P17-2087,0,0.147877,"lexity. . . What is typically used to broadly define complexity measures? What is defined no is typically used to broadly define complexity measures? Table 1: Example passage and fluent questions from the SQ UAD dataset and their disfluent versions provided by human raters, categorized by the type of disfluency along with their estimated percentage in the D ISFL -QA dataset. writing the disfluent version of a question, we instructed raters not to include partial words or filled pauses (e.g., “um”, “uh”, “ah” etc.), as they can be detected relatively easily (Johnson and Charniak, 2004; Jamshid Lou and Johnson, 2017). Raters were shown example disfluencies from each of the categories in Table 1. On average, raters spent 2.5 minutes per question. Introduction of a disfluency increased the mean length of a question from 10.3 to 14.6 words. Human Evaluation + Re-annotation. To assess and ensure high quality of the dataset, we asked a another set of human raters the following yes/no questions: 1. Is the disfluent question consistent with respect to the fluent question? i.e., the disfluent question is semantically equivalent to the original question in that they share the same answer. 2. Is the disfluent quest"
2021.findings-acl.293,2020.acl-main.346,0,0.121747,"the Duchy of Normandy founded? When was the Duchy of Normandy offered ugh I mean founded? A DJ What is the original meaning of the word Norman? What is the English rather original meaning of the word Norman? A DV Who did Beyonc´e perform privately for in 2011? Who did Beyonc´e perform publicly oops privately for in 2011? E NT Who was a prominent Huguenot in Holland? Who was a prominent Saint Nicholas no I mean Huguenot in Holland? Table 3: Example of synthetically generated disfluent questions using the contextual heuristics. state-of-the-art BERT-based disfluency correction model by Jamshid Lou and Johnson (2020) trained on S WITCHBOARD. We also train T5 models on D ISFL -QA to prevent the distribution skew between S WITCHBOARD and D ISFL -QA, and account for new phenomena like coreferences. 3.2 Who removed [BSkyB’s] operating license no scratch that who do [they] have [their] operating license from ? Training Settings We train the BERT and T5 variants on the following two data configurations: Table 2 summarizes the key differences between D ISFL -QA and the S WITCHBOARD dataset. 3 Fluent Q Experimental Setup Models to Compare We use two different modeling approaches to answer disfluent questions in D"
2021.findings-acl.293,D17-1215,0,0.0264742,"stion Answering Under Noise In the QA literature, our work is related to two threads that aim to improve robustness of QA models: (i) QA under adversarial noise, and (ii) noise arising from speech phenomena. Prior work on adversarial QA have predominantly generated adversaries automatically (Zhao et al., 2018), which are verified by humans to ensure semantic equivalence (i.e. answer remains same after perturbation). For instance, Ribeiro et al. (2018) generated adversaries using para3316 phrasing, while Mudrakarta et al. (2018) perturbed questions based on attribution. Closest work to ours is Jia and Liang (2017), who modified SQ UAD to contain automatically generated adversarial sentence insertions. Our work is more closely related to prior work on making NLP models robust to noise arising from speech phenomena. Earlier work (Surdeanu et al., 2006; Leuski et al., 2006) have built QA models which are robust to disfluency-like phenomenon, but they were limited in the corpus complexity, domain, and scale. Recently there has been renewed interest in constructing audio enriched versions of existing NLP datasets, for example, the S POKEN -SQ UAD (Li et al., 2018) and S POKEN -C O QA (You et al., 2020) with"
2021.findings-acl.293,P04-1005,0,0.127401,"on complexity and decision tree complexity. . . What is typically used to broadly define complexity measures? What is defined no is typically used to broadly define complexity measures? Table 1: Example passage and fluent questions from the SQ UAD dataset and their disfluent versions provided by human raters, categorized by the type of disfluency along with their estimated percentage in the D ISFL -QA dataset. writing the disfluent version of a question, we instructed raters not to include partial words or filled pauses (e.g., “um”, “uh”, “ah” etc.), as they can be detected relatively easily (Johnson and Charniak, 2004; Jamshid Lou and Johnson, 2017). Raters were shown example disfluencies from each of the categories in Table 1. On average, raters spent 2.5 minutes per question. Introduction of a disfluency increased the mean length of a question from 10.3 to 14.6 words. Human Evaluation + Re-annotation. To assess and ensure high quality of the dataset, we asked a another set of human raters the following yes/no questions: 1. Is the disfluent question consistent with respect to the fluent question? i.e., the disfluent question is semantically equivalent to the original question in that they share the same a"
2021.findings-acl.293,W06-1303,0,0.0559284,"ted adversaries automatically (Zhao et al., 2018), which are verified by humans to ensure semantic equivalence (i.e. answer remains same after perturbation). For instance, Ribeiro et al. (2018) generated adversaries using para3316 phrasing, while Mudrakarta et al. (2018) perturbed questions based on attribution. Closest work to ours is Jia and Liang (2017), who modified SQ UAD to contain automatically generated adversarial sentence insertions. Our work is more closely related to prior work on making NLP models robust to noise arising from speech phenomena. Earlier work (Surdeanu et al., 2006; Leuski et al., 2006) have built QA models which are robust to disfluency-like phenomenon, but they were limited in the corpus complexity, domain, and scale. Recently there has been renewed interest in constructing audio enriched versions of existing NLP datasets, for example, the S POKEN -SQ UAD (Li et al., 2018) and S POKEN -C O QA (You et al., 2020) with the aim to show the effect of speech recognition errors on QA task. However, since collecting audio is challenging, another line of work involves testing the robustness of NLP models to ASR errors in transcribed texts containing synthetic noise using TTS → ASR"
2021.findings-acl.293,P18-2124,0,0.163885,"of using gold data for fine-tuning. We argue that we need large-scale disfluency datasets in order for NLP models to be robust to them. The dataset is publicly available at: https://github.com/ google-research-datasets/disfl-qa. 1 ♣ Shyam Upadhyay Diyi Yang ♠ Google Assistant ♢ The University of Texas at Austin ♣ Georgia Institute of Technology disfl-qa@google.com Figure 1: (a) Categories of disfluencies (Shriberg, 1994) (b) A passage and questions (qi ) from SQ UAD, along with their disfluent versions (dqi ) and predictions from a T5-QA model. question answering (QA) setting, namely SQ UAD (Rajpurkar et al., 2018), affects the prediction of a state-of-the-art T5 model (Raffel et al., 2020). For example, the original question q1 is seeking an answer about the location of Normandy. In the disfluent version dq1 (which is semantically equivalent to q1 ), the user starts asking about Norse and then corrects themselves to ask about the Normandy instead. The presence of this correctional disfluency confuses the QA model, which tend to rely on shallow textual cues from question for making predictions. 3309 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3309–3319 August 1–6, 2"
2021.findings-acl.293,D16-1264,0,0.0237349,"SFL -QA builds upon the existing SQ UAD-v2 dataset, a question answering dataset which contains curated paragraphs from Wikipedia and associated questions. Each question associated with the paragraph is sent for a human annotation task to add a contextual disfluency using the paragraph as a source of distractors. Finally, to ensure the quality of the dataset, a subsequent round of human evaluation with an option to re-annotate is conducted. 2.1 Source of Questions We sourced passages and questions from SQ UAD-v2 (Rajpurkar et al., 2018) development set. SQ UAD-v2 is an extension of SQ UAD-v1 (Rajpurkar et al., 2016) that contains unanswerable questions written adversarially by crowd workers to look similar to answerable ones from SQ UAD-v1. We use both answerable and unanswerable questions for each passage in the annotation task. 2.2 Annotation Task To ensure high quality of the dataset, our annotation process consists of 2 rounds of annotation: First Round of Annotation. Expert raters were shown the passage along with all the associated questions and their answers, with one of the 1 question-answer pair highlighted for annotation. The raters were instructed to use the provided context in crafting disflu"
2021.findings-acl.293,2021.eacl-main.259,0,0.0293543,"but they were limited in the corpus complexity, domain, and scale. Recently there has been renewed interest in constructing audio enriched versions of existing NLP datasets, for example, the S POKEN -SQ UAD (Li et al., 2018) and S POKEN -C O QA (You et al., 2020) with the aim to show the effect of speech recognition errors on QA task. However, since collecting audio is challenging, another line of work involves testing the robustness of NLP models to ASR errors in transcribed texts containing synthetic noise using TTS → ASR technique (Peskov et al., 2019; Peng et al., 2020; Liu et al., 2020; Ravichander et al., 2021). Our work suggests a complementary approach to data collection to surface a specific speech phenomenon that affects NLP. 6 Conclusion This work presented D ISFL -QA, a new challenge set containing contextual semantic disfluencies in a QA setting. D ISFL -QA contains diverse set of disfluencies rooted in context, particularly a large fraction of corrections and restarts, unlike prior datasets. D ISFL -QA allows one to directly quantify the effect of presence of disfluencies in a downstream task, namely QA. We analyze the performance of models under varying when subjected to disfluencies under"
2021.findings-acl.293,P18-1079,0,0.0124737,"or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations. 5.2 Question Answering Under Noise In the QA literature, our work is related to two threads that aim to improve robustness of QA models: (i) QA under adversarial noise, and (ii) noise arising from speech phenomena. Prior work on adversarial QA have predominantly generated adversaries automatically (Zhao et al., 2018), which are verified by humans to ensure semantic equivalence (i.e. answer remains same after perturbation). For instance, Ribeiro et al. (2018) generated adversaries using para3316 phrasing, while Mudrakarta et al. (2018) perturbed questions based on attribution. Closest work to ours is Jia and Liang (2017), who modified SQ UAD to contain automatically generated adversarial sentence insertions. Our work is more closely related to prior work on making NLP models robust to noise arising from speech phenomena. Earlier work (Surdeanu et al., 2006; Leuski et al., 2006) have built QA models which are robust to disfluency-like phenomenon, but they were limited in the corpus complexity, domain, and scale. Recently there has been renewed inte"
2021.findings-acl.293,D17-1296,0,0.0206369,"popular approach in literature poses disfluency correction as a sequence tagging task, in which the fluent version of the utterance is obtained by identifying and removing the disfluent segments (Zayats et al., 2014; Ferguson et al., 2015; Zayats et al., 2016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques have been proposed (Yang et al., 2020; McDougall and Duckworth, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations. 5.2 Question Answeri"
2021.findings-acl.293,2020.emnlp-main.113,1,0.759671,"016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques have been proposed (Yang et al., 2020; McDougall and Duckworth, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations. 5.2 Question Answering Under Noise In the QA literature, our work is related to two threads that aim to improve robustness of QA models: (i) QA under adversarial noise, and (ii) noise arising from speech phenomena. Prior work on adversarial QA have predominantly generated adv"
2021.findings-acl.293,N19-1008,0,0.0149292,"sfluency Correction The most popular approach in literature poses disfluency correction as a sequence tagging task, in which the fluent version of the utterance is obtained by identifying and removing the disfluent segments (Zayats et al., 2014; Ferguson et al., 2015; Zayats et al., 2016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques have been proposed (Yang et al., 2020; McDougall and Duckworth, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations."
2021.findings-acl.293,D18-1316,0,0.0120647,"h, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited in terms of disfluencies types and may not well capture natural disfluencies in daily conversations. 5.2 Question Answering Under Noise In the QA literature, our work is related to two threads that aim to improve robustness of QA models: (i) QA under adversarial noise, and (ii) noise arising from speech phenomena. Prior work on adversarial QA have predominantly generated adversaries automatically (Zhao et al., 2018), which are verified by humans to ensure semantic equivalence (i.e. answer remains same after perturbation). For instance, Ribeiro et al. (2018) generated adversaries using para3316 phrasing, while Mudrakarta et al. (2018) perturbed questions based on attribution. Closest work to ours is Jia and Liang (2017), who modified SQ UAD to contain automatically generated adversarial sentence insertions. Our work is more closely related to prior work on making NLP models robust to noise arising from speech phenomena. Earlier work (Surdeanu et al., 2006; Leuski et al., 2006) have built QA models which a"
2021.findings-acl.293,P11-1071,0,0.0241983,"such complex cases require better modeling, preferably in an end-to-end setup. 5 5.1 Related Work Disfluency Correction The most popular approach in literature poses disfluency correction as a sequence tagging task, in which the fluent version of the utterance is obtained by identifying and removing the disfluent segments (Zayats et al., 2014; Ferguson et al., 2015; Zayats et al., 2016; Lou and Johnson, 2017; Jamshid Lou and Johnson, 2020; Wang et al., 2020). . Traditional disfluency correction models use syntactic features (Honnibal and Johnson, 2014), language models (Johnson et al., 2004; Zwarts and Johnson, 2011), discourse markers (Crible, 2017), or prosody-based features for learning (Zayats and Ostendorf, 2019; Wang et al., 2017) while recent disfluency correction models largely utilize pre-trained neural representations (Lou et al., 2018). Most of these models depend on human-annotated data. As a result, recently, data augmentation techniques have been proposed (Yang et al., 2020; McDougall and Duckworth, 2017) to alleviate the strong dependence on labeled data. However, the resulting augmented data either via heuristics (Wang et al., 2020) or generation models (Yang et al., 2020) is often limited"
D15-1243,P12-1015,0,0.0404941,"i et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Eac"
D15-1243,J90-1003,0,0.350901,"which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ Uk ΣVk T (Landauer and Dumais, 1997). 3 https://code.google.com/p/word2vec 4 https://github.com/wlin12/wang2vec 5 http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 Semant"
D15-1243,W06-1670,0,0.0317102,"Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0."
D15-1243,P14-5004,1,0.611603,"sis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6 https://github.com/mfaruqui/ retrofitting 7 We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8 http://qwone.com/~jason/20Newsgroups 2051 2014a).9 The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. 5 Results To test the efficiency of QVEC in capturing the semantic content of word vectors, we evaluate how well QVEC’s scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings. We begin with comparison of QVEC with one extrinsic task"
D15-1243,P15-2076,1,0.309428,"Missing"
D15-1243,N15-1184,1,0.864444,"Missing"
D15-1243,N13-1092,0,0.0368039,"Missing"
D15-1243,D14-1012,0,0.0362549,"of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequent"
D15-1243,J15-4004,0,0.243621,"Missing"
D15-1243,N15-1142,1,0.522248,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,H93-1061,0,0.140934,"our model obtains high correlation (0.34 ≤ r ≤ 0.89) with the extrinsic tasks (§5). 2 Linguistic Dimension Word Vectors The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re2049 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb le"
D15-1243,I08-2105,0,0.0125447,"thods in Natural Language Processing, pages 2049–2054, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN . BODY, NOUN . ANIMAL, VERB . CONSUMPTION , or VERB . MOTION . SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD fish duck chicken NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· VB . MOTI"
D15-1243,D14-1162,0,0.112475,"Missing"
D15-1243,D15-1036,0,0.233502,"Missing"
D15-1243,N13-1076,1,0.805767,"Missing"
D15-1243,D13-1170,0,0.0376388,"ed lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequently, it is not clear how to score a non-interp"
D15-1243,P14-1024,1,0.829115,"Missing"
D15-1243,tsvetkov-etal-2014-augmenting-english,1,0.809802,"Missing"
D15-1243,P10-1040,0,0.162082,"f word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguistic vectors described in this paper are available at htt"
D15-1243,P14-1074,0,0.0118254,"9 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2 -regularized logistic regression classif"
D15-1243,D13-1196,0,0.0612772,"nexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1 The evaluation script and linguisti"
D15-1243,D15-1161,1,0.0792052,"VEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD 2 VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD 2 VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD 2 VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word"
D15-1243,P14-2131,0,\N,Missing
D18-1028,P16-1231,0,0.0141616,"ordpiece model (Schuster and Nakajima, 2012) with a vocabulary of 16,000. Experimental Design. We train one version of this model on the same set of 23M English examples as the discriminative insertion model from §6.1; we refer to the model trained on this data as Edits. For comparison, we train an identical model on a set of simulated insertions which we create by sampling sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Do"
D18-1028,W12-4006,0,0.388033,"a is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Str"
D18-1028,N13-1055,0,0.0610098,"parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in j"
D18-1028,max-wisniewski-2010-mining,0,0.0968145,"ns, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited"
D18-1028,L18-1008,0,0.0227062,"13). We train this language model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and"
D18-1028,D17-1070,0,0.0589149,"Missing"
D18-1028,C12-1044,0,0.175753,"” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best actress of the film in japan and is the best actress of the Indian cinema in june in 2011 and is the best actress of the film industry He is married to Aida Leanca and has two children Edits General and has a daughter in january , and has"
D18-1028,D17-1064,0,0.0514149,"Missing"
D18-1028,D13-1055,0,0.14171,"Missing"
D18-1028,N09-2061,0,0.0135701,"1.1 42.9 Table 2: The number of instances (in millions) of atomic insertions/deletions for each language. 3 3.1 WikiAtomicEdits: Corpus Creation Extracting Edits Wikipedia edits can be accessed through Wikipedia dumps. The edits are stored as diffs on the entire Wikipedia page, meaning some processing is required to reconstruct the changes that were made at the sentence level. We use historical snapshots of each Wikipedia document and compare against subsequent snapshots to extract sentence-level edits. We strip the HTML tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited"
D18-1028,L18-1550,0,0.0242656,"guage model for each language on an average of ∼ 500 million tokens from Wikipedia. Second, we evaluate a discriminative model specifically trained on the insertion data (Discriminative Model). This model represents the base sentence using a sentence encoder that produces a contextdependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the representation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initialized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018).7 We hold out 50K and 10K insertion edits for each language as development and test sets, and use the remaining edits (insertions and deletions) as training data. This provides us with at least 1 million examples for training in each language (cf. Table 2). See Supplementary Material for additional details. General LM 68.1 58.7 67.0 69.9 69.0 73.0 72.9 65.5 68.0 Discr. Model 72.9 68.4 70.1 73.4 72.9 74.2 74.3 68.9 71.8 Table 8: Insertion accuracy on the test set. make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and 50 examples on which"
D18-1028,P02-1040,0,0.105117,"L tags and Wikipedia markup of the page and then run a sentence splitter (Gillick, 2009) to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time (Myers, 1986) sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation. Given n sentences in one snapshot (“base”) and m sentences in a subsequent one (“edited”), we assume that most edits are local and restrict our attention to a fixed-size window. For each sentence si in the base snapshot, we compute pairwise BLEU scores (Papineni et al., 2002) between si i+k and the sentences {tj }j=i−k (k = 5) in the edited snapshot. We consider the sentence with the highest BLEU score in this window as a candidate. If the sentences are not identical and the difference consists of an insertion or deletion of a single contiguous phrase2 , we add this example to the corpus. For each article, we run this algorithm over the most recent 100,000 snapshots as of February 2018. We extract edits for 8 languages. Statistics are shown in Table 2. 1. The original sentence s does not effectively communicate some piece of information. 2. A reasonable reader of"
D18-1028,D14-1162,0,0.0809643,"n to reporting standard LM perplexity, we compute two measures of performance, which are intended to provide an intuitive picture of how well each model captures the nature of the information that is introduced by the human editors. Specifically, we compute Exact Match as the proportion of sentences for which the model produced the gold phrase (i.e. the phrase inserted by the human editor) somewhere among the top 10 phrases. We also compute Similarity@1 as the mean cosine similarity of each top-ranked phrase and respective gold phrase over the test set. We use the sum of the Glove embeddings (Pennington et al., 2014) of each word in the phrase as a simple approximation of the phrase vector. Table 11 shows the results. We see that, compared to the model trained on General Wikipedia, the model trained on WikiAtomicEdits generates edits which are more similar to the human insertions, according to all of our metrics. Table 10 provides a few qualitative examples of how the phrases generated by the Edits model differ from those generated by the General model. Specifically, we see that the Edits model proposes phrases which better capture the discourse function of the human edit: e.g. providing context for/elabo"
D18-1028,N18-1202,0,0.0802425,"Missing"
D18-1028,Q18-1031,0,0.0624248,"Missing"
D18-1028,P14-2066,0,0.0255462,"e(s)) have been POS-tagged and dependency parsed (Andor et al., 2016) as well as scored using a SOTA LM (Jozefowicz et al., 2016). We also release the 5K 5-way human insertion annotations for English, and 1K 3-way annotations each for Spanish and German, as described in §4. Table 11: Comparison of how closely each model’s generated phrases match the phrase inserted by the human editor. “Edits” was trained on WikiAtomicEdits and “General” was trained on comparable data not derived from human edits. We consider the top 10 phrases generated by each model. and to better understand argumentation (Tan and Lee, 2014). Particular attention has been given to spam edits (Adler et al., 2011) and editor quality (Leskovec et al., 2010). Our work differs in that WikiAtomicEdits is much larger than currently available corpora, both by number of languages and by size of individual languages. In addition, our focus on atomic edits should facilitate more controlled studies of semantics and discourse. 8 Conclusion We have introduced the WikiAtomicEdits corpus, derived from Wikipedia’s edit history, which contains 43M examples of atomic insertions and deletions in 8 languages. We have shown that the language in this c"
D18-1028,D15-1059,0,0.0126598,"index i, generate candidate phrases that would be appropriate to insert into s at i (§6.2). 6.1 Predicting Insertion Locations Task. This task–given a phrase p and a sentence s, choose the best index i in s at which to insert p–is identical to the task we asked humans to perform in §4. We consider two simple models for performing this task: a basic language model and a discriminative model trained on the insertion data. We report performance as overall accuracy. We analyze whether a model which is trained to 6 We note that the addition of “former” is likely tied to changes in the real world (Wijaya et al., 2015). 310 model insertions directly captures something different than a general language model in terms of the types of errors each model makes. German English Spanish French Italian Japanese Russian Chinese Average Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from Jozefowicz et al. (2016), which obtained SOTA results on language modeling on the one billion words benchmark for English (Ch"
D18-1028,P08-2035,0,0.0313163,"g sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more reali"
D18-1028,D17-1213,0,0.555849,"n at both the phrase and the sentence level. We mine Wikipedia edit history to create a corpus of 43 million atomic insertion and deletion edits covering 8 languages. We argue that the corpus contains distinct semantic signals not present in raw text. We thus focus our experiments on answering the following questions: Introduction Written language often undergoes several rounds of revision as human authors determine exactly what information they want their words to convey. On Wikipedia, this process is carried out collectively by a large community at a rate of nearly two revisions per second (Yang et al., 2017). While Wikipedia’s revision history contains arbitrarily complex edits, our corpus and analysis focuses on atomic insertion edits: instances in which an editor has inserted a single, contiguous span of text into an existing complete sentence (Table 1). This restriction allows us to make several assumptions which we believe make the data an especially powerful source of signal. Namely, we can assume that 1) some information was not communicated by the original sentence, 2) that information should have been communicated (according to a human editor), and 3) that information is communicated by t"
D18-1028,N10-1056,0,0.06066,"and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the WikiAtomicEdits data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than us"
D18-1028,E12-1054,0,0.080273,"its data, we parse the sampled sentences (Andor et al., 2016) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence.11 We generate 23M such “psuedo-edits” for training, the same 7 Related Work Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a variety of NLP tasks, including sentence compression and simplification (Yamangil and Nelken, 2008; Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), and writing assistance (Zesch, 2012; Cahill et al., 2013; Grundkiewicz and Junczys-Dowmunt, 2014). User edits from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made (Daxenberger and Gurevych, 2012, 2013; Yang et al., 2017) 11 Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans. 312 She is cited as the first female superstar of Hindi Cinema and India ’s Meryl Streep Edits General and is the best act"
D18-1028,W10-3504,0,\N,Missing
D18-1080,D11-1038,0,0.525718,"its. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSpli"
D18-1080,P18-2114,0,0.345794,"ddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtained on WebSplit by Aharoni and Goldberg (2018). Both authors contributed equally. 732 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1080,Q15-1021,0,0.177909,"kipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and"
D18-1080,P17-1017,0,0.0261232,"oldberg, 2018). For training, we delimit the simple sentences with a special symbol. We depart from the prior work by only using a subset of the WebSplit training set: we take a fixed sub-sample such that each distinct C is paired with a single S, randomly selected from the multiple possibilities in the dataset. This scheme produced superior performance in preliminary experiments. As a quality measure, we report multi-reference corpus-level BLEU4 (Papineni et al., 2002), but Comparison to WebSplit Narayan et al. (2017) derived the WebSplit corpus by matching up sentences in the WebNLG corpus (Gardent et al., 2017) according to partitions of their underlying meaning representations (RDF triples). The WebNLG corpus itself was created by having crowd workers write sentential realizations of one or more RDF triples. The resulting language is often unnatural, for example, “Akeem Dent once played for the Houston Texans team which is based in Houston in Texas.”2 Repetition arises because the same sentence fragment may appear in many different examples. 3 We use WebSplit v1.0 throughout, which is the scaledup re-release by Narayan et al. (2017) at http://github. com/shashiongithub/Split-and-Rephrase, commit a9"
D18-1080,P08-2035,0,0.0421371,"lter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all c"
D18-1080,N09-2061,0,0.0244161,"napshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candidate split into S = (S1 , S2 ), we 2.2 Corpus Statistics and Quality Our extraction heuristic is imperfect, so we manually assess corpus quality using the same categorization schema proposed by Aharoni and Goldberg (2018); see Table 1 for"
D18-1080,D17-1213,0,0.0136428,"d pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candid"
D18-1080,P16-1154,0,0.06498,"ata produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resourc"
D18-1080,N10-1056,0,0.0293049,"pineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we us"
D18-1080,D15-1076,0,0.0320725,"iddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtaine"
D18-1080,D17-1004,0,0.0332047,"simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release"
D18-1080,P17-4012,0,0.0291392,"(sBLEU) for direct comparison to past work.5 We also report lengthbased statistics to quantify splitting. We use the same sequence-to-sequence architecture that produced the top result for Aharoni and Goldberg (2018), “Copy512”, which is a onelayer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014) and a copying mechanism (See et al., 2017) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of Aharoni and Goldberg (2018) using the OpenNMT-py framework (Klein et al., 2017).6 3.1 58.7 55.7 30.5 34.2 60.4 62.4 sBLEU – 56.1 53.0 25.5 30.5 58.0 60.1 In contrast, the W IKI S PLIT model achieves 59.4 BLEU on the WebSplit validation set, without observing any in-domain data. It also outperforms the two deterministic baselines on both validation sets by a non-trivial BLEU margin. This indicates that the WikiSplit training data enable better generalization than when using WebSplit by itself. Reintroducing the downsampled, in-domain training data (B OTH) further improves performance on the WebSplit evaluation. Results We compare to the S OURCE baseline, which is the prev"
D18-1080,C10-1152,0,0.275307,"heir extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. ("
D18-1080,W17-3204,0,0.0248613,"same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containi"
D18-1080,D17-1064,0,0.516619,"alex,jasonbaldridge,dipanjand}@google.com Google AI Language A classic leaf symptom is water-soaked lesions between the veins which appear as angular leaf-spots where the lesion edge and vein meet. Abstract Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia’s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et a"
D18-1080,P02-1040,0,0.102593,"t for training them. To that end, we introduce the WikiSplit corpus and detail its construction next. 2.1 Correct 161 168 169 Unsupp. 35 35 31 Miss. 6 4 4 Size 1.4m 1.0m 0.5m Table 2: Quality vs corpus size trade-off when setting the similarity threshold. The counts are for a random sample of 100 split-and-rephrase examples extracted using our method (i.e., 200 simple sentences). Keys: Unsupported; Missing require that C and S1 have the same trigram prefix, C and S2 have the same trigram suffix, and S1 and S2 have different trigram suffixes. To filter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010"
D18-1080,P17-1099,0,0.206771,"del with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatica"
D18-1080,D16-1033,0,0.104655,"Missing"
D18-1091,D15-1159,0,0.0213141,"fication. Model We use a feed-forward neural network with 2 hidden layers with ReLU activations (Glorot et al., 2011) on each layer and a softmax at the output layer predicting 0 or 1. We extract a variety of features from the query which can be helpful in the classification. We extract character-3, 4-grams and word-1, 2-grams as they can be helpful in capturing spelling errors. In addition to lexical features, we also extract syntactic features that can inform the model on any anomaly in the structure of the query. Specifically, we annotate the query with POS-tags using SyntaxNet POS tagger (Alberti et al., 2015) and extract POS-1, 2, 3-grams.3 Every feature in the network is represented as a real-valued embedding. All the n-grams embeddings of every feature type are summed together and concatenated to form the input layer as shown in Figure 2. The model is trained using crossentropy loss against the gold labels for each query. The hyperparameters are tuned to maximize accuracy on the dev set and results are reported on the test set. Model majority class baseline word bi-LSTM baseline question word baseline word-1 word-1, 2 word-1, 2 char-3, 4 word-1, 2 POS-1, 2, 3 word-1, 2 char-3, 4 POS-1, 2, 3 Appr"
D18-1091,P17-1123,0,0.0854325,"del using wellformedness probability. Sentence: montana is home to the rocky mountain elk foundation and has a historic big game hunting tradition. Gold question: what is the name of the big game hunting foundation in montana? seq2seq: what is a historic big game hunting tradition? (pwf = 0.7) Reranked: what is the name of the historic big game tradition? (pwf = 0.8) Improving Question Generation Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training"
D18-1091,P13-1158,0,0.0683087,"t not a question Ungrammatical and not a question Grammatical and an explicit question An explicit question but ungrammatical Table 1: Examples of well-formed and non-wellformed queries according to the annotation guideline. 2 Query (q) population of owls just in north america? who disscoverd rihanna? what countries have genocide happened in? what is released when an ion is formed? Well-formed Natural Language Question Classifier In this section we describe the data annotation, and the models used for question well-formedness classification. 2.1 Dataset Construction We use the Paralex corpus (Fader et al., 2013) that contains pairs of noisy paraphrase questions. These questions were issued by users in WikiAnswers (a Question-Answer forum) and consist of both web-search query like constructs (“5 parts of chloroplast?”) and well-formed questions (“What is the punishment for grand theft?”), and thus is a good resource for constructing the question well-formedness dataset. We select 25,100 queries from the unique list of queries extracted from the corpus such that no two queries in the selected set are paraphrases. The queries are then annotated into well-formed or non-wellformed questions. We define a q"
D18-1091,D08-1107,0,0.100356,"Missing"
D18-1091,N10-1086,0,0.031318,"seq question generation model using wellformedness probability. Sentence: montana is home to the rocky mountain elk foundation and has a historic big game hunting tradition. Gold question: what is the name of the big game hunting foundation in montana? seq2seq: what is a historic big game hunting tradition? (pwf = 0.7) Reranked: what is the name of the historic big game tradition? (pwf = 0.8) Improving Question Generation Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k"
D18-1091,D07-1086,0,0.104793,"Missing"
D18-1091,N16-1062,0,0.0686451,"Missing"
D18-1091,D15-1075,0,0.064273,"Missing"
D18-1091,P09-1097,0,0.0804238,"Missing"
D18-1091,E06-1032,0,0.0608393,"nd BLEU-4 scores.6 Table 4 shows that the reranked question selected using our query well-formedness clas6 BLEU-1 41.3 41.6 Figure 3: Example showing question selection from the n-best list using our reranking model. sifier improves the BLEU-4 score of a seq-toseq question generation model from 12.0 to 12.2. The oracle improvement, by selecting the sentence from the list that maximizes the BLEU-4 score is 15.2. However, its worth noting that an increase in well-formedness doesn’t guarantee an improved BLEU score, as the oracle sentence maximizing the BLEU score might be fairly non-wellformed (Callison-Burch et al., 2006). For example, “who was elected the president of notre dame in?” has a higher BLEU score to the reference “who was the president of notre dame in 1934?” than our wellformed question “who was elected the president of notre dame?”. Figure 3 shows a question generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the gr"
D18-1091,P16-1170,0,0.0476613,"Missing"
D18-1091,W14-1701,0,0.0165175,"of notre dame in?” has a higher BLEU score to the reference “who was the president of notre dame in 1934?” than our wellformed question “who was elected the president of notre dame?”. Figure 3 shows a question generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the grammatical errors (if any) in a piece of text (Ng et al., 2014). As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text, its a more complex task. However, grammatical error prediction (Schmaltz et al., 2016; Daudaravicius et al., 2016) is the task of classifying whether or not a sentence is grammatical, which is more closely related to BLEU-x uses precision computed over [1, x]-grams. 801 our task as classifying a question as well-formed requires making judgement on both the style and grammar of the text. 5 Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language par"
D18-1091,D17-1061,0,0.10441,"Missing"
D18-1091,P02-1040,0,0.10072,"ing dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training, development and test examples. Their current best model selects the top ranked question from the n-best list produced by the decoder as the output. We augment their system by training a discriminative reranker (Collins and Koo, 2005) with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score (Papineni et al., 2002) between the selected question from the 10-best list and the reference question on the development set. We then use this reranker to select the best question from the 10-best list of the test set. We use the evaluation package released by Chen et al. (2015) to compute BLEU-1 and BLEU-4 scores.6 Table 4 shows that the reranked question selected using our query well-formedness clas6 BLEU-1 41.3 41.6 Figure 3: Example showing question selection from the n-best list using our reranking model. sifier improves the BLEU-4 score of a seq-toseq question generation model from 12.0 to 12.2. The oracle im"
D18-1091,D16-1264,0,0.0591965,"c question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende, 2008; Heilman and Smith, 2010). Du et al. (2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdanau et al., 2015), where the encoder reads in a given text and the decoder is an LSTM RNN that produces the question by predicting one word at a time. Du et al. (2017) use the SQuAD questionanswering dataset (Rajpurkar et al., 2016) to develop a question generation dataset by pairing sentences from the text with the corresponding questions. The question generation dataset contains approx 70k, 10k, and 12k training, development and test examples. Their current best model selects the top ranked question from the n-best list produced by the decoder as the output. We augment their system by training a discriminative reranker (Collins and Koo, 2005) with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score (Papineni et al., 2002) between the s"
D18-1091,W16-0528,0,0.0211642,"uestion generation example with the output of Du et al. (2017) as the baseline result and the reranked question using the wellformed probability. 4 Related Work We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the grammatical errors (if any) in a piece of text (Ng et al., 2014). As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text, its a more complex task. However, grammatical error prediction (Schmaltz et al., 2016; Daudaravicius et al., 2016) is the task of classifying whether or not a sentence is grammatical, which is more closely related to BLEU-x uses precision computed over [1, x]-grams. 801 our task as classifying a question as well-formed requires making judgement on both the style and grammar of the text. 5 Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Comput. Linguist., 31(1):25–70. Ann A Copestake and Dan Flickinger. 2000. An open source grammar development environment and broad-coverage english grammar using HPSG. In Proc. of LREC. Conclusion We p"
D18-1091,W17-4121,1,0.887661,"Missing"
E12-1064,P88-1018,0,0.358685,"arallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and disc"
E12-1064,P11-1078,0,0.0343121,"ra to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yar"
E12-1064,C10-2010,0,0.0180212,"th century) indicated T (“thee”, “didst”). We cleaned the English and German novels manually by deleting the tables of contents, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, t"
E12-1064,E03-1009,0,0.0189628,"eature selection (Manning et al., 2008) on the training set. Preliminary experiments established that selecting the top 800 word features yielded a model with good generalization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most indicative for V, ranked by the ratio of probabilities for T and V, estimated on the training set. Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 19"
E12-1064,P10-1015,0,0.0655988,"ion projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using"
E12-1064,P11-2082,1,0.151896,"the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a"
E12-1064,D09-1015,0,0.0211312,"eneralization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most indicative for V, ranked by the ratio of probabilities for T and V, estimated on the training set. Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 1987). Brown and Levinson’s prediction is that politeness levels will be detectable in concrete utterances in a number of ways, e.g. a higher use of conjunctive or hedges in polite"
E12-1064,W09-0420,0,0.030874,"s that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy."
E12-1064,C90-3028,0,0.0461537,"sh sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, inve"
E12-1064,W03-1612,0,0.0144459,"label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger"
E12-1064,2005.mtsummit-papers.11,0,0.0148346,"feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a parallel corpus. We found commonly used parallel corpora like EUROPARL (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006) to be unsuitable for our study since they either contain almost no direct address at all or, if they do, just formal address (V). Fortunately, for many literary texts from the 19th and early 20th century, copyright has expired, and they are freely available in several languages. We identified 110 stories and novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German)1 that were available in both languages, with a total of 0.5M sentences per language. Examples are Dickens’ David Copperfield or Tolstoy’s An"
E12-1064,D08-1108,0,0.0158898,"ith annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in de"
E12-1064,J03-1002,0,0.00445845,"ts, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, these usages should be distinguished by capitalization (V pronouns are generally capitalized in German), but many T inst"
E12-1064,W95-0107,0,0.0782785,"Missing"
E12-1064,P98-2193,0,0.395739,"tly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studi"
E12-1064,steinberger-etal-2006-jrc,0,0.0403227,"Missing"
E12-1064,N01-1026,0,0.0604276,"guistics V projection Darf ich Sie etwas fragen? Step 1: German pronoun provides overt T/V label V Please permit me to ask you a question. Step 2: copy T/V class label to English sentence Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and"
E12-1064,C98-2188,0,\N,Missing
E14-1049,P05-1074,0,0.0214439,"Missing"
E14-1049,D13-1167,0,0.00450737,"nd at http://cs.cmu. edu/˜mfaruqui/soft.html. Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that captur"
E14-1049,J90-1003,0,0.136179,"matrix. We construct a word co-occurrence frequency matrix F for a given training corpus where each row w, represents one word in the corpus and every column c, is the context feature in which the word is observed. In our case, every column is a word which occurs in a given window length around the target word. For scalability reasons, we only select words with frequency greater than 10 as features. We also remove the top 100 most frequent words (mostly stop words) from the column features. We then replace every entry in the sparse frequency matrix F by its pointwise mutual information (PMI) (Church and Hanks, 1990; Turney, 2001) resulting in X. PMI is designed to give a high value to xij where there is a interesting relation between wi and cj , a small or negative value of xij indicates that the occurrence of wi in cj is uninformative. Finally, we factorize the matrix X using singular value decomposition (SVD). SVD decomposes X into the product of three matrices: X = U ΨV > (7) where, U and V are in column orthonormal form and Ψ is a diagonal matrix of singular values (Golub and Van Loan, 1996). We obtain a reduced dimensional representation of words from size |V |to k: A = U k Ψk (8) 4.2 Semantic Rela"
E14-1049,P08-1088,0,0.0150447,"Missing"
E14-1049,P12-1092,0,0.395239,"Missing"
E14-1049,C12-1089,0,0.853281,"y associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a multitask learning framework to encourage the word representations learned by neural language models to agree cross-lingually. CCA can be used for dimension reduction and to draw correspondences between two sets of data.Haghighi et al. (2008) use CCA to draw translation lexicons between words of two different languages using only monolingual corpora. CCA has also been used for constructing monolingual word representations by correlating word vectors that capture aspects of word meaning and different types of distributional profile of the word Acknowledgements We thanks Kevin Gimpel, Noa"
E14-1049,N13-1090,0,0.627131,",1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–471, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics space embeddings of two different vocabularies where rows represent words. Since the two vocabularies are of different sizes (n1 and n2 ) and there might not exist translation for every word 0 0 of Σ in Ω, let Σ ⊆ Σ where every word in Σ 0 is translated to one other word3 in Ω ⊆ Ω and Σ"
E14-1049,P10-4002,1,0.113625,"and Spanish we used the WMT-20115 monolingual news corpora and for French we combined the WMT-2011 and 20126 monolingual news corpora so that we have around 300 million tokens for each language to train the word vectors. For CCA, a one-to-one correspondence between the two sets of vectors is required. Obviously, the vocabulary of two languages are of different sizes and hence to obtain one-to-one mapping, for every English word we choose a word from the other language to which it has been aligned the maximum number of times7 in a parallel corpus. We got these word alignment counts using cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English{German, French, Spanish}. 5 http://www.statmt.org/wmt11/ http://www.statmt.org/wmt12/ 7 We also tried weighted average of vectors across all aligned words and did not observe any significant difference in results. 6 8 465 See section 5.5 for further discussion on vector length. Lang En De-En Fr-En Es-En Average Dim 640 512 512 512 – WS-353 46.7 68.0 68.4 67.2 56.6 WS-SIM 56.2 74.4 73.3 71.6 64.5 WS-REL 36.5 64.6 65.7 64.5 51.0 RG-65 50.7 75.5 73.5 70.5 62.0 MC-30 42.3 81.9 81.3 78.2 65.5 MTur"
E14-1049,P13-2136,1,0.54011,"er continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as"
E14-1049,N10-1135,0,0.0939216,"Missing"
E14-1049,D13-1168,0,0.259867,"Missing"
E14-1049,N01-1026,0,0.0175523,"nt for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact that both SG and SVD vectors have similar behavior across language pairs can be treated as evidence that semantics or information at a conceptual level (since both of them basically model word cooccurrence counts) transfers well across languages (Dyvik, 2004) although syntax has been projected across languages as well (Hwa et al., 2005; Yarowsky and Ngai, 2001). The pattern of results in the case of RNN vectors are indicative of the fact that these vectors encode syntactic information as explained in §6 which might not generalize well as compared to semantic information. 7 8 Conclusion We have presented a canonical correlation analysis based method for incorporating multilingual context into word representations generated using only monolingual information and shown its applicability across three different ways of generating monolingual vectors on a variety of evaluation benchmarks. These word representations obtained after using multilingual eviden"
E14-1049,P06-2124,0,0.0125756,"t al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and"
E14-1049,W05-0804,0,0.0545703,"ased word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly different for every language pair. SG vectors show a uniform pattern across different language pairs and the performance with multilingual context converges to the monolingual performance when the vector length becomes equal to the monolingual case (k = 80). The fact th"
E14-1049,P00-1054,0,0.167896,"ys have a fixed length of 80, they are just shown in the plots for comparison. 468 (Dhillon et al., 2011). Although our primary experimental emphasis was on LSA based monolingual word representations, which we later generalized to two different neural network based word embeddings, these monolingual word vectors can also be obtained using other continuous models of language (Collobert and Weston, 2008; Mnih and Hinton, 2008; Morin and Bengio, 2005; Huang et al., 2012). Bilingual representations have previously been explored with manually designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) and with unsupervised algorithms like LDA and LSA (Boyd-Graber and Blei, 2012; Zhao and Xing, 2006). Bilingual evidence has also been exploited for word clustering which is yet another form of representation learning, using both spectral methods (Zhao et al., 2005) and structured prediction approaches (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). saturate towards the end. For all the three language pairs the SVD vectors show uniform pattern of performance which gives us the liberty to use any language pair at hand. This is not true for the RNN vectors whose curves are significantly diff"
E14-1049,D13-1141,0,0.826271,"ations obtained after using multilingual evidence perform significantly better on the evaluation tasks compared to the monolingual vectors. We have also shown that our method is more suitable for vectors that encode semantic information than those that encode syntactic information. Our work suggests that multilingual evidence is an important resource even for purely monolingual, semantically aware applications. The tool for projecting word vectors can be found at http://cs.cmu. edu/˜mfaruqui/soft.html. Related Work Our method of learning multilingual word vectors is most closely associated to Zou et al. (2013) who learn bilingual word embeddings and show their utility in machine translation. They optimize the monolingual and the bilingual objective together whereas we do it in two separate steps and project to a common vector space to maximize correlation between the two. Vuli´c and Moens (2013) learn bilingual vector spaces from non parallel data induced using a seed lexicon. Our method can also be seen as an application of multi-view learning (Chang et al., 2013; Collobert and Weston, 2008), where one of the views can be used to capture cross-lingual information. Klementiev et al. (2012) use a mu"
E14-1049,N12-1052,0,0.105356,"Missing"
E14-1049,P10-1040,0,0.0970205,"ur method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462–"
E14-1049,J06-3003,0,0.00817052,"rd representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques. 1 Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using cooccurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010),1 it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). The observation that vectors representing cooccurrence tendencies would capture meaning is expected according to the distributional hypothesis (Harris, 1954), famously articulated by Firth 1 Related approaches use the internal representations from neural network models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsels (Mikolov et al., 2013a) to arrive at vector representations that likewise capture cooccurence tendencies and meanings. 462 Proceedings of the 14th Confere"
E14-1049,N09-1003,0,\N,Missing
K17-2001,E14-1060,1,0.855884,"Missing"
K17-2001,P17-1136,0,0.0562959,"Missing"
K17-2001,N15-1107,1,0.861491,"Missing"
K17-2001,chrupala-etal-2008-learning,0,0.152497,"Missing"
K17-2001,W16-2004,0,0.0654088,"Missing"
K17-2001,P14-2102,1,0.910119,"Missing"
K17-2001,Q15-1031,1,0.919531,"Missing"
K17-2001,P16-1156,1,0.87299,"Missing"
K17-2001,K17-2002,0,0.132442,"Missing"
K17-2001,E17-2120,1,0.859022,"Missing"
K17-2001,E17-1049,1,0.894341,"Missing"
K17-2001,N07-1048,0,0.21796,"Missing"
K17-2001,P17-1182,1,0.876831,"Missing"
K17-2001,D09-1011,1,0.888474,"Missing"
K17-2001,D08-1113,1,0.874358,"Missing"
K17-2001,P16-2090,0,0.46072,"Missing"
K17-2001,N13-1138,0,0.147728,"Missing"
K17-2001,P08-1115,0,0.089787,"Missing"
K17-2001,L16-1498,1,0.792213,"Missing"
K17-2001,N16-1077,1,0.812854,"Missing"
K17-2001,W10-2211,0,0.123726,"Missing"
K17-2001,W16-2006,0,0.0673676,"Missing"
K17-2001,P82-1020,0,0.75423,"Missing"
K17-2001,P08-1103,0,0.0541683,"Missing"
K17-2001,D15-1272,1,0.92636,"Missing"
K17-2001,D14-1095,0,0.0938069,"Missing"
K17-2001,K17-2011,0,0.0350181,"Missing"
K17-2001,N15-1093,0,0.19098,"Missing"
K17-2001,K17-2008,0,0.055504,"Missing"
K17-2001,K17-2010,0,0.158704,"Missing"
K17-2001,W16-2007,0,\N,Missing
K17-2001,L16-1497,1,\N,Missing
K17-2001,K17-2012,0,\N,Missing
K17-2001,K17-2003,0,\N,Missing
K17-2001,P17-1029,0,\N,Missing
L18-1293,E14-1060,1,0.914644,"Missing"
L18-1293,P16-1156,1,0.911216,"Missing"
L18-1293,N07-1048,0,0.136877,"Missing"
L18-1293,N13-1138,0,0.235978,"Missing"
L18-1293,P08-1115,0,0.0456449,"Missing"
L18-1293,N16-1077,1,0.880807,"Missing"
L18-1293,L16-1498,1,0.945257,"The Universal Morphology (UniMorph) project, centered at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University is a collaborative effort to improve how NLP systems handle complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. Kirov et al. (2016) introduced version 1.0 of the UniMorph morphological database, created by extracting and normalizing the inflectional paradigms included in Wiktionary (www.wiktionary.org), a large, broadly multi-lingual crowd-sourced collection of lexical data. This paper describes UniMorph 2.0. It details improvements in Wiktionary extraction and annotation, as well as normalization of non-Wiktionary resources, leading to a much higher quality morphological database. The new dataset spans 52 languages representing a range of language families. As in UniMorph 1.0, we provide paradigms from highlyinflected op"
L18-1293,D14-1095,0,0.116143,"Missing"
L18-1293,N15-1093,0,0.150689,"Missing"
L18-1293,Q15-1026,0,0.0836395,"Missing"
L18-1293,P15-2111,1,0.852807,"rsing and normalization of Wiktionary. Wiktionary is a broadly multilingual resource with many crowd-sourced morphological paradigms in the form of custom HTML tables. Figure 1 illustrates the challenge associated with extracting this data. Wiktionary is designed for human, rather than machine readability, and authors have extensive freedom in formatting data. This leads to wildly differing table layouts across languages which need to be converted to a consistent tabular format. The extraction process developed for UniMorph 1.0 relied heavily on statistical, visual, and positional heuristics (Sylak-Glassman et al., 2015b) to: 1. Determine which entries in an HTML table are inflected forms and which are grammatical descriptors. 2. Link each inflected form with its appropriate descriptors. 3. Convert each set of linked descriptors into a universal feature annotation schema, described in detail in Sylak-Glassman (2016).1 This led to a large dataset of 952,530 unique noun, verb, and adjective lemmas across 350 languages. Unfortunately, 1 pdf unimorph.github.io/doc/unimorph-schema. Figure 1: Paradigm extraction and normalization. the UniMorph 1.0 dataset was very error-prone due to the inability of our heuristics"
L18-1293,zeman-2008-reusable,0,0.0233658,"Each group represents a different type of paradigm (e.g., regular verb). For each group, a sample table was selected, and an annotator replaced each inflected form in the table with the appropriate UniMorph features. All annotation was compliant with the UniMorph Schema, which was designed to represent the full range of semantic distinctions that can be captured by inflectional morphology in any language (SylakGlassman et al., 2015a). The schema is similar in form and spirit to other tagset universalization efforts, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008), but is designed specifically for typological completeness for inflectional morphology, including a focus on the morphology of especially low-resource languages. It includes over 200 base features distributed among 23 dimensions of meaning (i.e., morphological categories), including both common dimensions like tense and aspect as well as rarer dimensions like evidentiality and switch-reference. Despite the high coverage of the UniMorph tagset, for UniMorph 2.0, annotators were allowed to employ additional ‘language specific’ LGSPEC(1, 2, 3, etc.) features to mark any missing distinctions, or"
L18-1293,W16-2002,1,\N,Missing
N15-1151,P08-1004,0,0.0258561,"g a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, the sentences are analyzed using a part of speech tagger, a dependency parser and possibly a named-entity recognizer. In languages other than English, these tools are either unavailable or not accurate enough to be used. In comparison, it is easier to obtain parallel bilingual corpora which can be used to build machine translation systems (Resnik and Smith, 2003;"
N15-1151,P14-1133,0,0.0155661,"oss-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, t"
N15-1151,H05-1091,0,0.0724262,"inguistic tools such as part-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns fro"
N15-1151,P07-1073,0,0.0411905,"rt-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in"
N15-1151,D11-1142,0,0.26616,"results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, the sentences are analyzed using a part of speech tagger, a dependency parser and possibly a named-entity recognizer. In languages other than English,"
N15-1151,W12-0702,0,0.0182227,"Missing"
N15-1151,P12-2010,0,0.104829,"of similar BLEU scores. Interestingly, the highest BLEU score bin (1) contains the maximum number of relations in 1354 4 Related Work Cross-lingual projection has been used for transfer of syntactic (Yarowsky and Ngai, 2001; Hwa et al., 2005) and semantic information (Riloff et al., 2002; Pad´o and Lapata, 2009). There has been a growing interest in RE for languages other than English. Gamallo et al. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dependency parser are employed to learn a RE system for Korean. Tseng et al. (2014) describe an open RE for Chinese that employs word segmentation, POS-tagging, dependency parsing. Lewis and Steedman (2013) learn clusters of semantically equivalent relations across French and English by creating a semantic signature of relations by entity-typing. These relations are extracted using CCG parsing"
N15-1151,I11-1083,0,0.0614994,"er of extracted relations across bins of similar BLEU scores. Interestingly, the highest BLEU score bin (1) contains the maximum number of relations in 1354 4 Related Work Cross-lingual projection has been used for transfer of syntactic (Yarowsky and Ngai, 2001; Hwa et al., 2005) and semantic information (Riloff et al., 2002; Pad´o and Lapata, 2009). There has been a growing interest in RE for languages other than English. Gamallo et al. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dependency parser are employed to learn a RE system for Korean. Tseng et al. (2014) describe an open RE for Chinese that employs word segmentation, POS-tagging, dependency parsing. Lewis and Steedman (2013) learn clusters of semantically equivalent relations across French and English by creating a semantic signature of relations by entity-typing. These rela"
N15-1151,D13-1064,0,0.0333234,"l. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dependency parser are employed to learn a RE system for Korean. Tseng et al. (2014) describe an open RE for Chinese that employs word segmentation, POS-tagging, dependency parsing. Lewis and Steedman (2013) learn clusters of semantically equivalent relations across French and English by creating a semantic signature of relations by entity-typing. These relations are extracted using CCG parsing in English and dependency parsing in French. Blessing and Sch¨utze (2012) use inter-wiki links to map relations from a relation database in a pivot language to the target language and use these instances for learning in a distant supervision setting. Gerber and Ngomo (2012) describe a multilingual pattern extraction system for RDF predicates that uses preexisting knowledge bases for different languages. 5"
N15-1151,D12-1048,0,0.641626,"pologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, the sentences are analyzed using a part of speech tagger, a dependency parser and possibly a named-entity recognizer. In languages other than English, these tools are eithe"
N15-1151,P09-1113,0,0.116419,"ggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to"
N15-1151,J04-4002,0,0.0186194,"nts are many-to-many, each English word can be possibly mapped to more than one source word which leads to ambiguity in its projection; second, a word level mapping can produce non-contiguous phrases in the source sentence, which are hard to interpret semantically. To tackle these problems, we introduce a novel algorithm that incorporates a BLEU score (Papineni et al., 2002) based phrase similarity metric to perform cross-lingual projection of relations. Given a source sentence, its translation, and the wordto-word alignment, we first extract phrase-pairs P using the phrase-extract algorithm (Och and Ney, 2004). In each extracted phrase pair (phrs , phrt ) ∈ P , phrs and phrt are contiguous word sequences in s and t respectively. We next determine the translations of arg1, rel and arg2 from the extracted phrase-pairs. For each English phrase p ∈ {arg1, rel, arg2}, we first obtain the phrase-pair (phrs , phrt ) ∈ P such that phrt has the highest BLEU score relative to p subject to the condition that p ∩ phrt 6= ∅ i.e, there is at least one word overlap between the two phrases. This condition is necessary since we use BLEU score with smoothing and may obtain a nonzero BLEU score even with zero word ov"
N15-1151,P02-1040,0,0.0990547,"arly astronomers) clause. We ignore this clausal information. 4 naive word-alignment based projection would map every word from a phrase extracted in English to the source sentence. This algorithm has two drawbacks: first, since the word alignments are many-to-many, each English word can be possibly mapped to more than one source word which leads to ambiguity in its projection; second, a word level mapping can produce non-contiguous phrases in the source sentence, which are hard to interpret semantically. To tackle these problems, we introduce a novel algorithm that incorporates a BLEU score (Papineni et al., 2002) based phrase similarity metric to perform cross-lingual projection of relations. Given a source sentence, its translation, and the wordto-word alignment, we first extract phrase-pairs P using the phrase-extract algorithm (Och and Ney, 2004). In each extracted phrase pair (phrs , phrt ) ∈ P , phrs and phrt are contiguous word sequences in s and t respectively. We next determine the translations of arg1, rel and arg2 from the extracted phrase-pairs. For each English phrase p ∈ {arg1, rel, arg2}, we first obtain the phrase-pair (phrs , phrt ) ∈ P such that phrt has the highest BLEU score relativ"
N15-1151,C02-1070,0,0.0668389,"extracted relations. This could be attributed to the fact that the average relation length (in number of words) is the shortest for Russian. From table 1, we observe that the length of the relation phrase is inversely correlated with the BLEU score. Figure 2 shows the distribution of the number of extracted relations across bins of similar BLEU scores. Interestingly, the highest BLEU score bin (1) contains the maximum number of relations in 1354 4 Related Work Cross-lingual projection has been used for transfer of syntactic (Yarowsky and Ngai, 2001; Hwa et al., 2005) and semantic information (Riloff et al., 2002; Pad´o and Lapata, 2009). There has been a growing interest in RE for languages other than English. Gamallo et al. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dependency parser are employed to learn a RE system for Korean. Tsen"
N15-1151,P13-1135,0,0.0123787,"it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, the sentences are analyzed using a part of speech tagger, a dependency parser and possibly a named-entity recognizer. In languages other than English, these tools are either unavailable or not accurate enough to be used. In comparison, it is easier to obtain parallel bilingual corpora which can be used to build machine translation systems (Resnik and Smith, 2003; Smith et al., 2013). In this paper, we present a system that performs RE on a sentence in a source language by first translating the sentence to English, performing RE in English, and finally projecting the relation phrase back to the source language sentence. Our system assumes the availability of a machine translation system from a source language to English and an open RE system in English but no any other analysis tool in the source language. The main contributions of this work are: • A pipeline to develop relation extraction system for any source language. • Extracted open relations in ten languages based o"
N15-1151,E14-4003,0,0.0131311,"2002; Pad´o and Lapata, 2009). There has been a growing interest in RE for languages other than English. Gamallo et al. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dependency parser are employed to learn a RE system for Korean. Tseng et al. (2014) describe an open RE for Chinese that employs word segmentation, POS-tagging, dependency parsing. Lewis and Steedman (2013) learn clusters of semantically equivalent relations across French and English by creating a semantic signature of relations by entity-typing. These relations are extracted using CCG parsing in English and dependency parsing in French. Blessing and Sch¨utze (2012) use inter-wiki links to map relations from a relation database in a pivot language to the target language and use these instances for learning in a distant supervision setting. Gerber and Ngomo (2012) describe a"
N15-1151,P14-1090,0,0.0338955,"Missing"
N15-1151,N01-1026,0,0.0219639,"s but has the highest BLEU score between the automatic and the human extracted relations. This could be attributed to the fact that the average relation length (in number of words) is the shortest for Russian. From table 1, we observe that the length of the relation phrase is inversely correlated with the BLEU score. Figure 2 shows the distribution of the number of extracted relations across bins of similar BLEU scores. Interestingly, the highest BLEU score bin (1) contains the maximum number of relations in 1354 4 Related Work Cross-lingual projection has been used for transfer of syntactic (Yarowsky and Ngai, 2001; Hwa et al., 2005) and semantic information (Riloff et al., 2002; Pad´o and Lapata, 2009). There has been a growing interest in RE for languages other than English. Gamallo et al. (2012) present a dependency-parser based open RE system for Spanish, Portuguese and Galician. RE systems for Korean have been developed for both open-domain (Kim et al., 2011) and closed-domain (Kim and Lee, 2012; Kim et al., 2014) using annotation projection. These approaches use a Korean-English parallel corpus to project relations extracted in English to Korean. Following projection, a Korean POS-tagger and a dep"
N15-1151,N07-4013,0,0.00961366,"a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 1 Introduction Relation extraction (RE) is the task of assigning a semantic relationship between a pair of arguments. The two major types of RE are closed domain and open domain RE. While closed-domain RE systems (Bunescu and Mooney, 2005; Bunescu, 2007; Mintz et al., 2009; Yao and Van Durme, 2014; Berant and Liang, 2014) consider only a closed set of relationships between two arguments, open domain systems (Yates et al., 2007; Carlson et al., 2010; Fader et al., 2011; Mausam et al., 2012) use an arbitrary phrase to specify a relationship. In this paper, we focus on open-domain RE for multiple languages. Although there are advantages to closed domain RE (Banko and Etzioni, 2008), it is expensive to construct a closed set of relation types which would be meaningful across multiple languages. Open RE systems extract patterns from sentences in a given language to identify relations. For learning these patterns, the sentences are analyzed using a part of speech tagger, a dependency parser and possibly a named-entity re"
N15-1151,J03-3002,0,\N,Missing
N15-1184,N09-1003,0,0.562483,"Missing"
N15-1184,N09-1014,0,0.00466202,"nalysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector tr"
N15-1184,P98-1013,0,0.739801,"l., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-b"
N15-1184,P12-1015,0,0.139533,"cts of the representations along with an extrinsic sentiment analysis task. Word Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. The second benchmark is the RG-65 (Rubenstein and Goodenough, 1965) dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus. We calculate cosine similarity between the vectors of two words forming a test item, and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently"
N15-1184,D13-1167,0,0.0169875,"luable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors obtained using any ve"
N15-1184,P11-1061,0,0.0334295,"se to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original v"
N15-1184,P11-1144,1,0.318338,"ue qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original vector training model o"
N15-1184,E14-1049,1,0.393831,"e use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vectors are of length 512.4 4 Semantic Lexicons We use three different semantic lexicons to evaluate their utility in improving the word vectors. We include both"
N15-1184,N13-1092,0,0.314995,"Missing"
N15-1184,W06-3808,0,0.0111527,"elief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performance across tasks, semantic lexicons, and languages and showed that it outperforms existing alternatives. The retrofitting tool is available at: https:// g"
N15-1184,D14-1012,0,0.00727559,"cal semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations"
N15-1184,I05-1067,0,0.0116987,"Spanish. We used the Universal WordNet (de Melo and Weikum, 2009), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information a"
N15-1184,D09-1124,0,0.0382359,"09), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information and retrofitting might be less helpful. We train SG vec12 https://github.c"
N15-1184,P12-1092,0,0.106061,"btained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vector"
N15-1184,J03-1003,0,0.0119063,"Missing"
N15-1184,D11-1122,0,0.00432874,"ilar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performan"
N15-1184,W14-1618,0,0.0675244,"rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine different kinds of relations and overall there are 10,675 syntactic pairs of word tuples. The task is to find a word d that best fits the following relationship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences f"
N15-1184,N13-1090,0,0.687964,"tors. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space. These vectors were trained on 6 billion words from Wikipedia and English Gigaword 1608 Lexicon PPDB WordNetsyn WordNetall FrameNet Words 102,902 148,730 148,730 10,822 Edges 374,555 304,856 934,705 417,456 Table 1: Approximate size of the graphs obtained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3"
N15-1184,D14-1162,0,0.120168,"Missing"
N15-1184,D13-1170,0,0.00650579,"nship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an `2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier. 6 Experiments We first show experiments measuring"
N15-1184,D10-1017,0,0.0133974,"red word vector to be close to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agno"
N15-1184,P10-1149,0,0.00454798,"y induction (Yih et al., 2012) and multi-relational latent semantic analysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information wh"
N15-1184,P10-1040,0,0.0887589,"tery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and pa"
N15-1184,J06-3003,0,0.0129744,", and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level inf"
N15-1184,D12-1111,0,0.0389385,"ons should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors"
N15-1184,P14-2089,0,0.636299,"search on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-pr"
N15-1184,J90-1003,0,\N,Missing
N15-1184,J07-2002,0,\N,Missing
N15-1184,C98-1013,0,\N,Missing
N15-1184,D13-1141,0,\N,Missing
N16-1077,E14-1060,0,0.356302,"roach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflect"
N16-1077,N15-1107,0,0.591763,"on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected for"
N16-1077,D15-1041,1,0.717699,"out of characters. These  character vectors are parameters that are learned by our model, exactly as other character vectors. Regarding the second difference, to provide the model the ability to learn the transformation of semantics from input to output, we apply an affine transformation on the encoded vector e: e ← Wtrans e + btrans (5) where, Wtrans , btrans are the transformation parameters. Also, in the encoder we use a bidirectional LSTM (Graves et al., 2005) instead of a uni-directional LSTM, as it has been shown to capture the sequence information more effectively (Ling et al., 2015; Ballesteros et al., 2015; Bahdanau et al., 2015). Our resultant inflection generation model is shown in Figure 3. 637 4.1 Supervised Learning The parameters of our model are the set of character vectors, the transformation parameters (Wtrans , btrans ), and the parameters of the encoder and decoder LSTMs (§3.2). We use negative loglikelihood of the output character sequence as the loss function: XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) (6) t=1 We minimize the loss using stochastic updates with AdaDelta (Zeiler, 2012). This is our purely supervised model for inflection generation and we evaluate it in two different"
N16-1077,D13-1174,1,0.807869,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,N13-1140,1,0.855557,"sult of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an al"
N16-1077,E03-1009,0,0.0140071,"phology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al.,"
N16-1077,P11-1004,0,0.0372597,"ses and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed tra"
N16-1077,N15-1140,0,0.0257116,"Missing"
N16-1077,Q15-1031,0,0.0314936,", back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer. Our model obtains state-"
N16-1077,N16-1080,0,0.0507045,"Missing"
N16-1077,D11-1057,0,0.0342392,"004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection"
N16-1077,N13-1138,0,0.784674,"tions. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wik"
N16-1077,P02-1001,0,0.31819,"al. (2015), and Nicolai et al. (2015), by DDN13, AFH14, AFH15, and NCK15 respectively. These models perform inflection generation as string transduction and largely consist of three major components: (1) Character alignment of word forms in a table; (2) Extraction of string transformation rules; (3) Application of rules to new root forms. The first step is learning character alignments across inflected forms in a table. Figure 2 (a) shows alignment between three word forms of Kalb. Different models use different heuristic algorithms for alignments such as edit distance, dynamic edit distance (Eisner, 2002; Oncina and Sebban, 2006), and longest subsequence alignment (Bergroth et al., 2000). Aligning characters across word forms provide spans of characters that have changed and spans that remain unchanged. These spans are used to extract rules for inflection generation for different inflection types as shown in Figure 2 (b)–(d). By applying the extracted rules to new root forms, inflected words can be generated. DDN13 use a semi-Markov model (Sarawagi and Cohen, 2004) to predict what rules should be applied, using character n-grams (n = 1 to 4) as features. AFH14 and AFH15 use substring features"
N16-1077,E12-1068,0,0.158263,"cted forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impra"
N16-1077,H05-1085,0,0.0322115,"n from the back to the front vowels. 7 Related Work Similar to the encoder in our framework, Rastogi et al. (2016) extract sub-word features using a forwardbackward LSTM from a word, and use them in a traditional weighted FST to generate inflected forms. Neural encoder-decoder models of string transduction have also been used for sub-word level transformations like grapheme-to-phoneme conversion (Yao and Zweig, 2015; Rao et al., 2015). Generation of inflectional morphology has been particularly useful in statistical machine translation, both in translation from morphologically rich languages (Goldwater and McClosky, 2005), and into morphologically rich languages (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 641 Figure 5: Plot of character vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonologic"
N16-1077,D11-1125,0,0.036259,"ed corpus. We use this language model to make predictions about the next character in the sequence given the previous characters, in following two settings. Output Reranking. In the first setting, we first train the inflection generation model using the supervised setting as described in §4.1. While making predictions for inflections, we use beam search to generate possible output character sequences and rerank them using the language model probability along with other easily extractable features as described in Table 2. We use pairwise ranking optimization (PRO) to learn the reranking model (Hopkins and May, 2011). The reranker is trained on the beam output of dev set and evaluated on test set. Language Model Interpolation. In the second setting, we interpolate the probability of observing the next character according to the language model with the probability according to our inflection genferent (equ. 5), and observed consistently worse results. 638 root forms 2764 2027 4055 6400 7249 11200 6957 Infl. 8 27 57 28 53 9 48 Table 3: The number of root forms and types of inflections across datasets. eration model. Thus, the loss function becomes: 1 XT 0 −log p(~y |~x) = − log p(yt |e, ~y&lt;t ) t=1 Z − λlog"
N16-1077,W14-2804,0,0.0911554,"ection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table cont"
N16-1077,J94-3001,0,0.687857,"and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological process"
N16-1077,D15-1176,1,0.0464737,"Missing"
N16-1077,P07-1017,0,0.372308,"m Kalb (calf) when it is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they ca"
N16-1077,Q15-1012,0,0.0435906,"nization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to sequence string transducer."
N16-1077,N15-1093,0,0.72903,"state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large number of training inflection tables extracted from Wiktionary. Every inflection table contains the inflected form of a given root word"
N16-1077,W04-0409,0,0.110421,"Missing"
N16-1077,J96-1003,0,0.0654284,"al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ("
N16-1077,N09-1024,0,0.0607111,"vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural network sequence to seq"
N16-1077,N16-1076,0,0.151754,"Missing"
N16-1077,P08-1084,0,0.0451158,"haracter vectors of Finnish vowels. Their organization shows that front, back and neutral vowel groups have been discovered. The arrows show back and front vowel correspondences. 2011; Fraser et al., 2012). Modeling the morphological structure of a word has also shown to improve the quality of word clusters (Clark, 2003) and word vector representations (Cotterell and Sch¨utze, 2015). Inflection generation is complementary to the task of morphological and phonological segmentation, where the existing word form needs to be segmented to obtained meaningful sub-word units (Creutz and Lagus, 2005; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015; Cotterell et al., 2015; Cotterell et al., 2016). An additional line of work that benefits from implicit modeling of morphology is neural character-based natural language processing, e.g., part-of-speech tagging (Santos and Zadrozny, 2014; Ling et al., 2015) and dependency parsing (Ballesteros et al., 2015). These models have been successful when applied to morphologically rich languages, as they are able to capture word formation patterns. 8 Conclusion We have presented a model that generates inflected forms of a given root form using a neural netw"
N16-1077,P08-1059,0,0.285332,"is used in different cases and numbers. The inflected forms are the result of both ablaut (e.g., a→¨a) and suffixation (e.g., +ern). Inflection generation is useful for reducing data sparsity in morphologically complex languages. For example, statistical machine translation suffers from data sparsity when translating morphologically-rich languages, since every surface form is considered an independent entity. Translating into lemmas in the target language, and then applying inflection generation as a post-processing step, has been shown to alleviate the sparsity problem (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013a). Modeling inflection generation has also been used to improve language modeling (Chahuneau et al., 2013b), identification of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1"
N16-1077,W04-0109,0,0.0383169,"(Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, based on a large numb"
N16-1077,P00-1027,0,0.147447,"cation of multi-word expressions (Oflazer et al., 2004), among other applications. The traditional approach to modeling inflection relies on hand-crafted finite state transducers and lexicography, e.g., using two-level morphology (Koskenniemi, 1983; Kaplan and Kay, 1994). Such systems are appealing since they correspond to linguistic theories, but they are expensive to create, they can be fragile (Oflazer, 1996), and the composed transducers can be impractically large. As an alternative, machine learning models have been proposed to generate inflections from root forms as string transduction (Yarowsky and Wicentowski, 2000; Wicentowski, 2004; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Hulden, 2014; Ahlberg et al., 2015; Nicolai et al., 2015). However, these impose either assumptions about the set of possible morphological processes 634 Proceedings of NAACL-HLT 2016, pages 634–643, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (a) kalb case=nominative number=plural inflection generation &lt;w&gt; kälber &lt;w&gt; Inflection Generation: Background Durrett and DeNero (2013) formulate the task of supervised inflection generation for a given root form, ba"
N16-1077,D14-1179,0,\N,Missing
N16-1161,P14-2131,0,0.011684,"e tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360 words—constitute 10–"
N16-1161,P15-1033,1,0.113206,"Missing"
N16-1161,E14-1049,1,0.21174,") that polyglot phonetic feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages c"
N16-1161,D14-1012,0,0.0184753,"Missing"
N16-1161,D15-1127,0,0.0128799,"feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from"
N16-1161,D13-1196,0,0.0422633,"at (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360"
N16-1161,L16-1529,1,0.829863,"learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguistic matrix and 5 This matrix is described in Littell et al. (2016) and is available at https://github.com/dmort27/panphon/. 1364 manually examined aligned dimensions in the phone vectors from §5.3 (trained on six languages). In the maximally-correlated columns—corresponding to linguistic features long, consonant, nasalized—we examined phones with highest coefficients. These &gt; were: [5:, U:, i:, O:, E:] for long; [v, ñ, dZ, d, f, j, &gt; ts, N] for consonant; and [˜O, ˜E, A˜, œ] ˜ for nasalized. Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., &gt; /dZ/ is pres"
N16-1161,N15-1028,0,0.0136804,"ions are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventor"
N16-1161,W11-2124,0,0.0106161,"gful related groupings across languages. 6 Related Work Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still require a trained model per language, and do not allow parameter sharing at training time. Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003). Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, require adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages. Independently, Ammar et al. (2016) used a different polyglot architecture for multilingual dependency parsing. This work has also confirmed the utility of polyglot architectures in leveraging multilinguality. Multimodal neural language models. Multimodal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al."
N16-1161,qian-etal-2010-python,0,0.0593655,"Missing"
N16-1161,D13-1170,0,0.00194071,"Missing"
N16-1161,P15-2021,1,0.840243,"Missing"
N16-1161,D15-1243,1,0.588955,"with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors. 5.5 Qualitative analysis of vectors Phone vectors learned by Polyglot LMs are mere sequences of real numbers. An interesting question is whether these vectors capture linguistic (phonological) qualities of phones they are encoding. To analyze to what extent our vectors capture linguistic properties of phones, we use the QVEC—a tool to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguisti"
N16-1161,P10-1040,0,0.0233197,"nd /U/ in “bit” and “book.” Only through linguistic analysis does it become evident that (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for"
N16-1161,P15-1130,0,0.00692841,"Missing"
N16-1161,P15-1113,0,0.0360795,"Missing"
N16-1161,P13-2037,0,\N,Missing
N19-1263,D14-1179,0,0.0730948,"Missing"
N19-1263,P16-1188,0,0.227806,"Graff et al., 2003; Napoles et al., 2012). Gigaword contains news articles sourced from various news services over the last two decades. To produce the dataset, we follow the split and preprocessing by Rush et al. (2015), and pair the first sentences and the headlines in the news articles. It results in a 3.8M/190K/1,951 train/dev./test split. The average lengths of the source and target texts are 31.4 and 8.2, respectively. • New York Times Annotated Corpus (NYT; Sandaus, 2008). It contains news articles published between 1996 and 2007 by New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3"
N19-1263,W18-2706,0,0.0338599,"train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Model R G-1 R G-2 R G-L Open-NMT † Cao et al., 2018a (BASIC ) † Cao et al., 20"
N19-1263,W04-0601,0,0.616883,"oder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 20"
N19-1263,W09-0613,0,0.0705198,"en-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018"
N19-1263,D18-1443,0,0.0236684,"New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best publis"
N19-1263,W02-2211,0,0.766154,"used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et a"
N19-1263,P16-1154,0,0.334646,"roduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision comp"
N19-1263,P18-1015,0,0.489164,"inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from the training set (§3.2).2 There is word o"
N19-1263,Q18-1031,0,0.471171,"nd White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar"
N19-1263,D15-1166,0,0.0738266,"Missing"
N19-1263,W14-1602,0,0.0227032,"ng at the bottom example, the model in general follows the exemplar in using noun adjuncts or prepositional phrases (e.g., new home sales vs. sales of new homes), except the first one. Perhaps confused by the distraction in Exemplar 3, the model makes a judgment on the specific amount of growth, but gets it wrong. 6 Exemplar 3: Related Work Exemplar-based generation. Partly inspired by traditional template-based generation (Kukich, 1983; Reiter and Dale, 1997, inter alia), many recent efforts have been devoted to augmenting text generation models with retrieved exemplars (Hodosh et al., 2013; Mason and Charniak, 2014; Song et al., 2016; Lin et al., 2017, inter alia). Source: Sales of new homes in the U.S. increased by 11.8 percent in May, the biggest gain in 26 years... Exemplar 1: U.S. sales of new homes up strongly in March. Output 1: US new home sales rise 11.8 percent in May. Exemplar 2: The sales of new homes in the U.S. grow strongly. Output 2: Sales of new homes in US rise in May. Exemplar 3: U.S. economic statistics: new home sales grow by 2.5 percent. Output 3: US new home sales grow 26 percent in May. Figure 3: Two randomly sampled Gigaword development instances used for qualitative evaluation ("
N19-1263,W12-3018,0,0.0171138,"Missing"
N19-1263,P09-5002,0,0.0129907,"digm, it first encodes the content representation from the given input text; to produce the output, it retrieves exemplar text from the training data as “soft templates,” which are then used to construct an exemplar-specific decoder. We evaluate the proposed model on abstractive text summarization and data-to-text generation. Empirical results show that this model achieves strong performance and outperforms comparable baselines. 1 Introduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degen"
N19-1263,P83-1022,0,0.500588,"tation from the given input text; to produce the output, it retrieves exemplar text from the training data as “soft templates,” which are then used to construct an exemplar-specific decoder. We evaluate the proposed model on abstractive text summarization and data-to-text generation. Empirical results show that this model achieves strong performance and outperforms comparable baselines. 1 Introduction Conditioned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utteranc"
N19-1263,D16-1128,0,0.165538,"et al., 2014; Cho et al., 2014). In the interest of the noConditioned text generation and the encoderdecoder architecture. Our discussion centers around conditioned text generation, i.e., the model aims to output the target y = y1 y2 . . . yT given the source input x = x1 x2 . . . xS , both of which are sequences of tokens. Each token xi , yi takes one value from a vocabulary V. x and y could vary depending on the tasks, e.g., they will respectively be articles and summaries for text summarization; and for data-to-text generation, x would be structured data, which can sometimes be linearized (Lebret et al., 2016; Wiseman et al., 2018, inter alia), and y is the output text. We aim to learn a (parameterized) conditional distribution of the target text y given the source x, p y|x = T Y t=1 p yt |y&lt;t , x , (1) where y&lt;t = y1 . . . yt 1 is the prefix of y up to the (t 1)th token (inclusive). The probability of each target token is usually estimated with a softmax function: exp h&gt; t 1 w yt p (yt |y&lt;t , x) = P . &gt; y exp ht 1 wy (2) wy denotes a learned vector for token y 2 V. ht 1 depends on y&lt;t and x, and is computed by a function which we will describe soon. A typical implementation choice for computing h"
N19-1263,N16-1014,0,0.0688925,"d Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability"
N19-1263,P09-1011,0,0.0392243,"nstead of F1 on fulllength predictions. 2560 Model ROUGE-1 ROUGE-2 Durrett et al. (2016) Paulus et al. (2018) 42.2 42.9 24.9 26.0 This work (S EQ 2 SEQ) † This work (ATT E XP ) 41.9 42.5 25.1 25.7 † This 43.2 26.4 work (A DA D EC) Table 3: NYT text summarization test performance in ROUGE recall values. This is a smaller portion of the original test data, after filtering out instances with summaries shorter than 50 tokens (§4.2; Durrett et al., 2016). † denotes the models using retrieved exemplars, and bold font indicates best performance. seen as a table consisting of a collection of records (Liang et al., 2009). For a given entity, each record is an (attribute, value) tuple. Figure 2 shows an example for entity Jacques-Louis David. The table specifies the entity’s properties with tuples (born, 30 August 1748), (nationality, French), and so forth. The table is paired with a description, which the model is supposed to generate using the table as input. We refer the readers to Lebret et al. (2016) for more details about the task. Dataset and implementation details. We use the Wikibio dataset (Lebret et al., 2016). It is automatically constructed by pairing the tables and the opening sentences of biogra"
N19-1263,P18-1123,0,0.179153,"tt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from the training set (§3.2)"
N19-1263,P02-1040,0,0.104453,"Missing"
N19-1263,D18-1152,1,0.829983,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,P17-1186,1,0.815842,"r h i (p) Vp = v1 , . . . , vr(p) . (7b) Equations 5 and 6 can be compactly written as P = Up ⇤ Vp&gt; . (8) where ⇤ is the diagonal matrix built from the rdimensional coefficient vector = [ 1 , . . . , r ]&gt; : 2 3 6 ⇤ = diag( ) = 4 1 .. . r 7 5. (9) The construction of Q is similar, but with a different set of parameters matrices Uq and Vq :5 Q = Uq ⇤ Vq&gt; . (10) Note that, despite their similarities to SVD at a first glance, Equations 8 and 10 are not performing matrix factorization. Rather, we are learning {Up , Vp , Uq , Vq } directly; P, Q, {Pi }, and {Qi } are never explicitly instantiated (Peng et al., 2017, 2018c). To summarize, we reparameterize P and Q as interpolations of rank-1 matrices. By the fact that rank(A + B)  rank(A) + rank(B), the ranks of P and Q are upper-bounded by r. As pointed out by Krueger and Memisevic (2017), the parameter matrices of a trained RNN tend to have full rank. Therefore, in the experiments, we set r equal to the hidden size d, aiming to allow the adaptive decoder to use full-rank matrices in the recurrent computation. Yet, if one holds a priori beliefs that the matrices should have lower ranks, using r &lt; d could be desirable. When r = d, an adaptive RNN constr"
N19-1263,P18-1173,1,0.807432,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,N18-1135,1,0.833271,"oder construction. 1: 2: 3: 4: 5: 6: procedure (x) Retrieve the exemplar zx . §3.2 Compute zx ’s representation a . §3.2 Compute coefficients . Eq.11 Construct the decoder f . Eqs.8, 10 end procedure Closing this section, Algorithm 1 summarizes the procedure to construct an adaptive decoder. 3.3 Discussion. Although we’ve based our discussion on Elman networks so far, it is straightforward to apply this method to its gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014, inter alia), and other quasi-/non-recurrent neural architectures (Bradbury et al., 2017; Vaswani et al., 2017; Peng et al., 2018a, inter alia). Throughout the experiments, we will be using an adaptive LSTM decoder (§4). As a drop-in replacement in the encoder-decoder architecture, it introduces a reasonable amount of additional parameters and computational overhead, especially when one uses a small encoder for the exemplar (i.e., the sizes of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full tra"
N19-1263,W04-1013,0,0.0798224,"ng 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Model R G-1 R G-2 R G-L Open-NMT † Cao et al., 2018a (BASIC ) † Cao et al., 2018a (F ULL ) ? Cao et al. (2018b) 35.0 36.0 37.0 37.3 16.6 17.1 19.0 17.6 32.4 33.2 34.5 34.2"
N19-1263,D14-1162,0,0.0865355,"., 2016). It is automatically constructed by pairing the tables and the opening sentences of biography articles from English Wikipedia. We follow the split and preprocessing provided along with the dataset, with around 583K/73K/73K train/dev./test instances. Following Lebret et al. (2016), we linearize the tables, such that we can conveniently train the sequence-to-sequence style models described in §4.1. Table 1 summarizes some statistics of the dataset. In contrast to the text summarization experiment (§4.2), we do not apply BPE here. Further, the word embeddings are initialized with GloVe (Pennington et al., 2014; fixed during training), and not tied with the softmax weights. In addition to the models introduced in §4.1, we additionally compare to A DA D EC +ATT E XP, aiming to study whether the adaptive decoder can further benefit from attention and copy mechanisms over the exemplars. Empirical results. Following Liu et al. (2018) we report ROUGE-4 and B LEU scores (Papineni Jacques-Louis David (30 August 1748 – 29 December 1825) was a French painter in the Neoclassical style. Figure 2: A training instance from the Wikibio dataset. It consists of a collections of records for Jacques-Louis David (top)"
N19-1263,D18-1039,0,0.21384,"es of the ci vectors in Equation 11 are small). It can benefit from the highly-optimized GPU implementations, e.g., CuDNN, since it uses the same recurrent computation as a standard nonadaptive RNN. In addition to the neural networks, the adaptive decoder requires access to the full training set due to the retrieval step. In this sense it is semi-parametric.9 The idea to dynamically construct the parameters is inspired by Hypernetworks (Ha et al., 2017) and earlier works therein. It proves successful in tasks such as classification (Jia et al., 2016; Liu et al., 2017) and machine translation (Platanios et al., 2018). Many recent template-based generation models include the exemplars as content in addition to the source, and allow the decoder to attend over and copy from both (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia). We compare to this approach in the experiments, and show that our model offers fa9 Nothing prohibits adaptively constructing other components of the model, e.g., the encoder g✓ . Yet, our motivation is to use exemplars to inform how to say it, which is primarily determined by the decoder (in contrast, the encoder relates more"
N19-1263,E17-2025,0,0.0220484,"describe the architectures of the compared models in §4.1. 4.1 Compared Models In addition to previous works, we compare to the following baselines, aiming to control for confounding factors due to detailed implementation choices. • S EQ 2 SEQ. The encoder-decoder architecture enhanced with attention and copy mechanisms. The encoder is implemented with a bi-directional LSTM (BiLSTM; Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997; Graves, 2012), and the decoder a uni-directional one. We tie the input embeddings of both the encoder and the decoder, as well as the softmax weights (Press and Wolf, 2017). We use beam search during evaluation, with length penalty (Wu et al., 2016). • ATT E XP. It is based on S EQ 2 SEQ. It encodes, attends over, and copies from the exemplars, in addition to the source inputs. Our model using the adaptive decoder (A DA D EC) closely builds upon S EQ 2 SEQ. It uses a dynamically constructed LSTM decoder, and does not use attention or copy mechanisms over the encoded exemplars. The extracted exemplars are the same as those used by ATT E XP. To ensure fair comparisons, we use comparable training procedures and regularization techniques for the above models. The re"
N19-1263,D15-1044,0,0.071714,"the same as those used by ATT E XP. To ensure fair comparisons, we use comparable training procedures and regularization techniques for the above models. The readers are referred to the appendix for further details such as hyperparameters. 2559 4.2 Text Summarization Datasets. We empirically evaluate our model on two benchmark text summarization datasets: • Annotated Gigaword corpus (Gigaword; Graff et al., 2003; Napoles et al., 2012). Gigaword contains news articles sourced from various news services over the last two decades. To produce the dataset, we follow the split and preprocessing by Rush et al. (2015), and pair the first sentences and the headlines in the news articles. It results in a 3.8M/190K/1,951 train/dev./test split. The average lengths of the source and target texts are 31.4 and 8.2, respectively. • New York Times Annotated Corpus (NYT; Sandaus, 2008). It contains news articles published between 1996 and 2007 by New York Times. We use the split and preprocessing by Durrett et al. (2016).10 Following their effort, we evaluate on a smaller portion of the test set, where the gold summaries are longer than 50 tokens. We further randomly sample 9,000 instances from the training data for"
N19-1263,P17-1099,0,0.371675,"oned text generation is the essence of many natural language processing (NLP) tasks, e.g., text summarization (Mani, 1999), machine translation (Koehn, 2009), and data-to-text generation (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997). In its common neural sequence-tosequence formulation (Sutskever et al., 2014; Cho et al., 2014), an encoder-decoder architecture is used. The decoder generates the text autoregressively, token-by-token, conditioning on the feature representations encoded from the source, typically with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016; See et al., 2017). This paradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural appr"
N19-1263,P16-1162,0,0.0246657,"stances from the training data for validation, resulting in a 91,834/9,000/3,452 train/dev./test split. Compared to Gigaword, the inputs and targets in NYT are much longer (averaging 939.0 and 48.6, respectively). Table 1 summarizes some statistics of the datasets. We note that some recent works use a different split of the NYT corpus (Paulus et al., 2018; Gehrmann et al., 2018), and thus are not comparable to the models in Table 3. We decide to use the one by Durrett et al. (2016) because their preprocessing script is publicly available. For both datasets, we apply byte-paired encoding (BPE; Sennrich et al., 2016), which proves to improve the generation of proper nouns (Fan et al., 2018). Empirical results. Table 2 compares the models on Gigaword test set in ROUGE F1 (Lin, 2004).11 By using adaptive decoders, our model (A DA D EC) improves over S EQ 2 SEQ by more than 1.1 ROUGE scores. Cao et al. (2018b) and the F ULL model by Cao et al. (2018a) hold the best published results. The former uses extensive handcrafted features and relies on external information extraction and syntactic parsing systems; 10 https://github.com/gregdurrett/ berkeley-doc-summarizer. 11 Version 1.5.5 of the official script. Mod"
N19-1263,W18-5713,0,0.312935,"iter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b) concatenating the exemplar to the source as additional inputs, allowing the decoder to attend over and copy from both. We propose a different strategy for using exemplars. For motivation, Figure 1 shows a sourcetarget pair together with its exemplar from the Gigaword dataset (Graff et al., 2003). The target is a summary of the source sentence, and the exemplar is retrieved from th"
N19-1263,D17-1239,0,0.0873391,"aradigm is capable of generating fluent abstractive text, but in an uncontrolled and sometimes unreliable way, often producing degenerate outputs and favoring generic utterances (Vinyals and Le, 2015; Li et al., 2016). The encoder-decoder approach differs considerably from earlier template-based meth⇤ Work done during internship at Google. ods (Becker, 2002; Foster and White, 2004; Reiter et al., 2005; Gatt and Reiter, 2009, inter alia), where the source content is filled into the slots of a handcrafted template. These solutions offer higher generation precision compared to neural approaches (Wiseman et al., 2017), but tend to lack the naturalness of neural systems, and are less scalable to open domain settings, where the number of required templates can be prohibitively large. To sidestep the scalability problems with handcrafted templates, it has been proposed to use similar training samples as exemplars, to guide the decoding process (Gu et al., 2018; Guu et al., 2018; Weston et al., 2018; Pandey et al., 2018; Cao et al., 2018a, inter alia).1 In general, existing methods accomplish this by (a) using traditional information retrieval (IR) techniques for exemplar extraction (e.g., TF-IDF), and then (b"
N19-1263,D18-1356,0,0.437454,"al., 2014). In the interest of the noConditioned text generation and the encoderdecoder architecture. Our discussion centers around conditioned text generation, i.e., the model aims to output the target y = y1 y2 . . . yT given the source input x = x1 x2 . . . xS , both of which are sequences of tokens. Each token xi , yi takes one value from a vocabulary V. x and y could vary depending on the tasks, e.g., they will respectively be articles and summaries for text summarization; and for data-to-text generation, x would be structured data, which can sometimes be linearized (Lebret et al., 2016; Wiseman et al., 2018, inter alia), and y is the output text. We aim to learn a (parameterized) conditional distribution of the target text y given the source x, p y|x = T Y t=1 p yt |y&lt;t , x , (1) where y&lt;t = y1 . . . yt 1 is the prefix of y up to the (t 1)th token (inclusive). The probability of each target token is usually estimated with a softmax function: exp h&gt; t 1 w yt p (yt |y&lt;t , x) = P . &gt; y exp ht 1 wy (2) wy denotes a learned vector for token y 2 V. ht 1 depends on y&lt;t and x, and is computed by a function which we will describe soon. A typical implementation choice for computing ht is the encoder-decod"
P11-2082,P88-1018,0,0.656711,"guage Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing"
P11-2082,C10-2010,0,0.0533823,"cleaned by deleting the index, prologue, epilogue and Gutenberg license from the beginning and end of the files. To some extent the chapter numbers and titles occurring at the beginning of each chapter were cleared as well. The files were then formatted to contain one sentence per line and a blank line was inserted to preserve the segmentation information. The sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005) were used. We obtained a comparable corpus of English and German novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/ 468 ID (1) (2) (3) (4) Position any non-initial non-initial non-initial Lemma du sie ihr ihr Cap any yes no yes Category T V T V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to k"
P11-2082,E03-1009,0,0.101583,"Missing"
P11-2082,P10-1015,0,0.0951839,"al/informal (T/V) address distinction in 5 We experimented with logistic regression models, but were unable to obtain better performance, probably because we introduced a frequency threshold to limit the feature set size. Top 10 words for V P (w|V ) Word w P (w|T ) Fogg 49.7 32.5 Oswald Ma 31.8 25.2 Gentlemen 24.2 Madam Parfenovitch 23.2 Monsieur 22.6 22.5 Fix Permit 22.5 ’am 22.4 Top 10 words for T P (w|T ) Word w P (w|V ) Thee 67.2 Trot 46.8 Bagheera 37.7 Khan 34.7 Mowgli 33.2 Baloo 30.2 Sahib 30.2 Clare 29.7 didst 27.7 Reinhard 27.2 ever, is the induction of social networks in such novels (Elson et al., 2010): Information on the social relationship between a speaker and an addressee should provide global constraints on all instances of communications between them, and predict the form of address much more reliably than word features can. Acknowledgments Manaal Faruqui has been partially supported by a Microsoft Research India Travel Grant. References Table 4: Words that are indicative for T or V modern English, where it is not determined through pronoun choice or other overt means. We see this task as an instance of the general problem of recovering “hidden” information that is not expressed overt"
P11-2082,W09-0420,0,0.0528938,"ddress. Our results could be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June"
P11-2082,C90-3028,0,0.310283,"ction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science,"
P11-2082,W03-1612,0,0.284331,"short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in parti"
P11-2082,2005.mtsummit-papers.11,0,0.00963744,"lation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are freely available in several languages. We identified 115 novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per language.1 Examples include Dickens’ David Copperfield or Tolstoy’s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align. 2.2 Data Preparation As the Ger"
P11-2082,D08-1108,0,0.203039,"Missing"
P11-2082,J03-1002,0,0.0035304,"n novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/ 468 ID (1) (2) (3) (4) Position any non-initial non-initial non-initial Lemma du sie ihr ihr Cap any yes no yes Category T V T V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as efficient as possible. 2.3 T/V Gold Labels for English Utterances The goal of creating our corpus is to enable the investigation of contextual correlates of T/V in English. In order to do this, we need to decide for as many English utterances in our corpus as possible whether they instantiate formal or informal address. Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and"
P11-2082,P98-2193,0,0.590452,"be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, c Portland, Oregon, June 19-24, 2011. 2011 Associati"
P11-2082,N01-1026,0,0.154055,"Missing"
P11-2082,C98-2188,0,\N,Missing
P13-2136,W09-0420,0,0.0223645,"Missing"
P13-2136,P10-1156,0,0.0609848,"Missing"
P13-2136,J92-4003,0,0.468129,"score when using bilingual word clusters instead of monolingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surpris"
P13-2136,2012.eamt-1.60,0,0.0204639,"zer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3 . The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the F1 score Inference Figure 1 shows th"
P13-2136,E03-1009,0,0.0459483,"nstead of monolingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence mode"
P13-2136,D11-1005,0,0.0536264,"Missing"
P13-2136,P08-1068,0,0.0546467,"olingual clusters. 1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the mo"
P13-2136,D09-1092,0,0.0706998,"Missing"
P13-2136,N13-1073,1,0.762293,"evelopment set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora are of different sizes and hence the monolingual German data from every parallel corpus is different. We treat the F1 score Inference Figure 1 shows the factor graph representation of our clustering models. Finding the optimal clustering under both the monolingual and bilingual objectives is a computationally hard combinatorial optimization problem (Och, 1995). We use a greedy hil"
P13-2136,E12-1064,1,0.874828,"Missing"
P13-2136,E99-1010,0,0.0664772,"for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the monolingual clustering objective. Let ci denote the word class of word wi . Our objective assumes that the probability of a word sequence w = hw1 , w2 , . . . , wM i is p(w) = M Y i=1 p(ci |ci−1 ) × p(wi |ci ), (2.1"
P13-2136,D09-1015,0,0.0153308,"2010). We therefore chose to focus our evaluation on the latter problem. For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3 . The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5 , which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken. Monolingual Clustering: For every language pair, we train German word clusters on the monolingual German data from the parallel data. Note that the parallel corpora"
P13-2136,P98-2193,0,0.0481045,"Missing"
P13-2136,D07-1014,0,0.0619852,"Missing"
P13-2136,P08-1084,0,0.028793,"Missing"
P13-2136,N12-1052,0,0.185105,"Missing"
P13-2136,P10-1040,0,0.734145,"1 2 Word Clustering A word clustering C is a partition of a vocabulary Σ = {x1 , x2 , . . . , x|Σ |} into K disjoint subsets, C1 , C2 , . . . , CK . That is, C = S {C1 , C2 , . . . , CK }; Ci ∩ Cj = ∅ for all i 6= j and K k=1 Ck = Σ. Introduction A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; T¨ackstr¨om et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters. We propose a novel bilingual word clustering objective (§2). The first term deals with each 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the monolingual clustering o"
P13-2136,C98-2188,0,\N,Missing
P14-5004,N09-1003,0,0.385497,"Missing"
P14-5004,P12-1015,0,0.088831,"n given similarity rankings by humans. These differ from WS-353 in that it contains only nouns whereas the former contains all kinds of words. The sixth benchmark is the MTurk-2875 (Radinsky et al., 2011) dataset that constitutes 287 pairs of words and is different from the previous benchmarks in that it has been constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk (AMT). Similar in spirit is the MTruk-7716 (Halawi et al., 2012) dataset that contains 771 word pairs whose similarity was crowdsourced from AMT. Another, AMT created dataset is the MEN7 benchmark (Bruni et al., 2012) that consists of 3000 word pairs, randomly selected from words that occur at least 700 times in the freely available ukWaC and Wackypedia8 corpora combined. The next two benchmarks were created to put emphasis on different kinds of word types. To specifically emphasize on verbs, Yang and Powers (2006) created a new benchmark YP-130 of 130 verb pairs with human similarity judgements. Since, most of the earlier discussed datasets contain word pairs that are relatively more frequent in a corpus, Luong et al. (2013) create a new benchMultilingual Benchmarks. As is the case with most NLP problems,"
P14-5004,E14-1049,1,0.186072,"Missing"
P14-5004,D13-1170,0,0.00745772,"tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user vectors for exhaustive offline evaluation and leader board ranking • Publicly available repository of word vectors with performance details Availability of such an evaluation system will help in enabling better consistency and uniformity in evaluation of word vector representations as well as provide an easy to use interface for endusers in a similar spirit to Socher et al. (2013a), a website for text classification.2 Apart from the online demo version, we also provide a software that can be run in an offline mode on the command line. Both the online and offline tools will be kept updated with continuous addition of new relevant tasks and vectors. 1 2 www.wordvectors.org/suite.php www.etcml.com 19 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 19–24, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics 2 mark (Rare-Word)9 that contains rare-words by sampling words"
P14-5004,P10-1040,0,0.764387,"ord vectors in R2 • Evaluation and comparison of the available open-source vectors on the suite Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using co-occurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010), it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). A number of approaches that use the internal representations from models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsets (Mikolov et al., 2013) to arrive at vector representations have also been shown to likewise capture co-occurrence tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user vectors for exhaustive offline evaluation and leader board ranking • Publicly available"
P14-5004,P12-1092,0,0.541252,"These images can then be downloaded and used. We have 3 http://www.cs.technion.ac.il/˜gabr/ resources/data/wordsim353/ 4 http://alfonseca.org/eng/research/ wordsim353.html 5 http://tx.technion.ac.il/˜kirar/ Datasets.html 6 http://www2.mta.ac.il/˜gideon/ mturk771.html 7 http://clic.cimec.unitn.it/˜elia. bruni/MEN.html 8 http://wacky.sslmit.unibo.it/doku. php?id=corpora 9 http://www-nlp.stanford.edu/˜lmthang/ morphoNLM/ 10 http://homepage.tudelft.nl/19j49/ t-SNE_files/tsne_python.zip 20 Figure 1: Antonyms (red) and synonyms (green) of beautiful represented by Faruqui and Dyer (2014) (left) and Huang et al. (2012) (right). Metaoptimize. These word embeddings 11 have been trained in (Turian et al., 2010) using a neural network language model and were shown to be useful for named entity recognition (NER) and phrase chunking. included two datasets by default which exhibit different properties of the language: • Antonyms and synonyms of beautiful • Common male-female nouns and pronouns SENNA. It is a software12 which outputs a host of predictions: part-of-speech (POS) tags, chunking, NER etc (Collobert et al., 2011). The software uses neural word embeddings trained over Wikipedia data for over 2 months. In"
P14-5004,J06-3003,0,0.0110359,"evaluation benchmarks • Evaluation of user computed word vectors • Visualizing word vectors in R2 • Evaluation and comparison of the available open-source vectors on the suite Introduction Data-driven learning of vector-space word embeddings that capture lexico-semantic properties is a technique of central importance in natural language processing. Using co-occurrence statistics from a large corpus of text (Deerwester et al., 1990; Turney and Pantel, 2010), it is possible to construct high-quality semantic vectors — as judged by both correlations with human judgements of semantic relatedness (Turney, 2006; Agirre et al., 2009) and as features for downstream applications (Turian et al., 2010). A number of approaches that use the internal representations from models of word sequences (Collobert and Weston, 2008) or continuous bags-of-context wordsets (Mikolov et al., 2013) to arrive at vector representations have also been shown to likewise capture co-occurrence tendencies and meanings. With an overwhelming number of techniques to obtain word vector representations the task of comparison and choosing the vectors best suitable for a particular task becomes difficult. This is • Submission of user"
P14-5004,W13-3512,0,0.0264642,"ty was crowdsourced from AMT. Another, AMT created dataset is the MEN7 benchmark (Bruni et al., 2012) that consists of 3000 word pairs, randomly selected from words that occur at least 700 times in the freely available ukWaC and Wackypedia8 corpora combined. The next two benchmarks were created to put emphasis on different kinds of word types. To specifically emphasize on verbs, Yang and Powers (2006) created a new benchmark YP-130 of 130 verb pairs with human similarity judgements. Since, most of the earlier discussed datasets contain word pairs that are relatively more frequent in a corpus, Luong et al. (2013) create a new benchMultilingual Benchmarks. As is the case with most NLP problems, the lexical semantics evaluation benchmarks for languages other than English have been limited. Currently, we provide a link to some of these evaluation benchmarks from our website and in future will expand the website to encompass vector evaluation for other languages. 3 Visualization The existing benchmarks provide ways of vector evaluation in a quantitative setting. To get an idea of what kind of information the vectors encode it is important to see how these vectors represent words in n-dimensional space, wh"
P15-1144,J08-4004,0,0.041742,"Missing"
P15-1144,P98-1013,0,0.241048,"Missing"
P15-1144,P14-2131,0,0.0164479,"Missing"
P15-1144,E14-1049,1,0.0297678,"., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which con"
P15-1144,N15-1184,1,0.140197,"Missing"
P15-1144,P14-1046,0,0.0799848,"gularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporat"
P15-1144,N15-1004,0,0.157051,"rameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimizatio"
P15-1144,N04-1039,0,0.0582336,"rk To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V"
P15-1144,S13-1001,0,0.0655251,"Missing"
P15-1144,D14-1012,0,0.017442,"overcomplete representations A and also binarized representations B. Initial vectors are discussed in §A and tasks in §B. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L. achieves a binary, sparse vector (B) by applying:  bi,j = 1 if xi,j > 0 0 otherwise (7) The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:   1 if xi,j ≥ M + −1 if xi,j ≤ M − ai,j = (8)  0 otherwise where M + (M − ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability"
P15-1144,J15-4004,0,0.0321756,"Missing"
P15-1144,P12-1092,0,0.066485,"ia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word S"
P15-1144,W03-1018,0,0.0399485,"dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164"
P15-1144,D13-1147,0,0.0458503,"Missing"
P15-1144,D13-1196,0,0.025068,"d accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10"
P15-1144,Q13-1015,0,0.0245514,"Missing"
P15-1144,W14-2406,0,0.0387394,"Missing"
P15-1144,C02-1150,0,0.0787722,"e classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (8"
P15-1144,J93-2004,0,0.058184,"der three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validati"
P15-1144,D11-1139,1,0.497885,"Missing"
P15-1144,P14-1074,1,0.851886,"are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234"
P15-1144,P14-2089,0,0.0191967,"Missing"
P15-1144,C12-1118,0,0.72512,"is vectors. λ is a regularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can"
P15-1144,D14-1162,0,0.123387,"ymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Conte"
P15-1144,D13-1170,0,0.0032486,"/code.google.com/p/word2vec http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6 http://cs.cmu.edu/˜mfaruqui/soft.html 1497 5 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many"
P15-1144,D13-1015,0,0.0389009,"Missing"
P15-1144,C98-1013,0,\N,Missing
P15-1144,P14-1009,0,\N,Missing
P15-2076,W02-1001,0,0.376393,"Missing"
P15-2076,N15-1184,1,0.732372,".3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve"
P15-2076,P15-1144,1,0.438728,".3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some ve"
P15-2076,P13-1174,0,0.0165144,"ets and records a number of relations among these synsets or their members. For a word we look up its synset for all possible part of speech (POS) tags that it can assume. For example, film will have S YNSET.F ILM .V.01 and S YNSET.F ILM .N.01 as features as it can be both a verb and a noun. In addition to synsets, we include the hyponym (for ex. H YPO .C OLLAGE F ILM .N.01), hypernym (for ex. H YPER :S HEET.N.06) and holonym synset of the word as features. We also collect antonyms and pertainyms of all the words in a synset and include those as features in the linguistic vector. Connotation. Feng et al. (2013) construct a lexicon that contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories"
P15-2076,N09-1003,0,0.447409,"Missing"
P15-2076,N15-1004,0,0.0165613,"text (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove & LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors and on the NP-bracketing task they outperf"
P15-2076,D14-1012,0,0.069217,"Missing"
P15-2076,P98-1013,0,0.303539,"entiment analysis (Socher et al., 2013). Additionally, because they can be induced directly from unannotated corpora, they are likewise available in domains and languages where traditional linguistic resources do not exhaust. Intrinsic evaluations on various tasks are helping refine vector learning methods to discover representations that captures many facts about lexical semantics (Turney, 2001; Turney and Pantel, 2010). Yet induced word vectors do not look anything like the representations described in most lexical semantic theories, which focus on identifying classes of words (Levin, 1993; Baker et al., 1998; Schuler, 2005; Miller, 1995). Though expensive to construct, conceptualizing word meanings sym1 Our vectors can be downloaded at: https:// github.com/mfaruqui/non-distributional 464 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 464–469, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Lexicon WordNet Supersense FrameNet Emotion Connotation Color Part of Speech Syn. & Ant. Union Vocabulary 10,794 71,836 9,462 6,468 76,134 1"
P15-2076,J15-4004,0,0.129279,"Missing"
P15-2076,P14-2131,0,0.0285485,"Missing"
P15-2076,D13-1196,0,0.064685,"of the words. 3 Sentiment Analysis. Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as features in an `2 -regularized logistic regression for classification. The classifier is tuned on the dev set and accuracy is reported on the test set. NP-Bracketing. Lazaridou et al. (2013) constructed a dataset from the Penn TreeBank (Marcus et al., 1993) of noun phrases (NP) of length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit left and right bracketing respectively. We append the word vectors of the three words in the NP in order and use them as features in an `2 -regularized logistic regression classifier. The dataset contains 2,227 noun phrases split into 10 folds. The cl"
P15-2076,W06-1670,0,0.0208086,"t contains information about connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector."
P15-2076,J93-2004,0,0.0533649,"h envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS tags for every word in the corpus. We collect all the possible POS tags that a word is annotated with and use it as features in the linguistic vectors. For example, love has PTB.N OUN, 2 http://www.cs.cmu.edu/˜ytsvetko/ adj-supersenses.tar.gz 465 Word love hate ugly beauty refundable P OL .P OS 1 0 0 1 0 C OLOR .P INK 1 0 0 1 0 SS.N OUN .F EELING 1 1 0 0 0 PTB.V ERB 1 1 0 0 0 A NTO .FAIR 0 0 1 0 0 ··· C ON .N OUN .P OS 1 0 0 1 1 Table 2: Some linguistic word vectors. 1 indicates presence and"
P15-2076,tsvetkov-etal-2014-augmenting-english,1,0.527689,"nnotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource"
P15-2076,E06-1011,0,0.0220642,"Missing"
P15-2076,W11-0611,0,0.0191767,"ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C OLOR .R ED is a feature evoked by the word blood. FrameNet. FrameNet (Baker et al., 1998; Fillmore et al., 2003) is a rich linguistic resource that contains information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word Part of Speech Tags. The Penn Treebank (Marcus et al., 1993) annotates naturally occurring text for linguistic structure. It contains syntactic parse trees and POS t"
P15-2076,P14-2089,0,0.0147205,"pedia with vector length 300 and context window of 5 words. 3.3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using linguistic resources to enrich word vector representations. In these approaches, relational information among words obtained from WordNet, Freebase etc. is used as a constraint to encourage words with similar properties in lexical ontologies to have similar word vectors (Xu et al., 2014; Yu and Dredze, 2014; Bian et al., 2014; Fried and Duh, 2014; Faruqui et al., 2015a). Distributional representations have also been shown to improve by using experiential data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identif"
P15-2076,C12-1118,0,0.0326162,"al data in addition to distributional context (Andrews et al., 2009). We have shown that simple vector concatenation can likewise be used to improve representations (further confirming the established finding that lexical resources and cooccurrence information provide somewhat orthogonal information), but it is certain that more careful combination strategies can be used. Although distributional word vector dimensions cannot, in general, be identified with linguistic properties, it has been shown that some vector construction strategies yield dimensions that are relatively more interpretable (Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015; Faruqui et al., 2015b). However, such analysis is difficult to generalize across models of representation. In constrast to distributional word vectors, linguistic Results Table 3 shows the performance of different word vector types on the evaluation tasks. It can be seen that although Skip-Gram, Glove & LSA perform better than linguistic vectors on WS-353, the linguistic vectors outperform them by a huge margin on SimLex. Linguistic vectors also perform better at RG-65. On sentiment analysis, linguistic vectors are competitive with Skip-Gram vectors an"
P15-2076,I08-2105,0,0.0498544,"t connotation of words that are seemingly objective but often allude nuanced sentiment. They assign positive, negative and neutral connotations to these words. This lexicon differs from Mohammad and Turney (2013) in that it has a more subtle shade of sentiment and it extends to many more words. For example, delay has a negative connotation C ON .N OUN .N EG, floral has a positive connotation C ON .A DJ .P OS and outline has a neutral connotation C ON .V ERB .N EUT. Supsersenses. WordNet partitions nouns and verbs into semantic field categories known as supsersenses (Ciaramita and Altun, 2006; Nastase, 2008). For example, lioness evokes the supersense SS.N OUN .A NIMAL. These supersenses were further extended to adjectives (Tsvetkov et al., 2014).2 We use these supsersense tags for nouns, verbs and adjectives as features in the linguistic word vectors. Color. Most languages have expressions involving color, for example green with envy and grey with uncertainly are phrases used in English. The word-color associtation lexicon produced by Mohammad (2011) using crowdsourcing lists the colors that a word evokes in English. We use every color in this lexicon as a feature in the vector. For example, C O"
P15-2076,J08-4003,0,0.0366018,"Missing"
P15-2076,D14-1162,0,0.111111,"ic vectors comparable to publicly available distributional word vectors, we perform singular value decompostion (SVD) on the linguistic matrix to obtain word vectors of lower dimensionality. If L ∈ {0, 1}N ×D is the linguistic matrix with N word types and D linguistic features, then we can obtain U ∈ RN ×K from the SVD of L as follows: L = UΣV&gt; , with K being the desired length of the lower dimensional space. We compare both sparse and dense linguistic vectors to three widely used distributional word vector models. The first two are the pre-trained Skip-Gram (Mikolov et al., 2013)5 and Glove (Pennington et al., 2014)6 word vectors each of length 300, trained on 300 billion and 6 billion words respectively. We used latent semantic analysis (LSA) to obtain word vectors from the SVD decomposition of a word-word cooccurrence matrix (Turney and Pantel, 2010). These were trained on 1 billion words of Wikipedia with vector length 300 and context window of 5 words. 3.3 4 Discussion Linguistic resources like WordNet have found extensive applications in lexical semantics, for example, for word sense disambiguation, word similarity etc. (Resnik, 1995; Agirre et al., 2009). Recently there has been interest in using l"
P15-2076,W96-0213,0,0.17772,"Missing"
P15-2076,D13-1170,0,0.00443223,"Missing"
P15-2076,C98-1013,0,\N,Missing
P15-2076,P14-1046,0,\N,Missing
P16-1013,D13-1111,1,0.823343,"and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word e"
P16-1013,N07-1058,0,0.0202033,"1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this featur"
P16-1013,W06-1670,0,0.00983607,"t); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). According to the theory, more prototypical words are acquired earlier. We use lexical semantic databases to operationalize insights from the prototype theory in the following semantic features; the features are computed on token level and averaged over paragraphs: • Relative frequency in a supersense was computed by marginalizing the word frequencies in the training corpus over coarse semantic categories defined in the WordNet (Fellbaum, 1998; Ciaramita and Altun, 2006). There are 41 supersense types: 26 for nouns and 15 for verbs, e.g., NOUN . ANIMAL and VERB . MOTION . For example, in NOUN . ANIMAL the relative frequency of human is 0.06, of dog is 0.01, of bird is 0.01, of cattle is 0.009, and of bumblebee is 0.0002. • Relative frequency in a synset was calculated similarly to the previous feature category, but word frequencies were marginalized over WordNet synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, veteran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87. 3 Evaluation Benchmarks W"
P16-1013,P13-1004,0,0.0755168,"Missing"
P16-1013,W02-1001,0,0.118037,"ss curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for En"
P16-1013,P15-1033,1,0.791568,"scribed in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) for English on the universal dependencies v1.1 treebank (Agi´c et al., 2015) with the standard development and test splits, reporting unlabeled attachment scores (UAS) on the test data. We remove all part-of-speech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the pretrained embeddings. • Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embeddings models, each model"
P16-1013,N16-1030,1,0.480643,"40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and spirituality as 1.07. 2 http://http://people.sutd.edu.sg/ ~yue_zhang/doc 133 # paragraphs 2,532,361 et al., 2016). The `2 -regularized logistic regression classifier is tuned on the development set and accuracy is reported on the test set. # tokens 100,872,713 # types 156,663 Table 1: Training data sizes. Named Entity Recognition (NER). Named entity recognition is the task of identifying proper names in a sentence, such as names of persons, locations etc. We use the recently proposed LSTMCRF NER model (Lample et al., 2016) which trains a forward-backward LSTM on a given sequence of words (represented as word vectors), the hidden units of which are then used as (the only) features in a CRF model (Lafferty et al., 2001) to predict the output label sequence. We use the CoNLL 2003 English NER dataset (Tjong Kim Sang and De Meulder, 2003) to train our models and present results on the test set. Setup. 100-dimensional word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013).3 All training data was used, either shuffled or ordered by a curriculum. As described in §3,"
P16-1013,D13-1170,0,0.00455636,"most prototypical word, scoring 0.87. 3 Evaluation Benchmarks We evaluate the utility of the pretrained word embeddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features: • Age of acquisition (AoA) of words was extracted from the crowd-sourced database, containing over 50 thousand English words (Kuperman et al., 2012). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vector for classification (Faruqui et al., 2015; Sedoc • Concreteness ratings on the scale of 1–5 (1 is most abstract) for 40 thousand English lemmas (Brysbaert et al., 2014). For example, cookie is rated as 5, and sp"
P16-1013,N10-1116,0,0.528653,"g♣ Brian MacWhinney♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu, lingwang@google.com, macw@cmu.edu Abstract complexity (Bengio et al., 2009; Spitkovsky et al., 2010). In language modeling, this preference for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabulary from frequent to less frequent words (Bengio et al., 2009). In unsupervised grammar induction, an effective curriculum comes from increasing length of training sentences as training progresses (Spitkovsky et al., 2010). These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning (Kail, 1990; Skinner, 1938). Had different heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum using Bayesian optimization. A curriculum is defined to be the ordering of the training inst"
P16-1013,J93-2004,0,0.0565311,"ithout additional features. All models were learned under same conditions, across curricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, respectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors. Part of Speech Tagging (POS). For POS tagging, we again use the LSTM-CRF model (Lample et al., 2016), but instead of predicting the named entity tag for every word in a sentence, we train the tagger to predict the POS tag of the word. The tagger is trained and evaluated with the standard Penn TreeBank (PTB) (Marcus et al., 1993) training, development and test set splits as described in Collins (2002). Experiments. In all the experiments we first train word embedding models, then the word embeddings are used as features in four extrinsic tasks (§3). We tune the tasks on development data, and report results on the test data. The only component that varies across the experiments is order of paragraphs in the training corpus—the curriculum. We compare the following experimental setups: Dependency Parsing (Parse). Dependency parsing is the task of identifying syntactic relations between the words of a sentence. For depend"
P16-1013,D08-1020,0,0.0314538,"SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that"
P16-1013,D13-1100,0,0.0250457,"Missing"
P16-1013,P14-1024,1,0.818283,"s the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the paragraph. • Average sentence length • Verb-token ratio • Noun-token ratio • Parse tree depth •"
P16-1013,W12-2019,0,0.0531361,"Missing"
P16-1013,P05-1065,0,0.0298158,"P • Quadratic entropy (Rao, 1982):1 i,j dij pi pj SIMPLICITY . Spitkovsky et al. (2010) have validated the utility of syntactic simplicity in curriculum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit without prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are inspired by prior research in second language acquisition, text simplification, and readability assessment (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Pitler and Nenkova, 2008; Vajjala and Distributional and Linguistic Features To characterize and quantify a curriculum, we define three categories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 In"
P16-1013,D15-1251,0,0.0243606,"l., 2012, GP), providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators (Bergstra et al., 2011, TPE), tailored to handle conditional spaces. Choices of the acquisition functions include probability of improvement (Kushner, 1964), expected improvement (EI) (Moˇckus et al., 1978; Jones, 2001), GP upper confidence bound (Srinivas et al., 2010), Thompson sampling (Thompson, 1933), entropy search (Hennig and Schuler, 2012), and dynamic combinations of the above functions (Hoffman et al., 2011); see Shahriari et al. (2016) for an extensive comparison. Yogatama et al. (2015) found that the combination of EI as the acquisition function and TPE as the surrogate model performed favorably in Bayesian optimization of text representations; we follow this choice in our model. 2.2 are used in many contrasting fields, from ecology and biology (Rosenzweig, 1995; Magurran, 2013), to economics and social studies (Stirling, 2007). Diversity has been shown effective in related research on curriculum learning in language modeling, vision, and multimedia analysis (Bengio et al., 2009; Jiang et al., 2014). Let pi and pj correspond to empirical frequencies of word types ti and tj"
P16-1013,J11-1005,0,0.033983,"ories of features, focusing on various distributional, syntactic, and semantic aspects of training data. We now detail the feature categories along with motivations for feature selection. DIVERSITY . Diversity measures capture the distributions of types in data. Entropy is the bestknown measure of diversity in statistical research, but there are many others (Tang et al., 2006; Gimpel et al., 2013). Common measures of diversity 1 Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words. 132 Meurers, 2012). We use an off-the-shelf syntactic parser2 (Zhang and Clark, 2011) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntactic complexity of training paragraphs: • Imageability ratings are taken from the MRC psycholinguistic database (Wilson, 1988). Following Tsvetkov et al. (2014), we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embeddings as features in an `2 -regularized logistic regression classifier. • Language model score • Character language model score • Conventionalization features count the number of “conventional” words and phrases in a p"
P16-1157,W13-3520,0,0.0130875,"ector representations in a unified framework, and conduct experiments to compare the performance of existing 1668 models in a unbiased manner. We chose existing cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al.,"
P16-1157,P13-1133,0,0.0310622,"Missing"
P16-1157,W06-2920,0,0.0208404,"ce-side embeddings as lexical features. These embeddings can be replaced by target-side embeddings at test time. All models are trained for 5000 iterations with fixed word embeddings during training. Since our goal is to determine the utility of word embeddings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in Guo et al. (2015). We use the universal dependency treebank (McDonald et al., 2013) version2.0 for our evaluation. For Chinese, we use the treebank released as part of the CoNLL-X shared task (Buchholz and Marsi, 2006). We first evaluate how useful the word embeddings are in cross-lingual model transfer of dependency parsers (Table 6). On an average, BiCCA does better than other models. BiSkip is a close second, with an average performance gap of less than 1 point. BiSkip outperforms BiCVM on German and French (over 2 point improvement), owing to word alignment information BiSkip’s model uses during training. It is not surprising that English-Chinese transfer scores are low, due to the significant difference in syntactic structure of the 10 github.com/jiangfeng1124/ acl15-clnndep l1 l2 BiSkip BiCVM BiCCA Bi"
P16-1157,D15-1131,0,0.0393742,"ng cross-lingual word vector models that can be trained on two languages at a given time. In recent work, Ammar et al. (2016) train multilingual word vectors using more than two languages; our comparison does not cover this setting. It is also worth noting that we compare here different crosslingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013). The paper does not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.11 However, the models that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of crosslingual supervision required by them. For example, BilBOWA (Gouws et al., 2015) and crosslingual Auto-encoder (Chandar et al., 2014) are similar to BiCVM in this respect. Multi-view CCA (Rastogi et al., 2015) and deep CCA (Lu et al.,"
P16-1157,N13-1073,1,0.749116,"gs of size 200. We provide all models with parallel corpora, irrespective of their requirements. Whenever possible, we also report statistical significance of our results. 3 www.statmt.org/europarl/v7/{de, sv}-en.tgz 4 www.statmt.org/wmt15/ translation-task.html Parameter Selection BiSkip. All models were trained using a window size of 10 (tuned over {5, 10, 20}), and 30 negative samples (tuned over {10, 20, 30}). The cross-lingual weight was set to 4 (tuned over {1, 2, 4, 8}). The word alignments for training the model (available at github. com/lmthang/bivec) were generated using fast_align (Dyer et al., 2013). The number of training iterations was set to 5 (no tuning) and we set α = 1 and β = 1 (no tuning). BiCVM. We use the tool (available at github. com/karlmoritz/bicvm) released by Hermann and Blunsom (2014) to train all embeddings. We P train an additive model (that is, f (~x) = g(~x) = i xi ) with hinge loss margin set to 200 (no tuning), batch size of 50 (tuned over 50, 100, 1000) and noise parameter of 10 (tuned over {10, 20, 30}). All models are trained for 100 iterations (no tuning). BiCCA. First, monolingual word vectors are trained using the skip-gram model5 with negative sampling (Miko"
P16-1157,E14-1049,1,0.924174,"ity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while"
P16-1157,N15-1157,0,0.0143038,", 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different"
P16-1157,P15-1119,0,0.498815,"els often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is"
P16-1157,J15-4004,0,0.129398,"Missing"
P16-1157,C12-1089,0,0.159635,"r different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014;"
P16-1157,2005.mtsummit-papers.11,0,0.0208035,"sh-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document clas"
P16-1157,N15-1028,0,0.060572,"do-bilingual document and P (t |s) ∝ exp(tT s). Note that t, s ∈ W ∪ V . Although BiVCD is designed to use comparable corpus, we provide it with parallel data in our experiments (to ensure comparability), and treat two aligned sentences as comparable. 3 Data We train cross-lingual embeddings for 4 language pairs: English-German (en-de), English-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the 1663 l1 l2 #sent #l1 -words #l2 -words en de fr sv zh 1.9 2.0 1.7 2.0 53 55 46 58 51 61 42 50 4.1 We follow the BestAvg parameter selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh"
P16-1157,W15-1521,0,0.671634,", inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word level (Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), while some require both sentence and word alignments (Luong et al., 2015). However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.1 First, we show that different models can be viewed as instances of a more general framework for inducing"
P16-1157,N15-1058,0,0.0609469,"Missing"
P16-1157,D15-1036,0,0.0547762,"Missing"
P16-1157,P15-1165,0,0.0457057,"Missing"
P16-1157,N12-1052,0,0.0186424,"Missing"
P16-1157,I05-3027,0,0.050151,"er selection strategy from Lu et al. (2015): we selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks. Table 1: The size of parallel corpora (in millions) of different language pairs used for training cross-lingual word vectors. Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1. 4 Evaluation We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: • monolingual word similarity for English • Cross-lingual dictionary induction • Cross-lingual document classification • Cross-lingual syntactic dependency parsing The first two tasks intrinsically measure how much can monolingual and cross-lingual similarity benefit from cross-lingual training. The last two tasks measure the ability of cross"
P16-1157,D15-1243,1,0.603503,"e respective English side of each language is shown in column marked Mono. In all cases (except BiCCA on ensv), the bilingually trained vectors achieve better scores than the mono-lingually trained vectors. Overall, across all language pairs, BiCVM is the best performing model in terms of Spearman’s correlation, but its improvement over BiSkip and BiVCD is often insignificant. It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned corpus only, which are easier to obtain than parallel data with word alignments required by BiSkip. Q VEC. Tsvetkov et al. (2015) proposed an intrinsic evaluation metric for estimating the quality of English word vectors. The score produced by Q VEC measures how well a given set of word vectors is able to quantify linguistic properties 6 We implemented the code for performing the merging as we could not find a tool provided by the authors. en-de en-fr en-sv en-zh 0.29 0.30 0.28 0.28 0.34 0.35 0.32 0.34 0.37 0.39 0.34 0.39 0.30 0.31 0.27 0.30 0.32 0.36 0.32 0.31 avg. 0.29 0.34 0.37 0.30 0.33 Table 2: Word similarity score measured in Spearman’s correlation ratio for English on SimLex-999. The best score for each language"
P16-1157,N13-1011,0,0.0375489,"Missing"
P16-1157,D13-1168,0,0.0341321,"Missing"
P16-1157,P15-2118,0,0.408453,"Missing"
P16-1157,D13-1141,0,0.630951,"ing intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks. 1 Introduction Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vuli´c and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014, inter alia), with improvements observed both on monolingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and cross-lingual tasks (Guo et al., 2015; Søgaard et al., 2015; Guo et al., 2016). Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision – some can use document-level alignments (Vuli´c and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015"
P16-1157,D15-1245,0,\N,Missing
P19-1483,W00-1401,0,0.21157,"e an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated tab"
P19-1483,H05-1042,0,0.0465407,"Missing"
P19-1483,E06-1040,0,0.0647752,"ema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours i"
P19-1483,W07-0718,0,0.171507,"Missing"
P19-1483,E06-1032,0,0.111485,"al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show an"
P19-1483,W17-3209,0,0.0568619,"Missing"
P19-1483,D16-1128,0,0.611103,"the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has lar"
P19-1483,P18-1060,0,0.0664973,"Missing"
P19-1483,W14-3346,0,0.0160021,"f the longest common subsequence between x and y. The LCS function, borrowed from ROUGE, ensures that entity names in r¯k appear in the same order in the text as the table. Higher values of Er (T i ) denote that more records are likely to be mentioned in Gi . The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the re"
P19-1483,P09-1011,0,0.345045,"Missing"
P19-1483,W04-1013,0,0.218157,"et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, including BLEU, correlate"
P19-1483,W14-3348,0,0.355805,"led precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the reference against the table is high, it already covers most of the information, and we can assign it a high weight in Eq. 4. This leads to a separate value of λ automatically set for each instance.7  is set to 10−5 for all experiments. 4 Evaluation via Information Extractio"
P19-1483,P17-1017,0,0.371266,"al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Th"
P19-1483,W05-1208,0,0.0215623,"recision Epn for n-grams of order n is given by: Epn = P g∈Gin P   / Rni )w(g) #Gin (g) P r(g ∈ Rni ) + P r(g ∈ P , g∈Gin #Gin (g) g∈Gin = #Gin (g)w(g) + #Gin ,Rni (g)[1 − w(g)] P . g∈Gin #Gin (g) (2) In words, an n-gram receives a reward of 1 if it appears in the reference, with probability P r(g ∈ Rni ), and otherwise it receives a reward of w(g). Both numerator and denominator are weighted by the count of the n-gram in Gin . P r(g ∈ Rni ) rewards an n-gram for appearing as many times as it appears in the reference, not more. We combine precisions for n-gram orders 1-4 using a geometric 5 Glickman and Dagan (2005) used a product instead of geometric mean. Here we use a geometric mean to ensure that n-grams of different lengths have comparable probabilities of being entailed. 6 It is unlikely that an automated system produces the same extra n-gram as present in the reference, thus a match with the reference n-gram is considered positive. For example, in Figure 1, it is highly unlikely that a system would produce “Silkworm” when it is not present in the table. 4886 model M is the average of instance level PARENT scores across the evaluation set: average, similar to BLEU: Ep = exp 4 X 1 n=1 4 ! log Epn N"
P19-1483,D14-1020,0,0.0226453,"an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table 2. The distribution of correlations for the best performing metrics are shown in Figure 3. Table 2 also indicates whether PARENT is significantly better than a baseline metric. Graham and Baldwin (2014) suggest using the William’s test for this purpose, but since we are computing correlations between only 4/13 systems at a time, this test has very weak power in our case. Hence, we use the bootstrap samples to obtain a 1 − α confidence interval of the difference in correlation 4889 1.0 WikiBio-Systems 0.8 1.0 WikiBio-Hyperparams 0.8 0.8 0.6 0.6 0.4 0.6 0.2 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W Figure 3: Distribution of metric correlations across 500 bootstrap samples. PRT = PARENT. between PARENT and any other metric and check whether this is ab"
P19-1483,D17-1274,0,0.041252,"Missing"
P19-1483,P83-1022,0,0.360705,"gh a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system."
P19-1483,D10-1090,0,0.0335412,"Bio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic met"
P19-1483,D16-1230,0,0.0372707,"nder the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks t"
P19-1483,D18-1429,0,0.0249933,"relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006"
P19-1483,D17-1238,0,0.267569,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,W17-5525,0,0.311156,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,P02-1040,0,0.106059,"7; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, inc"
P19-1483,D14-1162,0,0.0825959,"Missing"
P19-1483,J18-3002,0,0.0906513,"PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first st"
P19-1483,D18-1437,0,0.0608232,"curring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks the quality of metrics when tableto-text references are divergent. We show that in this case even system level correlations can be unreliable. Hallucination (Rohrbach et al., 2018; Lee et al., 2018) refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table. PARENT draws inspiration from iBLEU (Sun and Zhou, 2012), a metric for evaluating paraphrase generation, which compares the generated text to both the source text and the reference. Conclusions We study the automatic evaluation of table-to-text systems"
P19-1483,P12-2008,0,0.369925,"erences were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio. 5.3 Compared Metrics Text only: We compare BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr and CIDErD (Vedantam et al., 2015) using their publicly available implementations. Information Extraction based: We compare the CS, RG and RG-F metrics discussed in §4. Text & Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEUT draws inspiration from iBLEU (Sun and Zhou, 2012) but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment. We also compare versions where a single λ is tuned on the entire dataset to maximize correlation with human judgments, denoted as PARENT*-W/C. WikiBio Systems WikiBio Hyperparams Avg ROUGE CIDEr CIDEr-D METEOR BLEU 0.518±0.07C,W 0.674±0.06C,W 0.646±0.06C,W 0.697±0.06C,W 0.548±0.07C,W -0.585±0.15C,W -0.516±0.15C,W -0.372±0.16C,W -0.079±0.24C,W 0.407±0.15C,W -0.034 0.079 0.137 0.309 0.478 C"
P19-1483,N18-1136,0,0.0306942,"Missing"
P19-1483,D17-1239,0,0.530722,"red data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et"
P19-1483,Q16-1029,0,0.0311333,"ally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Bu"
P19-1483,E17-1019,0,\N,Missing
P19-1483,P17-1099,0,\N,Missing
Q16-1001,E14-1060,0,0.0554739,"Missing"
Q16-1001,N15-1107,0,0.0870818,"Missing"
Q16-1001,banea-etal-2008-bootstrapping,0,0.0765869,"Missing"
Q16-1001,D12-1133,0,0.0746248,"Missing"
Q16-1001,W10-0701,0,0.100991,"Missing"
Q16-1001,E03-1009,0,0.309131,"onship between the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters."
Q16-1001,P11-1061,0,0.0856553,"Missing"
Q16-1001,N12-1086,0,0.0512554,"Missing"
Q16-1001,de-marneffe-etal-2014-universal,0,0.0615903,"Missing"
Q16-1001,P07-1116,0,0.0877182,"Missing"
Q16-1001,Y09-1013,0,0.0251906,"ges; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-synt"
Q16-1001,C12-2026,0,0.0604854,"Missing"
Q16-1001,D11-1057,0,0.196897,"Missing"
Q16-1001,dukes-habash-2010-morphological,0,0.0178039,"we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green a"
Q16-1001,N13-1138,0,0.0977876,"Missing"
Q16-1001,dzeroski-etal-2000-morphosyntactic,0,0.143082,"Missing"
Q16-1001,erjavec-2004-multext,0,0.140248,"Missing"
Q16-1001,D13-1105,0,0.0544315,"Missing"
Q16-1001,N16-1077,1,0.886147,"Missing"
Q16-1001,P13-1057,0,0.036681,"Missing"
Q16-1001,gimenez-marquez-2004-svmtool,0,0.125427,"Missing"
Q16-1001,E09-1038,0,0.0329114,"l. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised"
Q16-1001,P12-1016,0,0.0160868,", 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of w"
Q16-1001,P98-1080,0,0.316876,"Missing"
Q16-1001,A00-2013,0,0.230514,"Missing"
Q16-1001,huet-etal-2008-morphosyntactic,0,0.0707153,"Missing"
Q16-1001,W10-0717,0,0.0783632,"Missing"
Q16-1001,kokkinakis-etal-2000-annotating,0,0.00855555,"language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Min"
Q16-1001,P08-1068,0,0.0642064,"n the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was intro"
Q16-1001,P13-2017,1,0.875506,"Missing"
Q16-1001,N13-1090,0,0.0160739,"Missing"
Q16-1001,P07-1017,0,0.0315038,"000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes"
Q16-1001,D09-1063,0,0.090751,"Missing"
Q16-1001,D15-1151,0,0.0159257,"ailable lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon"
Q16-1001,N15-1055,0,0.0503508,"Missing"
Q16-1001,D13-1032,0,0.0729096,"Missing"
Q16-1001,W11-1107,0,0.0735884,"Missing"
Q16-1001,Q15-1012,0,0.0930734,"Missing"
Q16-1001,N15-1093,0,0.0487045,"Missing"
Q16-1001,J04-2003,0,0.0247049,"(Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth refe"
Q16-1001,A94-1024,0,0.492659,"Missing"
Q16-1001,N13-1039,0,0.0320114,"Missing"
Q16-1001,W11-4644,0,0.0722929,"Missing"
Q16-1001,N09-1024,0,0.219471,"Missing"
Q16-1001,W96-0213,0,0.533254,"nd Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing denotes gerund verb forms like, studying, playing and -ed denotes past tense like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge featur"
Q16-1001,W13-5005,0,0.0554608,"Missing"
Q16-1001,simov-etal-2004-language,0,0.109415,"Missing"
Q16-1001,H05-1060,0,0.0500174,"e), with seed lexicon (Seed), with propagated lexicon (Propagation). (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998). The model we use is a standard atomic sequence classifier, that classifies the morphological bundle for each word independent of the others (with the exception of features derived from these words). Specifically, we use a linear SVM model classifier with hand tuned features. This is similar to commonly used analyzers like SVMTagger (Gim´enez and Marquez, 2004) and MateTagger (Bohnet and Nivre, 2012). Our taggers are trained in a language independent manner (Hajiˇc, 2000; Smith et al., 2005; M¨uller et al., 2013). The list of features used in training the tagger are listed in Table 6. In addition to the standard features, we use the morpho-syntactic attributes present in the lexicon for every word as features in the tagger. As shown in M¨uller and Schuetze (2015), this is typically the most important feature for morphological tagging, even more useful than clusters or word embeddings. While predicting the contextual morphological tags for a given word, the morphological attributes present in the lexicon for the current word, the previous word and the next word are used as featur"
Q16-1001,P08-1084,0,0.263349,"Missing"
Q16-1001,N15-1186,1,0.766624,"like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge features.2 We introduce an edge between two words sharing a particular suffix or prefix feature. Morphological Transformations. Soricut and Och (2015) presented an unsupervised method of inducing prefix- and suffix-based morphological transformations between words using word embeddings. In their method, statistically, most of the transformations are induced between words with the same lemma (without using any prior information about the word lemma). For example, their method induces the transformation between played and playing as suffix:ed:ing. This feature indicates T ENSE :PAST to turn off and T ENSE :P RES to turn on.3 We train the morphological transformation prediction tool of Soricut and Och (2015) on the news corpus (same as the one"
Q16-1001,D08-1114,0,0.279809,"Missing"
Q16-1001,D10-1017,0,0.0938659,"Missing"
Q16-1001,N12-1052,1,0.899187,"Missing"
Q16-1001,N07-1037,0,0.0803348,"Missing"
Q16-1001,P10-1149,0,0.0821283,"Missing"
Q16-1001,W02-1028,0,0.213563,"Missing"
Q16-1001,tron-etal-2006-morphdb,0,0.086576,"Missing"
Q16-1001,P10-1040,0,0.0756671,"f the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word"
Q16-1001,P08-1086,0,0.0458253,"ubgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing"
Q16-1001,N10-1119,1,0.867886,"Missing"
Q16-1001,W04-0109,0,0.0792441,"Missing"
Q16-1001,P00-1027,0,0.070682,"Missing"
Q16-1001,P11-2033,0,0.119186,"Missing"
Q16-1001,C98-1077,0,\N,Missing
S18-1155,P10-1089,0,0.0137,"stical and knowledge-driven semantics. This classification depends on whether the assumption is that human knowledge is encapsulated in language man947 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 947–952 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Dataset Training Validation Test comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval (Tandon and De Melo, 2010; Klein and Nelson, 2009), lexical disambiguation (Bergsma et al., 2009), improving general purpose NLP classifiers (Bergsma et al., 2010), and improving parsing performance (Pitler et al., 2010). Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (Speer et al., 2017), ImageNet (Russakovsky et al., 2015), WordNet (Miller and Fellbaum, 1998), Wiktionary, Open Mind Common Sense (Singh et al., 2002) and DBpedia (Mendes et al., 2012). In this work we follow a statistical based approach and show the strengths and weakness of the distributional semantics of the word vectors and ngram frequency counts in capturing the different types of"
S18-1155,N16-2002,0,0.012595,"with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedness (Islam et al., 2012), This paper describes our system submission to the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Given two concepts and an attribute, the task is to determine whether the attribute is semantically related to one concept and not the other. In this work we assume that discri"
S18-1155,N13-1090,0,0.119332,"al application to the Firthian dictum “You shall know a word by the company it keeps” (Firth, 1957) which has become commonsense wisdom in lexical semantics. Features of the statistical model are extracted from unstructured data, such as words embeddings, n-gram counts, or directly from raw data. The basic idea with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedn"
S18-1155,D14-1162,0,0.0768369,"in lexical semantics. Features of the statistical model are extracted from unstructured data, such as words embeddings, n-gram counts, or directly from raw data. The basic idea with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedness (Islam et al., 2012), This paper describes our system submission to the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Giv"
S18-1155,C10-1100,0,0.0133334,"depends on whether the assumption is that human knowledge is encapsulated in language man947 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 947–952 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Dataset Training Validation Test comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval (Tandon and De Melo, 2010; Klein and Nelson, 2009), lexical disambiguation (Bergsma et al., 2009), improving general purpose NLP classifiers (Bergsma et al., 2010), and improving parsing performance (Pitler et al., 2010). Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (Speer et al., 2017), ImageNet (Russakovsky et al., 2015), WordNet (Miller and Fellbaum, 1998), Wiktionary, Open Mind Common Sense (Singh et al., 2002) and DBpedia (Mendes et al., 2012). In this work we follow a statistical based approach and show the strengths and weakness of the distributional semantics of the word vectors and ngram frequency counts in capturing the different types of discriminative attributes. 2 # of triples 17,547 2,722 2,"
S18-1155,S18-1117,0,0.0412099,"encoded. The two resources are the Google n-gram counts and the Google News Word2Vec. Google n-gram counts. We use the Google 5-gram counts as provided by Google Books ngrams1 (Michel et al., 2011; Lin et al., 2012). Google News Word2Vec. This is a publicly available pre-trained word vector2 , built with the word2vec architecture (Mikolov et al., 2013b) from a news corpus of 100B words (3M vocabulary entries) with 300 dimensions, negative sampling, using continuous bag of words and window size of 5. Task and Data Description The goal of the shared task on Capturing Discriminative Attributes (Krebs et al., 2018) is to detect semantic difference between pairs of concepts, or in other words, determine whether a semantic property differentiates between two possibly related concepts. For example both ‘bear’ and ‘goat’ are animals, but only a ‘bear’ has ‘claws’. Therefore ‘claws’ is considered as a discriminative feature. The shared task data is formatted in triples that represent a ternary relation between two concepts (Word1 , Word2 ) on one hand and an attribute (Word3 ) on the other. Word3 is considered as a discriminative attribute if, and only if, it characterizes Word1 but not Word2 . For example,"
S18-1155,P12-3029,0,0.0122596,"butes. The basic idea with deep learning is to use hidden layers of neural nets to automatically capture the underlying factors that lead from the input to the output, eliminating the need for feature engineering. The system is trained on features extracted from two main publicly available resources that fall within the paradigm of unstructured data as no manual lexical or encyclopedic knowledge is encoded. The two resources are the Google n-gram counts and the Google News Word2Vec. Google n-gram counts. We use the Google 5-gram counts as provided by Google Books ngrams1 (Michel et al., 2011; Lin et al., 2012). Google News Word2Vec. This is a publicly available pre-trained word vector2 , built with the word2vec architecture (Mikolov et al., 2013b) from a news corpus of 100B words (3M vocabulary entries) with 300 dimensions, negative sampling, using continuous bag of words and window size of 5. Task and Data Description The goal of the shared task on Capturing Discriminative Attributes (Krebs et al., 2018) is to detect semantic difference between pairs of concepts, or in other words, determine whether a semantic property differentiates between two possibly related concepts. For example both ‘bear’ a"
S18-1155,mendes-etal-2012-dbpedia,0,0.0548022,"Missing"
tsvetkov-etal-2014-augmenting-english,W10-0719,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,D08-1027,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W06-1670,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,H93-1061,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,J12-3005,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P06-2072,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P14-1024,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,P12-2050,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,N13-1132,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,I08-2105,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,peters-peters-2000-treatment,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,E14-1049,1,\N,Missing
W11-0111,N03-1003,0,0.13299,"Missing"
W11-0111,W05-1209,0,0.163444,"Missing"
W11-0111,E06-1028,0,0.0221759,"rom some corpora is serious exactly because, according to our intuition, the “easier” news agency corpora (like Reuters) are domain102 Corpus Reuters StuttZ D( · |deWac) 0.98 0.93 Die Zeit 0.64 words w with highest P (w)/Q(w) Händler (trader), Börse (exchange), Prozent (per cent), erklärte (stated) DM (German Mark), Prozent (per cent), Millionen (millions), Geschäftsjahr (fiscal year), Milliarden (billions) heißt (means), weiß (knows), läßt (leaves/lets) Table 8: Exp. 2: Domain specificity (KL distance from deWac); typical content words specific. We quantify this intuition with an approach by Ciaramita and Baroni (2006), who propose to model the representativeness of web-crawled corpora as the KL divergence between their Laplacesmoothed unigram distribution P and that of a reference corpus, Q (w ∈ W are vocabulary words): D(P, Q) = X P (w) log w∈W P (w) Q(w) (4) We use the deWac German web corpus (Baroni et al., 2009) as reference, making the idealizing assumption that it is representative for the German language. We interpret a large distance from deWac as domain specificity. The results in Table 8 bear out our hypothesis: Die Zeit is less domain specific than StuttZ, which in turn is less specific than Reu"
W11-0111,P08-1118,0,0.0667294,"Missing"
W11-0111,P10-1122,0,0.0123552,"n and Machine Translation often adopt very different, task-specific semantic processing strategies. Textual entailment (TE) was introduced by Dagan et al. (2006) as a “meta-task” that can subsume a large part of the semantic processing requirements of such applications by providing a generic concept of inference that corresponds to “common sense” reasoning patterns. Textual Entailment is defined as a relation between two natural language utterances (a Premise P and a Hypothesis H) that holds if “a human reading P would infer that H is most likely true”. See, e.g., the ACL “challenge paper” by Sammons et al. (2010) for further details. The successive TE workshops that have taken place yearly since 2005 have produced annotation for English which amount to a total of several thousand entailing Premise-Hypothesis sentence pairs, which we will call entailment pairs: (1) P: Swedish bond yields end 21 basis points higher. H: Swedish bond yields rose further. From the machine learning perspective assumed by many approaches to TE, this is a very small number of examples, given the complex nature of entailment. Given the problems of manual annotation, therefore, Burger and Ferro (2005) proposed to take advantage"
W11-0111,W07-1401,0,\N,Missing
W11-3605,P04-1036,0,0.0132822,"the Google Translate output by replacing them with the English word sharing the same Soundex code that has the highest frequency in the English document collection. 2.4 Disambiguation (GTR+SoExNER+LD(mod)) Generally, a word that has been wrongly transliterated from Urdu maps onto the same Soundex code as several English words. The median number of English words per transliteration is 7. This can be seen as a sort of ambiguity, and the strategy adopted by the previous models is to just choose the most frequent candidate, similar to the “predominant” sense baseline in word sense disambiguation (McCarthy et al., 2004). We found however that the most frequent candidate is often wrong, since Soundex conflates fairly different words (cf. Section 2.2). For example, Subhas, the first name of an Indian freedom fighter, receives the soundex code S120 but it is mapped onto the English term Space (f req=7243) instead of Subhas (f req=2853). We therefore experimented with a more informed strategy that chooses the English candidate based on two variants of Levenshtein distance. The first model, GTR+SoExNER+LD, uses standard Levenshtein distance with a cost of 1 for 1 http://translate.google.com. All queries were tran"
W11-3605,W02-1201,0,0.0157249,"ies in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1 An analysis of the output of the GTR+SoEx mo"
W11-3605,N10-1077,0,0.0318057,"ts in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1 An analysis of the output of the GTR+SoEx model showed that the model indeed ensured that a"
W11-3605,W04-2905,0,0.0757525,"Missing"
W11-3605,E09-1079,0,0.0159523,"search is the study of systems that accept queries in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led us to develop a hierarchy of increasingly sophisticated strategies. 2.1"
W11-3605,I08-7017,0,0.0238511,"roduction ð ð Cross-language information retrieval (CLIR) research is the study of systems that accept queries in one language and return text documents in a different language. CLIR is of considerable practical importance in countries with many languages like India. One of the most widely used languages is Urdu, the official language of five Indian states as well as the national language of Pakistan. There are around 60 million speakers of Urdu – 48 million in India and 11 million in Pakistan (Lewis, 2009). Despite this large number of speakers, NLP for Urdu is still at a fairly early stage (Hussain, 2008). Studies have been conducted on POS tagging (Sajjad and Schmid, 2009), corpus construction (Becker and Riaz, 2002), word segmentation (Durrani and Hussain, 2010), lexicographic 2 Translation Strategies for Urdu–English We present a series of strategies for translating Urdu queries into English so that they can be pre25 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 25–29, Chiang Mai, Thailand, November 8-12, 2011. 2.3 sented to a monolingual English IR system that works on some English document collection. Inspection of the strategies’ errors led u"
W11-3605,C04-1137,0,0.0617671,"Missing"
W11-3605,I08-6010,0,\N,Missing
W11-3605,J98-4003,0,\N,Missing
W13-1736,J92-4003,0,0.123486,"Missing"
W13-1736,U07-1006,0,0.0307986,"fy that language. This task has a clear empirical motivation. Nonnative speakers make different errors when they write English, depending on their native language (Lado, 1957; Swan and Smith, 2001); understanding the different types of errors is a prerequisite for correcting them (Leacock et al., 2010), and systems such as the one we describe here can shed interesting light on such errors. Tutoring applications can use our system to identify the native language of students and offer better-targeted advice. Forensic linguistic applications are sometimes required to determine the L1 of authors (Estival et al., 2007b; Estival et al., 2007a). Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language. We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted. Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into “s"
W13-1736,C90-2036,0,0.0232636,"rb and Masuda, 2008). Since English’s orthography is largely phonemic—even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse l and r relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector (Kernighan, 1990) to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay. Restored tags We focus on three important token classes defined above: punctuation marks, function words and cohesive verbs. We first remove words in these classes from the texts, and then recover the most likely hidden tokens in a sequence of words, according to an n-gram language model"
W13-1736,P08-1068,0,0.0252399,"tions). To restore hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-clu"
W13-1736,P11-1132,1,0.750772,"e markers These are 40 function words (and short phrases) that have a strong discourse function in texts (however, because, in fact, etc.). Translators tend to spell out implicit utterances and render them explicitly in the target text (Blum-Kulka, 1986). We use the list of Volansky et al. (forthcoming). Cohesive verbs This is a list of manually compiled verbs that are used, like cohesive markers, to spell out implicit utterances (indicate, imply, contain, etc.). Function words Frequent tokens, which are mostly function words, have been used successfully for various text classification tasks. Koppel and Ordan (2011) define a list of 400 such words, of which we only use 100 (using the entire list was not significantly different). Note that pronouns are included in this list. Contextual function words To further capitalize on the ability of function words to discriminate, we define pairs consisting of a function word from the list mentioned above, along with the POS tag of its adjacent word. This feature captures patterns such as verbs and the preposition or particle immediately to their right, or nouns and the determiner that precedes them. We also define 3-grams consisting of one or two function words an"
W13-1736,N13-1039,1,0.79567,"Missing"
W13-1736,W13-1706,0,0.102937,"Missing"
W13-1736,N03-1033,0,0.010714,"etreault et al., 2013). The training data consists of 1000 essays from each native language. The essays are short, consisting of 10 to 20 sentences each. We used the provided splits of 900 documents for training and 100 for development. Each document is annotated with the author’s English proficiency level (low, medium, high) and an identification (1 to 8) of the essay prompt. All essays are tokenized and split into sentences. In table 1 we provide some statistics on the training corpora, listed by the authors’ proficiency level. All essays were tagged with the Stanford part-of-speech tagger (Toutanova et al., 2003). We did not parse the dataset. # Documents # Tokens # Types Low 1,069 245,130 13,110 Medium 5,366 1,819,407 37,393 High 3,456 1,388,260 28,329 Table 1: Training set statistics. 4 Model For our classification model we used the creg regression modeling framework to train a 11-class logistic regression classifier.1 We parameterize the classifier as a multiclass logistic regression: P exp j λ j h j (x, y) pλ (y |x) = , Zλ (x) where x are documents, h j (·) are real-valued feature functions of the document being classified, λ j are the corresponding weights, and y is one of the eleven L1 class lab"
W13-1736,W07-0602,0,0.263634,"Missing"
W13-1736,P10-1040,0,0.0107663,"hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-cluster http://www.ark.c"
W13-1736,U09-1008,0,0.0944702,"Missing"
W13-1736,D11-1148,0,0.0757912,"Missing"
W13-2307,2020.lrec-1.643,0,0.293721,"Missing"
W13-2307,P99-1010,0,0.418476,"fficiently mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Bo"
W13-2307,N06-1019,0,0.236476,"mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Both annotations and analys"
W13-2307,W07-2416,0,0.0563019,"ator labels are as in table 2. Per-annotator com (with lexical reconciliation) and inter-annotator softComPrec are aggregated over sentences by arithmetic mean. less burdensome, and the specialized annotations did prove complementary to each other.19 5.4 Treebank Comparison Though the annotators in our study were native speakers well acquainted with representations of English syntax, we sought to quantify their agreement with the expert treebankers who created the EWTB (the source of the Reviews sentences). We converted the EWTB’s constituent parses to dependencies via the PennConverter tool (Johansson and Nugues, 2007),20 then removed punctuation. Agreement with the converted treebank parses appears in the bottom two rows of table 3. Because the EWTB commits to a single analysis, precision scores are quite lopsided. Most of its attachments are consistent with our annotations (softComPrec &gt; 0.9), but these allow many additional analyses (hence the scores below 0.5). Annotator Specialization As an experiment in using underspecification for labor division, two of the annotators of Reviews data were assigned specific linguistic phenomena to focus on. Annotator “D” was tasked with the internal structure of base"
W13-2307,P06-2066,0,0.0538705,"nd compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced language"
W13-2307,W12-1706,0,0.033779,"When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced languages (to name two examples). Traditional syntactic annotation projects like the Penn Treebank (Marcus ∗ 2 A Dependency Grammar for Annotation Although depende"
W13-2307,J93-2004,0,0.0444543,"portion of the English Web Treebank 14 Malagasy is a VOS Austronesian language spoken by 15 million people, mostly in Madagascar. Kinyarwanda is an SVO Bantu language spoken by 12 million people mostly in Rwanda. All annotations were done by native speakers of English. The Kinyarwanda and Malagasy annotators had basic proficiency in these languages. 15 As a point of comparison, during the Penn Treebank project, annotators corrected the syntactic bracketings produced by a high-quality hand-written parser (Fidditch) and achieved a rate of only 375 tokens/hour using a specialized GUI interface (Marcus et al., 1993). 16 Included with the data and software release (footnote 1). 57 com thus reduces the commitment averages for each annotation—to a greater extent for annotator “A” (.96 in table 2 vs. .82 in table 3) because “A” marked more multiwords. An analysis fully compatible with both annotations exists for only 27/60 sentences; the finer-grained softComPrec measure (§4.2), however, offers insight into the balance between commitment and agreement. Qualitatively, we observe three leading causes of incompatibilities (disagreements): obvious annotator mistakes (such as the marked as a head); inconsistent h"
W13-2307,1993.iwpt-1.22,0,0.0410599,"lop algorithms to evaluate and compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and"
W13-2307,D07-1014,1,0.853574,"edges that are known to be incompatible with the annotation before searching for spanning trees. Our “upward-downward” method for constructing a graph of supported edges first enumerates a set of candidate top nodes for every fudge expression, then uses that information to infer a set of supported parents for every node.12 The supported edge graph then consists of vertices lexnodes(A) ∪ {root} and edges S 0 0 v∈lexnodes(A) {(v → v ) ∀ v ∈ suppParentsA (v)}. From this graph we can count all directed spanning trees in cubic time using Kirchhoff’s matrix tree theorem (Chaiken and Kleitman, 1978; Smith and Smith, 2007; Margoliash, 2010).13 If some lexical node has no supported parents, this reflects conflicting constraints in the annotation, and no spanning tree will be found. Promiscuity will tend to be higher for longer sentences. To control for this, we define a second quantity, the annotation’s commitment quotient (commitment being the opposite of promiscuity), 4.2 Inter-Annotator Agreement FUDG can encode flat groupings and coreference at the lexical level, as well as syntactic structure over lexical items. Inter-annotator agreement can be measured separately for each of these facets. Pilot annotator"
W13-2307,D08-1027,1,0.400347,"Missing"
W13-2307,N13-1039,1,0.775169,"Missing"
W13-2307,W13-2307,1,0.0512826,"Missing"
W16-2506,N09-1003,0,0.38845,"Missing"
W16-2506,W14-1618,0,0.0531837,"ty datasets. In this paper, we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence"
W16-2506,D14-1034,0,0.0955207,"Missing"
W16-2506,Q15-1016,0,0.0805488,", we give a comprehensive analysis of the problems that are associated with the evaluation of word vector representations using word similarity tasks.1 We survey existing literature to construct a list of such problems and also summarize existing solutions to some of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are calle"
W16-2506,D12-1091,0,0.0128401,"Missing"
W16-2506,P12-1015,0,0.0446647,"solutions (if available) to address them. 2.1 Semantic or task-specific similarity? Subjectivity of the task The notion of word similarity is subjective and is often confused with relatedness. For example, cup, and coffee are related to each other, but not similar. Coffee refers to a plant (a living organism) or a hot brown drink, whereas cup is a manmade object, which contains liquids, often coffee. Nevertheless, cup and coffee are rated more similar than pairs such as car and train in WS-353 (Finkelstein et al., 2002). Such anomalies are also found in recently constructed datasets like MEN (Bruni et al., 2012). Thus, such datasets unfairly penalize word vector models that capture the fact that cup and coffee are dissimilar. 2.3 No standardized splits & overfitting To obtain generalizable machine learning models, it is necessary to make sure that they do not overfit to a given dataset. Thus, the datasets are usually partitioned into a training, development and test set on which the model is trained, tuned and finally evaluated, respectively (Manning and Schütze, 1999). Existing word similarity datasets are not partitioned into training, development and test sets. Therefore, optimizing the word vecto"
W16-2506,N15-1028,0,0.0179048,"y extend to the word analogy tasks. 31 itly tunes on the test set and overfits the vectors to the task. On the other hand, if researchers decide to perform their own splits of the data, the results obtained across different studies can be incomparable. Furthermore, the average number of word pairs in the word similarity datasets is small (≈ 781, cf. Table 2), and partitioning them further into smaller subsets may produce unstable results. We now present some of the solutions suggested by previous work to avoid overfitting of word vectors to word similarity tasks. Faruqui and Dyer (2014b), and Lu et al. (2015) evaluate the word embeddings exclusively on word similarity and word analogy tasks. Faruqui and Dyer (2014b) tune their embedding on one word similarity task and evaluate them on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the"
W16-2506,P11-2031,1,0.115822,"ded each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation ratio of each model with the gold ranking. This problem was solved by Rastogi et al. (2015), which we describe next. Rastogi et al. (2015) observed that the impro"
W16-2506,W13-3512,0,0.0510995,"r refine this hubness problem to show that there exists a power-law relationship between the frequency-rank5 of a word and the frequency-rank of its neighbors. Specifically, they showed that the average rank of the 1000 nearest neighbors of a word follows: nn-rank ≈ 1000 · word-rank0.17 Inability to account for polysemy (3) 3 This shows that pairs of words which have similar frequency will be closer in the vector-space, thus showing higher word similarity than they should according to their word meaning. Even though newer datasets of word similarity sample words from different frequency bins (Luong et al., 2013; Hill et al., 2014), this still does not solve the problem that cosine similarity in the vector-space gets polluted by frequency-based effects. Different distance normalization schemes have been proposed to downplay the frequency/hubness effect when computing nearest neighbors in the vector space (Dinu et al., 2014; Tomašev et al., 2011), Conclusion In this paper we have identified problems associated with word similarity evaluation of word vector models, and reviewed existing solutions wherever possible. Our study suggests that the use of word similarity tasks for evaluation of word vectors"
W16-2506,P14-5004,1,0.41903,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,N13-1090,0,0.0600247,"of the problems. Our findings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even"
W16-2506,E14-1049,1,0.384784,"h (1965) who constructed a list of 65 word pairs with annotations of human similarity judgment. They created this dataset to validate the veracity of the distributional hypothesis (Harris, 1954) according to which the meaning of words is evidenced by the context they occur in. They found a positive correlation between contextual similarity and human-annotated similarity of word pairs. Since then, the lack of a standard evaluation method for word vectors has led to the creation of several ad hoc word similarity datasets. Table 2 provides a list of such benchmarks obtained from wordvectors.org (Faruqui and Dyer, 2014a). Introduction Despite the ubiquity of word vector representations in NLP, there is no consensus in the community on what is the best way for evaluating word vectors. The most popular intrinsic evaluation task is the word similarity evaluation. In word similarity evaluation, a list of pairs of words along with their similarity rating (as judged by human annotators) is provided. The task is to measure how well the notion of word similarity according to humans is captured by the word vector representations. Table 1 shows some word pairs along with their similarity judgments from WS-353 (Finkel"
W16-2506,D14-1113,0,0.023745,"texts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of freque"
W16-2506,D14-1162,0,0.104113,"indings suggest that word similarity tasks are not appropriate for evaluating word vector representations, and call for further research on better evaluation methods 2 2.2 Distributional word vector models capture some aspect of word co-occurrence statistics of the words in a language (Levy and Goldberg, 2014b; Levy et al., 2015). Therefore, to the extent these models produce semantically coherent representations, it can be seen as evidence of the distributional hypothesis of Harris (1954). Thus, word embeddings like Skip-gram, CBOW, Glove, LSA (Turney and Pantel, 2010; Mikolov et al., 2013a; Pennington et al., 2014) which are trained on word co-occurrence counts can be expected to capture semantic word similarity, and hence can be evaluated on word similarity tasks. Word vector representations which are trained as part of a neural network to solve a particular task (apart from word co-occurrence prediction) are called distributed word embeddings (Collobert and Weston, 2008), and they are task-specific in nature. These embeddings capture task-specific word similarity, for example, if the task is of POS tagging, two nouns cat and man might be considered similar by the model, even though they are not semant"
W16-2506,J15-4004,0,0.101034,"Missing"
W16-2506,N15-1058,1,0.07718,"Missing"
W16-2506,N10-1013,0,0.0397994,"mpute similarity between two words given the contexts they occur in. For example, the words bank and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by se"
W16-2506,P15-1173,0,0.0267298,"k and money should have a low similarity score given the contexts: “along the east bank of the river”, and “the basis of all money laundering”. Using cues from the word’s context, the correct word-sense can be identified and the appropriate word vector can be used. Unfortunately, word senses are also ignored by majority of the frequently used word vector models like Skip-gram and Glove. However, there has been progress on obtaining multiple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through t"
W16-2506,D15-1036,0,0.739851,"n with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothesis of no difference has"
W16-2506,W14-1601,0,0.0150773,"electing the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferences (Clark et al., 2011). The problem of statistical significance in word similarity evaluation was first systematically addressed by Shalaby and Zadrozny (2015), who used Steiger’s test (Steiger, 1980)4 to compute how significant the difference between rankings produced by two different models is against the gold ranking. However, their method needs explicit ranked list of words produced by the models and cannot work when provided only with the correlation"
W16-2506,P15-1124,0,0.00829903,"on all other tasks. This ensures that their vectors are being evaluated on held-out datasets. Lu et al. (2015) propose to directly evaluate the generalization of a model by measuring the performance of a single model on a large gamut of tasks. This evaluation can be performed in two different ways: (1) choose the hyperparameters with best average performance across all tasks, (2) choose the hyperparameters that beat the baseline vectors on most tasks.2 By selecting the hyperparameters that perform well across a range of tasks, these methods ensure that the obtained vectors are generalizable. Stratos et al. (2015) divided each word similarity dataset individually into tuning and test set and reported results on the test set. 2.4 to evaluation. 2.5 Absence of statistical significance There has been a consistent omission of statistical significance for measuring the difference in performance of two vector models on word similarity tasks. Statistical significance testing is important for validating metric gains in NLP (BergKirkpatrick et al., 2012; Søgaard et al., 2014), specifically while solving non-convex objectives where results obtained due to optimizer instability can often lead to incorrect inferen"
W16-2506,D15-1243,1,0.825403,"llowing: Low correlation with extrinsic evaluation Word similarity evaluation measures how well the notion of word similarity according to humans is captured in the vector-space word representations. Word vectors that can capture word similarity might be expected to perform well on tasks that require a notion of explicit semantic similarity between words like paraphrasing, entailment. However, it has been shown that no strong correlation is found between the performance of word vectors on word similarity and extrinsic evaluation NLP tasks like text classification, parsing, sentiment analysis (Tsvetkov et al., 2015; Schnabel et al., 2015).3 An absence of strong correlation between the word similarity evaluation and downstream tasks calls for alternative approaches (rAB &lt; r)∧(|ˆ rBT −ˆ rAT |&lt; σpr0 ) =⇒ pval > p0 (2) Here pval is the probability of the test statistic under the null hypothesis that rAT = rBT found using the Steiger’s test. The above conditional ensures that if the empirical difference between the rank correlations of the scores of the competing methods to the gold ratings is less than σpr0 then either the true correlation between the competing methods is greater than r, or the null hypothe"
W16-2506,P10-1040,0,0.060644,"iple vectors per word-type to account for different word-senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Jauhar et al., 2015; Rothe and Schütze, 2015). Frequency effects in cosine similarity The most common method of measuring the similarity between two words in the vector-space is to compute the cosine similarity between the corresponding word vectors. Cosine similarity implicitly measures the similarity between two unitlength vectors (eq. 1). This prevents any biases in favor of frequent words which are longer as they are updated more often during training (Turian et al., 2010). Ideally, if the geometry of embedding space is primarily driven by semantics, the relatively small number of frequent words should be evenly distributed through the space, while large number of rare words should cluster around related, but more frequent words. However, it has been shown that vector-spaces contain hubs, which are vectors that are close to a large number of other vectors in the space (Radovanovi´c et al., 2010). This problem manifests in word vector-spaces in the form of words that have high cosine similarity with a large number of other words (Dinu et al., 2014). Schnabel et"
W16-2506,P12-1092,0,\N,Missing
W16-2506,N15-1070,1,\N,Missing
W16-2520,P14-2131,0,0.0265954,"ks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with extrinsic scores is"
W16-2520,P12-1015,0,0.0445806,"ector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extri"
W16-2520,P15-1033,1,0.816876,"in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the linguistic matrix which already contains POS-annotated columns does not affect correlations of QV"
W16-2520,P14-5004,1,0.866932,"Missing"
W16-2520,N15-1184,1,0.835181,"vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaph"
W16-2520,N16-1030,1,0.460377,"h are a concatenation of the semantic and syntactic matrices described in §3 for words that occur in both matrices; this setup is denoted as PTB + SST. WS-353 MEN SimLex PTB PTB + SST QVEC QVEC - CCA QVEC QVEC - CCA POS -0.38 -0.32 0.20 0.23 0.23 0.28 0.23 Parse 0.68 0.51 -0.21 0.39 0.50 0.37 0.63 Table 4: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream syntactic tasks. We extend the setup of Tsvetkov et al. (2015) with two syntactic benchmarks, and evaluate QVEC - CCA with the syntactic matrix. The first task is POS tagging; we use the LSTM-CRF model (Lample et al., 2016), and the second is dependency parsing (Parse), using the stack-LSTM model of Dyer et al. (2015). Although some word similarity tasks obtain high correlations with syntactic applications, these results are inconsistent, and vary from a high negative to a high positive correlation. Conversely, QVEC and QVEC - CCA consistently obtain moderate-to-high positive correlations with the downstream tasks. Comparing performance of QVEC - CCA in PTB and PTB + SST setups sheds light on the importance of linguistic signals captured by the linguistic matrices. Appending supersense-annotated columns to the l"
W16-2520,D13-1196,0,0.0549103,"range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation"
W16-2520,D15-1161,1,0.815675,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,N15-1142,1,0.831815,"43 0.02 ··· ··· ··· ··· PTB . JJ In table 3, we show correlations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our sem"
W16-2520,J93-2004,0,0.0550028,"then describe QVEC - CCA. Both QVEC and QVEC - CCA rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. Linguistic resources are invaluable as they capture generalizations made by domain experts. However, resource construction is expensive, therefore it is not always possible to find an existing resource that captures exactly the set of optimal lexical properties for a downstream task. Resources that capture more coarse-grained, general properties can be used instead, for example, WordNet for semantic evaluation (Fellbaum, 1998), or Penn Treebank (Marcus et al., 1993, PTB) for syntactic evaluation. Since these properties are not an exact match to the task, the intrinsic evaluation tests for a necessary (but possibly not sufficient) set of generalizations. QVEC . The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N . To quantify the semantic content of embeddings, a semantic/syntactic linguistic matrix S ∈ RP ×N is constructed from a semantic/syntac"
W16-2520,H93-1061,0,0.0541212,"the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two matrices. Specifically, let A ∈ {0, 1}D×P be a matrix of alignments such that aij = 1 iff xi is aligned to sj , otherwise aij = 0. If r(xi , sj ) is the Pearson’s correlation between vectors xi and sj , then QVEC is defined as: QVEC = A: max P j X X S X aij ≤1 Linguistic Dimension Word Vectors Semantic vectors. To evaluate the semantic content of word vectors, Tsvetkov et al. (2015) exploit supersense annotations in a WordNetannotated corpus—SemCor (Miller et al., 1993). The resulting supersense-dimension matrix has 4,199 rows (supersense-annotated nouns and verbs that occur in SemCor at least 5 times2 ), and 41 columns: 26 for nouns and 15 for verbs. Example vectors are shown in table 1. r(xi , sj ) × aij WORD fish duck chicken i=1 j=1 NN . ANIMAL 0.68 0.31 0.33 NN . FOOD 0.16 0.00 0.67 ··· ··· ··· VB . MOTION 0.00 0.69 0.00 P The constraint j aij ≤ 1, warrants that one distributional dimension is aligned to at most one linguistic dimension. Table 1: Linguistic dimension word vector matrix with semantic vectors, constructed using SemCor. QVEC - CCA . To mea"
W16-2520,D14-1162,0,0.0959606,"relations between intrinsic scores (word similarity/QVEC/QVEC - CCA) and extrinsic scores across semantic benchmarks for 300-dimensional vectors. QVEC - CCA obtains high positive correlation with all the semantic tasks, and outperforms QVEC on two tasks. 0.00 0.00 0.41 Table 2: Linguistic dimension word vector matrix with syntactic vectors, constructed using PTB. • We first train 21 word vector models: variants of CBOW and Skip-Gram models (Mikolov et al., 2013); their modifications CWindow, Structured Skip-Gram, and CBOW with Attention (Ling et al., 2015b; Ling et al., 2015a); GloVe vectors (Pennington et al., 2014); Latent Semantic Analysis (LSA) based vectors (Church and Hanks, 1990); and retrofitted GloVe and LSA vectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the"
W16-2520,D13-1170,0,0.035608,"tic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally efficient intrinsic evaluation that correlates with"
W16-2520,P14-1024,1,0.80936,"ectors (Faruqui et al., 2015). • We then evaluate these word vector models using existing intrinsic evaluation methods: QVEC and the proposed QVEC - CCA , and also word similarity tasks using the WordSim353 dataset (Finkelstein et al., 2001, WS-353), MEN dataset (Bruni et al., 2012), and SimLex999 dataset (Hill et al., 2014, SimLex).3 • In addition, the same vectors are evaluated using extrinsic text classification tasks. Our semantic benchmarks are four binary categorization tasks from the 20 Newsgroups (20NG); sentiment analysis task (Socher et al., 2013, Senti); and the metaphor detection (Tsvetkov et al., 2014, Metaphor). • Finally, we compute the Pearson’s correlation coefficient r to quantify the linear relationship between the intrinsic and extrinsic scorings. The higher the correlation, the better suited the intrinsic evaluation to be used as a proxy to the extrinsic task. WS-353 MEN SimLex QVEC QVEC - CCA 20NG 0.55 0.76 0.56 0.74 0.77 Metaphor 0.25 0.49 0.44 0.75 0.73 Senti 0.46 0.55 0.51 0.88 0.93 Table 3: Pearson’s correlations between word similarity/QVEC/QVEC - CCA scores and the downstream text classification tasks. In table 4, we evaluate QVEC and QVEC - CCA on syntactic benchmarks. We f"
W16-2520,D15-1243,1,0.582936,"ations Yulia Tsvetkov♠ Manaal Faruqui♠ Chris Dyer♣♠ ♠ Carnegie Mellon University ♣ Google DeepMind {ytsvetko,mfaruqui,cdyer}@cs.cmu.edu Abstract dimensions is an auxiliary mechanism for analyzing how these properties affect the target downstream task. It thus facilitates refinement of word vector models and, consequently, improvement of the target task. Finally, an intrinsic evaluation that approximates a range of related downstream tasks (e.g., semantic text-classification tasks) allows to assess generality (or specificity) of a word vector model, without actually implementing all the tasks. Tsvetkov et al. (2015) proposed an evaluation measure—QVEC—that was shown to correlate well with downstream semantic tasks. Additionally, it helps shed new light on how vector spaces encode meaning thus facilitating the interpretation of word vectors. The crux of the method is to correlate distributional word vectors with linguistic word vectors constructed from rich linguistic resources, annotated by domain experts. Q VEC can easily be adjusted to specific downstream tasks (e.g., part-of-speech tagging) by selecting task-specific linguistic resources (e.g., part-of-speech annotations). However, QVEC suffers from t"
W16-2520,P10-1040,0,0.0694995,"ffective proxy for a range of extrinsic semantic and syntactic tasks. We also show that the proposed evaluation obtains higher and more consistent correlations with downstream tasks, compared to existing approaches to intrinsic evaluation of word vectors that are based on word similarity. 1 Introduction Being linguistically opaque, vector-space representations of words—word embeddings—have limited practical value as standalone items. They are effective, however, in representing meaning— through individual dimensions and combinations of thereof—when used as features in downstream applications (Turian et al., 2010; Lazaridou et al., 2013; Socher et al., 2013; Bansal et al., 2014; Guo et al., 2014, inter alia). Thus, unless it is coupled with an extrinsic task, intrinsic evaluation of word vectors has little value in itself. The main purpose of an intrinsic evaluation is to serve as a proxy for the downstream task the embeddings are tailored for. This paper advocates a novel approach to constructing such a proxy. What are the desired properties of an intrinsic evaluation measure of word embeddings? First, retraining models that use word embeddings as features is often expensive. A computationally effici"
W16-2520,J90-1003,0,\N,Missing
W16-2520,D14-1012,0,\N,Missing
