2020.acl-main.468,S18-2005,0,0.0969809,"Missing"
2020.acl-main.468,W19-3823,0,0.295616,"central metric: D(Y, Yˆ |A) = 2(log(p(Y |A)) − log(p(Yˆ |A))) where p(Y |A) is the specified ideal distribution (either derived empirically or theoretically) and p(Yˆ |A) is the distribution within the data. For error disparity the ideal distribution is always the Uniform and Yˆ is replaced with the error. KL divergence (DKL [P (Yˆ |A)P (Y |A)]) can be used as a secondary, more scalable alternative. Our measure above attempts to synthesize metrics others have used in works focused on specific biases. For example, the definition of outcome disparity is analogous to that used for semantic bias. Kurita et al. (2019) quantify bias in embeddings as the difference in log probability score when replacing words suspected to carry semantic differences (‘he’, ‘she’) with a mask: log(P ([M ask] = “hP RON i”|[M ask] is “hN OU N i”)) − log(P ([M ask] = “hP RON i”|[M ask] is [M ask]))) hN OU N i is replaced with a specific noun to check for semantic bias (e.g., an occupation), and hP RON i is an associated demographic word (e.g., “he” or “she”). 3 Four Origins of Bias But what leads to an outcome disparity or error disparity? We identify four points within the standard supervised NLP pipeline where bias may origina"
2020.acl-main.468,P18-2005,0,0.15638,"ork differently than one for teenagers). Still, undetected and unaddressed, biases can lead to negative consequences: There are aggregate effects for demographic groups, which combine to produce predictive bias. I.e., the label distribution of a predictive model reflects a human attribute in a way that diverges from a theoretically defined “ideal distribution.” For example, a Part Of Speech (POS) tagger reflecting how an older generation uses words (Hovy and Søgaard, 2015) diverges from the population as a whole. A variety of papers have begun to address countermeasures for predictive biases (Li et al., 2018; Elazar and Goldberg, 2018; Coavoux et al., 2018).1 Each identifies a specific bias and counter1 An even more extensive body of work on fairness exists as part of the FAT* conferences, which goes beyond the scope origin consequence over-amplification label bias The model discriminates on a given human attribute beyond its source base-rate. Biased annotations, interaction, or latent bias from past classifications. Embedding Corpus Source Population features features ?embedding Xsource (Pre-trained Side) fit outcome disparity The distribution of outcomes, given attribute A, is dissimilar than t"
2020.acl-main.468,D17-1119,1,0.928159,"ather than specifying a specific distribution (e.g., moving toward a uniform distribution of 3 We have substituted “test"" with “predictive model”. “Attributes” include both continuously valued user-level variables, like age, personality on a 7-point scale, etc. (also referred to as “dimensional” or “factors”), and discrete categories like membership in an ethnic group. Psychological research suggests that people are better represented by continuously valued scores, where possible, than discrete categories (Baumeister et al., 2007; Widiger and Samuel, 2005; McCrae and Costa Jr., 1989). In NLP, Lynn et al. (2017) shows benefits from treating user-level attributes as continuously when integrating into NLP models. 4 pronouns associated with computer science). Our framework should enable its users to apply evolving standards and norms across NLP’s many application contexts. A prototypical example of outcome disparity is gender disparity in image captions. Zhao et al. (2017) and Hendricks et al. (2018) demonstrate a systematic difference with respect to gender in the outcome of the model, Yˆ even when taking the source distribution as an ideal target distribution: Q(Yˆtarget |gender)  Q(Ytarget |gender)"
2020.acl-main.468,W14-2702,0,0.10424,"arital status, and income), they use poststratification to reduce the bias (some of these methods cross into addressing selection bias). Selection bias. The primary source for selection bias is the mismatch between the sample distribution and the ideal distribution. Consequently, any countermeasures need to re-align the two distributions to minimize this mismatch. The easiest way to address the mismatch is to re-stratify the data to more closely match the ideal distribution. However, this often involves downsampling an overly represented class, which reduces the number of available instances. Mohammady and Culotta (2014) use a stratified sampling technique to reduce the selection bias in the data. Almeida et al. (2015) use demographic user attributes, including age, gender, and social status, to predict the election results in six different cities of Brazil. They use stratified sampling on all the resulting groups to reduce selection bias. Rather than re-sampling, others use reweighting or poststratifying to reduce selection bias. Culotta (2014) estimates county-level health statistics based on social media data. He shows we can stratify based on external socio-demographic data about a community’s composition"
2020.acl-main.468,Q14-1025,0,0.0170513,"iew. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that method. Other approaches have explored harnessing the inherent disagreement among annotators to guide the training process (Plank et al., 2014). By weighting updates by the amount of disagreement on the labels, this method prevents bias towards any one label. The weighted updates act as a regularizer during training, which might also help prevent overamplification. If annotators behave in predictab"
2020.acl-main.468,Q18-1040,1,0.860763,"proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that method. Other approaches have explored harnessing the inherent disagreement among annotators to guide the training process (Plank et al., 2014). By weighting updates by the amount of disagreement on the labels, this method prevents bias towards any one label. The weighted updates act as a regularizer during training, which might also help prevent overamplification. If annotators behave in predictable ways to produce a"
2020.acl-main.468,Q14-1007,0,0.0110563,"ajafian (2019) extend such measures. Similarly, Romanov et al. (2019) define bias based on the correlation between the embeddings of human attributes with the difference in the True Positive rates between human traits. This approach is reflective of an error disparity. Our framework encompasses bias-related work in the social sciences. Please see the supplement in A.1 for a brief overview. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al., 2014). Disagreement between annotators has long been an active research area in NLP, with various approaches to measure and quantify disagreement through interannotator agreement (IAA) scores to remove outliers (Artstein and Poesio, 2008). Lately, there has been more of an emphasis on embracing variation through the use of Bayesian annotation models (Hovy et al., 2013; Passonneau and Carpenter, 2014; Paun et al., 2018). These models arrive at a much less biased estimate for the final label than majority voting, by attaching confidence scores to each annotator, and reweighting them through that meth"
2020.acl-main.468,E14-1078,1,0.846636,"ng or testing (selection bias), (3) the representation of data (semantic bias), or (4) due to the fit method itself (overamplification). Label Bias Label bias emerges when the distribution of the dependent variable in the data source diverges substantially from the ideal distribution: Q(Ys |As )  P (Ys |As ) Here, the labels themselves are erroneous concerning the demographic attribute of interest (as compared to the source distribution). Sometimes, this bias is due to a non-representative group of annotators (Joseph et al., 2017). In other cases, it may be due to a lack of domain expertise (Plank et al., 2014), or due to preconceived notions and stereotypes held by the annotators (Sap et al., 2019). Selection bias. Selection bias emerges due to non-representative observations. I.e., when the users generating the training (source) observations differ from the user distribution of the target, where the model will be applied. Selection bias (sometimes also referred to as sample bias) has long been a concern in the social sciences. At this point, testing for such a bias is a fundamental consideration in study design (Berk, 1983; Culotta, 2014). Non-representative data is the origin for selection bias."
2020.acl-main.468,W15-1203,1,0.889201,"Missing"
2020.acl-main.468,W15-1205,1,0.899795,"Missing"
2020.acl-main.468,N18-2002,0,0.110534,"Missing"
2020.acl-main.468,P19-1163,0,0.24012,"[predictive model] cannot be called biased without reference to a specific prediction situation; thus, the same instrument may be biased in one application, but unbiased in another."" A prototypical example of error disparity is the “Wall Street Journal Effect” – a systematic difference in error as a function of demographics, first documented by Hovy and Søgaard (2015). In theory, POS tagging errors increase the further an author’s demographic attributes differ from the average WSJ author of the 1980s and 1990s (on whom many POS taggers were trained – a selection bias, discussed next). Work by Sap et al. (2019) shows error disparity from a different origin, namely unfairness in hate speech detection. They find that annotators for hate speech on social media make more mistakes on posts of black individuals. Contrary to the case above, the disparity is not necessarily due to a difference between author and annotator population (a selection bias). Instead, the label disparity stems from annotators failing to account for the authors’ racial background and sociolinguistic norms. Source and Target Populations. An important assumption of our framework is that disparities are dependent on the population for"
2020.acl-main.468,P19-1164,0,0.107748,"d instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more male than the original authors. Several authors have suggested it is essential for language to be understood within the context of the author and their social environment Jurgens (2013); Danescu-Niculescu-Mizil et al. (2013); Hovy (2018); Yang et al. (2019). Considering the author demographics improves the accuracy of text classifiersVolkova et al. (2013);"
2020.acl-main.468,P19-1159,0,0.172512,"bias often do not apply to another. As a consequence, much work has focused on bias effects and symptoms rather than their origins. While it is essential to address the effects of bias, it can leave the fundamental origin unchanged (Gonen and Goldberg, 2019), requiring researchers to rediscover the issue over and over. The “bias” discussed in one paper may, therefore, be quite different than that in another.2 A shared definition and framework of predictive bias can unify these efforts, provide a common terminology, help to identify underlying causes, and allow coordination of countermeasures (Sun et al., 2019). However, such a general framework had yet to be proposed within the NLP community. To address these problems, we suggest a joint conceptual framework, depicted in Figure 1, outlining and relating the different origins of bias. We base our framework on an extensive survey of the relevant NLP literature, informed by selected works in social science and adjacent fields. We identify four distinct sources of bias: selection bias, label bias, model overamplification, and semantic bias. We can express all of these as differences between (a) a “true” or intended distribution (e.g., over users, label"
2020.acl-main.468,P19-1162,0,0.0288519,"the causal effect of a variable on the outcome. E.g., when the causal risk ratio (CRR) differs from associational risk ratio (ARR). Similarly, Baker et al. (2013) define bias as uncontrolled covariates or “disturbing variables” that are related to measures of interest. Others provide definitions restricted to particular applications. For example, Caliskan et al. (2017) propose the Word-Embedding Association Test (WEAT). It quantifies semantic bias based on the distance between words with demographic associations in the embedding space. The previously mentioned work by Kurita et al. (2019) and Sweeney and Najafian (2019) extend such measures. Similarly, Romanov et al. (2019) define bias based on the correlation between the embeddings of human attributes with the difference in the True Positive rates between human traits. This approach is reflective of an error disparity. Our framework encompasses bias-related work in the social sciences. Please see the supplement in A.1 for a brief overview. 4 Countermeasures We group proposed countermeasures based on the origin(s) on which they act. Label Bias. There are several ways to address label bias, typically by controlling for biases of the annotators (Pavlick et al."
2020.acl-main.468,D13-1187,0,0.102789,"Missing"
2020.acl-main.468,Q18-1042,0,0.0156524,"fication. In its simplest form, overamplification of inherent bias by the model can be corrected by downweighting the biased instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more male than the original authors. Several authors have suggested it is essential for language to be understood within the context of the author and their social environment Jurgens (2013); Danescu-Niculescu-Mizil et al. (2013); Hovy (2"
2020.acl-main.468,D17-1323,0,0.0473255,"group. Psychological research suggests that people are better represented by continuously valued scores, where possible, than discrete categories (Baumeister et al., 2007; Widiger and Samuel, 2005; McCrae and Costa Jr., 1989). In NLP, Lynn et al. (2017) shows benefits from treating user-level attributes as continuously when integrating into NLP models. 4 pronouns associated with computer science). Our framework should enable its users to apply evolving standards and norms across NLP’s many application contexts. A prototypical example of outcome disparity is gender disparity in image captions. Zhao et al. (2017) and Hendricks et al. (2018) demonstrate a systematic difference with respect to gender in the outcome of the model, Yˆ even when taking the source distribution as an ideal target distribution: Q(Yˆtarget |gender)  Q(Ytarget |gender) ∼ Q(Ysource |gender). As a result, captions overpredict females in images with ovens and males in images with snowboards. Error disparity. We say there is an error disparity when model predictions have larger error for individuals with a given user attribute (or range of attributes in the case of continuously-valued attributes). Formally, the error of a predicted"
2020.acl-main.468,N18-2003,0,0.0612627,"hey show that it also protects user privacy. The findings from Elazar and Goldberg (2018), however, suggest that even with adversarial training, internal representations still retain traces of demographic information. Overamplification. In its simplest form, overamplification of inherent bias by the model can be corrected by downweighting the biased instances in the sample, to discourage the model from exaggerating the effects. A common approach involves using synthetic matched distributions. To address gender bias in neural network approaches to coreference resolution Rudinger et al. (2018); Zhao et al. (2018) suggest matching the label distributions in the data, and training the model on the new data set. They swap male and female instances and merge them with the original data set for training. In the same vein, Webster et al. (2018) provide a genderbalanced training corpus for coreference resolution. Based on the first two corpora, Stanovsky et al. (2019) introduce a bias evaluation for machine translation, showing that most systems overamplify gender bias (see also Prates et al. (2018)). Hovy et al. (2020) show that this overamplification consistently makes translations sound older and more mal"
2020.acl-main.468,P19-1161,0,0.0378486,"et al. (2004), propose Directed Acyclic graphs for various heterogeneous types of selection bias, and suggest using stratified sampling, regression adjustment, or inverse probability weighting to avoid the bias in the data. Zagheni and Weber (2015), study the use of Internet Data for demographic studies and propose two approaches to reduce the selection bias in their task. If the ground truth is available, they adjust selection bias based on the calibration of a stochastic microsimulation. If unavailable, they suggest using a difference-in-differences technique to find out trends on the Web. Zmigrod et al. (2019) show that gender-based selection bias could be addressed by data augmentation, i.e., by adding slightly altered examples to the data. This addition addresses selection bias originating in the features (Xsource ), so that the model is fit on a more gender-representative sample. Their approach is similar to the reweighting of poll data based on demographics, which can be applied more directly to tweet-based population surveillance (see our last case study, A.2). Li et al. (2018) introduce a model-based countermeasure. They use an adversarial multitasklearning setup to model demographic attribut"
2020.acl-main.472,N19-1423,0,0.26312,"el representation. DAN + Attn. Identical to the DAN variant except takes the weighted (rather than unweighted) average using learned attention weights. Sequence Network (SN). Similar to our proposed model but using the final state of each GRU, rather than word or message attention. Transformer (TN). This variant of our proposed model uses a two-layer transformer (Vaswani et al., 2017) with double-headed attention, rather than a GRU, at the message or word level. BERT. Whereas our proposed model learns message-level representations, we instead experiment with using pre-trained BERT embeddings (Devlin et al., 2019) as our message representations. These 768-dimension message embeddings are produced by averaging across all BERT token embeddings for each message (Matero et al., 2019). 5.3 Training All models were implemented using PyTorch (Paszke et al., 2017), with the exception of Ridge Regression which used scikit-learn (Pedregosa et al., 2011). One model was trained for each of the five personality dimensions. All deep learning models use two feed-forward layers with 512 hidden units each, followed by a final prediction layer. The GRU layers have a hidden size of 200 to match the number of embedding di"
2020.acl-main.472,W18-1112,0,0.517689,"Missing"
2020.acl-main.472,P15-1162,0,0.0567864,"Missing"
2020.acl-main.472,W18-0604,1,0.733624,"at the BERT-based models outperform our proposed model in 3 out of 5 cases. Table 3 compares our proposed model against the state-of-the-art. Unsurprisingly, Ridge (Embeddings) is the worst-performing model overall. Although Park et al. (2015) also used ridge Attention for Personality Prediction Table 1 compares the performance of our proposed model, SN+Attn, against variations using different architectures to aggregate from the word to 4 Disattenuated Pearson correlation helps account for the error of the measurement instrument (Murphy and Davidshofer, 1988; Kosinski et al., 2013). Following Lynn et al. (2018), we use reliabilities: rxx = 0.70 and ryy = 0.77. 5310 Ridge (Embeddings) Our Proposed Model Ridge with PCA (N-Grams/Topics) (Park et al., 2015) Ridge with PCA (N-Grams/Topics) + Our Proposed Model d OPE CON EXT AGR NEU 200 200 5106 5306 .538 .626 .627 .657† .500 .521 .518 .538† .505 .552 .558 .583† .444 .509 .545 .557† .505 .541† .531 .564† Table 3: Combining our best model with that of Park et al. (2015) obtains new state-of-the-art performance in terms of Disattenuated Pearson R. Number of input dimensions (d) is shown for each model. † indicates a statistically significant improvement ove"
2020.acl-main.472,D17-1119,1,0.8331,"ted higher by a personality prediction model than the latter. This paper applies the idea of modeling document relevance to the task of personality prediction. Inferring an individual’s personality traits is a fundamental task in psychology (McCrae and Costa Jr, 1997; Mischel et al., 2007), with social scientific applications ranging from public health (Friedman and Kern, 2014) and marketing (Matz et al., 2017) to personalized medicine (Chapman et al., 2011), mental health care (Bagby et al., 1995), and even providing useful information for downstream NLP tasks (Preot¸iuc-Pietro et al., 2015; Lynn et al., 2017). Recently, researchers from both NLP and psychology have turned toward more accurately assessing personality and other human attributes via language (Mairesse et al., 2007; Schwartz et al., 2013; Park et al., 2015; Kulkarni et al., 2018). The idea behind “language-based assessments” (Park et al., 2015) is that language use patterns can supplement and, in part, replace traditional and expensive questionnaire-based human assessments. Here, we present a hierarchical neural sequence model over both the words and messages of the user and correspondingly applies attention to each level. The documen"
2020.acl-main.472,W19-3005,1,0.882283,"(SN). Similar to our proposed model but using the final state of each GRU, rather than word or message attention. Transformer (TN). This variant of our proposed model uses a two-layer transformer (Vaswani et al., 2017) with double-headed attention, rather than a GRU, at the message or word level. BERT. Whereas our proposed model learns message-level representations, we instead experiment with using pre-trained BERT embeddings (Devlin et al., 2019) as our message representations. These 768-dimension message embeddings are produced by averaging across all BERT token embeddings for each message (Matero et al., 2019). 5.3 Training All models were implemented using PyTorch (Paszke et al., 2017), with the exception of Ridge Regression which used scikit-learn (Pedregosa et al., 2011). One model was trained for each of the five personality dimensions. All deep learning models use two feed-forward layers with 512 hidden units each, followed by a final prediction layer. The GRU layers have a hidden size of 200 to match the number of embedding dimensions. Similarly, we learn a projection down to 200 dimensions for our BERT embeddings. All hyperparameters (dropout and learning rate 5309 word-to-message message-to"
2020.acl-main.472,N16-1174,0,0.0499808,"message-level attention. The representation for a user u is thus a weighted combination of the hidden states representing that person’s messages. Once the user representation has been produced, u is further passed through some fully-connected layers before being used for prediction at the final layer. 5307 In this way, important words and messages don’t get lost to noise and are instead carried through to later portions of the model, where they can have a greater impact on the final prediction. Our model is similar in structure and motivation to the Hierarchical Attention Network proposed by Yang et al. (2016). However, our work focuses on a different level of analysis: whereas Yang et al. (2016) encode words → sentences → documents, our work seeks to encode words → documents → users. This idea of applying attention at a document level when modeling user-level attributes is, to the best of our knowledge, entirely novel. We hypothesize that where attention is applied is crucial and that message-level attention is of particular importance for modeling personality. 3 Dataset We draw our data from consenting users of a Facebook application (Kosinski et al., 2013), which allowed users to take various ps"
2020.acl-main.472,W15-1203,1,0.781968,"Missing"
2020.coling-main.261,W14-3207,0,0.0214834,"of 1,900 users and weekly + daily scores for 6 emotions and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (r = .66). We make our anonymized dataset as well as task setup and evaluation code available for others to build on. 1 Introduction With the growth of social media, natural language processing has increasingly turned toward problems in the social scientific domains. Language has been used to predict personality disorders, political ideology, and mental health (Preotiuc-Pietro et al., 2016a; Iyyer et al., 2014; Coppersmith et al., 2014). However, such predictions are typically made cross-documents or -users rather than cross-time (forecasting the future). Predictions across time can enable another level of applications for NLP in the social sciences. For example, knowing how someone’s mood may change next week can be a valuable insight into preemptively preparing mental health assistance. We may be able to better predict a substance abuse relapse or suicide attempt, or provide individual insights into the precipitators of such events.1 Applications also exist in other domains, such as correlating mood and personal buying hab"
2020.coling-main.261,N19-1423,0,0.0128995,"ext using a human curated lexicon, as defined by the NRC (Mohammad and Kiritchenko, 2015). The lexicon leverages the amount of words that display each emotion per message to measure each emotion. Embeddings: Language is also directly modeled as covariates to our psychological attributes in the form of word embeddings. 2 different sets of representations are learned from our data in an effort to evaluate which works better for temporal tasks. First, we learn 50-dimensional word2vec embeddings over a large corpus of social media data (15 million tweets). Second, we extract BERT representations (Devlin et al., 2019) for our tweets using bert-large and concatenate outputs from the last 4 layers. After extraction of BERT features, a non-negative matrix factorization (F´evotte and Idier, 2011) is learned and applied using a subset of 50,000 tweets stratified across users. This matrix factorization brings the BERT features down to 50-dimensions to match word2vec. Embeddings are averaged across all words from a given week from a user. Models that leverage these additional context features are referred to as multivariate, as opposed to pure autoregressive(univariate) models. 3.2 Daily data To consider the effe"
2020.coling-main.261,W17-5217,0,0.0170892,"often lagged values of the predicted outcome, while still offering a more flexible architecture and non-linear modeling capabilities. Such architectures have been used extensively in the medical domain, by leveraging electronic health records, to forecast a patient’s future physical health. For example, the work of (Choi et al., 2016) designs a stacked GRU architecture to forecast the diagnosis of a patient at their next doctor’s visit, while jointly learning to predict the duration until the next visit(an example of a poison process). In the language domain the only similar work is that of (Halder et al., 2017). In which, their goal is to predict an online forum participants ”emotional progression” measured by their negative emotionindex,  num(neg words)−num(pos words) a simple metric that defines the polarity of a user’s word count . We num(total words) differentiate our work by focusing on forecasting established psychological definitions of affect and by using only linguistic features to forecast future linguistic trends. 3 Data and Time-Series We derived our data set from a collection of publicly posted tweets previously used for demographic prediction (Volkova et al., 2013; Sap et al., 2014)."
2020.coling-main.261,P14-1105,0,0.0115043,"vel Twitter dataset of 1,900 users and weekly + daily scores for 6 emotions and 2 additional linguistic attributes, we find a novel dual-sequence GRU model with decayed hidden states achieves best results (r = .66). We make our anonymized dataset as well as task setup and evaluation code available for others to build on. 1 Introduction With the growth of social media, natural language processing has increasingly turned toward problems in the social scientific domains. Language has been used to predict personality disorders, political ideology, and mental health (Preotiuc-Pietro et al., 2016a; Iyyer et al., 2014; Coppersmith et al., 2014). However, such predictions are typically made cross-documents or -users rather than cross-time (forecasting the future). Predictions across time can enable another level of applications for NLP in the social sciences. For example, knowing how someone’s mood may change next week can be a valuable insight into preemptively preparing mental health assistance. We may be able to better predict a substance abuse relapse or suicide attempt, or provide individual insights into the precipitators of such events.1 Applications also exist in other domains, such as correlating m"
2020.coling-main.261,P15-1162,0,0.0314603,"Missing"
2020.coling-main.261,W16-0404,1,0.8934,"Missing"
2020.coling-main.261,D14-1121,0,0.0192759,"lder et al., 2017). In which, their goal is to predict an online forum participants ”emotional progression” measured by their negative emotionindex,  num(neg words)−num(pos words) a simple metric that defines the polarity of a user’s word count . We num(total words) differentiate our work by focusing on forecasting established psychological definitions of affect and by using only linguistic features to forecast future linguistic trends. 3 Data and Time-Series We derived our data set from a collection of publicly posted tweets previously used for demographic prediction (Volkova et al., 2013; Sap et al., 2014). As one of the first studies of language forecasting of any kind, we sought to primarily focus on a wellrepresented time resolution – weekly aggregates of tweets. This helped us focus on the fundamentals 2 3 Data available at https://github.com/MatthewMatero/AffectiveLanguageForecasting Ensuring that our time-series maintains the same, approximate, statistical properties over time. Such as mean and variance 2914 of the methods rather than missing data issues.4 Only tweets without URLs (i.e. no retweets) were considered and users were filtered based on having 3+ original tweets per week, resul"
2020.coling-main.261,D13-1187,0,0.18489,"ar work is that of (Halder et al., 2017). In which, their goal is to predict an online forum participants ”emotional progression” measured by their negative emotionindex,  num(neg words)−num(pos words) a simple metric that defines the polarity of a user’s word count . We num(total words) differentiate our work by focusing on forecasting established psychological definitions of affect and by using only linguistic features to forecast future linguistic trends. 3 Data and Time-Series We derived our data set from a collection of publicly posted tweets previously used for demographic prediction (Volkova et al., 2013; Sap et al., 2014). As one of the first studies of language forecasting of any kind, we sought to primarily focus on a wellrepresented time resolution – weekly aggregates of tweets. This helped us focus on the fundamentals 2 3 Data available at https://github.com/MatthewMatero/AffectiveLanguageForecasting Ensuring that our time-series maintains the same, approximate, statistical properties over time. Such as mean and variance 2914 of the methods rather than missing data issues.4 Only tweets without URLs (i.e. no retweets) were considered and users were filtered based on having 3+ original twe"
2020.nlpcss-1.21,N10-1122,0,0.0312009,"(LDA; Blei et al. (2003)). However, tracking significant world events present 2 challenges to standard topic models: (1) the need for rapidly evolving topics rather than topics from a single snapshot of language at a time, and (2) the need to focus on event-relevant lexical patterns rather than general patterns. Latent Dirichlet Allocation (LDA) is one of the most commonly used topic modeling methods, whereby probabilities of words belonging to topics (clusters of semantically related words) are derived from textual data. Not only can this provide a good set of features for predictive models (Brody and Elhadad, 2010; Zamani et al., 2018b), but also a tool for generating hypotheses and gaining insight in a manner easily interpretable by humans (Schwartz et al., 2013; Hu et al., 2012). In this paper, we present and evaluate modifications to LDA, addressing the two aforementioned challenges, to focus on capturing topics that can characterize evolving interests specifically for COVID-19. Addressing such challenges enables several applications including monitoring the impact of a specific event on social, emotional, mental well-being and behaviours (Zamani et al., 2018a; Mirzaei et al., 2019). Building on pre"
2020.nlpcss-1.21,N10-1000,0,0.237485,"Missing"
2020.nlpcss-1.21,W18-0619,1,0.818406,". However, tracking significant world events present 2 challenges to standard topic models: (1) the need for rapidly evolving topics rather than topics from a single snapshot of language at a time, and (2) the need to focus on event-relevant lexical patterns rather than general patterns. Latent Dirichlet Allocation (LDA) is one of the most commonly used topic modeling methods, whereby probabilities of words belonging to topics (clusters of semantically related words) are derived from textual data. Not only can this provide a good set of features for predictive models (Brody and Elhadad, 2010; Zamani et al., 2018b), but also a tool for generating hypotheses and gaining insight in a manner easily interpretable by humans (Schwartz et al., 2013; Hu et al., 2012). In this paper, we present and evaluate modifications to LDA, addressing the two aforementioned challenges, to focus on capturing topics that can characterize evolving interests specifically for COVID-19. Addressing such challenges enables several applications including monitoring the impact of a specific event on social, emotional, mental well-being and behaviours (Zamani et al., 2018a; Mirzaei et al., 2019). Building on previous work in online"
2020.nlpcss-1.21,D18-1392,1,0.838633,". However, tracking significant world events present 2 challenges to standard topic models: (1) the need for rapidly evolving topics rather than topics from a single snapshot of language at a time, and (2) the need to focus on event-relevant lexical patterns rather than general patterns. Latent Dirichlet Allocation (LDA) is one of the most commonly used topic modeling methods, whereby probabilities of words belonging to topics (clusters of semantically related words) are derived from textual data. Not only can this provide a good set of features for predictive models (Brody and Elhadad, 2010; Zamani et al., 2018b), but also a tool for generating hypotheses and gaining insight in a manner easily interpretable by humans (Schwartz et al., 2013; Hu et al., 2012). In this paper, we present and evaluate modifications to LDA, addressing the two aforementioned challenges, to focus on capturing topics that can characterize evolving interests specifically for COVID-19. Addressing such challenges enables several applications including monitoring the impact of a specific event on social, emotional, mental well-being and behaviours (Zamani et al., 2018a; Mirzaei et al., 2019). Building on previous work in online"
2020.nlpcss-1.21,D17-2010,1,0.877509,"Missing"
2020.peoples-1.13,P17-1067,1,0.7801,"regression, random forests)? 2. How many observations do you believe are necessary for deep learning approaches to provide a clear benefit over traditional discriminative learning? Thank you for completing the survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, con"
2020.peoples-1.13,I17-2024,0,0.0413058,"Missing"
2020.peoples-1.13,Q17-1010,0,0.0159771,"below 100 observations, the vast majority of participants (20) states that 1,000 or more instances are necessary for that. Yet, no one responded with a number between 100 and 1,000. While we do not validate the claim of this minority, the remainder of the paper provides strong evidence that the majority of the participants largely overestimated the data requirements of deep learning. 3 Data For the following study, we selected four small (< 3000 instances) and typologically diverse datasets described below. Pre-trained, publicly available word2vec (Mikolov et al., 2013) and FastText vectors (Bojanowski et al., 2017) of matching language and target domain were used as model input. Table 1 summarizes the employed data. Illustrative examples of the particular styles and annotation formats of those corpora are provided in Table 2. SE07: The test set of SemEval 2007 Task 14 (Strapparava and Mihalcea, 2007) comprises 1000 English news headlines that are annotated according to six Basic Emotions, joy, anger, sadness, fear, disgust, 131 Corpus SE07 WASSA ANPST MAS Text Val Aro Dom Joy Ang Sad Fea Dis Sur Inter Milan set Serie A win record - - - 50 2 0 0 0 9 TBS to pay $2M fine for ad campaign bomb scare - - - 11"
2020.peoples-1.13,C18-1179,0,0.015303,"; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classificat"
2020.peoples-1.13,C18-1245,1,0.905525,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,L18-1028,1,0.930957,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,N18-1173,1,0.921064,"Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * Work partially conducted at the University of Pennsylvania. License details: http:// 129 Proceedings of the Third Workshop on Computational Modeling of PEople’s Opinions, PersonaLity, and Emotions in Social media, pages 129–139 Barcelona, Spain (Online), December 13, 2020. Data Requirements for"
2020.peoples-1.13,S07-1094,0,0.121246,"Missing"
2020.peoples-1.13,W14-4012,0,0.0557733,"Missing"
2020.peoples-1.13,N19-1423,0,0.124642,"hat high-quality, pre-trained word embeddings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often"
2020.peoples-1.13,D17-1169,0,0.0296226,"VM, penalized linear regression, random forests)? 2. How many observations do you believe are necessary for deep learning approaches to provide a clear benefit over traditional discriminative learning? Thank you for completing the survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et"
2020.peoples-1.13,W15-4322,0,0.0480146,"Missing"
2020.peoples-1.13,L18-1550,0,0.150562,"l., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts of annotated data—and this seems particularly troublesome for the field of emotion analysis because such phenomena are intrinsically hard to annotate. However, we suspect that this gold data dependency may, for emotion analysis at least, be less severe than anticipated because large, pre-trained embedding models already seem to encode wordlevel"
2020.peoples-1.13,2020.tacl-1.5,0,0.0127331,"dings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories"
2020.peoples-1.13,P14-1062,0,0.0144161,"models which both rely on Ridge regression, an `2 -regularized version of linear regression. The first one, Ridgengram , is based on n-gram features where we use n ∈ {1, 2, 3}. The second one, RidgeBV uses bag-of-vectors features, i.e., the pointwise mean of the embeddings of the words in a text. Regarding the deep learning approaches, we compare Feed-Forward Networks (FFN), Gated Recurrent Unit Networks (GRU), Long Short-Term Memory Networks (LSTM), Convolutional Neural Networks (CNN), as well as a combination of the latter two (CNN-LSTM) (Cho et al., 2014; Hochreiter and Schmidhuber, 1997; Kalchbrenner et al., 2014). Since holding out a dev set from the already limited training data does not seem feasible for some of the datasets (see Table 1), we decided to instead use constant hyperparameter settings across all corpora. We also keep most hyperparameters constant between models. Hence, hyperparameter choices followed well-established recommendations described in the next paragraph. The input to the DL models is based on pre-trained word vectors. ReLu activation was used everywhere except in recurrent layers. Dropout is used for regularization with a probability of .2 for embedding layers and .5 for dens"
2020.peoples-1.13,I17-2042,0,0.0239748,"e survey! Do you have any additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning sup"
2020.peoples-1.13,E17-1071,0,0.0174329,"comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts"
2020.peoples-1.13,S17-1007,0,0.0147001,"performance by Beck (2017); p < .001). Our GRU also outperforms IAA, as already did B ECK. This may sound improbable at first glance. However, Strapparava and Mihalcea (2007) employ a rather weak notion of human performance which is—broadly speaking—based on the reliability of a single human rater.2 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise). WASSA 2017 Shared Task Data Table 7 displays the official results of the four best systems (out of 21 submissions) of the WASSA 2017 shared task (Mohammad and Bravo-Marquez, 2017b) as well as the performance our GRU achieved. For this experiment, we deviated from the above 10×10-CV set-up but instead used the official train-dev-test split for comparability. As for all experiments in this paper, hyperparameters were kept constant and were not adjusted to this dataset. Consequently, train and dev sets were combined for training. Training and testing were repeated ten times with different random seeds but otherwise identical configuration following the recommendation by Reimers and Gurevych (2018). Table 7 shows our average performance over those ten runs. As can be seen"
2020.peoples-1.13,W17-5205,0,0.0172932,"performance by Beck (2017); p < .001). Our GRU also outperforms IAA, as already did B ECK. This may sound improbable at first glance. However, Strapparava and Mihalcea (2007) employ a rather weak notion of human performance which is—broadly speaking—based on the reliability of a single human rater.2 Interestingly, the GRU shows particularly large improvements over human performance for categories where the IAA is low (anger, disgust, and surprise). WASSA 2017 Shared Task Data Table 7 displays the official results of the four best systems (out of 21 submissions) of the WASSA 2017 shared task (Mohammad and Bravo-Marquez, 2017b) as well as the performance our GRU achieved. For this experiment, we deviated from the above 10×10-CV set-up but instead used the official train-dev-test split for comparability. As for all experiments in this paper, hyperparameters were kept constant and were not adjusted to this dataset. Consequently, train and dev sets were combined for training. Training and testing were repeated ten times with different random seeds but otherwise identical configuration following the recommendation by Reimers and Gurevych (2018). Table 7 shows our average performance over those ten runs. As can be seen"
2020.peoples-1.13,S18-1001,0,0.0205739,"ation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity (Buechel and Hahn, 2018b). And, second, especially large-scale annotated corpora are almost exclusively available for English, leaving most of the world’s languages with little or no gold data at all. This work is licensed under a Creative Co"
2020.peoples-1.13,D14-1162,0,0.0833341,"regression on only 100 data points. Our analysis suggests that high-quality, pre-trained word embeddings are a main factor for achieving those results. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by boosting performance figures in almost all application areas. Yet in contrast to more conventional techniques, such as n-gram based linear models, neural methodologies seem to rely on vast amounts of training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018;"
2020.peoples-1.13,N18-1202,0,0.0369629,"mena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depends on vast amounts of annotated data—and this seems particularly troublesome for the field of emotion analysis because such phenomena are intrinsically hard to annotate. However, we suspect that this gold data dependency may, for emotion analysis at least, be less severe than anticipated because large, pre-trained embedding models already seem to encode wordlevel emotion quite well (Du and Zhang, 2016; Li et al., 2017; Buechel and Hahn, 2018c), possibly allowing to fit sentence-level DL architectures on rather small datasets. If"
2020.peoples-1.13,S17-2088,0,0.0140358,"training data, as is obvious in areas such as machine translation (Vaswani et al., 2017) or representation learning for individual words (Mikolov et al., 2013; Pennington et al., 2014) or contextualized word sequences (Devlin et al., 2019; Yang et al., 2019; Joshi et al., 2020). With this profile, DL seems ill-suited for many prediction tasks in sentiment and subjectivity analysis (Balahur et al., 2014). For the widely studied problem of polarity prediction (distinguishing only between positive and negative emotion), training data is relatively abundant especially for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more compl"
2020.peoples-1.13,N07-2036,0,0.0363685,"ny additional comments regarding this questionnaire? Figure 1: Survey on expected data requirements of deep learning. For the social media domain, this lack of gold data can be partly countered by (pre-)training with distant supervision using signals such as emojis or hashtags as a surrogate for manual annotation (Mohammad and Kiritchenko, 2015; Felbo et al., 2017; Abdul-Mageed and Ungar, 2017). Yet, this procedure is less viable for target domains other than social media, as well as for predicting other subjective phenomena such as empathy, uncertainty, or personality (Khanpour et al., 2017; Rubin, 2007; Liu et al., 2017). Besides pre-training the entirety of the model with distant supervision, an alternative strategy is pre-training word representations, only. This approach is feasible for a wide range of languages, including otherwise less-resourced ones, since raw text is much more readily available than gold data, e.g., through Wikipedia (Grave et al., 2018). Very recently, contextualized word representations generated by pre-trained language models have established themselves as a powerful alternative (Peters et al., 2018; Devlin et al., 2019). In summary, deep learning supposedly depen"
2020.peoples-1.13,S07-1013,0,0.431474,"ly for the social media domain (Rosenthal et al., 2017). However, in recent years, there has been a growing interest in more nuanced and informative annotation formats for affective states (Bostan and Klinger, 2018; De Bruyne et al., 2019). Such annotation schemes often follow distinct psychological theories such as the dimensional approach to emotion representation (Bradley and Lang, 1994) or basic emotions (Ekman, 1992). Yet, annotating for more complex representations of affective states seems to be significantly harder in terms of both time consumption and inter-annotator agreement (IAA) (Strapparava and Mihalcea, 2007). Adding even more complexity, computational work following this trend often uses numerical scores as target variables making, emotion analysis a regression, rather than a classification problem (Buechel and Hahn, 2016; Mohammad et al., 2018). What makes this situation even worse is that, first, we currently have a situation where there is no community-wide consensus on how emotion should be represented. That is, different ways of annotating emotion (see, e.g., Table 2) compete with each other, leading to decreased inter-operability of language resources and provoking additional data sparsity"
2021.findings-acl.363,2020.emnlp-main.19,0,0.0415598,"antization. Bhandare et al. (2019); Shen et al. (2020); Prato et al. (2020) have shown benefits from selecting the quantization range, which motivates us to prune the attention before quantization (Section 3). Kim et al. (2021); Zafrir et al. (2019); Prato et al. (2020) required re-training while ours does not. Zhang et al. (2020); Bai et al. (2020); Zadeh et al. (2020) focused on quantizing the weights rather than the attention values, which is out of our scope. Sparse transformers and attention visualization Parmar et al. (2018); Child et al. (2019); Ho et al. (2019); Beltagy et al. (2020); Ainslie et al. (2020); Li and Chan (2019); Tay et al. (2020) have proposed/summarized various kinds of efficient transformers utilizing induced attention sparsity. However, none of them quantitatively analyzed the statistical distribution and the tiny values of the attention. Vig (2019); Hoover et al. (2020) proposed instance-level attention visualization tools. These are complementary to our quantitative visualization of the distributions of all attention values. 5 Conclusion We demonstrated that pruning near-zero values and large reductions in the number of bits needed for attention, even at application time wit"
2021.findings-acl.363,W19-4828,0,0.0985206,"es that most heads focus strongly on fewer than 10 tokens on average (details in Appendix A), leading to notable sparsity and suggesting large potential for conveying the same information as continuous attention values using fewer discrete levels. Beyond these, we occasionally observe outlier attention histograms (like the outliers between [10−4 , 10−1 ] in Figure 1b). We also found noticeable differences on the attention histograms from layer to layer. These findings are related to the works on the syntactic heads/special tokens (Voita et al., 2019; Kovaleva et al., 2019; Voita et al., 2018; Clark et al., 2019; Rogers et al., 2020)) and the differences of the layers/heads (Correia et al., 2019; Clark et al., 2019). We discuss how our findings relate to them in Appendices B and C. Limited effect of near-zero attention values during inference. The inherent sparsity we observed motivates us to explore the sparsity of attention at inference-time—how much attention can be pruned during inference, without impacting the model accuracy? By setting up a series of pruning thresholds, we clamp different proportions of the attention to zero and examine how attention sparsity affects the accuracy, on both pretr"
2021.findings-acl.363,D19-1223,0,0.0352554,"Missing"
2021.findings-acl.363,2020.acl-demos.22,0,0.0254619,"ng while ours does not. Zhang et al. (2020); Bai et al. (2020); Zadeh et al. (2020) focused on quantizing the weights rather than the attention values, which is out of our scope. Sparse transformers and attention visualization Parmar et al. (2018); Child et al. (2019); Ho et al. (2019); Beltagy et al. (2020); Ainslie et al. (2020); Li and Chan (2019); Tay et al. (2020) have proposed/summarized various kinds of efficient transformers utilizing induced attention sparsity. However, none of them quantitatively analyzed the statistical distribution and the tiny values of the attention. Vig (2019); Hoover et al. (2020) proposed instance-level attention visualization tools. These are complementary to our quantitative visualization of the distributions of all attention values. 5 Conclusion We demonstrated that pruning near-zero values and large reductions in the number of bits needed for attention, even at application time without retraining or fine-tuning, is possible with little loss of accuracy. This suggests attention plays a very coarse role in model accuracy at inference-time, yielding opportunities to run transformers more efficiently over applications. While quantization during training had previously"
2021.findings-acl.363,N19-1357,0,0.0508516,"Missing"
2021.findings-acl.363,D19-1445,0,0.0750814,"hat sum up to 0.5 in each αi . This indicates that most heads focus strongly on fewer than 10 tokens on average (details in Appendix A), leading to notable sparsity and suggesting large potential for conveying the same information as continuous attention values using fewer discrete levels. Beyond these, we occasionally observe outlier attention histograms (like the outliers between [10−4 , 10−1 ] in Figure 1b). We also found noticeable differences on the attention histograms from layer to layer. These findings are related to the works on the syntactic heads/special tokens (Voita et al., 2019; Kovaleva et al., 2019; Voita et al., 2018; Clark et al., 2019; Rogers et al., 2020)) and the differences of the layers/heads (Correia et al., 2019; Clark et al., 2019). We discuss how our findings relate to them in Appendices B and C. Limited effect of near-zero attention values during inference. The inherent sparsity we observed motivates us to explore the sparsity of attention at inference-time—how much attention can be pruned during inference, without impacting the model accuracy? By setting up a series of pruning thresholds, we clamp different proportions of the attention to zero and examine how attention spar"
2021.findings-acl.363,D19-5620,0,0.0255348,"al. (2019); Shen et al. (2020); Prato et al. (2020) have shown benefits from selecting the quantization range, which motivates us to prune the attention before quantization (Section 3). Kim et al. (2021); Zafrir et al. (2019); Prato et al. (2020) required re-training while ours does not. Zhang et al. (2020); Bai et al. (2020); Zadeh et al. (2020) focused on quantizing the weights rather than the attention values, which is out of our scope. Sparse transformers and attention visualization Parmar et al. (2018); Child et al. (2019); Ho et al. (2019); Beltagy et al. (2020); Ainslie et al. (2020); Li and Chan (2019); Tay et al. (2020) have proposed/summarized various kinds of efficient transformers utilizing induced attention sparsity. However, none of them quantitatively analyzed the statistical distribution and the tiny values of the attention. Vig (2019); Hoover et al. (2020) proposed instance-level attention visualization tools. These are complementary to our quantitative visualization of the distributions of all attention values. 5 Conclusion We demonstrated that pruning near-zero values and large reductions in the number of bits needed for attention, even at application time without retraining or f"
2021.findings-acl.363,2021.ccl-1.108,0,0.0216015,"Missing"
2021.findings-acl.363,N19-1423,0,0.0292032,"l., 2020), and the interpretability of such sparsity (Chen et al., 2020; Rogers et al., 2020). Yet, little is known about our ability to induce sparsity or reduce its values at applicationtime, and what role the inherent sparsity could play in building inference-time efficient transformers. This work focuses on a systematic study of the quantitative distribution of the attention values across the layers and heads as well as the potential for reducing the information content of attention values during inference at application-time1 . We consider two popular pretrained transformer models: BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) over tasks of Masked Language Modeling as well as question answering and sentiment analysis. We explore the attention distributions on the different models and tasks, and quantitatively profile the sparse attention that commonly exists in the transformer model. Motivated by the high levels of inherent sparsity in these distributions, we design a pruning and quantization technique and test the limits of information necessary from attention. We find that most attention values can be pruned (i.e. set to zero) and the remaining non-zero values can be mapped to a sma"
2021.findings-acl.363,2021.eacl-main.9,0,0.0259669,"note that the benefits of attention sparsity may extend much further than just computing attention values themselves; other computations in the transformer network can also 4150 benefit from leveraging the high degree of sparsity without retraining/fine-tuning, potentially yielding larger benefits. Future work will investigate the computational benefits of utilizing attention sparsity and the design of customized hardware accelerators to efficiently do so. 4 Related Work Attention distribution. Many have abstractly studied the attention distribution from different aspects (Clark et al., 2019; Pascual et al., 2021; Ramsauer et al., 2020; Correia et al., 2019), but none specifically have shown the histogram of the αi directly, nor did they investigate the sparse attention values quantitatively. Correia et al. (2019) indicated that not all of the sparsity in attention was caused by the softmax, and it remained unclear whether such sparsity affected accuracy (which is inspected in this paper). Pruning. Voita et al. (2019); Sajjad et al. (2020); Michel et al. (2019); Kovaleva et al. (2019) pruned one or more heads/layers resulting in comparable or higher model accuracy, either with or without fine-tuning."
2021.findings-acl.363,2020.findings-emnlp.1,0,0.042182,"acy (which is inspected in this paper). Pruning. Voita et al. (2019); Sajjad et al. (2020); Michel et al. (2019); Kovaleva et al. (2019) pruned one or more heads/layers resulting in comparable or higher model accuracy, either with or without fine-tuning. These approaches assume that some heads/layers interpret the information redundantly, which is not always true (Brunner et al., 2020; Rogers et al., 2020). In contrast, our work focuses on a more general method of inducing attention sparsity without operating at layer/head granularity. Quantization. Bhandare et al. (2019); Shen et al. (2020); Prato et al. (2020) have shown benefits from selecting the quantization range, which motivates us to prune the attention before quantization (Section 3). Kim et al. (2021); Zafrir et al. (2019); Prato et al. (2020) required re-training while ours does not. Zhang et al. (2020); Bai et al. (2020); Zadeh et al. (2020) focused on quantizing the weights rather than the attention values, which is out of our scope. Sparse transformers and attention visualization Parmar et al. (2018); Child et al. (2019); Ho et al. (2019); Beltagy et al. (2020); Ainslie et al. (2020); Li and Chan (2019); Tay et al. (2020) have proposed/"
2021.findings-acl.363,D16-1264,0,0.0130802,"-bits we map the continuous attention values to one of 2k real values2 . We use two methods: (i) Linear - Bin the attention values to 2k quantiles and set the midpoint of each as the quantized value. (ii) Log - Bin the log transformed attention values and pick the mid-point of each on the log scale as the quantized value. The quantization methods are explained in detail in Appendix E. We apply these inference-time (i.e. no training) techniques on three tasks: masked language modeling, question answering and sentiment analysis. For QA we used BERT3 and RoBERTa4 models fine-tuned on SQuAD v1.1 (Rajpurkar et al., 2016). For sentiment analysis we used RoBERTa5 finetuned on the SST-2 dataset (Socher et al., 2013). For both these tasks we report accuracy on the corresponding development sets. For the Masked Language Modeling (MLM) task we report pseudoperplexity (Salazar et al., 2020) computed on the Huggingface Wikipedia dataset6 . 3 Evaluation Attention distribution and sparsity. A thorough quantitative analysis on the attention distribution could help build efficient transformers by providing useful information, such as the degree of sparsity and the range of the attention values. We plot the histogram of e"
2021.findings-acl.363,2020.tacl-1.54,0,0.0828275,"ocus strongly on fewer than 10 tokens on average (details in Appendix A), leading to notable sparsity and suggesting large potential for conveying the same information as continuous attention values using fewer discrete levels. Beyond these, we occasionally observe outlier attention histograms (like the outliers between [10−4 , 10−1 ] in Figure 1b). We also found noticeable differences on the attention histograms from layer to layer. These findings are related to the works on the syntactic heads/special tokens (Voita et al., 2019; Kovaleva et al., 2019; Voita et al., 2018; Clark et al., 2019; Rogers et al., 2020)) and the differences of the layers/heads (Correia et al., 2019; Clark et al., 2019). We discuss how our findings relate to them in Appendices B and C. Limited effect of near-zero attention values during inference. The inherent sparsity we observed motivates us to explore the sparsity of attention at inference-time—how much attention can be pruned during inference, without impacting the model accuracy? By setting up a series of pruning thresholds, we clamp different proportions of the attention to zero and examine how attention sparsity affects the accuracy, on both pretrained and finetuned mo"
2021.findings-acl.363,2020.acl-main.240,0,0.0464051,"Missing"
2021.findings-acl.363,D13-1170,0,0.00747895,"Linear - Bin the attention values to 2k quantiles and set the midpoint of each as the quantized value. (ii) Log - Bin the log transformed attention values and pick the mid-point of each on the log scale as the quantized value. The quantization methods are explained in detail in Appendix E. We apply these inference-time (i.e. no training) techniques on three tasks: masked language modeling, question answering and sentiment analysis. For QA we used BERT3 and RoBERTa4 models fine-tuned on SQuAD v1.1 (Rajpurkar et al., 2016). For sentiment analysis we used RoBERTa5 finetuned on the SST-2 dataset (Socher et al., 2013). For both these tasks we report accuracy on the corresponding development sets. For the Masked Language Modeling (MLM) task we report pseudoperplexity (Salazar et al., 2020) computed on the Huggingface Wikipedia dataset6 . 3 Evaluation Attention distribution and sparsity. A thorough quantitative analysis on the attention distribution could help build efficient transformers by providing useful information, such as the degree of sparsity and the range of the attention values. We plot the histogram of each token’s attention to all the others (αi ) and provide three examples of the heads in Figur"
2021.findings-acl.363,W19-4808,0,0.0400191,"Missing"
2021.findings-acl.363,P18-1117,0,0.0226523,"ch αi . This indicates that most heads focus strongly on fewer than 10 tokens on average (details in Appendix A), leading to notable sparsity and suggesting large potential for conveying the same information as continuous attention values using fewer discrete levels. Beyond these, we occasionally observe outlier attention histograms (like the outliers between [10−4 , 10−1 ] in Figure 1b). We also found noticeable differences on the attention histograms from layer to layer. These findings are related to the works on the syntactic heads/special tokens (Voita et al., 2019; Kovaleva et al., 2019; Voita et al., 2018; Clark et al., 2019; Rogers et al., 2020)) and the differences of the layers/heads (Correia et al., 2019; Clark et al., 2019). We discuss how our findings relate to them in Appendices B and C. Limited effect of near-zero attention values during inference. The inherent sparsity we observed motivates us to explore the sparsity of attention at inference-time—how much attention can be pruned during inference, without impacting the model accuracy? By setting up a series of pruning thresholds, we clamp different proportions of the attention to zero and examine how attention sparsity affects the acc"
2021.findings-acl.363,P19-1580,0,0.0237327,"ed hardware accelerators to efficiently do so. 4 Related Work Attention distribution. Many have abstractly studied the attention distribution from different aspects (Clark et al., 2019; Pascual et al., 2021; Ramsauer et al., 2020; Correia et al., 2019), but none specifically have shown the histogram of the αi directly, nor did they investigate the sparse attention values quantitatively. Correia et al. (2019) indicated that not all of the sparsity in attention was caused by the softmax, and it remained unclear whether such sparsity affected accuracy (which is inspected in this paper). Pruning. Voita et al. (2019); Sajjad et al. (2020); Michel et al. (2019); Kovaleva et al. (2019) pruned one or more heads/layers resulting in comparable or higher model accuracy, either with or without fine-tuning. These approaches assume that some heads/layers interpret the information redundantly, which is not always true (Brunner et al., 2020; Rogers et al., 2020). In contrast, our work focuses on a more general method of inducing attention sparsity without operating at layer/head granularity. Quantization. Bhandare et al. (2019); Shen et al. (2020); Prato et al. (2020) have shown benefits from selecting the quantizat"
2021.findings-acl.363,D19-1002,0,0.043136,"Missing"
2021.findings-acl.363,2020.acl-main.422,0,0.0288815,"Missing"
2021.findings-emnlp.253,2021.ccl-1.108,0,0.0589491,"Missing"
2021.findings-emnlp.253,2020.acl-main.472,1,0.758745,"However, such work explicitly integrated user- or social-context into the stance model, as a separate component. We ask if there is a more direct integration of user context when processing a target message. To this end, we process the target message as a part of the sequence of messages from the user. This way of using historical language from a person enables us to both model within message information (word-level) and to process the message within the author context (message-level). While there have been some models that take advantage of hierarchy through words and sequences of messages (Lynn et al., 2020; Yu et al., 2020; Zhao and Yang, 2020) there has been little work in providing generic pre-training routines for large capacity transfer learning style models beyond the word-level. Instead, many of these hierarchical models are either applied directly to a downstream task or, if pre-trained, on an adjacent version of the downstream task. Being able to pre-train general message-level models could enable inclusion of message-level contextual information that is not easily obtainable with task-specific training that is limited in data sizes as compared to larger unlabeled corpora available for"
2021.findings-emnlp.253,W19-2103,1,0.91399,". While the multilevel aspect is rarely looked at beyond words-todocuments, some work has suggested benefits to modeling language as a hierarchy, such as building document representations from a collection of its sentences or a user vector given a history of their language (Song et al., 2020; Acheampong et al., 2021; Grail et al., 2021; Matero et al., 2019; Ganesan et al., 2021). We consider stance detection, a message-level task, where the social or personal context in which the message appears (e.g., such as a person’s profile) has been shown relevant to capturing the stance of the message (Lynn et al., 2019; Aldayel and Magdy, 2019). However, such work explicitly integrated user- or social-context into the stance model, as a separate component. We ask if there is a more direct integration of user context when processing a target message. To this end, we process the target message as a part of the sequence of messages from the user. This way of using historical language from a person enables us to both model within message information (word-level) and to process the message within the author context (message-level). While there have been some models that take advantage of hierarchy through words"
2021.findings-emnlp.253,W19-3005,1,0.770351,"Missing"
2021.findings-emnlp.253,2020.coling-main.261,1,0.760821,"et (4,100 total tweets) which we did not have available due to accounts or messages being deleted on twitter since release. . sages have message-level PAD tokens appended to their sequence. However, users that have multiple sequences will not be assigned a PAD token, if their last sequence falls short of 40 we include the amount of missing messages from their previous sequence. Dataset For pre-training our model we select users from publicly available tweets that were previously used for other user-level predictions, such as demographic prediction or emotion forecasting (Volkova et al., 2013; Matero and Schwartz, 2020). A subset of data is selected as our pretraining dataset, approximately 10 million tweets sampled from 6 thousand users, resulting in a dataset 1.3 GB in size. We use a limited dataset to highlight the utility of the pre-training routine itself and not rely on “bigger is better&quot; mindset. 4 Stance Detection with MeLT We use the stance dataset available from the SemEval 2016 shared task (Mohammad et al., 2016). This data includes tweets that were annotated either against, neutral, or favoring of a specific target mentioned within the tweet, across 5 distinct targets in the dataset. However, thi"
2021.findings-emnlp.253,S16-1003,0,0.208618,"models could enable inclusion of message-level contextual information that is not easily obtainable with task-specific training that is limited in data sizes as compared to larger unlabeled corpora available for modeling at the message-level. In this study, we propose a hierarchical messagelevel transformer (MeLT) trained over a novel pretraining routine of Masked Document Modeling1 , where the goal is to encode documents in latent space using surrounding contextual documents. We then fine-tune MeLT to a stance detection dataset derived from Twitter as defined in the SemEval 2016 shared task (Mohammad et al., 2016). Our contributions include: (1) introduction of a new pre-training routine for hierarchical message-level transformers2 , (2) demonstration of efficacy of our 1 In this work a document is a single tweet (referred to as a message) 2 Code: https://github.com/MatthewMatero/MeLT pre-training routine for stance detection, and (3) exploratory analysis comparing model size with respect to the number of additional message-level layers and amount of user history leveraged in finetuning. 2 Related Work Our approach is inspired by the success word-todocument level transfer learning has had since popular"
2021.findings-emnlp.253,2020.lrec-1.163,0,0.0779474,"Missing"
2021.findings-emnlp.253,D13-1187,0,0.0357631,"the SemEval2016 dataset (4,100 total tweets) which we did not have available due to accounts or messages being deleted on twitter since release. . sages have message-level PAD tokens appended to their sequence. However, users that have multiple sequences will not be assigned a PAD token, if their last sequence falls short of 40 we include the amount of missing messages from their previous sequence. Dataset For pre-training our model we select users from publicly available tweets that were previously used for other user-level predictions, such as demographic prediction or emotion forecasting (Volkova et al., 2013; Matero and Schwartz, 2020). A subset of data is selected as our pretraining dataset, approximately 10 million tweets sampled from 6 thousand users, resulting in a dataset 1.3 GB in size. We use a limited dataset to highlight the utility of the pre-training routine itself and not rely on “bigger is better&quot; mindset. 4 Stance Detection with MeLT We use the stance dataset available from the SemEval 2016 shared task (Mohammad et al., 2016). This data includes tweets that were annotated either against, neutral, or favoring of a specific target mentioned within the tweet, across 5 distinct targets"
2021.findings-emnlp.253,2020.emnlp-main.108,0,0.193672,"explicitly integrated user- or social-context into the stance model, as a separate component. We ask if there is a more direct integration of user context when processing a target message. To this end, we process the target message as a part of the sequence of messages from the user. This way of using historical language from a person enables us to both model within message information (word-level) and to process the message within the author context (message-level). While there have been some models that take advantage of hierarchy through words and sequences of messages (Lynn et al., 2020; Yu et al., 2020; Zhao and Yang, 2020) there has been little work in providing generic pre-training routines for large capacity transfer learning style models beyond the word-level. Instead, many of these hierarchical models are either applied directly to a downstream task or, if pre-trained, on an adjacent version of the downstream task. Being able to pre-train general message-level models could enable inclusion of message-level contextual information that is not easily obtainable with task-specific training that is limited in data sizes as compared to larger unlabeled corpora available for modeling at the m"
2021.findings-emnlp.253,S16-1074,0,0.0168097,"ec Rec 59 61 55 68 57 66 62 64 69 69 66 69 Table 3: Evaluation of using a different word-level model for our experiments (DistilRoBERTa). All MeLT variants are fine-tuned with the word-level model unfrozen. While we do not see this version outperform the DistilBERT variant, there are still clear benefits from using MeLT over just the word-level distil-RoBERTa. Bold results are found to be significant with p &lt; .05 w.r.t DistilRoBERTa + History. the average of 40 recent messages. This allows the model to have a global context within user. We also include the top participant from the shared task Zarrella and Marsh (2016) which uses a different F1 score as defined for the shared task, referred to here as SemEval F13 . Lastly, we compare our results to the approach of Lynn et al. (2019), from whom we received the extended history dataset, which uses the labeled tweet and a list of accounts the author follows. However, they only report the weighted-F1 score for their best performing model. We find that fine-tuning DistilBERT directly to the task of stance detection proves difficult, only scoring a modest F1. However when we include some context language from the user, an average representation of their recent la"
2021.findings-emnlp.253,P19-1499,0,0.0259507,"layers and amount of user history leveraged in finetuning. 2 Related Work Our approach is inspired by the success word-todocument level transfer learning has had since popularized by the BERT language model (Devlin et al., 2018). Offering the idea of a “contextual embedding&quot; allows models to properly disambiguate words based on their surrounding context. While other types of language models are also used, usually autoregressive based such as GPT and XLNet (Brown et al., 2020; Yang et al., 2019), many models are variants of the BERT autoencoder style (Liu et al., 2019; Lan et al., 2019). Both Zhang et al. (2019) and Liu and Lapata (2019) use hierarchical encoder models for summarization tasks. While both models encode sentences using some surrounding context, their pre-training tasks are still that of text generation rather than latent modeling. Yu et al. (2020) encode global context in conversation threads on social media by generating a history vector (concatenated representations of each sub-thread) during the fine-tuning step and Zhao and Yang (2020) propose a capsule network to aggregate fine-tuned word representations to perform automatic stance detection. Stance detection is an ideal task to d"
2021.naacl-main.357,N19-1300,0,0.0158582,"the growing num- fine-tuning with few samples (Jiang et al., 2020) ber of tasks focused on human-level NLP. but it also becomes unreliable even with thousands 4515 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4515–4532 June 6–11, 2021. ©2021 Association for Computational Linguistics of training examples (Mosbach et al., 2020). On the other hand, some of the common transformer-based approaches of deriving contextual embeddings from the top layers of a pretrained model (Devlin et al., 2019; Clark et al., 2019) still leaves one with approximately an equal number of embedding dimensions as training size. In fact, in one of the few successful cases of using transformers for a human-level task, further dimensionality reduction was used to avoid overfit (Matero et al., 2019), but an empirical understanding of the application of transformers for human-level tasks — which models are best and the relationship between embedding dimensions, sample size, and accuracy — has yet to be established. In this work, we empirically explore strategies to effectively utilize transformer-based LMs for relatively small s"
2021.naacl-main.357,W14-3207,0,0.0276422,"in humanlevel tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results compara1 of the ble to the best performance with just 12 embedding dimensions. Human-level NLP tasks, rooted in computational social science, focus on making predictions about people from their language use patterns. Some of the more common tasks include age and gender prediction (Sap et al., 2014; Morgan-Lopez et al., 2017) , personality (Park et al., 2015; Lynn et al., 2020), and mental health prediction (Coppersmith et al., 2014; Guntuku et al., 2017; Lynn et al., 2018). Such tasks present an interesting challenge for the NLP community to model the people behind the language rather than the language itself, and the social scientific community has begun to see success of such approaches as an alternative or supplement to standard psychological assessment techniques like questionnaires (Kern et al., 2016; Eichstaedt et al., 2018). Generally, such work is helping to embed NLP in a greater social and human context (Hovy and Spruit, 2016; Lynn et al., 2019). Despite the simultaneous growth of both (1) the use of transform"
2021.naacl-main.357,N19-1423,0,0.18956,") human-level NLP, the effective merging of transformers for humanlevel tasks has received little attention. In a recent human-level shared task on mental health, most participants did not utilize transformers (Zirikly 1 Introduction et al., 2019). A central challenge for their utiTransformer based language models (LMs) have lization in such scenarios is that the number of quickly become the foundation for accurately ap- training examples (i.e. sample size) is often only proaching many tasks in natural language process- hundreds while the parameters for such deep moding (Vaswani et al., 2017; Devlin et al., 2019). Ow- els are in the hundreds of millions. For examing to their success is their ability to capture both ple, recent human-level NLP shared tasks focused syntactic and semantic information (Tenney et al., on mental health have had N = 947 (Milne 2019), modeled over large, deep attention-based et al., 2016), N = 9, 146 (Lynn et al., 2018) and networks (transformers) with hidden state sizes on N = 993 (Zirikly et al., 2019) training examples. the order of 1000 over 10s of layers (Liu et al., Such sizes all but rules out the increasingly popular 2019; Gururangan et al., 2020). In total such mod-"
2021.naacl-main.357,P19-1163,0,0.0395787,"Missing"
2021.naacl-main.357,D14-1162,0,0.107923,"Missing"
2021.naacl-main.357,2020.acl-main.468,1,0.772778,"Missing"
2021.naacl-main.357,P19-1452,0,0.0575794,"Missing"
2021.naacl-main.357,2020.emnlp-demos.6,0,0.0692552,"Missing"
2021.naacl-main.357,W19-3003,0,0.392875,"training examples (i.e. sample size) is often only proaching many tasks in natural language process- hundreds while the parameters for such deep moding (Vaswani et al., 2017; Devlin et al., 2019). Ow- els are in the hundreds of millions. For examing to their success is their ability to capture both ple, recent human-level NLP shared tasks focused syntactic and semantic information (Tenney et al., on mental health have had N = 947 (Milne 2019), modeled over large, deep attention-based et al., 2016), N = 9, 146 (Lynn et al., 2018) and networks (transformers) with hidden state sizes on N = 993 (Zirikly et al., 2019) training examples. the order of 1000 over 10s of layers (Liu et al., Such sizes all but rules out the increasingly popular 2019; Gururangan et al., 2020). In total such mod- approach of fine-tuning transformers whereby all els typically have from hundreds of millions (De- its millions of parameters are allowed to be updated vlin et al., 2019) to a few billion parameters (Raffel toward the specific task one is trying to achieve (Deet al., 2020). However, the size of such models vlin et al., 2019; Mayfield and Black, 2020). Represents a challenge for tasks involving small num- cent research not"
C12-1148,S10-1013,0,0.0410594,"Missing"
C12-1148,P10-1089,0,0.0514466,"the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than those we find with selectors. Our approach utilizes web-scale N-grams, a source of unlabeled data which has previously been used for many other supervised lexico-semantic tasks including delimiting named entities, preposition selection, spelling correction, search query processing, adjective ordering, verb POS disambiguation, and noun compound bracketing (Downey et al., 2007; Bergsma et al., 2009; Huang et al., 2010; Bergsma et al., 2010). All of these systems utilized n-grams to find frequency information for specific n-grams. In contrast, we use the n-grams as a source for acquiring sets of lexical data (selectors), where we search with context and ask for the missing piece rather than search for a complete n-grams. We believe this is the first work to use web-scale N-grams as a source for selectors; motivation for using this source is discussed in the next section. 3 Acquiring Selectors A selector is a word which appears in the same local context as a given instance of a focus word. For example, in the sentence below, with"
C12-1148,J92-4003,0,0.212205,"es (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training d"
C12-1148,D07-1108,0,0.0285815,"ta and to examine if selectors help for another lexico-semantic task: named-entity classification. Details about the OntoNotes test sets are included when discussing those results (sections 4.3.4 and 4.3.5). 4.2 Baseline Features As a consistent baseline throughout our experiments, we use the same features as Zhong et al. (2008)’s state-of-the-art system, first explored by Lee and Ng (2002). These features give the best published results that we are aware of over the Wall Street Journal portion of OntoNotes, plus they are the common denominator in many high-performance supervised WSD systems (Cai et al., 2007; Chan et al., 2007; Zhong et al., 2008). • collocations (coll). Tokens relative to the target, denoted ci, j , starting at i; ending at j. 1-grams: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , 2-grams: c−2,−1 , c+1,+2 , 3-grams: c−3,−1 , c+1,+3 , c−1,+1 , 4-grams: c−2,+1 , c−1,+2 • parts-of-speech (pos). The part-of-speech for the following words relative to the target word: p−3 , p−2 , p−1 , p0 , p+1 , p+2 , p+3 (0 is the target word). • surrounding words (surr). The bag-of-words from the current, previous, and next sentence. 4.3 Results 4.3.1 SemEval-2007 Table 2 shows the results with and without s"
C12-1148,S07-1054,0,0.153222,"t onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar c"
C12-1148,P08-2008,0,0.0224664,"selectors function as an abstraction of word instance context rather than as a list of semantically similar words. Our current goal is to get the most out of supervised training data by leveraging unannotated data via selectors (no use of a knowledge-base or similarity metrics). Consequently, 2425 our system achieves state-of-the-art results in line with top supervised systems while our earlier knowledge-based approaches produce results in line with systems not utilizing training data. A couple previous works have integrated unannotated data as features into supervised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probab"
C12-1148,N09-1037,0,0.0149837,"078(test) For the W SJ we stick with standard training and test sets, while we divide X h and S r corpora similarly. Out of the 1,000 randomly selected sentences across these corpora there are 2,106 total named entity instances: 1,847 training examples and 259 test examples. We find this to be a representative sample of the W SJ, X h, and S r portions of OntoNotes 5 . We choose our features by looking at the most common types of features used during the CoNLL-2003 Shared Task in Named Entity Recognition(Tjong Kim Sang and De Meulder, 2003), and more recent developments(Ratinov and Roth, 2009; Finkel and Manning, 2009). To the best of our knowledge state-of-the-art features have not been established for labeling all classes of Named Entities in OntoNotes, though Finkel and Manning use the three most common classes and group the others into a misc category. • character n-grams. Character sequences of length 1 to 6. • case information. Case of the first, second, and last letter, as well as an indicator for punctuation. • lexical information. The target word, its base form, as well as the same collocations used in WSD: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , c−2,−1 , c+1,+2 , c−3,−1 , c+1,+3 , c−1,+1 , c−2,+1 , co"
C12-1148,W02-1006,0,0.0405918,"adjectives, fine-grained senses, and difference in corpus gives us a more robust evaluation of selectors. Lastly, we experiment with random samples over portions of the full Ontonotes 4.0 in order to test on out-of-domain data and to examine if selectors help for another lexico-semantic task: named-entity classification. Details about the OntoNotes test sets are included when discussing those results (sections 4.3.4 and 4.3.5). 4.2 Baseline Features As a consistent baseline throughout our experiments, we use the same features as Zhong et al. (2008)’s state-of-the-art system, first explored by Lee and Ng (2002). These features give the best published results that we are aware of over the Wall Street Journal portion of OntoNotes, plus they are the common denominator in many high-performance supervised WSD systems (Cai et al., 2007; Chan et al., 2007; Zhong et al., 2008). • collocations (coll). Tokens relative to the target, denoted ci, j , starting at i; ending at j. 1-grams: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , 2-grams: c−2,−1 , c+1,+2 , 3-grams: c−3,−1 , c+1,+3 , c−1,+1 , 4-grams: c−2,+1 , c−1,+2 • parts-of-speech (pos). The part-of-speech for the following words relative to the target word: p−3 , p"
C12-1148,P97-1009,0,0.619247,"3), it is difficult to connect any two instances based on local context; the parts-of-speech even differ substantially. Models for disambiguation can benefit from the addition of a feature that does not rely directly on the local context. We present a new class of features which encodes an abstraction of a word’s context, rather than encoding contents of the local context itself. We refer to this new feature class as selectors, borrowing the term from an approach to knowledge-based (unsupervised) word sense disambiguation which uses the idea of searching for words that share the same context (Lin, 1997; Schwartz and Gomez, 2008). More precisely, selectors are words that show up in the same local context as a given instance of another word. For example, selectors for ‘port’ in sentence 1 might be ‘bottles’, ‘crates’, ‘passengers’, ‘wine’, ‘luggage’, etc. Considering that the other sentences may share some of the same selectors such as ‘bottles’ or ‘wine’, one can see how this abstraction of context to selectors can be beneficial. Figure 1 demonstrates mapping the context from one instance to selectors, which match the selectors of another instance. In this sense, it is the contexts (or word"
C12-1148,P98-2127,0,0.160335,"1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency re"
C12-1148,lin-etal-2010-new,0,0.0254235,"loaded the port onto the ship last night. More formally, for a given word instance, w i , selectors are found based on the particular context of w i . What defines the context may vary from syntactic or dependency relations (i.e., other nouns which are objects of the verb ‘loaded’) to simple sequences of tokens (e.g.,finding words that fill in the blank in “The workers loaded the ___ onto the ship last night.”). 3.1 Approach We find selectors by searching for sequences of tokens in the Google N-grams version 2, which contains 4.1 billion n-grams that were automatically part-of-speech tagged (Lin et al., 2010). The primary reason we chose web-scale N-grams as a source is because it has become difficult to get selectors via search engines.1 Still, using web-scale n-grams for context searches has advantages: there is a decent likelihood of finding selectors for a given instance, the search 1 The Web search engines which support wildcard queries no longer run public APIs or allow scripted access. 2426 Workers loaded the port onto the ship last night. workers loaded 〈det〉? (〈noun〉+) onto loaded 〈det〉? (〈noun〉+) onto (〈noun〉+) onto 〈d et〉 ship last My objective was to fight as a mother for what I hold d"
C12-1148,P04-1036,0,0.042276,"e with systems not utilizing training data. A couple previous works have integrated unannotated data as features into supervised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probability distributions of substitutes being within the same context as target instances. Unlike selectors which are open-ended, substitutes were chosen from an a priori word list derived from thesauri, and contextual part-of-speech was not considered. Additionally, the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than tho"
C12-1148,W04-2405,0,0.0294378,"ing actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1"
C12-1148,N07-1025,0,0.0296414,"on 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are"
C12-1148,W04-0807,0,0.27123,"data. Because selectors leverage unlabeled data, their inclusion in a supervised system constitutes semi-supervised learning. The paper proceeds with a discussion of related work in semi-supervised WSD and the use of web-scale data in language processing (Section 2). Then, we present our approach to acquiring selectors as features from n-grams, and show how we translate selectors into features (Section 3). The effectiveness of selectors is evaluated within supervised word sense disambiguation classifiers over the SemEval-2007 Task 17 (Pradhan et al., 2007), Senseval 3 English Lexical Sample (Mihalcea et al., 2004), and OntoNotes 4 (Weischedel et al., 2011) (Section 4). We also test selectors as features for the classification step of named-entity recognition over a representative sample of OntoNotes. Lastly, we discuss the robustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selector"
C12-1148,S07-1006,0,0.0322964,"tion of selectors and we record a simple accuracy of |al l_inst ances |∗ 100 of the testing data.3 In particular, we use support vector classifiers implemented with Scikit-learn (Pedregosa et al., 2011) with a radial basis kernel and other parameters set via 5-fold crossvalidation over the training set. As a standard point of comparison, most frequent sense (M F S) accuracy is also reported, indicating the testing accuracy if the system always predicted the most common sense according to the training data. As often noted, state-of-the-art supervised systems often perform just above the M F S (Navigli et al., 2007; Pradhan et al., 2007). 2 An implementation of this method is included in supplementary data. accur ac y = pr ecision = r ecal l under the standard (SemEval) definition of precision and recall for W SD, and because we attempt all instances of our samples. 3 2428 4.1 Data Sets We test selectors over three sense-annotated corpora. For our primary corpus, we use the SemEval-2007 Task 17: Lexical Sample (Pradhan et al., 2007) (results in sections 4.3.1 and 4.3.3). This corpus is an early selection from the Wall Street Journal portion of OntoNotes (Weischedel et al., 2011), and contains coarse-gra"
C12-1148,P93-1024,0,0.531211,"Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), de"
C12-1148,S07-1016,0,0.236361,"tch against an orders-of-magnitude-larger unlabeled set of data. Because selectors leverage unlabeled data, their inclusion in a supervised system constitutes semi-supervised learning. The paper proceeds with a discussion of related work in semi-supervised WSD and the use of web-scale data in language processing (Section 2). Then, we present our approach to acquiring selectors as features from n-grams, and show how we translate selectors into features (Section 3). The effectiveness of selectors is evaluated within supervised word sense disambiguation classifiers over the SemEval-2007 Task 17 (Pradhan et al., 2007), Senseval 3 English Lexical Sample (Mihalcea et al., 2004), and OntoNotes 4 (Weischedel et al., 2011) (Section 4). We also test selectors as features for the classification step of named-entity recognition over a representative sample of OntoNotes. Lastly, we discuss the robustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: W"
C12-1148,W09-1119,0,0.0514657,"rain); sections 1060 - 1078(test) For the W SJ we stick with standard training and test sets, while we divide X h and S r corpora similarly. Out of the 1,000 randomly selected sentences across these corpora there are 2,106 total named entity instances: 1,847 training examples and 259 test examples. We find this to be a representative sample of the W SJ, X h, and S r portions of OntoNotes 5 . We choose our features by looking at the most common types of features used during the CoNLL-2003 Shared Task in Named Entity Recognition(Tjong Kim Sang and De Meulder, 2003), and more recent developments(Ratinov and Roth, 2009; Finkel and Manning, 2009). To the best of our knowledge state-of-the-art features have not been established for labeling all classes of Named Entities in OntoNotes, though Finkel and Manning use the three most common classes and group the others into a misc category. • character n-grams. Character sequences of length 1 to 6. • case information. Case of the first, second, and last letter, as well as an indicator for punctuation. • lexical information. The target word, its base form, as well as the same collocations used in WSD: c−1,−1 , c+1,+1 , c−2,−2 , c+2,+2 , c−2,−1 , c+1,+2 , c−3,−1 , c+"
C12-1148,W97-0209,0,0.166468,". This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency relationships over a small corpus were used to find noun selectors. We previously extended this to the Web, treating context as surrounding text and introduced the ideas of acquiring selectors for additional parts-of-speech as well as for words in context in addition to the target word (Schwartz and Gomez, 2008, 2009). Similar to selectional preferences (Resnik, 1997), selectors essentially indicate the types of concepts expected in a given syntactic or grammatical position. In these knowledge-based approaches, disambiguation is performed by computing the semantic distance between selectors and senses of the target word. These approaches rely on both a knowledge source such as WordNet (Miller et al., 1993) and a semantic distance metric. In contrast, in the current approach we do not need such a knowledge source or similarity judgments, and since our approach is data-driven, selectors function as an abstraction of word instance context rather than as a lis"
C12-1148,J98-1004,0,0.297808,"lcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992; Pereira et al., 1993; Lin, 1998; Schütze, 1998; Pantel and Lin, 2002)). Distributional clusters are made up of words that appear in similar contexts to each other, whereas selectors are words which show up in the specific context of a single instance. In other words, selectors are instance-specific while distributional clusters are created based on observing many instances of context. This key difference should become more clear when we present our method of acquiring selectors. The traditional use of selectors is in knowledge-based word sense disambiguation systems, not utilizing training data. In Lin (1997), dependency relationships ove"
C12-1148,W08-2114,1,0.92865,"ifficult to connect any two instances based on local context; the parts-of-speech even differ substantially. Models for disambiguation can benefit from the addition of a feature that does not rely directly on the local context. We present a new class of features which encodes an abstraction of a word’s context, rather than encoding contents of the local context itself. We refer to this new feature class as selectors, borrowing the term from an approach to knowledge-based (unsupervised) word sense disambiguation which uses the idea of searching for words that share the same context (Lin, 1997; Schwartz and Gomez, 2008). More precisely, selectors are words that show up in the same local context as a given instance of another word. For example, selectors for ‘port’ in sentence 1 might be ‘bottles’, ‘crates’, ‘passengers’, ‘wine’, ‘luggage’, etc. Considering that the other sentences may share some of the same selectors such as ‘bottles’ or ‘wine’, one can see how this abstraction of context to selectors can be beneficial. Figure 1 demonstrates mapping the context from one instance to selectors, which match the selectors of another instance. In this sense, it is the contexts (or word instances) that have select"
C12-1148,W09-2405,1,0.894124,"Missing"
C12-1148,W03-0419,0,0.187378,"Missing"
C12-1148,P95-1026,0,0.398462,"obustness of selectors as features by inspecting actual instances from our experimental corpus (Section 5). 2424 The workers loaded the port onto the ship this morning. beer crates luggage beer supplies wine wine Gatoraid cough_syrup medicine ... medicine ... She purchased a couple bottles of port from the store. Figure 1: Word instances which do not share context can share selectors. 2 Related Work The idea of improving a supervised classifier by utilizing unlabeled data has been investigated at different levels. For example, other approaches to disambiguation have used bootstrapped samples (Yarowsky, 1995; Mihalcea and Moldovan, 1999; Mihalcea, 2004; Pham et al., 2005), Wikipedia concepts (Mihalcea, 2007), or parallel corpora (Chan et al., 2007). Most of these approaches, which are considered semi-supervised learning (Zhu, 2008), exploit some facet of unannotated text to collect more training instances. Rather than produce more training instances, we introduce a method to leverage massive unlabeled corpora to create a richer and robust set of features. One can contrast selectors with clusters of words formed via context or distributional similarity (for seminal examples see (Brown et al., 1992"
C12-1148,S07-1044,0,0.0255207,"vised disambiguation systems. Dligach and Palmer (2008) used dynamic dependency neighbors, a feature encoding verbs with the same object, according to a dependency parsed corpus, as a given target verb in a verb WSD task. Besides our method not being limited to verbs, selectors are much more specific than dependency neighbors; They are found by matching a larger context and from a much larger, web-scale, dataset. Cárcamo et al. (2008) adapt the predominant sense method of McCarthy et al. (2004) to find the best sense choice for a word instance rather than it’s most common sense over a corpus. Yuret (2007) leveraged web-scale data to acquire probability distributions of substitutes being within the same context as target instances. Unlike selectors which are open-ended, substitutes were chosen from an a priori word list derived from thesauri, and contextual part-of-speech was not considered. Additionally, the substitutes’ probability distribution itself was the entire feature set, rather than used to supplement an existing feature set, and the resulting accuracies were lower than those we find with selectors. Our approach utilizes web-scale N-grams, a source of unlabeled data which has previous"
C12-1148,D08-1105,0,0.342076,"cluding selectors for the classification step of named-entity recognition over a representative sample of OntoNotes. These significant improvements come free of any human annotation cost, only requiring unlabeled Web-Scale corpora. KEYWORDS: word sense disambiguation, lexical semantics, semi-supervised learning. Proceedings of COLING 2012: Technical Papers, pages 2423–2440, COLING 2012, Mumbai, December 2012. 2423 1 Introduction Supervised word sense disambiguation (WSD) systems often rely directly on the local contexts in which target words appear. For example, the state-of-the-art system of Zhong et al. (2008) uses features based on collocations centered on the target word. Models relying on such features do well with copious amounts of training data, but they are prone to errors when the local context of a test instance differs from local context observed during training. Consider the sentences below. 1. The workers loaded the port onto the ship this morning. 2. She purchased a couple of bottles of port from the store. 3. The couple enjoyed their richly-flavored port. Though referring to the same sense of ‘port’, “a sweet dark-red dessert wine” (Miller et al., 1993), it is difficult to connect any"
C12-1148,S07-1053,0,\N,Missing
C12-1148,C98-2122,0,\N,Missing
C12-2053,W10-0731,0,0.299246,"what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based microtask marketplace where the workers (called “Turkers”) do simple, one-off tasks (called “human intelligence tasks” or “HITs”), for small payments. See Snow et al. (2008); Callison-Burch (2010); and Akkaya et al. (2010) for MTurk’s use in NLP, and Chandler and Kapelner (2010) and Mason and Suri (2011) for further reading on MTurk as a research platform. We performed the first extensive look at coarse-grained WSD on MTurk. We studied a large and variegated set of words: 1,000 contextual examples of 89 distinct words annotated by 10 unique Turkers each. In the closest related literature, Snow et al. (2008) found high Turker annotation accuracy but only annotated a single word, while Passonneau et al. (2011) focused on only a few words and annotated fine-grained senses. The extensive size of our study lends its"
C12-2053,brown-etal-2010-number,0,0.0161488,"uality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqualified 9 of the 1,000 because they had words with only one sense. 540 target word affect-v allow-v announce-v approve-v area-n ask-v attempt-v authority-n avoid-v base-n begin-v believe-v bill-n build-v buy-v capital-n care-v carrier-n chance-n claim-v come-v comp"
C12-2053,W10-0701,0,0.0129617,". Why is WSD difficult and what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based microtask marketplace where the workers (called “Turkers”) do simple, one-off tasks (called “human intelligence tasks” or “HITs”), for small payments. See Snow et al. (2008); Callison-Burch (2010); and Akkaya et al. (2010) for MTurk’s use in NLP, and Chandler and Kapelner (2010) and Mason and Suri (2011) for further reading on MTurk as a research platform. We performed the first extensive look at coarse-grained WSD on MTurk. We studied a large and variegated set of words: 1,000 contextual examples of 89 distinct words annotated by 10 unique Turkers each. In the closest related literature, Snow et al. (2008) found high Turker annotation accuracy but only annotated a single word, while Passonneau et al. (2011) focused on only a few words and annotated fine-grained senses. The extensive s"
C12-2053,W97-0206,0,0.0424948,"the task was found readily on the homepage which drove the rapid completion. 3 As Kapelner and Chandler (2010) found, this accomplishes three things: (1) Turkers who plan on cheating will be more likely to leave our task, (2) Turkers will spend more time on the task and, most importantly, (3) Turkers will more carefully read and concentrate on the meaning of the text. 541 Figure 1: An example of the WSD task that appears inside an MTurk HIT. This was displayed piecewise as each word in the example (""snippet"") and senses faded-in slowly. the senses in descending frequency order as observed by Fellbaum et al. (1997). We also limited participation to US Turkers to encourage fluency in English. Upon completion, the Turker was given an option to do another of our WSD tasks (this is the MTurk default). A Turker was not limited in the number of annotations they could do.4 The entire study took 51 hours and cost $110. The code, raw data, and analysis scripts are available under GPL2 at github.com/kapelner/wordturk_pilot. 3 Results and data analysis We were interested in investigating (1) which features in the target word, the context, and sense definition text affect Turker accuracy, (2) which characteristics"
C12-2053,N06-2015,0,0.0415799,"rating that Turkers are respectably accurate (Snow et al., 2008), they’re approximately equal in ability (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqua"
C12-2053,J98-1001,0,0.0248244,"; and (c) spending more time is associated with a decrease in accuracy. We also observe that all participants are about equal in ability, practice (without feedback) does not seem to lead to improvement, and that having many participants label the same example provides a partial substitute for more expensive annotation. KEYWORDS: Word sense disambiguation, crowdsourcing. Proceedings of COLING 2012: Posters, pages 539–548, COLING 2012, Mumbai, December 2012. 539 1 Introduction Word sense disambiguation (WSD) is the process of identifying the meaning, or “sense,” of a word in a written context (Ide and Véronis, 1998). In his comprehensive survey, Navigli (2009) considers WSD an AI-complete problem — a task which is at least as hard as the most difficult problems in artificial intelligence. Why is WSD difficult and what is driving its difficulty? This study examines human WSD performance and tries to identify drivers of accuracy. We hope that our findings can be incorporated into future WSD systems. To examine human WSD performance, we tap pools of anonymous untrained human labor; this is known as “crowdsourcing.” A thriving pool of crowdsourced labor is Amazon’s Mechanical Turk (MTurk), an Internet-based"
C12-2053,W10-0703,0,0.0190085,"factors affecting annotator accuracy. Our contribution is three-fold. First, we use regression to identify a variety of factors that drive accuracy such as (a) the number of rephrasings within a sense definition is associated with higher accuracy; (b) as word frequency increases, accuracy decreases even if the number of senses is kept constant; and (c) time-spent on an annotation is associated with lower accuracy. Second, we echo previous findings, mostly from non-WSD experiments, demonstrating that Turkers are respectably accurate (Snow et al., 2008), they’re approximately equal in ability (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 200"
C12-2053,S07-1016,0,0.0260694,"ility (Parent, 2010; Passonneau et al., 2011), spam is virtually non-existent (Akkaya et al., 2010), responses from multiple Turkers can be pooled to achieve high quality results (Snow et al., 2008; Akkaya et al., 2010), and that workers do not improve with experience (Akkaya et al., 2010). Third, we present a system of crowdsourcing WSD boasting a throughput of about 5,000 disambiguations per day at $0.011 per annotation. 2 Methods and data collection We selected a subset of the OntoNotes data (Hovy et al., 2006), the SemEval-2007 coarse-grained English Lexical Sample WSD task training data (Pradhan et al., 2007). The coarse-grained senses in OntoNotes address a concern that nuanced differences in sense inventories drives disagreement among annotators (Brown et al., 2010). We picked 1,000 contextual examples at random from the full set of 22,281.1 Our sample is detailed in table 1. It consisted of 590 nouns and 410 verb examples that had between 2-15 senses each (nouns: 5.7 ± 3.0 senses, verbs: 4.7 ± 3.3 senses). For each snippet, ten annotations were completed by ten unique Turkers. 1 We later disqualified 9 of the 1,000 because they had words with only one sense. 540 target word affect-v allow-v ann"
C12-2053,D08-1027,0,0.359862,"Missing"
D16-1217,abdul-mageed-diab-2014-sana,1,0.828748,"MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the p"
D16-1217,W12-3709,0,0.0172632,"g camp is developing methods to estimate personality and emotion, asking “how does she feel?” rather than “how much does she like the product?” (Mohammad and Kiritchenko, 2015; Park et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-spec"
D16-1217,D08-1014,0,0.0225301,"s well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields"
D16-1217,P12-3005,0,0.0677526,"Missing"
D16-1217,P07-1123,0,0.096535,"rk et al., 2014). In social media, the well-being of individuals as well as communities has been studied, on various platforms such as Facebook and Twitter (Bollen et al., 2011; Schwartz et al., 2013; Eichstaedt et al., 2015; Schwartz et al., 2016). 2.1 Translating sentiment Past work has, on the whole, regarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic,"
D16-1217,N15-1078,0,0.0358916,"t languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015). It is less obvious how well expressions of emotion or subjective well-being translate between languages and cultures; the words for liking a phone or TV may be more similar across cultures than the ones for finding life and relationships satisfying, or work meaningful and engaging. 2.2 Well-being In contrast to classic sentiment analysis, well-being is not restricted to positive and negative emotion. In 2011, the psychologist Martin Selig2043 man proposed PERMA (Seligman, 2011), a fivedimensional model of well-being where ‘P’ stands for positive emotion, ‘E’ is engagement, ‘R’ is positive re"
D16-1217,D14-1121,1,0.829336,"irani, 1996) models to predict the average annotation score for each of the crowdsourced PERMA labels. Separate models, each consisting of regression weights for each term in the lexicon, were built for each of the ten (five positive and five negative) PERMA components in both English and Spanish1 . Each model was validated using 10-fold cross validation, with Pearson correlations averaged over the 10 positive/negative PERMA components. Re1 Available at www.wwbp.org. 2044 sults are presented in Table 1. The models were then transformed into a predictive lexicon using the methods described in (Sap et al., 2014), where the weights in the lexicon were derived from the above Lasso regression model. Model Spanish English r 0.36 0.36 Table 1: Performance as measured by Pearson r correlation averaged over the 10 positive/negative PERMA components using 10-fold cross validation. 3.3 Translating the models We used Google Translate to translate both the original English and Spanish Tweets and the words in the models. We also created versions of the translated models in which we manually corrected apparent translation errors for 25 terms with the largest regression coefficients for each of the 10 PERMA compon"
D16-1217,D08-1058,0,0.0554644,"egarded state-of-theart automatic translation for sentiment analysis optimistically. In assessing statistical MT, (Balahur and Turchi, 2012) found that modern SMT systems can produce reliable training data for languages other than English. Comparative evaluations between English and Romanian (Mihalcea et al., 2007) and English and both Spanish and Romanian (Banea et al., 2008) based on the English MPQA sentiment data suggest that, in spite of word ambiguity in either the source or target language, automatic translation is a viable alternative to the construction of models in target languages. Wan (2008) shows that it is useful to improve a system in a target language (Chinese) by applying ensemble methods exploiting sentiment-specific data and lexica from the target language and a source language (English). More recent work has examined how sentiment changes with translation between English and Arabic, also finding that automatic translation of English texts yields competitive results (Abdul-Mageed and Diab, 2014; Mohammad et al., 2016). However, translated texts tend to lose sentiment information such that the translated data is more neutral than the source language (Salameh et al., 2015)."
D16-1219,W14-3214,1,0.845748,"5.4 million status updates. All users completed a 100-item personality questionnaire (an International Personality Item Pool (IPIP) proxy to the NEO-PI-R (Goldberg, 1999). User-level degree of depression (DDep) was estimated as the average response to seven depression facet items (nested within the larger Neuroti2 To quantify the pervasiveness of pronoun studies in social science, we consider the citation count, via Google Scholar (July, 2016), to works mentioning “pronoun” by one of the top researchers, James W. Pennebaker, which number over 10,000. 2055 cism item pool of the questionnaire) (Schwartz et al., 2014). Dependency Features. We used dependency annotations in order to determine the syntactic function of personal pronouns i.e. subject (S) and direct object (DO). We obtained dependency parses of our corpus using Stanford Parser (Socher et al., 2013) that provides universal dependencies in (relation, head, dependent) triples. In the next step, we extracted the words in in the nominal subject (“nsubj&quot;) and direct object (“dobj&quot;) positions including nsubj 1st-person singular pronoun “I&quot;, and dobj 1st-person singular pronoun “me&quot;. We also extracted the corresponding verbs for each of the nominal su"
D16-1219,P13-1045,0,0.0225755,"o seven depression facet items (nested within the larger Neuroti2 To quantify the pervasiveness of pronoun studies in social science, we consider the citation count, via Google Scholar (July, 2016), to works mentioning “pronoun” by one of the top researchers, James W. Pennebaker, which number over 10,000. 2055 cism item pool of the questionnaire) (Schwartz et al., 2014). Dependency Features. We used dependency annotations in order to determine the syntactic function of personal pronouns i.e. subject (S) and direct object (DO). We obtained dependency parses of our corpus using Stanford Parser (Socher et al., 2013) that provides universal dependencies in (relation, head, dependent) triples. In the next step, we extracted the words in in the nominal subject (“nsubj&quot;) and direct object (“dobj&quot;) positions including nsubj 1st-person singular pronoun “I&quot;, and dobj 1st-person singular pronoun “me&quot;. We also extracted the corresponding verbs for each of the nominal subjects, and direct object words. Verb categorization. In order to integrate the verbal semantic categories in the syntactic analysis of pronouns, we utilize two verb categorization methods (a) linguistically-driven Levin’s Verb Classes, and (b) emp"
D17-1119,P14-2134,0,0.0968071,"Missing"
D17-1119,Q14-1043,0,0.0948479,"Missing"
D17-1119,P07-1033,0,0.361838,"Missing"
D17-1119,P15-1073,0,0.264689,"Missing"
D17-1119,P15-2079,0,0.604861,"Missing"
D17-1119,K15-1011,0,0.115731,"Missing"
D17-1119,W15-2905,0,0.0998029,"Missing"
D17-1119,D14-1108,0,0.113554,"Missing"
D17-1119,D15-1130,0,0.0625841,"Missing"
D17-1119,S16-1003,0,0.0699059,"Missing"
D17-1119,S13-2053,0,0.0807243,"Missing"
D17-1119,D14-1121,0,0.135909,"Missing"
D17-1119,D13-1187,0,0.344453,"Missing"
D17-1119,N13-1039,0,0.100413,"Missing"
D17-1119,S13-2052,0,\N,Missing
D17-1250,P07-1053,0,0.0401273,"ength of comment to be the dominant predictor, with other features providing minimal benefit. However, a few studies (including our own) have found this baseline can be overcome. For example, Racherla and Friske (Racherla and Friske, 2012) investigated perceived usefulness of consumer reviews on Yelp and found that reputation and expertise were more important than total number of words on perceived usefulness. All of these prior works focus on assessing subjective aspects of comments (usefulness, quality, and helpfulness); Perhaps the study coming closest in spirit to our own was Ghose et al. Ghose et al. (2007) who quantified quality of reviews by the economic change they produced. However, they still were not dealing with a randomized experiment and so conclusions were correlational and the objective was better sales of the product rather than benefit to the reader (i.e. leading to a better decision). 6 Conclusion Our results suggest three key findings. First, what one writes in a comment is more important than simply how much one writes; this is true across both subjective and objective outcomes, though length had virtually no predictive ability for improving forecaster accuracy. Second, we found"
D17-1250,D07-1035,0,0.0594546,"al rater biases; for example, forecasters are subjectively biased in favor of recommendations mentioning business deals and material things, even though such recommendations do not indeed prove any more useful objectively. 1 Introduction Finding good recommendations is an integral part of a modern information-seeking life – from purchasing products based on reviews to finding answers to baffling questions. Following the tradition of sentiment analysis, many have proposed methods to automatically assess the quality of recommendations or comments based on subjective ratings of their usefulness (Liu et al., 2007; Siersdorfer et al., 2010; Becker et al., 2012; Momeni et al., 2013) or of persuasiveness (Wei et al., 2016). However, information thought to be useful does not always prove so, and subjective ratings may be driven by biases. Reviews which convince you to watch a movie or buy a product do not guarantee that you will enjoy the product, and the most convincing or highest rated answers to questions on sites like Stack Overflow or Yahoo Answers are not always the most accurate. We explore recommendations in a unique dataset, an online forecasting competition, which offers a rare glimpse into both"
D17-1250,J11-2003,0,0.0337835,"best to include such noisy data when training, it does not provide a very accurate assessment. Therefore, we use dedicated training and test sets, where the test set is a random sample of 500 comments with more than 3 updates and thus a more reliable mean change in forecaster accuracy. As a baseline, we use the square-root of the number of words in the comment. This may seem like weak measure of quality, but the history of automatic quality assessment is saturated with findings that length is the best predictor of quality. This holds true for both answers to questions (Agichtein et al., 2008; Surdeanu et al., 2011; Beygelzimer et al., 2015); as well as e-commerce reviews (Cao et al., 2011; Racherla and Friske, 2012). Of course, length is not as shallow as it may seem at first; given no strong incentive for authors to leave long comments, length is likely a proxy for thoroughness of the comment. Still, because we would like to understand the content distinguishing various metrics of quality, we view length as a baseline to move beyond. Table 2 compares the accuracy of models built on content (ngrams, parts-of-speech, and concepts) to the baseline of length. In all cases, our models, based on content, pe"
D17-1250,P16-2032,0,0.0878527,"iness deals and material things, even though such recommendations do not indeed prove any more useful objectively. 1 Introduction Finding good recommendations is an integral part of a modern information-seeking life – from purchasing products based on reviews to finding answers to baffling questions. Following the tradition of sentiment analysis, many have proposed methods to automatically assess the quality of recommendations or comments based on subjective ratings of their usefulness (Liu et al., 2007; Siersdorfer et al., 2010; Becker et al., 2012; Momeni et al., 2013) or of persuasiveness (Wei et al., 2016). However, information thought to be useful does not always prove so, and subjective ratings may be driven by biases. Reviews which convince you to watch a movie or buy a product do not guarantee that you will enjoy the product, and the most convincing or highest rated answers to questions on sites like Stack Overflow or Yahoo Answers are not always the most accurate. We explore recommendations in a unique dataset, an online forecasting competition, which offers a rare glimpse into both subjective and objective quality. In this competition, the users (forecasters) had a measurable goal — to fo"
D17-1250,D17-2010,1,0.819689,"ugh typically not over thousands of potential independent variables as we do here. Therefore, we also correct for multiple hypotheses by using the Benjamini-Hochberg procedure (Benjamini and Hochberg, 1995) over our significance tests. Differential language analysis allows us to observe and test the unique relationship between each feature and each metric, holding length constant. In addition, we use the difference between standardized metric scores to find the features that distinguish high quality comments in one metric versus another. All methods were implemented within the package, dlatk (Schwartz et al., 2017). 4.2 Quality Comment Features Figure 2 shows the n-grams most highly correlated with each of our quality metrics. Size indicates correlation strength while color represents overall frequency. Across both subjective ratings and objective update rates, we see discussion of news plays an important role (e.g. “news”, “article”, and “www.reuters.com”). We do not see the same from comments resulting in positive changes of forecaster accuracy (benefit), which seemed to be distinguished by negation (e.g. “no”, “unlikely”). For influence, we see other features indicating probabilistic reasoning (e.g."
D17-2010,D14-1082,0,0.0303662,"regression models in order to take in features as well as extra-linguistic information and output a continuous value predictions. These include variants on penalized linear regression: Ridge, 57 threshold which of the units of analyses are available (e.g. only include users with at least 1000 words or counties with 50,000 words). Integration of Popular Packages. DLATK sits on top of many popular open source packages used for data analysis and machine learning (scikitlearn (Pedregosa et al., 2011) and statsmodels (Seabold and Perktold, 2010)) as well as NLP specific packages (Stanford parser (Chen and Manning, 2014), TweetNLP (Gimpel et al., 2011) and NLTK (Loper and Bird, 2002)). LDA topics can be created with the Mallet (McCallum, 2002) interface. After creation these topics can then be used downstream in any standard DLATK analysis pipeline. The pip and conda package management systems control python library dependencies. (a) Age (pos) (b) Age (neg) (c) Educator (d) Technology Worker Figure 2: 1- to 3-grams correlated with age and occupation class. or language categories, such as ‘positive emotion’ or references to work and occupational terms). According to citations, the most popular tool is Linguist"
D17-2010,W16-0404,1,0.403608,"Missing"
D17-2010,W15-1205,1,0.858201,"Missing"
D17-2010,D14-1121,1,0.298185,".42 Agreeableness R = 0.35 Neuroticism R = 0.35 Temporal orientation (message-level) 3-way classif Acc = 0.72 Intensity & affect (message-level) Intensity R = 0.85 Affect R = 0.65 Mental health (user-level) PTSD AUC = 0.86 Depression AUC = 0.87 Degree of dprssn R = 0.39 Physical health (US county-level) Heart disease morR = 0.42 tality graphic or socioeconomic baselines. 6 Evaluations DLATK has been used as a data analysis platform in over 30 peer-reviewed publications, with venues ranging from general-interest (PLoS ONE: Schwartz et al., 2013b) to computer science methods proceedings (EMNLP: Sap et al., 2014) to psychology journals (JPSP: Park et al., 2015). The most straightforward use for DLATK is to provide insight on linguistic features associated with a given outcome, the differential language analyses presented in Schwartz et al. (2013b). Other works to primarily use DLATK for correlation-type analyses examine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Tab"
D17-2010,W14-3214,1,0.769622,"xamine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Table 1 summarizes some predictive models reported in peer-reviewed publications. DLATK works to create models at multiple scales, i.e., for predicting aspects of single messages (e.g., tweet-wise temporal orientation; Schwartz et al., 2015), or predicting user-level attributes (e.g., severity of depression; Schwartz et al., 2014), or predicting community-level health outcomes (e.g., heart disease mortality; Eichstaedt et al., 2015). 7 Source Sap et al. (2014) Park et al. (2015) Schwartz et al. (2015) Preot¸iuc-Pietro et al. (2016) Preot¸iuc-Pietro et al. (2015) Schwartz et al. (2014) Eichstaedt et al. (2015) Table 1: Survey of predictive model scores trained using DLATK in peer-reviewed publications. Scores reported are: R: Pearson correlation; Acc: accuracy; AUC: area under the ROC curve. Thomas Apicella, Masoud Rouhizadeh, Daniel Rieman, Selah Lynch and Daniel Preot¸iuc-Pietro. References Alan Agresti and Barbara Fi"
D17-2010,S13-1042,1,0.8385,"gression where, by assuming a dichotomous outcome, statistical significance tests are usually more accurate (Menard, 2002). Where controls are not needed, there are many other options, often less computationally complex, such as TF-IDF, Informative Dirichlet Prior,3 or classification accuracy metrics like Differential Language Analyses The prototypical use of DLATK is to perform differential language analysis – the identification of linguistic features which either (a) independently explain the most variance for continuous outcomes or (b) are individually most predictive of discrete outcomes (Schwartz et al., 2013b). Unlike predictive techniques where one seeks to produce outcome(s) given language (discussed next), here, the 2 Even in basic prediction methods, like linear regression, the relationship between each linguistic feature and the outcome is complex – dependent on the covariance structure between all the variables. DLA works in a univariate, perfeature fashion or with a limited set of control variables (e.g. age and gender when discriminating personality). 3 Bayesian approach to log-odds (Monroe et al., 2008). 56 Lasso, Elastic-Net, as well as non-linear techniques such as Extremely Random For"
D17-2010,N15-1044,1,0.849586,"Schwartz et al. (2013b). Other works to primarily use DLATK for correlation-type analyses examine outcomes like age (Kern et al., 2014b), gendered language and stereotypes (Park et al., 2016; Carpenter et al., 2016b), and efficacy of app-based well-being interventions (Carpenter et al., 2016a). Another area one can evaluate the utility of DLATK is in building predictive models. Table 1 summarizes some predictive models reported in peer-reviewed publications. DLATK works to create models at multiple scales, i.e., for predicting aspects of single messages (e.g., tweet-wise temporal orientation; Schwartz et al., 2015), or predicting user-level attributes (e.g., severity of depression; Schwartz et al., 2014), or predicting community-level health outcomes (e.g., heart disease mortality; Eichstaedt et al., 2015). 7 Source Sap et al. (2014) Park et al. (2015) Schwartz et al. (2015) Preot¸iuc-Pietro et al. (2016) Preot¸iuc-Pietro et al. (2015) Schwartz et al. (2014) Eichstaedt et al. (2015) Table 1: Survey of predictive model scores trained using DLATK in peer-reviewed publications. Scores reported are: R: Pearson correlation; Acc: accuracy; AUC: area under the ROC curve. Thomas Apicella, Masoud Rouhizadeh, Dan"
D17-2010,D16-1217,1,0.641174,"2 regularization, as well ensemble and gradient boosting techniques such as Extremely Randomized Trees. As with regression, techniques have been setup so as to leverage extra-linguistic information effectively either as additional predictors or controls to try to “-out-predict”. 5 Notable Functionality Linguistic information Because DLATK was designed to exploit the full power of social media, a special emoticon-aware tokenizer is used while also leveraging Python’s unicode capabilities. Though not specifically designed to be language independent, DLATK has been used in one non-English study (Smith et al., 2016). Predictive Methods As with traditional NLP, many social-scientific research objectives can be framed as prediction tasks, in which a model is fit to language features to predict an outcome. DLATK implements many available regression and classification tools, supplemented with feature selection functions for refining the feature space. A wide range of feature selection techniques have been empirically refined for accurate use in regression problems. Extra-linguistic information. Most functionality in DLATK is designed with extra-linguistic, also referred to as “outcomes”, in mind. Such inform"
D17-2010,P11-2008,0,0.0182113,"Missing"
D17-2010,W02-0109,0,0.223537,"inguistic information and output a continuous value predictions. These include variants on penalized linear regression: Ridge, 57 threshold which of the units of analyses are available (e.g. only include users with at least 1000 words or counties with 50,000 words). Integration of Popular Packages. DLATK sits on top of many popular open source packages used for data analysis and machine learning (scikitlearn (Pedregosa et al., 2011) and statsmodels (Seabold and Perktold, 2010)) as well as NLP specific packages (Stanford parser (Chen and Manning, 2014), TweetNLP (Gimpel et al., 2011) and NLTK (Loper and Bird, 2002)). LDA topics can be created with the Mallet (McCallum, 2002) interface. After creation these topics can then be used downstream in any standard DLATK analysis pipeline. The pip and conda package management systems control python library dependencies. (a) Age (pos) (b) Age (neg) (c) Educator (d) Technology Worker Figure 2: 1- to 3-grams correlated with age and occupation class. or language categories, such as ‘positive emotion’ or references to work and occupational terms). According to citations, the most popular tool is Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015), foll"
D17-2010,W14-3207,0,\N,Missing
D18-1145,P16-1231,0,0.0160791,"quency distribution of the Dominance score in the Affective Norms in English Words (ANEW) (Bradley and Lang, 1999). Word2Vec Topics: We use word clusters built on top of Word2Vec, by Preot¸iuc-Pietro, Lampos and Aletras (2015). They built 200 open-ended word clusters by applying spectral clustering to the word-to-word similarity matrix from the neural embeddings Mikolov et. al (2013). Pronouns: We clustered all pronouns (except for possessives) into 1st-, 2nd-, and 3rd-person regardless of their syntactic role. 4.2 Syntax-based features We acquire dependency parses of our corpus by SyntaxNet (Andor et al., 2016) with Parsey McParseface model3 that produces universal dependencies in relation, head, dependent triples in CONLL format. We obtain subject-verb tuples (SVs) and subject-verb-object triples (SVOs) from the dependency trees. In our in-house evaluation on a random set of 100 Tweets, SyntaxNet with the Parsey McParseface model outperforms the Stanford Parser (Socher et al., 2013) on extracting SVs and SVOs from social media (P=.75, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlat"
D18-1145,D16-1066,0,0.163723,"is easy to identify, it is more challenging to label it is internally or externally controlled, with lexical features outperforming syntactic features at the task. Our findings could have important implications for studying selfexpression in social media. 1 Introduction Language is a form of action, and written occurrences constitute a performance of power and control (Fairclough, 2001). Research in natural language processing has long focused on distilling the constituents of writing that conveys authority (Danescu-Niculescu-Mizil et al., 2012), dominance (Bradley and Lang, 1999), dogmatism (Fast and Horvitz, 2016), expertise (Levy and Potts, 2015) and politeness (Danescu-Niculescu-Mizil et al., 2012). These studies have shown how authors’ use of certain lexical and syntactic patterns achieve specific rhetorical effects. In this study, we contribute to the growing literature with an analysis of how individuals express control, and compare the insights and predictive power obtained from both, lexical and syntactic features. We operationalize the key aspects of locus of control described by existing psychology theories ∗ Masoud Rouhizadeh & Kokil Jaidka co-lead this work. (Rotter, 1966) to identify an ext"
D18-1145,P18-2032,1,0.816179,"on). Internal control is often associated with causing a given event or doing something that is clearly a choice. The external control language, on the other hand, is characterized by lack of intention or awareness, or by concrete mention of being controlled by others. It is usually lined to describing an out-of-control event, or something that is not a choice Organizations and governments are attempting to better understand issues of locus of control and self-efficacy, which are closely related with physical health and job performance (Marmot et al., 1991; Harter et al., 2003). The study by (Jaidka et al., 2018b) offered to explore the relationship between locus of control (measured through surveys) and the Big 5 personality taxonomy (John and Srivastava, 1999), and touched briefly upon the content-related linguistic signals. However, the present study is more interested in comparing the lexical (aka open-vocabulary or bag-of-words) and syntactic signals of control that are embedded in language. If deployed on a large scale with the informed consent of the authors, it may allow unobtrusive, cost-effective estimates of well-being1 . Signals within the language should provide insight into the cognitiv"
D18-1145,D14-1108,0,0.035318,"Missing"
D18-1145,N13-1039,0,0.0193918,"Missing"
D18-1145,P15-1169,0,0.0768304,"Missing"
D18-1145,I17-1077,1,0.843805,"“miss” or “feel” are correlated with lack of control of the surroundings. A caveat of using language-based models is that their association with traits is correlational, not causal. Furthermore, the differences in platform affordances (Jaidka et al., 2018c) and diachronic drifts in language use over time (Jaidka et al., 2018a) imply that language models to identify control may need domain adaptation before they are applied on other corpora, language from other time periods and other social media platforms, as well as posthoc domain adaptation before they can scale to measure community traits (Rieman et al., 2017). Existing psychological theories are mainly based on self-expressed and self-perceived locus of control in questionnaires, but they may be susceptible to self-report biases. Instead, we demonstrate that some of these constructs can reliably be extracted from language samples, such as social media posts, which are unsolicited selfexpressions of control. Internal locus of control has been argued to be important for mental and physical health, and to characterize well-run (“empowered”) work teams; We hope that the model presented here can be used–with appropriate consent and privacy–for unobtrus"
D18-1145,D16-1219,1,0.860524,"5, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlatk.wwbp.org https://github.com/tensorflow/models/ tree/master/research/syntaxnet 3 Parser (Kong et al., 2014), that only provides dependency graphs and not the relations. Verb predicate features: We identify the verb classes using (a) linguistically-driven Levin’s Classes (Lev) (Levin, 1993), and (b) an in-house manual verb clustering on the most frequent 130 verb into 40 semantic classes (M). Inspired by the previous research (Rouhizadeh et al., 2016), we extract five sets of dependency-driven verb predicate features. (1) Pronouns-SVO: occurrence of 1st, 2nd, and 3rd person pronouns in the subject/object positions, (2) VerbCat-1-2-3PP occurrence of 1st, 2nd, and 3rd person pronouns in the subject/object positions of each verb category (from the Levin’s or our own verb classes), (3) VerbCat-1PP: occurrence of 1st or non-1st pronouns (i.e. self-reference ratio) in the subject/object positions of each verb category, (4) VerbCat-SVO: all words in the subject/object positions of each verb category, and (5) VerbCatall categories of all the verbs"
D18-1145,D17-2010,1,0.848028,"Missing"
D18-1145,P13-1045,0,0.015675,"(2013). Pronouns: We clustered all pronouns (except for possessives) into 1st-, 2nd-, and 3rd-person regardless of their syntactic role. 4.2 Syntax-based features We acquire dependency parses of our corpus by SyntaxNet (Andor et al., 2016) with Parsey McParseface model3 that produces universal dependencies in relation, head, dependent triples in CONLL format. We obtain subject-verb tuples (SVs) and subject-verb-object triples (SVOs) from the dependency trees. In our in-house evaluation on a random set of 100 Tweets, SyntaxNet with the Parsey McParseface model outperforms the Stanford Parser (Socher et al., 2013) on extracting SVs and SVOs from social media (P=.75, R= 68, TN Rate =.90 for the former; P=.51, R= .55, TN Rate =.80 for the latter). SyntaxNet is also a better tool for our purpose compared to the Tweebo 2 https://dlatk.wwbp.org https://github.com/tensorflow/models/ tree/master/research/syntaxnet 3 Parser (Kong et al., 2014), that only provides dependency graphs and not the relations. Verb predicate features: We identify the verb classes using (a) linguistically-driven Levin’s Classes (Lev) (Levin, 1993), and (b) an in-house manual verb clustering on the most frequent 130 verb into 40 semant"
D18-1148,W15-1201,0,0.195395,"Missing"
D18-1148,P13-1098,1,0.882773,"Missing"
D18-1148,P12-3005,0,0.0944372,"Missing"
D18-1148,W16-4320,0,0.304397,"Missing"
D18-1148,D17-2010,1,0.855028,"Missing"
D18-1372,D15-1263,0,0.156367,"nits are clear, parsers may still fail to accurately identify discourse relations since the content of social media is quite different than that of newswire which is typically used for discourse parsing. In order to overcome these difficulties of discourse relation parsing in social media, we simplify and minimize the use of syntactic parsing results and capture relations between discourse arguments, and investigate the use of a recursive neural network model (RNN). Recent work has shown that RNNs are effective for utilizing discourse structures for their downstream tasks (Ji and Smith, 2017; Bhatia et al., 2015; Wieting et al., 2015; Paulus et al., 2014), but they have yet to be directly used for discourse relation prediction in social media. We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models. We found that the SVM and random forest classifiers work better than the LSTM classifier for the causality 1 Off-the-shelf Penn Discourse Treebank (PDTB) end-toend parsers perform poorly on our Facebook causal prediction dataset (see Table 3) 2 Each discourse relation theory uses a different term for minimal discourse text spans: ‘Elementary Dis"
D18-1372,W15-4612,0,0.117179,"swire text (e.g. Wall Street Journal) from the discourse treebank and focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re3351 lation parsing approach for capturing counterfactual conditionals from tweets (Bhatia et al., 2015; Ji and Smith, 2017; Son et al., 2017). These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a"
D18-1372,P14-1048,0,0.101021,"d focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re3351 lation parsing approach for capturing counterfactual conditionals from tweets (Bhatia et al., 2015; Ji and Smith, 2017; Son et al., 2017). These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a pretrained model (not fully optimal for some parts of the given t"
D18-1372,P14-1002,0,0.132831,"the discourse treebank and focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re3351 lation parsing approach for capturing counterfactual conditionals from tweets (Bhatia et al., 2015; Ji and Smith, 2017; Son et al., 2017). These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a pretrained model (not fully optimal for so"
D18-1372,P17-1092,0,0.0805666,"when the discourse units are clear, parsers may still fail to accurately identify discourse relations since the content of social media is quite different than that of newswire which is typically used for discourse parsing. In order to overcome these difficulties of discourse relation parsing in social media, we simplify and minimize the use of syntactic parsing results and capture relations between discourse arguments, and investigate the use of a recursive neural network model (RNN). Recent work has shown that RNNs are effective for utilizing discourse structures for their downstream tasks (Ji and Smith, 2017; Bhatia et al., 2015; Wieting et al., 2015; Paulus et al., 2014), but they have yet to be directly used for discourse relation prediction in social media. We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models. We found that the SVM and random forest classifiers work better than the LSTM classifier for the causality 1 Off-the-shelf Penn Discourse Treebank (PDTB) end-toend parsers perform poorly on our Facebook causal prediction dataset (see Table 3) 2 Each discourse relation theory uses a different term for minimal discourse text s"
D18-1372,D14-1108,0,0.0218652,"rst three words of two discourse arguments of the relation, and Word Pairs are the cross product of words of those discourse arguments. These two features enable our model to capture interaction between two discourse arguments. (Pitler et al., 2009) reported that these two features along with verbs, modality, context, and polarity (which can be captured by N-grams, sentiment tags and POS tags in our previous features) obtained the best performance for predicting Contingency class to which causality belongs. Discourse Argument Extraction As the first step of our pipeline, we use Tweebo parser (Kong et al., 2014) to extract syntactic features from messages. Then, we demarcate sentences using punctuation (‘,’) tag and periods. Among those sentences, we find discourse connectives defined in PDTB annotation along with a Tweet POS tag for conjunction words which can also be a discourse marker. In order to decide whether these connectives are really discourse connectives (e.g., I went home, but he stayed) as opposed to simple connections of two words (I like apple and banana) we see if verb phrases 4 exist before and after the connective by using dependency parsing results. Although discourse connective di"
D18-1372,D14-1220,0,0.0158342,"ubtask. Some have optimized models for discourse relation classification (i.e. given a document indicating if the relation existing) without discourse argument parsing using models such as Naive-Bayes or SVMs, achieve relatively stronger accuracies but a simpler task than that associated with discourse arguments (Park and Cardie, 2012; Zhou et al., 2010; Pitler et al., 2009). Researchers who, instead, tried to build the end-to-end parsing pipelines considered a wider range of approaches including sequence models and RNNs (Biran and McKeown, 2015; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014). Particularly, when they tried to utilize the discourse structures for out-domain applications, they used RNNbased models and found that those models are advantageous for their downstream tasks (Bhatia et al., 2015; Ji and Smith, 2017). In our case, for identifying causal explanations from social media using discourse structure, we build an RNN-based model for its structural effectiveness in this task (see details in section 3.2). However, we also note that simpler models such as SVMs and logistic regression obtained the state-of-the-art performances for text categorization tasks in social me"
D18-1372,D17-1119,1,0.921212,"ticularly, when they tried to utilize the discourse structures for out-domain applications, they used RNNbased models and found that those models are advantageous for their downstream tasks (Bhatia et al., 2015; Ji and Smith, 2017). In our case, for identifying causal explanations from social media using discourse structure, we build an RNN-based model for its structural effectiveness in this task (see details in section 3.2). However, we also note that simpler models such as SVMs and logistic regression obtained the state-of-the-art performances for text categorization tasks in social media (Lynn et al., 2017; Mohammad et al., 2013), so we build relatively simple models with different properties for each stage of the full pipeline of our parser. 3 Methods We build our model based on PDTB-style discourse relation parsing since PDTB has a relatively simpler text segmentation method;3 for explicit discourse relations, it finds the presence of discourse connectives within a document and extracts discourse arguments which parametrize the connective while for implicit relations, it considers all adjacent sentences as candidate discourse arguments. 3.1 Dataset We created our own causal explanation datase"
D18-1372,S13-2053,0,0.0986399,"y tried to utilize the discourse structures for out-domain applications, they used RNNbased models and found that those models are advantageous for their downstream tasks (Bhatia et al., 2015; Ji and Smith, 2017). In our case, for identifying causal explanations from social media using discourse structure, we build an RNN-based model for its structural effectiveness in this task (see details in section 3.2). However, we also note that simpler models such as SVMs and logistic regression obtained the state-of-the-art performances for text categorization tasks in social media (Lynn et al., 2017; Mohammad et al., 2013), so we build relatively simple models with different properties for each stage of the full pipeline of our parser. 3 Methods We build our model based on PDTB-style discourse relation parsing since PDTB has a relatively simpler text segmentation method;3 for explicit discourse relations, it finds the presence of discourse connectives within a document and extracts discourse arguments which parametrize the connective while for implicit relations, it considers all adjacent sentences as candidate discourse arguments. 3.1 Dataset We created our own causal explanation dataset by collecting 3,268 ra"
D18-1372,W12-1614,0,0.158068,"rsing. The Penn Discourse Treebank (PDTB) (Prasad et al., 2007) has a ‘Cause’ and ‘Pragmatic Cause’ discourse type under a general ‘Contingency’ class and Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) has a ‘Relations of Cause’. In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse arguments in newswire text (e.g. Wall Street Journal) from the discourse treebank and focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social"
D18-1372,D14-1162,0,0.0814181,"In order to decide whether these connectives are really discourse connectives (e.g., I went home, but he stayed) as opposed to simple connections of two words (I like apple and banana) we see if verb phrases 4 exist before and after the connective by using dependency parsing results. Although discourse connective disambiguation is a complicated task which can be much improved by syntactic features (Pitler and Nenkova, 2009), we try to minimize effects of syntactic parsing and simplify it since it is highly error-prone in social Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of extracted discourse arguments from messages. For the distributional representation of discourse arguments, we run a Word-level LSTM on the words’ embeddings within each discourse argument and concatenate last hidden state vectors → − of forward LSTM ( h ) and backward LSTM ← − ( h ) which is suggested by (Ji and Smith, 2017) → − ← − (DA = [ h ; h ]). Then, we feed the sequence of the vector representation of discourse arguments to the Discourse-argument-level LSTM (DA-level LSTM) to make a final prediction with log softmax function. With this structure,"
D18-1372,P09-1077,0,0.206739,"discourse relation parsing. The Penn Discourse Treebank (PDTB) (Prasad et al., 2007) has a ‘Cause’ and ‘Pragmatic Cause’ discourse type under a general ‘Contingency’ class and Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) has a ‘Relations of Cause’. In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse arguments in newswire text (e.g. Wall Street Journal) from the discourse treebank and focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text ca"
D18-1372,P09-2004,0,0.0351152,"tuation (‘,’) tag and periods. Among those sentences, we find discourse connectives defined in PDTB annotation along with a Tweet POS tag for conjunction words which can also be a discourse marker. In order to decide whether these connectives are really discourse connectives (e.g., I went home, but he stayed) as opposed to simple connections of two words (I like apple and banana) we see if verb phrases 4 exist before and after the connective by using dependency parsing results. Although discourse connective disambiguation is a complicated task which can be much improved by syntactic features (Pitler and Nenkova, 2009), we try to minimize effects of syntactic parsing and simplify it since it is highly error-prone in social Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of extracted discourse arguments from messages. For the distributional representation of discourse arguments, we run a Word-level LSTM on the words’ embeddings within each discourse argument and concatenate last hidden state vectors → − of forward LSTM ( h ) and backward LSTM ← − ( h ) which is suggested by (Ji and Smith, 2017) → − ← − (DA = [ h ; h ]). Then, we fe"
D18-1372,P17-2103,1,0.846264,"eet Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re3351 lation parsing approach for capturing counterfactual conditionals from tweets (Bhatia et al., 2015; Ji and Smith, 2017; Son et al., 2017). These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a pretrained model (not fully optimal for some parts of the given task) in their pipeline; Ji’s model performed worse than the baseline on the categorization of legislative bills, which is thought to be due to legislative discourse structures differing from those of the training set (WSJ corpus). Bhatia also used a pretrained model finding that utilizing discourse relation features did not boost accuracy (Bhatia et al., 2015; Ji and Smith, 2017). Both Bhatia and Son used manual schemes which may"
D18-1372,C10-2172,0,0.0707588,"se Treebank (PDTB) (Prasad et al., 2007) has a ‘Cause’ and ‘Pragmatic Cause’ discourse type under a general ‘Contingency’ class and Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) has a ‘Relations of Cause’. In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse arguments in newswire text (e.g. Wall Street Journal) from the discourse treebank and focused on exploring different features and optimizing various types of models for predicting relations (Pitler et al., 2009; Park and Cardie, 2012; Zhou et al., 2010). In order to further develop automated systems, researchers have proposed end-to-end discourse relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These corpora consist of documents from Wall Street Journal (WSJ) which are much more well-organized and grammatical than social media texts (Biran and McKeown, 2015; Lin et al., 2014; Ji and Eisenstein, 2014; Feng and Hirst, 2014). Only a few works have attempted to parse discourse relations for out-of-domain problems such as text categorizations on social media texts; Ji and"
D18-1392,P07-1033,0,0.123195,"Missing"
D18-1392,D18-1148,1,0.877454,"Missing"
D18-1392,P15-1073,0,0.236221,"ch over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating sociodemographic contexts. 1 Introduction Adapting to human factors has been shown to benefit NLP tasks, especially in tasks that involve predictions over individual social media posts (e.g., sentiment (Hovy, 2015), sarcasm, and stance detection (Lynn et al., 2017)). The main idea behind these approaches is that knowing who wrote a piece of text can help models better understand how to process it. This paper develops methods that apply this idea to community-level prediction tasks, which require making decisions over posts from a community of users. Many communitylevel outcomes and community-wide language are linked to socio-demographic factors (age, gender, race, education, income levels) with many social scientific studies supporting their predictive value (Cohen et al., 2003), and should therefore af"
D18-1392,D17-1119,1,0.908894,"ctive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating sociodemographic contexts. 1 Introduction Adapting to human factors has been shown to benefit NLP tasks, especially in tasks that involve predictions over individual social media posts (e.g., sentiment (Hovy, 2015), sarcasm, and stance detection (Lynn et al., 2017)). The main idea behind these approaches is that knowing who wrote a piece of text can help models better understand how to process it. This paper develops methods that apply this idea to community-level prediction tasks, which require making decisions over posts from a community of users. Many communitylevel outcomes and community-wide language are linked to socio-demographic factors (age, gender, race, education, income levels) with many social scientific studies supporting their predictive value (Cohen et al., 2003), and should therefore affect how a model treats social media-based language"
D18-1392,D13-1187,0,0.070512,"w the meaning of language changes depending on who states it. For instance, when an NLP PhD student says the word ‘paper’ he/she usually means something different than when a 5th grade student uses the same word (i.e. ‘research paper’ versus ‘piece of paper’). Hu et al. (2017) noted the same words can have different meanings if different people say them. This idea of contextualizing language with extralinguistic information has been the basis for multiple models: Hovy (2015) learn age- and genderspecific word embeddings, leading to significant improvements for three text classification tasks. Volkova et al. (2013) found that using genderspecific features lead to improvements in sentiment analysis over a gender-agnostic model. Most recently, Lynn et al. (2017) proposed a domain adaptation-inspired method for composing userlevel, extra-linguistic information with messagelevel features, leading to improvements for multiple text classification tasks; we build off of this approach and that of Zamani and Schwartz (2017) in this paper. While Lynn et al. (2017) injected user-level info into message-level tasks, we are investigating whether same-level adaptation techniques are similarly useful. We also try to f"
D18-1392,W18-0619,1,0.688887,"Missing"
D18-1392,E17-2005,1,0.275735,"area, while its high prevalence in Mobile, Alabama might indicate greater interest in motor bikes. We present a method for building language-based predictive models which integrate in and adapt to attributes of the communities generating the language. This work aims to unify two different approaches developed for adapting to human factors and use them for incorporating community attributes in community-level prediction tasks: (1) residualized controls: whereby a model is trained in two steps: first over the factors/controls and then fitting the language to the residuals of the control model (Zamani and Schwartz, 2017), and (2) user-factor adaptation: whereby linguistic features are adapted, or treated differently, based on the continuous-valued factors of the authors of the features (Lynn et al., 2017). Combining factor adaptation (FA) and residualized control (RC) into RFA is a non-trivial task. The intent behind both methods are quite different: whereas RC attempts to address the inherent heterogeneity between robust control variables and noisy linguistic variables, FA enables a model to treat linguistic features differently depending on the factors. From a statistical learning perspective, RC separates"
D18-1392,W16-4320,0,0.0179418,"ra-linguistic and language features, (2) the first empirical evaluation of applying factor adaptation for communitylevel prediction tasks, (3) analysis of the impact of the size of factors and factor selection in adaptation, and (4) state-of-the-art accuracies for each of the five tasks for which we evaluate RFA. 2 Background Social media provides easy access to a vast amount of language written by a diverse group of users, making it an increasingly popular resource for measuring community health, psychology, and economics (Coppersmith et al., 2015; Eichstaedt et al., 2015; Weeg et al., 2015; Mowery et al., 2016; Haimson and Hayes, 2017). Coppersmith et al. (2015), for instance, examine trends in language use among Twitter users who selfreported one of ten mental health diagnoses. Eichstaedt et al. (2015) and Weeg et al. (2015) use Twitter to predict the prevalence rates of various health outcomes, such as heart disease mortality and depression, at the county level. Haimson and Hayes (2017) tracked changes in the emotional well-being of transgender communities on Tumblr between 2015 and 2016. Socio-demographics are often correlated with health outcomes (such as age and heart disease), which is why su"
D18-1392,D17-2010,1,0.836581,"Missing"
D18-1392,W15-1201,0,\N,Missing
E17-2005,N10-1038,0,0.0935583,"Missing"
I17-1077,E14-1043,0,0.0639917,"Missing"
I17-1077,D11-1120,0,0.109168,"Missing"
I17-1077,P07-1033,0,0.160665,"Missing"
I17-1077,P15-1169,0,0.113881,"Missing"
I17-1077,P11-2071,0,0.0366988,"Missing"
I17-1077,W10-2608,0,0.0373361,"Missing"
N15-1044,N13-1121,0,0.0605509,"Missing"
N15-1044,D11-1120,0,0.0643487,"erform traditional computational linguistics tasks, while others focus particularly on predicting user attributes. User network information has been used for tweet summarization or filtering (Panigrahy et al., 2012; Chang et al., 2013; Feng and Wang, 2013). Others utilize psychological knowledge about people, such as exploiting the human tendency to report more positive extreme feelings than negative in order to improve on sentiment analysis (Guerra et al., 2014). Toward attribute prediction, a large proportion of works have focused on demographics (Argamon et al., 2009; Goswami et al., 2009; Burger et al., 2011; Al Zamal et al., 2012; Bergsma et al., 2013; Sap et al., 2014). and personality prediction (Mairesse et al., 2007; Iacobelli et al., 2011; Schwartz et al., 2013; Park et al., 2015). Human temporal orientation, as we study it here, differs from previous studies of user attribute prediction in that temporal orientation calls for consideration of additional language features (some more sophisticated, such as time expressions), and exploration of classification techniques (e.g. that can capture non-linear relationships or interactions). We also add multidisciplinary applications, showing not jus"
N15-1044,chang-manning-2012-sutime,0,0.380774,"f writing (e.g. the time expression ‘yesterday’ in a document written on January 15, 2014 is resolved as January 14, 2014). Many methods have been used, ranging from hand-crafting rules to machine learning models. Unlike other areas of natural language processing where stochastic techniques dominate, rule-based systems have been quite competitive in time expressions recognition, especially in less domain dependent settings or for relaxed matching tasks (UzZaman et al., 2013). A number of useful toolkits have been produced for temporal text analysis (Verhagen et al., 2005; Ling and Weld, 2010; Chang and Manning, 2012). In this work, we use Stanford University’s rulebased temporal tagger, SUTime, which geve accuracy in line with the state-of-the-art systems at identifying time expressions at TempEval (Chang and Manning, 2012).2 SUTime, built on top of Stanford’s part-of-speech and named entity taggers, la2 Our goals differ slightly from the TempEval accuracy criteria. For example, when SUTime fails to distinguish “one and a half weeks” from “one week”, it does not affect our performance. However, other errors, such as confusing the verb ‘march’ with the month March will harm our accuracy. 411 bels times, du"
N15-1044,D14-1121,1,0.810387,"ocus particularly on predicting user attributes. User network information has been used for tweet summarization or filtering (Panigrahy et al., 2012; Chang et al., 2013; Feng and Wang, 2013). Others utilize psychological knowledge about people, such as exploiting the human tendency to report more positive extreme feelings than negative in order to improve on sentiment analysis (Guerra et al., 2014). Toward attribute prediction, a large proportion of works have focused on demographics (Argamon et al., 2009; Goswami et al., 2009; Burger et al., 2011; Al Zamal et al., 2012; Bergsma et al., 2013; Sap et al., 2014). and personality prediction (Mairesse et al., 2007; Iacobelli et al., 2011; Schwartz et al., 2013; Park et al., 2015). Human temporal orientation, as we study it here, differs from previous studies of user attribute prediction in that temporal orientation calls for consideration of additional language features (some more sophisticated, such as time expressions), and exploration of classification techniques (e.g. that can capture non-linear relationships or interactions). We also add multidisciplinary applications, showing not just how accurately our models predict, but also studying how tempo"
N15-1044,N03-1033,0,0.00526355,"2012), discussed previously. Specific features recorded include the temporal difference itself (e.g. -2.5 for “two and half days ago”), its base 2 log (log(1 + value)), its absolute value, total number of time expressions, and binary variables indicating if any past, present, or future expressions appear in the text. We also include binary features for each of the named-entity time tags for the time expression provided by SUTime (e.g. “future ref”, “present ref”, “next immediate”). POS tags: The relative frequency of each part-ofspeech tag. Tagging is done via Stanford’s part-ofspeech tagger (Toutanova et al., 2003). Stanford’s tagger does not have explicit social media tags, but we are most interested in capturing tense which it does well.4 Also, it is already being used as part of SUTime. Each part-of-speech tag is encoded as the frequency of tag usage (f req(tag, msg)) divided by the total number of tokens in the message (tokensmsg ): p(tag|msg) = f req(tag, msg) |tokensmsg | lexica: The relative frequency of categories, based on the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2007). We use the 2007 version of LIWC which includes 64 categories of psychologicallyrelevant lan"
N15-1044,S13-2001,0,0.147471,"Missing"
N15-1044,P05-3021,0,0.0339366,"pressed time and date relative to the time of writing (e.g. the time expression ‘yesterday’ in a document written on January 15, 2014 is resolved as January 14, 2014). Many methods have been used, ranging from hand-crafting rules to machine learning models. Unlike other areas of natural language processing where stochastic techniques dominate, rule-based systems have been quite competitive in time expressions recognition, especially in less domain dependent settings or for relaxed matching tasks (UzZaman et al., 2013). A number of useful toolkits have been produced for temporal text analysis (Verhagen et al., 2005; Ling and Weld, 2010; Chang and Manning, 2012). In this work, we use Stanford University’s rulebased temporal tagger, SUTime, which geve accuracy in line with the state-of-the-art systems at identifying time expressions at TempEval (Chang and Manning, 2012).2 SUTime, built on top of Stanford’s part-of-speech and named entity taggers, la2 Our goals differ slightly from the TempEval accuracy criteria. For example, when SUTime fails to distinguish “one and a half weeks” from “one week”, it does not affect our performance. However, other errors, such as confusing the verb ‘march’ with the month M"
N15-1044,S07-1014,0,0.0447369,"on seem plausible (e.g. one might suspect it is smart to think about the future, or wonder if one’s reflection on the past is related to their popularity as measure by number of friends). Related work. Studying temporal language is by no means new to the field of computational linguistics (or NLP). Most recently, time annotation has gained greater interest with a successive sequence of three SemEval tasks (TempEval-1, -2 and -3). The SemEval competitions have provided data sets that facilitate the comparison of different methods for evaluating time expressions, events, and temporal relations (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Such research on temporal text analysis generally focuses on determining when events start and end or how they relate temporally to each other; specific goals include information extraction of time-dependent facts from news media (Ling and Weld, 2010; Talukdar et al., 2012), or extracting personal histories in social media (Wen et al., 2013). In contrast, our goal is to find the temporal orientation of people. Of the numerous TempEval tasks, we build upon those which identify time expressions and resolve their expressed time and date relative to"
N15-1044,P13-2145,0,0.037317,"Missing"
N15-1044,S10-1010,0,\N,Missing
P17-2013,P16-1080,1,0.850033,"multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is particularly important"
P17-2013,W14-3207,0,0.0234073,"babilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is"
P17-2013,P03-1037,0,0.0794531,"). The bar on the left of each plot represents the percentage of observations that are zero for each feature where the shading represents the percent of features reaching the given threshold. As the bar gets darker it means more features out of 500 are zero in that percentage of individuals. The right portion of each plot is based on standardized relative frequencies of the variables (mean centered and divided by the standard deviation). component is governed by a Bernoulli distribution that generates excess zeros, while the second component generates counts, some of which also could be zero (Jansche, 2003). is semi-transparent such that an aggregate trend across multiple features will emerge darkest. As we move along a row ranging specific features (unigrams) to generic features (lexicon), the empirical distribution gradually changes from resembling a “power law” (or binomial distribution with low number of trials and probability of success) to something more “Normal”. Similar shifts are also observed as we move across levels of modeling. We investigate whether the best-fitting distributions vary across the three levels of analysis and three types of lexical features. We consider the following"
P17-2013,W11-0310,0,0.0244414,"llum et al. (1998) compares the results of probabilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche Introduction NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016). Figure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across 2 While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities. 3 While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, bo"
P17-2013,D13-1187,0,0.0569268,"Missing"
P17-2103,W12-1614,0,0.068548,"Missing"
P17-2103,W15-4612,0,0.352173,"Missing"
P17-2103,P09-1077,0,0.151697,"Missing"
P17-2103,prasad-etal-2008-penn,0,0.11777,"Missing"
P17-2103,P11-2008,0,0.0884882,"Missing"
P17-2103,P14-1002,0,0.0591999,"Missing"
P17-2103,D09-1036,0,\N,Missing
W14-3214,S13-2053,0,0.114763,"ession as assessed by surveys. Red line is a LOESS smoothed trend (+/- 1 SE) over the average of scores from users who completed the survey on that day. rate). Figure 1a shows the distribution of surveyassessed DDep (standardized). The items can be seen in Table 1. Figure 2 shows the daily averages of surveyassessed DDep, collapsed across years. A LOESS smoother over the daily averages illustrates a seasonal trend, with depression rising over the winter months and dropping during the summer. lexica: 64 LIWC categories (Pennebaker et al., 2007) as well as the sentiment lexicon from NRC Canada (Mohammad et al., 2013).2 Usage of a lexicon (lex) was calculated similar to the LDA topics, where w is the weight of the word in the lexicon in the case of sentiment and always 1 in the case of LIWC which has no weights: Regression modeling. In order to get a continuous value output from our model, we explored regression techniques over our training data. Since this first work exploring regression was concerned primarily with language content, our features for predicting depression were based entirely on language use (other social media activity and friend networks may be considered in future work). These features"
W15-1203,W14-3207,0,0.256668,"ne 5, 2015. 2015 Association for Computational Linguistics mental illness. We show that a model which uses only the text-predicted user level ‘Big Five’ personality dimensions plus age and gender perform with high accuracy, comparable to methods that use standard dictionaries of psychology as features. Users who self-report a diagnosis appear more neurotic and more introverted when compared to average users. 2 Data We use a dataset of Twitter users reported to suffer from a mental illness, specifically depression and post traumatic stress disorder (PTSD). This dataset was first introduced in (Coppersmith et al., 2014a). The self-reports are collected by searching a large Twitter archive for disclosures using a regular expression (e.g. ‘I have been diagnosed with depression’). Candidate users were filtered manually and then all their most recent tweets have been continuously crawled using the Twitter Search API. The selfdisclosure messages were excluded from the dataset and from the estimation of user inferred demographics and personality scores. The control users were selected at random from Twitter. In total there are 370 users diagnosed only with PTSD, 483 only with depression and 1104 control users. On"
W15-1203,E14-1043,1,0.570596,"Missing"
W15-1203,W11-1515,0,0.19154,"Missing"
W15-1203,W15-1205,1,0.892742,"Missing"
W15-1203,D14-1121,1,0.12773,"Missing"
W15-1203,N15-1044,1,0.787795,"uild a large dataset of users and their textual information. 3 Features We use the Twitter posts of a user to infer several user traits which we expect to be relevant to mental illnesses based on standard clinical criteria (American Psychiatric Association, 2013). Recently, automatic user profiling methods have used on usergenerated text and complementary features in order to predict different user traits such as: age (Nguyen et al., 2011), gender (Sap et al., 2014), location (Cheng et al., 2010), impact (Lampos et al., 2014), political preference (Volkova et al., 2014), temporal orientation (Schwartz et al., 2015) or personality (Schwartz et al., 2013). 22 3.1 Age, Gender and Personality We use the methods developed in (Schwartz et al., 2013) to assign each user scores for age, gender and personality from the popular five factor model of personality – ‘Big Five’ – (McCrae and John, 1992), which consists of five dimensions: extraversion, agreeableness, conscientiousness, neuroticism and openness to experience. The model was trained on a large sample of around 70,000 Facebook users who have taken Big Five personality tests and shared their posts using a model using 1-3 grams and topics as features (Park"
W15-1203,P14-1018,0,0.123068,"Missing"
W15-1205,J92-4003,0,0.248875,"Missing"
W15-1205,W14-3207,0,0.123307,"Missing"
W15-1205,W15-1204,0,0.109158,"Missing"
W15-1205,E14-1043,1,0.854115,"Missing"
W15-1205,N13-1090,0,0.00236123,"PMI). This information-theoretic measure indicates which words co-occur in the same context (Bouma, 2009) where the context is the entire tweet. To obtain hard clusters of words we use spectral clustering (Shi and Malik, 2000; Ng et al., 2002). This methods was shown to deal well with highdimensional and non-convex data (von Luxburg, 2007). In our experiments we used 1000 clusters from 54,592 tokens. Word2Vec Word Clusters (W2V) Neural methods have recently been gaining popularity in order to obtain low-rank word embeddings and obtained state-of-the-art results for a number of semantic tasks (Mikolov et al., 2013b). These methods, like many recent word embeddings, also allow to capture local context order rather than just ‘bag-of-words’ relatedness, which leads to also capture syntactic information. We use the skip-gram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings from a corpus of 400 million tweets also used in (Lampos et al., 2014). We use a hidden layer size of 50 with the Gensim implementation.2 We then apply spectral clustering on these embeddings to obtain hard clusters of words. We create 2000 clusters from 46,245 tokens. GloVe Word Clusters (GloVe) A different"
W15-1205,N13-1039,0,0.0269951,"Missing"
W15-1205,D14-1162,0,0.115824,"Missing"
W15-1205,W15-1203,1,0.90032,"Missing"
W15-1205,D14-1121,1,0.531179,"Missing"
W16-0404,N10-1122,0,0.00952595,"al media. This can be used in applications such as mental illness detection or in automated large-scale psychological studies. 1 Introduction Sentiment analysis is a very active research area that aims to identify, extract and analyze subjective information from text (Pang and Lee, 2008). This generally includes identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people posting subjective messages. This is of great interest to applications in social science which study text at a la"
W16-0404,P15-1073,0,0.0187284,"s with age for both genders, especially at the start and end of our age intervals (13–16 and 30–35), confirming the aging positivity bias (Mather and Carstensen, 2005). Valence is higher for females across almost the entire age range. Posts written by females are also significantly higher in arousal for all age groups. Age does not play a significant effect in post arousal, although there is a slight increase with age especially for females. Overall, these figures again illustrate the importance of age and gender as factors to be considered in these types of application (Volkova et al., 2013; Hovy, 2015). 3 Predicting Valence and Arousal To study the linguistic differences of both dimensions, we build a bag-of-words prediction model of valence and arousal from our corpus.2 We train two linear regression models with `2 regularisation on the posts and test their predictive power in a 10fold cross-validation setup. Results for predicting the two scores are presented in Table 4. We compare to a number of different existing general purpose lexicons. First, we use the ANEW (Bradley and Lang, 1999) weighted dictionary to compute a valence and arousal score as the weighted sum of individual word vale"
W16-0404,S13-2053,0,0.0134793,"s 700 500 300 300 100 100 1 2 3 4 5 6 Valence 7 8 9 1 (a) Valence. 2 3 4 5 6 Arousal 7 8 9 (b) Arousal. Figure 1: Histrograms of average rating scores. of words obtained by extending ANEW with human ratings for ∼14000 words (Warriner et al., 2013). We also benchmark with standard methods for estimating valence from sentiment analysis. First, we use the MPQA lexicon (Wilson et al., 2005), which contains 7629 words rated for positive or negative sentiment, to obtain a score based on the difference between positive and negative words in the post. Second, we use the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), which obtained the best performance on the Semeval Twitter Sentiment Analysis tasks.3 Our method achieves very high correlations with the target score. Arousal is easier to predict, reaching r = 0.85 correlation between predicted and rater score. ANEW obtains significant correlations with both of our ratings, however these are significantly lower than our model. The extended list of affective norms obtains, perhaps surprisingly, lower correlation for valence, but stronger correlation with arousal than ANEW. For valence, both sentiment analysis lexicons provide better performance 3 https://ww"
W16-0404,S15-2078,0,0.0307292,"identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people posting subjective messages. This is of great interest to applications in social science which study text at a large scale and with orders of magnitude more users than traditional studies. Emotion classification is a widely debated topic in psychology (Gendron and Barrett, 2009). Two main theories about emotions exist: the first posits a discrete and finite set of emotions, while the second suggests that emotions are a co"
W16-0404,S07-1013,0,0.384501,"th arousal annotations. Our data set offers a building block to a deeper study of personal affect as expressed in social media. This can be used in applications such as mental illness detection or in automated large-scale psychological studies. 1 Introduction Sentiment analysis is a very active research area that aims to identify, extract and analyze subjective information from text (Pang and Lee, 2008). This generally includes identifying if a piece of text is subjective or objective, what sentiment it expresses (positive or negative; often referred to as valence), 9 what emotion it conveys (Strapparava and Mihalcea, 2007) and towards which entity or aspect of the text i.e., aspect based sentiment analysis (Brody and Elhadad, 2010). Downstream applications are mostly interested in automatically inferring public opinion about products or actions. Besides expressing attitudes towards other objects, texts can also express the emotions of the ones writing them, most common recently with the rise of Social Media usage (Rosenthal et al., 2015). This study focuses on presenting a gold standard data set as well as a model trained on this data in order to drive research in learning about the affective norms of people po"
W16-0404,strapparava-valitutti-2004-wordnet,0,0.0311959,"ons in social science which study text at a large scale and with orders of magnitude more users than traditional studies. Emotion classification is a widely debated topic in psychology (Gendron and Barrett, 2009). Two main theories about emotions exist: the first posits a discrete and finite set of emotions, while the second suggests that emotions are a combination of different scales. Research in Natural Language Processing (NLP) has been focused mostly on Ekman’s model of emotion (Ekman, 1992) which posits the existence of six basic emotions: anger, disgust, fear, joy, sadness and surprise (Strapparava and Valitutti, 2004; Strapparava and Mihalcea, 2008; Calvo and D’Mello, 2010). In this study, we focus on the most popular dimensional model of emotion: the circumplex model introduced in (Russell, 1980). This model suggests that all affective Proceedings of NAACL-HLT 2016, pages 9–15, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics states are represented in a two-dimensional space with two independent neurophysiological systems: valence (or sentiment) and arousal. Any affective experience is a linear combination of these two independent systems, which is then interprete"
W16-0404,D13-1187,0,0.0129217,"data. Valence increases with age for both genders, especially at the start and end of our age intervals (13–16 and 30–35), confirming the aging positivity bias (Mather and Carstensen, 2005). Valence is higher for females across almost the entire age range. Posts written by females are also significantly higher in arousal for all age groups. Age does not play a significant effect in post arousal, although there is a slight increase with age especially for females. Overall, these figures again illustrate the importance of age and gender as factors to be considered in these types of application (Volkova et al., 2013; Hovy, 2015). 3 Predicting Valence and Arousal To study the linguistic differences of both dimensions, we build a bag-of-words prediction model of valence and arousal from our corpus.2 We train two linear regression models with `2 regularisation on the posts and test their predictive power in a 10fold cross-validation setup. Results for predicting the two scores are presented in Table 4. We compare to a number of different existing general purpose lexicons. First, we use the ANEW (Bradley and Lang, 1999) weighted dictionary to compute a valence and arousal score as the weighted sum of individ"
W16-0404,H05-1044,0,0.0554045,"ia, June 12-17, 2016. 2016 Association for Computational Linguistics states are represented in a two-dimensional space with two independent neurophysiological systems: valence (or sentiment) and arousal. Any affective experience is a linear combination of these two independent systems, which is then interpreted as representing a particular emotion. For example, fear is a state involving the combination of negative valence and high arousal (Posner et al., 2005). Previous research in NLP focused mostly on valence or sentiment, either binary or having a strength component coupled with sentiment (Wilson et al., 2005; Thelwall et al., 2010; Thelwall et al., 2012). In this paper we build a new data set consisting of 2895 anonymized Facebook posts labeled with both valence and arousal by two annotators with psychology training. The ratings are made on two independent nine point scales, reaching a high agreement correlations of .768 for valence and .827 for arousal. Data set statistics suggest that while the dimensions of valence and arousal are associated, they present distinct information, especially in posts with a clear positive or negative valence. Further, we train a bag-of-words linear regression mode"
W16-5617,W14-3207,1,0.794803,"ill inadContinual Engagement vertently be perceived as crass, discriminating, and out-of-touch with the community that might benefit Early in this process, CLPsych was fortunate that a from their research (Singer and Erreger, 2015). group of researchers and clinicians from the suicide The #SPSM community helped the Q researchers prevention community (Hs) came upon some pop- to understand the important context of their work and ular press coverage of recent research and reached the realities of the mental healthcare system. Access out to the Q researchers involved (Coppersmith et 2 al., 2014a; Coppersmith et al., 2014b). #SPSM (Suihttp://spsmchat.com or #SPSM on social media channels. to have the NLP and Psych constituencies integrated at every sensible step: program committee, reviews, dialog, and co-presentation. 134 to the community also helped to impress upon Q researchers the potential impact of the work they are doing, encouraging the work to continue and reshaping it for greater impact and utility. New partnerships have been borne out of online discussions. In turn, the Q researchers helped the #SPSM’ers to understand the realm of the possible in data science. Informed discussion of data, access, an"
W16-5617,W15-1201,1,0.82711,"certed manner at the 2016 Frederick Jelinek memorial workshop, hosted by the Center for Language and Speech Processing at Johns Hopkins University (Hollingshead et al., 2016). Novel datasets were made available for the workshop to advance the analysis of mental health through social media: 1. The Social Mediome project at the University of Pennsylvania provided electronic medical records and paired Facebook data from users who opted in to the study (Padrez et al., 2015); 2. Qntfy provided anonymized data from users who discussed mental health diagnoses or suicide attempts publicly on Twitter (Coppersmith et al., 2015; Coppersmith et al., 2016); and 3. OurDataHelps.org provided anonymized Twitter data for users who attempted suicide. A team of researchers, primarily Qs and primarily from NLP and data science, came to Johns Hopkins University for 6 weeks to explore temporal patterns of social media language relevant for mental health. In order to make sure the analyses were on the right path and to get some of the benefits of the CLPsych discussants in real time, a clinical panel was formed. 135 This panel was comprised of practicing clinicians, people with lived experience with mental health issues, epidem"
W16-5617,W16-0311,1,0.797749,"Frederick Jelinek memorial workshop, hosted by the Center for Language and Speech Processing at Johns Hopkins University (Hollingshead et al., 2016). Novel datasets were made available for the workshop to advance the analysis of mental health through social media: 1. The Social Mediome project at the University of Pennsylvania provided electronic medical records and paired Facebook data from users who opted in to the study (Padrez et al., 2015); 2. Qntfy provided anonymized data from users who discussed mental health diagnoses or suicide attempts publicly on Twitter (Coppersmith et al., 2015; Coppersmith et al., 2016); and 3. OurDataHelps.org provided anonymized Twitter data for users who attempted suicide. A team of researchers, primarily Qs and primarily from NLP and data science, came to Johns Hopkins University for 6 weeks to explore temporal patterns of social media language relevant for mental health. In order to make sure the analyses were on the right path and to get some of the benefits of the CLPsych discussants in real time, a clinical panel was formed. 135 This panel was comprised of practicing clinicians, people with lived experience with mental health issues, epidemiologists, and psychology r"
W18-0604,W14-3207,0,0.0252189,"g it easier to draw comparisons between system performances. We take the measurement error from literature on the reliability of the adult psychological distress measure (rmeas1 = 0.77; Ploubidis et al. (2017)) and of similar, languagebased prediction measures (rmeas2 = 0.70; Park et al. (2015)). The metric is thus: Innovation Challenge: Future Language Generation One of the limitations of traditional psychological assessments is that they typically only capture one or a few psychological factors. In contrast, language has been shown to capture many aspects of an individual (Pennebaker, 2011; Coppersmith et al., 2014; Schwartz and Ungar, 2015; Kern et al., 2016), making language-based assessments an attractive compliment or alternative. Language generation tools for mental health could indicate whether an individual is likely to produce signs of mental distress in future, e.g. “I want to end my life.” Should language generation tools be adequately reliable and valid indicators of future mental health states, these tools could serve as a means of identifying individuals who could be targeted for early intervention or preventative treatments. The Innovation Challenge is a difficult rdis = √ rP earson rmeas1"
W18-0604,W14-3348,0,0.0122096,"as the average embedding for all words in the document — and measure the cosine similarity between the generated essay’s embedding and that of the actual essay. The word-level embeddings are Word2Vec (Mikolov et al., 2013) embeddings learned from the age 50 essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not individual documents as we do here. Secon"
W18-0604,W16-6203,0,0.150517,".edu Abstract and easy to conduct at scale. Further, whereas traditional risk assessments are typically limited to capturing one or a few psychological factors, language analysis has the advantage of being theoretically unlimited in what it can capture. By evaluating the relationship between linguistic markers and lifetime health outcomes, such research may provide benefits for intake assessment, monitoring, and preventative care. Computational linguistics has now shown strong potential for aiding in mental health assessment and treatment. With few exceptions (e.g. De Choudhury et al. (2016), Sadeque et al. (2016)), work thus far from the NLP community has focused on predicting current mental health from language, and most exceptions have still only looked at the short-term future. While such research is valuable, predictions about the long-term future can aid with another class of applications: the understanding of early life markers and development of preventative care. Here we describe the CLPsych 2018 shared task, the purpose of which is to evaluate multiple methods for analyzing linguistic markers as a signal for current and future psychological outcomes (i.e. risk assessment). We present three ta"
W18-0604,P17-4012,0,0.0230066,"Missing"
W18-0604,W09-0441,0,0.0150507,"ument-level embeddings — computed as the average embedding for all words in the document — and measure the cosine similarity between the generated essay’s embedding and that of the actual essay. The word-level embeddings are Word2Vec (Mikolov et al., 2013) embeddings learned from the age 50 essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not in"
W18-0604,W17-3110,1,0.759461,"in individuals by analyzing social media signals up to a year in advance of its reported onset. Similarly, De Choudhury et al. (2016) aims to identify individuals who are likely to engage in suicidal ideation in the future. Sadeque et al. (2016) predict whether posters on a mental health forum will leave the forum within a particular (one, six, or twelve month) time frame. In addition to these cases, some have used temporal information within cross-sectional analyses. Zirikly et al. (2016), for example, use timestamp data to help classify the severity levels of posts to a mental health forum. Loveys et al. (2017) explore mental health within the context of micropatterns, or sequences of posts occurring within a small time frame. The goal of this shared task is to predict mental health not only at the time of writing, but years or decades into the future. 2 Figure 1: Example of an essay written by an NCDS participant at age 11, imagining where they saw themself at age 25. dents displayed characteristics such as “does not know what to do with himself, can never stick at anything long” or “miserable, depressed, seldom smiles”. For the purposes of the shared task, we focused on the total BSAG score, as we"
W18-0604,W16-0321,1,0.892183,"Missing"
W18-0604,P02-1040,0,0.101515,"essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not individual documents as we do here. Second, this score was designed for machine translation, which our task is not. Instead, we are trying to predict a person’s response to an open-ended prompt, based on their response to a similar prompt forty years prior. For these reasons, we employ a second"
W18-0619,P17-2013,1,0.825459,"ople. Second, we select features with at least a small relationship with our trust labels according to having a univariate family-wise error rate < 60. Third, we ran a singular value decomposition (in randomized batches) to effectively decrease the size of feature space and reduce colinearity across dimensions(Boutsidis et al., 2015). We performed this process based on the training data, and then applied the resulting feature reduction on the test data. Each type of feature (i.e. ngram relative frequencies, booleans, and topics) is qualitatively and distributionally different from each other (Almodaresi et al., 2017). Thus, we perform • I believe that others have good intentions. • I suspect hidden motives in others.∗ • I trust what people say. Some questions (e.g. ∗ above) are “reverse scored” so a 1 becomes a 5 and vice-versa. One’s final trust score is based on taking the mean of the responses to the individual trust questions. Although 3-question trust is less accurate,3 it may be useful to enable training data from more users. From MyPersonality, we used a dataset containing 19, 455 Facebook users who wrote at least 1, 000 words across all of their status updates. We additionally included 6, 590 user"
W18-0619,P15-1169,0,0.0603368,"Missing"
W18-0619,S13-2053,0,0.0137829,"feature sets: ngr r: ngrams as relative frequencie, ngr b: ngrams as boolean variables. Bold indicates the best performance. Pearson rdis is dissattenuated Pearson r and MSE is the mean squared error. ments we only train on 3-question trust labels and test on 10-question trust labels. We next evaluate the performance of our trust model by comparing to two baseline models. Because positiveness is associated with trust (Helliwell and Wang, 2010), we consider a baseline of sentiment scores using the NRC hashtag sentiment lexicon, an integral part of the best system participating in SemEval-2013 (Mohammad et al., 2013). We also compare it to clusters of words derived from word2vec embeddings (Mikolov et al., 2013) using spectral clustering (Preot¸iuc-Pietro et al., 2015). Figure 2: Effect of increasing the number of training users, who have more than 1, 000 word count, while there are 6, 590 users with less than 1, 000 word count in train set: “Threshold-1000” is training ridge-regression on users with at least 1, 000 words, “threshold-200” is training ridge-regression on users with at least 200 words, “linear” is training weighted ridge-regression on users with at least 200 words, and finally “logistic” is"
W18-0619,E17-2005,1,0.559364,"Missing"
W19-2103,C16-1154,0,0.0183717,"ucture (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned user embeddings for stance detection and found that the inclusion of such embeddings significantly improved model performance. Going in a somewhat different direction, Joseph et al. (2017) found that the amount and type of user attributes, such as political party affiliation or Twitter profile description, provided to annotators of a stance detection dataset significantly impacted annotation quality, suggesting that considering user attributes is important not just during classification but also during dataset creation. Background Recent work has shown that considering language within"
W19-2103,W15-1204,0,0.0421488,"Missing"
W19-2103,I13-1191,0,0.0219059,"compared to “state-like” attributes (frequently changing, e.g. sentiment). We evaluate this theory by looking at the role of user attributes across different predictive tasks. (3) We provide a set of considerations and metrics, for task participants and designers alike, for the inclusion of user information within new social science-related tasks. 2 Some work in stance detection has focused on document context and discourse structure (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned user embeddings for stance detection and found that the inclusion of such embeddings significantly improved model performance. Going in a somewhat different dire"
W19-2103,P15-1073,0,0.168955,"and H. Andrew Schwartz1 1 Stony Brook University, 2 University of Pennsylvania {velynn, niranjan, has}@cs.stonybrook.edu, sgiorgi@sas.upenn.edu Abstract Our work aims to answer the following questions: 1) What and how much information do user attributes alone carry for different social media tasks, particularly for predictive tasks that are more about the user than the document (e.g. stance)? 2) When are user attributes useful and what do language features contribute in these cases? While there are multiple works that show that adding user attributes is useful for different prediction tasks (Hovy, 2015; Zamani and Schwartz, 2017; Lynn et al., 2017), there is no single systematic study that answers these questions. To this end we conduct a systematic evaluation of user attribute-only models on multiple tasks including stance detection, sarcasm detection, sentiment analysis, and prepositional phrase attachment. We evaluate the impact of user attribute-only models through a range of features derived from publicly available information about the users including: written profile bio, inferred demographics and personality, self-reported location, profile picture, who one follows in a social netwo"
W19-2103,D18-1070,0,0.0193545,"eet to that expressed in historical tweets. Martin et al. (2016) found that, when predicting retweet count, a user’s past success (measured as the average number of retweets received for other tweets in the past) was nearly as predictive as a model using all features they tried, including those drawn from the tweet itself. Jurgens et al. (2017) find that they are able to accurately predict the attributes of a user based on communications targeted at them (as opposed to written by them), emphasizing that a person’s social network is itself an important source of userlevel information. Finally, Hovy and Fornaciari (2018) demonstrate that user attributes can be used to improve the quality of author embeddings via retrofitting. 3 Prediction Models This paper seeks to systematically and empirically understand the role of user attributes within the context of social media tasks. To that end, we consider a variety of user-level features and evaluate their importance for four tweet-level prediction tasks. 3.1 Tasks The following section provides details for the systems and datasets used for analysis. Development sets were used for hyperparameter tuning. Statistics for each task are given in Table 1. Stance. For sta"
W19-2103,K16-1017,0,0.0540985,"eful beyond stance detection. Bamman and Smith (2015) extensively evaluate the effects of extralinguistic information, including author, audience, and environment features, in the context of sarcasm detection. They observe an almost 10 point increase in performance when adding extralinguistic features to the textonly model and find such features perform well even without the textual features. Although their work is similar to ours, we explore more tasks and a different set of extralinguistic features, including inferred factors; we see our work as complementary to — and expanding on — theirs. Amir et al. (2016) outperformed Bamman and Smith (2015) on the same dataset by incorporating user embeddings, learned from users’ past tweets, 19 Sarcasm. Sarcasm detection replicates the work of Bamman and Smith (2015) by using the tweet features described in the paper (e.g n-grams, sentiment scores, Brown clusters) and evaluating on their dataset using a logistic regression classifier via ten-fold cross validation. The folds are split such that no user appears in both the training and testing sets. Bamman and Smith (2015)’s dataset was constructed by sampling tweets that did or did not contain hashtags indica"
W19-2103,W15-2905,0,0.190104,"notators of a stance detection dataset significantly impacted annotation quality, suggesting that considering user attributes is important not just during classification but also during dataset creation. Background Recent work has shown that considering language within the context of user attributes can improve classification accuracy (Volkova et al., 2013; Bamman et al., 2014; Yang and Eisenstein, 2015; Hovy, 2015; Kulkarni et al., 2016; Lynn et al., 2017). Other work has used network or other meta data, such as in Bamman and Smith (2015); Johnson and Goldwasser (2016); Joseph et al. (2017); Khattri et al. (2015). In a sense these trail-blazing works might be viewed as case studies on user attributes — identifying particular pieces of information for particular tasks where user information has lead to an advantage. We believe this is the first systematic study on the extent to which tasks are more easily achieved with user information or by combining user attributes with document language. In addition, prior work has explored what user attributes add on top of language, whereas we focus primarily on user attributes, with the contributions from document-level features being secondary. Models designed s"
W19-2103,Q14-1043,0,0.0292131,"n character n-grams, word n-grams, and features from multiple sentiment and emotion lexicons (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2010, 2013; Mohammad et al., 2013; Kiritchenko et al., 2014). The train/test split of the original dataset was used for evaluation. PP-Attachment. A prepositional-phrase attachment dataset for Twitter was constructed by combining annotated data from Tweebank (Kong et al., 2014) and Lynn et al. (2017). Candidate heads are ranked using an SVM-Rank (Joachims, 2006) model trained on n-gram, WordNet, and Treebank features similar to those used in Belinkov et al. (2014). Cross validation is used for evaluation. into a deep sarcasm model. Khattri et al. (2015) use past tweets to improve sarcasm detection by comparing the sentiment expressed towards an entity in the target tweet to that expressed in historical tweets. Martin et al. (2016) found that, when predicting retweet count, a user’s past success (measured as the average number of retweets received for other tweets in the past) was nearly as predictive as a model using all features they tried, including those drawn from the tweet itself. Jurgens et al. (2017) find that they are able to accurately predict"
W19-2103,W16-5614,0,0.0245126,"7), we train a ridge regression model on topic and n-gram features to predict real-valued political ideology scores between 1 (very conservative) and 7 (very liberal) from each user’s tweets. This model achieved a Pearson r = .374 through cross validation of the training data. User Embeddings. Five-dimensional latent factors were derived from each user’s prior tweets using the generative factor analysis approach proposed by Kulkarni et al. (2017). Factors obtained using this method have been shown to correlate with outcomes such as income and IQ. Profile Name. We used the Demographer package (Knowles et al., 2016) to predict gender from the profile name. We also used NamePrism (Ye et al., 2017) to predict scores for six ethnicities and thirty-nine nationalities. Profile Description & Location. Character 2- to 5-grams and word 1- to 3-grams were extracted from the users’ description and location fields. Profile Picture. Borrowing from a popular method in transfer learning, we used a pre-trained image classification model, Inception-v3 (Szegedy et al., 2016), to obtain 2048-dimensional embeddings from the next-to-last layer of the model. Followees. For each task, we identified the top 5000 Twitter accoun"
W19-2103,D14-1108,0,0.0227712,"nd neutral are available from the SemEval-2013 dataset (Nakov et al., 2013). We mostly replicate the top-performing system on this task (Mohammad et al., 2013) by training a linear SVM on character n-grams, word n-grams, and features from multiple sentiment and emotion lexicons (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2010, 2013; Mohammad et al., 2013; Kiritchenko et al., 2014). The train/test split of the original dataset was used for evaluation. PP-Attachment. A prepositional-phrase attachment dataset for Twitter was constructed by combining annotated data from Tweebank (Kong et al., 2014) and Lynn et al. (2017). Candidate heads are ranked using an SVM-Rank (Joachims, 2006) model trained on n-gram, WordNet, and Treebank features similar to those used in Belinkov et al. (2014). Cross validation is used for evaluation. into a deep sarcasm model. Khattri et al. (2015) use past tweets to improve sarcasm detection by comparing the sentiment expressed towards an entity in the target tweet to that expressed in historical tweets. Martin et al. (2016) found that, when predicting retweet count, a user’s past success (measured as the average number of retweets received for other tweets in"
W19-2103,P17-1068,0,0.0579542,"Missing"
W19-2103,D14-1121,0,0.0173381,"t happens to correlate well with some external quantity. Because we were interested in what a person’s language says about themselves, any discrepancies between a user’s predicted and actual attribute may provide additional predictive power: a 50-year-old whose writing style is more typical of a 20-year-old is likely better represented using their predicted age (20) than their actual age (50). Demographics & Personality. Real-valued estimates of these attributes were obtained by applying pre-existing predictive lexica to each user’s past tweets. Age and gender were obtained from the models of Sap et al. (2014). For personality, we used Park et al. (2015) to predict each of the Big Five traits: openness, conscientiousness, extroversion, agreeableness, and neuroticism. Political Ideology. Using the dataset from Preot¸iuc-Pietro et al. (2017), we train a ridge regression model on topic and n-gram features to predict real-valued political ideology scores between 1 (very conservative) and 7 (very liberal) from each user’s tweets. This model achieved a Pearson r = .374 through cross validation of the training data. User Embeddings. Five-dimensional latent factors were derived from each user’s prior tweet"
W19-2103,D17-1119,1,0.884492,"University, 2 University of Pennsylvania {velynn, niranjan, has}@cs.stonybrook.edu, sgiorgi@sas.upenn.edu Abstract Our work aims to answer the following questions: 1) What and how much information do user attributes alone carry for different social media tasks, particularly for predictive tasks that are more about the user than the document (e.g. stance)? 2) When are user attributes useful and what do language features contribute in these cases? While there are multiple works that show that adding user attributes is useful for different prediction tasks (Hovy, 2015; Zamani and Schwartz, 2017; Lynn et al., 2017), there is no single systematic study that answers these questions. To this end we conduct a systematic evaluation of user attribute-only models on multiple tasks including stance detection, sarcasm detection, sentiment analysis, and prepositional phrase attachment. We evaluate the impact of user attribute-only models through a range of features derived from publicly available information about the users including: written profile bio, inferred demographics and personality, self-reported location, profile picture, who one follows in a social network, and a background of users’ past language. T"
W19-2103,P15-1012,0,0.0147556,"ch capture more “trait-like” human attributes (those that are stable over time, e.g. stance) benefit more from user-level information as compared to “state-like” attributes (frequently changing, e.g. sentiment). We evaluate this theory by looking at the role of user attributes across different predictive tasks. (3) We provide a set of considerations and metrics, for task participants and designers alike, for the inclusion of user information within new social science-related tasks. 2 Some work in stance detection has focused on document context and discourse structure (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned user embeddings for stance"
W19-2103,S16-1003,0,0.107538,"Missing"
W19-2103,W06-1639,0,0.0628667,"ser-level information as compared to “state-like” attributes (frequently changing, e.g. sentiment). We evaluate this theory by looking at the role of user attributes across different predictive tasks. (3) We provide a set of considerations and metrics, for task participants and designers alike, for the inclusion of user information within new social science-related tasks. 2 Some work in stance detection has focused on document context and discourse structure (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned user embeddings for stance detection and found that the inclusion of such embeddings significantly improved model performance. Going in a"
W19-2103,S13-2053,0,0.0291852,"their dataset using a logistic regression classifier via ten-fold cross validation. The folds are split such that no user appears in both the training and testing sets. Bamman and Smith (2015)’s dataset was constructed by sampling tweets that did or did not contain hashtags indicating sarcasm (e.g. I love when it snows #sarcastic); these hashtags were removed during preprocessing. Sentiment. Message-level sentiment annotations indicating positive, negative, and neutral are available from the SemEval-2013 dataset (Nakov et al., 2013). We mostly replicate the top-performing system on this task (Mohammad et al., 2013) by training a linear SVM on character n-grams, word n-grams, and features from multiple sentiment and emotion lexicons (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2010, 2013; Mohammad et al., 2013; Kiritchenko et al., 2014). The train/test split of the original dataset was used for evaluation. PP-Attachment. A prepositional-phrase attachment dataset for Twitter was constructed by combining annotated data from Tweebank (Kong et al., 2014) and Lynn et al. (2017). Candidate heads are ranked using an SVM-Rank (Joachims, 2006) model trained on n-gram, WordNet, and Treebank feature"
W19-2103,D13-1187,0,0.592308,"rofile bio, inferred demographics and personality, self-reported location, profile picture, who one follows in a social network, and a background of users’ past language. The evaluations show that user attributes can have a large impact and, depending on the nature of the task, even outperform document-only features — inference on a document without even looking at its contents! We conduct further evaluations comparing document contributions to an inference task relative to user-level features. Recent research has explored how user-level attributes add value on top of document-level language (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Zamani et al., 2018). Instead we quantify how well user attributes alone can predict and then what document-level language can uniquely add, identifying cases where the document is essential. Contributions. Our specific contributions are three-fold: (1) We show that the stance of a NLP naturally puts a primary focus on leveraging document language, occasionally considering user attributes as supplemental. However, as we tackle more social scientific tasks, it is possible user attributes might be of primary importance and the document supplemental. Here, we syst"
W19-2103,W10-0204,0,0.0576574,"ith (2015)’s dataset was constructed by sampling tweets that did or did not contain hashtags indicating sarcasm (e.g. I love when it snows #sarcastic); these hashtags were removed during preprocessing. Sentiment. Message-level sentiment annotations indicating positive, negative, and neutral are available from the SemEval-2013 dataset (Nakov et al., 2013). We mostly replicate the top-performing system on this task (Mohammad et al., 2013) by training a linear SVM on character n-grams, word n-grams, and features from multiple sentiment and emotion lexicons (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2010, 2013; Mohammad et al., 2013; Kiritchenko et al., 2014). The train/test split of the original dataset was used for evaluation. PP-Attachment. A prepositional-phrase attachment dataset for Twitter was constructed by combining annotated data from Tweebank (Kong et al., 2014) and Lynn et al. (2017). Candidate heads are ranked using an SVM-Rank (Joachims, 2006) model trained on n-gram, WordNet, and Treebank features similar to those used in Belinkov et al. (2014). Cross validation is used for evaluation. into a deep sarcasm model. Khattri et al. (2015) use past tweets to improve sarcasm detection"
W19-2103,N12-1072,0,0.01588,"a theory that tasks which capture more “trait-like” human attributes (those that are stable over time, e.g. stance) benefit more from user-level information as compared to “state-like” attributes (frequently changing, e.g. sentiment). We evaluate this theory by looking at the role of user attributes across different predictive tasks. (3) We provide a set of considerations and metrics, for task participants and designers alike, for the inclusion of user information within new social science-related tasks. 2 Some work in stance detection has focused on document context and discourse structure (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned"
W19-2103,walker-etal-2012-corpus,0,0.0112033,"a theory that tasks which capture more “trait-like” human attributes (those that are stable over time, e.g. stance) benefit more from user-level information as compared to “state-like” attributes (frequently changing, e.g. sentiment). We evaluate this theory by looking at the role of user attributes across different predictive tasks. (3) We provide a set of considerations and metrics, for task participants and designers alike, for the inclusion of user information within new social science-related tasks. 2 Some work in stance detection has focused on document context and discourse structure (Walker et al., 2012a,b; Sridhar et al., 2015), though user attributes have been considered as well. When predicting stance for debates, Thomas et al. (2006) and Hasan and Ng (2013) benefited from enforcing the constraint that multiple statements from the same person should receive the same predicted stance, making the assumption that stance is unlikely to change over the course of a single conversation. Johnson and Goldwasser (2016), who predict the stance of Twitter users as opposed to individual tweets, consider both temporal activity and political party affiliation in their models. Chen and Ku (2016) learned"
W19-2103,H05-1044,0,0.0341903,"g sets. Bamman and Smith (2015)’s dataset was constructed by sampling tweets that did or did not contain hashtags indicating sarcasm (e.g. I love when it snows #sarcastic); these hashtags were removed during preprocessing. Sentiment. Message-level sentiment annotations indicating positive, negative, and neutral are available from the SemEval-2013 dataset (Nakov et al., 2013). We mostly replicate the top-performing system on this task (Mohammad et al., 2013) by training a linear SVM on character n-grams, word n-grams, and features from multiple sentiment and emotion lexicons (Hu and Liu, 2004; Wilson et al., 2005; Mohammad and Turney, 2010, 2013; Mohammad et al., 2013; Kiritchenko et al., 2014). The train/test split of the original dataset was used for evaluation. PP-Attachment. A prepositional-phrase attachment dataset for Twitter was constructed by combining annotated data from Tweebank (Kong et al., 2014) and Lynn et al. (2017). Candidate heads are ranked using an SVM-Rank (Joachims, 2006) model trained on n-gram, WordNet, and Treebank features similar to those used in Belinkov et al. (2014). Cross validation is used for evaluation. into a deep sarcasm model. Khattri et al. (2015) use past tweets t"
W19-2103,D18-1392,1,0.905384,"lf-reported location, profile picture, who one follows in a social network, and a background of users’ past language. The evaluations show that user attributes can have a large impact and, depending on the nature of the task, even outperform document-only features — inference on a document without even looking at its contents! We conduct further evaluations comparing document contributions to an inference task relative to user-level features. Recent research has explored how user-level attributes add value on top of document-level language (Volkova et al., 2013; Hovy, 2015; Lynn et al., 2017; Zamani et al., 2018). Instead we quantify how well user attributes alone can predict and then what document-level language can uniquely add, identifying cases where the document is essential. Contributions. Our specific contributions are three-fold: (1) We show that the stance of a NLP naturally puts a primary focus on leveraging document language, occasionally considering user attributes as supplemental. However, as we tackle more social scientific tasks, it is possible user attributes might be of primary importance and the document supplemental. Here, we systematically investigate the predictive power of user-l"
W19-2103,E17-2005,1,0.932442,"ew Schwartz1 1 Stony Brook University, 2 University of Pennsylvania {velynn, niranjan, has}@cs.stonybrook.edu, sgiorgi@sas.upenn.edu Abstract Our work aims to answer the following questions: 1) What and how much information do user attributes alone carry for different social media tasks, particularly for predictive tasks that are more about the user than the document (e.g. stance)? 2) When are user attributes useful and what do language features contribute in these cases? While there are multiple works that show that adding user attributes is useful for different prediction tasks (Hovy, 2015; Zamani and Schwartz, 2017; Lynn et al., 2017), there is no single systematic study that answers these questions. To this end we conduct a systematic evaluation of user attribute-only models on multiple tasks including stance detection, sarcasm detection, sentiment analysis, and prepositional phrase attachment. We evaluate the impact of user attribute-only models through a range of features derived from publicly available information about the users including: written profile bio, inferred demographics and personality, self-reported location, profile picture, who one follows in a social network, and a background of use"
W19-2103,S16-1074,0,0.0594617,"e as a real concern, feminism, Hillary Clinton, and legalization of abortion. Note that neutral does not indicate “neither for nor against”, but rather not enough information to say either way (for example, “I know who I’m voting for!” would be neutral towards Hillary Clinton). Similar to the top baseline system in this task, we train a logistic regression classifier on character ngrams of size two to five and word n-grams of size one to three. We preserve the train/test split of the original dataset. For evaluation purposes, we obtain the predictions from the top participating system, MITRE (Zarrella and Marsh, 2016), and subset them to our test set. Task Tweets Users Instances Stance Sarcasm Sentiment PP-Attachment 3021 17084 10339 1319 2349 10966 9917 1319 3021 17084 10339 2365 Table 1: Number of tweets, users, and instances represented in each task. 3.2 User Attribute Features Each user’s name, location, description, and picture were extracted from their Twitter profile. We also collected up to 200 of their tweets, excluding retweets and those included in the task data. Finally, we collected a list of every account that each user follows. Features were derived from this data as described below. We excl"
W19-3005,D14-1121,0,0.12352,"Missing"
W19-3005,N19-1423,0,0.0326671,"istory (including their r/SuicideWatch posts). The third task (Task C) consisted of users’ entire Reddit post history apart from posts in r/SuicideWatch. Additionally Task C includes a set of ‘control users’ who are labeled as no risk1 . Task A and B shared the same number of users(Training = 496, Test = 128), while Task C had 993 training and 248 test. Open-Vocabulary Features. We also included higher dimensional features meant to capture open ended content. This included dimensionally reduced BERT embeddings – originally a 768dimensional representation is extracted from a pre-trained model (Devlin et al., 2019) for post contents and titles (separately). Given the training sizes, we decided to further reduce these dimensions down to 50 and 20 dimensions for body and title respectively, using non-negative matrix factorization (NMF) (F´evotte and Idier, 2011). Following successful use of topics for mental health modeling in the past (Eichstaedt et al., 2018), we also inferred 25 LDA Topics (Blei et al., 2003) trained using Gibb’s Sampling over suicide watch posts excluding words used more frequently outside of the forum. Ethics Statement: This research was evaluated by an institutional review board and"
W19-3005,W14-3214,1,0.71964,"vidual to alleviate the risk. 2 Data The dataset was collected from Reddit, released as the CLPsych 2019 Shared Task (Zirikly et al., 2019), where collections of users’ posts were annotated into 4 suicide risk categories (no risk, low, moderate, severe) and then aggregated into sin39 Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 39–44 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics et al., 2014), assessments of big-5 personality traits (Schwartz et al., 2013) as well as trait anxiety, anger, and depression (Schwartz et al., 2014). gle labels representing their highest suicide risk across all collections (Shing et al., 2018). All users had posted in r/SuicideWatch and had at least 10 posts total across the platform. The task of suicide risk prediction was sub-divided into 3 sub-tasks, each based on different levels of data. The first task (Task A) consisted of users’ posts from r/SuicideWatch annotated for suicide risk level. The second (Task B) consisted of the same users as in Task A and included their entire Reddit post history (including their r/SuicideWatch posts). The third task (Task C) consisted of users’ entir"
W19-3005,D17-2010,1,0.886962,"Missing"
W19-3005,W18-0603,0,0.478109,"Missing"
W19-3005,D17-1119,1,0.859863,"ount by having a separate GRU cell). We used the separate attention weights for SuicideWatch (SW) GRU hidden vectors and non-SuicideWatch (NSW) GRU hidden vectors as following: → −−−→ [− v− SW ; vN SW ] = hX −−→ X −−→i αsw hsw ; αnsw hnsw Figure 3: Dual-context, RNN-attention, use-factor adaptation architecture used in Task B. The left RNN handles features related to suicide watch posts and the right RNN handles non-SuicideWatch. User factors are multiplied into the concatenated vector for adaptation, as well as simply concatenated before softmax layer Then, we applied user-factor adaptation (Lynn et al., 2017) to the concatenation of the sum of hidden vectors with attentions of the SW GRU cell and the NSW GRU cell as following: − → → −−−→ −−→ −−−→ f v = [F0 ×[− v− SW ; vN SW ]; . . . ; [FN ×[vSW ; vN SW ]] Task C. We build logistic regression models using a) BERT embeddings alone: ’Bert’; b) openvocabulary, theoretical dimensions, meta-features, and subreddit latent factors ‘OpenTheoryUser’; and c) same as b but without user traits of personality, age/gender, and anxiety, anger, depression scores ‘OpenTheorySubr’. Here, we used age, gender, and latent factors of users with the following transformat"
W19-3005,P18-1017,0,0.0127752,"ociated with lower suicide risk while higher neuroticism was positively correlated with higher suicide risk. Prior studies have found similar associations in other samples through traditional surveys (Velting, 1999) establishing that language on social media Theoretical dimensions. Our theoretical dimensions ranged from capturing message-level user states (able to change) to user-level traits (slow changing). The Messsage-level states, calculated separately for both the title and content, included affect and intensity (Preot¸iuc-Pietro et al., 2016) as well as valence, arousal, and dominance (Mohammad, 2018). These features were generated per-message and aggregated to the users. User-level traits included language-based inferences of demographics age/gender (Sap 1 Control users are those who have no r/SuicideWatch or other mental health subreddit posts 40 Dimension Age Gender Anger Anxiety Depression r – .14 .32 .33 .32 Dimension Agreeableness Conscientious. Extroversion Neuroticism Openness r -.14 -.14 -.17 .32 – pression, and mean affect scores of users belonging to each risk level. For emotional stability, as the value gets lower the less stability a person expresses, which holds across the ri"
W19-3005,N16-1174,0,0.0269469,"ression alone for Task C. All non-neural models were implemented via the DLATK Python package (Schwartz et al., 2017). Figure 1: Topics correlated with higher risk (blue, top 4 rows) and lower risk (red, bottom row), treating risk as a continuous value. All correlations significant at p &lt; .05, Benjamini-Hochberg corrected. Task A. The logistic regression model used open-vocabulary, theoretical, and meta-features as input (termed as ‘OpenTheory’). We also evaluated the performance of BERT embeddings alone (termed as ‘Bert’). The neural model used an LSTM with hierarchical post-level attention (Yang et al., 2016). We fed it the concatenation of openvocabulary features, Affect, Intensity, and VAD NRC Lexicon scores of each SuicideWatch post. The model was run on all posts of each user in the time order of their posting to make a prediction on the risk level of each user. This model is referred to as DeepAtt. forums could be a good proxy for measuring suicidal ideation. Corroborating these findings, users with high anger, anxiety and depression scores were associated with higher suicide risk. We also analyze the correlations between r/SuicideWatch topic dimensions, as shown in figure 1. Here, we showcas"
W19-3005,W19-3003,0,0.158903,"their own life (Nock et al., 2008). With deaths by suicide increasing substantially (Curtin et al., 2016), researchers are turning to automated analysis of user generated content to potentially provide methods for early detection of suicide risk severity (Coppersmith et al., 2018; De Choudhury et al., 2016; Shing et al., 2018). If an automated process could detect elevated risk in a person, personalized (potentially digital and early) interventions could be provided to the individual to alleviate the risk. 2 Data The dataset was collected from Reddit, released as the CLPsych 2019 Shared Task (Zirikly et al., 2019), where collections of users’ posts were annotated into 4 suicide risk categories (no risk, low, moderate, severe) and then aggregated into sin39 Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 39–44 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics et al., 2014), assessments of big-5 personality traits (Schwartz et al., 2013) as well as trait anxiety, anger, and depression (Schwartz et al., 2014). gle labels representing their highest suicide risk across all collections (Shing et al., 2018). All users had poste"
W19-3005,W16-0404,1,0.890955,"Missing"
