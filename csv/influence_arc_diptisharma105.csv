2019.icon-1.15,I17-4008,1,0.892225,"Missing"
2019.icon-1.15,bakliwal-etal-2012-hindi,0,0.0783715,"Missing"
2019.icon-1.15,S14-2038,0,0.0213083,"टवेयर और परफॉमेरस ं Aspect Class In Roman software Specification aur feature hamara faisla camera aur battery life specification aur software camera performance look vah banawat battery life hamara faisla camera performance design design aur look NULL specification design aur build design aur display specification, software aur performance Count 52 360 9 5 137 76 826 26 1 300 16 138 168 352 139 390 49 40 Table 1: Class Set sisted of English reviews annotated at sentence level with their aspects followed by their polarity. Some of the systems that emerged who targeted this task were Zhiqiang Toh (2014), Chernyshevich (2014); Joachim Wagner and Tounsi (2014); Giuseppe Castellucci (2014), Shweta Yadav (2015). However, almost all these systems are related to some specific languages, especially English. In 2016, SemEval released new datasets of similar domains(mobile, laptop, restaurant) 1 but in multiple languages. In 2016, the datasets were released in English, Arabic, Chinese, Dutch, French, Russian, Spanish and Turkish. But this area of field is largely unexplored in Indian languages due to the unavailability of high quality datasets and other tools and resources required. The datasets whic"
2019.icon-1.15,S14-2135,0,0.0459724,"Missing"
2019.icon-1.15,S14-2036,0,0.0589912,"Missing"
2019.icon-1.18,P92-1002,0,0.713835,"on of a non-finite verb phrase, introduced by an auxiliary or the particle ‘to’(Kenyon-Dean et al., 2016). 1 https://ltrc.iiit.ac.in/Anusaaraka/anu_ home.html 150 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 150–159 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) serial verbs, and verb complements in English. impossible antecedents by looking at be-do conflicts, contained antecedents and assigned scores based on co-reference of the noun subjects and clausal relationships, such as ‘as’ constructions. (Hardt, 1992) Our solution is based on an understanding of verb phrase ellipsis in Hindi. Hindi does not exhibit VPE in the same manner as English; however, it exhibits a phenomenon called verb-stranding verb phrase ellipsis (VVPE) (Manetta, 2018). This means that a verbal phrase, including objects and other arguments, may be completed elided, stranding the head verb, which then appears at the site of ellipsis. The necessity for tools to deal with VPE is widely recognized in literature, for the purposes of information extraction, finding event co-occurrence, etc (Kenyon-Dean et al., 2016). In the context o"
2019.icon-1.18,D16-1179,0,0.0604664,"tification 2) identification of the antecedent head verb 3) addition of necessary auxiliaries and/or complements to the head verb. Finally, it modifies the input sentence by inducing the identified verb segment at the site of ellipsis. We tailor this algorithm for the purpose of English-Hindi MT, and therefore provide a special treatment to compound verbs (including phrasal verbs), Introduction Verb phrase ellipsis is a particularly frequent form of ellipsis, both in speech and in text. English VPE is the elimination of a non-finite verb phrase, introduced by an auxiliary or the particle ‘to’(Kenyon-Dean et al., 2016). 1 https://ltrc.iiit.ac.in/Anusaaraka/anu_ home.html 150 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 150–159 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) serial verbs, and verb complements in English. impossible antecedents by looking at be-do conflicts, contained antecedents and assigned scores based on co-reference of the noun subjects and clausal relationships, such as ‘as’ constructions. (Hardt, 1992) Our solution is based on an understanding of verb phrase ellipsis in Hindi. Hindi does not e"
2019.icon-1.18,W16-0705,0,0.0131802,"t to tailor English VPE resolution in the context of English-Hindi MT. There have been several previous works addressing the problem of antecedent head resolution for VPE in English. Cheung et al adapt the Margin-Infused-Relaxed Algorithm (MIRA) for target detection and antecedent resolution and obtain an accuracy of 65 percent (Kenyon-Dean et al., 2016). Nielson, 2005, re-implements Daniel Hardt’s VPE-RES algorithm on the Penn Treebank to obtain a highest Head Overlap success of 85.87 percent and a lower Exact Match success, about 78 percent on the Brown corpus (Nielsen, 2005) (Hardt, 1992). Liu et al, 2016 experiment with various joint modelling techniques, and obtain a recall of 83.46 percent for boundary Earlier identification (Liu et al., 2016). works include Daniel Hardt’s linguistically motivated rule-based system, that eliminates 3 Ellipsis in English Verb phrase ellipsis in English is introduced by an auxiliary. We consider five classes of el151 lipsis depending upon the auxiliary at the site of ellipsis: 1) to_be, 2) to_have, 3) to_do, 4) modals, and 5) to_particle ellipsis. Cheung et al include a sixth class: the do-so anaphora, while acknowledging that modals and DoX anaphora are not"
2019.icon-1.18,H92-1073,0,0.0722981,"ion in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of ellipsis 4) Modify input sentence; i.e. resolving VPE and inducing the required verb segment. This system is tested in two parts. It obtains 94.83 percent precision and 83.04 percent recall on subtask (1), tested on 3900 sentences from the BNC corpus (Leech, 1992). This is competitive with state-of-the-art results. We measure accuracy of subtasks (2) and (3) together, and obtain a 91 percent accuracy on 200 sentences taken from the WSJ corpus(Paul and Baker, 1992). Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55. 1 Ram cooked the food quickly, but Shyam did not → *Ram ne jaldi se Ram-ERG - quickly-ABL khana banaya, lekin Shyam food-OBJ cook-PT, but Shyam-ERG ne nahi diya. - not give-PT (created example) In our initial analysis, we find that Google Translate could translate only 6/50 VPE sentences, taken f"
2019.icon-1.18,P19-1116,0,0.02857,"text. If we are seeking to eliminate ellipsis with our system, we must select one of these interpretations to paste at the site of ellipsis, or the whole verb phrase. We may also not adopt intermediate policies such as importing noun objects but not adjuncts, because this risks placing undue emphasis on certain arguments, and may result in an incorrect semantics. 2. All non-to_be forms of ellipsis do not have an antecedent with a to_be auxiliary, except for gerunds, which are permissible. 4 Antecedent Resolution in the Context of MT We know that state-of-the-art machines cannot interpret VPE.(Voita et al., 2019). Translation to Hindi from English requires a prediction of the elided English verb. An elided VP 152 The second problem with this strategy is that it may render sentences, after pasting, clumsy, unnatural or nonsensical, as illustrated, respectively, by the following example, taken from the WSJ corpus (Bos and Spenader, 2011). before translating, now, then we create a valid, syntactical Hindi sentence with all the interpretations of the original sentence intact. We illustrate with an example: a (Original sentence) Ram would have liked to eat out with you on Sunday afternoon, but he can’t. Th"
2019.icon-1.22,E14-2007,0,0.0230587,"Missing"
2019.icon-1.22,aziz-etal-2012-pet,0,0.0335632,"ranslation memories, glossaries, concordances and terminologies). 3 Related Work There are some translation workbenches with separate resource management systems such as: Smartcat 1 , Memsource 2 and Matecat (Federico et al., 2014) are web based CAT tools, provide central resources management for a project but they do not provide the provision for the linguistic resources. CASMACAT (Alabau et al., 2014) is a translation workbench which is web based and offers advanced functionality for computer-aided translation. It offers TMs but it also lacks the provision for the linguistic resources. PET (Aziz et al., 2012) is a tool to postedit to for evaluating In this paper, background and motivation for Kunji is described in Section-2 followed by related work in Section-3. The detailed architecture and functionality of Kunji is described in Section1 2 185 https://www.smartcat.ai https://www.memsource.com the quality of translations. It evaluates the efforts required in translations in order to be fixed. SDL Trados, MemoQ3 and Anubis (Jaworski, 2013) are CAT tools which are not web based and both lack the provision to use linguistic resources. Brat (Stenetorp et al., 2012) is basically a web based annotation"
2019.icon-1.22,C14-2028,0,0.0713302,"Missing"
2019.icon-1.22,I08-2141,0,0.239345,"ssaries to translators but there is a lack of the resource management systems which provide the provision by which a translator can use additional linguistic resources like NE, MWE and lexeme dictionaries. Additionally there is a lack of the provisions of reuse, management and verification of these resources which can lead a process to improvement and development of the MT systems. There are several tools for resource development but they are built with separate purposes and languages as they are for task specific. Like for annotators they have separate tools in Indian languages like Sanchay (Singh, 2008) but it is desktop based application. Some tools like GATE (Cunningham, 2002) addresses the some of the problems faced by NLP researchers while developing NLP resources but it is desktop based application. It motivated us to develop Kunji, a resource management tool, which facilitates the translators in translator workbenches as well as MT system development process. It tries to address the issues faced by both translators as well as MT module developers. It allows the translators to use and manage the linguistic resources along with their terminologies and glossaries. Additionally we performe"
2020.acl-srw.19,I08-2099,1,0.650058,"); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed for Indian languages. The treebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependency tree The treebank i"
2020.acl-srw.19,J95-3006,0,0.793513,"Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed for Indian languages. The treebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependenc"
2020.acl-srw.19,W06-2920,0,0.177613,"ebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependency tree The treebank is converted from SSF to CoNLLX format (Buchholz and Marsi, 2006)2 . 4 Our Approach We propose to replace the rich hand-crafted feature templates used in Malt parser systems with a minimally defined feature set which uses automatically learned word representations from BERT. We do not make use of any additional pipeline features like POS or morphological information assuming this information is captured within the vectors. We train a BERT baseline model (Devlin et al., 2019) on the Telugu wikipedia data, which comprises 71289 articles. We use the ILMT tokenizer included in the Telugu shallow parser 3 to segment the data into sentences. The sentence segmente"
2020.acl-srw.19,K18-2005,0,0.154549,"istic features using dense vector representations in a neural network based parser (Tandon and Sharma, 2017). Recent developments in the field of NLP led to the arrival of contextual word vectors (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) and their extensive use in downstream NLP tasks, from POS tagging (Peters et al., 2018) to more complex tasks like Question Answering and Natural Language Inference tasks (Devlin et al., 2019). Contextual vectors have also been applied to dependency parsing systems. The top-ranked system in CoNLL-2018 shared task on Dependency Parsing(Che et al., 2018) used ELMo representations along with conventional word vectors in a graph based parser. Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is"
2020.acl-srw.19,D14-1082,0,0.0525434,"ons (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dep"
2020.acl-srw.19,N19-1423,0,0.456537,"et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative lan"
2020.acl-srw.19,P18-1031,0,0.119682,"like part-of-speech and morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that fo"
2020.acl-srw.19,W12-5617,0,0.0251942,"nsition based approach. The resulting parser has a very simple architecture with minimal feature engineering and achieves state-of-the-art results for Telugu. 1 Introduction Dependency parsing is extremely useful for many downstream tasks. However, robust dependency parsers are not available for several Indian languages. One reason is the unavailability of annotated treebanks. Another reason is that most of the existing dependency parsers for Indian languages use hand-crafted features using linguistic information like part-of-speech and morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model"
2020.acl-srw.19,Q16-1023,0,0.0183137,"10; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused"
2020.acl-srw.19,D19-1279,0,0.144727,"in the field of NLP led to the arrival of contextual word vectors (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) and their extensive use in downstream NLP tasks, from POS tagging (Peters et al., 2018) to more complex tasks like Question Answering and Natural Language Inference tasks (Devlin et al., 2019). Contextual vectors have also been applied to dependency parsing systems. The top-ranked system in CoNLL-2018 shared task on Dependency Parsing(Che et al., 2018) used ELMo representations along with conventional word vectors in a graph based parser. Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed fo"
2020.acl-srw.19,D19-1277,0,0.0296941,"Missing"
2020.acl-srw.19,P18-1130,0,0.0163753,"harma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused on rule based systems (Kesidi et al., 2011) or data-d"
2020.acl-srw.19,W04-0308,0,0.389163,"of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative language like Telugu, just three word features with good quality vector representations can effectively capture the information required for parsing. We put the feature representations through a feed forward network and train using a greedy transition based parser (Nivre, 2004, 2008). Past work on Telugu dependency parsing has only been focused on predicting inter-chunk dependency relations (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several"
2020.acl-srw.19,J08-4003,0,0.118423,"sequentially and treat parsing as a sequence of actions that produce a parse tree. They predict a sequence of transition operations starting from an initial configuration to a terminal configuration, and construct a dependency parse tree in the process. A configuration consists of a stack, an input buffer of words, and a set of relations representing a dependency tree. They make use of a classifier to predict the next transition operation based on a set of features derived from the current configuration. A couple of widely used transition systems are Arc-standard (Nivre, 2004) and Arc-eager (Nivre, 2008). We make use of the Arc-standard transition system in our parser and briefly describe it here. 4.1.1 • LEFT-ARC: Adds a head-dependent relation between the word at the top of stack and the 5 • RIGHT-ARC: Adds a head-dependent relation between the second word on the stack and the top word and removes the top word from the stack. • SHIFT: Moves the word from the front of the buffer onto the stack. In the labeled version of parsing, there are a total of 2` + 1 transitions, where ` is the number of different dependency labels. There is a left-arc and a right-arc transition corresponding to each l"
2020.acl-srw.19,nivre-etal-2006-maltparser,0,0.11085,"ional Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused on rule based systems (Kesidi et al., 2011) or data-driven transition based systems (Kanneganti et al., 2016) using Malt parser (Nivre et al., 2006). The Malt parser uses a classifier to predict the transition operations taking a feature template as input. The feature templates used in Telugu parsers commonly consist of several hand-crafted features like words, their partof-speech tags, gender, number and other morphological features (Kosaraju et al., 2010; Kanneganti et al., 2016). There has been some work done on representing these linguistic features using dense vector representations in a neural network based parser (Tandon and Sharma, 2017). Recent developments in the field of NLP led to the arrival of contextual word vectors (Howard"
2020.acl-srw.19,L16-1262,0,0.0399096,"Missing"
2020.acl-srw.19,N18-1202,0,0.669699,"morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically r"
2020.acl-srw.19,P16-1162,0,0.0821748,"uses automatically learned word representations from BERT. We do not make use of any additional pipeline features like POS or morphological information assuming this information is captured within the vectors. We train a BERT baseline model (Devlin et al., 2019) on the Telugu wikipedia data, which comprises 71289 articles. We use the ILMT tokenizer included in the Telugu shallow parser 3 to segment the data into sentences. The sentence segmented data consists of approximately 2.6M sentences. We convert all of the data from UTF to WX4 notation for faster processing. We use byte-pair encoding (Sennrich et al., 2016) to tokenize the data and generate a vocabulary file. We pass this vocabulary 144 1 https://github.com/ltrc/ Shift-Reduce-Chunk-Expander 2 https://github.com/ltrc/ SSF-to-CONLL-Convertor 3 https://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 4 https://en.wikipedia.org/wiki/WX_ notation file to BERT 5 for pre-training. After pre-training, we extract contextual token representations for all the sentences in the treebank from the pre-trained BERT model. In case a single word is split into multiple tokens, we treat these tokens as continuous bag of words and add the represen"
2020.acl-srw.19,W17-6529,1,0.893888,"feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative language like Telugu, just three word features with good quality vector representations can effectively capture the information required for parsing. We put the feature representations through a feed forward network and train using a greedy transition based parser (Nivre, 2004, 2008). Past work on Telugu dependency parsing has only been focused on predicting inter-chunk dependency relations (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2"
2020.acl-srw.19,P19-1452,0,0.0250688,"t al. (2018) and Che et al. (2018) suggest that concatenating conventional word vectors with contextual word vectors could result in a boost in accuracies. We try out the same by concatenating word2vec vectors with BERT vectors and observe some improvement in label scores. The results are mentioned in Table 6. BERT layers: We also experiment with vector representations from different layers of BERT. The results are mentioned in Table 7. We find that the 4th layer from the top of our BERT baseline model results in the highest accuracy for the parser. This finding is consistent with the work of Tenney et al. (2019) which suggests that dependencies are better captured between layers 6 and 9. We use the vector representations from 4th layer from the top in all our experiments. 146 BPE vs Inverse-BPE: Byte-pair encoding (Sennrich et al., 2016) segments words from left to right. In Telugu, most grammatical information System Annotation Method UAS LS LAS Baseline Baseline + POS Baseline + POS + suffix Tandon et al, 2017 re-impl This work Intra-chunk Intra-chunk Intra-chunk Intra-chunk Intra-chunk MLP with word MLP with word, POS MLP with word, POS, suffix MLP with word, POS, suffix MLP using BERT 84.56 88.90"
2020.acl-srw.22,jha-2010-tdil,0,0.0203459,"adding more languages into one model may result in better knowledge transfer (i.e multilingual NMT) but it can also result in ambiguities between languages at the inference time. Accordingly, a multilingual NMT model fine-tuned on the language pair of interest can potentially remove all the inconsistencies at the inference time. 3 3.1 Experimental Settings Dataset In our experiments, we use the IIT-Bombay (Kunchukuttan et al., 2017) parallel data for HindiEnglish. The training corpus consists of data from mixed domains. We use the multilingual ILCI (Indian Language Corpora Initiative) corpus (Jha, 2010), which contains roughly 50,000 parallel sentences for each of the Indian languages ( Gujarati, Punjabi, Marathi, Bengali) and also for English. The ILCI data is from tourism and health domains. For every XX-EN language pair ( where XX is Gujarati, Marathi, Bengali or Punjabi), the English side of the data is same because of the multilingual nature of the corpus. We check and clean the ILCI corpus manually as it contains a lot of misalign165 ments and mistranslations. Table 1: Statistics of our cleaned and processed parallel data, where XX is Gujarati, Marathi, Bengali or Punjabi 3.2 Dataset S"
2020.acl-srw.22,W18-1817,0,0.0129042,"X-EN Dev 1,528,631 46,490 2,000 500 Data Processing We use the Moses (Koehn et al., 2007) toolkit1 for tokenization and cleaning the English side of the data. All the Indian language data is first normalized with the Indic NLP library2 followed by tokenization with the same library. As our preprocessing step, we remove all sentences of length greater than 80 words from our training corpus and lowercase the English side of the data. In all cases, we use BPE segmentation with 16k merge operations as described in Section 2.2. 3.3 Training Details For all of our experiments, we use the OpenNMTpy (Klein et al., 2018) toolkit3 . We use the Transformer model with 6 layers in both the encoder and decoder, each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use the Adam (Kingma and Ba, 2014) optimizer to optimize model parameters. We validate the model every 5,000 steps via BLEU (Papineni et al., 2002) and perplexity on the development set. We train all our NMT models for 150k steps except for finetuning which is done for 10k steps. After translation at the test time, we rejoin the translat"
2020.acl-srw.22,W18-6325,0,0.0315764,"anguages) becomes a primary challenge. The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Some of the most well-known methods to improve NMT models with monolingual data range from backtranslation (Sennrich et al., 2016), dual learning (He et al., 2016) to Unsupervised MT (Artetxe et al., 2017; Lample et al., 2017, 2018). Similarly, parallel data from other languages can be exploited to either pretrain the network or jointly learn the representations (Zoph et al., 2016; Firat et al., 2017; Johnson et al., 2017; Kocmi and Bojar, 2018). Currently, Transfer Learning (TL) is being widely used for low-resource language translation because it is one of the vital directions for addressing the data sparsity problem in low-resource NMT (Zoph et al., 2016; Nguyen and Chiang, 2017; Pass162 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 162–168 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ban et al., 2017; Kocmi and Bojar, 2018). However, most of the existing approaches that take advantage of transfer learning have a major limitatio"
2020.acl-srw.22,P09-5002,0,0.00934081,"n recent years, Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) (NMT) has become the most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. However, in order to reach high accuracies, NMT systems tend to require very large parallel training corpora (Koehn and Knowles, 2017). As a matter of fact, such corpora are not yet available for many language pairs. Indian languages are not an exception to this; however they are extremely diverse, belonging to different language families, employing various scripts and spanning a multitude of dialects. The majority of Indian languages are morphologically rich and depict unique characteristics, which are significantly different from languages such as"
2020.acl-srw.22,P07-2045,0,0.0179313,"English. The ILCI data is from tourism and health domains. For every XX-EN language pair ( where XX is Gujarati, Marathi, Bengali or Punjabi), the English side of the data is same because of the multilingual nature of the corpus. We check and clean the ILCI corpus manually as it contains a lot of misalign165 ments and mistranslations. Table 1: Statistics of our cleaned and processed parallel data, where XX is Gujarati, Marathi, Bengali or Punjabi 3.2 Dataset Sentences IITB HI-EN Train ILCI XX-EN Train ILCI XX-EN Test ILCI XX-EN Dev 1,528,631 46,490 2,000 500 Data Processing We use the Moses (Koehn et al., 2007) toolkit1 for tokenization and cleaning the English side of the data. All the Indian language data is first normalized with the Indic NLP library2 followed by tokenization with the same library. As our preprocessing step, we remove all sentences of length greater than 80 words from our training corpus and lowercase the English side of the data. In all cases, we use BPE segmentation with 16k merge operations as described in Section 2.2. 3.3 Training Details For all of our experiments, we use the OpenNMTpy (Klein et al., 2018) toolkit3 . We use the Transformer model with 6 layers in both the enc"
2020.acl-srw.22,W17-3204,0,0.0174407,"e most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. However, in order to reach high accuracies, NMT systems tend to require very large parallel training corpora (Koehn and Knowles, 2017). As a matter of fact, such corpora are not yet available for many language pairs. Indian languages are not an exception to this; however they are extremely diverse, belonging to different language families, employing various scripts and spanning a multitude of dialects. The majority of Indian languages are morphologically rich and depict unique characteristics, which are significantly different from languages such as English. Since NMT models learn poorly from small corpora, building effective NMT systems for lowresource languages (e.g. Indian languages) becomes a primary challenge. The bulk"
2020.acl-srw.22,J82-2005,0,0.502909,"Missing"
2020.acl-srw.22,D15-1166,0,0.0547823,"iently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for lowresource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines. 1 Introduction In recent years, Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) (NMT) has become the most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. Howev"
2020.acl-srw.22,I17-2050,0,0.0233438,"lingual data range from backtranslation (Sennrich et al., 2016), dual learning (He et al., 2016) to Unsupervised MT (Artetxe et al., 2017; Lample et al., 2017, 2018). Similarly, parallel data from other languages can be exploited to either pretrain the network or jointly learn the representations (Zoph et al., 2016; Firat et al., 2017; Johnson et al., 2017; Kocmi and Bojar, 2018). Currently, Transfer Learning (TL) is being widely used for low-resource language translation because it is one of the vital directions for addressing the data sparsity problem in low-resource NMT (Zoph et al., 2016; Nguyen and Chiang, 2017; Pass162 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 162–168 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ban et al., 2017; Kocmi and Bojar, 2018). However, most of the existing approaches that take advantage of transfer learning have a major limitation: they do not exploit multiple languages together and in an efficient manner. The idea presented by Zoph et al. (2016) may have the shortcoming of exploiting only one high-resource model (parent) at a time to optimize the low-resource model"
2020.acl-srw.22,P02-1040,0,0.109801,"the English side of the data. In all cases, we use BPE segmentation with 16k merge operations as described in Section 2.2. 3.3 Training Details For all of our experiments, we use the OpenNMTpy (Klein et al., 2018) toolkit3 . We use the Transformer model with 6 layers in both the encoder and decoder, each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use the Adam (Kingma and Ba, 2014) optimizer to optimize model parameters. We validate the model every 5,000 steps via BLEU (Papineni et al., 2002) and perplexity on the development set. We train all our NMT models for 150k steps except for finetuning which is done for 10k steps. After translation at the test time, we rejoin the translated BPE segments and convert the translated sentences back to their original language scripts. Finally, we evaluate the accuracy of our translation models using BLEU. 4 English and Punjabi-English language pairs for both translation directions (XX-EN and EN-XX). Table 2 shows our main results for the Indian language to English (XX-EN) translation direction. Multilingual models for XX-EN language direction"
2020.acl-srw.22,P16-1009,0,0.0562576,"and spanning a multitude of dialects. The majority of Indian languages are morphologically rich and depict unique characteristics, which are significantly different from languages such as English. Since NMT models learn poorly from small corpora, building effective NMT systems for lowresource languages (e.g. Indian languages) becomes a primary challenge. The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Some of the most well-known methods to improve NMT models with monolingual data range from backtranslation (Sennrich et al., 2016), dual learning (He et al., 2016) to Unsupervised MT (Artetxe et al., 2017; Lample et al., 2017, 2018). Similarly, parallel data from other languages can be exploited to either pretrain the network or jointly learn the representations (Zoph et al., 2016; Firat et al., 2017; Johnson et al., 2017; Kocmi and Bojar, 2018). Currently, Transfer Learning (TL) is being widely used for low-resource language translation because it is one of the vital directions for addressing the data sparsity problem in low-resource NMT (Zoph et al., 2016; Nguyen and Chiang, 2017; Pass162 Proceedings of the 58th Annual"
2020.acl-srw.22,D16-1163,0,0.392234,"ffective NMT systems for lowresource languages (e.g. Indian languages) becomes a primary challenge. The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Some of the most well-known methods to improve NMT models with monolingual data range from backtranslation (Sennrich et al., 2016), dual learning (He et al., 2016) to Unsupervised MT (Artetxe et al., 2017; Lample et al., 2017, 2018). Similarly, parallel data from other languages can be exploited to either pretrain the network or jointly learn the representations (Zoph et al., 2016; Firat et al., 2017; Johnson et al., 2017; Kocmi and Bojar, 2018). Currently, Transfer Learning (TL) is being widely used for low-resource language translation because it is one of the vital directions for addressing the data sparsity problem in low-resource NMT (Zoph et al., 2016; Nguyen and Chiang, 2017; Pass162 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 162–168 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ban et al., 2017; Kocmi and Bojar, 2018). However, most of the existing approach"
2020.acl-srw.38,jha-2010-tdil,0,0.00926591,"anguage Sentences H = ( h(n − k)...,h(n) ) at last k epochs for given sentence Output: Sentence having highest number of distinct words and lowest repetitive words for each sentence hj in H do if hj ← (w1 ,w2 ,w3 ...wl ) then D → DIST IN CT ((w1 ,w2 ,w3 ...wl )) F → F REQ(w1 )×F REQ(w2 )... scorej →D/F end if return sentence with highest scorej end for=0 For a sentence, FREQ is the count of each word; DISTINCT is the total count of unique words. For each hypothesis in the K-best list we divide DISTINCT with FREQ and select the highest scorer. 5 5.1 Experiments and Results DataSet We used ILCI Jha (2010) corpus, which has eleven language pairs from which we chose Telugu and Hindi as our parallel data during the training process. The entire corpus is manually cleaned to remove the misalignments. Table 1 shows the split ratio of sentences followed in the process. 287 Data Training Validation Test Size 45000 4000 990 Table 1: Corpus Division 5.2 System Baseline Reranking Oracle Experiments ´ We adopt the Keras implementation Alvaro Peris and Casacuberta (2018) for our experiments.We use a two-layer encoder-decoder model with 500dimensional source and target embeddings and 500 units in each of th"
2020.acl-srw.38,N16-1046,0,0.0289398,"we demonstrate all of our Experiments along with the results obtained, and finally, the paper is concluded in Section 6 with future directions. 286 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 286–291 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 2 Related Work The work of Imamura and Sumita (2017) explains the concepts of reranking and ensembling in detail. It introduces a method of bidirectional reranking in which it combines the hypothesis from l2r and r2l decoding following the works of Liu et al. (2016), which proposes an agreement model to solve unbalanced outputs of recurrent neural networks. Marie and Fujita (2018) has introduced a reranking system that uses a smorgasbord of informative features in tasks where PBSMT and NMT produce translations of different quality. The work by Shen et al. (2004) shows how to apply perceptron-like reranking algorithms to improve the overall translation quality, and Olteanu et al. (2006) shows the usage of Language Models (LMs) for reranking on hypotheses generated by phrase-based Statistical Machine Translation systems. Wang et al. (2007) has shown lingui"
2020.acl-srw.38,W18-1811,0,0.0210736,"ection 6 with future directions. 286 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 286–291 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 2 Related Work The work of Imamura and Sumita (2017) explains the concepts of reranking and ensembling in detail. It introduces a method of bidirectional reranking in which it combines the hypothesis from l2r and r2l decoding following the works of Liu et al. (2016), which proposes an agreement model to solve unbalanced outputs of recurrent neural networks. Marie and Fujita (2018) has introduced a reranking system that uses a smorgasbord of informative features in tasks where PBSMT and NMT produce translations of different quality. The work by Shen et al. (2004) shows how to apply perceptron-like reranking algorithms to improve the overall translation quality, and Olteanu et al. (2006) shows the usage of Language Models (LMs) for reranking on hypotheses generated by phrase-based Statistical Machine Translation systems. Wang et al. (2007) has shown linguistically motivated and computationally efficient structured language models for reranking in SMT systems. The concept"
2020.acl-srw.38,D16-1096,0,0.055819,"target given source sentences. It first encodes the source sentence into a single vector, and the decoder predicts it.With the Attention Mechanism, it tries to apply weights to the input sentence at each time step. Recent approaches like the transformer model Vaswani et al. (2017) have achieved the state-of-the-art results for Machine Translation. Neural Machine Translation (NMT) however, leads to over-translation and under-translation as it tends to ignore the past alignment information, and it is effectively tackled by introducing a coverage vector Tu et al. (2016). Other approaches such as Mi et al. (2016a) and Mi et al. (2016b) too solve the coverage problem in NMT. Without the coverage vector, it could result in a decrease in translation quality. We propose a method that selects a better hypothesis giving high importance to distinct words generated from decoder without the usage of any language model or data.After applying the proposed reranking method, an overall improvement in translation quality is observed as compared to the baseline system. Introduction Neural Machine Translation(NMT) has brought excellent results in the field of Machine TranslationSutskever et al. (2014); Bahdanau et a"
2020.acl-srw.38,D16-1249,0,0.0162228,"target given source sentences. It first encodes the source sentence into a single vector, and the decoder predicts it.With the Attention Mechanism, it tries to apply weights to the input sentence at each time step. Recent approaches like the transformer model Vaswani et al. (2017) have achieved the state-of-the-art results for Machine Translation. Neural Machine Translation (NMT) however, leads to over-translation and under-translation as it tends to ignore the past alignment information, and it is effectively tackled by introducing a coverage vector Tu et al. (2016). Other approaches such as Mi et al. (2016a) and Mi et al. (2016b) too solve the coverage problem in NMT. Without the coverage vector, it could result in a decrease in translation quality. We propose a method that selects a better hypothesis giving high importance to distinct words generated from decoder without the usage of any language model or data.After applying the proposed reranking method, an overall improvement in translation quality is observed as compared to the baseline system. Introduction Neural Machine Translation(NMT) has brought excellent results in the field of Machine TranslationSutskever et al. (2014); Bahdanau et a"
2020.acl-srw.38,W06-3122,0,0.15159,"Missing"
2020.acl-srw.38,P02-1040,0,0.112399,"oint averaging method for their model. Liu et al. (2018) has focused on decoding techniques that utilize existing models at parameter, word, and sentence level corresponding to checkpoint averaging, model ensembling, and candidate reranking and found that all of these improve the translation quality without retraining the model. 3 Checkpoint Based Reranking In our approach, the iteration outputs are selected as the N -best list. It implies for the last K iterations; we have the corresponding K-best list for a sentence. We take our Oracle scores as the one that is having the largest BLEU Score Papineni et al. (2002) on the test reference hypothesis from this K-best list. After obtaining the oracle scores from this K-best list, we observe that this score is larger than the baseline system, and it indicates that there is scope for further improvement of translation quality. So we propose a reranking method that improves the translation quality over the baseline system without any language model or data. We try to focus on the nature of translations that the decoder generates with and without coverage penalty. In the initial step, we keep track of the number of distinct words in the generated hypothesis, an"
2020.acl-srw.38,W17-4739,0,0.0196842,"translations of different quality. The work by Shen et al. (2004) shows how to apply perceptron-like reranking algorithms to improve the overall translation quality, and Olteanu et al. (2006) shows the usage of Language Models (LMs) for reranking on hypotheses generated by phrase-based Statistical Machine Translation systems. Wang et al. (2007) has shown linguistically motivated and computationally efficient structured language models for reranking in SMT systems. The concept of Checkpoint ensembles is introduced by Sennrich et al. (2016) and was later on improvised to independent ensembling Sennrich et al. (2017). Vaswani et al. (2017) included a checkpoint averaging method for their model. Liu et al. (2018) has focused on decoding techniques that utilize existing models at parameter, word, and sentence level corresponding to checkpoint averaging, model ensembling, and candidate reranking and found that all of these improve the translation quality without retraining the model. 3 Checkpoint Based Reranking In our approach, the iteration outputs are selected as the N -best list. It implies for the last K iterations; we have the corresponding K-best list for a sentence. We take our Oracle scores as the o"
2020.acl-srw.38,W16-2323,0,0.0242139,"es a smorgasbord of informative features in tasks where PBSMT and NMT produce translations of different quality. The work by Shen et al. (2004) shows how to apply perceptron-like reranking algorithms to improve the overall translation quality, and Olteanu et al. (2006) shows the usage of Language Models (LMs) for reranking on hypotheses generated by phrase-based Statistical Machine Translation systems. Wang et al. (2007) has shown linguistically motivated and computationally efficient structured language models for reranking in SMT systems. The concept of Checkpoint ensembles is introduced by Sennrich et al. (2016) and was later on improvised to independent ensembling Sennrich et al. (2017). Vaswani et al. (2017) included a checkpoint averaging method for their model. Liu et al. (2018) has focused on decoding techniques that utilize existing models at parameter, word, and sentence level corresponding to checkpoint averaging, model ensembling, and candidate reranking and found that all of these improve the translation quality without retraining the model. 3 Checkpoint Based Reranking In our approach, the iteration outputs are selected as the N -best list. It implies for the last K iterations; we have the"
2020.acl-srw.38,N04-1023,0,0.46921,"erated from decoder without the usage of any language model or data.After applying the proposed reranking method, an overall improvement in translation quality is observed as compared to the baseline system. Introduction Neural Machine Translation(NMT) has brought excellent results in the field of Machine TranslationSutskever et al. (2014); Bahdanau et al. (2014); Cho et al. (2014) due to generation of high-quality translations for different language pairs. Yet even higher quality can be achieved by combining multiple models by techniques like ensembles Hansen and Salamon (1990) and reranking Shen et al. (2004). Our work deals with how Neural Machine The rest of the paper is organized as follows; Section 2 discusses the work related to re-utilizing existing models for Machine Translation. Section 3 describes our approach for Checkpoint based Reranking. In Section 4, we present our Reranking Algorithm. In Section 5, we demonstrate all of our Experiments along with the results obtained, and finally, the paper is concluded in Section 6 with future directions. 286 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 286–291 c July 5 -"
2020.acl-srw.38,P16-1008,0,0.0231271,"tly trained to maximize the probability of target given source sentences. It first encodes the source sentence into a single vector, and the decoder predicts it.With the Attention Mechanism, it tries to apply weights to the input sentence at each time step. Recent approaches like the transformer model Vaswani et al. (2017) have achieved the state-of-the-art results for Machine Translation. Neural Machine Translation (NMT) however, leads to over-translation and under-translation as it tends to ignore the past alignment information, and it is effectively tackled by introducing a coverage vector Tu et al. (2016). Other approaches such as Mi et al. (2016a) and Mi et al. (2016b) too solve the coverage problem in NMT. Without the coverage vector, it could result in a decrease in translation quality. We propose a method that selects a better hypothesis giving high importance to distinct words generated from decoder without the usage of any language model or data.After applying the proposed reranking method, an overall improvement in translation quality is observed as compared to the baseline system. Introduction Neural Machine Translation(NMT) has brought excellent results in the field of Machine Transla"
2020.acl-srw.38,D18-1342,0,0.0212248,"21.96 (+0.15) 23.11 (+1.30) Table 13: Last 7 Iterations with 0.2 coverage penalty 6 Conclusions and Future Work In this paper, we introduce a method of selecting an N -best list for NMT systems and propose a way of reranking to the generated hypotheses from the system. We observe that our approach is giving better results over the baseline model by following the proposed reranking method and is also evaluated with the coverage penalty. One can investigate our approach with varying beam sizes and analyzing the effect of length 289 penalty Wu et al. (2016) and comparing it with methods such as Yang et al. (2018). We also look forward to coming up with better reranking ways that are closer to the oracle scores and investigate the efficacy of the approach in low-resourced data conditions. Language models are used for getting the likelihood of sentences and is a widely used concept for reranking hypotheses. Introducing Language Models during reranking could establish a tradeoff between perplexity and the scores to the hypotheses generated. We also plan to explore the work by C¸aglar G¨ulc¸ehre et al. (2017) and C ¸ aglar G¨ulc¸ehre et al. (2015) that introduces language models into the existing neural a"
2020.icon-adapmt.2,tiedemann-2012-parallel,0,0.106632,"Missing"
2020.icon-adapmt.2,D14-1179,0,0.0596714,"Missing"
2020.icon-adapmt.2,D17-1155,0,0.0161692,"ternational Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 6–10 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) (2018) proposed an approach on NMT decoding with terminology constraints using decoder attentions which enables reduced output duplication and better constraint placement compared to existing methods. Apart from traditional approaches there is a stack-based lattice search algorithm, constraining its search space with lattices generated by phrase-based machine translation (PBMT) improves the robustness(Khayrallah et al., 2017). Wang et al. (2017) proposed two instance weighting methods with a dynamic weight learning strategy for NMT domain adaptation. Although huge amount of research exists in this area , there exists very few works on Indian languages. As per our knowledge there is no work on technical domains like ours (Artificial Intelligence and Chemistry). Therefore there is a need to handle these technical domains and work on morphological rich and resource poor languages. 3 tional mechanism over the input sequence. In this work, following Luong et al. (2015) and Sutskever et al. (2014) we used LSTM architectures for our NMT Mod"
2020.icon-adapmt.2,D19-1078,0,0.0187583,"n adaptation setup like ours, we have a large amount of outof-domain bilingual training data for which we need to train a NMT model, we can treat this as a baseline model. Now given only an additional small amount of in-domain data, the challenge is to improve the translation perfor2 Background & Motivation Domain Adaptation has became an active research topic in NMT. Freitag and Al-Onaizan (2016) proposed two approaches, continue the training of the baseline model(general model) only on the in-domain data (domain data) and ensemble the continue model with the baseline model at decoding time. Zeng et al. (2019) proposed iterative dual domain adaptation framework for NMT, which continuously fully exploits the mutual complementarity between indomain and out-domain corpora for translation knowledge transfer. Apart from these domain adaptation techniques, there exists some approaches which has domain terminology and how to use that in NMT. Similarly Hasler et al. 6 Proceedings of the 17th International Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 6–10 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) (2018) proposed an approach on NMT decoding wit"
2020.icon-adapmt.2,N18-2081,0,0.0362476,"Missing"
2020.icon-adapmt.2,D13-1176,0,0.0537469,"rther discussed in section 3.3. Our approach follows attention-based NMT implementation similar to Bahdanau et al. (2014) and Luong et al. (2015). Our model is very much similar to the model described in Luong et al. (2015) and supports label smoothing, beam-search decoding and random sampling. The brief explanation about NMT is described in section 3.1. 3.1 Neural Machine Translation NMT system tries to find the conditional probability of target sentence with the given source sentence. In our case targets are indic languages. There are many ways to parameterize these conditional probability. Kalchbrenner and Blunsom (2013) used combination of a convolutional neural network and a recurrent neural network , Sutskever et al. (2014) used a deep Long Short-Term Memory (LSTM) model, Cho et al. (2014) used an architecture similar to the LSTM, and Bahdanau et al. (2014) used a more elaborate neural network architecture that uses an atten3.3 Technical Domain Adaptation Freitag and Al-Onaizan (2016) discussed two problems when we combine general data and domain data for training. First, training a neural machine translation system on large data sets can take several weeks and training a new model based on the combined tr"
2020.icon-adapmt.2,I17-2004,0,0.0169737,"Proceedings of the 17th International Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 6–10 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) (2018) proposed an approach on NMT decoding with terminology constraints using decoder attentions which enables reduced output duplication and better constraint placement compared to existing methods. Apart from traditional approaches there is a stack-based lattice search algorithm, constraining its search space with lattices generated by phrase-based machine translation (PBMT) improves the robustness(Khayrallah et al., 2017). Wang et al. (2017) proposed two instance weighting methods with a dynamic weight learning strategy for NMT domain adaptation. Although huge amount of research exists in this area , there exists very few works on Indian languages. As per our knowledge there is no work on technical domains like ours (Artificial Intelligence and Chemistry). Therefore there is a need to handle these technical domains and work on morphological rich and resource poor languages. 3 tional mechanism over the input sequence. In this work, following Luong et al. (2015) and Sutskever et al. (2014) we used LSTM architect"
2020.icon-adapmt.2,W17-3204,0,0.0232872,"ments using Byte Pair Encoding(BPE) as it solves rare word problems. It has been observed that with addition of little amount of in-domain data to the general data improves the BLEU score significantly. 1 Introduction Due to the fact that Neural Machine Translation (NMT) is performing better compared to the traditional statistical machine translation (SMT) models, it has become very popular in the recent years. NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). One of the challenges in NMT is domain adaptation, it becomes more challenging when it comes to low resource Indic languages and technical domains like Artificial Intelligence(AI) and Chemistry as these domains may contain many technical terms and equations etc. In a typical domain adaptation setup like ours, we have a large amount of outof-domain bilingual training data for which we need to train a NMT model, we can treat this as a baseline model. Now given only an additional small amount of in-domain data, the challenge is to improve the translation perfor2 Background & Motivation Domain A"
2020.icon-adapmt.2,D15-1166,0,0.688632,"ne translation (PBMT) improves the robustness(Khayrallah et al., 2017). Wang et al. (2017) proposed two instance weighting methods with a dynamic weight learning strategy for NMT domain adaptation. Although huge amount of research exists in this area , there exists very few works on Indian languages. As per our knowledge there is no work on technical domains like ours (Artificial Intelligence and Chemistry). Therefore there is a need to handle these technical domains and work on morphological rich and resource poor languages. 3 tional mechanism over the input sequence. In this work, following Luong et al. (2015) and Sutskever et al. (2014) we used LSTM architectures for our NMT Models, which uses a LSTM to encode the input sequence and a separate LSTM to output the translation. The encoder reads the source sentence, one word at a time, and produces a large vector that represents the entire source sentence. The decoder is initialized with this vector and generates a translation, one word at a time, until it emits the end of sentence symbol. For better translations we use bi-directional LSTM (Bahdanau et al., 2014) and attention mechanism described in Luong et al. (2015). 3.2 Byte Pair Encoding (BPE) B"
2020.icon-adapmt.2,P02-1040,0,0.109091,"Missing"
2020.icon-techdofication.7,2020.findings-emnlp.445,0,0.0716115,"Missing"
2020.icon-techdofication.7,2021.ccl-1.108,0,0.1188,"Missing"
2020.icon-techdofication.7,D19-1586,0,0.0273479,"view The section presents an overview of the system which was used to evaluate the scores described in the Results section of the paper. BERT vs RoBERTa 4.1 BERT is a bi-directional transformer for pretraining over huge amount of unlabeled textual data to learn a language representation. It can be then used to fine-tune for specific machine learning tasks like text classification. BERT outperformed the NLP state-of-the-art on several challenging tasks, attributed to the bidirectional transformer, novel pre-training tasks of Masked Language Model(Song et al., 2019) and Next Sentence Prediction(Shi and Demberg, 2019). RoBERTa has a very similar architecture as compared to BERT with improved training methodology and more data. To improve the training, RoBERTa removes the Next Sentence Prediction task from BERT’s pre-training and introduces dynamic masking so that the masked token changes during the training epochs. Originally BERT is trained for 1M steps with a batch size of 256 sequences. RoBERTa on the other hand is trained with 125 steps of 2K sequences and 31K steps with 8K sequences of batch size. Large batches are also easier to parallelize via distributed parallel training. Pre-Processing In the fir"
2020.icon-techdofication.7,P18-1206,0,0.0480258,"Missing"
2020.icon-termtraction.1,W04-3252,0,0.0402351,"Missing"
2020.icon-termtraction.1,W01-1005,0,0.260407,"Missing"
2020.icon-termtraction.1,C02-2013,0,0.118114,"y play a vital role in many Natural Language Processing Applications such as Neural Machine Translation(NMT) (Dinu et al., 2019), Information Retrieval (Chien, 1999), Information Extraction (Yangarber et al., 2000), Text Classification (Liu et al., 2005), etc. The task of automatically extracting domain specific terms from a given text of a certain academic or technical domain, is known as Automatic Technical Domain Term Extraction. This is a predominant task in NLP. Extracted terms can be useful in more complex tasks such as NMT (Dinu et al., 2019), Ontology Construction (Kietz et al., 2000; Wu and Hsu, 2002), Domain Identification, Semantic Search, Question-Answering, Word Sense Induction, etc. Several research works have been carried out to extract domain-specific terms. Most of them are either rule based (Collard et al., 2018) or dictionary based (Kim and Cavedon, 2011). Also, there 2 Background & Motivation There have been a lot of studies regarding the automatic domain term extraction. But very less work carried on unsupervised approaches that too on technical domains like, computer science, chemistry, etc. Automatic domain term extraction is a categorization or classification task where term"
2020.icon-termtraction.1,xu-etal-2002-domain,0,0.288092,"Missing"
2020.icon-termtraction.1,C00-2136,0,0.224572,"ia dipti@iiit.ac.in Introduction Domain Term, is a word or group of words, carrying a special, possibly complex, conceptual meaning, within a specific domain or subject field or community. Because of their low ambiguity and high specificity, these words are also particularly useful to conceptualize a knowledge subject. For each domain, there is an essential need to identify the domain-specific terms as they play a vital role in many Natural Language Processing Applications such as Neural Machine Translation(NMT) (Dinu et al., 2019), Information Retrieval (Chien, 1999), Information Extraction (Yangarber et al., 2000), Text Classification (Liu et al., 2005), etc. The task of automatically extracting domain specific terms from a given text of a certain academic or technical domain, is known as Automatic Technical Domain Term Extraction. This is a predominant task in NLP. Extracted terms can be useful in more complex tasks such as NMT (Dinu et al., 2019), Ontology Construction (Kietz et al., 2000; Wu and Hsu, 2002), Domain Identification, Semantic Search, Question-Answering, Word Sense Induction, etc. Several research works have been carried out to extract domain-specific terms. Most of them are either rule"
2020.icon-termtraction.1,P19-1294,0,0.0240445,"ithout much noise in domain terms. 1 Dipti Misra Sharma LTRC, IIIT-Hyderabad, India dipti@iiit.ac.in Introduction Domain Term, is a word or group of words, carrying a special, possibly complex, conceptual meaning, within a specific domain or subject field or community. Because of their low ambiguity and high specificity, these words are also particularly useful to conceptualize a knowledge subject. For each domain, there is an essential need to identify the domain-specific terms as they play a vital role in many Natural Language Processing Applications such as Neural Machine Translation(NMT) (Dinu et al., 2019), Information Retrieval (Chien, 1999), Information Extraction (Yangarber et al., 2000), Text Classification (Liu et al., 2005), etc. The task of automatically extracting domain specific terms from a given text of a certain academic or technical domain, is known as Automatic Technical Domain Term Extraction. This is a predominant task in NLP. Extracted terms can be useful in more complex tasks such as NMT (Dinu et al., 2019), Ontology Construction (Kietz et al., 2000; Wu and Hsu, 2002), Domain Identification, Semantic Search, Question-Answering, Word Sense Induction, etc. Several research works"
2020.icon-termtraction.3,P06-4018,0,0.0469927,"key areas even today. PageRank, an algorithm developed by the founders of Google, was the primary algorithm used to rank webpage searches until 2018. The fundamental idea behind any graph based ranking algorithm is make use of global knowledge for making local decisions. To determine the importance of a node in a graph, we recursively look Before passing a document through the model, it was crucial to carry out fundamental preprocessing in order to achieve a high standard of results. We first removed non-essential punctuations and tokenized the the document. In addition to the elementary NLTK(Bird, 2006)/Spacy Stop word list, we curated an additional specific list of common words that we observed added no meaning to our algorithm. POS Tagging(Brants, 2002) was a critical part of our model and was based on the powerful assumption that if a term is domain specific then it is often a Noun or a Verb, which we made after 9 Proceedings of the 17th International Conference on Natural Language Processing: TermTraction 2020 Shared Task, pages 9–12 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) analyzing the data meticulously. The addition of a POS tagger gave us a signifi"
2020.icon-termtraction.3,W04-3252,0,0.16551,"POS(Manning, 2011) tags in order to remove excessive domain-less words. The rapid growth of the internet has given us a wealth of information and data spread across the web. However, as the data begins to grow we simultaneously face the grave problem of an Information Explosion. An abundance of data can lead to large scale data management problems as well as the loss of the true meaning of the data. In this paper, we present an advanced domain specific keyword extraction algorithm in order to tackle this problem of paramount importance. Our algorithm is based on a modified version of TextRank(Mihalcea and Tarau, 2004) algorithm - an algorithm based on PageRank(Page et al., 1998) to successfully determine the keywords from a domain specific document. Furthermore, this paper proposes a modification to the traditional TextRank algorithm that takes into account bigrams and trigrams and returns results with an extremely high precision. We observe how the precision and f1-score of this model outperforms other models in many domains and the recall can be easily increased by increasing the number of results without affecting the precision. We also discuss about the future work of extending the same algorithm to In"
2020.icon-termtraction.3,C04-1162,0,0.168201,"Missing"
2020.lrec-1.456,N06-2001,0,0.0607239,"f three different attentions: i) the encoder self-attention, in which each position attends to all positions in the previous layer, including the position itself, ii) the encoder-decoder attention, in which each position of the decoder attends to all positions in the last encoder layer, and iii) the decoder self-attention, in which each position attends to all previous positions including the current position. 2.3. Adding Input Linguistic Features Our main innovation over the standard Transformer encoder-decoder architecture is that we represent its encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006). Let E ∈ Rm×K be the word embedding matrix for the standard Transformer encoder with no input features where m is the word embedding size and K is the vocabulary size of the source language. Therefore, the m-dimensional word embedding e(xi ) of the token xi (one-hot encoded representation i.e. 1-of-K vector) in the input sequence x = (x1 , x2 , ..., xn ) can be written as: e(xi ) = Exi (4) We generalize this embedding layer to some arbitrary number of features |F |: |F | e¯(xi ) = mergej=1 (Ej xij ) (5) mj ×Kj where Ej ∈ R are the feature embedding matrices with mj as the feature embedding si"
2020.lrec-1.456,Q17-1024,0,0.0248501,"tic features like POS tag, lemma and morph features to improve the translation performance. We compare the results obtained on incorporating this knowledge with the baseline systems and demonstrate significant performance improvements. Although, the Transformer NMT models have a strong efficacy to learn language constructs, we show that the usage of specific features further help in improving the translation performance. Keywords: Neural Machine Translation, Transformer, Linguistic Features 1. Introduction In recent years, Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) (NMT) has become the most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. Unlike SMT, NMT does not rely on sub-modules and e"
2020.lrec-1.456,W18-1817,0,0.0140025,"sentences of length greater than 80 from our training corpus and lowercase the English side of the data. We use BPE segmentation with 32k merge operations. We use Hindi Shallow Parser 4 to extract all the linguistic features (i.e POS tags, morph features, lemma) and annotate Hindi text with the same. We also remove all punctuations from both Hindi and English data to avoid any possible errors thrown by the shallow parser. All the linguistic features are joined with the original word or a subword using the pipe (“|”) symbol. 4.3. Training Details For all of our experiments, we use OpenNMT-py (Klein et al., 2018) toolkit. We use Transformer model with 6 layers in both encoder and decoder each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use Adam optimizer to optimize model parameters. We validate the model every 5,000 steps via BLEU (Papineni et al., 2002) and perplexity on the development set. Table 2: Size of embedding layer of linguistic features, in a system that includes all features and contrastive experiments that add a single feature over the baseline. Embedding size Featu"
2020.lrec-1.456,D07-1091,0,0.145958,"Missing"
2020.lrec-1.456,P07-2045,0,0.0247622,"used if a subword unit corresponds to the full word. To incorporate the word-level linguistic features in a subword model, we copy the word’s feature values to all of its subword units. 4. Experimental Settings 4.1. Dataset In our experiments, we use IIT-Bombay (Kunchukuttan et al., 2017) Hindi-English parallel data. The training corpus consists of data from mixed domains. There are roughly 1.5M samples in the training data from diverse sources, while the development and test sets are from news domains. 1,528,631 2,507 520 21.5M / 20.3M 62.3k / 55.8k 9.7k / 10.3k Data Processing We use Moses (Koehn et al., 2007) toolkit for tokenization and cleaning the English side of the data. Hindi side of the data is first normalized with Indic NLP library3 followed by tokenization with the same library. As our preprocessing step, we remove all the sentences of length greater than 80 from our training corpus and lowercase the English side of the data. We use BPE segmentation with 32k merge operations. We use Hindi Shallow Parser 4 to extract all the linguistic features (i.e POS tags, morph features, lemma) and annotate Hindi text with the same. We also remove all punctuations from both Hindi and English data to a"
2020.lrec-1.456,P09-5002,0,0.0105272,"n recent years, Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) (NMT) has become the most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. Unlike SMT, NMT does not rely on sub-modules and explicit linguistic features in crafting the translation . Instead, it learns the translation knowledge directly from parallel sentences without resorting to additional linguistic analysis. Although NMT is a promising approach, it still lacks the ability of modeling deeper semantic and syntactic aspects of the language. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. Addition of explicit linguistic knowledg"
2020.lrec-1.456,D15-1166,0,0.148666,"Missing"
2020.lrec-1.456,N13-1090,0,0.0118988,"ein may be unknown and the system will not be able to translate it. While this problem does not show up as strongly in English due to the very limited morphological inflection in English, it does constitute a significant problem for morphologically rich languages such as Hindi, Telugu, Tamil etc. Lemmatization can reduce data sparseness, and allow inflectional variants of the same word to explicitly share a representation in the model. In principle, neural models can learn that inflectional variants are semantically related, and represent them as similar points in the continuous vector space (Mikolov et al., 2013). However, while this has been demonstrated for high-frequency words, we expect that a lemmatized representation increases data efficiency. We verify the use of lemmas in both word based model and also in a subword model. 3.2. POS Tags Linguistic resources such as part-of-speech (POS) tags have been extensively used in statistical machine translation (SMT) frameworks and have yielded better performances. POS tags provide the linguistic knowledge and the syntactic role of each token in the context, which helps in information extraction and reducing data ambiguity. However, usage of such linguis"
2020.lrec-1.456,W17-4708,0,0.0606808,"ysis. Although NMT is a promising approach, it still lacks the ability of modeling deeper semantic and syntactic aspects of the language. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. Addition of explicit linguistic knowledge may be of great benefits to NMT models, potentially reducing language ambiguity and alleviating data sparseness further. Some recent studies have shown that incorporating linguistic features in the NMT model can improve the translation performance (Sennrich and Haddow, 2016; Niehues and Cho, 2017; Li et al., 2018). But most of the previous works have shown the effectiveness of usage of linguistic features with the RNN models. However, it is essential to verify whether the strong learning capability of the current state-of-the-art Transformer models make the explicit linguistic features redundant or if they can be easily incorporated to provide further improvements in translation performance. Also, there is an immense scope in the development of translation systems which cater to the specific characteristics of languages under consideration. Indian languages are not an exception to thi"
2020.lrec-1.456,P02-1040,0,0.10875,"to avoid any possible errors thrown by the shallow parser. All the linguistic features are joined with the original word or a subword using the pipe (“|”) symbol. 4.3. Training Details For all of our experiments, we use OpenNMT-py (Klein et al., 2018) toolkit. We use Transformer model with 6 layers in both encoder and decoder each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use Adam optimizer to optimize model parameters. We validate the model every 5,000 steps via BLEU (Papineni et al., 2002) and perplexity on the development set. Table 2: Size of embedding layer of linguistic features, in a system that includes all features and contrastive experiments that add a single feature over the baseline. Embedding size Features all single subword tags POS tags Morph Features Lemma Word or subword 6 10 20 100 * 5 10 20 150 * The embedding layer size of the all the linguistic features used varies, and is set to bring the total embedding layer size to 512 so as to ensure that the performance improvements are not simply due to an increase in the number of model parameters. All the features ha"
2020.lrec-1.456,W16-2209,0,0.198312,"additional linguistic analysis. Although NMT is a promising approach, it still lacks the ability of modeling deeper semantic and syntactic aspects of the language. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. Addition of explicit linguistic knowledge may be of great benefits to NMT models, potentially reducing language ambiguity and alleviating data sparseness further. Some recent studies have shown that incorporating linguistic features in the NMT model can improve the translation performance (Sennrich and Haddow, 2016; Niehues and Cho, 2017; Li et al., 2018). But most of the previous works have shown the effectiveness of usage of linguistic features with the RNN models. However, it is essential to verify whether the strong learning capability of the current state-of-the-art Transformer models make the explicit linguistic features redundant or if they can be easily incorporated to provide further improvements in translation performance. Also, there is an immense scope in the development of translation systems which cater to the specific characteristics of languages under consideration. Indian languages are"
2020.wildre-1.8,I08-2099,1,0.531578,"1: Telugu treebank stats The treebank is annotated using Paninian dependency grammar(Bharati et al., 1995). The paninian dependency relations are created around the notion of karakas, various participants in an action. These dependency relations are syntacto-semantic in nature. There are 40 different dependency labels specified in the panianian dependency grammar. These relations are hierarchical and certain relations can be under-specified in cases where a finer analysis is not required or when in certain cases the decision making is more difficult for the annotators(Bharati et al., 2009b). Begum et al. (2008) describe the guidelines for annotating dependency relations for Indian languages using paninian dependencies. The treebank is annotated with part-of-speech tags and morphological information like root, gender, number, person, TAM, vibhakti or case markers etc at word level. The dependency relations are annotated at chunk level. The treebank is made available in SSF format(Bharati et al., 2007). An example is shown in Figure 1. The dependency tree for the sentence is shown in Figure 2. In the example sentence, the intra-chunk dependencies, i.e dependency labels for cAlA (many) and I (this) are"
2020.wildre-1.8,J95-3006,0,0.875998,"rouped together and a lexicon consisting of words and symbols. Dependency grammars on the other hand model the syntactic relationship between the words of a sentence directly using headdependent relations. Dependency grammars are useful in modeling free word order languages. Indian languages are primarily free word order languages. There are few different dependency formalisms that have been developed for different languages. In recent years, Universal dependencies(Nivre et al., 2016) have been developed to arrive at a common dependency formalism for all languages. Paninian dependency grammar(Bharati et al., 1995) is specifically developed for Indian languages which are morphologically rich and free word order languages. Case markers and postpositions play crucial roles in these languages and word order is considered only at a surface level when required. Most Indian languages are also low resource languages. ICON-2009 and 2010 tools contests made available the initial dependency treebanks for Hindi, Telugu and Bangla. These treebanks are small in size and are annotated using the Paninian dependency grammar. Further efforts are being taken to build dependency annotated treebanks for Indian languages. H"
2020.wildre-1.8,W09-3036,1,0.755365,"Missing"
2020.wildre-1.8,W06-2920,0,0.167668,"speech tags, at morphological level with root, gender, number, person, TAM, vibhakti and case features and the dependency relations are annotated at a chunk level. The dependency relations within a chunk are left unannotated. Intra-chunk dependency annotation has been done on Hindi(Kosaraju et al., 2012) and Urdu(Bhat, 2017) treebanks previously. Annotating intra-chunk dependencies leads to a complete parse tree for every sentence in the treebank. Having completely annotated parse trees is essential for building robust end to end dependency parsers or making the treebanks available in CoNLL (Buchholz and Marsi, 2006) format and thereby making use of readily available parsers. In this paper, we extend one of those approaches for the Telugu treebank to annotate intra-chunk dependency relations. Telugu is a highly inflected morphologically rich language and has a few constructions like classifiers etc that do not occur in Hindi which makes the expansion task challenging. The fully expanded Telugu treebank is made publicly available 1 . The part-of-speech and chunk annotation of the Telugu treebank is done following the Anncorra (Bharati et al., 2009b) tagset developed for Indian languages. In the recent year"
2020.wildre-1.8,W12-3607,1,0.805763,"eebank is made publicly available. Keywords: Dependency Treebank, Intra-chunk dependencies, Low resource Language, Telugu 1. Introduction the Indian Language Treebanking project. These treebanks are annotated in Shakti Standard Format(SSF)(Bharati et al., 2007). Each sentence is annotated at word level with part of speech tags, at morphological level with root, gender, number, person, TAM, vibhakti and case features and the dependency relations are annotated at a chunk level. The dependency relations within a chunk are left unannotated. Intra-chunk dependency annotation has been done on Hindi(Kosaraju et al., 2012) and Urdu(Bhat, 2017) treebanks previously. Annotating intra-chunk dependencies leads to a complete parse tree for every sentence in the treebank. Having completely annotated parse trees is essential for building robust end to end dependency parsers or making the treebanks available in CoNLL (Buchholz and Marsi, 2006) format and thereby making use of readily available parsers. In this paper, we extend one of those approaches for the Telugu treebank to annotate intra-chunk dependency relations. Telugu is a highly inflected morphologically rich language and has a few constructions like classifie"
2020.wildre-1.8,J93-2004,0,0.0769178,"d across all Indian Languages. This tagset is commonly referred to as the BIS 2 (Bureau of Indian standards) tagset. All the latest annotation of part of speech tagging of Indian languages is done using the BIS tagset. In this paper, we convert the existing Telugu treebank from Anncorra to BIS standard. BIS tagset is a fine grained hierarchical tagset Treebanks play a crucial role in developing parsers as well as investigating other linguistic phenomena. Which is why there has been a targeted effort to create treebanks in several languages. Some such notable efforts include the Penn treebank (Marcus et al., 1993), the Prague Dependency treebank (Hajiˇcov´a, 1998). A treebank is annotated with a grammar. The grammars used for annotating treebanks can be broadly categorized into two types, Context Free Grammars and dependency grammars. A Context Free Grammar consists of a set of rules that determine how the words and symbols of a language can be grouped together and a lexicon consisting of words and symbols. Dependency grammars on the other hand model the syntactic relationship between the words of a sentence directly using headdependent relations. Dependency grammars are useful in modeling free word or"
2020.wildre-1.8,nivre-etal-2006-maltparser,0,0.136164,"ls Symbols are separated into symbols and punctuations. Question words They are separated into pronoun question words and demonstrative question words in BIS tagset. Demonstrative question words are always followed by a noun. While resolving question words (WQ), if the word 41 Figure 4: Intra-chunk dependency annotation in SSF format. intf Intensifiers (RP INTF) can modify both adjectives and adverbs. So we replace the jjmod intf with intf and use the same dependency label when an intensifier modifies an adverb or adjective. marked based on these rules. In the statistical approach Malt Parser(Nivre et al., 2006) is used to identify the intrachunk dependencies. A model is trained on a few manually annotated chunks with Malt parser and the same model is used to predict the intra-chunk dependencies for the rest of the treebank. nmod adj lwg psp lwg neg lwg vaux lwg rp lwg uh lwg cont pof redup pof cn pof cv jjmod intf rsym adjectives modifying nouns or pronouns post-positions negation verb auxiliaries particles interjection continuation reduplication compound nouns compound verbs adjectival intensifier symbols nmod wq This dependency relation is used when question words modify nouns inside a chunk. adv"
2020.wildre-1.8,L16-1262,0,0.0749111,"Missing"
2020.wildre-1.8,W04-0308,0,0.0484501,"reebank publicly available to facilitate further research. 6. In this paper, we follow the approach proposed by Bhat (2017) that makes use of a Context Free Grammar (CFG) and a shift-reduce parser for automatically annotating intrachunk dependencies. We use the treebank expander code made available by Bhat (2017) 3 and write the Context Free Grammar for Telugu. The Context Free Grammar is generated using the POS tags and creates a mapping between head and child POS tags and dependency labels. The intra-chunk annotation is done using a shift-reduce parser which internally uses the Arc-Standard(Nivre, 2004) transition system. The parser predicts a sequence of transitions starting from an initial configuration to a terminal configuration, and annotate the chunk dependencies in the process. A configuration consists of a stack, a buffer, and a set of dependency arcs. In the initial configuration, the stack is empty, buffer contains all the words in the chunk and intra-chunk dependencies are empty. In the terminal configuration, buffer is empty and stack contains only one element, the chunk head, and the chunk sub-tree is given by the set of dependency arcs. The next transition is predicted based on"
2020.wmt-1.48,P16-1154,0,0.0358797,"arathi Model BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn Feature Word level Word + Shared Vocab (SV) BPE BPE+SV+MORPH Segmentation BPE+SV+MORPH+POS BPE+SV+MORPH+POS + BT BPE (Merge ops) 20K 20K 20K 20K BLEU 21.42 23.84 24.56 25.36 25.55 23.80 Table 3: BLEU scores on Development data for Marathi-Hindi • Morph + BPE based subword segmentation, POS tags as feature • Embedding size : 500 • RNN for encoder and decoder: bi-LSTM • Bi-LSTM dimension : 500 • encoder - decoder layers : 2 • Attention : luong (general) • copy attention(Gu et al., 2016) on dynamically generated dictionary • label smoothing : 1.0 on given training data for a direction (i.e, Marathi to Hindi) to enrich training data of the opposite directional NMT training (i.e, Hindi - Marathi) by populating synthetic data. We used around 5M back translated pairs (after perplexity based pruning with respect to sentence length) for both translation directions. Using above described configuration, we performed experiments based on different parameter (feature) configurations. We trained and tested our models on word level, BPE level and morph + BPE level for input and output. W"
2020.wmt-1.48,2020.amta-research.9,0,0.422159,"tions. Using above described configuration, we performed experiments based on different parameter (feature) configurations. We trained and tested our models on word level, BPE level and morph + BPE level for input and output. We also used POS tagger and experimented with shared vocabulary across the translation task. The results are discussed in following Result section. • dropout : 0.30 6 • Optimizer : Adam • Beam size : 4 (train) and 10 (test) As these are two similar languages, share writing scripts and large sets of named entities, we used shared vocab across training. We used Opennmt-py (Klein et al., 2020) toolkit with above configuration for our experiments. 5 Back Translation Back translation is a widely used data augmentation method for low resource neural machine translation(Sennrich et al., 2016a). We utilised monolingual data (i.e of Marathi) and a NMT model trained Result Table-2 and Table-3 show performance of systems with different configuration in terms of BLEU score(Papineni et al., 2002) for Hindi-Marathi and Marathi-Hindi respectively on the validation data. We achieved 20.62 and 25.55 development and 5.94 and 18.14 test BLEU scores for Hindi-Marathi and Marathi-Hindi systems respe"
2020.wmt-1.48,P09-5002,0,0.0823202,"M 3.2M Token 7.6M 5.6M - Type 39K 66K - Table 1: Hindi-Marathi WMT2020 Training data translation. Introduction Machine Translation (MT) is the field of Natural Language Processing which aims to translate a text from one natural language (i.e Hindi) to another (i.e Marathi). The meaning of the resulting translated text must be fully preserved as the source text in the target language. For the translation task, different types of machine translation systems have been developed and they are mainly Rule based Machine Translation (RBMT)(Forcada et al., 2011), Statistical Machine Translation (SMT) (Koehn, 2009) and Neural Machine Translation (NMT) (Bahdanau et al., 2014). Statistical Machine Translation (SMT) aims to learn a statistical model to determine the correspondence between a word from the source language and a word from the target language. Neural Machine Translation is an end to end approach for automatic machine translation without heavily hand crafted feature engineering. Due to recent advances, NMT has been receiving heavy attention and achieved state of the art performance in the task of language translation. With this work, we intend to check how NMT systems could be used for low reso"
2020.wmt-1.48,D15-1166,0,0.0607068,"oder based architecture. Here, the encoder takes the input (source sentence) and encodes it into a single vector (called as a context vector). Then the decoder takes this context vector to generate an output sequence (target sentence) by generating a word at a time(Sutskever et al., 2014). Attention mechanism is an extension to this sequence to sequence architecture to avoid attempting to learn a single vector. Instead, based on learnt attention weights, it focuses more on specific words at the source end and generates a word at a time. More details can be found here (Bahdanau et al., 2014), (Luong et al., 2015). For our experiments, we utilize sequence to sequence NMT model with attention for all of our experiments with following configuration. 2 http://anoopkunchukuttan.github.io/indic nlp library/ 415 http://ltrc.iiit.ac.in/analyzer/ Model BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn BiLSTM + LuongAttn Feature Word level Word + Shared Vocab (SV)+ POS BPE BPE+SV+MORPH Segmentation BPE+SV+MORPH+POS BPE+SV+MORPH+POS + BT BPE (Merge ops) 10K 10K 10K 10K BLEU 19.70 20.49 20.1 20.44 20.62 16.49 Table 2: BLEU scores on Development data for Hindi-Marathi M"
2020.wmt-1.48,P02-1040,0,0.111022,"imizer : Adam • Beam size : 4 (train) and 10 (test) As these are two similar languages, share writing scripts and large sets of named entities, we used shared vocab across training. We used Opennmt-py (Klein et al., 2020) toolkit with above configuration for our experiments. 5 Back Translation Back translation is a widely used data augmentation method for low resource neural machine translation(Sennrich et al., 2016a). We utilised monolingual data (i.e of Marathi) and a NMT model trained Result Table-2 and Table-3 show performance of systems with different configuration in terms of BLEU score(Papineni et al., 2002) for Hindi-Marathi and Marathi-Hindi respectively on the validation data. We achieved 20.62 and 25.55 development and 5.94 and 18.14 test BLEU scores for Hindi-Marathi and Marathi-Hindi systems respectively. The results show that for low resource similar language settings, MT models based on sequence to sequence neural network can be improved with linguistic information like morph based segmentation and POS features. The results also show that morph based segmentation along with 416 byte pair encoding improves BLEU score for both directions. But Marathi-Hindi directed translation shows conside"
2020.wmt-1.48,W16-2209,0,0.041778,"the carnivorous birds swooped on the carcasses, Abram blew them away.’ (2) aur jab maansaa##haaree pakshee loth##on par jhapat##e , tab ab##raam ne unhen uda diya . ‘And when the carnivorous birds swooped on the carcasses, Abram blew them away.’ 1 3.2 aur jab maan@@ saa##haaree pakshee loth##on par jha@@ pat##e , tab ab##raam ne unhen uda diya . ‘And when the carnivorous birds swooped on the carcasses, Abram blew them away.’ Features For Hindi to Marathi translation, we carried out experiments using Part of Speech (POS) tags as a word level as well as a subword level feature as described in (Sennrich and Haddow, 2016). We use LTRC shallow parser2 toolkit to get POS tags. 4 Training Configuration Recurrent Neural Network (RNN) based machine translation models work on encoder-decoder based architecture. Here, the encoder takes the input (source sentence) and encodes it into a single vector (called as a context vector). Then the decoder takes this context vector to generate an output sequence (target sentence) by generating a word at a time(Sutskever et al., 2014). Attention mechanism is an extension to this sequence to sequence architecture to avoid attempting to learn a single vector. Instead, based on lear"
2020.wmt-1.48,P16-1009,0,0.152171,"of these two languages are the same as they are Indo-aryan languages(wikipedia, 2020). Hindi is said to have evolved from Sauraseni Prakrit (wikipedia Hindi, 2020) whereas Marathi is said to have evolved from Maharashtri Prakrit (wikipedia Marathi, 2020). They also have evolved as two major languages in different regions of India. In this work, we focused only on recurrent neural network with attention based sequence to sequence architecture throughout all experiments. Along with it, we also explored the morph(Virpioja et al., 2013) induced sub-word segmentation with byte pair encoding (BPE)(Sennrich et al., 2016b) to enable open vocabulary translation. We used POS tags as linguistic feature and back translation to leverage synthetic data for machine translation task in both directions. In the similar language translation task of WMT-2020, we participated as team named “f1plusf6”. 2 Data We utilised parallel and monolingual corpora provided for the task on Hindi&lt;->Marathi language pairs. Table-1 describes the training data (parallel 414 Proceedings of the 5th Conference on Machine Translation (WMT), pages 414–417 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and monoli"
2020.wmt-1.48,P16-1162,0,0.809624,"of these two languages are the same as they are Indo-aryan languages(wikipedia, 2020). Hindi is said to have evolved from Sauraseni Prakrit (wikipedia Hindi, 2020) whereas Marathi is said to have evolved from Maharashtri Prakrit (wikipedia Marathi, 2020). They also have evolved as two major languages in different regions of India. In this work, we focused only on recurrent neural network with attention based sequence to sequence architecture throughout all experiments. Along with it, we also explored the morph(Virpioja et al., 2013) induced sub-word segmentation with byte pair encoding (BPE)(Sennrich et al., 2016b) to enable open vocabulary translation. We used POS tags as linguistic feature and back translation to leverage synthetic data for machine translation task in both directions. In the similar language translation task of WMT-2020, we participated as team named “f1plusf6”. 2 Data We utilised parallel and monolingual corpora provided for the task on Hindi&lt;->Marathi language pairs. Table-1 describes the training data (parallel 414 Proceedings of the 5th Conference on Machine Translation (WMT), pages 414–417 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics and monoli"
2021.acl-srw.12,W17-0811,0,0.0270017,"n Figure 1. Also in computer science, string similarity is an important family of algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string. Researchers have already put the efforts and showed that these algorithms effectively calculate the similarity between two strings (Levenshtein, 1965; Yujian and Bo, 2007; Masek and Paterson, 1980; Larsen, 1992; Kondrak, 2005). Some studies have also been done on calculating similarity particularly for Indian languages (Singh and Surana, 2007; Wagner and Fischer, 1974; Islam and Inkpen, 2008; Akhtar et al., 2017; Sengupta and Saha, 2015). In this work, we will consider sentences as a string and use some of the above algorithms for calculating the similarity between two languages. 2.1 Token Overlap This is the most general approach that works by converting strings into sets of their tokens and then counting the number of tokens which are shared between the both sets. Similarity between two languages using token overlap is calculated as follows: • Collect parallel data of languages of which we want to calculate similarity. • Transliterate those languages to a common 113 |T okens1 ∩T okens2 | 1 max(|T o"
2021.acl-srw.12,N16-4006,0,0.0287455,"tacharyya (2020) also presents an impressive case study for utilizing language relatedness for Machine translation but that study was more inclined toward exploring statistical approaches to MT. Prasanna (2018) in his work has explored efficient ways of exploiting relatedness in multilingualism and transfer learning for low resource machine translation. India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation (Kunchukuttan and Bhattacharyya, 2020), sentiment analysis systems, etc. So in this paper, we performed an extensive case study on similarity involving languages of the Indian subcontinent. Language similarity prediction is defined as the task of measuring how similar the two languages are on the basis of their lexical, morphological and syntactic features. In this study, we concentrate only on"
2021.acl-srw.12,2020.sustainlp-1.4,0,0.0163521,"ate, NLP tasks become challenging for low resource languages like Indian languages. India is a multicultural country, a country with highly religious and ethnically diverse people. People of different races and classes live in different parts of the country, and they speak a variety of languages. Most of the Indian languages are divided into two main language families namely Indo-Aryan1 and Dravidian2 . Underlying the vast diversity in Indian languages are many commonalities. Because of contact over thousands of years, most of the Indian languages have undergone convergence to a large extent (Shridhar et al., 2020). Therefore, exploiting language relatedness becomes very crucial in NLP related tasks for Indian languages. Kunchukuttan and Bhattacharyya (2020) also presents an impressive case study for utilizing language relatedness for Machine translation but that study was more inclined toward exploring statistical approaches to MT. Prasanna (2018) in his work has explored efficient ways of exploiting relatedness in multilingualism and transfer learning for low resource machine translation. India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these la"
2021.acl-srw.12,D07-1003,0,0.00776808,"ogical and syntactic features. In this study, we concentrate only on the approach to calculate lexical similarity between Indian languages by looking at various factors such as size and type of corpus, similarity algorithms, subword segmentation, etc. The main takeaways from our work are: (i) Relative order of the language similarities largely remain the same, regardless of the factors mentioned above, (ii) Similarity within the same language family is higher, (iii) Languages share more lexical features at the subword level. 1 Introduction Recently, there has been an explosion in information (Wang et al., 2007) and a massive amount of natural language data is added daily on the Internet. Moreover, the human literature in different cultures is digitalized and became available in digital libraries (Farouk, 2019). A very large amount of this data is formatted in natural language. This makes NLP techniques crucial to make the use of this high amount of data. Since most of the NLP techniques either require linguistic knowledge But no such large scale study has been done on exploring different factors that may affect the process of calculating similarity among Indian languages. This could really help the"
2021.acl-srw.12,jha-2010-tdil,0,0.0339313,"am) Similarity This works by converting strings into sets of qgrams (sequences of q characters, also sometimes called k-shingles ) Kondrak (2005). The similarity or distance between the two strings is then the similarity or distance between the sets. Here we are using Jaccard index as our similarity technique which is a special case of shingle based algorithms. We can compute similarity using Jaccard between two languages as follows: Pn simqgram = 3 1 qgram(s1, s2) ∗ 100 n Experiments For our case study, we are performing all the experiments using the ILCI (Indian Language Corpora Initiative) Jha (2010) and PMI (Prime Minister of 114 Case 1: In this case, we are evaluating the effect of algorithm used for calculating sentence similarity on the similarity among the language pairs. We are computing the similarity for every language pair present in our ILCI corpora using each algorithm mentioned in subsections 2. Also, as per the requirement of our pipeline, we are also mapping each language to Devanagari script to share the same surface form. Case 2: Here, we are performing the experiments to confirm whether the choice of script selection matters in transliteration step of our pipeline for cal"
2021.disrpt-1.2,afantenos-etal-2010-learning,0,0.0611709,"Missing"
2021.disrpt-1.2,L16-1432,0,0.0202909,"ameworks encourages the design of flexible systems capable of dealing with multiple formalisms. 4 System Overview In this section, we present our approach towards the tasks of discourse unit segmentation and connective identification. We establish a baseline using a bidirectional LSTM classifier and propose a final system using the transformers architecture (Devlin et al., 2018) with a classification layer on top. 4.1 Bidirectional LSTM The organizers have provided data for 11 languages. There are 4 datasets for the English language (Prasad et al., 2008 ; Zeldes, 2017 ; Carlson et al., 2003 ; Asher et al., 2016), 2 for Spanish (Da Cunha et al., 2011 ; Cao et al., 2018), 2 for Mandarin Chinese (Zhou and Xue, 2015 ; Cao et al., 2018) and 1 each for German (Stede and Neumann, 2014), Basque (Iruskieta et al., 2013 ; Aranzabe et al., 2015), Persian (Shahmohammadi et al., 2021), French (Péry-Woodley et al., 2011), Dutch (Redeker et al., 2012), Portuguese (Cardoso et al., 2011), Russian (Toldova et al., 2017), and Turkish (Zeyrek and Kurfalı, 2017). The Persian RST corpus was added as a surprise language dataset at the release of the test data. Bidirectional LSTMs are basically an extension to the LSTM mode"
2021.disrpt-1.2,P17-2037,0,0.0277179,"Missing"
2021.disrpt-1.2,D17-1258,0,0.0137806,"(2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (2016) introduced a hybrid pipeline for discourse connective and argument identification in Hindi using sub-tree extraction and linear tagging approaches. The data for this system was obtained from the Hindi Discourse Relation Bank (HDRB) (Oza et al., 2009). The importance of dependency information was investigated by Braud et al. (2017b) by introducing 3 Datasets In this section, we describe the datasets provided by the organizers of the CODI-DISRPT2021: Discourse Relation Parsing and Treebanking Shared Task at EMNLP 20211 . The data provided consists of 16 datasets comprising of 11 languages (German, English, Basque, Persian, French, Dutch, Portuguese, Russian, Spanish, Turkish, and Mandarin Chinese). This is the first iteration of the Persian RST corpus (Shahmohammadi et al., 2021) being included for the task of discourse segmentation. The Chinese PDTB dataset (Zhou and Xue, 2015) is not available freely. Hence, the organ"
2021.disrpt-1.2,W18-4917,0,0.0296618,"Missing"
2021.disrpt-1.2,N18-2075,0,0.065494,"Missing"
2021.disrpt-1.2,C04-1048,0,0.181336,"Missing"
2021.disrpt-1.2,W11-0401,0,0.0366261,"ble systems capable of dealing with multiple formalisms. 4 System Overview In this section, we present our approach towards the tasks of discourse unit segmentation and connective identification. We establish a baseline using a bidirectional LSTM classifier and propose a final system using the transformers architecture (Devlin et al., 2018) with a classification layer on top. 4.1 Bidirectional LSTM The organizers have provided data for 11 languages. There are 4 datasets for the English language (Prasad et al., 2008 ; Zeldes, 2017 ; Carlson et al., 2003 ; Asher et al., 2016), 2 for Spanish (Da Cunha et al., 2011 ; Cao et al., 2018), 2 for Mandarin Chinese (Zhou and Xue, 2015 ; Cao et al., 2018) and 1 each for German (Stede and Neumann, 2014), Basque (Iruskieta et al., 2013 ; Aranzabe et al., 2015), Persian (Shahmohammadi et al., 2021), French (Péry-Woodley et al., 2011), Dutch (Redeker et al., 2012), Portuguese (Cardoso et al., 2011), Russian (Toldova et al., 2017), and Turkish (Zeyrek and Kurfalı, 2017). The Persian RST corpus was added as a surprise language dataset at the release of the test data. Bidirectional LSTMs are basically an extension to the LSTM model (Hochreiter and Schmidhuber, 1997) a"
2021.disrpt-1.2,D16-1035,0,0.0495395,"Missing"
2021.disrpt-1.2,2020.emnlp-main.380,0,0.0367439,"Missing"
2021.disrpt-1.2,P14-1048,0,0.0588477,"Missing"
2021.disrpt-1.2,P07-1062,0,0.0643146,"segment sentences into EDUs and integrated constraints about textual adjacency and textual organization in a beam search for text level segmentation. Soricut and Marcu (2003) used probabilistic models for EDU identification on the RSTDT corpus (Carlson et al., 2003). Building on this, Subba and Di Eugenio (2007) developed a neural network model for discourse segmentation using the same RST-DT dataset (Carlson et al., 2003). A token based classifier was introduced by Sporleder and Lapata (2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (2016) introduced a hybrid pipeline for discourse connective and argument identification in Hindi using sub-tree extraction and linear tagging approaches. The data for this system was obtained from the Hindi Discourse Relation Bank (HDRB) (Oza et al., 2009). The importance of dependency information was investigated by Braud et al. (2017b) by introducing 3 Datasets In this section, we describe the datasets provided by the organizers of the CODI-DISR"
2021.disrpt-1.2,W19-2715,0,0.033091,"e obtain the segmentation outputs by adding a linear layer that takes the outputs from the hidden states and converts them into the segmentation boundary labels. 4 https://github.com/google-research/ bert 5 https://stanfordnlp.github.io/ CoreNLP/ssplit.html This system is suited for the treebanked (.conllu files) data since the lengths of the input sequences stay below the threshold of 512 tokens. However, in the case of plain tokenized documents (.tok files), the input sequences are entire documents whose length goes beyond the 512 token limit. We work around this by adopting the approach of Muller et al. (2019) and using the StanfordNLP sentence splitter5 for splitting the input document sequences into sentences. Figure 1 illustrates the basic architecture and data flow in our proposed system. The results obtained from our system indicate that fine-tuning the multilingual BERT model by considering discourse unit segmentation as a token classification task instead of a sequence labeling task leads to better model performance. 16 Corpus deu.rst.pcc eng.rst.gum eng.rst.rstdt eng.sdrt.stac eus.rst.ert fas.rst.prstc fra.sdrt.annodis nld.rst.nldt por.rst.cstn rus.rst.rrt spa.rst.rststb spa.rst.sctb zho.rs"
2021.disrpt-1.2,W09-3029,1,0.710917,"n et al., 2003). A token based classifier was introduced by Sporleder and Lapata (2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (2016) introduced a hybrid pipeline for discourse connective and argument identification in Hindi using sub-tree extraction and linear tagging approaches. The data for this system was obtained from the Hindi Discourse Relation Bank (HDRB) (Oza et al., 2009). The importance of dependency information was investigated by Braud et al. (2017b) by introducing 3 Datasets In this section, we describe the datasets provided by the organizers of the CODI-DISRPT2021: Discourse Relation Parsing and Treebanking Shared Task at EMNLP 20211 . The data provided consists of 16 datasets comprising of 11 languages (German, English, Basque, Persian, French, Dutch, Portuguese, Russian, Spanish, Turkish, and Mandarin Chinese). This is the first iteration of the Persian RST corpus (Shahmohammadi et al., 2021) being included for the task of discourse segmentation. The Ch"
2021.disrpt-1.2,N16-2010,1,0.833549,"ut and Marcu (2003) used probabilistic models for EDU identification on the RSTDT corpus (Carlson et al., 2003). Building on this, Subba and Di Eugenio (2007) developed a neural network model for discourse segmentation using the same RST-DT dataset (Carlson et al., 2003). A token based classifier was introduced by Sporleder and Lapata (2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (2016) introduced a hybrid pipeline for discourse connective and argument identification in Hindi using sub-tree extraction and linear tagging approaches. The data for this system was obtained from the Hindi Discourse Relation Bank (HDRB) (Oza et al., 2009). The importance of dependency information was investigated by Braud et al. (2017b) by introducing 3 Datasets In this section, we describe the datasets provided by the organizers of the CODI-DISRPT2021: Discourse Relation Parsing and Treebanking Shared Task at EMNLP 20211 . The data provided consists of 16 datasets comprising of 11 languages (Germ"
2021.disrpt-1.2,P14-1002,0,0.0713209,"Missing"
2021.disrpt-1.2,prasad-etal-2008-penn,0,0.310307,"plicit (when explicitly marked in the text by a discourse connective) or implicit. Segmentation refers to the task of identifying these EDUs. There have been several approaches towards the identification of these discourse segments. Another popular framework is the Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). Both these frameworks segment the text into non-overlapping spans covering entire documents. The discourse segmentation task, in this case, corresponds to identifying the starting point of each discourse unit. In 2008, the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) was released with a corpus of over 1 million words. The unit identification task for this framework corresponds to identifying the spans of discourse connectives that explicitly identify the existence of a discourse relation. The task of EDU segmentation has been widely researched in the past due to its importance as a building block for further downstream natural language processing tasks. Most of the previous research concerning EDU segmentation has relied on the information obtained from syntactic elements of the text such as syntactic parse trees (Tofiloski et al., 2009 ; Le Thanh et al.,"
2021.disrpt-1.2,W19-2713,0,0.021912,"egmentation outputs by adding a linear layer that takes the outputs from the hidden states and converts them into the segmentation boundary labels. 4 https://github.com/google-research/ bert 5 https://stanfordnlp.github.io/ CoreNLP/ssplit.html This system is suited for the treebanked (.conllu files) data since the lengths of the input sequences stay below the threshold of 512 tokens. However, in the case of plain tokenized documents (.tok files), the input sequences are entire documents whose length goes beyond the 512 token limit. We work around this by adopting the approach of Muller et al. (2019) and using the StanfordNLP sentence splitter5 for splitting the input document sequences into sentences. Figure 1 illustrates the basic architecture and data flow in our proposed system. The results obtained from our system indicate that fine-tuning the multilingual BERT model by considering discourse unit segmentation as a token classification task instead of a sequence labeling task leads to better model performance. 16 Corpus deu.rst.pcc eng.rst.gum eng.rst.rstdt eng.sdrt.stac eus.rst.ert fas.rst.prstc fra.sdrt.annodis nld.rst.nldt por.rst.cstn rus.rst.rrt spa.rst.rststb spa.rst.sctb zho.rs"
2021.disrpt-1.2,redeker-etal-2012-multi,0,0.0521404,"Missing"
2021.disrpt-1.2,W17-0809,0,0.0268043,"nal LSTM The organizers have provided data for 11 languages. There are 4 datasets for the English language (Prasad et al., 2008 ; Zeldes, 2017 ; Carlson et al., 2003 ; Asher et al., 2016), 2 for Spanish (Da Cunha et al., 2011 ; Cao et al., 2018), 2 for Mandarin Chinese (Zhou and Xue, 2015 ; Cao et al., 2018) and 1 each for German (Stede and Neumann, 2014), Basque (Iruskieta et al., 2013 ; Aranzabe et al., 2015), Persian (Shahmohammadi et al., 2021), French (Péry-Woodley et al., 2011), Dutch (Redeker et al., 2012), Portuguese (Cardoso et al., 2011), Russian (Toldova et al., 2017), and Turkish (Zeyrek and Kurfalı, 2017). The Persian RST corpus was added as a surprise language dataset at the release of the test data. Bidirectional LSTMs are basically an extension to the LSTM model (Hochreiter and Schmidhuber, 1997) and can be thought of as joining two LSTMs together. This architecture allows the model to read input sequences in both forward and backward directions, which results in an effective capture of context, which helps in improving model performance for classification tasks. As a baseline, we propose a PyTorch3 implementation of a simple bidirectional LSTM which encodes input tokens using word embeddin"
2021.disrpt-1.2,N03-1030,0,0.22367,"et al. (2010) for French, van der Vliet (2010) for Dutch, Pardo and Nunes (2008) for Brazilian Portuguese, Da Cunha et al. (2012) for Spanish and Yang and Li (2018) for Chinese. Related Work Early work in the domain for discourse parsing involved rule-based models which used syntactic information for the prediction of discourse segments and connectives (Tofiloski et al., 2009). Le Thanh et al. (2004) used syntactic information and cue phrases to segment sentences into EDUs and integrated constraints about textual adjacency and textual organization in a beam search for text level segmentation. Soricut and Marcu (2003) used probabilistic models for EDU identification on the RSTDT corpus (Carlson et al., 2003). Building on this, Subba and Di Eugenio (2007) developed a neural network model for discourse segmentation using the same RST-DT dataset (Carlson et al., 2003). A token based classifier was introduced by Sporleder and Lapata (2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (20"
2021.disrpt-1.2,H05-1033,0,0.120126,"iscourse segments and connectives (Tofiloski et al., 2009). Le Thanh et al. (2004) used syntactic information and cue phrases to segment sentences into EDUs and integrated constraints about textual adjacency and textual organization in a beam search for text level segmentation. Soricut and Marcu (2003) used probabilistic models for EDU identification on the RSTDT corpus (Carlson et al., 2003). Building on this, Subba and Di Eugenio (2007) developed a neural network model for discourse segmentation using the same RST-DT dataset (Carlson et al., 2003). A token based classifier was introduced by Sporleder and Lapata (2005) which used POS tags, syntactic chunks, and clause information as features for the segmentation task. Fisher and Roark (2007) investigated the approach towards segmentation using sentence level syntactic parse trees on the RST-DT corpus. On a similar note, Jain and Sharma (2016) introduced a hybrid pipeline for discourse connective and argument identification in Hindi using sub-tree extraction and linear tagging approaches. The data for this system was obtained from the Hindi Discourse Relation Bank (HDRB) (Oza et al., 2009). The importance of dependency information was investigated by Braud e"
2021.disrpt-1.2,stede-neumann-2014-potsdam,0,0.0230321,"e tasks of discourse unit segmentation and connective identification. We establish a baseline using a bidirectional LSTM classifier and propose a final system using the transformers architecture (Devlin et al., 2018) with a classification layer on top. 4.1 Bidirectional LSTM The organizers have provided data for 11 languages. There are 4 datasets for the English language (Prasad et al., 2008 ; Zeldes, 2017 ; Carlson et al., 2003 ; Asher et al., 2016), 2 for Spanish (Da Cunha et al., 2011 ; Cao et al., 2018), 2 for Mandarin Chinese (Zhou and Xue, 2015 ; Cao et al., 2018) and 1 each for German (Stede and Neumann, 2014), Basque (Iruskieta et al., 2013 ; Aranzabe et al., 2015), Persian (Shahmohammadi et al., 2021), French (Péry-Woodley et al., 2011), Dutch (Redeker et al., 2012), Portuguese (Cardoso et al., 2011), Russian (Toldova et al., 2017), and Turkish (Zeyrek and Kurfalı, 2017). The Persian RST corpus was added as a surprise language dataset at the release of the test data. Bidirectional LSTMs are basically an extension to the LSTM model (Hochreiter and Schmidhuber, 1997) and can be thought of as joining two LSTMs together. This architecture allows the model to read input sequences in both forward and b"
2021.disrpt-1.2,P09-2020,0,0.25221,"rse TreeBank (PDTB) (Prasad et al., 2008) was released with a corpus of over 1 million words. The unit identification task for this framework corresponds to identifying the spans of discourse connectives that explicitly identify the existence of a discourse relation. The task of EDU segmentation has been widely researched in the past due to its importance as a building block for further downstream natural language processing tasks. Most of the previous research concerning EDU segmentation has relied on the information obtained from syntactic elements of the text such as syntactic parse trees (Tofiloski et al., 2009 ; Le Thanh et al., 2004). However, recent works have explored the task of segmentation using systems based on neural networks using the BiLSTM - CRF framework (Wang et al., 2018) or the attention mechanism (Li et al., 2016). Another important factor in all the previous works has been the presence/absence of gold sentence boundaries (Ji and Eisenstein, 2014 ; Feng and Hirst, 2014) in the data. The DISRPT 2021 Shared Task introduces the second iteration of a cross-formalism shared task on discourse unit segmentation and connective detection, and the first iteration of a cross-formalism discours"
2021.disrpt-1.2,W17-3604,0,0.0204269,"ication layer on top. 4.1 Bidirectional LSTM The organizers have provided data for 11 languages. There are 4 datasets for the English language (Prasad et al., 2008 ; Zeldes, 2017 ; Carlson et al., 2003 ; Asher et al., 2016), 2 for Spanish (Da Cunha et al., 2011 ; Cao et al., 2018), 2 for Mandarin Chinese (Zhou and Xue, 2015 ; Cao et al., 2018) and 1 each for German (Stede and Neumann, 2014), Basque (Iruskieta et al., 2013 ; Aranzabe et al., 2015), Persian (Shahmohammadi et al., 2021), French (Péry-Woodley et al., 2011), Dutch (Redeker et al., 2012), Portuguese (Cardoso et al., 2011), Russian (Toldova et al., 2017), and Turkish (Zeyrek and Kurfalı, 2017). The Persian RST corpus was added as a surprise language dataset at the release of the test data. Bidirectional LSTMs are basically an extension to the LSTM model (Hochreiter and Schmidhuber, 1997) and can be thought of as joining two LSTMs together. This architecture allows the model to read input sequences in both forward and backward directions, which results in an effective capture of context, which helps in improving model performance for classification tasks. As a baseline, we propose a PyTorch3 implementation of a simple bidirectional LSTM which"
2021.disrpt-1.2,D18-1116,0,0.0287637,"Missing"
2021.mtsummit-loresmt.16,2020.amta-research.9,0,0.0350261,"sformer for encoder and decoder, rnn size 512 feature Embedding 100 (only for POS), heads 4 encoder - decoder layers : 2, label smoothing : 1.0, dropout : 0.30, Optimizer : Adam, Beam size : 4 (train) and 10 (test), training steps : 20K • Unconstrained Morph + BPE based subword segmentation, Embedding size : 512 Transformer for encoder and decoder, RNN size 512, heads 8 encoder - decoder layers : 6, label smoothing : 1.0, dropout : 0.30, Optimizer : Adam, Beam size : 4 (train) and 10 (test), training steps : 20K For these experiments, we used shared vocab across trainings. We used Opennmt-py (Klein et al., 2020) toolkit with above configuration for our experiments. Using the above described configuration, we performed experiments based on different parameter (feature) configurations. We trained and tested our models on word level, BPE level and morph + BPE level for input and output. We also used POS tagger and experimented with shared vocabulary across the translation task. The results are discussed in following Result section. 5 Result Table-3 and Table-4 show performance of our systems with different configurations in terms of BLEU score (Papineni et al., 2002) for English-Marathi and Marathi-Engl"
2021.mtsummit-loresmt.16,P09-5002,0,0.0180028,"eld of Natural Language Processing which aims to translate a text from one natural language (i.e English) to another (i.e Marathi). The meaning of the source text must be fully preserved in the resulting translated text in the target language. Recent years have seen significant quality advancements in machine translation with the advent of Neural Machine Translation. For the translation task, different types of machine translation systems have been developed and they are mainly categorized into Rule based Machine Translation (RBMT)(Forcada et al., 2011), Statistical Machine Translation (SMT) (Koehn, 2009) and Neural Machine Translation (NMT) (Bahdanau et al., 2014). Rule based Machine Translation (RBMT) translates on the basis of grammatical rules. It involves a grammatical analysis of the source language and the target language. Based on the analysis, it generates the translated sentence (Dwivedi and Sukhadeve, 2010). Statistical Machine Translation (SMT) is based on statistical models, which analyse large parallel and monolingual text and tries to determine the correspondence between a source language word and a target language word. NMT (Bahdanau et al., 2014) is an end to end approach for"
2021.mtsummit-loresmt.16,2020.wmt-1.48,1,0.747706,"clean both English and Marathi corpora (train, test, valid and monolingual) as a first step. Following subsections explain other pre-processing steps for our MT experiments. 3.1 Morph + BPE Segmentation Based on token/type ratio, Marathi is morphologically richer compared to English from Table-1. Translating from morphologically-rich agglutinative languages is more difficult due to their complex morphology and large vocabulary. We address this issue with a segmentation method which is based on morphology and BPE segmentation (Sennrich et al., 2016b) as a pre-processing step as prescribed in (Mujadia and Sharma, 2020). We utilized unsupervised Figure 1: Morph and Subword based pre-processing for a Marathi sentence. Here ## denotes UMorph based segmentation and @@ denotes subword based segmentation Morfessor (Virpioja et al., 2013) by training it on monolingual data for Marathi. We then applied this trained Morfessor model on our corpora (train, test, validation) to get meaningful stem, morpheme, suffix segmented sub-tokens for each word in a sentence. Subsequently, we applied the subword algorithm on top of the morph segmentation as shown in Figure-1. For English, we only applied subword segmentation throu"
2021.mtsummit-loresmt.16,P02-1040,0,0.110485,"Missing"
2021.mtsummit-loresmt.16,W16-2209,0,0.0190983,"gmentation and @@ denotes subword based segmentation Morfessor (Virpioja et al., 2013) by training it on monolingual data for Marathi. We then applied this trained Morfessor model on our corpora (train, test, validation) to get meaningful stem, morpheme, suffix segmented sub-tokens for each word in a sentence. Subsequently, we applied the subword algorithm on top of the morph segmentation as shown in Figure-1. For English, we only applied subword segmentation throughout the experiments. 3.2 Features We carried out experiments using Part of Speech (POS) tag as a word and subword level feature (Sennrich and Haddow, 2016) only for English. We used Spacy (Honnibal et al., 2020) toolkit to get POS tags for English and used them by concatenating their embedding with word embedding for NMT training as shown in Figure-2. 3.3 Hindi centric parallel data For unconstrained experiments, we experimented and studied the use of available parallel data. Along with the English-Marathi parallel data, we utilized a small chunk of English-Hindi parallel data from Samanantar corpus (Ramesh et al., 2021) as Hindi is a close and related language to Marathi. We appended the English-Hindi parallel data to the existing English-Marat"
2021.mtsummit-loresmt.16,P16-1009,0,0.708251,"nguage pairs on COVID-related texts. For our work, we focused only on English-Marathi language pair (both directions) and participated for categories where in first, we only used given parallel training data (constrained) and in second, we utilized available parallel corpora from different sources for English-Marathi and English-Hindi (unconstrained). In this work, we experimented only with Transformer (Vaswani et al., 2017) based Neural Machine Translation throughout. Along with it, we also explored the morph (Virpioja et al., 2013) induced sub-word segmentation with byte pair encoding (BPE)(Sennrich et al., 2016b) to enable open vocabulary translation. We used POS tags as linguistic feature for EnglishMarathi direction along with forward and back translation to leverage synthetic data for machine translation. We also explored the use of English-Hindi parallel data for English-Marathi as origin of these two languages are the same and they are Indo-aryan languages (wikipedia, 2021). Hindi is said to have evolved from Sauraseni Prakrit (wikipedia Hindi, 2021) whereas Marathi is said to have evolved from Maharashtri Prakrit (wikipedia Marathi, 2021) and they both use the same writing script - Devanagari3"
2021.mtsummit-loresmt.16,P16-1162,0,0.75006,"nguage pairs on COVID-related texts. For our work, we focused only on English-Marathi language pair (both directions) and participated for categories where in first, we only used given parallel training data (constrained) and in second, we utilized available parallel corpora from different sources for English-Marathi and English-Hindi (unconstrained). In this work, we experimented only with Transformer (Vaswani et al., 2017) based Neural Machine Translation throughout. Along with it, we also explored the morph (Virpioja et al., 2013) induced sub-word segmentation with byte pair encoding (BPE)(Sennrich et al., 2016b) to enable open vocabulary translation. We used POS tags as linguistic feature for EnglishMarathi direction along with forward and back translation to leverage synthetic data for machine translation. We also explored the use of English-Hindi parallel data for English-Marathi as origin of these two languages are the same and they are Indo-aryan languages (wikipedia, 2021). Hindi is said to have evolved from Sauraseni Prakrit (wikipedia Hindi, 2021) whereas Marathi is said to have evolved from Maharashtri Prakrit (wikipedia Marathi, 2021) and they both use the same writing script - Devanagari3"
2021.sigmorphon-1.7,J96-4003,0,0.613836,"1) , CopyReplace (x , w , 1)) The T OKEN system does not synthesize these 67 the rule for long vowels applies, and one where the rule for words without long vowels applies. 6 we hope to apply it to learning more complex types of lingusitic rules in the future. In addition to being a way to learn rules from data, the ability to explicity control the generalization behaviour of the model allows for the use of program synthesis to understand the kinds of learning biases and operations that are required to model various linguistic processes. We leave this exploration to future work. Related work Gildea and Jurafsky (1996) also study the problem of learning phonological rules from data, and explicitly controlling generalization behaviour. We pursue a similar goal, but in a few-shot setting. Barke et al. (2019) and Ellis et al. (2015) study program synthesis applied to linguistic rule learning. They make much stronger assumptions about the data (the existence of an underlying form, and the availability of additional information like IPA features). We take a different approach, and study program synthesis models that can work only on the tokens in the word (like N O F EATURE), and also explore the effect of provi"
2021.sigmorphon-1.7,2020.sigmorphon-1.2,0,0.0154612,"2019) and Ellis et al. (2015) study program synthesis applied to linguistic rule learning. They make much stronger assumptions about the data (the existence of an underlying form, and the availability of additional information like IPA features). We take a different approach, and study program synthesis models that can work only on the tokens in the word (like N O F EATURE), and also explore the effect of providing features in these cases. We also test our approach on a more varied set of problems that involves aspects of morphology, transliteration, multilinguality, and stress. S¸ahin et al. (2020) also present a set of Linguistics Olympiad problems as a test of the metalinguistic reasoning abilities of NLP models. While problems in their set involve finding phonological rules, they also require the knowledge of syntax and semantics that are out of the scope of our study. We present a set of problems that only requires reasoning about surface word forms, and without requiring the meanings. 7 Acknowledgements We would like to thank Partho Sarthi for invaluable help with PROSE and NDSyn. We would also like to thank the authors of the ProLinguist paper for their assistance. Finally, we wou"
2021.sigmorphon-1.7,W15-3049,0,0.0176907,"Missing"
2021.sigmorphon-1.7,2020.acl-main.115,0,0.0613396,"Missing"
2021.sigmorphon-1.7,N07-1047,0,0.0659117,"ther aspects of the model remain the same across variants. Morphophonology and multilingual problems: For every pair of columns (s, t) in the problem matrix M , we synthesize the program M:s → M:t . To predict the form of a test sample Mij , we find a column k such that the program M:k → M:j has the best ranking score, and evaluate it on Mik . Transliteration problems: Given a problem matrix M , we construct a new matrix M 0 for each pair of columns (s, t) such that all entries in M 0 are in the same script. We align word pairs (Mis , Mit ) using the Phonetisaurus many-to-many alignment tool (Jiampojamarn et al., 2007), and build a simple mapping f for each source token to the target token with which it is most frequently aligned. We 0 by applying f to each token of M and fill in Mis is Dataset statistics The dataset we present is highly multilingual. The 34 problems contain samples from 38 languages, drawn from across 19 language families. There are 15 morphophonology problems, 7 multilingual problems, 6 stress, and 6 transliteration problems. The set contains 1452 training words with an average of 43 words per problem, and 319 test words with an average of 9 per problem. Each problem has a matrix that has"
2021.sigmorphon-1.7,P84-1070,0,0.728015,"ng programming language that satisfy (user) intent expressed in some form of constraints” (Gulwani et al., 2017). This method allows us to specify domain-specific assumptions as a language, and use generic synthesis approches like FlashMeta (Polozov and Gulwani, 2015) to synthesize programs. The ability to explicitly encode domain-specific assumptions gives program synthesis broad applicability to various tasks. In this paper, we explore applying it to the task of learning phonological rules. Whereas previous work on rule-learning has focused on learning rules of a specific type (Brill, 1992; Johnson, 1984), the DSL in program synthesis allows learning rules of different types, and in different rule formalisms. In this work, we explore learning rules similar to rewrite rules (Chomsky and Halle, 1968) that are used extensively to describe phonology. Sequences of rules are learnt using a noisy disjunctive synthesis algorithm NDSyn (Iyer et al., 2019) extended to learn stateful multi-pass rules (Sarthi et al., 2021). 2.1 2.2 Domain-specific language The domain-specific language (DSL) is the declarative language which defines the allowable string transformation operations. The DSL is defined by a se"
2021.sigmorphon-1.7,2020.lrec-1.521,0,0.0527236,"Missing"
2021.wat-1.25,N16-4006,0,0.0222719,"ws. Section 2 describes the methodology behind our experiments. Section 3 talks about the experimental details like dataset pre-processing and training details. Results and analysis have been discussed in Section 4, followed by conclusion in Section 5. 2 Methodology 2.1 Exploiting Language Relatedness India is one of the most linguistically diverse countries of the world but underlying this vast diversity in Indian languages are many commonalities. These languages exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). These languages share many common cognates and therefore, it is very important to utilize the lexical similarity of these languages to build good quality multilingual NMT systems. To do this, we are using the two different approaches namely Unified Transliteration and Sub-word Segmentation proposed by (Goyal et al., 2020). 2.1.1 Unified Transliteration The major Indian languages have a long written tradition and use a variety of scripts but correspondences can be established between equivalent characters across scripts. These scripts are derived from the ancient Brahmi script. In order to ac"
2021.wat-1.25,2020.acl-srw.22,1,0.688325,"istically diverse countries of the world but underlying this vast diversity in Indian languages are many commonalities. These languages exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). These languages share many common cognates and therefore, it is very important to utilize the lexical similarity of these languages to build good quality multilingual NMT systems. To do this, we are using the two different approaches namely Unified Transliteration and Sub-word Segmentation proposed by (Goyal et al., 2020). 2.1.1 Unified Transliteration The major Indian languages have a long written tradition and use a variety of scripts but correspondences can be established between equivalent characters across scripts. These scripts are derived from the ancient Brahmi script. In order to achieve this, we transliterated all the Indian languages into a common Devanagari script (which in our case is the script for Hindi) to share the same surface form. This unified transliteration is a string homomorphism, replacing characters in all the languages to a single desired script. 2.1.2 Subword Segmentation Despite sh"
2021.wat-1.25,P17-4012,0,0.0140409,"urce language pair is significantly improved. Furthermore, We are also fine tuning our multilingual system on PMI (multilingual) domain by the means of transfer learning b/w the parent and the child model. 3 3.1 Experimental Details Dataset and Preprocessing We are using the dataset provided in WAT 2021 shared task. Our experiments mainly use PMI (Haddow and Kirefu, 2020), CVIT (Siripragada et al., 2020) and IIT-B (Kunchukuttan et al., 2017) parallel dataset, along with monolingual data of PMI for further improvements Table 2. We used Training For all of our experiments, we use the OpenNMTpy (Klein et al., 2017) toolkit for training the NMT systems. We used the Transformer model with 6 layers in both the encoder and decoder, each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use Adam (Kingma and Ba, 2014) optimizer to optimize model parameters. We validate the model every 5,000 steps via BLEU (Papineni et al., 2002) and perplexity on the development set. We are training all of our models with early stopping criteria based on validation set accuracy. During testing, we rejoin trans"
2021.wat-1.25,W18-6325,0,0.0142947,"kever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) is the current state-of-the-art approach for Machine Translation in both academia and industry. The success of NMT heavily relies on substantial amounts of parallel sentences as training data (Koehn and Knowles, 2017) which is again an arduous task for low resource languages like Indian languages (Philip et al., 2021). Many techniques have been devised to improve the translation quality of low resource languages like back translation (Sennrich et al., 2015), dual learning (Xia et al., 2016), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), etc. Also, using the traditional approaches, one would still need to train a separate model for each translation direction. So, building multilingual neural machine translation models by means of sharing parameters with high-resource languages is a common practice to improve the performance of low-resource language pairs (Firat et al., 2017; Johnson et al., 2017; Ha et al., 2016). Low resource language pairs perform better when combined opposed to the case where the models are trained separately due to sharing of parameters. It also enables training a single model that supports translation f"
2021.wat-1.25,P07-2045,0,0.0167101,"rpus Gu Mr Bn 123008 118848 116835 En-kn En-ml En-ta En-te 28901 - 26916 43087 - 32638 115968 - 33380 44720 - Or 103331 Kn 79024 Ml 81786 Ta 90912 Te 111325 Table 2: Training dataset statistics translation is particularly useful for low resource languages. We use back translation to augment our multilingual models. The back translation data is generated by multilingual models in the reverse direction, hence some implicit multilingual transfer is incorporated in the back translated data also. For the scope of this paper, we have used monolingual data of the PMI given on the WAT website. Moses (Koehn et al., 2007) toolkit for tokenization and cleaning of English and Indic NLP library (Kunchukuttan, 2020) for normalizing, tokenization and transliteration of all Indian languages. For our bilingual model we used BPE segmentation with 16K merge operation and for multilingual models we learned the Joint-BPE on source and target side with 16K merges (Sennrich et al., 2015). 2.4 3.2 Multilingual NMT and Fine-tuning Multilingual model enables us to translate to and from multiple languages using a shared word piece vocabulary, which is significantly simpler than training a different model for each language pair"
2021.wat-1.25,W17-3204,0,0.0179746,"he baselines by an average of 11.3 and 19.6 BLEU points for EnglishIndic (en-xx) and Indic-English (xx-en) directions, respectively. 1 Introduction Good translation systems are an important requirement due to substantial government, business and social communication among people speaking different languages. Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) is the current state-of-the-art approach for Machine Translation in both academia and industry. The success of NMT heavily relies on substantial amounts of parallel sentences as training data (Koehn and Knowles, 2017) which is again an arduous task for low resource languages like Indian languages (Philip et al., 2021). Many techniques have been devised to improve the translation quality of low resource languages like back translation (Sennrich et al., 2015), dual learning (Xia et al., 2016), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), etc. Also, using the traditional approaches, one would still need to train a separate model for each translation direction. So, building multilingual neural machine translation models by means of sharing parameters with high-resource languages is a common pr"
2021.wat-1.25,P02-1040,0,0.110394,"y find the common sub-words between multiple languages but it also ensures consistency of segmentation among each considered language pair. 2.2 Data Selection Strategy Since the traditional approaches of training a multilingual system simply work by combining all the parallel dataset in hand, making it infeasible in terms of both time as well as computational resources. Therefore, in order to select only the relevant domains, we are incrementally adding all the domains in decreasing order of their vocab overlap with the PMI domain (Haddow and Kirefu, 2020). Detection of dip in the BLEU score (Papineni et al., 2002) is considered as the stopping criteria for our strategy. The vocab overlap between any two domains is calculated using the formula shown below: Vocab Overlap = |V ocabd1 ∩ V ocabd2 | ∗100 max(|V ocabd1 |, |V ocabd2 |) Here, Vocabd1 & Vocabd2 represents vocabulary of domain 1 and domain 2 respectively. Vocab overlap of each domain with PMI is shown in Table 1. 2.3 Back Translation Back translation (Sennrich et al., 2015)is a widely used data augmentation method where the reverse direction is used to translate sentences from target side monolingual data into the source language. This synthetic"
2021.wat-1.25,P16-1009,0,0.0940631,"Missing"
2021.wat-1.25,2020.lrec-1.462,0,0.0259666,"ross language boundaries during training. It is also observed that when language pairs with little available data and language pairs with abundant data are mixed into a single model, translation quality on the low resource language pair is significantly improved. Furthermore, We are also fine tuning our multilingual system on PMI (multilingual) domain by the means of transfer learning b/w the parent and the child model. 3 3.1 Experimental Details Dataset and Preprocessing We are using the dataset provided in WAT 2021 shared task. Our experiments mainly use PMI (Haddow and Kirefu, 2020), CVIT (Siripragada et al., 2020) and IIT-B (Kunchukuttan et al., 2017) parallel dataset, along with monolingual data of PMI for further improvements Table 2. We used Training For all of our experiments, we use the OpenNMTpy (Klein et al., 2017) toolkit for training the NMT systems. We used the Transformer model with 6 layers in both the encoder and decoder, each with 512 hidden units. The word embedding size is set to 512 with 8 heads. The training is done in batches of maximum 4096 tokens at a time with dropout set to 0.3. We use Adam (Kingma and Ba, 2014) optimizer to optimize model parameters. We validate the model every"
2021.wat-1.25,D16-1163,0,0.0206748,"e translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017) is the current state-of-the-art approach for Machine Translation in both academia and industry. The success of NMT heavily relies on substantial amounts of parallel sentences as training data (Koehn and Knowles, 2017) which is again an arduous task for low resource languages like Indian languages (Philip et al., 2021). Many techniques have been devised to improve the translation quality of low resource languages like back translation (Sennrich et al., 2015), dual learning (Xia et al., 2016), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), etc. Also, using the traditional approaches, one would still need to train a separate model for each translation direction. So, building multilingual neural machine translation models by means of sharing parameters with high-resource languages is a common practice to improve the performance of low-resource language pairs (Firat et al., 2017; Johnson et al., 2017; Ha et al., 2016). Low resource language pairs perform better when combined opposed to the case where the models are trained separately due to sharing of parameters. It also enables training a single model tha"
ambati-etal-2010-high,J93-2004,0,\N,Missing
ambati-etal-2010-high,W09-3036,1,\N,Missing
ambati-etal-2010-high,P05-1040,0,\N,Missing
ambati-etal-2010-high,W00-1907,0,\N,Missing
ambati-etal-2010-high,W06-2932,0,\N,Missing
ambati-etal-2010-high,J05-1004,0,\N,Missing
ambati-etal-2010-high,I08-2099,1,\N,Missing
begum-etal-2008-developing,kingsbury-palmer-2002-treebank,0,\N,Missing
begum-etal-2008-developing,P98-1013,0,\N,Missing
begum-etal-2008-developing,C98-1013,0,\N,Missing
begum-etal-2008-developing,I08-2099,1,\N,Missing
bhat-etal-2014-towards,N10-1077,0,\N,Missing
bhat-etal-2014-towards,E12-2012,0,\N,Missing
bhat-etal-2014-towards,W12-5605,1,\N,Missing
bhat-etal-2014-towards,I08-2099,1,\N,Missing
bhatia-etal-2010-empty,J05-1004,1,\N,Missing
bhatia-etal-2010-empty,I08-2099,1,\N,Missing
C16-1039,W11-3406,0,0.0214977,"and interoperability of their individual resources. Most of the works suggest that Hindi and Urdu resources can be used interchangeably with some modifications (Sinha, 2009; Visweswariah et al., 2010). Sinha (2009) show that an English-Urdu machine translation system can be easily build by using Hindi as a bridge language. Urdu translations of English sentences are derived from the output of an existing English-Hindi MT system. They use lexical mappings between Hindi and Urdu words, lexical and syntactic disambiguation rules, and a transliteration module for converting the MT output to Urdu. Adeeba and Hussain (2011) used a transliteration-based approach to create an Urdu WordNet from an existing Hindi WordNet (Narayan et al., 2002). They used a rule-based transliteration system to convert 405 the lexical database of the Hindi WordNet to Urdu (Perso-Arabic). They manually pruned typical Sanskrit words that are not used in Urdu texts and added additional entries specific to Urdu. Similarly Ahmed and Hautli (2010) proposed to use a simple transliteration-based approach to access Hindi WordNet for Urdu texts, instead of creating a separate Urdu WordNet. Mukund et al. (2010) have explored the use of Hindi spe"
C16-1039,J95-3006,0,0.874739,"s (henceforth HDTB and UDTB) for conducting all the experiments. Both treebanks are multi-layered and multi-representational (Bhatt et al., 2009). They contain three layers of annotation namely dependency structure (DS) for annotation of modifiedmodifier relations, PropBank-style annotation for predicate-argument structure, and an independently motivated phrase-structure annotation. For our experiments, we only need annotations in the first layer of the treebanks i.e., annotations in the DS layer. Dependency Structure involves dependency analysis based on the P¯an.inian Grammatical framework (Bharati et al., 1995). In addition to dependency analysis, the sentence annotation also includes morphological analysis, part-of-speech (POS) tagging and chunking. POS tagging and chunking are based on Indian Language Machine Translation (ILMT) guidelines (Bharati et al., 2006). There are around 32 POS tags and 11 chunk tags used in the treebanks, while the dependency labels are around 82. We split the treebank data with a ratio of 80:10:10 for training, testing and tuning separate models for POS tagging, chunking and parsing. For both treebanks, the internal structure of annotation files is preserved. However, we"
C16-1039,W09-3036,1,0.890666,"List_of_ languages_by_number_of_native_speakers 397 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 397–408, Osaka, Japan, December 11-17 2016. socially and even officially considered two separate languages. These apparent divergences have also led to parallel efforts for resource creation and application building in computational linguistics. The Hindi-Urdu treebanking project is one such example where the influence of differences between Hindi and Urdu texts have led to the creation of separate treebanks for Hindi and Urdu (Bhatt et al., 2009; Bhat et al., 2015). However, pursuing them separately in computational linguistics makes sense. If the two texts differ in form and vocabulary they can not be processed with same models unless the differences are accounted for and addressed. In this paper, we aim to remove these differences between Hindi and Urdu texts. We learn accurate machine transliteration models for the common orthographic representation of their texts. To resolve their lexical divergences, we learn cross-register word embeddings from the harmonized Hindi and Urdu corpora. So as to evaluate our approach, we empirically"
C16-1039,W11-2905,0,0.0192668,"n though, pratiksh¯a and intiz¯ar are different word forms, they have identical syntactic distributions which could be used as an approximation of their semantic similarity. To capture the similarity between Sanskrit and Perso-Arabic words in Hindi and Urdu vocabularies, we could apply distributional similarity methods on the union of harmonized (represented in same script) Hindi and Urdu corpora. Augmenting source domain corpus with the target domain data to learn distributional representation of words is a common practice to address lexical sparseness encountered in domain adaptation tasks (Candito et al., 2011; Plank and Moschitti, 2013). We could also use bilingual word clustering or word embedding approaches which have been used to address the loss of lexical information in delexicalized parsing (T¨ackstr¨om et al., 2012; Xiao and Guo, 2014). In case of Hindi and Urdu, the former approach is more simple and direct way to capture the distributional similarity. The similar grammar and partially shared vocabularies would ensure semantically similar words of Hindi and Urdu are assigned similar distributional representations. The cross-lingual approaches, on the other hand, are computationally complex"
C16-1039,D14-1082,0,0.283875,"ults. 2 Experimental Setup To experiment on resource sharing and augmentation between Hindi and Urdu, we need to mitigate their orthographic and lexical differences. To that end, we propose a simple approach which uses machine transliteration and distributional similarity-based methods (see §3 and §4). After script harmonization, we learn distributed word representations to project Hindi and Urdu lexicon in the same distributional space. To make effective use of these word representations, we employ the non-linear neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). We use a similar architecture for sequence labeling as well. 2.1 Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system, which is one of the famous transition-based parsing systems. Arc-eager algorithm defines four types of transitions to derive a parse tree namely: 1) Shift, 2) Left-Arc, 3) Right-Arc, and 4) Reduce. To predict these transitions, a classifier is employed. We follow Chen and Manning (2014) and use a non-linear neural network to predict these transitions for any parser configu"
C16-1039,W02-1001,0,0.00981644,"ables coupled with a set of heuristics to resolve ambiguous mappings. Statistical approaches have hardly been explored (Sajjad et al., 2011). Unlike rule-based systems, statistical approaches are more robust and efficient. Among data-driven approaches, machine learning methods like noisy-channel model and structured prediction algorithms have been widely used for machine transliteration (Knight and Graehl, 1998; Zelenko and Aone, 2006). In this work, we model Hindi-Urdu transliteration as a structured prediction problem using a linear model. We use the structured perceptron (DHMM) of Collins (Collins, 2002) to learn the parameters of our transliteration model. Given an input training data of aligned character sequences D = d1 ...dn , a vector feature function ~f (d), and an initial weight vector ~ w, the algorithm performs two steps for each training example di ∈ D: (1) Decode: tˆ = arg max(~w · ~f (d)), and (2) Update: ~w = ~w + ~f (d) − ~f (tˆ). We use Viterbi-search for decoding in case t1 ···tn of Devanagari to Perso-Arabic transliteration, while we use beam-search for Perso-Arabic to Devanagari transliteration to decode the best letter sequence in the target script. The latter is used to ex"
C16-1039,P07-1033,0,0.0245602,"Missing"
C16-1039,C12-1059,0,0.0128422,"e distributed representation of its lexical form, POS tag, chunk tag and/or dependency label. We use the Hindi and Urdu monolingual corpora to learn the distributed representation of the lexical units. The Hindi monolingual data contains around 40M raw sentences, while the Urdu data is comparatively smaller and contains around 6M raw sentences. The distributed representation of non-lexical units such as POS, chunk and dependency labels are randomly initialized within a range of -0.01 to +0.01 (Chen and Manning, 2014). To decode a transition sequence, we use dynamic oracle recently proposed by Goldberg and Nivre (2012) instead of the vanilla static oracle. Dynamic oracle allows training by exploration that helps to mitigate the effect of error propagation. We use the same value for the exploration hyperparameter as suggested by Goldberg and Nivre (2012). 2.2 Sequence Labeling Models For the training of POS tagging and chunking models, we use a similar neural network architecture as discussed above. Unlike Collobert et al. (2011), we do not learn separate transition parameters. Instead we include the structural features in the input layer of our model with other lexical and non-lexical units. For POS tagging"
C16-1039,C12-2063,0,0.0173524,"these scripts (see §3.1.1 for more details). To measure the suitability of both scripts for the common representation of Hindi and Urdu texts, we perform extrinsic evaluation on the dependency parsing pipeline which involves POS tagging, chunking and dependency parsing. The script that maximizes the accuracy across the pipeline would imply its feasibility for uniformly representing the Hindi and Urdu texts for computational purposes. 3.1 Hindi-Urdu Transliteration Hindi and Urdu transliteration has received a lot of attention from the NLP research community of South Asia (Malik et al., 2008; Lehal and Saini, 2012; Lehal and Saini, 2014). It has been seen to break the barrier that makes the two look different. Most of the existing works on Hindi-Urdu transliteration have considered basic rule-based models which use character tables coupled with a set of heuristics to resolve ambiguous mappings. Statistical approaches have hardly been explored (Sajjad et al., 2011). Unlike rule-based systems, statistical approaches are more robust and efficient. Among data-driven approaches, machine learning methods like noisy-channel model and structured prediction algorithms have been widely used for machine translite"
C16-1039,W14-5135,0,0.30261,"ts. To address this problem, we need to represent both Hindi and Urdu texts in a single script. For this purpose, we can either use Devanagari or Perso-Arabic script. We can transliterate Hindi texts in Devanagari to Perso-Arabic or Urdu texts in Perso-Arabic to Devanagari. Either way, transliteration between these two scripts is a non-trivial task. There are genuine cases of character ambiguity due to one-to-many character mappings in both directions of transliteration. A detailed description of the challenges in Hindi-Urdu transliteration can be found in the works of Malik et al. (2008) and Lehal and Saini (2014). In addition to character ambiguity, Perso-Arabic to Devanagari 399 transliteration has to deal with missing short vowels in Urdu texts also. In Urdu writing, short vowels are hardly represented, even though the Perso-Arabic script has the provision for their representation. A major drawback of dropping short vowels in Urdu writing is that it generates homographs. For example, without an appropriate short vowel on the first letter, @ñïf could mean ‘air’ ( @ñï f) or ‘become’ ( @ñï f) depending on the context. These homographs would lead to ambiguity in the Devanagari script. There would be mor"
C16-1039,C08-1068,0,0.121842,"n in two different scripts. To address this problem, we need to represent both Hindi and Urdu texts in a single script. For this purpose, we can either use Devanagari or Perso-Arabic script. We can transliterate Hindi texts in Devanagari to Perso-Arabic or Urdu texts in Perso-Arabic to Devanagari. Either way, transliteration between these two scripts is a non-trivial task. There are genuine cases of character ambiguity due to one-to-many character mappings in both directions of transliteration. A detailed description of the challenges in Hindi-Urdu transliteration can be found in the works of Malik et al. (2008) and Lehal and Saini (2014). In addition to character ambiguity, Perso-Arabic to Devanagari 399 transliteration has to deal with missing short vowels in Urdu texts also. In Urdu writing, short vowels are hardly represented, even though the Perso-Arabic script has the provision for their representation. A major drawback of dropping short vowels in Urdu writing is that it generates homographs. For example, without an appropriate short vowel on the first letter, @ñïf could mean ‘air’ ( @ñï f) or ‘become’ ( @ñï f) depending on the context. These homographs would lead to ambiguity in the Devanagari"
C16-1039,J08-4003,0,0.0311927,"propose a simple approach which uses machine transliteration and distributional similarity-based methods (see §3 and §4). After script harmonization, we learn distributed word representations to project Hindi and Urdu lexicon in the same distributional space. To make effective use of these word representations, we employ the non-linear neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). We use a similar architecture for sequence labeling as well. 2.1 Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system, which is one of the famous transition-based parsing systems. Arc-eager algorithm defines four types of transitions to derive a parse tree namely: 1) Shift, 2) Left-Arc, 3) Right-Arc, and 4) Reduce. To predict these transitions, a classifier is employed. We follow Chen and Manning (2014) and use a non-linear neural network to predict these transitions for any parser configuration. The neural network model is the standard feed-forward neural network with a single layer of hidden units. The output layer uses softmax function for probabilistic"
C16-1039,J03-1002,0,0.00557573,"be noted that it is plausible to score Urdu sentences in Devanagari using a language model trained on Hindi data, since there is a considerable overlap in the Hindi and Urdu grammar and vocabulary. 3.1.1 Transliteration Pair Extraction and Character Alignment Like any other supervised machine learning approach, supervised machine transliteration requires a strong list of transliteration pairs to learn the model parameters. We use the sentence aligned ILCI Hindi-Urdu parallel corpora (Jha, 2010) to extract the transliteration pairs. Initially, the parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003). We extract all the word pairs which occur as 1-to-1 alignments in the word-aligned corpus as potential transliteration equivalents. We extracted a total of 54,035 translation pairs from the parallel corpus of 50,000 sentences. To further complement the translation pairs, we also extracted 66,668 pairs from IndoWordNet (Narayan et al., 2002) synset mappings.2 A rule-based approach with edit distance metric is used to extract the transliteration pairs from these translation pairs. To compute the edit distances, we use the Hindi-Urdu character mappings presented in (Lehal and Saini, 2014). We c"
C16-1039,P13-1147,0,0.0167465,"nd intiz¯ar are different word forms, they have identical syntactic distributions which could be used as an approximation of their semantic similarity. To capture the similarity between Sanskrit and Perso-Arabic words in Hindi and Urdu vocabularies, we could apply distributional similarity methods on the union of harmonized (represented in same script) Hindi and Urdu corpora. Augmenting source domain corpus with the target domain data to learn distributional representation of words is a common practice to address lexical sparseness encountered in domain adaptation tasks (Candito et al., 2011; Plank and Moschitti, 2013). We could also use bilingual word clustering or word embedding approaches which have been used to address the loss of lexical information in delexicalized parsing (T¨ackstr¨om et al., 2012; Xiao and Guo, 2014). In case of Hindi and Urdu, the former approach is more simple and direct way to capture the distributional similarity. The similar grammar and partially shared vocabularies would ensure semantically similar words of Hindi and Urdu are assigned similar distributional representations. The cross-lingual approaches, on the other hand, are computationally complex, while capture the distribu"
C16-1039,W12-5001,0,0.0578802,"Missing"
C16-1039,I11-1015,0,0.0153964,"for uniformly representing the Hindi and Urdu texts for computational purposes. 3.1 Hindi-Urdu Transliteration Hindi and Urdu transliteration has received a lot of attention from the NLP research community of South Asia (Malik et al., 2008; Lehal and Saini, 2012; Lehal and Saini, 2014). It has been seen to break the barrier that makes the two look different. Most of the existing works on Hindi-Urdu transliteration have considered basic rule-based models which use character tables coupled with a set of heuristics to resolve ambiguous mappings. Statistical approaches have hardly been explored (Sajjad et al., 2011). Unlike rule-based systems, statistical approaches are more robust and efficient. Among data-driven approaches, machine learning methods like noisy-channel model and structured prediction algorithms have been widely used for machine transliteration (Knight and Graehl, 1998; Zelenko and Aone, 2006). In this work, we model Hindi-Urdu transliteration as a structured prediction problem using a linear model. We use the structured perceptron (DHMM) of Collins (Collins, 2002) to learn the parameters of our transliteration model. Given an input training data of aligned character sequences D = d1 ...d"
C16-1039,2009.mtsummit-caasl.12,0,0.0126444,"x-office Gadget Recipe Urdu UAS 87.90 86.64 83.27 81.12 86.13 Parsing LS 85.26 83.43 81.30 79.31 83.15 LAS 79.72 78.98 75.35 71.37 78.93 POS tagging 94.02 89.55 85.93 88.95 89.34 Table 8: Comparison of parsing and tagging accuracy of Hindi parser and POS tagger on Urdu test data and four different domains of Hindi. 5 Related Work Recently, there have been several attempts at leveraging the similarity between Hindi and Urdu for sharing and interoperability of their individual resources. Most of the works suggest that Hindi and Urdu resources can be used interchangeably with some modifications (Sinha, 2009; Visweswariah et al., 2010). Sinha (2009) show that an English-Urdu machine translation system can be easily build by using Hindi as a bridge language. Urdu translations of English sentences are derived from the output of an existing English-Hindi MT system. They use lexical mappings between Hindi and Urdu words, lexical and syntactic disambiguation rules, and a transliteration module for converting the MT output to Urdu. Adeeba and Hussain (2011) used a transliteration-based approach to create an Urdu WordNet from an existing Hindi WordNet (Narayan et al., 2002). They used a rule-based trans"
C16-1039,N12-1052,0,0.0296583,"Missing"
C16-1039,P10-1040,0,0.0236701,"2012; Xiao and Guo, 2014). In case of Hindi and Urdu, the former approach is more simple and direct way to capture the distributional similarity. The similar grammar and partially shared vocabularies would ensure semantically similar words of Hindi and Urdu are assigned similar distributional representations. The cross-lingual approaches, on the other hand, are computationally complex, while capture the distributional similarity indirectly using a seed bilingual 403 lexicon. The distributional similarity can be incorporated in a statistical model either by using word clusters or word vectors (Turian et al., 2010). Similar to Collobert et al. (2011) and Chen and Manning (2014), we represent lexical units in the input layer of our neural network model by the word embeddings instead of one-hot vectors. We augment Hindi monolingual data with the transliterated Urdu data and use word2vec toolkit6 to learn the word embeddings. The toolkit provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram (SG) approaches of Mikolov et al. (2013) to compute distributed representation of words. To learn distributed word representations, we considered context windows of 2 to 5 words to eit"
C16-1039,C10-2147,0,0.0242703,"et Recipe Urdu UAS 87.90 86.64 83.27 81.12 86.13 Parsing LS 85.26 83.43 81.30 79.31 83.15 LAS 79.72 78.98 75.35 71.37 78.93 POS tagging 94.02 89.55 85.93 88.95 89.34 Table 8: Comparison of parsing and tagging accuracy of Hindi parser and POS tagger on Urdu test data and four different domains of Hindi. 5 Related Work Recently, there have been several attempts at leveraging the similarity between Hindi and Urdu for sharing and interoperability of their individual resources. Most of the works suggest that Hindi and Urdu resources can be used interchangeably with some modifications (Sinha, 2009; Visweswariah et al., 2010). Sinha (2009) show that an English-Urdu machine translation system can be easily build by using Hindi as a bridge language. Urdu translations of English sentences are derived from the output of an existing English-Hindi MT system. They use lexical mappings between Hindi and Urdu words, lexical and syntactic disambiguation rules, and a transliteration module for converting the MT output to Urdu. Adeeba and Hussain (2011) used a transliteration-based approach to create an Urdu WordNet from an existing Hindi WordNet (Narayan et al., 2002). They used a rule-based transliteration system to convert"
C16-1039,W14-1613,0,0.0213025,"in Hindi and Urdu vocabularies, we could apply distributional similarity methods on the union of harmonized (represented in same script) Hindi and Urdu corpora. Augmenting source domain corpus with the target domain data to learn distributional representation of words is a common practice to address lexical sparseness encountered in domain adaptation tasks (Candito et al., 2011; Plank and Moschitti, 2013). We could also use bilingual word clustering or word embedding approaches which have been used to address the loss of lexical information in delexicalized parsing (T¨ackstr¨om et al., 2012; Xiao and Guo, 2014). In case of Hindi and Urdu, the former approach is more simple and direct way to capture the distributional similarity. The similar grammar and partially shared vocabularies would ensure semantically similar words of Hindi and Urdu are assigned similar distributional representations. The cross-lingual approaches, on the other hand, are computationally complex, while capture the distributional similarity indirectly using a seed bilingual 403 lexicon. The distributional similarity can be incorporated in a statistical model either by using word clusters or word vectors (Turian et al., 2010). Sim"
C16-1039,W06-1672,0,0.0178496,"been seen to break the barrier that makes the two look different. Most of the existing works on Hindi-Urdu transliteration have considered basic rule-based models which use character tables coupled with a set of heuristics to resolve ambiguous mappings. Statistical approaches have hardly been explored (Sajjad et al., 2011). Unlike rule-based systems, statistical approaches are more robust and efficient. Among data-driven approaches, machine learning methods like noisy-channel model and structured prediction algorithms have been widely used for machine transliteration (Knight and Graehl, 1998; Zelenko and Aone, 2006). In this work, we model Hindi-Urdu transliteration as a structured prediction problem using a linear model. We use the structured perceptron (DHMM) of Collins (Collins, 2002) to learn the parameters of our transliteration model. Given an input training data of aligned character sequences D = d1 ...dn , a vector feature function ~f (d), and an initial weight vector ~ w, the algorithm performs two steps for each training example di ∈ D: (1) Decode: tˆ = arg max(~w · ~f (d)), and (2) Update: ~w = ~w + ~f (d) − ~f (tˆ). We use Viterbi-search for decoding in case t1 ···tn of Devanagari to Perso-Ar"
C16-1039,J98-4003,0,\N,Missing
D19-5216,D15-1166,0,0.383414,"t WAT 2019 Vikrant Goyal IIIT Hyderabad vikrant.goyal@research.iiit.ac.in Abstract Transformer model. This paper describes the Neural Machine Translation systems of IIIT-Hyderabad (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent Neural Networks & Transformer architectures. We also show the results of our experiments of training NMT models using additional data via backtranslation. 1 2.1 Attention-based encoder-decoder In this architecture, the NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in (Luong et al., 2015). The model directly estimates the posterior distribution Pθ (y|x) of translating a source sentence x = (x1 , .., xn ) to a target sentence y = (y1 , .., ym ) as: Introduction Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts. NMT often outperforms Statistical Machine Translation (SMT) techniques but it still struggles if the parallel data is insufficient lik"
D19-5216,D10-1092,0,0.030451,"ource languages. In this paper, we showed the effectiveness of Transformer models on a low resource languages pair Hindi-English. Additionally we show, how synthetic data can help improving the NMT systems for Hindi-English. Data Processing We used Moses (Koehn et al., 2007) toolkit for tokenization and cleaning the English side of the data. Hindi side of the data is first normalized with Indic NLP library1 followed by tokenization with 1 Results In table 2, we report Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) score, Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010), Adequacy-fluency metrics (AM-FM) (Banchs et al., 2015) and the Human Evaluation results provided by WAT 2019 for all our attempts. The results show that our NMT system based on Transformer & backtranslation is ranked 2nd among all the constraint submissions made in WAT 2019 Hindi-English shared task & is ranked 3rd overall. Table 1: Statistics of our processed parallel data. Dataset Training Details 2 https://anoopkunchukuttan.github.io/indic nlp library/ 139 https://pypi.org/project/truecase/ Table 2: This table describes the results of WAT 2019 evaluation of our submitted systems & compare"
D19-5216,P02-1040,0,0.110014,"k We believe that NMT is indeed a promising approach for Machine Translation of low resource languages. In this paper, we showed the effectiveness of Transformer models on a low resource languages pair Hindi-English. Additionally we show, how synthetic data can help improving the NMT systems for Hindi-English. Data Processing We used Moses (Koehn et al., 2007) toolkit for tokenization and cleaning the English side of the data. Hindi side of the data is first normalized with Indic NLP library1 followed by tokenization with 1 Results In table 2, we report Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) score, Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010), Adequacy-fluency metrics (AM-FM) (Banchs et al., 2015) and the Human Evaluation results provided by WAT 2019 for all our attempts. The results show that our NMT system based on Transformer & backtranslation is ranked 2nd among all the constraint submissions made in WAT 2019 Hindi-English shared task & is ranked 3rd overall. Table 1: Statistics of our processed parallel data. Dataset Training Details 2 https://anoopkunchukuttan.github.io/indic nlp library/ 139 https://pypi.org/project/truecase/ Table 2: This"
D19-5216,P16-1009,0,0.0580458,"Missing"
D19-5216,W18-1817,0,0.0182698,"h IITB corpus, we employ back translation. Backtranslation (Sennrich et al., 2015a) is a widely used data augmentation technique for aiding Neural Machine Translation for languages low on parallel data. The method works by generating synthetic data on the source side from target side monolingual data using a target-to-source NMT model. The synthetic parallel data thus formed is combined with the actual parallel data to train a new NMT model. We used around 10M English sentences and backtranslated them into Hindi using a English-Hindi NMT model. 3 For all of our experiments, we used OpenNMTpy (Klein et al., 2018) toolkit. We used both attention-based LSTM models and Transformer models in our submissions. We used an LSTM based Bi-directional encoder and a unidirectional decoder along with global attention mechanism. We kept 4 layers in both the encoder & decoder with embedding size set to 512. The batch size was set to 64 and a dropout rate of 0.3. We used Adam optimizer (Kingma and Ba, 2014) for all our experiments. For our transformer model, we used 6 layers in both encoder and decoder with 512 hidden units in each layer. The word embedding size was set to 512 with 8 heads. The training is run in bat"
D19-5216,P07-2045,0,0.0228284,"nolingual data from WMT14 newscrawl articles is used in our backtranslation enabled attempts at training an NMT system. 4 3.2 Sentences Tokens IITB Train IITB Test IITB Dev 15,28,631 2,507 520 21.5M / 20.3M 62.3k / 55.8k 9.7k / 10.3k 5 Conclusion Future Work We believe that NMT is indeed a promising approach for Machine Translation of low resource languages. In this paper, we showed the effectiveness of Transformer models on a low resource languages pair Hindi-English. Additionally we show, how synthetic data can help improving the NMT systems for Hindi-English. Data Processing We used Moses (Koehn et al., 2007) toolkit for tokenization and cleaning the English side of the data. Hindi side of the data is first normalized with Indic NLP library1 followed by tokenization with 1 Results In table 2, we report Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) score, Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Isozaki et al., 2010), Adequacy-fluency metrics (AM-FM) (Banchs et al., 2015) and the Human Evaluation results provided by WAT 2019 for all our attempts. The results show that our NMT system based on Transformer & backtranslation is ranked 2nd among all the constraint submis"
D19-5538,L16-1727,1,0.92282,"play is not uncommon in social media data. These tokens are detected as proper nouns. We added them as predicates, according to their context, manually. 3 • Predicate + Phrasetype • Predicate + Headword Semantic Arguments are identified at a phrase or chunk level. Hence we used features such as Headword of the chunk, phrasetype category, as baseline features. We also saw the impact of the part of speech (POS) tag of the Headword. Semantic Role Labeller 3.1.2 Features specific to Indian Languages Previous work on Semantic Role Labelling have used the following features for Hindi specifically (Anwar and Sharma, 2016): Our Semantic Role Labeller has a 2-step architecture. The first step is a binary classification task wherein each token in the tweet is classified as ‘Argument’ or ‘Not an Argument’. This step is called Argument Identification. In the second step, the identified arguments from the previous step are classified into the various semantic roles. This is called Argument Classification. • Dependency(karaka relation): Paninian dependency label • Named Entities • HeadwordPOS + Phrasetype We used Support Vector Models (SVM) for binary classification. The identified arguments from this step are then c"
D19-5538,N18-1090,1,0.842602,"icular, doesn’t strictly adhere to the syntax, morphology or structure of any of the involved languages, which results in standard NLP tools not performing well with this data for a lot of 2 Data and Pre-Processing We used a dataset of 1460 Hindi-English codemixed tweets comprising of 20,949 tokens labelled with their semantic roles (Pal and Sharma, 291 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 291–296 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics 2019). This dataset is built on a dependency labelled corpus by Bhat et al. (2018). The tokens are parsed and labelled with Proposition Bank (PropBank) labels shown in table 1, depicting semantic roles of the arguments with respect to the predicates in the sentence (Palmer et al., 2005; Bhatt et al., 2009). Label ARGA ARG0 ARG1 ARG2 ARG2 ATTR ARG2 LOC ARG2 GOL ARG2 SOU ARG3 ARGM DIR ARGM LOC ARGM MNR ARGM EXT ARGM TMP ARGM REC ARGM PRP ARGM CAU ARGM DIS ARGM ADV ARGM NEG ARGM PRX the English word ‘hour’. In our corpus, it referred to the Hindi word ‘har’ which is a quantifier and means ‘every’. • Elongation - tokens such as “Loooonng”, “Heyyyyy”, “pyaaaar” and so on. • Typi"
D19-5538,W09-3036,1,0.567452,"dataset of 1460 Hindi-English codemixed tweets comprising of 20,949 tokens labelled with their semantic roles (Pal and Sharma, 291 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 291–296 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics 2019). This dataset is built on a dependency labelled corpus by Bhat et al. (2018). The tokens are parsed and labelled with Proposition Bank (PropBank) labels shown in table 1, depicting semantic roles of the arguments with respect to the predicates in the sentence (Palmer et al., 2005; Bhatt et al., 2009). Label ARGA ARG0 ARG1 ARG2 ARG2 ATTR ARG2 LOC ARG2 GOL ARG2 SOU ARG3 ARGM DIR ARGM LOC ARGM MNR ARGM EXT ARGM TMP ARGM REC ARGM PRP ARGM CAU ARGM DIS ARGM ADV ARGM NEG ARGM PRX the English word ‘hour’. In our corpus, it referred to the Hindi word ‘har’ which is a quantifier and means ‘every’. • Elongation - tokens such as “Loooonng”, “Heyyyyy”, “pyaaaar” and so on. • Typing errors. For example, “saluet”, which should have been ‘salute’. Description Causer Agent or Experiencer or Doer Theme or Patient Benificiary Attribute or Quality Physical Location Destination or Goal Source Instrument Dire"
D19-5538,bonial-etal-2014-propbank,0,0.0139244,"ion was the most commonly found error in our corpus. These were all normalised and corrected manually to ensure a consistent spelling throughout the corpus. 2.2 A word can have different meanings according to the context in which it is used. T2 is an example from the corpus. The token “dikhny” refers to the Hindi verb ‘xeKa’2 which means to look. This verb can have different senses according to its context as shown in table 2. From context we know the relevant roleset here would be [xeKa.01]. Available Frame files are used to identify rolesets for the verbs in the corpus (Vaidya et al., 2013; Bonial et al., 2014). Table 1: PropBank Tagset Social media data doesn’t conform to the rules of spelling, grammar or punctuation. These need to be taken into account to maintain uniformity for our system. We incorporated this in our preprocessing steps. 2.1 T2: “We are journilist and hmy sechae dikhny se kiu rok ni skta” Translation: We are journalists and no one can stop us from seeing the truth. Misspelling One of the most widely seen errors in social media data is ‘typos’, which are errors in spelling, usually slangs or typing errors. These errors can be broadly classified as follows: Different senses for xeK"
D19-5538,W16-5801,0,0.0460295,"Missing"
D19-5538,W19-4020,1,0.880363,"Missing"
D19-5538,J05-1004,0,0.164644,"-Processing We used a dataset of 1460 Hindi-English codemixed tweets comprising of 20,949 tokens labelled with their semantic roles (Pal and Sharma, 291 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 291–296 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics 2019). This dataset is built on a dependency labelled corpus by Bhat et al. (2018). The tokens are parsed and labelled with Proposition Bank (PropBank) labels shown in table 1, depicting semantic roles of the arguments with respect to the predicates in the sentence (Palmer et al., 2005; Bhatt et al., 2009). Label ARGA ARG0 ARG1 ARG2 ARG2 ATTR ARG2 LOC ARG2 GOL ARG2 SOU ARG3 ARGM DIR ARGM LOC ARGM MNR ARGM EXT ARGM TMP ARGM REC ARGM PRP ARGM CAU ARGM DIS ARGM ADV ARGM NEG ARGM PRX the English word ‘hour’. In our corpus, it referred to the Hindi word ‘har’ which is a quantifier and means ‘every’. • Elongation - tokens such as “Loooonng”, “Heyyyyy”, “pyaaaar” and so on. • Typing errors. For example, “saluet”, which should have been ‘salute’. Description Causer Agent or Experiencer or Doer Theme or Patient Benificiary Attribute or Quality Physical Location Destination or Goal S"
D19-5538,N06-1025,0,0.0690464,"too much’ We present a 2-step system for automated Semantic Role Labelling of Hindi-English codemixed tweets. The first step is to identify the arguments of the predicates in the sentence. The second step is to then classify these identified arguments into various semantic roles. We discuss the effect of 14 linguistic features on our system, of which 6 are derived from literature and rest are specific to Hindi or to the nature of code-mixed text. Semantic Role Labelling will aid in various NLP tasks such as building question-answering systems (Shen and Lapata, 2007), co-reference resolution (Ponzetto and Strube, 2006), document summarization (Khan et al., 2015), information retrieval (Moschitti et al., 2003; Osman et al., 2012) and so on. The structure of this paper is as follows. We describe our data and the normalisation done for pre-processing of the text for our system in Section 2. The features used and compared are explained in detail in Section 3 along with the architecture of our system. We analyse the experiments and its results in Section 4. In Section 5, we conclude the paper. Introduction Semantic Role Labelling (SRL) deals with identifying arguments of a given predicate or verb, in a sentence"
D19-5538,N04-1030,0,0.152825,"sed Support Vector Models (SVM) for binary classification. The identified arguments from this step are then classified into various semantic roles mentioned in Table 1. We used the Linear SVC class of SVM (Pedregosa et al., 2011) for one-vs-rest multi-class classification. The data was split in the ratio of 80:20 for training and testing respectively. All parameters of the LinearSVC were set to default for training. 3.1 • Headword + Phrasetype We used the same features in our system. Named Entities have previously been seen to be a critical feature for Argument Identification task in English (Pradhan et al., 2004). Vaidya et al. (2011) showed the strong corelation between Paninian dependency (karta) labels and Propbank labels for Hindi. This feature was also seen to give the best results for Hindi and Urdu monolingual corpus (Anwar and Sharma, 2016). Universal Dependencies (UD) have gained a lot of attention lately for cross-lingual parsing. Tandon et al. (2016) discussed and evaluated UD scheme for Hindi and also compared them to Paninian dependency labels. We evaluated UD part of speech(POS) tags and UD dependency labels as features in our system, as mentioned below. Features used Hindi and English h"
D19-5538,D07-1002,0,0.0507188,"life is revolving around ‘I am hungry’ and ‘I ate too much’ We present a 2-step system for automated Semantic Role Labelling of Hindi-English codemixed tweets. The first step is to identify the arguments of the predicates in the sentence. The second step is to then classify these identified arguments into various semantic roles. We discuss the effect of 14 linguistic features on our system, of which 6 are derived from literature and rest are specific to Hindi or to the nature of code-mixed text. Semantic Role Labelling will aid in various NLP tasks such as building question-answering systems (Shen and Lapata, 2007), co-reference resolution (Ponzetto and Strube, 2006), document summarization (Khan et al., 2015), information retrieval (Moschitti et al., 2003; Osman et al., 2012) and so on. The structure of this paper is as follows. We describe our data and the normalisation done for pre-processing of the text for our system in Section 2. The features used and compared are explained in detail in Section 3 along with the architecture of our system. We analyse the experiments and its results in Section 4. In Section 5, we conclude the paper. Introduction Semantic Role Labelling (SRL) deals with identifying a"
D19-5538,D08-1110,0,0.0204058,"Missing"
D19-5538,W16-1716,1,0.856891,"l parameters of the LinearSVC were set to default for training. 3.1 • Headword + Phrasetype We used the same features in our system. Named Entities have previously been seen to be a critical feature for Argument Identification task in English (Pradhan et al., 2004). Vaidya et al. (2011) showed the strong corelation between Paninian dependency (karta) labels and Propbank labels for Hindi. This feature was also seen to give the best results for Hindi and Urdu monolingual corpus (Anwar and Sharma, 2016). Universal Dependencies (UD) have gained a lot of attention lately for cross-lingual parsing. Tandon et al. (2016) discussed and evaluated UD scheme for Hindi and also compared them to Paninian dependency labels. We evaluated UD part of speech(POS) tags and UD dependency labels as features in our system, as mentioned below. Features used Hindi and English have very different grammatical rules and vary greatly syntactically as well. We incorporated linguistic features in our system which may take into account these differences and help the labeller attain higher accuracy in identifying and classifying arguments. 3.1.1 Baseline Features We used 6 baseline features which have been used extensively for the ta"
D19-5538,W11-0403,0,0.0221424,"ls (SVM) for binary classification. The identified arguments from this step are then classified into various semantic roles mentioned in Table 1. We used the Linear SVC class of SVM (Pedregosa et al., 2011) for one-vs-rest multi-class classification. The data was split in the ratio of 80:20 for training and testing respectively. All parameters of the LinearSVC were set to default for training. 3.1 • Headword + Phrasetype We used the same features in our system. Named Entities have previously been seen to be a critical feature for Argument Identification task in English (Pradhan et al., 2004). Vaidya et al. (2011) showed the strong corelation between Paninian dependency (karta) labels and Propbank labels for Hindi. This feature was also seen to give the best results for Hindi and Urdu monolingual corpus (Anwar and Sharma, 2016). Universal Dependencies (UD) have gained a lot of attention lately for cross-lingual parsing. Tandon et al. (2016) discussed and evaluated UD scheme for Hindi and also compared them to Paninian dependency labels. We evaluated UD part of speech(POS) tags and UD dependency labels as features in our system, as mentioned below. Features used Hindi and English have very different gra"
D19-5538,W13-1018,0,0.0223329,"rmity in transliteration was the most commonly found error in our corpus. These were all normalised and corrected manually to ensure a consistent spelling throughout the corpus. 2.2 A word can have different meanings according to the context in which it is used. T2 is an example from the corpus. The token “dikhny” refers to the Hindi verb ‘xeKa’2 which means to look. This verb can have different senses according to its context as shown in table 2. From context we know the relevant roleset here would be [xeKa.01]. Available Frame files are used to identify rolesets for the verbs in the corpus (Vaidya et al., 2013; Bonial et al., 2014). Table 1: PropBank Tagset Social media data doesn’t conform to the rules of spelling, grammar or punctuation. These need to be taken into account to maintain uniformity for our system. We incorporated this in our preprocessing steps. 2.1 T2: “We are journilist and hmy sechae dikhny se kiu rok ni skta” Translation: We are journalists and no one can stop us from seeing the truth. Misspelling One of the most widely seen errors in social media data is ‘typos’, which are errors in spelling, usually slangs or typing errors. These errors can be broadly classified as follows: Di"
D19-5538,W04-3212,0,0.143764,"Paninian dependency labels. We evaluated UD part of speech(POS) tags and UD dependency labels as features in our system, as mentioned below. Features used Hindi and English have very different grammatical rules and vary greatly syntactically as well. We incorporated linguistic features in our system which may take into account these differences and help the labeller attain higher accuracy in identifying and classifying arguments. 3.1.1 Baseline Features We used 6 baseline features which have been used extensively for the task of Semantic Role Labelling for English (Gildea and Jurafsky, 2002; Xue and Palmer, 2004). They are as follows: • HeadwordPOS(UD) - UD part of speech tag of the headword • UD dependency label 293 3.1.3 Features for code-mixed data only predicate as a feature (Table 3). T4 is an example from the corpus where the token “ban” is the Hindi verb [bana], ‘to become’. This can be confused with the English verb ‘ban’ (legal prohibition). In such cases, the language of the predicate token can play an important role. Since we are dealing with code-mixed text, we wanted to see the effect the identified language of a token may have. We thus used the following features: • Predicate + language:"
E17-2052,Q16-1031,0,0.0528961,"Missing"
E17-2052,P15-1119,0,0.0296859,"e-trained word representations while for the non-lexical features, we use randomly initialized embeddings within a range of −0.25 to +0.25.9 We use Hindi and English monolingual corpora to learn the distributed representation of the lexical units. The English monolingual data contains around 280M sentences, while the Hindi data is comparatively smaller and contains around 40M sentences. The word representations are learned using Skip-gram model with negative sampling which is implemented in word2vec toolkit (Mikolov et al., 2013). For multilingual models, we use robust projection algorithm of Guo et al. (2015) to induce bilingual representations 8 In our experiments we fixed these to be {-0.25,0.25} for Hindi and {0.25,-0.25 } for English 9 Dimensionality of input units in POS and parsing models: 80 for words, 20 for POS tags, 2 for language tags and 20 for affixes. POS Models We train POS tagging models using a similar neural network architecture as dis327 Data-set CMd CMt HINt ENGt Monolingual UAS LAS 60.77 49.24 60.05 48.52 93.29 90.60 85.12 82.86 Gold (POS + language tag) Interpolated Multilingual Multipassf UAS LAS UAS LAS UAS LAS 74.62 64.11 75.77 65.32 69.37 58.83 74.40 63.65 74.16 64.11 68."
E17-2052,C16-1234,1,0.876885,"Missing"
E17-2052,D14-1082,0,0.0106639,"OS data sets. In the Multilingual approach, we train a single model on combined data sets of the languages in the codemixed data. We concatenate an additional 1x2 vector8 in the input layer of the neural network representing the language tag of the current word. Table 2 gives the POS tagging accuracies of the two models. Count 29 1290 1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B"
E17-2052,W02-1001,0,0.119048,"Missing"
E17-2052,N15-3017,0,0.0320652,"able 1. Normalization and Transliteration We model the problem of both normalization and backtransliteration of (noisy) Romanized Hindi words as a single transliteration problem. Our goal is to learn a mapping for both standard and nonstandard Romanized Hindi word forms to their respective standard forms in Devanagari. For this purpose, we use the structured perceptron of Collins (Collins, 2002) which optimizes a given loss function over the entire observation sequence. For training the model, we use the transliteration pairs (87,520) from the Libindic transliteration project6 and Brahmi-Net (Kunchukuttan et al., 2015) and augmented them with noisy transliteration pairs (63,554) which are synthetically generated by dropping non-initial vowels and replacing consonants based on their phonological proximity. We use Giza++ (Och and Ney, 2003) to character align the transliteration pairs for training. At inference time, our transliteration model would predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering an overall sentential context. Contracted word forms in social media content are quite often ambiguous and can repres"
E17-2052,N16-1159,1,0.896121,"Missing"
E17-2052,D08-1102,0,0.0693574,"languages or language varieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Pro"
E17-2052,D08-1110,0,0.71943,"languages or language varieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Pro"
E17-2052,W14-3907,0,0.136667,"Missing"
E17-2052,L16-1680,0,0.0326726,"Missing"
E17-2052,D14-1105,0,0.460363,"arieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Proceedings of the 15th"
E17-2052,W03-3017,0,0.115112,"1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): 1) Shift, 2) Left-Arc, 3) Right-Arc, and 4) Reduce."
E17-2052,J08-4003,0,0.0469972,"2 gives the POS tagging accuracies of the two models. Count 29 1290 1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of tr"
E17-2052,J03-1002,0,0.00619803,"nonstandard Romanized Hindi word forms to their respective standard forms in Devanagari. For this purpose, we use the structured perceptron of Collins (Collins, 2002) which optimizes a given loss function over the entire observation sequence. For training the model, we use the transliteration pairs (87,520) from the Libindic transliteration project6 and Brahmi-Net (Kunchukuttan et al., 2015) and augmented them with noisy transliteration pairs (63,554) which are synthetically generated by dropping non-initial vowels and replacing consonants based on their phonological proximity. We use Giza++ (Och and Ney, 2003) to character align the transliteration pairs for training. At inference time, our transliteration model would predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering an overall sentential context. Contracted word forms in social media content are quite often ambiguous and can represent different standard word forms such as ‘pt’ may refer to ‘put’, ‘pit’, ‘pat’, ‘pot’ and ‘pet’. To resolve this ambiguity, we extract n-best transliterations from the transliteration model using beam-search decoding. The b"
E17-2052,D16-1121,0,0.0236397,"Missing"
gupta-etal-2010-partial,W03-2401,0,\N,Missing
gupta-etal-2010-partial,miltsakaki-etal-2004-penn,0,\N,Missing
gupta-etal-2010-partial,J93-2004,0,\N,Missing
gupta-etal-2010-partial,W99-0502,0,\N,Missing
gupta-etal-2010-partial,brants-2000-inter,0,\N,Missing
gupta-etal-2010-partial,W04-2703,0,\N,Missing
gupta-etal-2010-partial,W98-1207,0,\N,Missing
gupta-etal-2010-partial,E06-2027,0,\N,Missing
gupta-etal-2010-partial,W09-3036,1,\N,Missing
gupta-etal-2010-partial,W09-3030,1,\N,Missing
gupta-etal-2010-partial,W05-1201,0,\N,Missing
gupta-etal-2010-partial,P09-2056,0,\N,Missing
gupta-etal-2010-partial,P93-1015,0,\N,Missing
gupta-etal-2010-partial,J05-1004,0,\N,Missing
gupta-etal-2010-partial,I08-2099,1,\N,Missing
gupta-etal-2010-partial,di-eugenio-2000-usage,0,\N,Missing
I08-2099,J93-2004,0,0.0403278,"enomenon such as complex verbs, ellipses, etc. Empirical results of some experiments done on the currently annotated sentences are also reported. 1 Introduction A major effort is currently underway to develop a large scale tree bank for Indian Languages (IL). The lack of such a resource has been a major limiting factor in the development of good natural language tools and applications for ILs. Apart from that, a rich and large-scale tree bank can be an indispensable resource for linguistic investigations. Some notable efforts in this direction for other languages have been the Penn Tree Bank (Marcus et al., 1993) for English and the Prague Dependency Bank (Hajicova, 1998) for Czech. It is well known that context free grammar (CFG) is not well-suited for free-word order languages (Shieber, 1985); instead dependency framework appears to be better suited (Hudson, 1984; Mel&apos;Cuk, 1988, Bharati et al., 1995). Also, the dependency framework is arguably closer to semantics than the phrase structure grammar (PSG) if the dependency relations are judiciously chosen. In recent times many research groups have been shifting to the dependency paradigm due to this reason. Modern dependency grammar is attributed to Te"
I08-2099,P93-1015,1,0.655475,"Missing"
I08-2099,W04-1502,0,0.0665205,"lations. All the modifier-modified relations between the heads of the chunks (interchunk relations) are marked in this manner. Intrachunk relations can be marked by a set of rules at a later point. Experiments have been conducted with high performance in automatically marking intrachunk dependencies. Using information such as karakas based on some vibhaktis (post-positions) and other information like TAM (tense, aspect and modality) of the main verb seems very well suited for handling free word order languages. Other works based on this scheme like (Bharati et al., 1993; Bharati et al., 2002; Pedersen et al., 2004) have shown promising results. We, therefore, propose the use of dependency annotation based on the Paninian model in the Indian context. 3 Annotation Procedure and Corpus Description The annotation task is planned on a million word Hindi corpora obtained from CIIL (Central Institute for Indian Languages), Mysore, India. It is a representative corpus which contains texts from various domains like newspaper, literature, gov722 ernment reports, etc. The present subset on which the dependency annotation is being performed has already been manually tagged and chunked. Currently the annotation is b"
I08-2099,rambow-etal-2002-dependency,0,0.0804221,"Missing"
I08-2099,W04-1501,0,\N,Missing
I08-5005,I08-5014,0,0.100418,"Missing"
I08-5005,I08-5010,1,0.706849,"Missing"
I08-5005,N03-1028,0,0.0631804,"Missing"
I08-5005,W99-0612,0,0.0933665,"Missing"
I08-5005,I08-2099,1,0.294667,"Missing"
I08-5005,W07-1306,1,0.880915,"Missing"
I08-5005,A97-1029,0,0.41865,"Missing"
I08-5005,A00-1034,0,0.0634211,"Missing"
I08-5005,I08-1009,1,0.869023,"Missing"
I08-5005,C96-1071,0,0.0519417,"Missing"
I08-7010,I08-2099,1,0.820437,"Missing"
I08-7010,W05-0312,0,0.26675,"e connectives. 1 (1) The federal government suspended sales of U.S. savings bonds because Congress hasn’t lifted the ceiling on government debt. One of the questions that arises is how the PDTB style annotation can be carried over to languages other than English. It may prove to be a challenge cross-linguistically, as the guidelines and methodology appropriate for English may not apply as well or directly to other languages, especially when they differ greatly in syntax and morphology. To date, cross-linguistic investigations of connectives in this direction have been carried out for Chinese (Xue, 2005) and Turkish (Deniz and Webber, 2008). This paper explores discourse relation annotation in Hindi, a language with rich morphology and free word order. We describe our study of “explicit connectives” in a small corpus of Hindi texts, discussing them from two perspectives. First, we consider the type and distribution of Hindi connectives, proposing to annotate a wider range Introduction An increasing interest in human language technologies such as textual summarization, question answering, natural language generation has recently led to the development of several discourse annotation projects a"
I08-7010,I08-7009,0,0.404376,"Missing"
I08-7010,J03-4002,1,\N,Missing
I13-1008,W09-3036,1,0.755267,"Missing"
I13-1008,N12-2002,0,0.0598028,"ection 3, we will describe our approach for acquiring animacy in Hindi using case markers listed in (2). Section 3.1 describes the data used in our experiments, followed by discussion on feature extraction and normalization. In Section 4, we discuss the extraction of data sets from Hindi Wordnet for the evaluation of results of our experiments. In Section 5, we describe the results with thorough error analysis and conclude the paper with some future directions in Section 6. 2 tributional patterns regarding their general syntactic and morphological properties. Other works in the direction are (Bowman and Chopra, 2012) for English and (Baker and Brew, 2010) for English and Japanese. All these works use supervised learning methods on a manually labeled data set. These works use highly rich linguistic features (e.g., grammatical relations) extracted using syntactic parsers and anaphora resolution systems. The major drawback of these approaches is that they can not be extended to resource poor languages because these languages can not satisfy the prerequisites of these approaches. Not only the availability of manually annotated training data, but also the features used restrict their portability to resource po"
I13-1008,W05-0509,0,0.0342678,"Missing"
I13-1008,J01-3003,0,0.0561286,"tion, in linguistics, means that it is more frequent, natural, and predictable than a marked observation (Croft, 2002). Although, a given nominal can have varying degrees of control in different contexts irrespective of its animacy, its unmarked behavior should correlate well with Related Work In NLP, the role of animacy has been recently realized. It provides important information, to mention a few, for anaphora resolution (Evans and Orasan, 2000), argument disambiguation (Dell’Orletta et al., 2005), syntactic parsing (Øvrelid and Nivre, 2007), (Bharati et al., 2008) and verb classification (Merlo and Stevenson, 2001). Lexical resources like wordnet usually feature animacy of nominals of a given language (Fellbaum, 2010; Narayan et al., 2002). However, using wordnet, as a source for animacy, is not straightforward. It has its own challenges (Orsan and Evans, 2001; Orasan and Evans, 2007). Also, it’s only a few privileged languages that have such lexical resources available. Due to the unavailability of such resources that could provide animacy information, there have been some notable efforts in the last few years to automatically acquire animacy. The important and worth mentioning works in this direction"
I13-1008,W01-0716,0,0.0438816,"should correlate well with Related Work In NLP, the role of animacy has been recently realized. It provides important information, to mention a few, for anaphora resolution (Evans and Orasan, 2000), argument disambiguation (Dell’Orletta et al., 2005), syntactic parsing (Øvrelid and Nivre, 2007), (Bharati et al., 2008) and verb classification (Merlo and Stevenson, 2001). Lexical resources like wordnet usually feature animacy of nominals of a given language (Fellbaum, 2010; Narayan et al., 2002). However, using wordnet, as a source for animacy, is not straightforward. It has its own challenges (Orsan and Evans, 2001; Orasan and Evans, 2007). Also, it’s only a few privileged languages that have such lexical resources available. Due to the unavailability of such resources that could provide animacy information, there have been some notable efforts in the last few years to automatically acquire animacy. The important and worth mentioning works in this direction are (Øvrelid, 2006) and (Øvrelid, 2009). The works focus on Swedish and Norwegian common nouns using dis2 Our Approach Indo-Aryan is a major language family in India. 65 its literal animacy, i.e., animates should more frequently be used in contexts o"
I13-1008,E06-3008,0,0.0197485,"ical resources like wordnet usually feature animacy of nominals of a given language (Fellbaum, 2010; Narayan et al., 2002). However, using wordnet, as a source for animacy, is not straightforward. It has its own challenges (Orsan and Evans, 2001; Orasan and Evans, 2007). Also, it’s only a few privileged languages that have such lexical resources available. Due to the unavailability of such resources that could provide animacy information, there have been some notable efforts in the last few years to automatically acquire animacy. The important and worth mentioning works in this direction are (Øvrelid, 2006) and (Øvrelid, 2009). The works focus on Swedish and Norwegian common nouns using dis2 Our Approach Indo-Aryan is a major language family in India. 65 its literal animacy, i.e., animates should more frequently be used in contexts of high control while in-animates should be used in contexts of low control. A high degree of animacy necessarily implies high degree of control. So the prototypical use of animates is in the contexts of high control and of inanimates in the contexts of low control. As the discussion suggests, animates should occur more frequently with the case markers towards the lef"
I13-1008,E09-1072,0,0.067151,"wordnet usually feature animacy of nominals of a given language (Fellbaum, 2010; Narayan et al., 2002). However, using wordnet, as a source for animacy, is not straightforward. It has its own challenges (Orsan and Evans, 2001; Orasan and Evans, 2007). Also, it’s only a few privileged languages that have such lexical resources available. Due to the unavailability of such resources that could provide animacy information, there have been some notable efforts in the last few years to automatically acquire animacy. The important and worth mentioning works in this direction are (Øvrelid, 2006) and (Øvrelid, 2009). The works focus on Swedish and Norwegian common nouns using dis2 Our Approach Indo-Aryan is a major language family in India. 65 its literal animacy, i.e., animates should more frequently be used in contexts of high control while in-animates should be used in contexts of low control. A high degree of animacy necessarily implies high degree of control. So the prototypical use of animates is in the contexts of high control and of inanimates in the contexts of low control. As the discussion suggests, animates should occur more frequently with the case markers towards the left of the Scale (1),"
I13-1022,R09-2001,0,0.0218088,"ay et al. (2012) illustrated that semantic annotation delivers a significant improvement in parsing, confirming the hypothesis that semantics can assist syntactic analysis. Among Indian languages, notable efforts on using semantic information in dependency parsing are on Hindi. Bharati et al. (2008) illustrated that mere animacy (human, non-human and inanimate) of a nominal significantly improves the accuracy of the parser. Later studies on extending such information with finer semantic distinctions like time, place, abstract reconfirmed the substantial role of semantics in syntactic parsing (Ambati et al., 2009). These studies are carried out on a dataset with hand annotated semantics. Although these studies provide deep insights on the role of semantics in parsing, they are limited in application as such information can not be automatically generated while parsing new sentences. Introduction Last decade has witnessed several efforts towards developing robust data driven dependency parsing techniques (K¨ubler et al., 2009). The efforts, in turn, initiated a parallel drive for building dependency annotated treebanks (Tsarfaty et al., 2013), which serve as a data source for training data driven depende"
I13-1022,W10-1403,1,0.908427,"Missing"
I13-1022,W10-1411,0,0.0366886,"Missing"
I13-1022,I08-2099,1,0.894489,"Missing"
I13-1022,P08-1037,0,0.0550775,"Missing"
I13-1022,P93-1015,0,0.267678,"Missing"
I13-1022,P11-2123,0,0.0408801,"Missing"
I13-1022,W09-3036,1,0.904392,"Missing"
I13-1022,P99-1065,0,0.182133,"Missing"
I13-1022,W07-1204,0,0.0235415,"LAS) on 13,371 sentences over the baseline. The improvements are statistically significant at p&lt;0.01. The higher improvements on 1,000 sentences suggest that the semantic information could address the data sparsity problem. 1 The need for richer information invoked several efforts in the direction of annotating higher order linguistic information in treebanks. It was felt that semantics can be leveraged for syntactic disambiguation and thus semantic annotation was performed in syntactic treebanks to complement the morpho-syntactic annotations (Kingsbury et al., 2002; Montemagni et al., 2003). Fujita et al. (2007) and MacKinlay et al. (2012) illustrated that semantic annotation delivers a significant improvement in parsing, confirming the hypothesis that semantics can assist syntactic analysis. Among Indian languages, notable efforts on using semantic information in dependency parsing are on Hindi. Bharati et al. (2008) illustrated that mere animacy (human, non-human and inanimate) of a nominal significantly improves the accuracy of the parser. Later studies on extending such information with finer semantic distinctions like time, place, abstract reconfirmed the substantial role of semantics in syntact"
I13-1022,W12-5617,1,0.891299,"Missing"
I13-1022,W12-3607,1,0.881578,"Missing"
I13-1022,J13-1003,0,0.0407226,"Missing"
I13-1022,I05-1007,0,0.0857129,"Missing"
I13-1022,S12-1031,0,0.0162606,"over the baseline. The improvements are statistically significant at p&lt;0.01. The higher improvements on 1,000 sentences suggest that the semantic information could address the data sparsity problem. 1 The need for richer information invoked several efforts in the direction of annotating higher order linguistic information in treebanks. It was felt that semantics can be leveraged for syntactic disambiguation and thus semantic annotation was performed in syntactic treebanks to complement the morpho-syntactic annotations (Kingsbury et al., 2002; Montemagni et al., 2003). Fujita et al. (2007) and MacKinlay et al. (2012) illustrated that semantic annotation delivers a significant improvement in parsing, confirming the hypothesis that semantics can assist syntactic analysis. Among Indian languages, notable efforts on using semantic information in dependency parsing are on Hindi. Bharati et al. (2008) illustrated that mere animacy (human, non-human and inanimate) of a nominal significantly improves the accuracy of the parser. Later studies on extending such information with finer semantic distinctions like time, place, abstract reconfirmed the substantial role of semantics in syntactic parsing (Ambati et al., 2"
I13-1022,C02-1145,0,0.084188,"Missing"
I13-1130,I08-2099,1,0.820499,"availability of dependency data for such languages. In this paper, we explore the possibility of using dependency structure for anaphora resolution in Hindi. 2 Data set and Grammatical Framework In this work we use the data from the ‘Hindi/Urdu Dependency Treebank’ (Bhatt et al., 2009). It is a rich corpus with various linguistic information. The dependency annotation in this treebank is based on the Computational Paninian Grammar 977 International Joint Conference on Natural Language Processing, pages 977–981, Nagoya, Japan, 14-18 October 2013. (CPG-henceforth) framework, as is explained in (Begum et al., 2008) and (Bharati et al., 1995). This framework is based on the notion of ‘karaka’ which are syntactio-semantic relations representing the participant elements in the action specified by the verb and it emphasizes the role of case endings or markers such as post-positions and verbal inflections. 1 . Table 1 shows some of the relevant relations and their rough correspondence to the traditional grammatical relations in English. (2)select ‘k1’ k1 Evяy n  (vijaya) k4 rEv ko (to ravi) (1)move up to verb k2 EktAb (book) r6 apnFi (his own) Figure 1 Locative, Relative and Personal pronouns. The pronouns"
I13-1130,W09-3036,1,0.752039,"sestructure parse as a source of syntactic information. However, dependency structures are more suitable representations for relatively free word order languages such as Hindi (Bharati et al., 1995; Melˇcuk, 1988) and hence research in many such languages has focused on development of dependency based resources resulting in better availability of dependency data for such languages. In this paper, we explore the possibility of using dependency structure for anaphora resolution in Hindi. 2 Data set and Grammatical Framework In this work we use the data from the ‘Hindi/Urdu Dependency Treebank’ (Bhatt et al., 2009). It is a rich corpus with various linguistic information. The dependency annotation in this treebank is based on the Computational Paninian Grammar 977 International Joint Conference on Natural Language Processing, pages 977–981, Nagoya, Japan, 14-18 October 2013. (CPG-henceforth) framework, as is explained in (Begum et al., 2008) and (Bharati et al., 1995). This framework is based on the notion of ‘karaka’ which are syntactio-semantic relations representing the participant elements in the action specified by the verb and it emphasizes the role of case endings or markers such as post-position"
I13-1130,C12-2015,0,0.045699,"Missing"
I13-1130,P87-1022,0,0.602658,"ic references, while a decision tree classifier is used to resolve more ambiguous instances, using grammatical and semantic features. Our results show that, use of dependency structures provides syntactic knowledge which helps to resolve some specific types of references. Semantic information such as animacy and Named Entity categories further helps to improve the resolution accuracy. 1 Introduction In various approaches on anaphora resolution syntax has been used as an important feature. Some well-known syntax based approaches include Hobbs algorithm (Hobbs, 1986) and the Centering approach (Brennan et al., 1987). Various rule based and data driven approaches have been proposed which use syntactic information as an important feature. Most of the earlier works have used phrasestructure parse as a source of syntactic information. However, dependency structures are more suitable representations for relatively free word order languages such as Hindi (Bharati et al., 1995; Melˇcuk, 1988) and hence research in many such languages has focused on development of dependency based resources resulting in better availability of dependency data for such languages. In this paper, we explore the possibility of using"
I13-1130,Y12-1042,1,0.828597,"ed to the classifier which uses a learning algorithm to identify the referent. Label CPG relation Grammatical/thematic equivalent k1 karta Subject k2 karma Object k4 sampradan Experiencer/reciever k7p(or k2p) adhikaran Location r6 sambandh Genitive Table 1: CPG relations and equivalents We use a part of the treebank which is also annotated with animacy information for Noun phrases as described in (Jena et al., 2013). Also, we used NE-Recognizer for Hindi to get the Named entity categories. The treebank has been annotated with anaphora links for all the pronouns as per the scheme described in (Dakwale et al., 2012). The size of the data that we use for our experiments is 325 documents, containing 4970 pronouns out of which 3233 pronouns are annotated as entity pronouns 3 dF (gave) 3.1 Rule based resolution module The rule based module attempts to resolve the pronoun, using the dependency relations and other information, based on the category of the pronoun, which is decided using an exhaustive list of pronoun categories. We describe below some of the important rules used for different pronoun categories. 3.1.1 Reflexives In Hindi Possessive reflexives are the most frequent reflexives which are only used"
I13-1130,W13-2320,1,0.802799,"le-based resolution module in which different rules depending on the category of the pronoun are applied to identify the correct referent. If none of the possible rules apply to a pronoun, it is passed to the classifier which uses a learning algorithm to identify the referent. Label CPG relation Grammatical/thematic equivalent k1 karta Subject k2 karma Object k4 sampradan Experiencer/reciever k7p(or k2p) adhikaran Location r6 sambandh Genitive Table 1: CPG relations and equivalents We use a part of the treebank which is also annotated with animacy information for Noun phrases as described in (Jena et al., 2013). Also, we used NE-Recognizer for Hindi to get the Named entity categories. The treebank has been annotated with anaphora links for all the pronouns as per the scheme described in (Dakwale et al., 2012). The size of the data that we use for our experiments is 325 documents, containing 4970 pronouns out of which 3233 pronouns are annotated as entity pronouns 3 dF (gave) 3.1 Rule based resolution module The rule based module attempts to resolve the pronoun, using the dependency relations and other information, based on the category of the pronoun, which is decided using an exhaustive list of pro"
I13-1130,J01-4004,0,0.374733,".77 81 76 .93 706 343 .48 1071 653 .60 Table 2: Accuracy of the rule-based system Results in Table 2 show that performance of the rule based system is quite high for all types of pronouns except for third person personal pronouns. This motivates us to use a learning based approach for the pronouns which remain unresolved in the first module. Table (3) shows the overall performance of the hybrid system achieved over the rulebased system by using different sets of features. The best performance (0.70) is achieved with a combination of all the features. Classifier module We use the approach of (Soon et al., 2001) for classification. For training, a positive instance is created by pairing each anaphora and its actual antecedent, and negative instances are created by pairing the anaphora with multiple preceding non-antecedent Noun phrases. For testing, unlabeled instances are created by pairing the anaphora with all the Noun-phrases in upto 3 previous sentences. Testing instances are classified as positive Rule based system(RB) RB+Distance RB+Distance+Agreement RB+Distance+Animacy RB+Dist+Animacy+Agreement Total Correct Accuracy 1071 653 .60 1071 696 .64 1071 713 .66 1071 731 .68 1071 753 .70 Table 3: A"
I13-1151,W03-0318,0,0.144431,"7, results are being talked about. Section 8 gives the error analysis and in Section 9, we conclude and talk about future works in this area. 2 Related Work Chandrasekar et al.(1996) proposed Finite state grammar and Dependency based approach for sentence simplification. Automatic induction of rules for text simplification is discussed by Chandrashekhar and Srinivas (1997). A pipelined approach for text simplification has been presented by (Siddharthan, 2002). Sudoh et al. (2010) proposed divide and translate technique to address the issue of long distance reordering for machine translation. Doi and Sumita (2003) used splitting techniques for simplifying sentences and then utilizing the output for machine translation. Poorn1082 International Joint Conference on Natural Language Processing, pages 1082–1086, Nagoya, Japan, 14-18 October 2013. ima et al. (2011) proposed a rule based Sentence Simplification for English to Tamil Machine translation system. Though several attempts, in the past, have been carried out for English, we find few work on other languages. We find, no reported work on sentence simplification for Hindi, which is the language under focus in our work. 3 Table 1: Classification of a se"
I13-1151,D07-1013,0,0.0400779,"Missing"
I13-1151,W10-1762,0,0.0913412,"ibes the linguistic resources we used. In section 5, we discuss our algorithm. Section 6 outlines evaluation of the system. In section 7, results are being talked about. Section 8 gives the error analysis and in Section 9, we conclude and talk about future works in this area. 2 Related Work Chandrasekar et al.(1996) proposed Finite state grammar and Dependency based approach for sentence simplification. Automatic induction of rules for text simplification is discussed by Chandrashekhar and Srinivas (1997). A pipelined approach for text simplification has been presented by (Siddharthan, 2002). Sudoh et al. (2010) proposed divide and translate technique to address the issue of long distance reordering for machine translation. Doi and Sumita (2003) used splitting techniques for simplifying sentences and then utilizing the output for machine translation. Poorn1082 International Joint Conference on Natural Language Processing, pages 1082–1086, Nagoya, Japan, 14-18 October 2013. ima et al. (2011) proposed a rule based Sentence Simplification for English to Tamil Machine translation system. Though several attempts, in the past, have been carried out for English, we find few work on other languages. We find,"
I13-1151,P93-1015,0,0.0512519,"No Yes Table 2: verb-frame • Criterion2 : Number of verb chunks in the sentence is more than 1. Table 1 shows classification of a sentence based on the possible combinations of 3 criteria mentioned above. Criterion2 No No Yes Yes No No Yes Yes The Verb demand frames are built for the base form of a verb. The demands undergo a subsequent change based on the tense, aspect and modality (TAM) of the verb used in the sentence. The knowledge about the transformations induced on the base form of a verb by TAM is stored in 1 karakas are the typed dependency labels in Computational Paninian Framework(Bharati and Sangal, 1993) 1083 Table 3: karaka chart form of transformation charts for each distinct TAM. 5 arc-lbl k1 k2 Sentence Simplification Algorithm We present a rule based method for simplification of complex sentences. Our approach comprises two stages. The work flow of our approach is shown in Figure 1. In the first stage, we get the structural representation of the input using shallow parser. Input Sentence POS tagging Stage1 Chunking & Head Computation Stage2 Split on Conjuncts Simplify on verb frames Set of simple sentences necessity mandatory mandatory vibh 0 0 lex-cat noun noun src-pos l l tence and ass"
I13-1151,C96-2183,0,\N,Missing
I13-1151,W09-3036,1,\N,Missing
I13-1151,P02-1040,0,\N,Missing
I13-1151,begum-etal-2008-developing,1,\N,Missing
I17-3017,Q15-1042,0,0.0755737,"Missing"
I17-3017,Q15-1001,0,\N,Missing
I17-3017,D14-1058,0,\N,Missing
I17-3017,N16-1136,0,\N,Missing
I17-3017,P16-1202,0,\N,Missing
K18-3013,N13-1138,0,0.0563434,"rd among 23 systems and 4th among 23 systems for the low, medium and high data conditions respectively. The remainder of this paper is organized as follows. We present prior work on Morphological Inflection in Section 2. We describe our system in Section 3. The results of the shared task are presented in Section 4. In Section 5, we present ablation studies and discuss the contribution of the specific design decisions we made to the performance of our systems. We conclude the paper with Section 6. 2 Machine learning based approaches treat morphological inflection as a string transduction task (Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). These approaches extract rules automatically from the data, but they still require language specific feature engineering. Neural network based approaches successfully solve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approac"
K18-3013,N16-1077,0,0.133307,"ion of the specific design decisions we made to the performance of our systems. We conclude the paper with Section 6. 2 Machine learning based approaches treat morphological inflection as a string transduction task (Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). These approaches extract rules automatically from the data, but they still require language specific feature engineering. Neural network based approaches successfully solve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approach by using a single model instead of separate models for each morphological feature. They fed morphological tags into the encoder along with the sequence of characters of lemma. They also used attention mechanism (Bahdanau et al., 2014). Aharoni and Goldberg (2017) present an alternative to the soft attention in form of hard monotonic attention which models the almost monotonic"
K18-3013,P17-1183,0,0.0260426,"ve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approach by using a single model instead of separate models for each morphological feature. They fed morphological tags into the encoder along with the sequence of characters of lemma. They also used attention mechanism (Bahdanau et al., 2014). Aharoni and Goldberg (2017) present an alternative to the soft attention in form of hard monotonic attention which models the almost monotonic alignment between characters in lemma and the inflected form. The best performing system (Makarov et al., 2017) of the previous edition of this shared task extended the hard monotonic attention model of Aharoni and Goldberg (2017) with a copy mechanism (HACM model). To deal with low training data especially in the low and medium resource settings, some teams used data augmentation techniques (Kann and Sch¨utze, 2017; Bergmanis et al., Background Traditional approaches for morphol"
K18-3013,E14-1060,0,0.042305,"h among 23 systems for the low, medium and high data conditions respectively. The remainder of this paper is organized as follows. We present prior work on Morphological Inflection in Section 2. We describe our system in Section 3. The results of the shared task are presented in Section 4. In Section 5, we present ablation studies and discuss the contribution of the specific design decisions we made to the performance of our systems. We conclude the paper with Section 6. 2 Machine learning based approaches treat morphological inflection as a string transduction task (Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). These approaches extract rules automatically from the data, but they still require language specific feature engineering. Neural network based approaches successfully solve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approach by using a single m"
K18-3013,N15-1107,0,0.0197165,"r the low, medium and high data conditions respectively. The remainder of this paper is organized as follows. We present prior work on Morphological Inflection in Section 2. We describe our system in Section 3. The results of the shared task are presented in Section 4. In Section 5, we present ablation studies and discuss the contribution of the specific design decisions we made to the performance of our systems. We conclude the paper with Section 6. 2 Machine learning based approaches treat morphological inflection as a string transduction task (Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). These approaches extract rules automatically from the data, but they still require language specific feature engineering. Neural network based approaches successfully solve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approach by using a single model instead of separa"
K18-3013,W16-2010,0,0.331302,"Missing"
K18-3013,K17-2003,0,0.114214,"Missing"
K18-3013,K17-2002,0,0.0561289,"Missing"
K18-3013,D14-1179,0,0.0347414,"Missing"
K18-3013,P17-2031,0,0.0681764,"Missing"
K18-3013,K17-2005,0,0.123471,"Missing"
K18-3013,P07-1017,0,0.0737121,"require a lot of training data to work. We try to address this challenge by designing neural network architectures which work well even on the low resource setting of the task. Our system is based on attention based encoderdecoder models (Bahdanau et al., 2014). The Introduction Morphological Inflection is the process of inflecting a lemma according to a set of morphological features so that the lemma becomes in accordance with other words in the sentence. It is useful for alleviating data sparsity, especially in morphologically rich languages during Natural Language Generation. For example, Minkov et al. (2007) translate words from the source language to lemmas in the target language and then use Morphological Inflection as a post-processing step to make the words of the output sentence in agreement with each other. Not only their approach reduces the data sparsity by decreasing the number of candidate words while translating, it also gives better results. ∗ This research was conducted during the authors internship at IIIT Hyderabad. 105 Proceedings of the CoNLL–SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection, pages 105–111, c Brussels, Belgium, October 31, 2018. 2018 Association f"
K18-3013,N15-1093,0,0.0282955,"high data conditions respectively. The remainder of this paper is organized as follows. We present prior work on Morphological Inflection in Section 2. We describe our system in Section 3. The results of the shared task are presented in Section 4. In Section 5, we present ablation studies and discuss the contribution of the specific design decisions we made to the performance of our systems. We conclude the paper with Section 6. 2 Machine learning based approaches treat morphological inflection as a string transduction task (Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015). These approaches extract rules automatically from the data, but they still require language specific feature engineering. Neural network based approaches successfully solve this problem. These approaches require no feature engineering and the same architecture works for different languages. Faruqui et al. (2016) were the first to formulate morphological inflection as neural sequence to sequence learning problem (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Kann and Sch¨utze (2016) improved on their approach by using a single model instead of separate models for each morp"
K18-3013,K17-2008,0,0.0476601,"nic attention model of Aharoni and Goldberg (2017) with a copy mechanism (HACM model). To deal with low training data especially in the low and medium resource settings, some teams used data augmentation techniques (Kann and Sch¨utze, 2017; Bergmanis et al., Background Traditional approaches for morphological inflection involve crafting hand-engineered rules. Although these rules offer high accuracy, they are 106 The combined context vector is obtained by simply concatenating the lemma and the tag context vector. (8) h∗t = [h∗lt ; h∗tgt ] 2017; Silfverberg et al., 2017; Zhou and Neubig, 2017; Nicolai et al., 2017). 3 System Description In this section, we describe our system in detail. We report the neural network architecture, the training process, the hyperparameters and our submissions. 3.1 The combined context vector h∗t and the embedding of character predicted at the previous time step, yt−1 (while training to speed up convergence ∗ instead) is given we use the ground truth label yt−1 as input to the decoder. At the first time step, start character is given as input in place of yt−1 . Neural network architecture Our neural network architecture is based on Pointer-Generator Network (See et al., 201"
K18-3013,P17-1099,0,0.631224,"below. This paper describes the systems submitted by IIT (BHU), Varanasi/IIIT Hyderabad (IITBHU–IIITH) for Task 1 of CoNLL– SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection (Cotterell et al., 2018). The task is to generate the inflected form given a lemma and set of morphological features. The systems are evaluated on over 100 distinct languages and three different resource settings (low, medium and high). We formulate the task as a sequence to sequence learning problem. As most of the characters in inflected form are copied from the lemma, we use Pointer-Generator Network (See et al., 2017) which makes it easier for the system to copy characters from the lemma. PointerGenerator Network also helps in dealing with out-of-vocabulary characters during inference. Our best performing system stood 4th among 28 systems, 3rd among 23 systems and 4th among 23 systems for the low, medium and high resource setting respectively. 1 (touch, V;V.PTCP;PRS) → touching To assess the system’s ability to generalize in different resource settings, three varying amounts of labeled training data (low, medium, high) were given. The systems were evaluated separately for each language and the three data q"
K18-3013,K17-2010,0,0.0359037,"tion of this shared task extended the hard monotonic attention model of Aharoni and Goldberg (2017) with a copy mechanism (HACM model). To deal with low training data especially in the low and medium resource settings, some teams used data augmentation techniques (Kann and Sch¨utze, 2017; Bergmanis et al., Background Traditional approaches for morphological inflection involve crafting hand-engineered rules. Although these rules offer high accuracy, they are 106 The combined context vector is obtained by simply concatenating the lemma and the tag context vector. (8) h∗t = [h∗lt ; h∗tgt ] 2017; Silfverberg et al., 2017; Zhou and Neubig, 2017; Nicolai et al., 2017). 3 System Description In this section, we describe our system in detail. We report the neural network architecture, the training process, the hyperparameters and our submissions. 3.1 The combined context vector h∗t and the embedding of character predicted at the previous time step, yt−1 (while training to speed up convergence ∗ instead) is given we use the ground truth label yt−1 as input to the decoder. At the first time step, start character is given as input in place of yt−1 . Neural network architecture Our neural network architecture is based"
kolachina-etal-2012-evaluation,tonelli-etal-2010-annotation,1,\N,Missing
kolachina-etal-2012-evaluation,W04-2703,1,\N,Missing
kolachina-etal-2012-evaluation,mladova-etal-2008-sentence,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3029,1,\N,Missing
kolachina-etal-2012-evaluation,al-saif-markert-2010-leeds,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3036,1,\N,Missing
kolachina-etal-2012-evaluation,prasad-etal-2008-penn,1,\N,Missing
kolachina-etal-2012-evaluation,W11-0414,1,\N,Missing
kolachina-etal-2012-evaluation,I08-7009,0,\N,Missing
kolachina-etal-2012-evaluation,P12-1008,0,\N,Missing
kolachina-etal-2012-evaluation,I08-2099,1,\N,Missing
L16-1025,J08-4004,0,0.0933561,"otators. For every pair of values b and c (for sets), δbc is the distance between the values. nbi is the number of times the value b occurred in ith unit. In nominal scales δ = 0 when b = c (equivalent sets); otherwise δ = 1 (different sets). The δ value in above equation is depends upon comparison of two sets. As described in (Passonneau, 2004) the relation between two sets can be describe in four different ways i.e, identity, subscription, intersection and disjunctions and their δ values are, 0 for identity, 0.33 for subsumption, .67 for intersection and 1 for disjunctions. As discussed in (Artstein and Poesio, 2008), the assignment of δ value mislead the agreement, because it is not able to capture the length of sets (cardinality of sets). i.e. for the same sets discussed in (Passonneau, 2004), {C, H, J, K} and {C, H} has subsumption relation and {C, H, J, K} and {C} also has subsumption relation so according to (Passonneau, 2004), one needs to give 0.33 δ value. These two sets {C, H} and {C} have 2 and 1 elements respectively, and this difference in sets cannot be captured here. Therefore we use Jaccard index also known as the Jaccard similarity coefficient , for comparing the similarity anddiversity of"
L16-1025,I08-2099,1,0.777753,"un et al., 2009), linguistic head of a noun sequence/mention is mostly the last word and, main verb for the event (verb-noun sequence) mention. We built automatic mention head identifier based on that observation. We also observed that for a coreference chain, its head/governing element lies in its first mention thus we also added an automatic chain head marking facility to CAT. Annotators can always edit/update/delete automated annotation to correct the annotation process. 7 https://github.com/vmujadia/MentionIdentifier 167 7. Annotated Data Around 9000 sentences of Hindi dependency treebank(Begum et al., 2008) have been annotated with coreference and their relations with our described annotation scheme. During these process CAT was used by annotators for assistance. Hindi Dep. TreeBank # Documents # Sentences # Tokens Size 600 9000 90000 Table 2: Corpus detail Mention Type Personal Pronouns Reflexive Pronouns Relative Pronouns Locative Pronouns Verb-nominal sequences Definite noun sequences Indefinite noun sequences occurrences 1200 787 345 546 500 3000 1677 Table 3: Distribution of co-referential entities The annotation task was carried out on 600 news text documents of treebank. Table (2) shows t"
L16-1025,Y12-1042,1,0.791197,"d like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which are meant to be for English are not applicable for Indian languages due to their free word order. (Dakwale et al., 2012) presents an anaphora annotation scheme and applied it on Hindi Dependency Treebank for limited set of pronominal categories, particularly for concrete anaphors. (Dakwale et al., 2012) used key-value pair attributes on anaphor chunk-head to represent its referent(s). Compared to English coreference annotation (and its representation), very little work has been done on Indian languages. This paper tries to fill that gap by describing our coreference annota"
L16-1025,N01-1008,0,0.107702,"tatistics of coreference annotation on Hindi Dependency Treebank (HDTB). Keywords: Hindi, Coreference, Annotation Scheme 1. Introduction There has been considerable research for coreference annotation on various languages (English(Hovy et al., 2006), French(Mitkov et al., 2000), Spanish(Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which"
L16-1025,N06-2015,0,0.0740309,"relation types between continuous mentions of the same coreference chain such as ‘Part-of’, ‘Function-value pair’ etc. We used Jaccard similarity based Krippendorff‘s’ alpha to demonstrate consistency in annotation scheme, annotation and corpora. To ease the coreference annotation process, we built a semi-automatic Coreference Annotation Tool (CAT). We also provide statistics of coreference annotation on Hindi Dependency Treebank (HDTB). Keywords: Hindi, Coreference, Annotation Scheme 1. Introduction There has been considerable research for coreference annotation on various languages (English(Hovy et al., 2006), French(Mitkov et al., 2000), Spanish(Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work"
L16-1025,W01-1612,0,0.0817353,"erence, Annotation Scheme 1. Introduction There has been considerable research for coreference annotation on various languages (English(Hovy et al., 2006), French(Mitkov et al., 2000), Spanish(Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which are meant to be for English are not applicable for Indian languages due to their free word orde"
L16-1025,orasan-2000-clinka,0,0.0953334,"HDTB). Keywords: Hindi, Coreference, Annotation Scheme 1. Introduction There has been considerable research for coreference annotation on various languages (English(Hovy et al., 2006), French(Mitkov et al., 2000), Spanish(Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which are meant to be for English are not applicable for Indian lang"
L16-1025,passonneau-2004-computing,0,0.245829,"tator Study Coreference annotation is defined as a process of language corpora annotation, to indicate which textual expression have been used to co-specify the same entity in the discourse. When such an annotated corpora are collected from different coders, the reliability of the annotated data has to be quantified. Many times due to annotator‘s preferences, they do prejudice annotation, to standardize annotation process these types of errors have to be quantified. For coreference annotation, we used various reliability metrics to quantify the annotation scheme and annotation. As mention in (Passonneau, 2004), Krippendorff’s alpha (Krippendorff, 2012) is a better metric for calculating agreement for co-reference annotation as compared to other metrics. Because it considers degree of disagreement and can be apply for more than two annotators. Similar to (Dakwale et al., 2012) as explained in (Passonneau, 2004), we also consider coreference chain as discrete categories. Equation (Figure 1) demonstrate the Kripandorff‘s alpha, Where PDo is probability of observed disagreement and PDE is probability of expected disagree166 Figure 1: Kripandoff alpha Figure 2: Jaccard Index ments. For r coding units an"
L16-1025,recasens-etal-2010-typology,0,0.0317768,"Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which are meant to be for English are not applicable for Indian languages due to their free word order. (Dakwale et al., 2012) presents an anaphora annotation scheme and applied it on Hindi Dependency Treebank for limited set of pronominal categories, particularly for concrete anaphors. (Dak"
L16-1025,P08-4003,0,0.0290888,"uction There has been considerable research for coreference annotation on various languages (English(Hovy et al., 2006), French(Mitkov et al., 2000), Spanish(Recasens et al., 2007), Czech(Nedoluzhko et al., 2013), etc), on diverse domains like newspaper texts, bio-medical journals, etc. Coreference annotation is a time consuming, challenging and an expensive task. To overcome these challenges, various coreference annotation tools have been developed like CorefDraw (Harabagiu et al., 2001), GATE (Cunningham et al., 2002), PALinkA (Orasan and Sb, 2000), MMAX2 (Müller and Strube, 2001) and BART (Versley et al., 2008). All these provide text based visualization for annotation. Also there has been considerable work on coreference type relations, like, (Recasens et al., 2010) presented a typology of Near-Identity Relations and motivated the need for a middle ground category between identity and non-identity in the coreference task. For Indian languages, co-referentially annotated corpora are few in numbers and mostly for Hindi. As stated in (Dakwale et al., 2012), most schemes which are meant to be for English are not applicable for Indian languages due to their free word order. (Dakwale et al., 2012) presen"
L16-1276,I08-2099,1,0.938791,"Missing"
L16-1276,kolachina-etal-2012-evaluation,1,0.929388,"course analysis, Discourse connective identification, Hindi 1. Introduction Units within a piece of text are not meant to be understood independently but understood by linking them with other units in the text. These units may be clauses, sentences or even complete paragraphs. Establishing relations between units present in a text allows the text to be semantically well structured and understandable. Understanding the internal structure of text and the identification of discourse relations is called discourse analysis. The Hindi Discourse Relation Bank (Prasad et al., 2008b; Oza et al., 2009; Kolachina et al., 2012) was created to enable discourse analysis and research beyond sentence-level for Hindi. Similar discourse annotation projects for English (Miltsakaki et al., 2004; Prasad et al., 2008a), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008; Pol´akov´a et al., 2013) and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2010) have also been carried out. A fully automated discourse analyzer would thus greatly aid discourse analysis and NLP applications such as text summarization and question answering. The benefits of a fully automated discourse analyzer has led us to work towards"
L16-1276,J93-2004,0,0.057576,"The Hindi Discourse Relation Bank(HDRB) was previously created broadly following the lines of Penn Discourse TreeBank (PDTB)(Miltsakaki et al., 2004; Prasad et al., 2008a)’s lexically grounded approach along with a modified annotation workflow, additional grammatical categories for explicit connectives, semantically driven Arg1/Arg2 labelling and modified sense hierarchies.(Oza et al., 2009; Kolachina et al., 2012) HDRB was annotated on a subset of the Hindi TreeBank (Begum et al., 2008) which includes part-of-speech, chunk and dependency parse tree annotations. In comparison, Penn TreeBank (Marcus et al., 1993) has parts-of-speech, chunk and constituent parse tree annotations. The dependency annotations scheme in Hindi treats a sentence as a series of modifier-modified relations. Thus the root of the dependency tree would be the primary modified of the sentence which is generally the main verb of the sentence. The participant relations with the verb are called karakas. There are six basic karaka relations namely adhikarana(location), apaadaan(source), sampradaan(recipient), karana(instrument), karma(theme) and karta(agent). HDRB contains 1865 sentences and a word count of 42K. Furthermore HDRB conta"
L16-1276,miltsakaki-etal-2004-penn,0,0.591945,"Missing"
L16-1276,mladova-etal-2008-sentence,0,0.0572276,"Missing"
L16-1276,W09-3029,1,0.916645,"ask. Keywords: Discourse analysis, Discourse connective identification, Hindi 1. Introduction Units within a piece of text are not meant to be understood independently but understood by linking them with other units in the text. These units may be clauses, sentences or even complete paragraphs. Establishing relations between units present in a text allows the text to be semantically well structured and understandable. Understanding the internal structure of text and the identification of discourse relations is called discourse analysis. The Hindi Discourse Relation Bank (Prasad et al., 2008b; Oza et al., 2009; Kolachina et al., 2012) was created to enable discourse analysis and research beyond sentence-level for Hindi. Similar discourse annotation projects for English (Miltsakaki et al., 2004; Prasad et al., 2008a), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008; Pol´akov´a et al., 2013) and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2010) have also been carried out. A fully automated discourse analyzer would thus greatly aid discourse analysis and NLP applications such as text summarization and question answering. The benefits of a fully automated discourse analyzer h"
L16-1276,P09-2004,0,0.0273994,") lG udyog k  Ele nI yoяnA nhF { h aOr k-Vm ytF m  kVOtF kA sFDA asr Gr l udyog pr pX gA। There is no new project for small business and cutoffs in the custom duty would have a direct effect on the domestic businesses. (2) яAn aOr m {rF bA)Ar ge T । John and Mary went to the market. In sentence (1) and is a discourse connective between two clauses whereas in sentence (2) and occurs in a nondiscourse context. Our task is thus to differentiate between the discourse and non-discourse usage of a given connective. 3. Approach Existing work on discourse connective identifiers for English (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015) employ a classifier based approach with differences arising in choice of feature sets. We have decided to adopt the same classifier based approach for our task. We thus approach the task as classifying a set of connectives as positive or negative, where positive connectives are connectives marking the presence of a discourse relation and negative connectives are connectives marking no such presence. The set of possible connectives is obtained using a list of 125 known discourse connectives. Selection of features is a crucial aspect of classifier based ap"
L16-1276,I13-1011,0,0.0351668,"Missing"
L16-1276,prasad-etal-2008-penn,0,0.173707,"Missing"
L16-1276,I08-7010,1,0.849538,"Missing"
L16-1276,K15-2002,0,0.079614,"m ytF m  kVOtF kA sFDA asr Gr l udyog pr pX gA। There is no new project for small business and cutoffs in the custom duty would have a direct effect on the domestic businesses. (2) яAn aOr m {rF bA)Ar ge T । John and Mary went to the market. In sentence (1) and is a discourse connective between two clauses whereas in sentence (2) and occurs in a nondiscourse context. Our task is thus to differentiate between the discourse and non-discourse usage of a given connective. 3. Approach Existing work on discourse connective identifiers for English (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015) employ a classifier based approach with differences arising in choice of feature sets. We have decided to adopt the same classifier based approach for our task. We thus approach the task as classifying a set of connectives as positive or negative, where positive connectives are connectives marking the presence of a discourse relation and negative connectives are connectives marking no such presence. The set of possible connectives is obtained using a list of 125 known discourse connectives. Selection of features is a crucial aspect of classifier based approaches. Since we are to identify posi"
L16-1276,W05-0312,0,0.0916131,"Missing"
L16-1276,I08-7009,0,0.0682675,"Missing"
L16-1276,W10-1844,0,0.0483844,"Missing"
L16-1276,P12-1008,0,0.0307721,"Missing"
L16-1377,I08-2099,1,0.837098,"king the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency relation is defined between a dependent, a syntactically subordinate word and a head word on which it depends. In CPG, the verb is treated as the primary modified or as the root of the dependency tree and the elements modifying the verb event are defined by it. Karaka relations describe the manner in which arguments occur in the activity descri"
L16-1377,W12-3623,1,0.911754,"alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency relation is defined between a dependent, a syntactically subordinate word and a head word on which it depends. In CPG, the verb is treated as the primary modified or as the root of the dependency tree and the elements modifying the verb event are defined"
L16-1377,W14-4206,1,0.923651,"round 200K words) have been annotated with dependency structure. Each sentence contains an average of 29 words and an average of 13.7 chunks of average length 2.0. 4. Urdu PropBank Development Process In this section, we describe the development of Urdu PropBank. In general, PropBank annotation is a 2-step process. The initial step involves the creation of frame files. The subsequent step in this process is the actual annotation of predicate-argument structure using the frame files. In case of Urdu, instead of creating frame files afresh, the frame files from Hindi and Arabic were ported (see Bhat et al. (2014) for more details). 4.1. Porting and Adapting Predicate frames from Hindi and Arabic PropBanks The first step in building a PropBank is to make the predicate frames for simple and complex predicates (described later) available. Instead of creating predicate frames for each and every verb present in Urdu, which would indeed be very demanding and time-consuming task, (Bhat et al., 2014) explored the feasibility of porting predicate frames from already built Arabic and Hindi PropBank for use in Urdu PropBanking. This instigation was prompted from the fact that Hindi and Urdu are linguistically si"
L16-1377,W09-3036,1,0.800155,"ces are represented in the Shakti Standard Format (Bharati et al., 2007). The Dependency tagset consists of about 43 labels. PropBank is mainly concerned with those labels depicting dependencies in the context of verb predicates. The Dependency Treebanks for Hindi and Urdu are developed following a generic pipeline. It involves building multi-layered and multi-representational Treebanks for Hindi and Urdu. The steps in the process of building the Urdu Treebank under this pipeline consists of (i) Tokenization, (ii) MorphAnalysis, (iii) POS-tagging, (iv) Chunking, and (v) Dependency annotation (Bhatt et al., 2009). Annotation process commences with the tokenization of raw text. The tokens thus obtained are annotated with morphological and POS information. After morph-analysis and POS-tagging, words are grouped into chunks. All the above processing steps have been automated by high accuracy tools (rule-based or statistical) thus speeding up the manual process. The last process in this pipeline so far is the manual dependency annotation. The inter-chunk dependencies are marked leaving the dependencies between words in a chunk unspecified for the intra-chunk dependencies. PropBanking is the next step in t"
L16-1377,J96-2004,0,0.558258,"between the two annotators using a data set of 44,000 words double-annotated (double PropBanked) by two annotators, without either annotator knowing other’s judgment of marking semantic role labels. A healthy agreement on the data set will ensure that the decisions taken by the annotators during building UPB and thereafter the annotations on arguments of verb predicates are consistent and reliable. We measured the Inter-annotator agreement using Fleiss’ kappa (Randolph, 2005) which is the frequently used agreement coefficient for annotation tasks on categorical data. Kappa was introduced by (Carletta, 1996) and since then many linguistics resources have been evaluated by the coefficient. The kappa statistics in this section show the agreement between the annotators on a given data-set and the compatibility and consistency with which the annotations have been performed on predicate-argument structures by the two annotators. These measures also demonstrate the conformity in their understanding of the PropBank annotation guidelines. The Fleiss’ kappa is calculated as: k= P r(a) − P r(e) 1 − P r(e) (9) The factor 1−P r(e) estimates the degree of agreement that is attainable above chance, and P r(a)−"
L16-1377,choi-etal-2010-propbank-instance,1,0.801884,"served between the annotators. In this sentence, snaan karta is a complex predicate because there is a nominal element snaan present along with the verb kar. Simple predicates are those which only have a light verb present in them. For example (2) (Shoeb ne) (Saba ko) (kitaab) (di). Shoeb ERG Saba DAT kitaab give. Shoeb gave book to Saba. 5. 5.1. Annotating the Urdu PropBank Following the porting and adaptation of predicate frames from Hindi and Arabic PropBanks, the annotation process for marking the predicate argument structures for each verb instance commenced. The Jubilee annotation tool (Choi et al., 2010) was used for the annotation of the Urdu PropBank. Some changes were made in the tool to take into account the dependency trees of Urdu sentences and to make them available for annotators as a reference. The UPB currently consists of 20 labels including both numbered arguments and modifiers (Table 1). Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX ARGM-ADV ARGM-DIR ARGM-EXT ARGM-MNR ARGM-PRP ARGM-DIS ARGM-LOC ARGM-MNS ARGM-NEG ARGM-TMP ARGM-CAU Challenges and redundancies The challenges which the two annotators encountered are described below. We also discuss the approa"
L16-1377,duran-aluisio-2012-propbank,0,0.322353,"Missing"
L16-1377,P09-1033,0,0.0692065,"Missing"
L16-1377,W07-1513,0,0.0259067,"for several European and Asian languages. The first PropBank, English PropBank was built on top of the phrase structure trees in Penn Treebank and adds a semantic layer to its syntactic structures (Palmer et al., 2005). Among European languages, work on PropBank has been reported in French, German, Dutch and Portuguese languages. Merlo and Van Der Plas (2009) showed the cross-lingual validity of PropBank by applying the annotation scheme developed for English on a large portion of French sentences. Van Der Plas et al. (2010) used English FrameNet to build a corpus of German PropBank manually. Monachesi et al. (2007) used PropBank semantic labels for semi-automatic annotation of a corpus of Dutch. Duran and Alu´ısio (2012) talk about the annotation of a Brazilian Portuguese Treebank with semantic role labels based on Propbank guidelines. As far as Asian languages are concerned, PropBanks exist for Chinese, Korean and Arabic. A PropBank for Korean 2379 language is being built which treats each verb and adjective in the Korean Treebank as a semantic predicate and annotation is done to mark the arguments and adjuncts of the predicate (Palmer et al., 2006). For Arabic, a Semitic language, Palmer et al. (2010)"
L16-1377,J05-1004,1,0.348001,"on 5 illustrates the challenges pertaining to Urdu Propbank annotation. In section 6, we report the Inter-annotator Agreement between the annotators. In section 7, we present several experiments which we did on the Urdu Propbank and their results. We conclude the paper in Section 8 and give future directions in section 9. 2. Related Work and comparison with other PropBanks PropBanks are being built for several European and Asian languages. The first PropBank, English PropBank was built on top of the phrase structure trees in Penn Treebank and adds a semantic layer to its syntactic structures (Palmer et al., 2005). Among European languages, work on PropBank has been reported in French, German, Dutch and Portuguese languages. Merlo and Van Der Plas (2009) showed the cross-lingual validity of PropBank by applying the annotation scheme developed for English on a large portion of French sentences. Van Der Plas et al. (2010) used English FrameNet to build a corpus of German PropBank manually. Monachesi et al. (2007) used PropBank semantic labels for semi-automatic annotation of a corpus of Dutch. Duran and Alu´ısio (2012) talk about the annotation of a Brazilian Portuguese Treebank with semantic role labels"
L16-1377,singh-ambati-2010-integrated,0,0.0150235,"ependency annotation. The inter-chunk dependencies are marked leaving the dependencies between words in a chunk unspecified for the intra-chunk dependencies. PropBanking is the next step in this generic pipeline which is aimed at establishing another layer of semantics on the Urdu Treebank. As part of the overall effort, a PropBank for Hindi is also built thus adding a semantic layer to the Hindi Treebank. The Urdu Dependency Treebank is developed following this Treebanking pipeline for the newspaper articles using a team of linguistics annotators. The tool used for the annotation is Sanchay (Singh and Ambati, 2010). All the annotations are represented in Shakti Standard Format (SSF). So far, ∼7,000 sentences (around 200K words) have been annotated with dependency structure. Each sentence contains an average of 29 words and an average of 13.7 chunks of average length 2.0. 4. Urdu PropBank Development Process In this section, we describe the development of Urdu PropBank. In general, PropBank annotation is a 2-step process. The initial step involves the creation of frame files. The subsequent step in this process is the actual annotation of predicate-argument structure using the frame files. In case of Urd"
L16-1377,W11-0403,1,0.884415,"d Arabic proposition bank (APB) in which predicates are identified with their relevant arguments and adjuncts in Arabic texts and an automatic process was put in place to map existing annotation to the new trees. Xue (2008) shows the use of diathesis alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency re"
L16-1377,W10-1814,0,0.563171,"Missing"
L16-1377,J08-2004,0,0.0371245,"r as Asian languages are concerned, PropBanks exist for Chinese, Korean and Arabic. A PropBank for Korean 2379 language is being built which treats each verb and adjective in the Korean Treebank as a semantic predicate and annotation is done to mark the arguments and adjuncts of the predicate (Palmer et al., 2006). For Arabic, a Semitic language, Palmer et al. (2010) presents a revised Arabic proposition bank (APB) in which predicates are identified with their relevant arguments and adjuncts in Arabic texts and an automatic process was put in place to map existing annotation to the new trees. Xue (2008) shows the use of diathesis alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top o"
L16-1377,W10-1836,1,0.92849,"Missing"
L16-1409,E14-1060,0,0.0575171,"Missing"
L16-1409,Y10-1077,0,0.201912,"ملڪmulk ‘country’ demonstrating how badly encoded text is dealt with. The left side is the output and the right side is the input. Note that the [ ڪk] character also has initial, medial and final forms, but these are produced with a separate code point, ( ـU+0640). ble 2, also drawing a comparison with GF lexicon. There are total 72 paradigms in our analyser right now. for masculine nouns. 4. Developing the morphological analyser 4.1. Orthographic issues We initiated our work on developing Sindhi morphological analyser with the help of three resources. The first one was an article by (Rahman and Bhatti, 2010), which described how Sindhi nouns inflect. This aided us in creating our first few paradigms for nouns. Along with noun paradigms we also created paradigms for some closed categories, such as, prepositions, conjunctions, and open categories of adjectives and adverbs. The second resource was Sindhi GF library. It helped us in verifying some of the paradigms that we had already defined. It was also helpful in adding verb and pronoun paradigms and improve the paradigms for nouns. The third resource was a corpus, a collection of articles from Sindhi Wikipedia. Then, we used our knowledge of Sindh"
L16-1409,O12-1028,0,0.0415948,"Missing"
L16-1727,I08-2099,1,0.784758,"Treebank Indian Languages are very morphologically rich languages and Treebanks for Hindi and Urdu are available which include syntactico-semantic information in the form of dependency annotations as well as lexical semantic information in the form of predicate-argument structures, thus forming PropBanks. Hindi and Urdu PropBanks are part of a multi-dimensional and multi-layered resource creation effort for the Hindi-Urdu language (Bhatt et al., 2009). The Hindi Dependency Treebank (HDT) and Urdu Dependency Treebank (UDT) are built following the CPG (Computational Paninian Grammar) framework (Begum et al., 2008). PropBank establishes a layer of semantic representation in the Treebank which is already annotated with Dependency labels or phrase structures (as in Penn Treebank ARGM-ADV ARGM-DIR ARGM-EXT ARGM-MNR ARGM-PRP ARGM-DIS ARGM-LOC ARGM-MNS ARGM-NEG ARGM-TMP ARGM-CAU Description Agent, Experiencer or doer Patient or Theme Beneficiary Instrument Attribute or Quality Physical Location Goal Source noun-verb construction Adverb Direction Extent or Comparision Manner Purpose Discourse Abstract Location Means Negation Time Cause or Reason Table 2: Hindi and Urdu PropBank labels with definitions 6. Sema"
L16-1727,W09-3036,1,0.812319,"et al. (2005) used joint learning amd joint modeling of argument frames of verbs to improve the overall accuracy of semantic role labeler. 5. The PropBank and The Treebank Indian Languages are very morphologically rich languages and Treebanks for Hindi and Urdu are available which include syntactico-semantic information in the form of dependency annotations as well as lexical semantic information in the form of predicate-argument structures, thus forming PropBanks. Hindi and Urdu PropBanks are part of a multi-dimensional and multi-layered resource creation effort for the Hindi-Urdu language (Bhatt et al., 2009). The Hindi Dependency Treebank (HDT) and Urdu Dependency Treebank (UDT) are built following the CPG (Computational Paninian Grammar) framework (Begum et al., 2008). PropBank establishes a layer of semantic representation in the Treebank which is already annotated with Dependency labels or phrase structures (as in Penn Treebank ARGM-ADV ARGM-DIR ARGM-EXT ARGM-MNR ARGM-PRP ARGM-DIS ARGM-LOC ARGM-MNS ARGM-NEG ARGM-TMP ARGM-CAU Description Agent, Experiencer or doer Patient or Theme Beneficiary Instrument Attribute or Quality Physical Location Goal Source noun-verb construction Adverb Direction E"
L16-1727,W03-1006,0,0.0609722,"he former are defined on a verb-by-verb basis. Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX Table 1: Hindi and Urdu resources statistics 4. Related Work The last decade has seen an exhaustive research in semantic parsing for different languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for Chinese by showing that verb classes, induced from the predicateargument information in the frame files helps in sema"
L16-1727,W05-0622,0,0.0428884,"rent languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for Chinese by showing that verb classes, induced from the predicateargument information in the frame files helps in semantic role labeling. Johansson and Nugues (2006) came up with a FrameNet-based semantic role labeling system for Swedish text. Toutanova et al. (2005) used joint learning amd joint modeling of argument frames of verbs to improve the overall accuracy of seman"
L16-1727,W03-1008,0,0.0359212,"c labels are closely associated with the karaka relations (Vaidya et al., 2011) (described later) in their structure though the former are defined on a verb-by-verb basis. Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX Table 1: Hindi and Urdu resources statistics 4. Related Work The last decade has seen an exhaustive research in semantic parsing for different languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for"
L16-1727,J02-3001,0,0.537571,"adjuncts are realized can vary based on the senses. Table 2 shows the PropBank labels for Hindi and Urdu and these labels are also used for building the Semantic Role labeler. Propbank labels or semantic labels are closely associated with the karaka relations (Vaidya et al., 2011) (described later) in their structure though the former are defined on a verb-by-verb basis. Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX Table 1: Hindi and Urdu resources statistics 4. Related Work The last decade has seen an exhaustive research in semantic parsing for different languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditiona"
L16-1727,P06-2057,0,0.0348234,"e classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for Chinese by showing that verb classes, induced from the predicateargument information in the frame files helps in semantic role labeling. Johansson and Nugues (2006) came up with a FrameNet-based semantic role labeling system for Swedish text. Toutanova et al. (2005) used joint learning amd joint modeling of argument frames of verbs to improve the overall accuracy of semantic role labeler. 5. The PropBank and The Treebank Indian Languages are very morphologically rich languages and Treebanks for Hindi and Urdu are available which include syntactico-semantic information in the form of dependency annotations as well as lexical semantic information in the form of predicate-argument structures, thus forming PropBanks. Hindi and Urdu PropBanks are part of a mu"
L16-1727,nivre-etal-2006-maltparser,0,0.0321944,"+NEs 85 84 84 58 42 49 Using Automatic Parses So far, we have shown and discussed results using handcorrected parses of the Hindi and Urdu PropBank. However, if we are building a NLP application, the SRL will have to extract features from automatic parses of the sentences. Also, after getting the insights of the influence of hand-corrected dependency relations, we wanted to analyse the same using automatic parses and evaluate our semantic role labeler on it. These automatic parses are generated by the parser of respective language. To come up with such an evaluation, we used the Malt Parser (Nivre et al., 2006) on the same test data described above for both languages. Malt Parser requires data in CONLL format. By using state-of-art Urdu Parser (Bhat et al., 2012), we extracted the automatic parses from the Urdu test data and used them as feature for our identification and classification tasks. Parsing accuracy is measured by LAS, UAS and LA which stands for Labeled Attachment score, Unlabeled Attachment score and Label Accuracy respectively. LAS is the percentage of words that are assigned the correct head and dependency label; UAS is the percentage of words with the correct head, and the label accu"
L16-1727,N04-1030,0,0.453204,"Missing"
L16-1727,P03-1002,0,0.0625896,"hese labels are also used for building the Semantic Role labeler. Propbank labels or semantic labels are closely associated with the karaka relations (Vaidya et al., 2011) (described later) in their structure though the former are defined on a verb-by-verb basis. Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX Table 1: Hindi and Urdu resources statistics 4. Related Work The last decade has seen an exhaustive research in semantic parsing for different languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with differen"
L16-1727,P05-1073,0,0.0448081,"TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for Chinese by showing that verb classes, induced from the predicateargument information in the frame files helps in semantic role labeling. Johansson and Nugues (2006) came up with a FrameNet-based semantic role labeling system for Swedish text. Toutanova et al. (2005) used joint learning amd joint modeling of argument frames of verbs to improve the overall accuracy of semantic role labeler. 5. The PropBank and The Treebank Indian Languages are very morphologically rich languages and Treebanks for Hindi and Urdu are available which include syntactico-semantic information in the form of dependency annotations as well as lexical semantic information in the form of predicate-argument structures, thus forming PropBanks. Hindi and Urdu PropBanks are part of a multi-dimensional and multi-layered resource creation effort for the Hindi-Urdu language (Bhatt et al.,"
L16-1727,W11-0403,0,0.302539,"ns ˜ 200,000 ˜ 350,000 ˜ 180,000 ˜ 300,000 Sentences ˜ 8,000 ˜ 14,000 ˜ 7,000 ˜ 11,000 Predicates ˜ 2,200 ˜ 3,200 pbrel ˜ 7,000 ˜ 12,000 for English). Capturing the semantics through predicateargument structure involves quite a few challenges peculiar to each predicate type because the syntactic notions in which the verb’s arguments and adjuncts are realized can vary based on the senses. Table 2 shows the PropBank labels for Hindi and Urdu and these labels are also used for building the Semantic Role labeler. Propbank labels or semantic labels are closely associated with the karaka relations (Vaidya et al., 2011) (described later) in their structure though the former are defined on a verb-by-verb basis. Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX Table 1: Hindi and Urdu resources statistics 4. Related Work The last decade has seen an exhaustive research in semantic parsing for different languages. Gildea and Jurafsky (2002) started out the work on semantic role labeling on 2001 release of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar"
L16-1727,W04-3212,0,0.779567,"ease of English Propbank. Surdeanu et al. (2003) build a decision tree classifier for predicting the semantic labels. Gildea and Hockenmaier (2003) uses features from Combinatory Categorial Grammar (CCG) which is a form of dependency grammar. Chen and Rambow (2003) used a decision tree classifier and additional syntactic and semantic representations extracted from Tree Adjoining Grammar (TAG). Swier and Stevenson (2004) talks of a novel bootstrapping algorithm for identifying semantic labels. (Cohn and Blunsom, 2005) applied conditional random fields (CRFs) to the semantic role labeling task. Xue and Palmer (2004) experimented with different linguistic features related to role labeling task. Xue and Palmer (2005) built a role labeler for Chinese by showing that verb classes, induced from the predicateargument information in the frame files helps in semantic role labeling. Johansson and Nugues (2006) came up with a FrameNet-based semantic role labeling system for Swedish text. Toutanova et al. (2005) used joint learning amd joint modeling of argument frames of verbs to improve the overall accuracy of semantic role labeler. 5. The PropBank and The Treebank Indian Languages are very morphologically rich l"
L18-1048,W98-0707,0,0.418227,"Missing"
L18-1048,D08-1104,0,0.0401838,"ement in performance on conducting baseline experiments for the above mentioned tasks. 2. Related Work One of the earliest known work in idiom handling is a comparative study (Volk, 1998) between two contemporary translation systems, namely machine translation and translation memory systems. The study concluded that neither of the systems could handle idioms, and proposed a method of integrating both the systems along with idiom databases to form a phrase archive, which could then be recognised more efficiently by the translation systems. A popular idiom corpus was the one built for Japanese (Hashimoto and Kawahara, 2008). This resource contains Japanese phrases labelled as either idiomatic or literal, which helps to better understand the semantics of the sentence. Another well known work on idiom handling is a system to identify idiomatic expressions from a large bilingual English-Korean corpus, using phrasal alignments to make sense of phrases as well as words, instead of the previously explored word alignment method that purely made sense of words alone (Lee et al., 2010). (Post et al., 2012) crowdsourced a parallel corpus between English and six Indian languages namely: Bengali, Hindi, Malayalam, Tamil, Te"
L18-1048,ide-suderman-2004-american,0,0.029706,"hindering their effectiveness (Liu, 2003). We strive to create a multilingual parallel idiom dataset that covers the most commonly used idioms in everyday English, so that it can be used effectively for different NLP applications. We crawled the web through relevant websites to extract over 5000 idioms, their respective meanings, and their sample usages2 . The list of the websites crawled 2 through is provided here 3 . We then perform an intersection of the list of idioms obtained with those compiled from other well known American English corpora, including the American National Corpus (ANC) (Ide and Suderman, 2004); Michigan Corpus of Academic Spoken English (MICASE) (Simpson et al., 2002), and Brown Corpus (Francis and Kucera, 1979). The compilation of idioms from the above mentioned corpora was done using the method proposed by (Muzny and Zettlemoyer, 2013). A threshold count of 25 was set for eliminating non-frequent idioms after performing the intersection. The intersection and elimination was done for the removal of inaccurate programmatic detections but at the same time ensuring that it is a frequently occurring idiom. We were then able to filter out and consolidate a list of 2208 most commonly us"
L18-1048,jha-2010-tdil,0,0.0222507,"4.1. Machine Translation Statistical Machine Translation (SMT) (Koehn, 2009) and Neural Machine Translation (NMT) ((Sutskever et al., 2014), (Cho et al., 2014), (Bahdanau et al., 2014)) are the two major MT paradigms today which require large parallel corpora for training. Such corpora containing sufficient idiomatic sentences are not available for Indian languages. We employ IM IL and conduct experiments to analyse MT quality when the system is fed with an idiom mapping in addition to the parallel training corpora. We employ the multilingual Indian Language Corpora Initiative (ILCI) corpus (Jha, 2010) for training 4 . It contains 50,000 sentences from the health and tourism domains aligned across eleven Indian languages. We choose three Indo-Aryan languages (Hindi, Bengali, Urdu) and one Dravidian language (Telugu) as candidate languages for our experiments to maintain brevity. We employ preprocessing to eliminate misalignments - the resultant dataset has a size of 47,382 sentences (Training - 44000, Validation - 1382, Test - 2000). We create 250 manually annotated sentences with idiomatic usages, of which 50 are appended to the validation set, and 200 to the test set. The resultant size o"
L18-1048,E12-1014,0,0.0238875,"ositional 0 0 0 Actual + ++ - Table 5: Examples of non-compositionality of sentiments in idioms idiom. Idioms have to be matched for their syntactic compatibility while translating them. Secondly, there are components in idioms which are determined by the other tokens outside the idiom. e.g. ‘worth one’s salt’ is realized as ‘worth his salt’, ‘worth her salt’ and so on agreeing with the subject. These changes should be accommodated in the target language as well. As part of future work, the automatic generation of the bidirectional lexical and phrasal translation probabilities as proposed by (Klementiev et al., 2012) can be explored along with the feature addition in the phrase table for further improvement in performance for languages where large monolingual corpora are available. This could facilitate the coverage of words and phrases surrounding the idiom by the the decoder in addition to the idiom itself. 4.2. Sentiment Analysis This is one of the most interesting applications of the database due the non-compositional behavior of idioms in terms of semantic as well as sentiment information. Table 4 gives some of such examples, motivating the need for a sentiment-annotated idiom database. Model Stanf o"
L18-1048,P07-2045,0,0.0078883,"Missing"
L18-1048,P09-5002,0,0.0121858,"primarily BeautifulSoup4. 3 320 https://goo.gl/s4R4uH 2. In case it is neither possible to find an appropriate idiom nor an equivalent phrasal translation, “Skip” has to be entered in the target slot. 3. The sentiment of each idiom should be marked at each node in its parsed tree structure (detailed in Section 4.2.). The annotation scheme along with the statistics for each sentiment is given in Table 2. 4. Experiments and Results We demonstrate the application of our dataset by conducting experiments for two different NLP tasks: 4.1. Machine Translation Statistical Machine Translation (SMT) (Koehn, 2009) and Neural Machine Translation (NMT) ((Sutskever et al., 2014), (Cho et al., 2014), (Bahdanau et al., 2014)) are the two major MT paradigms today which require large parallel corpora for training. Such corpora containing sufficient idiomatic sentences are not available for Indian languages. We employ IM IL and conduct experiments to analyse MT quality when the system is fed with an idiom mapping in addition to the parallel training corpora. We employ the multilingual Indian Language Corpora Initiative (ILCI) corpus (Jha, 2010) for training 4 . It contains 50,000 sentences from the health and"
L18-1048,kunchukuttan-etal-2014-shata,0,0.0466827,"Missing"
L18-1048,N16-1040,0,0.0203577,"-) Somewhat negative (-) Neutral (0) Somewhat positive (+) Very positive (++) Total 196 657 726 503 126 2208 Table 2: Dataset Sentiment Annotation Statistics of our to Indian languages. A prominent work was the detection of MWEs for Hindi language, mainly noun compounds and noun+verb compounds, using Word Embeddings and WordNet-based features (Patel and Bhattacharyya, 2015). Another important work was the annotation of MWEs for Hindi and Marathi, and classifying them into either compound nouns or light verb constructions (Singh et al., 2016). A very recent work on the topic of idiom handling (Liu and Hwa, 2016) is a system that implements a phrasal substitution by replacing idioms with their corresponding meanings and transforming the meanings to fit the context of the sentence with the right conjugation. So far, there has been no significant work done on creation of a multilingual idiom dataset. To the best of our knowledge, our resource is the first of its kind for Indian languages. 3. Creation of IMIL 3.1. Data Collection A significant number of idioms in reference materials are ones that are seldom used, thereby hindering their effectiveness (Liu, 2003). We strive to create a multilingual parall"
L18-1048,D13-1145,0,0.0292999,"e web through relevant websites to extract over 5000 idioms, their respective meanings, and their sample usages2 . The list of the websites crawled 2 through is provided here 3 . We then perform an intersection of the list of idioms obtained with those compiled from other well known American English corpora, including the American National Corpus (ANC) (Ide and Suderman, 2004); Michigan Corpus of Academic Spoken English (MICASE) (Simpson et al., 2002), and Brown Corpus (Francis and Kucera, 1979). The compilation of idioms from the above mentioned corpora was done using the method proposed by (Muzny and Zettlemoyer, 2013). A threshold count of 25 was set for eliminating non-frequent idioms after performing the intersection. The intersection and elimination was done for the removal of inaccurate programmatic detections but at the same time ensuring that it is a frequently occurring idiom. We were then able to filter out and consolidate a list of 2208 most commonly used idioms, thus rendering the application of the corpus as close to natural human language as possible. 3.2. Annotation Guidelines We create a parallel idiom dataset for seven Indian languages in addition to English. The English idioms extracted in"
L18-1048,W15-5943,0,0.0189801,"d: GEN - Genitive case, LOC - Locative case, DAT - Dative case, PASS - Passive voice, MASC - Masculine gender, PRES - Present tense, PST - Past tense, INF - Infinitive form of a verb, PUR - Purpose of an action Category Number of Idioms Very negative (- -) Somewhat negative (-) Neutral (0) Somewhat positive (+) Very positive (++) Total 196 657 726 503 126 2208 Table 2: Dataset Sentiment Annotation Statistics of our to Indian languages. A prominent work was the detection of MWEs for Hindi language, mainly noun compounds and noun+verb compounds, using Word Embeddings and WordNet-based features (Patel and Bhattacharyya, 2015). Another important work was the annotation of MWEs for Hindi and Marathi, and classifying them into either compound nouns or light verb constructions (Singh et al., 2016). A very recent work on the topic of idiom handling (Liu and Hwa, 2016) is a system that implements a phrasal substitution by replacing idioms with their corresponding meanings and transforming the meanings to fit the context of the sentence with the right conjugation. So far, there has been no significant work done on creation of a multilingual idiom dataset. To the best of our knowledge, our resource is the first of its kin"
L18-1048,W12-3152,0,0.183637,"standard Statistical Machine Translation (SMT) system tends to achieve only about half the BLEU score of the same system when applied to sentences containing idioms, as compared to those that do not. Since idioms encode a very specific kind of linguistic knowledge, it is not easy to learn their automatic handling computationally, without an idiom database. This makes idiom handling a challenging problem for various NLP subtasks including sentiment analysis and question answering in addition to MT. The situation is far worse for Indian languages, a majority of which are low resource languages (Post et al., 2012) with regard to the availability of NLP tools, and yet representing 1.3 billion native speakers. Moreover, these languages are under-studied, while also exhibiting linguistic properties that make idiom handling for various NLP subtasks even more challenging. In this paper, we present a multilingual parallel dataset that maps 2208 commonly used idioms in English to their translations in seven Indian languages: Hindi, Urdu, Bengali, Tamil, Gujarati, Malayalam and Telugu1 . The idioms are also annotated with the appropriate sentiments that they channel, and their meanings in the respective langua"
L18-1048,W14-1007,0,0.106626,"ndian languages in addition to English and demonstrates its usefulness for two NLP applications - Machine Translation and Sentiment Analysis. We observe significant improvement for both the subtasks over baseline models trained without employing the idiom dataset. Keywords: Idioms, Machine Translation, Sentiment Analysis, Indian Languages 1. Introduction Idioms pose a problem to most NLP applications (Sag et al., 2002), including sentiment analysis, question answering, machine translation, parsing and so on. One of the most negatively affected subtasks among these is Machine Translation (MT) (Salton et al., 2014a). While parallel corpora can be used by MT systems to learn the language constructs, thereby generating decent translations from source to target language; the same cannot be said for the learning of idioms. Most machine translation systems existent today fail when it comes to the handling of idioms (Table 1). Past research (Salton et al., 2014a) has come up with results stating that a standard Statistical Machine Translation (SMT) system tends to achieve only about half the BLEU score of the same system when applied to sentences containing idioms, as compared to those that do not. Since idi"
L18-1048,W14-0806,0,0.117511,"ndian languages in addition to English and demonstrates its usefulness for two NLP applications - Machine Translation and Sentiment Analysis. We observe significant improvement for both the subtasks over baseline models trained without employing the idiom dataset. Keywords: Idioms, Machine Translation, Sentiment Analysis, Indian Languages 1. Introduction Idioms pose a problem to most NLP applications (Sag et al., 2002), including sentiment analysis, question answering, machine translation, parsing and so on. One of the most negatively affected subtasks among these is Machine Translation (MT) (Salton et al., 2014a). While parallel corpora can be used by MT systems to learn the language constructs, thereby generating decent translations from source to target language; the same cannot be said for the learning of idioms. Most machine translation systems existent today fail when it comes to the handling of idioms (Table 1). Past research (Salton et al., 2014a) has come up with results stating that a standard Statistical Machine Translation (SMT) system tends to achieve only about half the BLEU score of the same system when applied to sentences containing idioms, as compared to those that do not. Since idi"
L18-1048,L16-1369,0,0.0146887,", PUR - Purpose of an action Category Number of Idioms Very negative (- -) Somewhat negative (-) Neutral (0) Somewhat positive (+) Very positive (++) Total 196 657 726 503 126 2208 Table 2: Dataset Sentiment Annotation Statistics of our to Indian languages. A prominent work was the detection of MWEs for Hindi language, mainly noun compounds and noun+verb compounds, using Word Embeddings and WordNet-based features (Patel and Bhattacharyya, 2015). Another important work was the annotation of MWEs for Hindi and Marathi, and classifying them into either compound nouns or light verb constructions (Singh et al., 2016). A very recent work on the topic of idiom handling (Liu and Hwa, 2016) is a system that implements a phrasal substitution by replacing idioms with their corresponding meanings and transforming the meanings to fit the context of the sentence with the right conjugation. So far, there has been no significant work done on creation of a multilingual idiom dataset. To the best of our knowledge, our resource is the first of its kind for Indian languages. 3. Creation of IMIL 3.1. Data Collection A significant number of idioms in reference materials are ones that are seldom used, thereby hindering the"
L18-1048,D13-1170,0,0.00265888,"nal behavior of idioms in terms of semantic as well as sentiment information. Table 4 gives some of such examples, motivating the need for a sentiment-annotated idiom database. Model Stanf ordBase Stanf ordIM IL CALA 67.01 68.73 CARLA 70.23 73.56 Table 6: Sentiment Analysis results on T estConcat . CALA: Combined Approximate Label Accuracy. CARLA: Combined Approximate Root Label Accuracy. This is the primary motivating factor for the need of an idiom sentiment database like IM IL, which can help towards better Sentiment Analysis, especially the phrase-level approaches ((Wilson et al., 2005), (Socher et al., 2013)). IM IL can be employed for Sentiment Analysis for any candidate language among the languages in consideration. Due to space constraints, we demonstrate the application of IM IL to Sentiment Analysis for English, using Recursive Neural Tensor Networks (RNTNs) proposed by (Socher et al., 2013). The RNTNs can learn the phrase sentiments from a sentiment treebank containing trees with a sentiment annotated at each node in the parsed tree structure of a sentence. Our dataset is seamlessly integratable with the Stanford Sentiment Treebank, since the sentiment annotation scheme (mentioned in Sectio"
L18-1048,H05-1044,0,0.0565906,"due the non-compositional behavior of idioms in terms of semantic as well as sentiment information. Table 4 gives some of such examples, motivating the need for a sentiment-annotated idiom database. Model Stanf ordBase Stanf ordIM IL CALA 67.01 68.73 CARLA 70.23 73.56 Table 6: Sentiment Analysis results on T estConcat . CALA: Combined Approximate Label Accuracy. CARLA: Combined Approximate Root Label Accuracy. This is the primary motivating factor for the need of an idiom sentiment database like IM IL, which can help towards better Sentiment Analysis, especially the phrase-level approaches ((Wilson et al., 2005), (Socher et al., 2013)). IM IL can be employed for Sentiment Analysis for any candidate language among the languages in consideration. Due to space constraints, we demonstrate the application of IM IL to Sentiment Analysis for English, using Recursive Neural Tensor Networks (RNTNs) proposed by (Socher et al., 2013). The RNTNs can learn the phrase sentiments from a sentiment treebank containing trees with a sentiment annotated at each node in the parsed tree structure of a sentence. Our dataset is seamlessly integratable with the Stanford Sentiment Treebank, since the sentiment annotation sche"
L18-1048,2002.tmi-tutorials.2,0,0.167121,"Missing"
N10-1093,P09-3002,0,0.207239,"Missing"
N10-1093,H05-1066,0,0.0590908,"nce; nevertheless, the performance is sufficient enough to make an improvement in parsing experiments. On the other hand, the head of the clause (or, the root head in the clausal sub-tree) is identified efficiently. All the above experiments for parameter tuning were done on the development data of the ICON 2009 parsing contest. 3.3 Parser We used MSTParser7 for the actual parsing step. MST uses Chu-Liu-Edmonds Maximum Spanning Tree Algorithm for non-projective parsing and Eisner's algorithm for projective parsing (Eisner, 1996). It uses online large margin learning as the learning algorithm (McDonald et al., 2005b). We modified MST so that it uses the clause boundary. Unlike the normal features that MST uses, the clause boundary features span across many words. . 4 Experiments and Results We experimented with different combinations of the information provided in the data (as mentioned in 3.1). Vibhakti and TAM fields gave better results than others. This is consistent with the best previous settings for Hindi parsing (Bharati et al., 2008a, Ambati et al., 2009). We used the results obtained using this setting as our baseline (F1). We first experimented by giving only the clause inclusion (boundary) in"
N10-1093,R09-2001,1,0.883872,"Missing"
N10-1093,P05-1012,0,0.0832832,"nce; nevertheless, the performance is sufficient enough to make an improvement in parsing experiments. On the other hand, the head of the clause (or, the root head in the clausal sub-tree) is identified efficiently. All the above experiments for parameter tuning were done on the development data of the ICON 2009 parsing contest. 3.3 Parser We used MSTParser7 for the actual parsing step. MST uses Chu-Liu-Edmonds Maximum Spanning Tree Algorithm for non-projective parsing and Eisner's algorithm for projective parsing (Eisner, 1996). It uses online large margin learning as the learning algorithm (McDonald et al., 2005b). We modified MST so that it uses the clause boundary. Unlike the normal features that MST uses, the clause boundary features span across many words. . 4 Experiments and Results We experimented with different combinations of the information provided in the data (as mentioned in 3.1). Vibhakti and TAM fields gave better results than others. This is consistent with the best previous settings for Hindi parsing (Bharati et al., 2008a, Ambati et al., 2009). We used the results obtained using this setting as our baseline (F1). We first experimented by giving only the clause inclusion (boundary) in"
N10-1093,I08-2099,1,0.880426,"ying the clauses. Interestingly, it has been shown recently that most of the non-projective cases in Hindi are interclausal (Mannem et al., 2009). Identifying clausal boundaries, therefore, should prove to be helpful in parsing non-projective structures. The same holds true for many long-distance dependencies. 3 3.1 Experimental Setup Dataset The experiments reported in this paper have been done on Hindi; the data was released as part of the ICON 2009 parsing contest (Husain, 2009). The sentences used for this contest are subset of the Hyderabad Dependency Treebank (HyDT) developed for Hindi (Begum et al., 2008). The dependency relations in the treebank are syntacticosemantic. The dependency tagset in the annotation scheme has around 28 relations. The dependency trees in the treebank show relations between chunk heads. Note, therefore, that the experiments and results described in this paper are based on parse trees that have chunk head as nodes. The data provided in the task contained morphological features along with the lemma, POS tag, and coarse POS tag, for each word. These are six morphological features namely category, gender, 658 3.2 Clause Boundary Identifier We used the Stage1 5 parser of H"
N10-1093,P93-1015,1,0.830658,"Missing"
N10-1093,D07-1097,0,\N,Missing
N10-1093,W09-3812,1,\N,Missing
N10-1093,D07-1096,0,\N,Missing
N16-1159,W14-3914,0,0.126279,"Missing"
N16-1159,W14-3902,0,0.286734,"alizer. The POS tagger uses the output of the normalizer to assign each word a POS tag. Finally, the Shallow Parser assigns a chunk label with boundary. The functionality and performance of each module is described in greater detail in the following subsections. 4.1 While language identification at the document level is a well-established task (McNamee, 2005), identifying language in social media posts has certain challenges associated to it. Spelling errors, phonetic typing, use of transliterated alphabets and abbreviations combined with code-mixing make this problem interesting. Similar to (Barman et al., 2014), we performed two experiments treating language identification as a three class (‘hi’, ‘en’, ‘rest’) classification problem. The feature set comprised of BNC: normalized frequency of the word in British National Corpus (BNC)3 . LEXNORM: binary feature indicating presence of the word in the lexical normalization dataset released by Han et al. (2011). HINDI DICT: binary feature indicating presence of the word in a dictionary of 30,823 transliterated Hindi words as released by Gupta (2012). NGRAM: word n-grams. AFFIXES: prefixes and suffixes of the word. Using these features and introducing a co"
N16-1159,P11-2008,0,0.0243967,"Missing"
N16-1159,gupta-etal-2012-mining,0,0.0191516,"d below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69.27 70.44 72.61 73.18 73.55 75.07 Table 4: Feature Ablation for POS Tag"
N16-1159,P11-1038,0,0.0207668,"Missing"
N16-1159,D12-1039,0,0.0215036,"Missing"
N16-1159,R15-1033,0,0.122373,"Missing"
N16-1159,W01-0706,0,0.0975929,"ual speakers. Two other annotators reviewed and cleaned it. To measure interannotator agreement, another annotator read the guidelines and annotated 25 sentences (334 tokens) from scratch. The inter-annotator agreement calculated using Cohen’s κ (Cohen, 1960) came out to be 0.97, 0.83 and 0.89 for language identification, POS tagging and shallow parsing respectively. 4 Shallow Parsing Pipeline Shallow parsing is the task of identifying and segmenting text into syntactically correlated word groups (Abney, 1992; Harris, 1957). Shallow parsing is a viable alternative to full parsing as shown by (Li and Roth, 2001). Our shallow parsing pipeline is composed of four main modules, as shown in Figure 1. These modules, in the order of their usage, are Language Identification, Normalization, POS Tagger and Shallow Parser. Our pipeline takes a raw utterance in Roman script as input on which each module runs sequentially. Twokenizer2 (Owoputi et al., 2013) which 2 performs well on Hindi-English CSMT (Jamatia et al., 2015) was used to tokenize the utterance into words. The Language Identification module assigns each token a language label. Based on the language label assigned, the Normalizer runs the Hindi norma"
N16-1159,P12-1109,0,0.0291547,"Missing"
N16-1159,J03-1002,0,0.0453549,"i and other for English/Rest, had two subnormalizers each, as described below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69"
N16-1159,N13-1039,0,0.0428979,"Missing"
N16-1159,petrov-etal-2012-universal,0,0.0251756,"Missing"
N16-1159,D08-1110,0,0.0483601,"Missing"
N16-1159,D14-1105,0,0.227709,"xt analysis of Hindi English CSMT. The pipeline is accessible at 1 . 1 2 Introduction Multilingual speakers tend to exhibit code-mixing and code-switching in their use of language on social media platforms. Code-Mixing is the embedding of linguistic units such as phrases, words or morphemes of one language into an utterance of another language whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systems (Gumperz., 1982). Here we use code-mixing to refer to both the scenarios. Hindi-English bilingual speakers produce huge amounts of CSMT. Vyas et al. (2014) noted that the complexity in analyzing CSMT stems from nonadherence to a formal grammar, spelling variations, lack of annotated data, inherent conversational nature of the text and of course, code-mixing. Therefore, there is a need to create datasets and Natural 1 http://bit.ly/csmt-parser-api Background Bali et al. (2014) gathered data from Facebook generated by English-Hindi bilingual users which on analysis, showed a significant amount of codemixing. Barman et al. (2014) investigated language identification at word level on Bengali-HindiEnglish CSMT. They annotated a corpus with more than"
N16-2010,K15-2003,0,0.0329822,"Missing"
N16-2010,I08-2099,1,0.67482,") The Hindi Discourse Relation Bank(HDRB) was created broadly following the lines of Penn Discourse TreeBank(PDTB) (Miltsakaki et al., 2004; Prasad et al., 2008)’s lexically grounded approach along with a modified annotation workflow, additional grammatical categories for explicit connectives, semantically driven Arg1/Arg2 labelling and 66 Proceedings of NAACL-HLT 2016, pages 66–72, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics modified sense hierarchies.(Oza et al., 2009; Kolachina et al., 2012) HDRB was annotated on a subset of the Hindi TreeBank (Begum et al., 2008) which includes partof-speech, chunk and dependency parse tree annotations. HDRB contains 1865 sentences and a word count of 42K. Furthermore HDRB contains 650 explicit discourse relations and 1200 implicit discourse relations. In HDRB, one of the arguments occurs after the discourse connective and the other occurs before the connective. Discourse relations not adhering to this rarely occur in the corpus. However, due to the semantic labelling of Arg1 and Arg2, Arg2 does not always occur after the connective. For example: • cdFgY m  bh-pEtvAr kF sBh BArF vfA kF vяh s  isk  aAspAs aOr Enc"
N16-2010,W05-0305,0,0.100378,"sing Conditional random fields and n-best results. Lin et al. (2014) proposed a sub-tree extraction approach for argument identification. Firstly an argument position classifier was employed to decide the location of Arg1(PS/SS). In the case of PS, Arg1 was labelled as the entire preceding sentence. For tagging Arg1(SS) and Arg2, a argument node identifier was employed to decide which nodes were part of Arg1(SS) or Arg2. Next sub-tree extraction was used to extract Arg1(SS) and Arg2. However, since it is not necessary that arguments may be dominated entirely by a single node as pointed out by Dinesh et al. (2005), this method has inherent shortcomings. Kong et al. (2014) proposed a constituent based approach where, similar to Lin, an argument identifier is employed to decide which constituents are Arg1 and Arg2. Previous sentence was considered as a special constituent to handle Arg1(PS). A constituent pruner was also employed to reduce the number of candidate constituents considered for Arg1 and Arg2. In addition, Integer Linear Programming(ILP) with language specific constraints, was employed to ensure the argument identifier made legitimate global predictions. Approaches in English can be summed up"
N16-2010,I11-1120,0,0.05727,"ntence was considered as a special constituent to handle Arg1(PS). A constituent pruner was also employed to reduce the number of candidate constituents considered for Arg1 and Arg2. In addition, Integer Linear Programming(ILP) with language specific constraints, was employed to ensure the argument identifier made legitimate global predictions. Approaches in English can be summed up as two sub-tasks: (1) Considering the possible constituents/nodes/words to be identified as Arg1 or Arg2 by use of subtree extraction (Lin et al., 2014), constituent pruning (Kong et al., 2014) or simple baseline (Ghosh et al., 2011) approaches. (2) Classification of selected constituents/nodes/words as Arg1/Arg2/None by use of CRF(Ghosh et al., 2011) or classifier(Lin et al., 2014; Kong et al., 2014) based approaches. 4 A Hybrid Pipeline to Argument Identification We base our pipeline on the two sub tasks discussed in the previous section. We use a method similar to subtree extraction to extract possible candidates for Arg1/Arg2 and use CRF tagging to further refine the extent of the extracted arguments. We approach the task of Arg1 and Arg2 identification seperately since tagging Arg1 is inherently more difficult. We fi"
N16-2010,L16-1276,1,0.809739,"Missing"
N16-2010,kolachina-etal-2012-evaluation,1,0.845471,"d pipeline and we conclude in Section 6. 2 Hindi Discourse Relations Bank(HDRB) The Hindi Discourse Relation Bank(HDRB) was created broadly following the lines of Penn Discourse TreeBank(PDTB) (Miltsakaki et al., 2004; Prasad et al., 2008)’s lexically grounded approach along with a modified annotation workflow, additional grammatical categories for explicit connectives, semantically driven Arg1/Arg2 labelling and 66 Proceedings of NAACL-HLT 2016, pages 66–72, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics modified sense hierarchies.(Oza et al., 2009; Kolachina et al., 2012) HDRB was annotated on a subset of the Hindi TreeBank (Begum et al., 2008) which includes partof-speech, chunk and dependency parse tree annotations. HDRB contains 1865 sentences and a word count of 42K. Furthermore HDRB contains 650 explicit discourse relations and 1200 implicit discourse relations. In HDRB, one of the arguments occurs after the discourse connective and the other occurs before the connective. Discourse relations not adhering to this rarely occur in the corpus. However, due to the semantic labelling of Arg1 and Arg2, Arg2 does not always occur after the connective. For example"
N16-2010,D14-1008,0,0.0712363,"(2014) proposed a sub-tree extraction approach for argument identification. Firstly an argument position classifier was employed to decide the location of Arg1(PS/SS). In the case of PS, Arg1 was labelled as the entire preceding sentence. For tagging Arg1(SS) and Arg2, a argument node identifier was employed to decide which nodes were part of Arg1(SS) or Arg2. Next sub-tree extraction was used to extract Arg1(SS) and Arg2. However, since it is not necessary that arguments may be dominated entirely by a single node as pointed out by Dinesh et al. (2005), this method has inherent shortcomings. Kong et al. (2014) proposed a constituent based approach where, similar to Lin, an argument identifier is employed to decide which constituents are Arg1 and Arg2. Previous sentence was considered as a special constituent to handle Arg1(PS). A constituent pruner was also employed to reduce the number of candidate constituents considered for Arg1 and Arg2. In addition, Integer Linear Programming(ILP) with language specific constraints, was employed to ensure the argument identifier made legitimate global predictions. Approaches in English can be summed up as two sub-tasks: (1) Considering the possible constituent"
N16-2010,miltsakaki-etal-2004-penn,0,0.134762,"Missing"
N16-2010,W09-3029,1,0.92521,"nce of the proposed pipeline and we conclude in Section 6. 2 Hindi Discourse Relations Bank(HDRB) The Hindi Discourse Relation Bank(HDRB) was created broadly following the lines of Penn Discourse TreeBank(PDTB) (Miltsakaki et al., 2004; Prasad et al., 2008)’s lexically grounded approach along with a modified annotation workflow, additional grammatical categories for explicit connectives, semantically driven Arg1/Arg2 labelling and 66 Proceedings of NAACL-HLT 2016, pages 66–72, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics modified sense hierarchies.(Oza et al., 2009; Kolachina et al., 2012) HDRB was annotated on a subset of the Hindi TreeBank (Begum et al., 2008) which includes partof-speech, chunk and dependency parse tree annotations. HDRB contains 1865 sentences and a word count of 42K. Furthermore HDRB contains 650 explicit discourse relations and 1200 implicit discourse relations. In HDRB, one of the arguments occurs after the discourse connective and the other occurs before the connective. Discourse relations not adhering to this rarely occur in the corpus. However, due to the semantic labelling of Arg1 and Arg2, Arg2 does not always occur after th"
N16-2010,prasad-etal-2008-penn,0,0.351463,"Missing"
N16-2010,K15-2002,0,0.0560405,"ted shallow discourse parser would greatly aid in discourse analysis and improve the performance of Text summarization and Question answering systems. Given a text, a shallow discourse parser would identify discourse relations, consisting of two spans of text exhibiting some kind of relationship between each other. Discourse relations whose presence is marked explicitly by discourse connectives are called Explicit discourse relations and those which are not are called Implicit discourse relations. At present, complete shallow discourse parsers are only available for English (Lin et al., 2014; Wang and Lan, 2015; Ali and Bayer, 2015). The ongoing CoNLL 2016 shared task on Shallow Discourse Parsing has included Chinese as well. Work towards a complete shallow discourse parser in Hindi has also begun. Jain et al. (2016) reported state-of-theart results for discourse connective identification in Hindi. Our work focuses on the next part towards a shallow discourse parser for Hindi i.e. argument identification for Explicit discourse relations. In this paper, we discuss current approaches for this task and also propose a hybrid pipeline incorporating many of these approaches. We report high accuracies of 9"
N16-2014,D10-1047,0,0.0632496,"a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important"
N16-2014,P04-1051,0,0.045439,"ontext in the original corpus, placing it in the summary in a different context can render a wrong inference to the reader of the summary. The main intuition behind our approach begins with a crucial question about the linguistic nature of a text. Is text a bag of words every time? Psycholinguistic studies suggest that local coherence plays a vital role in inference formation while reading a text (McKoon and Ratcliff, 1992). Local coherence is undoubtedly necessary for global coherence and has received considerable attention in Computational Linguistics. ((Marcu, 2000), (Foltz et al., 1998), (Althaus et al., 2004), (Karamanis et al., 2004)). Linguistically, every sentence is uttered not in isolation but within a context in a given discourse. To make a coherent reading, sentences use various discourse connectives that bind one sentence with another. A set of such structurally related sentences forms a Locally Coherent Discourse Unit (hereafter referred to as LDU). In the current work, we suggest that it is important to leverage this structural coherence to improve the comprehensibility of the generated summary. It should be noted that the concept of LDU is different from the elementary discourse units ("
N16-2014,J08-1001,0,0.0185835,"endence score of individual sentences. Comprehensibility index for the generated summary is the average contextual independence score of a sentence in the summary. We verified, through human evaluators, whether the comprehensibility index is actually representative of the human comprehensibility. 2 Previous Work Identification of locally coherent discourse unit (LDU) and combining the information to create a comprehensible summary is a novel problem which is not attempted by any of the previous works in the field of natural language processing to the best of our knowledge. Barzilay and Lapata(Barzilay and Lapata, 2008) attempt to measure the global coherence in terms of local coherence which is measured in terms of entity role switch while GFlow(Christensen et al., 2013) came up with a metric to measure the coherence of the generated summary with respect to a corpus level discourse graph. Still, these two works are not directly relevant to local discourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998),"
N16-2014,P10-1084,0,0.0549254,"Missing"
N16-2014,N13-1136,0,0.137296,"omprehensibility. The problem 95 of extractive summarization can be formulated as a function maximization problem in the space of all candidate summaries as follows. X S ∗ ∈ argmaxS⊆V F (S)subject to ci <= b i∈S (1) where F is an objective function, is the summary which maximizes F with an adopted optimization method, S is a candidate summary, ci is the cost of selecting a sentence i into summary, b is the upper bound on the total cost and V is the set of total number of sentences in the corpus. The current work is inspired by two of the previous works namely (Lin and Bilmes, 2011) and GFlow (Christensen et al., 2013). Lin & Bilmes observed that if the objective function to score candidate summaries is monotone sub-modular, a greedy approach can ensure the approximation of the summary at the global maximum by a factor of 0.632 as follows. S∗ ˆ ≥ (1−1/e)∗F (Sopt ) ≈ 0.632∗F (Sopt ) (2) F (S) where Sˆ is the summary obtained from monotone sub-modular function F and Sopt is the summary at the global maximum of F. G-Flow aimed at generating coherent summaries by constructing a sentence-level discourse graph for the entire corpus and the information from the graph is utilized to quantify the coherence of candid"
N16-2014,C12-1056,0,0.0186027,"while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summ"
N16-2014,N09-1041,0,0.0360369,"ourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Con"
N16-2014,I11-1118,0,0.0550466,"Missing"
N16-2014,D11-1105,0,0.0585663,"Missing"
N16-2014,P11-1052,0,0.245467,"s of readability, coherence and comprehensibility. The problem 95 of extractive summarization can be formulated as a function maximization problem in the space of all candidate summaries as follows. X S ∗ ∈ argmaxS⊆V F (S)subject to ci <= b i∈S (1) where F is an objective function, is the summary which maximizes F with an adopted optimization method, S is a candidate summary, ci is the cost of selecting a sentence i into summary, b is the upper bound on the total cost and V is the set of total number of sentences in the corpus. The current work is inspired by two of the previous works namely (Lin and Bilmes, 2011) and GFlow (Christensen et al., 2013). Lin & Bilmes observed that if the objective function to score candidate summaries is monotone sub-modular, a greedy approach can ensure the approximation of the summary at the global maximum by a factor of 0.632 as follows. S∗ ˆ ≥ (1−1/e)∗F (Sopt ) ≈ 0.632∗F (Sopt ) (2) F (S) where Sˆ is the summary obtained from monotone sub-modular function F and Sopt is the summary at the global maximum of F. G-Flow aimed at generating coherent summaries by constructing a sentence-level discourse graph for the entire corpus and the information from the graph is utilize"
N16-2014,C10-1101,0,0.0805044,"Missing"
N16-2014,C10-1111,0,0.0233701,"tric to measure the coherence of the generated summary with respect to a corpus level discourse graph. Still, these two works are not directly relevant to local discourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summ"
N16-2014,E09-1089,0,0.375041,"2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally understood even when the sentences preceding/following it are not available"
N16-2014,C08-1124,0,0.0366908,"Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally understood even when the sentences preceding/following it are not available to the reader. Contextual dependence, signifies only the structural dependence of a sent"
N16-2014,D12-1022,0,0.0318867,"ll and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally underst"
N16-2014,P04-1050,0,\N,Missing
N16-3019,cassidy-etal-2014-alveo,0,0.199444,"Missing"
N16-3019,A97-2017,0,0.698901,"Missing"
N16-3019,ide-etal-2014-language,0,0.126315,"Missing"
N18-1090,E17-2052,1,0.811923,"uages making it much more diverse than any monolingual corpora (C¸etino˘glu et al., 2016). As the current computational models fail to cater to the complexities of CS data, there is often a need for dedicated techniques tailored to its specific characteristics. Given the peculiar nature of CS data, it has been widely studied in linguistics literature (Poplack, 1980; Gumperz, 1982; Myers-Scotton, 1995), and more recently, there has been a surge in studies concerning CS data in NLP as well (Solorio and Liu, 2008a,a; Vyas et al., 2014; Sharma et al., 2016; Rudra et al., 2016; Joshi et al., 2016; Bhat et al., 2017; Chandu et al., 2017; Rijhwani et al., Code-switching is a phenomenon of mixing grammatical structures of two or more languages under varied social constraints. The code-switching data differ so radically from the benchmark corpora used in NLP community that the application of standard technologies to these data degrades their performance sharply. Unlike standard corpora, these data often need to go through additional processes such as language identification, normalization and/or back-transliteration for their efficient processing. In this paper, we investigate these indispensable processes"
N18-1090,W16-5801,0,0.100771,"Missing"
N18-1090,K17-3002,0,0.0614427,"Missing"
N18-1090,D16-1250,0,0.0176928,"ta and a parsing model which leverages both data would significantly improve parsing performance. While a parsing model trained on our limited CS data might not be enough to accurately parse the individual grammatical fragments of Hindi and English, the preexisting Hindi and 992 English monolingual data contains around 280M sentences, while the Hindi data is comparatively smaller and contains around 40M sentences. The word representations are learned using Skip-gram model with negative sampling which is implemented in word2vec toolkit (Mikolov et al., 2013). We use the projection algorithm of Artetxe et al. (2016) to transform the Hindi and English monolingual embeddings into same semantic space using a bilingual lexicon (∼63,000 entries). The bilingual lexicon is extracted from ILCI and Bojar Hindi-English parallel corpora (Jha, 2010; Bojar et al., 2014). For normalization models, we use 32-dimensional character embeddings uniformly initialized within a range of [−0.1, +0.1]. model trained on augmented Hindi and English data. For tagging, we augment the input layer of the CS tagger with the MLP layer of the source tagger. For transferring parsing knowledge, hidden representations from the parser speci"
N18-1090,P11-2008,0,0.261694,"Missing"
N18-1090,N15-3017,0,0.0451594,"back-transliteration problems as a general sequence to sequence learning problem. In general, our goal is to learn a mapping for non-standard English and Romanized Hindi word forms to standard forms in their respective scripts. In case of Hindi, we address the problem of normalization and back-transliteration of Romanized Hindi words using a single model. We use the attention-based encoder-decoder model of Luong (Luong et al., 2015) with global attention for learning. For Hindi, we train the model on the transliteration pairs (87,520) from the Libindic transliteration project4 and BrahmiNet (Kunchukuttan et al., 2015) which are further augmented with noisy transliteration pairs (1,75,668) for normalization. Similarly, for normalization of noisy English words, we train the model on noisy word forms (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for 3 4 At inference time, our normalization models will predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering a"
N18-1090,D15-1166,0,0.0167549,"rate but similar character-level models for normalization-cum-transliteration of noisy Romanized Hindi words and normalization of noisy English words. We treat both normalization and back-transliteration problems as a general sequence to sequence learning problem. In general, our goal is to learn a mapping for non-standard English and Romanized Hindi word forms to standard forms in their respective scripts. In case of Hindi, we address the problem of normalization and back-transliteration of Romanized Hindi words using a single model. We use the attention-based encoder-decoder model of Luong (Luong et al., 2015) with global attention for learning. For Hindi, we train the model on the transliteration pairs (87,520) from the Libindic transliteration project4 and BrahmiNet (Kunchukuttan et al., 2015) which are further augmented with noisy transliteration pairs (1,75,668) for normalization. Similarly, for normalization of noisy English words, we train the model on noisy word forms (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for 3 4 At infere"
N18-1090,C12-1059,0,0.0188223,"set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce. We use the training by exploration method of Goldberg and Nivre (2012) for decoding a tran991 Tagger network: The input layer of the tagger encodes each input word in a sentence by concatenating a pre-trained word embedding with its character embedding given by a character Bi-LSTM. In the feature layer, the concatenated word and character representations are passed through two stacked Bi-LSTMs to generate a sequence of hidden representations which encode the contextual information spread across the sentence. The first Bi-LSTM is shared with the parser network while the other is specific to the tagger. Finally, output layer uses the feed-forward neural network wi"
N18-1090,P13-2121,0,0.0301872,"m ‘pt’ can expand to different standard word forms such as ‘put’, ‘pit’, ‘pat’, ‘pot’ and ‘pet’. The choice of word selection will solely depend on the sentential context. To select contextually relevant forms, we use exact search over n-best normalizations from the respective models extracted using beam-search decoding. The best word sequence is selected using the Viterbi decoding over bn word sequences scored by a trigram language model. b is the size of beam-width and n is the sentence length. The language models are trained on the monolingual data of Hindi and English using KenLM toolkit (Heafield et al., 2013). For each word, we extract five best normalizations (b=5). Decoding the best word sequence is a nontrivial problem for CS data due to lack of normalized and back-transliterated CS data for training a language model. One obvious solution is to apply decoding on individual language fragments in a CS sentence (Dutta et al., 2015). One major probhttp://ltrc.iiit.ac.in/icon2015/ https://github.com/libindic/indic-trans 989 English Decoding Raw Tweet Top 3 Normalizations Hindi Decoding Top 2 Dictionary Equivalents Top 3 Transliterations Best Yar year yarn yard friend buddy buddy ययर cn can con cano"
N18-1090,R15-1033,0,0.178929,"Missing"
N18-1090,W03-3017,0,0.110613,"s trained by minimizing a joint negative log-likelihood loss for both tasks. Unlike Zhang and Weiss (2016), we compute the gradients of the log-loss function simultaneously for each training instance. While the parser network is updated given the parsing loss only, the tagger network is updated with respect to both tagging and parsing losses. Both tagger and parser networks comprise of an input layer, a feature layer, and an output layer as shown in Figure 4. Following Zhang and Weiss (2016), we refer to this model as stack-prop. Our parsing models are based on an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce. We use the t"
N18-1090,P05-1013,0,0.186498,"Missing"
N18-1090,N13-1039,0,0.116777,"Missing"
N18-1090,C16-1234,1,0.879292,"Missing"
N18-1090,P17-1180,0,0.17978,"Missing"
N18-1090,Q16-1023,0,0.0925677,"Missing"
N18-1090,P17-4012,0,0.0633061,"Missing"
N18-1090,N16-1159,1,0.928777,"Missing"
N18-1090,W14-3907,0,0.26126,"Missing"
N18-1090,P16-1147,0,0.0840875,"Missing"
N18-1090,D08-1102,0,0.0634004,"Missing"
N18-1090,D08-1110,0,0.233261,"Missing"
N18-1090,D14-1105,0,0.184706,"Missing"
N18-1090,P17-1159,0,0.0329441,"… Linear Hidden … Hidden ℎ11 ℎ21 ℎ11 ℎ21 … … x2 … x1 Hidden Hidden ℎn1 ℎn1 xn Hidden Hin-Eng POS hidden layer x1 x2 … Hin-Eng Base Model POS output layer POS feature layer POS Input layer xn Figure 6: Neural Stacking-based parsing architecture for incorporating monolingual syntactic knowledge. English grammar As we discussed above, we adapted featurelevel neural stacking (Zhang and Weiss, 2016; Chen et al., 2016) for joint learning of POS tagging and parsing. Similarly, we also adapt this stacking approach for incorporating the monolingual syntactic knowledge into the base CS model. Recently, Wang et al. (2017) used neural stacking for injecting syntactic knowledge of English into a graph-based Singlish parser which lead to significant improvements in parsing performance. Unlike Wang et al. (2017), our base stacked models will allow us to transfer the POS tagging knowledge as well along the parse tree knowledge. As shown in Figure 6, we transfer both POS tagging and parsing information from the source Mixed grammar dis rat ki barish alwayz scares me . This night of rain always scares me . Figure 5: Code-switching tweet showing grammatical fragments from Hindi and English. 4.3 Linear Hidden Hidden Fe"
P16-3006,P96-1025,0,0.170838,"llenging task. Shallow parsing (Abney, 1992) is a task of automatic identification of correlated group of words (chunks) which reduces the computational effort at the level of full parsing by assigning partial structure to a sentence. To be precise, chunks are correlated group of words which contain only the head or content word and its modifiers. Shallow parser is not a single module but is a set of modules (Hammerton et al., 2002) with tokeniser, parts-of-speech tagger (pos-tagger) and chunker put in a pipeline. It has been experimentally proved that shallow parsers are useful in both text (Collins, 1996), (Buchholz and Daelemans, 2001) and speech processing domains (Wahlster, 2013). The current work aims to give an in-depth analysis on the effect of Sandhi in shallow parsing of Dravidian languages with a focus on Malayalam, the most agglutinative language (Sankaran and Jawahar, 2013) in the Dravidian family. For the purpose of analysis, we chose to create our own pos-tagger and chunker trained on a new 70k words annotated corpus with word internal features of morpho-phonological nature particularly because Sandhi evolved out of morphophonological reasons. In this paper, for the first time in"
P16-3006,W14-5125,1,0.875557,"Missing"
sachdeva-etal-2014-hindi,jha-2010-tdil,0,\N,Missing
sachdeva-etal-2014-hindi,W12-3102,0,\N,Missing
sachdeva-etal-2014-hindi,P02-1040,0,\N,Missing
sachdeva-etal-2014-hindi,P07-2045,0,\N,Missing
sachdeva-etal-2014-hindi,W12-3112,0,\N,Missing
sachdeva-etal-2014-hindi,P05-1033,0,\N,Missing
sachdeva-etal-2014-hindi,N03-1017,0,\N,Missing
sachdeva-etal-2014-hindi,2008.amta-srw.3,0,\N,Missing
sachdeva-etal-2014-hindi,I13-1176,1,\N,Missing
sachdeva-etal-2014-hindi,W13-2201,0,\N,Missing
sachdeva-etal-2014-hindi,zhang-etal-2004-interpreting,0,\N,Missing
sachdeva-etal-2014-hindi,O13-2003,0,\N,Missing
sachdeva-etal-2014-hindi,P00-1056,0,\N,Missing
sachdeva-etal-2014-hindi,P03-1021,0,\N,Missing
sachdeva-etal-2014-hindi,W12-3108,0,\N,Missing
W07-1608,W04-2608,0,0.612294,"‘representation of conceptualization’ based in turn on (Grimaud, 1988). The paper synthesizes this idea with the thesis of ideal meaning (Herskovits, 1986). (Tezuka et. al, 2001) have tried to resolve conceptual geographical prepositions using inference rule based on cognitive maps which people have of the 52 external world. (Hartrumpf et al., 2005) use knowledge representation formalism for PP interpretation. Some studies pertain to systems which have been implemented for MT; (Gustavii, 2005) uses aligned parallel corpora to induce automatic rules by applying transformation-based learning. (Alam, 2004) make use of contextual information to determine the meanings of over. (Trujillo, 1992) use a transfer rule based approach to translate locative PP-phrase, the approach uses the dependency relations marked as indices with individual word and a bilingual lexicon which has mapping between source and target lexical item (with indices). (Naskar and Bandyopadhyay, 2005) look at the semantics of the head noun of the reference object (this is their main criterion) to get the lexical meaning of prepositions in an English-Bengali MT system. The current paper presents a study of prepositions at, for, in"
W07-1608,2005.eamt-1.16,0,0.0938178,"al, 1991) attempts to translate locative prepositions between English and French. The paper introduces the notion of ‘representation of conceptualization’ based in turn on (Grimaud, 1988). The paper synthesizes this idea with the thesis of ideal meaning (Herskovits, 1986). (Tezuka et. al, 2001) have tried to resolve conceptual geographical prepositions using inference rule based on cognitive maps which people have of the 52 external world. (Hartrumpf et al., 2005) use knowledge representation formalism for PP interpretation. Some studies pertain to systems which have been implemented for MT; (Gustavii, 2005) uses aligned parallel corpora to induce automatic rules by applying transformation-based learning. (Alam, 2004) make use of contextual information to determine the meanings of over. (Trujillo, 1992) use a transfer rule based approach to translate locative PP-phrase, the approach uses the dependency relations marked as indices with individual word and a bilingual lexicon which has mapping between source and target lexical item (with indices). (Naskar and Bandyopadhyay, 2005) look at the semantics of the head noun of the reference object (this is their main criterion) to get the lexical meaning"
W07-1608,P91-1020,0,0.864003,"Missing"
W07-1608,W06-2113,0,0.537297,"Missing"
W07-1608,P98-2201,0,0.329039,"Missing"
W07-1608,1992.tmi-1.2,0,0.90252,"synthesizes this idea with the thesis of ideal meaning (Herskovits, 1986). (Tezuka et. al, 2001) have tried to resolve conceptual geographical prepositions using inference rule based on cognitive maps which people have of the 52 external world. (Hartrumpf et al., 2005) use knowledge representation formalism for PP interpretation. Some studies pertain to systems which have been implemented for MT; (Gustavii, 2005) uses aligned parallel corpora to induce automatic rules by applying transformation-based learning. (Alam, 2004) make use of contextual information to determine the meanings of over. (Trujillo, 1992) use a transfer rule based approach to translate locative PP-phrase, the approach uses the dependency relations marked as indices with individual word and a bilingual lexicon which has mapping between source and target lexical item (with indices). (Naskar and Bandyopadhyay, 2005) look at the semantics of the head noun of the reference object (this is their main criterion) to get the lexical meaning of prepositions in an English-Bengali MT system. The current paper presents a study of prepositions at, for, in, on, to and with in context of English to Indian language MT system. The paper is arra"
W07-1608,W06-2105,0,\N,Missing
W07-1608,C98-2196,0,\N,Missing
W09-3029,I08-2099,1,0.828553,"in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation of HDRB is presented in Prasad et al. (2008). present our characterization of discourse connectives and their arguments in Hindi (Section 2), our proposals for modifying the sense classification scheme (Section 3), and present some crosslinguistics comparisons based on annotations done so far (Section 4). Section 5 concludes with a summary and future work. 2 Discourse"
W09-3029,mladova-etal-2008-sentence,0,0.309198,"carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation"
W09-3029,W05-0312,0,0.242009,"nitial annotations carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse con"
W09-3029,I08-7009,0,0.484908,"Missing"
W09-3029,prasad-etal-2008-penn,1,\N,Missing
W09-3029,I08-7010,1,\N,Missing
W09-3030,P93-1015,1,0.782249,"Missing"
W09-3030,I08-2099,1,0.951963,"he other hand, pure shallow parsing techniques (PVS and Gali, 2007) are not enough for providing sufficient information for applications such as machine translation, query answering etc. It is here that the notion of a simple parser is born where the idea is to parse a sentence at a coarser level. One could go to a finer level of parse depending on the ease with which such a parse can be generated. The simple parser that we describe here is a grammar oriented model that makes use of linguistic features to identify relations. We have modeled the simple parser on the Paninian grammatical model (Begum et al., 2008; Bharati et al., 1995) which provides a dependency grammar framework. Paninian dependency grammar works well for analyzing Indian languages (Bharati et al., 1993). We have followed karaka1 based approach for parsing. An effort has been previously made in grammar driven parsing for Hindi by us (Gupta et al., 2008) where the focus was not to mark relations in a broad coverage sense but to mark certain easily identifiable relations using a rule base. In this paper, we show improvements in results over our previous work by including some additional linguistic features which help in identifying re"
W09-3030,W09-3812,1,\N,Missing
W09-3036,H05-1066,0,0.0147844,"ation of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conversion approach. 2 Two Kinds of Syntactic Structure Two different approaches to describing syntactic structure, dependency structure (DS) (Mel’čuk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides. Formally, in a PS tree,"
W09-3036,P99-1065,0,0.0911636,"ed; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dep"
W09-3036,P03-1054,0,0.00169422,"how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conve"
W09-3036,C02-2025,0,0.0152376,"d theories. The next section highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure version. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al. 2005). A multi-layered approach is also found in the Prague Dependency Treebank (Hajič et al. 2001), or in treebanks based on LFG (King et al. 2003) or HPSG (Oepen et al. 2002). A lesson learned here is that the addition of deeper, more semantic levels may be complicated if the syntactic annotation was not designed with the possibility of multiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of propbanking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not nat"
W09-3036,J05-1004,1,0.175107,"s are carefully coordinated. 1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natural language processing components. Today, treebanks have been constructed for many languages, including Arabic, Chinese, Czech, English, French, German, Korean, Spanish, and Turkish. This paper describes the creation of a Hindi/Urdu multi-representational and multilayered treebank. Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English PropBank (Palmer et al. 2005). Multirepresentational means that we distinguish conceptually what is being represented from how it is represented; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is"
W09-3036,P07-1021,0,\N,Missing
W10-1403,R09-2001,1,0.925372,". Libsvm consistently gave better performance over liblinear in all the experiments. For SVM settings, we tried out different combinations of best SVM settings of the same parser on different languages in CoNLL-2007 shared task (Hall et al., 2007) and applied the best settings. For feature model, apart from trying best feature settings of the same parser on different languages in CoNLL2007 shared task (Hall et al., 2007), we also tried out different combinations of linguistically intuitive features and applied the best feature model. The best feature model is same as the feature model used in Ambati et al. (2009a), which is the best 8 performing system in the ICON-2009 NLP Tools Contest (Husain, 2009). For the MSTParser, non-projective algorithm, order=2 and training-k=5 gave best results in all the approaches. For the MaxEnt, apart from some general useful features, we experimented considering different combinations of features of node, parent, siblings, and children of the node. 5 Results and Analysis All the experiments discussed in section 2 and 3 were performed considering both gold-standard shallow parser information and automatic shallow parser9 information. Automatic shallow parser uses a rul"
W10-1403,I08-2099,1,0.524713,"Missing"
W10-1403,J95-3006,0,0.938765,"(2) [NP raama/NNP ne/PSP] [NP seba/NN] ‘Ram’ ERG ‘apple’ [VGF khaa/VM liyaa/VAUX] ‘eat’ ‘PRFT’ ‘Ram ate an apple’ The suffix concatenation feature for khaa, which is the head of the VGF chunk, will be ‘0+yaa’ and is formed by concatenating the suffix of the main verb with that of its auxiliary. Similarly, the suffix concatenation feature for raama, which is head of the NP chunk, will be ‘0+ne’. This feature turns out to be very important. This is because in Hindi (and many other Indian languages) there is a direct correlation between the TAM markers and the case that appears on some nominals (Bharati et al., 1995). In (2), for example, khaa liyaa together gives the past perfective aspect for the verb khaanaa ‘to eat’. Since, Hindi is split ergative, the subject of the transitive verb takes an ergative case marker when the verb is past perfective. Similar 3 Inside, Outside, Beginning of the chunk. correlation between the case markers and TAM exist in many other cases. 3 An alternative approach to use best features: A 2-stage setup (2stage) So far we have been using various information such as POS, chunk, etc. as features. Rather than using them as features and doing parsing at one go, we can alternative"
W10-1403,W09-3812,1,0.91756,"en using various information such as POS, chunk, etc. as features. Rather than using them as features and doing parsing at one go, we can alternatively follow a 2-stage setup. In particular, we divide the task of parsing into: · · Intra-chunk dependency parsing Inter-chunk dependency parsing We still use POS, best morphological features (case, suffix, root) information as regular features during parsing. But unlike LMSaF mentioned in section 2.3, where we gave local morphosyntactic information as a feature, we divided the task of parsing into sub-tasks. A similar approach was also proposed by Bharati et al. (2009c). During intrachunk dependency parsing, we try to find the dependency relations of the words within a chunk. Following which, chunk heads of each chunk within a sentence are extracted. On these chunk heads we run an inter-chunk dependency parser. For each chunk head, in addition to POS tag, useful morphological features, any useful intra-chunk information in the form of lexical item, suffix concatenation, dependency relation are also given as a feature. Figure 2 shows the steps involved in this approach for (1). There are two noun chunks and one verb chunk in this sentence. raama and seba ar"
W10-1403,Y09-2020,1,0.903248,"en using various information such as POS, chunk, etc. as features. Rather than using them as features and doing parsing at one go, we can alternatively follow a 2-stage setup. In particular, we divide the task of parsing into: · · Intra-chunk dependency parsing Inter-chunk dependency parsing We still use POS, best morphological features (case, suffix, root) information as regular features during parsing. But unlike LMSaF mentioned in section 2.3, where we gave local morphosyntactic information as a feature, we divided the task of parsing into sub-tasks. A similar approach was also proposed by Bharati et al. (2009c). During intrachunk dependency parsing, we try to find the dependency relations of the words within a chunk. Following which, chunk heads of each chunk within a sentence are extracted. On these chunk heads we run an inter-chunk dependency parser. For each chunk head, in addition to POS tag, useful morphological features, any useful intra-chunk information in the form of lexical item, suffix concatenation, dependency relation are also given as a feature. Figure 2 shows the steps involved in this approach for (1). There are two noun chunks and one verb chunk in this sentence. raama and seba ar"
W10-1403,W09-3036,1,0.407936,"data-driven parsers, MaltParser (Nivre et al., 2007b) and MSTParser (McDonald et al., 2006). We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing. We then investigate the best way to incorporate this information during dependency parsing. All the experiments were done on a part of multi-layered and multirepresentational Hindi Treebank (Bhatt et al., 2009)1. The shallow parser performs three tasks, (a) it gives the POS tags for each lexical item, (b) provides morphological features for each lexical item, and (c) performs chunking. A chunk is a minimal (non-recursive) phrase consisting of correlated, inseparable words/entities, such that the intrachunk dependencies are not distorted (Bharati et 1 This Treebank is still under development. There are currently 27k tokens with complete sentence level annotation. 22 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22–30, c Los Angeles, C"
W10-1403,C96-1058,0,0.0190882,"ches using gold-standard shallow parser information. Malt is a classifier based shift/reduce parser. It provides option for six parsing algorithms, namely, arc-eager, arc-standard, convington projective, covington non-projective, stack projective, stack eager and stack lazy. The parser also provides option for libsvm and liblinear learning model. It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). MST uses Chu-LiuEdmonds (Chu and Liu, 1965; Edmonds, 1967) Maximum Spanning Tree algorithm for nonprojective parsing and Eisner&apos;s algorithm for projective parsing (Eisner, 1996). It uses online large margin learning as the learning algorithm (McDonald et al., 2005). In this paper, we use MST only for unlabeled dependency tree and use a separate maximum entropy model8 (MaxEnt) for labeling. Various combination of features such as node, its parent, siblings and children were tried out before arriving at the best results. As the training data size is small we did 5-fold cross validation on the training data for tuning the parameters of the parsers and for feature selection. Best settings obtained using cross-validated data are applied on test set. We present the results"
W10-1403,J08-3003,0,0.0251686,"vel parsing for Hindi. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages have not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not a"
W10-1403,N10-1093,1,0.900535,"munity has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages have not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper we explore various s"
W10-1403,D07-1097,0,0.0172794,"s small we did 5-fold cross validation on the training data for tuning the parameters of the parsers and for feature selection. Best settings obtained using cross-validated data are applied on test set. We present the results both on cross validated data and on test data. For the Malt Parser, arc-eager algorithm gave better performance over others in all the approaches. Libsvm consistently gave better performance over liblinear in all the experiments. For SVM settings, we tried out different combinations of best SVM settings of the same parser on different languages in CoNLL-2007 shared task (Hall et al., 2007) and applied the best settings. For feature model, apart from trying best feature settings of the same parser on different languages in CoNLL2007 shared task (Hall et al., 2007), we also tried out different combinations of linguistically intuitive features and applied the best feature model. The best feature model is same as the feature model used in Ambati et al. (2009a), which is the best 8 performing system in the ICON-2009 NLP Tools Contest (Husain, 2009). For the MSTParser, non-projective algorithm, order=2 and training-k=5 gave best results in all the approaches. For the MaxEnt, apart fr"
W10-1403,P05-1012,0,0.13126,"shift/reduce parser. It provides option for six parsing algorithms, namely, arc-eager, arc-standard, convington projective, covington non-projective, stack projective, stack eager and stack lazy. The parser also provides option for libsvm and liblinear learning model. It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). MST uses Chu-LiuEdmonds (Chu and Liu, 1965; Edmonds, 1967) Maximum Spanning Tree algorithm for nonprojective parsing and Eisner&apos;s algorithm for projective parsing (Eisner, 1996). It uses online large margin learning as the learning algorithm (McDonald et al., 2005). In this paper, we use MST only for unlabeled dependency tree and use a separate maximum entropy model8 (MaxEnt) for labeling. Various combination of features such as node, its parent, siblings and children were tried out before arriving at the best results. As the training data size is small we did 5-fold cross validation on the training data for tuning the parameters of the parsers and for feature selection. Best settings obtained using cross-validated data are applied on test set. We present the results both on cross validated data and on test data. For the Malt Parser, arc-eager algorithm"
W10-1403,W06-2932,0,0.372758,"t ways, and it has been hypothesized that the integration of morphological and syntactic information could be a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper we explore various strategies to incorporate local morphosyntactic features in Hindi dependency parsing. These features are obtained using a shallow parser. We conducted experiments with two data-driven parsers, MaltParser (Nivre et al., 2007b) and MSTParser (McDonald et al., 2006). We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing. We then investigate the best way to incorporate this information during dependency parsing. All the experiments were done on a part of multi-layered and multirepresentational Hindi Treebank (Bhatt et al., 2009)1. The shallow parser performs three tasks, (a) it gives the POS tags fo"
W10-1403,D07-1013,0,0.182735,"Missing"
W10-1403,P05-1013,0,0.0701222,"90.4 81.7 84.1 89.1 79.2 82.5 90.0 80.9 91.5 82.7 84.7 91.8 84.0 86.2 90.8 79.8 82.0 92.0 81.8 91.8 83.3 85.3 92.4 84.4 86.3 92.1 82.2 84.3 92.7 84.0 Table 1: Results of all the four approaches using gold-standard shallow parser information. Malt is a classifier based shift/reduce parser. It provides option for six parsing algorithms, namely, arc-eager, arc-standard, convington projective, covington non-projective, stack projective, stack eager and stack lazy. The parser also provides option for libsvm and liblinear learning model. It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). MST uses Chu-LiuEdmonds (Chu and Liu, 1965; Edmonds, 1967) Maximum Spanning Tree algorithm for nonprojective parsing and Eisner&apos;s algorithm for projective parsing (Eisner, 1996). It uses online large margin learning as the learning algorithm (McDonald et al., 2005). In this paper, we use MST only for unlabeled dependency tree and use a separate maximum entropy model8 (MaxEnt) for labeling. Various combination of features such as node, its parent, siblings and children were tried out before arriving at the best results. As the training data size is small we did 5-fold cross validation on the"
W10-1403,W09-3824,0,0.0257168,"1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages have not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expecte"
W10-1403,C08-1112,0,0.0531126,"empt at complete sentence level parsing for Hindi. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages have not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intui"
W10-1403,J08-4010,0,\N,Missing
W10-1403,D07-1096,0,\N,Missing
W10-2103,J09-3007,0,0.0268565,"and presents a few examples that call for a deeper engagement between the two fields. 1 Introduction The recent success of data-driven approaches in NLP has raised important questions as to what role linguistics must now seek to play in further advancing the field. Perhaps, it is also time to pose the same question from the other direction: As to how NLP techniques can help linguists make informed decisions? And how can the advances made in one field be applied to the other? Although, there has been some work on incorporating NLP techniques for linguistic fieldwork and language documentation (Bird, 2009), the wider use of NLP in linguistic studies is still fairly limited. However, it is possible to deepen the engagement between the two fields in a number of possible areas (as we shall see in the following sections), and gain new insights even during the formulation of linguistic theories and frameworks. Figure 1: Phylogenetic tree using feature n-grams biological species. Constructing a phylogenetic tree for languages usually requires the calculation of distances between pairs of languages (usually based on word lists). These distances are then given as input to a computational phylogenetic a"
W10-2103,2004.eamt-1.14,0,0.0190201,"es. Another insight comes from Machine Translation. More than any other sub-field in NLP, it is the data-driven approaches to machine translation that have proven to be particularly successful over the past few years. We have been exploring various approaches towards hybridization of our rulebased MT system. Building the transfer-grammar of such systems is perhaps one of the most timeintensive tasks that involves careful analysis of test data. However, data driven techniques can come to the aid of linguists in this case. The recent work on automatic acquisition of rules from parallel corpora (Lavie et al., 2004) can help identify a large number of common syntactic transformations across a pair of languages, and help unearth those transformations that might otherwise be missed by a rule-based grammar. They can be further used to prioritize the application of rules based on the observed frequencies of certain syntactic transformations. 3 Lexical Correspondence and Linguistic Units A further case in point is lexical correspondence across languages, which poses a problem for cross-lingual and multilingual applications. To address this and some other issues, a linguistic unit that behaves similarly across"
W10-2103,R09-1064,0,0.0253934,"Phylogenetic tree using feature n-grams biological species. Constructing a phylogenetic tree for languages usually requires the calculation of distances between pairs of languages (usually based on word lists). These distances are then given as input to a computational phylogenetic algorithm. Their successful use for languages has opened the possibility of using computational techniques for studying historical linguistics. They have already been used for estimating divergence times of language families (Atkinson et al., 2005). Figure 1 shows a phylogenetic tree created using feature n-grams (Rama and Singh, 2009). Another area for the application of NLP techniques is language typology. For example, linguistic similarity and its estimation can be seen as fundamental ideas in NLP. The systematic study of different kinds of linguistic similarity offers insights towards the theoretical studies of languages (Singh, 2010). In brief, the typology of linguistic similarity for computational purposes is related to linguistic levels (depth), differences among languages (linguality) and linguistic units (granularity). Thus, language can be seen as a system of symbols whose meanings are defined 2 Historical Lingui"
W10-2103,W07-1306,0,0.0160154,"ic levels (depth), differences among languages (linguality) and linguistic units (granularity). Thus, language can be seen as a system of symbols whose meanings are defined 2 Historical Linguistics and Linguistic Typology Computational techniques have been successfully used to classify languages and to generate phylogenetic trees. This has been tried not just with handcrafted word lists (Atkinson et al., 2005; Atkinson and Gray, 2006; Huelsenbeck et al., 2001) or syntactic data (Barbac¸on et al., 2007) but with lists extracted from written corpus with comparable results (Rama and Singh, 2009; Singh and Surana, 2007). These techniques are inspired from the work in computational phylogenetics, which was aimed at constructing evolutionary trees of 18 Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 18–21, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics in terms of their estimated similarity and distance with other symbols. Can this, together with what Cognitive Linguists have been studying (Robinson and Ellis, 2008), which also involves linguistic similarity, often directly, have some relevance for linguists? processing techn"
W10-2103,W10-1411,0,\N,Missing
W10-3216,I08-2099,1,0.880507,"minary work on Hindi causative verbs. Abstract This paper introduces a preliminary work on Hindi causative verbs: their classification, a linguistic model for their classification and their verb frames. The main objective of this work is to come up with a classification of the Hindi causative verbs. In the classification we show how different types of Hindi verbs have different types of causative forms. It will be a linguistic resource for Hindi causative verbs which can be used in various NLP applications. This resource enriches the already available linguistic resource on Hindi verb frames (Begum et al., 2008b). This resource will be helpful in getting proper insight into Hindi verbs. In this paper, we present the morphology, semantics and syntax of the causative verbs. The morphology is captured by the word generation process; semantics is captured by the linguistic model followed for classifying the verbs and the syntax has been captured by the verb frames using relations given by Panini. 1 2 Introduction Verbs play a major role in expressing the meaning of a sentence and its syntactic behavior. They decide the number of participants that will participate in the action. Semantically verbs are cl"
W10-3216,begum-etal-2008-developing,1,0.867055,"minary work on Hindi causative verbs. Abstract This paper introduces a preliminary work on Hindi causative verbs: their classification, a linguistic model for their classification and their verb frames. The main objective of this work is to come up with a classification of the Hindi causative verbs. In the classification we show how different types of Hindi verbs have different types of causative forms. It will be a linguistic resource for Hindi causative verbs which can be used in various NLP applications. This resource enriches the already available linguistic resource on Hindi verb frames (Begum et al., 2008b). This resource will be helpful in getting proper insight into Hindi verbs. In this paper, we present the morphology, semantics and syntax of the causative verbs. The morphology is captured by the word generation process; semantics is captured by the linguistic model followed for classifying the verbs and the syntax has been captured by the verb frames using relations given by Panini. 1 2 Introduction Verbs play a major role in expressing the meaning of a sentence and its syntactic behavior. They decide the number of participants that will participate in the action. Semantically verbs are cl"
W10-3216,J95-3006,0,0.185604,"erb (Tripathi, 1986; Reinhart, 2005). In this paper, we motivate our work by presenting our approach for classifying the causative verbs in Hindi. 4 4.1 Our Approach Linguistic Model for Classifying Causative verbs We have followed Paninian Grammatical framework in this model as the theoretical basis for our approach. The meaning of every verbal root (dhaatu) consists of: (a) activity (vyaapaara) and; (b) result (phala). Activity denotes the actions carried out by the various participants (karakas) involved in the action. Result denotes the state that is reached, when the action is completed (Bharati et al., 1995). The participants of the action expressed by the verb root are called karakas. There are six basic karakas, namely; adhikarana ‘location’, apaadaan ‘source’, sampradaan ‘recipient’, karana ‘instrument’, karma ‘theme’ and karta ‘agent’ (Begum et al., 2008a). Here the mapping between karakas and theta roles is a rough mapping. The karta karaka is the locus of the activity. Similarly karma karaka is the locus of the result. The locus of the activity implied by the verbal root can be animate or inanimate. Sentence (2) given above, is the example where the locus of the activity is animate. Sentenc"
W10-3216,W09-3812,0,0.0240209,"Missing"
W10-3216,W09-3036,1,0.799174,"Missing"
W11-0414,miltsakaki-etal-2004-penn,0,0.0369658,"answering, natural language generation and textual summarization. Such a study is possible in a given human language only if there are sufficient discourse annotated resources available for that language. The Penn Discourse Treebank (PDTB) is a project whose goal is to annotate the discourse relations holding between events described in a text. The PDTB is a lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text Dipti Misra Sharma International Institute of Information Technology Hyderabad, India dipti@iiit.ac.in (Miltsakaki et al. 2004, Prasad et al., 2008). To foster cross-linguistic studies in discourse relations, projects similar to the PDTB in discourse annotation were initiated in Czech (Mladová et al., 2008), Chinese (Xue, 2005), Turkish (Zeyrek and Webber, 2008) and Hindi (Prasad et al., 2008). We explore how the underlying framework and annotation guidelines apply to Tamil, a morphologically rich, agglutinative, free word order language. In this paper, we present how a corpus of Tamil texts was created on which we performed our pilot experiment. Next, in Section 3 we cover the basics of the PDTB guidelines that we f"
W11-0414,W98-0315,0,0.0557925,"d as being explicit, implicit, AltLex, EntRel or NoRel (Prasad et al. 2008). These classes are described in detail in Section 4. By convention, annotated explicit connectives are underlined and implicit connectives are shown by the marker, “(Implicit=)”. As can be seen in example (1), one of the arguments is shown enclosed between {} and the other argument is shown in []. The AltLex, EntRel or NoRel relations are shown by underlining, i.e., as “(AltLex=)”, “(EntRel)” and “(NoRel)”, respectively. (1) (2) Penn Discourse Treebank Guidelines The PDTB is a resource built on discourse structure in (Webber and Joshi, 1998) where discourse connectives are treated as discourse-level predicates that always take exactly two abstract objects such as events, states and propositions as their arguments. We now describe the types of connectives and their senses from the PDTB framework and provide examples from Tamil sentences. 3.1 sion:Conjunction „also‟ whereas in example (3) the same affix carries the sense of the subtype Contingency:Concession „however‟. {eN kAl uDaindadaN}Al [eNNAl viLayADa muDiyAdu]. „{My leg broke}, hence [I cannot play].‟ Sense Hierarchy The semantics of discourse relations are termed as senses a"
W11-0414,W05-0312,0,0.019587,"Discourse Treebank (PDTB) is a project whose goal is to annotate the discourse relations holding between events described in a text. The PDTB is a lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text Dipti Misra Sharma International Institute of Information Technology Hyderabad, India dipti@iiit.ac.in (Miltsakaki et al. 2004, Prasad et al., 2008). To foster cross-linguistic studies in discourse relations, projects similar to the PDTB in discourse annotation were initiated in Czech (Mladová et al., 2008), Chinese (Xue, 2005), Turkish (Zeyrek and Webber, 2008) and Hindi (Prasad et al., 2008). We explore how the underlying framework and annotation guidelines apply to Tamil, a morphologically rich, agglutinative, free word order language. In this paper, we present how a corpus of Tamil texts was created on which we performed our pilot experiment. Next, in Section 3 we cover the basics of the PDTB guidelines that we followed during our annotation process. In Section 4, we show various categories of Tamil discourse connectives that we identified after a preliminary study on discourse connectives in Tamil, illustrating"
W11-0414,I08-7009,0,0.204389,"DTB) is a project whose goal is to annotate the discourse relations holding between events described in a text. The PDTB is a lexically grounded approach where discourse relations are anchored in lexical items wherever they are explicitly realized in the text Dipti Misra Sharma International Institute of Information Technology Hyderabad, India dipti@iiit.ac.in (Miltsakaki et al. 2004, Prasad et al., 2008). To foster cross-linguistic studies in discourse relations, projects similar to the PDTB in discourse annotation were initiated in Czech (Mladová et al., 2008), Chinese (Xue, 2005), Turkish (Zeyrek and Webber, 2008) and Hindi (Prasad et al., 2008). We explore how the underlying framework and annotation guidelines apply to Tamil, a morphologically rich, agglutinative, free word order language. In this paper, we present how a corpus of Tamil texts was created on which we performed our pilot experiment. Next, in Section 3 we cover the basics of the PDTB guidelines that we followed during our annotation process. In Section 4, we show various categories of Tamil discourse connectives that we identified after a preliminary study on discourse connectives in Tamil, illustrating each with examples. In Section 5,"
W11-0414,I08-7010,1,\N,Missing
W11-3405,W09-3036,1,0.917668,"Missing"
W11-3405,E03-1068,0,0.796048,"inter-chunk dependencies unless explicitly stated. In the following section, we first describe the related work in the area of detecting errors in treebanks, in general. Then, we present the work on Hindi in particular. 4 gayaa. VAUX ‘went’. Validation and correction tools are an important part for making treebanks error-free and consistent. With an increase in demand for high quality annotated corpora over the last decade, major research works in the field of developing lexical resources have focused on detection of errors. One such approach for treebank error detection has been employed by Dickinson and Meurers (2003a; 2003b; 2005) and Boyd et al. (2008). The underlying principle in these works is to detect “variation n-grams” in syntactic annotation across large corpora. These variations could be present for a continuous sequence of words (POS and chunks) or for a non-continuous sequence of words (dependency structures), more the number of variation for a particular contiguous or non-contiguous sequence of tokens (or words), greater the chance of the particular variation being an error. They use these statistical patterns (n-grams) to detect anomalies in POS annotation in corpora such as the Penn treeban"
W11-3405,P05-1040,0,0.612584,"Missing"
W11-3405,ambati-etal-2010-high,1,0.643403,"rrent paper is to describe the methodology employed to detect errors in the dependency level (inter-chunk dependencies) of the DS representation. Error detection at intrachunk dependencies is out of scope of this paper. In the rest of the paper, by dependency level, we 25 by van Noord (2004) and de Kok et al. (2009) rely on the output of a reliable state-of-the-art parser which may not be available for many languages just as in the case of Hindi, the language in question for our work. It becomes a challenge to develop error detection tools for small treebanks like Hindi. There is an effort by Ambati et al. (2010) in this direction for Hindi treebank validation. They used a combination of a rule-based and hybrid system to detect treebank errors. Rule-based system works on the development of robust (high precision) rules which are formed using the annotation guidelines and the framework, whereas the hybrid system is a combination of statistical module with a rule-based post-processing module. The statistical module helps in detecting a wide array of potential errors and suspect cases. The rule-based post-processing module then prunes out the false positives, with the help of robust and efficient rules t"
W11-3405,W00-1907,0,0.712911,"Missing"
W11-3405,I08-2099,1,0.771926,"Hence, there are chances that errors are introduced either by human annotators or by the preprocessed output of automated tools. It is crucial that the annotated corpora are free of anomalies 23 Proceedings of the 9th Workshop on Asian Language Resources, pages 23–30, Chiang Mai, Thailand, November 12 and 13, 2011. 2 Dependency Relations: After POS, morph and chunk annotation, inter-chunk dependency annotation 1 is done following the set of dependency guidelines in Bharati et al. (2009). This information is encoded at the syntactico-semantic level following the Paninian dependency framework (Begum et al., 2008; Bharati et al., 1995). After inter-chunk annotation, plan is to use a high accuracy intra-chunk expander, which marks the intra-chunk dependencies 2 and expands the chunks arriving at sentence level dependency tree. Hindi Dependency Treebank A multi-layered and multi-representational treebank for Hindi (Bhatt et al., 2009; Xia et al., 2009) is being developed. The treebank will have dependency, verb-argument (PropBank, Palmer et al., 2005) and phrase structure (PS) representation. Automatic conversion from dependency structure (DS) to phrase structure (PS) is planned. Hence, it is important"
W11-3405,W09-2609,0,0.139343,"Missing"
W11-3405,J95-3006,0,0.236146,"ances that errors are introduced either by human annotators or by the preprocessed output of automated tools. It is crucial that the annotated corpora are free of anomalies 23 Proceedings of the 9th Workshop on Asian Language Resources, pages 23–30, Chiang Mai, Thailand, November 12 and 13, 2011. 2 Dependency Relations: After POS, morph and chunk annotation, inter-chunk dependency annotation 1 is done following the set of dependency guidelines in Bharati et al. (2009). This information is encoded at the syntactico-semantic level following the Paninian dependency framework (Begum et al., 2008; Bharati et al., 1995). After inter-chunk annotation, plan is to use a high accuracy intra-chunk expander, which marks the intra-chunk dependencies 2 and expands the chunks arriving at sentence level dependency tree. Hindi Dependency Treebank A multi-layered and multi-representational treebank for Hindi (Bhatt et al., 2009; Xia et al., 2009) is being developed. The treebank will have dependency, verb-argument (PropBank, Palmer et al., 2005) and phrase structure (PS) representation. Automatic conversion from dependency structure (DS) to phrase structure (PS) is planned. Hence, it is important to have a high quality"
W11-3405,J93-2004,0,0.0375079,"2003b; 2005) and Boyd et al. (2008). The underlying principle in these works is to detect “variation n-grams” in syntactic annotation across large corpora. These variations could be present for a continuous sequence of words (POS and chunks) or for a non-continuous sequence of words (dependency structures), more the number of variation for a particular contiguous or non-contiguous sequence of tokens (or words), greater the chance of the particular variation being an error. They use these statistical patterns (n-grams) to detect anomalies in POS annotation in corpora such as the Penn treebank (Marcus et al., 1993), TIGER corpus (Brants et al., 2002) etc. For discontinuous patterns as found most commonly in dependency annotation (Boyd et al., 2008), they tested their strategy on Talbanken05 (Nivre et al., 2006) apart from the corpora mentioned above. This we believe was the first mainstream work on error detection in dependency annotation. Some other earlier methods employed for error detection in syntactic annotation (mainly POS and chunk), are by Eskin (2000) and van Halteren (2000). Based on large corpora, van Noord (2004) and de Kok et al. (2009) employed error mining techniques. The basic underlyin"
W11-3405,W06-2932,0,0.0587954,"Missing"
W11-3405,nivre-etal-2006-talbanken05,0,0.0618345,"nuous sequence of words (POS and chunks) or for a non-continuous sequence of words (dependency structures), more the number of variation for a particular contiguous or non-contiguous sequence of tokens (or words), greater the chance of the particular variation being an error. They use these statistical patterns (n-grams) to detect anomalies in POS annotation in corpora such as the Penn treebank (Marcus et al., 1993), TIGER corpus (Brants et al., 2002) etc. For discontinuous patterns as found most commonly in dependency annotation (Boyd et al., 2008), they tested their strategy on Talbanken05 (Nivre et al., 2006) apart from the corpora mentioned above. This we believe was the first mainstream work on error detection in dependency annotation. Some other earlier methods employed for error detection in syntactic annotation (mainly POS and chunk), are by Eskin (2000) and van Halteren (2000). Based on large corpora, van Noord (2004) and de Kok et al. (2009) employed error mining techniques. The basic underlying strategy was to obtain a set of parsed and un-parsed sentences using a wide-coverage parser and compute suspicion ratio for detecting errors. Other examples of detection of annotation errors in tree"
W11-3405,P04-1057,0,0.131152,"Missing"
W11-3405,J05-1004,0,0.0123835,"of dependency guidelines in Bharati et al. (2009). This information is encoded at the syntactico-semantic level following the Paninian dependency framework (Begum et al., 2008; Bharati et al., 1995). After inter-chunk annotation, plan is to use a high accuracy intra-chunk expander, which marks the intra-chunk dependencies 2 and expands the chunks arriving at sentence level dependency tree. Hindi Dependency Treebank A multi-layered and multi-representational treebank for Hindi (Bhatt et al., 2009; Xia et al., 2009) is being developed. The treebank will have dependency, verb-argument (PropBank, Palmer et al., 2005) and phrase structure (PS) representation. Automatic conversion from dependency structure (DS) to phrase structure (PS) is planned. Hence, it is important to have a high quality version of the dependency treebank so that the process of automated conversion to PS does not induce errors in PS. The dependency treebank contains information encoded at the morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) levels. 3 Other Features: In the dependency treebank, apart from POS, morph, chunk and inter-chunk dependency annotations, some special fea"
W12-2302,2001.mtsummit-papers.24,0,0.0208229,"Number is an example of inflectional morphology. Example: cars = car + plural affix ’s’. The main objective of our work is to develop a tool which executes the derivational morphological Dipti Misra Sharma LTRC IIIT-Hyderabad India dipti@iiit.ac.in analysis of Hindi. Morphological analysis is an important step for any linguistically informed natural language processing task. Most morphological analyzers perform only inflectional analysis. However, derivational analysis is also crucial for better performance of several systems. They are used to improve the efficiency of machine translators (C Gdaniec et al., 2001). They are also used in search engines to improve the information extraction (J Vilares et al., 2001). Since derivational processes can often be productive in a language, the development of an effective derivational analyzer will prove beneficial in several aspects. We developed a derivational analyzer for Hindi over an already existing inflectional analyzer developed at IIIT Hyderabad. In this approach, first, the derived words in Hindi were studied to obtain the derivational suffixes of the language. Then the rules were designed by understanding the properties of the suffixes. The Hindi Wiki"
W12-2302,aswani-gaizauskas-2010-developing,0,0.0668872,"Missing"
W12-2302,A00-1030,0,0.0536342,"Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 10–16, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics of this language have been developed. There are derivational analyzers for other Indian languages like Marathi (Ashwini Vaidya, 2009) and Kannada (Bhuvaneshwari C Melinamath et al., 2011). The Marathi morphological analyzer was built using a Paradigm based approach whereas the Kannada analyzer was built using an FST based approach. As far as English is concerned, there are some important works (Woods, 2000; Hoeppner, 1982) pertaining to the area of derivational morphological analysis. However, both of these are lexicon based works. For our work, we employed a set of suffix replacement rules and a dictionary in our derivational analyzer, having taken insights from the Porter’s stemmer (Porter, 1980) and the K-stemmer (R. Krovetz. 1993). They are amongst the most cited stemmers in the literature. The primary goal of Porter’s stemmer is suffix stripping. So when a word is given as input, the stemmer strips all the suffixes in the word to produce a stem. It achieves the task in five steps applying"
W12-2302,C82-1021,0,0.506857,"ing of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 10–16, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics of this language have been developed. There are derivational analyzers for other Indian languages like Marathi (Ashwini Vaidya, 2009) and Kannada (Bhuvaneshwari C Melinamath et al., 2011). The Marathi morphological analyzer was built using a Paradigm based approach whereas the Kannada analyzer was built using an FST based approach. As far as English is concerned, there are some important works (Woods, 2000; Hoeppner, 1982) pertaining to the area of derivational morphological analysis. However, both of these are lexicon based works. For our work, we employed a set of suffix replacement rules and a dictionary in our derivational analyzer, having taken insights from the Porter’s stemmer (Porter, 1980) and the K-stemmer (R. Krovetz. 1993). They are amongst the most cited stemmers in the literature. The primary goal of Porter’s stemmer is suffix stripping. So when a word is given as input, the stemmer strips all the suffixes in the word to produce a stem. It achieves the task in five steps applying rules at each ste"
W12-2302,W04-0109,0,\N,Missing
W12-2302,J01-2001,0,\N,Missing
W12-2302,P99-1037,0,\N,Missing
W12-2302,P96-1004,0,\N,Missing
W12-2302,Y06-1005,0,\N,Missing
W12-2302,C00-1032,0,\N,Missing
W12-3607,W03-3017,0,0.0517139,"em. 5. Dependency Relation: If all the constraints are satisfied, then the dependency relation from this column is marked on the parent-child arc. 5.2 Results Rule-Based Approach: As discussed in section 5.1, the rule-based approach marks dependency relation mainly by using POS patterns in a chunk. Table 3 shows the result when evaluated for the test data. LAS UAS LS Sub-tree Parsing using MaltParser We use MaltParser (Nivre et al., 2007) as an alternative method to identify the intra-chunk relations. It is well known in the literature that transition-based dependency parsing techniques (e.g. Nivre, 2003) work best for marking short distance dependencies in a sentence. As must be clear by now, intra-chunk relations are in fact short distance dependencies; and we basically use MaltParser to predict the internal structure of a chunk. So instead of using it to parse a sentence, we parse individual chunks. Each chunk is treated as a sub-tree. The training data contains sub-trees with intra-chunk relations marked between chunkinternal nodes, the head of the chunk becomes the 54 97.89 98.50 98.38 Table 3: Parsing accuracies2 obtained using rulebased tool Statistical/MaltParser-based approach: Table"
W12-3607,J93-2004,0,0.039496,"Missing"
W12-3607,I08-2099,1,0.807821,"1: ई niilii kitaab gir gaii ‘blue’ ‘book’ ‘fall’ ’go-perf’ The blue book fell down 1 1.1 1.2 2 2.1 2.2 (( niilii kitaab )) (( gir gaii )) NP JJ NN <name=’NP’ drel=’k1:VGF’> <name='niilii'> <name='kitaab'> VGF <name=’VGF’> VM <name='gir'> VAUX <name='gaii'> Figure 1: SSF representation Figure 2 shows the schematic dependency tree for sentence 1. gir k1 kitaab Figure 2: Inter-chunk dependency tree of sentence1 The inter-chunk dependency annotation is done following the dependency guidelines in Bharati et al., (2009) that uses a dependency framework inspired by Panini's grammar of Sanskrit (see, Begum et al., 2008 for more details). Subsequent to inter-chunk dependency annotation, intra-chunk annotation is done automatically following the guidelines described in this paper. The final treebank for Hindi would have other layers annotation such as Propbank and Phrase structure. The conversion to phrase structure depends on the expanded version of the treebank (i.e. trees with inter-chunk, as well as, intra-chunk relations marked).Hence, it is important to have high quality complete dependency structure for each sentence, and since inter-chunk annotation is manual, this implies that the process of automati"
W12-3607,W09-3036,1,0.796449,"efforts in advance research and development of NLP tools and applications for Indian languages. Treebanks can be created manually or semiautomatically. Manual creation of treebank is a costly task both in terms of money and time. The annotators follow a set of prescribed guidelines for the annotation task. Semi-automatic creation of treebank involves first running of tools/parsers and then manual correction of errors. An accurate annotating parser/tool saves cost and time for both the annotation as well as the validation task. A multi-layered Hindi treebank is in the process of being created (Bhatt et al., 2009). Dependency treebank forms the first layer in this annotation. To save annotation effort, manual annotation of the dependency relations for Hindi dependency treebank is carried at the inter-chunk level. The intra-chunk relations are marked automatically. The focus of this paper is the task of automatically marking intra-chunk relations. We present both a rule-based and a statistical approach for this expansion process. We call this process ‘expansion’ since the intra-chunk dependencies are made explicit by removing the chunk 49 Proceedings of the 6th Linguistic Annotation Workshop, pages 49–5"
W12-3623,I08-2099,1,0.868192,"95), (Hajiˇc, 1998), (Hajicov´a, 1998), (Oflazer et al., 2003). Not only are dependency-based representations suitable for less configurational languages, they are also favorable for a number of natural language processing applications (Culotta and Sorensen, 2004), (Reichartz et al., 2009). Structural relations like subject and direct object are believed to be less relevant for the grammatical description of Indian languages (ILs) because of the less configurational nature of these languages (Bhat, 1991). Indian languages are morphologically rich and have a relatively free constituent order. (Begum et al., 2008) have argued in favor of using Karaka relations instead of structural relations for the syntactic analysis of ILs. They proposed an annotation scheme for the syntactic treebanking of ILs based on the Computational Paninian Grammar (CPG), a formalism inspired by Paninian grammatical theory. Currently dependency treebanks of four ILs, namely Hindi, Urdu, Bangla and Telegu, are under development following this annotation scheme. The dependency structures in all the four treebanks are, under this annotation scheme, annotated with the Karaka relations. Although English does not belong to the free w"
W12-3623,J95-3006,0,0.517798,"et al., 2002) for English, Alpino (Van der Beek et al., 2002) for Dutch, TUT (Bosco and Lombardo, 2004) for Italian, TIGER (Brants et al., 2002) for German and many others. Currently existing treebanks mainly differ in the grammatical formalism adopted. Dependency based formalism compared with the constituency based formalism is assumed to suit better for representing syntactic structures of free word order languages, its representation does not crucially rely on the position of a syntactic unit in a sentence thus easily handles the scrambling of arguments in such languages (Shieber, 1985), (Bharati et al., 1995), (Hajiˇc, 1998), (Hajicov´a, 1998), (Oflazer et al., 2003). Not only are dependency-based representations suitable for less configurational languages, they are also favorable for a number of natural language processing applications (Culotta and Sorensen, 2004), (Reichartz et al., 2009). Structural relations like subject and direct object are believed to be less relevant for the grammatical description of Indian languages (ILs) because of the less configurational nature of these languages (Bhat, 1991). Indian languages are morphologically rich and have a relatively free constituent order. (Beg"
W12-3623,W09-3036,1,0.856376,"di drawing its higher lexicon from Sanskrit and Urdu from Persian and Arabic) to the point where the two styles/languages become mutually unintelligible. In written form not only lexical items but the way Urdu and Hindi is written makes one believe that they are two separate languages. They are written in separate orthographies, Hindi being written in Devanagari, and Urdu in a modified Perso-Arabic script. Under the treebanking effort for Indian languages, two separate treebanks are being built for both Hindi and Urdu. Among the two, however, Hindi treebank has matured and grown considerably (Bhatt et al., 2009), (Palmer et al., 2009). The paper is arranged as follows, next Section gives a brief overview of the related works on syntactic treebanking. Section 3 describes the grammatical formalism chosen for the annotation. In Section 4 we discuss treebanking pipeline of Urdu followed by some of the Urdu specific issues in Section 5. In Section 6 we discuss the empirical results of interannotator agreement. Section 7, concludes the paper. 2 Related Work A treebank is a text corpus annotated with syntactic, semantic and sometimes even inter sentential relations (Hajiˇcov´a et al., 2010). Treebanks are o"
W12-3623,W04-1501,0,0.0254856,"built 157 Proceedings of the 6th Linguistic Annotation Workshop, pages 157–165, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics and are used for a number of NLP tasks like training and testing syntactic parsers. Owing to their great importance, a number of syntactic treebanking projects have been initiated for many different languages. Among the treebanks include Penn treebank (PTB) (Marcus et al., 1993), Prague Dependency treebank (PDT) (Hajicov´a, 1998) for Czech, (Rambow et al., 2002) for English, Alpino (Van der Beek et al., 2002) for Dutch, TUT (Bosco and Lombardo, 2004) for Italian, TIGER (Brants et al., 2002) for German and many others. Currently existing treebanks mainly differ in the grammatical formalism adopted. Dependency based formalism compared with the constituency based formalism is assumed to suit better for representing syntactic structures of free word order languages, its representation does not crucially rely on the position of a syntactic unit in a sentence thus easily handles the scrambling of arguments in such languages (Shieber, 1985), (Bharati et al., 1995), (Hajiˇc, 1998), (Hajicov´a, 1998), (Oflazer et al., 2003). Not only are dependenc"
W12-3623,J97-1002,0,0.0607265,"greement on the data set will assure that the annotations in UDT are reliable. The data set used contains 2595 head-dependent dependency chains marked with dependency relations belonging to a tag-set of 39 tags. The agreement measured is chunk based; for each chunk in a sentence agreement was measured with regard to its relation with the head it modifies. Inter-annotator agreement was measured using Cohen’s kappa (Cohen and others, 1960) which is the mostly used agreement coefficient for annotation tasks with categorical data. Kappa was introduced to the field of computational linguistics by (Carletta et al., 1997) and since then many linguistics resources have been evaluated using the matrix such as (Uria et al., 2009), (Bond et al., 2008), (Yong and Foo, 1999). The kappa statistics show the agreement between the annotators and the reproducibility of their annotated data sets. Similar results produced by the annotators on a given data set proves the similarity in their understanding of the annotation guidelines. However, a good agreement does not necessarily ensure validity, since annotators can make similar kind of mistakes and errors. The kappa coefficient κ is calculated as: Kappa Statistic &lt;0.00 0."
W12-3623,P04-1054,0,0.0650831,"Dependency based formalism compared with the constituency based formalism is assumed to suit better for representing syntactic structures of free word order languages, its representation does not crucially rely on the position of a syntactic unit in a sentence thus easily handles the scrambling of arguments in such languages (Shieber, 1985), (Bharati et al., 1995), (Hajiˇc, 1998), (Hajicov´a, 1998), (Oflazer et al., 2003). Not only are dependency-based representations suitable for less configurational languages, they are also favorable for a number of natural language processing applications (Culotta and Sorensen, 2004), (Reichartz et al., 2009). Structural relations like subject and direct object are believed to be less relevant for the grammatical description of Indian languages (ILs) because of the less configurational nature of these languages (Bhat, 1991). Indian languages are morphologically rich and have a relatively free constituent order. (Begum et al., 2008) have argued in favor of using Karaka relations instead of structural relations for the syntactic analysis of ILs. They proposed an annotation scheme for the syntactic treebanking of ILs based on the Computational Paninian Grammar (CPG), a forma"
W12-3623,N10-1077,0,0.0307259,"possible shapes a letter can acquire namely initial, medial, f inal form in a connected sequence of letters or an isolated form. The letters acquiring all these four shapes depending on the context of their occurrence are called as joiners. An another set of letters, however, called as non − joiners do not adhere to this four-way shaping. They only join with the letters before them and have only f inal and isolated forms. An ex and a ample of a joiner is Arabic Letter ‘T eh’ H non-joiner is Arabic letter ‘waaw’ ð. The concept of space as a word boundary marker is not present in Urdu writing (Durrani and Hussain, 2010), (Lehal, 2010). Space character is primarily required to generate correct shaping of words. For example a space is necessary within the word Qå  “needy” to generate the visually corYJ Ó H Pð rect and acceptable form of this word. Without 2 http://apps.sanchay.co.in/latest-builds/  Qå  which is visually inspace it appears as YJ Ü ßPð correct. In contrast to this, writers of Urdu find it unnecessary to insert a space between the two words “Urdu Center”, because the correct shap Ó ð XP@ Q»Q ing is produced automatically as the first word ends and Q»Q XP@ Ó ð XP@ Óð with a non-joiner. Therefo"
W12-3623,W10-3606,0,0.0133208,"acquire namely initial, medial, f inal form in a connected sequence of letters or an isolated form. The letters acquiring all these four shapes depending on the context of their occurrence are called as joiners. An another set of letters, however, called as non − joiners do not adhere to this four-way shaping. They only join with the letters before them and have only f inal and isolated forms. An ex and a ample of a joiner is Arabic Letter ‘T eh’ H non-joiner is Arabic letter ‘waaw’ ð. The concept of space as a word boundary marker is not present in Urdu writing (Durrani and Hussain, 2010), (Lehal, 2010). Space character is primarily required to generate correct shaping of words. For example a space is necessary within the word Qå  “needy” to generate the visually corYJ Ó H Pð rect and acceptable form of this word. Without 2 http://apps.sanchay.co.in/latest-builds/  Qå  which is visually inspace it appears as YJ Ü ßPð correct. In contrast to this, writers of Urdu find it unnecessary to insert a space between the two words “Urdu Center”, because the correct shap Ó ð XP@ Q»Q ing is produced automatically as the first word ends and Q»Q XP@ Ó ð XP@ Óð with a non-joiner. Therefore Q»Q look ide"
W12-3623,J93-2004,0,0.0387119,"sentential relations (Hajiˇcov´a et al., 2010). Treebanks are of multifold importance, they are an invaluable resource for testing linguistic theories on which they are built 157 Proceedings of the 6th Linguistic Annotation Workshop, pages 157–165, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics and are used for a number of NLP tasks like training and testing syntactic parsers. Owing to their great importance, a number of syntactic treebanking projects have been initiated for many different languages. Among the treebanks include Penn treebank (PTB) (Marcus et al., 1993), Prague Dependency treebank (PDT) (Hajicov´a, 1998) for Czech, (Rambow et al., 2002) for English, Alpino (Van der Beek et al., 2002) for Dutch, TUT (Bosco and Lombardo, 2004) for Italian, TIGER (Brants et al., 2002) for German and many others. Currently existing treebanks mainly differ in the grammatical formalism adopted. Dependency based formalism compared with the constituency based formalism is assumed to suit better for representing syntactic structures of free word order languages, its representation does not crucially rely on the position of a syntactic unit in a sentence thus easily h"
W12-3623,rambow-etal-2002-dependency,0,0.0352511,"e, they are an invaluable resource for testing linguistic theories on which they are built 157 Proceedings of the 6th Linguistic Annotation Workshop, pages 157–165, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics and are used for a number of NLP tasks like training and testing syntactic parsers. Owing to their great importance, a number of syntactic treebanking projects have been initiated for many different languages. Among the treebanks include Penn treebank (PTB) (Marcus et al., 1993), Prague Dependency treebank (PDT) (Hajicov´a, 1998) for Czech, (Rambow et al., 2002) for English, Alpino (Van der Beek et al., 2002) for Dutch, TUT (Bosco and Lombardo, 2004) for Italian, TIGER (Brants et al., 2002) for German and many others. Currently existing treebanks mainly differ in the grammatical formalism adopted. Dependency based formalism compared with the constituency based formalism is assumed to suit better for representing syntactic structures of free word order languages, its representation does not crucially rely on the position of a syntactic unit in a sentence thus easily handles the scrambling of arguments in such languages (Shieber, 1985), (Bharati et al."
W12-3623,W99-0502,0,0.028982,"th dependency relations belonging to a tag-set of 39 tags. The agreement measured is chunk based; for each chunk in a sentence agreement was measured with regard to its relation with the head it modifies. Inter-annotator agreement was measured using Cohen’s kappa (Cohen and others, 1960) which is the mostly used agreement coefficient for annotation tasks with categorical data. Kappa was introduced to the field of computational linguistics by (Carletta et al., 1997) and since then many linguistics resources have been evaluated using the matrix such as (Uria et al., 2009), (Bond et al., 2008), (Yong and Foo, 1999). The kappa statistics show the agreement between the annotators and the reproducibility of their annotated data sets. Similar results produced by the annotators on a given data set proves the similarity in their understanding of the annotation guidelines. However, a good agreement does not necessarily ensure validity, since annotators can make similar kind of mistakes and errors. The kappa coefficient κ is calculated as: Kappa Statistic &lt;0.00 0.0-0.20 0.21-0.40 0.41-0.60 0.61-0.80 0.81-1.00 Strength of agreement Poor Slight Fair Moderate Substantial Almost perfect Table 1: Coefficients for th"
W13-2320,R09-2001,0,0.101252,"Butt, 2006). They rank nominals on the continuum of control as shown in (1)1 . Nominals marked with Ergative case have highest control and the ones marked with Locative have lowest. Erg > Gen > Inst > Dat > Acc > Loc (1) Of late the systematic correspondences between animacy and linguistic phenomena have been explored for various NLP applications. It has been noted that animacy provides important information, to mention a few, for anaphora resolution (Evans and Orasan, 2000), argument disambiguation (Dell’Orletta et al., 2005), syntactic parsing (Øvrelid and Nivre, 2007; Bharati et al., 2008; Ambati et al., 2009) and verb classification (Merlo and Steven1 Ergative, Genitive, Instrumental, Dative, Accusative and Locative in the given order. 159 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 159–167, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics son, 2001). Despite the fact that animacy could play an important role in NLP applications, its annotation, however, is not usually featured in a treebank or any other annotated corpora used for developing these applications. There are a very few annotation projects that have i"
W13-2320,W10-1411,0,0.213409,"Missing"
W13-2320,I08-2099,1,0.895727,"Missing"
W13-2320,W09-3036,1,0.882786,"iscuss the annotation of nominal expressions with animacy and the motivation for the same, the discussion will follow as: Section 2 gives a brief overview of the Hindi Treebank with all its layers. Section 3 motivates the annotation of nominals with animacy, followed by the annotation efforts and issues encountered in Section 4. Section 5 concludes the paper with a discussion on possible future directions. 2 Description of the Hindi Treebank In the following, we give an overview of the Hindi Treebank (HTB), focusing mainly on its dependency layer. The Hindi-Urdu Treebank (Palmer et al., 2009; Bhatt et al., 2009) is a multi-layered and multi-representational treebank. It includes three levels of annotation, namely two syntactic levels and one lexical-semantic level. One syntactic level is a dependency layer which follows the CPG (Begum 160 et al., 2008), inspired by the P¯an.inian grammatical theory of Sanskrit. The other level is annotated with phrase structure inspired by the Chomskyan approach to syntax (Chomsky, 1981) and follows a binary branching representation. The third layer of annotation, a purely lexical semantic one, encodes the semantic relations following the English PropBank (Palmer et"
W13-2320,Y12-1042,1,0.662535,"=‘s b,n,m,sg,3,d,0,0’> )) (( VGF <fs name=‘VGF’> KAyA VM <fs af=‘KA,v,m,sg,any,,yA,yA’> )) </Sentence> Figure 1: Annotation of an Example Sentence in SSF. Despite the fact that the Hindi Treebank already features a number of layers as discussed above, there have been different proposals to enrich it further. Hautli et al. (2012) proposed an additional layer to the treebank, for the deep analysis of the language, by incorporating the f unctional structure (or f-structure) of Lexical Functional Grammar which encodes traditional syntactic notions such as subject, object, complement and adjunct. Dakwale et al. (2012) have also extended the treebank with anaphoric relations, with a motive to develop a data driven anaphora resolution system for Hindi. Given this scenario, our effort is to enrich the treebank with the animacy annotation. In the following sections, we will discuss in detail, the annotation of the animacy property of nominals in the treebank and the motive for the same. 3 Motivation: In the Context of Dependency Parsing Hindi is a morphologically rich language, grammatical relations are depicted by its morphology via case clitics. Hindi has a morphologically split-ergative case marking system"
W13-2320,W05-0509,0,0.0616397,"Missing"
W13-2320,J01-3003,0,0.0475433,"n the annotated portion of the Hindi Treebank. The high rate of co-occurrence between animacy and dependency relations makes a clear statement about the role animacy can play in parsing. Nominals marked with dependency relations as k1 ‘agent’, k4 ‘recipient’, k4a ‘experiencer’ are largely annotated as human while k3 ‘instrument’ is marked as inanimate, which confirms our conjecture that with animacy information a parser can reliably predict linguistic patterns. Apart from parsing, animacy has been reported to be beneficial for a number of natural language applications (Evans and Orasan, 2000; Merlo and Stevenson, 2001). Following these computational implications of animacy, we started encoded this property of nominals explicitly in our treebank. In the next section, we will present these efforts fol162 lowed by the inter-annotator agreement studies. k1 k2 k3 k4 k4a k5 k7 Human Other-Animates Inanimate n /ne (Erg) 2321 630 108 ko/ko (Dat/Acc) 172 8 135 s /se (Inst) 6 0 14 m /me (Loc) 0 0 7 pr/par (Loc) 0 0 1 kA/kaa (Gen) 135 2 99 φ (Nom) 1052 5 3072 n /ne (Erg) 0 0 0 ko/ko (Dat/Acc) 625 200 226 s /se (Inst) 67 0 88 m /me (Loc) 2 0 6 pr/par (Loc) 5 0 37 kA/kaa (Gen) 15 0 14 φ (Nom) 107 61 2998 n /ne"
W13-2320,E06-3008,0,0.0457852,"Missing"
W13-2320,E09-1072,0,0.0178479,"actic parsing has largely been limited to the use of only a few lexical features. Features like POStags are way too coarser to provide deep information valuable for syntactic parsing while on the other hand lexical items often suffer from lexical ambiguity or out of vocabulary problem. So in oder to assist the parser for better judgments, we need to complement the morphology somehow. A careful observation easily states that a simple world knowledge about the nature (e.g. living-nonliving, artifact, place) of the participants is enough to disambiguate. For Swedish, Øvrelid and Nivre (2007) and Øvrelid (2009) have shown improvement, with animacy information, in differentiation of core arguments of a verb in dependency parsing. Similarly for Hindi, Bharati et al. (2008) and Ambati et al. (2009) have shown that even when the training data is small simple animacy information can boost dependency parsing accuracies, particularly handling the differentiation of core arguments. In Table 5, we show the distribution of animacy with respect to case markers and dependency relations in the annotated portion of the Hindi Treebank. The high rate of co-occurrence between animacy and dependency relations makes a"
W13-2320,J05-1004,0,0.0316182,"al., 2009) is a multi-layered and multi-representational treebank. It includes three levels of annotation, namely two syntactic levels and one lexical-semantic level. One syntactic level is a dependency layer which follows the CPG (Begum 160 et al., 2008), inspired by the P¯an.inian grammatical theory of Sanskrit. The other level is annotated with phrase structure inspired by the Chomskyan approach to syntax (Chomsky, 1981) and follows a binary branching representation. The third layer of annotation, a purely lexical semantic one, encodes the semantic relations following the English PropBank (Palmer et al., 2005). In the dependency annotation, relations are mainly verb-centric. The relation that holds between a verb and its arguments is called a kar.aka relation. Besides kar.aka relations, dependency relations also exist between nouns (genitives), between nouns and their modifiers (adjectival modification, relativization), between verbs and their modifiers (adverbial modification including subordination). CPG provides an essentially syntactico-semantic dependency annotation, incorporating kar.aka (e.g., agent, theme, etc.), non-kar.aka (e.g. possession, purpose) and other (part of) relations. A comple"
W13-2320,thuilier-danlos-2012-semantic,0,0.262735,"Missing"
W13-2320,W04-0216,0,0.451411,"Missing"
W13-3705,I08-2099,1,0.950233,"interest in research based on parallel corpora, “surprisingly little work has been reported on parallel treebanks.” opine Volk et al. (2004). “A parallel treebank comprises syntactically annotated aligned sentences in two or more languages. In addition to this, the trees are aligned on a sub-sentential level.” (Tinsley et al., 2009) In this paper we report our study on parallel English and Hindi dependency treebanks based on the CPG model. The annotation labels used to mark the relations in the example trees here (as also in the treebanks) conform to the dependency annotation scheme given by Begum et al. (2008). An adaptation of this scheme was subsequently used for the English treebank, as reported in (Chaudhry and Sharma, 2011) We detail here, how we make use of the existing Hindi dependency treebank and its parallel English dependency treebank, to study systematic divergences in the treebank pair, given that both of these treebanks use the same dependency grammar formalism. We sought to find here, the types and reasons for these differences. We find that the two treebanks diverge mainly from two aspects: We present, here, our analysis of systematic divergences in parallel EnglishHindi dependency"
W13-3705,J95-3006,0,0.861604,"ped at IIIT-H, under the Hindi-Urdu Treebank (HUTB) project) was translated to English to form a parallel corpus. The English treebank used here (reported in (Chaudhry and Sharma, 2011)), has been developed on these translations and has 1143 sentences annotated using the dependency annotation scheme modeled on the CPG framework (Begum et al., 2008) (as also the Hindi treebank used here). 2.2 The Annotation Scheme As mentioned earlier, the annotation scheme used for the creation of the two parallel dependency treebanks (English and Hindi) is based on CPG, a dependency grammar model proposed by Bharati et al. (1995). This annotation scheme, developed for Hindi and other Indian languages, by Begum et al. (2008) was later applied to English first by Vaidya et al. (2009) and then, by Chaudhry and Sharma (2011) to develop their English dependency treebank (used for this work). Paninian Grammar assigns ‘karaka’ (verb argument relations) to arguments in a sentence, based on the relationship they have with the verb. “karaka relations are syntactico-semantic (or semanticosyntactic) relations between the verbals and other related constituents in a sentence.” (Bharati et al., 1995). There are six basic karakas, na"
W13-3705,W09-3036,1,0.927799,"larities in the two treebanks, and discuss our investigations into the reasons behind them. Section 4 presents our observations. Section 5 presents the task of our alignment of the two treebanks, where in we talk about our extraction of divergent structures in the trees, and discusses the results of this exercise. And, in Sections 6, we conclude and sketch the possibilities for some future work in this direction. 2 2.1 Methodology The Data The data for this study comprises a set of parallel English and Hindi dependency treebanks. A small section (25000 words) of the Hindi dependency treebank (Bhatt et al., 2009) (being de34 veloped at IIIT-H, under the Hindi-Urdu Treebank (HUTB) project) was translated to English to form a parallel corpus. The English treebank used here (reported in (Chaudhry and Sharma, 2011)), has been developed on these translations and has 1143 sentences annotated using the dependency annotation scheme modeled on the CPG framework (Begum et al., 2008) (as also the Hindi treebank used here). 2.2 The Annotation Scheme As mentioned earlier, the annotation scheme used for the creation of the two parallel dependency treebanks (English and Hindi) is based on CPG, a dependency grammar m"
W13-3705,georgi-etal-2012-measuring,0,0.020115,"cheme, a chunk (with boundaries marked), by definition, represents a group of adjacent words in a sentence, which are in dependency relation with each other, and where one of them is their head (Mannem et al., 2009). 2.3 Procedure For the purpose of a detailed comparative study of the two treebanks, about 700 sentence pairs from the two treebanks were manually aligned at sentence level and the trees were then aligned automatically. “A sentence pair is a pair of sentences which are translations of each other, and the dependency trees for the two sentences in a sentence pair form a tree pair.” (Georgi et al., 2012) After this, various instances of the dependency relations in the parallel sentences were automatically extracted for the study. We then (manually) compared the tree pairs as regards their similarities and contrasts. The comparisons were made not just for their spans as complete trees, but also at the level of their subtrees. Given a sentence pair, we first observed the entire tree spans for potential divergences. And, if they were divergent, we looked further on, to find where they diverged, followed by how much they diverged, and why. This has been discussed in detail in section 5. We sought"
W13-3705,2007.tmi-papers.11,0,0.0217915,"istic • Structural A good example of stylistic variation or translator preference, from our data would be: Introduction Treebanks play an increasingly important role in computational linguistics, and with the availability of a number of treebanks of various languages now, studies based on parallel treebanks are one of the directions application/use of treebanks has taken. “Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules” (Hearne et al., 2007) and so on. However, though recent years 33 (1) kendrIya sarkAr-ke anek varishtha netA bhI mojUd the. kendriiya sarkaar-ke anek varishtha ruling party-of many senior netaa bhii mojuud the leaders also in-attendance were ‘A number of senior leaders from the ruling party were also in attendance.’ Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 33–40, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. The Hindi sentence in example (1), has been translated as ‘A number of senior leaders from the ru"
W13-3705,P09-3002,1,0.847153,"uctions etc., too are therefore, taken care of, using the relational concepts prescribed by this annotation scheme. Table 1 provides information about the relation labels (from the two treebanks) referred to, in this work. The dependency relations are marked at interchunk level, instead of marking relations between words. So, function words are attached to (chunked with) their lexical heads. Per this scheme, a chunk (with boundaries marked), by definition, represents a group of adjacent words in a sentence, which are in dependency relation with each other, and where one of them is their head (Mannem et al., 2009). 2.3 Procedure For the purpose of a detailed comparative study of the two treebanks, about 700 sentence pairs from the two treebanks were manually aligned at sentence level and the trees were then aligned automatically. “A sentence pair is a pair of sentences which are translations of each other, and the dependency trees for the two sentences in a sentence pair form a tree pair.” (Georgi et al., 2012) After this, various instances of the dependency relations in the parallel sentences were automatically extracted for the study. We then (manually) compared the tree pairs as regards their simila"
W13-3705,W04-1910,0,0.156843,"Missing"
W13-3725,2007.tmi-papers.11,0,0.0285012,"n top of their parsing information. And, with the availability of a number of treebanks of various languages now, parallel treebanks are being put to use for analysis and further experiments. A parallel treebank comprises syntactically annotated aligned sentences in two or more languages. In addition to this, the trees are aligned on a sub-sentential level. (Tinsley et al., 2009). Further, such resources could be useful for many applications, e.g. as training or evaluation corpora for word/phrase alignment, as also for data driven MT systems and for the automatic induction of transfer rules. (Hearne et al., 2007) Our work using two parallel dependency treebanks is another such effort in this direction. It includes: The paper presents our work on the annotation of intra-chunk dependencies on an English treebank that was previously annotated with Inter-chunk dependencies, and for which there exists a fully expanded parallel Hindi dependency treebank. This provides fully parsed dependency trees for the English treebank. We also report an analysis of the inter-annotator agreement for this chunk expansion task. Further, these fully expanded parallel Hindi and English treebanks were word aligned and an anal"
W13-3725,W12-3607,1,0.838922,"these chunks) and intra-chunk Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 227–235, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. dependency annotation (relations between words inside the chunk). Some notable efforts in this direction include the automatic intra-chunk dependency annotation of an inter-chunk annotated Hindi dependency treebank, wherein they present both, a rule-based and a statistical approach to automatically mark intra-chunk dependencies on an existing Hindi treebank (Kosaraju et al., 2012). Zhou (2008) worked on the expansion of the chunks in the Chinese treebank TCT (Qiang, 2004) through automatic rule acquisition. The remainder of the paper is organized as follows: In Section 2, we give an overview of the two dependency treebanks used for our work, and their development. Section 3 describes the guidelines for intra-chunk dependency annotation. In Section 4, we talk about issues with the expansion and our resolutions for them. Further, it presents the results of the inter-annotator agreement. Section 5 comprises our work on alignment of parallel Hindi-English corpora and the i"
W13-3725,P09-3002,0,0.0178024,"rce language tree to a target language tree offers a mechanism to preserve the 227 1. Intra-chunk expansion of the English treebank previously annotated with Inter-chunk dependencies, for which there exists a fully expanded parallel Hindi dependency treebank. 2. An analysis of the inter-annotator agreement for the chunk expansion task mentioned in (1) above. 3. Alignment of the two treebanks at sentence and also at word level. A chunk, by definition, represents a set of adjacent words in a sentence, which are in dependency relation with each other, and where one of these words is their head. (Mannem et al., 2009). The task of dependency annotation is thus divided into: inter-chunk dependency annotation (relations between these chunks) and intra-chunk Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 227–235, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. dependency annotation (relations between words inside the chunk). Some notable efforts in this direction include the automatic intra-chunk dependency annotation of an inter-chunk annotated Hindi dependency treebank, wherein they present both, a rule-"
W13-3725,J93-2004,0,0.0419203,"ing of the phrase was being realized in the sentence via some other word that belongs to a category different from the category of the source-word. Here, it is not possible to align individual words due to position, word-order, nature of MWE (literal/metaphorical) and other issues which arise due to syntactic differences between the two languages. 5.1.3 POS Tag For the purpose of analysis and manual alignment quality evaluation, the POS tag mappings were 234 recorded in a table (Table-5). The POS tagsets are different for English and Hindi treebanks. The English Treebank uses Penn POS Tagset (Marcus et al., 1993), while Hindi treebank is annotated as per Bharati et al. (2006). Keeping the large number of POS categories and differences in tagsets in view, some POS category columns have been merged in the table. For Ex. We merged the categories for JJ,JJR,JJS into J (Adjective) for English POS tagset and JJC,JJ into J for Hindi POS tagset for comparison and error analysis over broad syntactic categories. Major categories such as verbs, adjectives, adverbs, nouns etc. have a considerable mapping ratio in the word aligned data. All the odd POS alignment pairs, such as PostPosition-Determiner, Verb-Preposi"
W13-3725,I08-2099,1,0.848805,"sh treebank is much smaller in size, with around 1000 sentence (nearly 20K words) as compared to its Hindi counterpart which has about 22800 sentences (nearly 450K words). There is a difference in size of nearly 1000 words between the English treebank and its parallel Hindi treebank from which it was translated. This is because the translations involve both literal and stylistic variations. The annotation labels used to mark the relations in the treebank conform to the dependency annotation scheme reported in Chaudhry and Sharma (2011), which is an adaptation of the annotation scheme given by Begum et al. (2008), for Hindi. Further, as per these annotation schemes, dependency relations in the treebank are marked at chunk level (between chunk heads), instead of 228 being marked between words. The Hindi treebank also had intra-chunk dependency relations marked on it, along with the interchunk dependencies. And since the English dependency treebank used here, is relatively much newer, there was scope for further work on it. We thus expanded this treebank at intra-chunk level, annotating each node within the chunk with its dependency label/information. Annotating the English treebank with this informatio"
W13-3725,P08-1066,0,0.0180423,"ncreasing interest in research based on parallel corpora. Statistical machine translation systems use parallel text corpora to learn pattern-based rules. These rules can be simple or sophisticated, based on the level of information present in the corresponding parallel corpus. Earlier research in statistical MT utilized just sentence and lexical alignment (Brown et al., 1990) which requires merely a sentence and word aligned parallel text. Later, to acquire these rules the alignment of a parsed structure in one language with a raw string in the other language emerged (Yamada and Knight, 2001; Shen et al., 2008). Of late, the focus has been on exploring these rules from the alignment of source/target language parse trees (Zhang et al., 2008; Cowan, 2008). Also, mapping from a source language tree to a target language tree offers a mechanism to preserve the 227 1. Intra-chunk expansion of the English treebank previously annotated with Inter-chunk dependencies, for which there exists a fully expanded parallel Hindi dependency treebank. 2. An analysis of the inter-annotator agreement for the chunk expansion task mentioned in (1) above. 3. Alignment of the two treebanks at sentence and also at word level"
W13-3725,J95-3006,0,0.70399,"our work, and their development. Section 3 describes the guidelines for intra-chunk dependency annotation. In Section 4, we talk about issues with the expansion and our resolutions for them. Further, it presents the results of the inter-annotator agreement. Section 5 comprises our work on alignment of parallel Hindi-English corpora and the issues related to that. We conclude and propose some future works towards the end of the paper. 2 Treebanks We make use of the English dependency treebank (reported in Chaudhry and Sharma (2011)), developed on the Computational Paninian Grammar (CPG) model (Bharati et al., 1995), for this work. This treebank is parallel to a section of the Hindi Dependency treebank (reported in Bhatt et al. (2009)) being developed under the HindiUrdu Treebank (HUTB) Project and was created by translating the sentences from HUTB. The English treebank is much smaller in size, with around 1000 sentence (nearly 20K words) as compared to its Hindi counterpart which has about 22800 sentences (nearly 450K words). There is a difference in size of nearly 1000 words between the English treebank and its parallel Hindi treebank from which it was translated. This is because the translations invol"
W13-3725,W09-3036,1,0.829707,"talk about issues with the expansion and our resolutions for them. Further, it presents the results of the inter-annotator agreement. Section 5 comprises our work on alignment of parallel Hindi-English corpora and the issues related to that. We conclude and propose some future works towards the end of the paper. 2 Treebanks We make use of the English dependency treebank (reported in Chaudhry and Sharma (2011)), developed on the Computational Paninian Grammar (CPG) model (Bharati et al., 1995), for this work. This treebank is parallel to a section of the Hindi Dependency treebank (reported in Bhatt et al. (2009)) being developed under the HindiUrdu Treebank (HUTB) Project and was created by translating the sentences from HUTB. The English treebank is much smaller in size, with around 1000 sentence (nearly 20K words) as compared to its Hindi counterpart which has about 22800 sentences (nearly 450K words). There is a difference in size of nearly 1000 words between the English treebank and its parallel Hindi treebank from which it was translated. This is because the translations involve both literal and stylistic variations. The annotation labels used to mark the relations in the treebank conform to the"
W13-3725,J90-2002,0,0.398338,"is for the task has been given. Issues related to intra-chunk expansion and alignment for the language pair HindiEnglish are discussed and guidelines for these tasks have been prepared and released. 1 Introduction Recent years have seen an increasing interest in research based on parallel corpora. Statistical machine translation systems use parallel text corpora to learn pattern-based rules. These rules can be simple or sophisticated, based on the level of information present in the corresponding parallel corpus. Earlier research in statistical MT utilized just sentence and lexical alignment (Brown et al., 1990) which requires merely a sentence and word aligned parallel text. Later, to acquire these rules the alignment of a parsed structure in one language with a raw string in the other language emerged (Yamada and Knight, 2001; Shen et al., 2008). Of late, the focus has been on exploring these rules from the alignment of source/target language parse trees (Zhang et al., 2008; Cowan, 2008). Also, mapping from a source language tree to a target language tree offers a mechanism to preserve the 227 1. Intra-chunk expansion of the English treebank previously annotated with Inter-chunk dependencies, for w"
W13-3725,P01-1067,0,0.0186198,"cent years have seen an increasing interest in research based on parallel corpora. Statistical machine translation systems use parallel text corpora to learn pattern-based rules. These rules can be simple or sophisticated, based on the level of information present in the corresponding parallel corpus. Earlier research in statistical MT utilized just sentence and lexical alignment (Brown et al., 1990) which requires merely a sentence and word aligned parallel text. Later, to acquire these rules the alignment of a parsed structure in one language with a raw string in the other language emerged (Yamada and Knight, 2001; Shen et al., 2008). Of late, the focus has been on exploring these rules from the alignment of source/target language parse trees (Zhang et al., 2008; Cowan, 2008). Also, mapping from a source language tree to a target language tree offers a mechanism to preserve the 227 1. Intra-chunk expansion of the English treebank previously annotated with Inter-chunk dependencies, for which there exists a fully expanded parallel Hindi dependency treebank. 2. An analysis of the inter-annotator agreement for the chunk expansion task mentioned in (1) above. 3. Alignment of the two treebanks at sentence an"
W13-3725,P08-1064,0,0.0245378,"n pattern-based rules. These rules can be simple or sophisticated, based on the level of information present in the corresponding parallel corpus. Earlier research in statistical MT utilized just sentence and lexical alignment (Brown et al., 1990) which requires merely a sentence and word aligned parallel text. Later, to acquire these rules the alignment of a parsed structure in one language with a raw string in the other language emerged (Yamada and Knight, 2001; Shen et al., 2008). Of late, the focus has been on exploring these rules from the alignment of source/target language parse trees (Zhang et al., 2008; Cowan, 2008). Also, mapping from a source language tree to a target language tree offers a mechanism to preserve the 227 1. Intra-chunk expansion of the English treebank previously annotated with Inter-chunk dependencies, for which there exists a fully expanded parallel Hindi dependency treebank. 2. An analysis of the inter-annotator agreement for the chunk expansion task mentioned in (1) above. 3. Alignment of the two treebanks at sentence and also at word level. A chunk, by definition, represents a set of adjacent words in a sentence, which are in dependency relation with each other, and w"
W13-3725,I08-2079,0,0.0299429,"chunk Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 227–235, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. dependency annotation (relations between words inside the chunk). Some notable efforts in this direction include the automatic intra-chunk dependency annotation of an inter-chunk annotated Hindi dependency treebank, wherein they present both, a rule-based and a statistical approach to automatically mark intra-chunk dependencies on an existing Hindi treebank (Kosaraju et al., 2012). Zhou (2008) worked on the expansion of the chunks in the Chinese treebank TCT (Qiang, 2004) through automatic rule acquisition. The remainder of the paper is organized as follows: In Section 2, we give an overview of the two dependency treebanks used for our work, and their development. Section 3 describes the guidelines for intra-chunk dependency annotation. In Section 4, we talk about issues with the expansion and our resolutions for them. Further, it presents the results of the inter-annotator agreement. Section 5 comprises our work on alignment of parallel Hindi-English corpora and the issues related"
W14-4006,N03-1017,0,0.016323,"core an improvement over the earlier baseline of 20.04 BLEU points. Baseline Components Baseline Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 Number of Development Sentences 500 4 Re-Ranking Experiments In this section, we describe the results of reranking the output of the translation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the basel"
W14-4006,P07-1092,0,0.030931,"med entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target lang"
W14-4006,P05-1066,0,0.154505,"Missing"
W14-4006,P00-1056,0,0.19918,"Model and Corpus Statistics We have used the ILCI corpora (Choudhary and Jha, 2011) for our experiments, which contains English-Hindi parallel sentences from tourism and health domain. We randomly divided the data into training (48970), development (500) and testing (500) sentences and for language modelling we used news corpus of English which is distributed as a part of WMT’14 translation task. The data is about 3 million sentences which also contains MT training data. We trained a phrase based (Koehn et al., 2003) MT system using the Moses toolkit with wordalignments extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model for the first stage of decoding. The result of this baseline system is shown in Table 1. 3.2 Number of Development Sentences 500 4 Re-Ranking Experiments In this section, we describe the results of reranking the output of the translation model using Recurrent Neural Networks (RNN) based language models using the same data which is used for language modelling in the baseline models. Unlike traditional n-gram based discrete language models, RNN do not make the Markov"
W14-4006,W14-3308,0,0.0166938,"eural Network based language model with different morphological features and second, we explore the use of lexical resources such as WordNet to overcome sparsity of content words. 1 2 Related Work In this paper, we present our efforts of reranking the n-best hypotheses produced by a PBMT (Phrase-Based MT) system using RNNLM (Mikolov et al., 2010) in the context of an EnglishHindi SMT system. The re-ranking task in machine translation can be defined as re-scoring the n-best list of translations, wherein a number of language models are deployed along with features of source or target language. (Dungarwal et al., 2014) described the benefits of re-ranking the translation hypothesis using simple n-gram based language model. In recent years, the use of RNNLM have shown significant improvements over the traditional n-gram models (Sundermeyer et al., 2013). (Mikolov et al., 2010) and (Liu et al., 2014) have shown significant improvements in speech recognition accuracy using RNNLM . Shi (2012) also showed the benefits of using RNNLM with contextual and linguistic features. We have also explored the use of morphological features (Hindi being a morphologically rich language) in RNNLM and deduced that these feature"
W14-4006,I08-1067,0,0.0140143,"t synset IDs. 5.2 Combining MT Models 6.1 Combination based on confusion networks We used the tool MANY (Barrault, 2010) for system combination. However, since the tool is configured to work with TERp evaluation metric, we modified it to use METEOR (Gupta et al., 2010) metric since it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is better correlated to human evaluation for morphologically rich Indian Languages. Factored Model Techniques such as factored modelling (Koehn and Hoang, 2007) are quite beneficial for Translation from English to Hindi language as shown by (Ramanathan et al., 2008). When we replace words in a source sentence with the synset IDˆas, we tend to lose morphological information associated with that word. We add inflections as features in a factored SMT model to minimize the impact of this replacement. 6.2 Linearly Interpolated Combination In this approach, we combined phrase-tables of the two models (Eng (sysnset) - Hindi and Baseline) using linear interpolation. We combined the two models with uniform weights – 0.5 for each model, in our case. We again tuned this model with the new interpolated phrase-table using standard algorithm MERT. We show the results"
W14-4006,W14-1002,0,0.0339724,"Missing"
W14-4006,I13-1029,0,0.0203587,"ror in source language, named entities, combination forms (e.g. widebody) and abbreviations. Apart from these issues, Hindi being a low-resourced language in terms of parallel corpora suffers from data sparsity. In the second part of the paper, we address the problem of data sparsity with the help of English WordNet (EWN) for English-Hindi PB-SMT. We increase the coverage of content words (excluding Named-Entities) by incorporating sysnset information in the source sentences. Combining Machine Translation (MT) systems has become an important part of statistical MT in past few years. Works by (Razmara and Sarkar, 2013; Cohn and Lapata, 2007) have shown that there is an increase in phrase coverage when combining different systems. To get more coverage of unigrams in phrase-table, we have explored system combination approaches to combine models trained with synset information and without synset information. We have explored two methodologies for system combination based on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language t"
W14-4006,P07-1040,0,0.0285939,"ty 5.1.1 Intra-Category Sense Selection First Sense: Among the different senses,we select the first sense listed in EWN corresponding to the POS-tag of a given lexical item. The choice is motivated by our observation that the senses of a We have used the HCU morph-analyzer. 53 6 lexical item are ordered in the descending order of their frequencies of usage in the lexical resource. Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There are two dominant approaches. (1) a system combination approach based on confusion networks (CN) (Rosti et al., 2007), which can work dynamically in combining the systems. (2) Combine the models by linearly interpolating and then using MERT to tune the combined system. Merged Sense: In this approach, we merge all the senses listed in EWN corresponding to the POS-tag of the given lexical item. The motivation behind this strategy is that the senses in the EWN for a particular word-POS pair are too finely classified resulting in classification of words that may represent the same concept, are classified into different synsets. For example : travel and go can mean the same concept in a similar context but the fi"
W14-4006,2010.amta-papers.13,0,0.0608694,"Missing"
W14-4006,P03-1054,0,0.00744186,"d on confusion matrix(dynamic) (Ghannay et al., 2014) and mixing models (Cohn and Lapata, 2007). 3 3.1 Number of Evaluation Sentences 500 BLEU 20.04 Table 1: Baseline Scores for Phrase-based Moses Model shown that pre-ordering source language to conform to target language word order significantly improves translation quality (Collins et al., 2005). We created a re-ordering module for transforming an English sentence to be in the Hindi order based on reordering rules provided by Anusaaraka (Chaudhury et al., 2010). The reordering rules are based on parse output produced by the Stanford Parser (Klein and Manning, 2003). The transformation module requires the text to contain only surface form of words, however, we extended it to support surface form along with its factors such as lemma and Part of Speech (POS). Input : the girl in blue shirt is my sister Output : in blue shirt the girl is my sister. Hindi : neele shirt waali ladki meri bahen hai ( blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux) With this transformation, the English sentence is structurally closer to the Hindi sentence which leads to better phrase alignments. The model trained with the transformed corpus produces a new baseline score of 21.84 BL"
W14-4006,D07-1091,0,\N,Missing
W14-4206,C12-2029,0,0.0269227,"Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging"
W14-4206,W10-1810,1,0.786556,"identified as noun-verb complex predicates (NVC) at the dependency level. Typically, a noun-verb complex predicate chorii ‘theft’ karnaa ‘to do’ has two components: a noun chorii and a light verb karnaa giving us the meaning ‘steal’. The verbal component in NVCs has reduced predicating power (although it is inflected for person, number, and gender agreement as well as tense, aspect and mood) and its nominal complement is considered the true predicate. In our annotation of NVCs, we follow a procedure common to all PropBanks, where we create frame files for the nominal or the ‘true’ predicate (Hwang et al., 2010). An example of a frame file for a noun such as chorii is described in Table (9). The creation of a frame file for the set of true predicates that occur in an NVC is important from the point of view of linguistic annotation. Given the large number of NVCs, a semiautomatic method has been proposed for creating Hindi nominal frame files, which saves the manual effort required for creating frames for nearly 6 light verb: kar‘do; to steal’ person who steals thing stolen light verb: ho ‘be/become; to get stolen’ thing stolen 5 Conclusion In this paper we have exploited the overlap between the lexic"
W14-4206,N13-1131,0,0.0210816,"PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging than a typical documen"
W14-4206,Q14-1003,0,0.0134097,"file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging than a typical document level language identification problem. The"
W14-4206,malik-etal-2010-transliterating,0,0.0726245,"Missing"
W14-4206,W12-3623,1,0.45342,"u vocabulary just by using letter-based heuristics. For example neither Arabic nor Persian has aspirated consonants like bH , ph Aspirated Bilabial Plosives; tSh , dZH Aspirated Alveolar Fricatives; ãH Aspirated Retroflex Plosive; gH , kh Aspirated Velar Plosives etc. while Hindi does. Similarly, the following sounds occur only in Arabic and Persian: Z Fricative Postalveolar; T, D Fricative Dental; è Fricative Pharyngeal; X Fricative Uvular etc. Using these heuristics we could identify 2,682 types as Indic, and 3,968 as either Persian or Arabic out of 12,223 unique types in the Urdu treebank (Bhat and Sharma, 2012). = arg max p(w|ci ) ∗ p(ci ) (1) ci The prior distribution p(c) of a class is estimated from the respective training sets shown in Table (3). Each training set is used to train a separate letter-based language model to estimate the probability of word w. The language model p(w) is implemented as an n-gram model using the IRSTLM-Toolkit (Federico et al., 2008) with Kneser-Ney smoothing. The language model is defined as: p(w) = n Y i=1 i−1 p(li |li−k ) (2) where, l is a letter and k is a parameter indicating the amount of context used (e.g., k = 4 means 5-gram model). 3.2 Etymological Data In o"
W14-4206,W09-3036,1,0.783639,"ni}@colorado.edu, taf seer@dsu.edu.pk Abstract drawing its higher lexicon from Sanskrit and Urdu from Persian and Arabic) to the point where the two styles/languages become mutually unintelligible. In written form, not only the vocabulary but the way Urdu and Hindi are written makes one believe that they are two separate languages. They are written in separate orthographies, Hindi being written in Devanagari, and Urdu in a modified Persio-Arabic script. Given such (apparent) divergences between the two varieties, two parallel treebanks are being built under The Hindi-Urdu treebanking Project (Bhatt et al., 2009; Xia et al., 2009). Both the treebanks follow a multi-layered and multi-representational framework which features Dependency, PropBank and Phrase Structure annotations. Among the two treebanks the Hindi treebank is ahead of the Urdu treebank across all layers. In the case of PropBanking, the Hindi treebank has made considerable progress while Urdu PropBanking has just started. The creation of predicate frames is the first step in PropBanking, which is followed by the actual annotation of verb instances in corpora. In this paper, we look at the possibility of porting related frames from Arabic"
W14-4206,palmer-etal-2008-pilot,1,0.813864,"for Urdu simple verbs. There were no significant differences found between the Urdu and Hindi rolesets, which describe either semantic variants of the same verb or its causative forms. Further, in order to name the frame files with their corresponding Urdu lemmas, we used Konstanz’s Urdu transliteration scheme As discussed in Section 2.1.1, the creation of predicate frames precedes the actual annotation of verb instances in a given corpus. In this section, we describe our approach towards the first stage of Urdu PropBanking by adapting related predicate frames from Arabic and Hindi PropBanks (Palmer et al., 2008; Vaidya et al., 2011). Since a PropBank is not available for Persian, we could only adapt those predicate frames which are shared with Arabic and Hindi. Although, Urdu shares or borrows most of its literary vocabulary from Arabic and Persian, it retains its simple verb (as opposed to compound or complex verbs) inventory from Indo-Aryan ancestry. Verbs from Arabic and Persian are borrowed less frequently, although there are examples such 5 Borrowed verbs often do not function as simple verbs rather they are used like nominals in complex predicate constructions such as mehsoos in ‘mehsoos karna"
W14-4206,choi-etal-2010-propbank,1,0.860681,"as the object in the other. This is the primary difference between PropBank’s approach to semantic role labels and the Paninian approach to karaka labels, de.01 to give Arg0 Arg1 Arg2 the giver thing given recipient Table 1: A Frame File The annotation process for the PropBank takes place in two stages: the creation of frame files for individual verb types, and the annotation of predicate argument structures for each verb instance. The annotation for each predicate in the corpus is carried out based on its frame file definitions. 48 The PropBank makes use of two annotation tools viz. Jubilee (Choi et al., 2010b) and Cornerstone (Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the prob"
W14-4206,choi-etal-2010-propbank-instance,1,0.818522,"as the object in the other. This is the primary difference between PropBank’s approach to semantic role labels and the Paninian approach to karaka labels, de.01 to give Arg0 Arg1 Arg2 the giver thing given recipient Table 1: A Frame File The annotation process for the PropBank takes place in two stages: the creation of frame files for individual verb types, and the annotation of predicate argument structures for each verb instance. The annotation for each predicate in the corpus is carried out based on its frame file definitions. 48 The PropBank makes use of two annotation tools viz. Jubilee (Choi et al., 2010b) and Cornerstone (Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the prob"
W14-4206,N13-1031,0,0.0330134,"Missing"
W14-4206,W11-0403,1,0.880982,". There were no significant differences found between the Urdu and Hindi rolesets, which describe either semantic variants of the same verb or its causative forms. Further, in order to name the frame files with their corresponding Urdu lemmas, we used Konstanz’s Urdu transliteration scheme As discussed in Section 2.1.1, the creation of predicate frames precedes the actual annotation of verb instances in a given corpus. In this section, we describe our approach towards the first stage of Urdu PropBanking by adapting related predicate frames from Arabic and Hindi PropBanks (Palmer et al., 2008; Vaidya et al., 2011). Since a PropBank is not available for Persian, we could only adapt those predicate frames which are shared with Arabic and Hindi. Although, Urdu shares or borrows most of its literary vocabulary from Arabic and Persian, it retains its simple verb (as opposed to compound or complex verbs) inventory from Indo-Aryan ancestry. Verbs from Arabic and Persian are borrowed less frequently, although there are examples such 5 Borrowed verbs often do not function as simple verbs rather they are used like nominals in complex predicate constructions such as mehsoos in ‘mehsoos karnaa’ to feel. 52 (Malik"
W14-4206,W13-1018,1,0.895298,"Missing"
W14-4206,kingsbury-palmer-2002-treebank,1,\N,Missing
W14-4206,D13-1084,0,\N,Missing
W14-4211,W14-1002,0,0.0239039,"Missing"
W14-4211,P08-2021,0,0.0248758,"rm weights. The combined triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare the approaches on our da"
W14-4211,P07-2045,0,0.0055531,"our experiment, the baseline translation model used was the direct system between the source and target languages which was trained on the same amount of data as the triangulated models. The parallel corpora for 4 Indian languages namely Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla (bn) was taken from Indian Languages Corpora Initiative (ILCI) (Choudhary and Jha, 2011) . The parallel corpus used in our experiments belonged to two domains - health and tourism and the training set consisted of 28000 sentences. The development and evaluation set contained 500 sentences each. We used MOSES (Koehn et al., 2007) to train the baseline Phrase-based SMT system for all the language pairs on the above mentioned parallel corpus as training, development and evaluation data. Trigram language models were trained using SRILM (Stolcke and others, 2002). Table 1 below shows the BLEU score for all the trained pairs. 4.1 Language Pair bn-mt mt-bn bn-gj gj-mt gj-bn mt-gj hn-mt hn-bn bn-hn mt-hn hn-gj gj-hn Phrase-table triangulation Our emphasis is on building an enhanced phrase table that incorporates the translation phrase tables of different models. This combined phrase table will be used by the decoder during t"
W14-4211,P07-1092,0,0.0487476,"not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 4 among these two when we have limitations on training data which in our case was small and restricted to a smal"
W14-4211,W10-1747,0,0.0140444,"ed triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare the approaches on our data, we performed experi"
W14-4211,I13-1029,0,0.070733,"wever the number of parallel sentences is still not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 4 among these two when we have limitations on training data which"
W14-4211,P07-1040,0,0.0193981,"se-table using uniform weights. The combined triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 4.2.3 In order to compare th"
W14-4211,N07-1061,0,0.0374704,"ntext. Each source phrase s is first translated to an intermediate (pivot) language i, and then to a target language t. This two stage translation process is termed as triangulation. Our basic approach involved making triangulated models by triangulating through different pivots and then interpolating triangulated models with the direct source-target model to make our combined model. In line with various previous works, we will be using multiple translation models to overcome the problems faced due to data sparseness and increase translational coverage. Rather than using sentence translation (Utiyama and Isahara, 2007) from source to pivot and then pivot to target, a phrase based translation model is built. Hence the main focus of our approach is on phrases rather than on sentences. Instead of using combination techniques on the output of several translation systems, we constructed a combined phrase table to be used by the decoder thus avoiding the additional inefficiencies observed while merging the output of various translation systems. Our method focuses on exploiting the availability of multi-parallel data, albeit small in size, to improve the phrase coverage and quality of our SMT system. Our approach"
W14-4211,D09-1141,0,\N,Missing
W14-5125,W03-0428,0,0.07729,"Missing"
W14-5125,P10-3015,0,0.0310825,"Missing"
W14-5125,dinh-etal-2008-word,0,\N,Missing
W14-5125,W03-0432,0,\N,Missing
W14-5146,I08-2099,1,0.802491,"Missing"
W14-5208,agarwal-etal-2012-gui,1,0.829058,"annotation, even going back to an earlier stage, if necessary, to correct mistakes. It allows all the annotation information to be situated in one contiguous place. The interface uses the Java API for SSF, which is perhaps the most developed among the different APIs for SSF. The API (a part of Sanchay) again allows transparency for the programmer as far as manipulating the data is concerned. It also ensures that there are fewer bugs when new programmers work on any part of the system where SSF data is being used. One recent addition to the interface was a GUI to correct mistakes in treebanks (Agarwal et al., 2012). The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the i"
W14-5208,I08-2099,1,0.812263,"r every module. Readability of SSF helps in development and debugging because the input and output of any module can be easily seen and read by humans, whether linguists or programmers. Even if a module fails, SSF helps to run the modules without any effect on normal operation of system. In such a case, the output SSF would have unfilled value of an attribute and downstream modules continue to operate on the data stream. 6.2 Annotation Interfaces and Other Tools Sanchay, mentioned above, has a syntactic annotation interface that has been used for development of treebanks for Indian languages (Begum et al., 2008). These treebanks have been one of the primary sources of information for the development the Sampark machine translation systems, among other things. This syntactic annotation interface provides facilities for everything that is required to be done to transform the selected data in the raw text format to the final annotated treebank. The usual stages of annotation include POS tagging, morphological annotation, chunking and dependency annotation. This interface has evolved over a period of several years based on the feedback received from the annotators and other users. There are plans to use"
W14-5208,J95-3006,0,0.42936,"Kannada and 4 bi-directional systems between Tamil and Malayalam / Telugu. Out of these, 8 pairs have been exposed via a web interface. A REST API is also available to acess the machine translation system over the Internet. 6 http://sanchay.co.in There are 22 constitutionally recognized languages in India, and many more which are not recognized. Hindi, Bengali, Telugu, Marathi, Tamil and Urdu are among the major languages of the world in terms of number of speakers, summing up to a total of 850 million. 8 73 http://sampark.org.in 7 The Sampark system uses Computational Paninian Grammar (CPG) (Bharati et al., 1995), in combination with machine learning. Thus, it is a hybrid system using both rule-based and statistical approaches. There are 13 major modules that together form a hybrid system. The machine translation system is based on the analyze-transfer-generate paradigm. It starts with an analysis of the source language sentence. Then a transfer of structure and vocabulary to target language is carried out. Finally the target language is generated. One of the benefits of this approach is that the language analyzer for a particular language can be developed once and then be combined with generators for"
W14-5208,W02-0109,0,0.181672,"riously threaten the sustainability of the system. This may apply to all large software systems, but the complexities associated with humans languages (both within and across languages) only add to the problem. To make it possible to build various components of an infrastructure that scales within and across languages for a wide variety of purposes, and to be able to do it by re-using the representation(s) and the code, deserves to be considered an achievement. GATE1 (Cunningham et al., 2011; Li et al., 2009), UIMA2 (Ferrucci and Lally, 2004; Bari et al., 2013; Noh and Pad´o, 2013) and NLTK3 (Bird, 2002) are well known achievements of this kind. This paper is about one other such effort that has proved to be successful over the last decade or more. 2 Related Work GATE is designed to be an architecture, a framework and a development environment, quite like UIMA, although the two differ in their realization of this goal. It enables users to develop and deploy robust language engineering components and resources. It also comes bundled with several commonly used baseline Natural Language Processing (NLP) applications. It makes strict distinction between data, algorithms, and ways of visualising t"
W14-5208,N10-1093,1,0.804836,"is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Standard Format (SSF). We showed how this scheme (an instance of the blackboard architectural model), which is based on certain organizational principles such as modularity, simplicity, robustness and transparency, can be used to create not only a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation systems (Shakti and Sampark) which use this scheme a"
W14-5208,I11-1143,1,0.813905,"r in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Standard Format (SSF). We showed how this scheme (an instance of the blackboard architectural model), which is based on certain organizational principles such as modularity, simplicity, robustness and transparency, can be used to create not only a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation systems (Shakti and Sampark) which use this scheme at their core level. Si"
W14-5208,singh-ambati-2010-integrated,1,0.859138,"n any part of the system where SSF data is being used. One recent addition to the interface was a GUI to correct mistakes in treebanks (Agarwal et al., 2012). The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number o"
W14-5208,I08-2141,1,0.649924,"and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Sta"
W14-5208,singh-2012-concise,1,0.790337,"ure tree would not allow. This overlaying of constrained links over the core trees allows multiple layers and/or types of annotation to be stored in the same structure. With a little more improvisation, we can even have links across sentences, i.e., at the discourse level (see section-3.3). It is possible, for example, to have a phrase structure tree (the core tree) overlaid with a dependency tree (via constrained links or ‘threads’), just as it is possible to have POS tagged and chunked data to be overlaid with named entities and discourse relations. The Sanchay Corpus Query Language (SCQL) (Singh, 2012) is a query language designed for threaded trees. It so turns out that SSF is also a representation that can be viewed as threaded trees. Thus, the SCQL can work over data in SSF. This language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries. 6 Applications 6.1 Sampark Machine Translation Architecture Overcom"
W14-5603,J95-3006,0,0.580699,"Missing"
W14-5603,W09-3036,1,0.828284,"ded by the morph, as shown in Figure 1. For example: INPUT: 1. ram chai aur paani peekar. 2. ram soyaa. OUTPUT: 1. ram chai aur paani peeyaa. 2. ram soyaa. Here soyaa is the main verb having yaa as TAM. Word generator1 has been used to generate the final verb given root form of the verb and TAM of the verb. Here pee is the root form of peekar and yaa is given as the TAM. Word generator generates peeyaa as the final word which is used in the sentence. 5 Evaluation We have taken a corpus of 100 complex sentences for the evaluation of our tool. These sentences were taken from the Hindi treebank (Bhatt et al., 2009; Palmer et al., 2009). Evaluation of both sentence simplification and its effects on google MT system for Hindi to English(google translate) was performed. The evaluation of sentence simplification is a subjective task which considers both readability and preservation of semantic information. Hence both manual as well as automatic evaluations have been performed. 5.1 Automatic Evaluation We have used BLEU score (Papineni et al., 2002) for automatic evaluation of both tasks; sentence simplification and enhancing MT system. Higher the BLEU score, closer the target set is to the reference set. T"
W14-5603,C96-2183,0,0.751285,"es 21–29, Dublin, Ireland, August 24th 2014. using both BLEU scores and human readability . In Section 6, we conclude and talk about future work in this area. 2 Related Work Siddharthan (2002) presents a three stage pipelined approach for text simplification. He has also looked into the discourse level problems arising from syntactic text simplification and proposed solutions to overcome them. In his later works (Siddharthan, 2006), he discussed syntactic simplification of sentences. He has formulated the interactions between discourse and syntax during the process of sentence simplification. Chandrasekar et al. (1996) proposed Finite state grammar and Dependency based approach for sentence simplification. They first build a stuctural representation of the sentence and then apply a sequence of rules for extracting the elements that could be simplified. Chandrasekar and Srinivas (1997) have put forward an approach to automatically induce rules for sentence simplification. In their approach all the dependency information of a words is localized to a single structure which provides a local domain of influence to the induced rules. Sudoh et al. (2010) proposed divide and translate technique to address the issue"
W14-5603,W03-0318,0,0.0465498,"inivas (1997) have put forward an approach to automatically induce rules for sentence simplification. In their approach all the dependency information of a words is localized to a single structure which provides a local domain of influence to the induced rules. Sudoh et al. (2010) proposed divide and translate technique to address the issue of long distance reordering for machine translation. They have used clauses as segments for splitting. In their approach, clauses are translated separately with non-terminals using SMT method and then sentences are reconstructed based on the non-terminals. Doi and Sumita (2003) used splitting techniques for simplifying sentences and then utilizing the output for machine translation. Leffa (1998) has shown that simplifying a sentence into clauses can help machine translation. They have built a rule based clause identifier to enhance the performance of MT system. Though the field of sentence simplification has been explored for enhancing machine translation for English as source language, we don’t find significant work for Hindi. Poornima et al. (2011) has reported a rule based technique to simplify complex sentences based on connectives like subordinating conjunction"
W14-5603,P02-1040,0,0.0919449,"ed in the sentence. 5 Evaluation We have taken a corpus of 100 complex sentences for the evaluation of our tool. These sentences were taken from the Hindi treebank (Bhatt et al., 2009; Palmer et al., 2009). Evaluation of both sentence simplification and its effects on google MT system for Hindi to English(google translate) was performed. The evaluation of sentence simplification is a subjective task which considers both readability and preservation of semantic information. Hence both manual as well as automatic evaluations have been performed. 5.1 Automatic Evaluation We have used BLEU score (Papineni et al., 2002) for automatic evaluation of both tasks; sentence simplification and enhancing MT system. Higher the BLEU score, closer the target set is to the reference set. The maximum attainable value is 1 while minimum possible value is 0. For our Automatic evaluation we adopted the same technique as Specia (2010) using BLEU metric. We have achieved 0.6949 BLEU score for sentence simplification task. For MT system, we have evaluated the system with and without sentence simplification tool. It was observed that the system with sentence simplification tool achieved 0.4986 BLEU score whereas the system with"
W14-5603,Y13-1053,1,0.838388,"’KA,v,m,sg,any,,yA,yA’ name=‘khaayaa’> CCP CC <fs name=‘CCP’> <fs af=’Ora,avy,,,,,,’ name=’aur‘> NP NN <fs name=‘NP3’ drel=‘k2:VGF2’> <fs af=’pAnI,n,m,sg,3,d,0,0’ name=‘paani’> VGF VM <fs name=‘VGF2’ drel=‘ccof:CCP’> <fs af=’pIyA,unk,,,,,,’ name=‘piyaa’> Figure 1: SSF representation for example 2 24 4.1.2 Clause boundary Identification and splitting of sentences This module takes the input from preprocessing module and identifies the clause boundaries in the sentence. Once clause boundaries are identified, the sentence is divided into different clauses. We have used the technique mentioned in Sharma et al. (2013) which has shown how implicit clause information present in dependency trees/relations can be used to extract clauses from a sentence. Once we mark the clause boundaries using this approach, we break the sentence into different simple clauses along those clause boundaries. The example(3) given below illustrates the same. (3) raam jisne khaanaa khaayaa ghar gayaa Ram who+rel. food eat+past home go+past ‘Ram who ate food, went home’ Example(3) with clause boundaries marked is, ( raam ( jisne khaanaa khaayaa ) ghar gayaa). Once the clause boundaries are marked, we break the sentence using those b"
W14-5603,I13-1151,1,0.671752,"ifficulty in translation. It seems intuitive to break down the sentence into simplified sentences and use them for the task. Phrase based translation systems exercise a similar approach where system divides the sentences into phrases and translates each phrase independently, later reordering and concatenating them into a single sentence. However, the focus of translation is not on producing a single sentence but to preserve the semantics of the source sentence, with a decent readability at the target side. We present a rule based approach which is basically an improvement on the work done by (Soni et al., 2013) for sentence simplification in Hindi. The approach adapted by them has some limitations since it uses verb frames to extract the core arguments of verb; there is no way to identify information like time, place, manner etc. of the event expressed by the verb which could be crucial for sentence simplification. A parse tree of a sentence could potentially address this problem. We use a dependency parser of Hindi for this purpose. (Soni et al., 2013) didn’t consider breaking the sentences at finite verbs while we split the sentences on finite verbs also. This paper is structured as follows: In Se"
W14-5603,W10-1762,0,0.0260292,"syntax during the process of sentence simplification. Chandrasekar et al. (1996) proposed Finite state grammar and Dependency based approach for sentence simplification. They first build a stuctural representation of the sentence and then apply a sequence of rules for extracting the elements that could be simplified. Chandrasekar and Srinivas (1997) have put forward an approach to automatically induce rules for sentence simplification. In their approach all the dependency information of a words is localized to a single structure which provides a local domain of influence to the induced rules. Sudoh et al. (2010) proposed divide and translate technique to address the issue of long distance reordering for machine translation. They have used clauses as segments for splitting. In their approach, clauses are translated separately with non-terminals using SMT method and then sentences are reconstructed based on the non-terminals. Doi and Sumita (2003) used splitting techniques for simplifying sentences and then utilizing the output for machine translation. Leffa (1998) has shown that simplifying a sentence into clauses can help machine translation. They have built a rule based clause identifier to enhance"
W15-4005,N03-1017,0,0.222571,"l., 2013a) and GloVe (Pennington et al., 2014) and show the effect of varying the context window and vector dimension for Hindi-English language pair. We use partial least squares (PLS) regression to learn the bilingual word embeddings using a bilingual dictionary, which is most readily available resource for any language pair. In this work we are not optimizing over the vector dimension and context window, but provide insights (through experiments) on how these two parameters effect the similarity tasks. Introduction The current state of the art Statistical Machine Translation (SMT) systems (Koehn et al., 2003) do not account for semantic information or semantic relatedness between the corresponding phrases while decoding the n-best list. The phrase pair alignments extracted from the parallel corpora offers further limitation of capturing contextual and linguistic information. Since the efficiency of statistical system depends on the quality of parallel corpora, low resourced language pair fails to meet the desired standards of translation. Word representation is being widely used in many Natural Language Processing (NLP) applications like information retrieval, machine translation and paraphrasing."
W15-4005,P07-2045,0,0.0117546,"the ILCI corpora (Jha, 2010) which contains 50000 Hindi-English parallel sentences (49300 after cleaning) from health and tourism domains. The corpus is randomly split (equal variation of sentence length) into training (48300 sentences), development (500 sentences) and testing (500 sentences). Division Training Development Testing Partial Least Square (PLS) Regression # of sentences 48300 500 500 Dimension 50 100 150 200 250 300 400 500 Table 2: MT system corpus statistics We trained two Phrase based (Koehn et al., 2003) MT systems (Hindi - English and English Hindi) using the Moses toolkit (Koehn et al., 2007) with phrase-alignments (maximum phrase length restricted to 4) extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model of order five and MERT (Och, 2003) for tuning the model with development data. We achieve a BLEU (Papineni et al., 2002) score of 19.89 and 22.82 on English-Hindi and HindiEnglish translation systems respectively. These translation scores serves as our baseline for further experiments. word2Vec CW 5 CW 7 0.53 0.51 0.47 0.49 0.44 0.47 0.42 0.45 0.41 0.43 0.40 0.4"
W15-4005,P08-1028,0,0.0307544,"rd2Vec model with a context window of 5. posed approach works better at lower dimensions for word similarity task. The word2Vec model is performing better than the GloVe model on word-similarity task. Within the same model the word2vec model with context window of five performs better than the model with context window of seven, while it is opposite for the GloVe model. Decoding with semantic similarity score In the phrase based MT system we add two features (semantic similarity scores) to the bilingual phrase pairs. Since we need the vector representation of a phrase, we employ the works of (Mitchell and Lapata, 2008) on compositional semantics (adding the vectors) to compute the phrase representation. For a give phrase pair (s,t), we transform each constituent word of the source phrase ’s’ to the target word space and add the the transformed word embedding to the resultant source vector. We ignore the word if it does not occur in the word embeddings vocabulary. Similarly, we compute the phrase representation of the target phrase ’t’ by simply adding the word vectors to the resultant target vector. We then compute the cosine similarity between the two vectors which acts as a feature for the MT decoder. We"
W15-4005,P00-1056,0,0.106878,"from health and tourism domains. The corpus is randomly split (equal variation of sentence length) into training (48300 sentences), development (500 sentences) and testing (500 sentences). Division Training Development Testing Partial Least Square (PLS) Regression # of sentences 48300 500 500 Dimension 50 100 150 200 250 300 400 500 Table 2: MT system corpus statistics We trained two Phrase based (Koehn et al., 2003) MT systems (Hindi - English and English Hindi) using the Moses toolkit (Koehn et al., 2007) with phrase-alignments (maximum phrase length restricted to 4) extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model of order five and MERT (Och, 2003) for tuning the model with development data. We achieve a BLEU (Papineni et al., 2002) score of 19.89 and 22.82 on English-Hindi and HindiEnglish translation systems respectively. These translation scores serves as our baseline for further experiments. word2Vec CW 5 CW 7 0.53 0.51 0.47 0.49 0.44 0.47 0.42 0.45 0.41 0.43 0.40 0.41 0.40 0.38 0.38 0.37 GloVe CW 5 CW 7 0.48 0.49 0.43 0.44 0.41 0.42 0.38 0.41 0.38 0.39 0.37 0.39 0.35 0."
W15-4005,P12-1092,0,0.0404812,"ts in the neural network is guided by the BLEU score (ultimate goal to improve the quality of translation through increase in BLEU score) which makes it sensitive towards the score. Wu (2014) proposed an approach of using supervised model of learning context-sensitive bilingual embedding where the aligned phrase pairs are marked as true labels. Since these defined methods depends heavily on the quality of word vectors, a number of approaches have been suggested in past to learn word representations from monolingual corpus: word2Vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014) and (Huang et al., 2012). In this work, we extend the phrase similarity work by using the regression approach to learn the bilingual word embeddings. We employ vector composition approach to compute the phrase vector, where we add vectors of each constituent word to achieve the phrase vector. We also present 3 Learning word representation We have used a part of WMT’141 monolingual data and news crawled monolingual data to learn word representations for English and Hindi respectively. We added the ILCI bilingual corpus (Jha, 2010) of English and Hindi to the monolingual data. The corpus statistics (after cleaning) are"
W15-4005,P03-1021,0,0.0124327,"(500 sentences). Division Training Development Testing Partial Least Square (PLS) Regression # of sentences 48300 500 500 Dimension 50 100 150 200 250 300 400 500 Table 2: MT system corpus statistics We trained two Phrase based (Koehn et al., 2003) MT systems (Hindi - English and English Hindi) using the Moses toolkit (Koehn et al., 2007) with phrase-alignments (maximum phrase length restricted to 4) extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model of order five and MERT (Och, 2003) for tuning the model with development data. We achieve a BLEU (Papineni et al., 2002) score of 19.89 and 22.82 on English-Hindi and HindiEnglish translation systems respectively. These translation scores serves as our baseline for further experiments. word2Vec CW 5 CW 7 0.53 0.51 0.47 0.49 0.44 0.47 0.42 0.45 0.41 0.43 0.40 0.41 0.40 0.38 0.38 0.37 GloVe CW 5 CW 7 0.48 0.49 0.43 0.44 0.41 0.42 0.38 0.41 0.38 0.39 0.37 0.39 0.35 0.36 0.34 0.36 Table 3: Average word cosine similarity scores on test set. Context Window (CW) 4.3 Learning Transformation matrix We employ PLS regression to learn bil"
W15-4005,P02-1040,0,0.0925971,"e (PLS) Regression # of sentences 48300 500 500 Dimension 50 100 150 200 250 300 400 500 Table 2: MT system corpus statistics We trained two Phrase based (Koehn et al., 2003) MT systems (Hindi - English and English Hindi) using the Moses toolkit (Koehn et al., 2007) with phrase-alignments (maximum phrase length restricted to 4) extracted from GIZA++ (Och and Ney, 2000). We have used the SRILM (Stolcke and others, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) for training a language model of order five and MERT (Och, 2003) for tuning the model with development data. We achieve a BLEU (Papineni et al., 2002) score of 19.89 and 22.82 on English-Hindi and HindiEnglish translation systems respectively. These translation scores serves as our baseline for further experiments. word2Vec CW 5 CW 7 0.53 0.51 0.47 0.49 0.44 0.47 0.42 0.45 0.41 0.43 0.40 0.41 0.40 0.38 0.38 0.37 GloVe CW 5 CW 7 0.48 0.49 0.43 0.44 0.41 0.42 0.38 0.41 0.38 0.39 0.37 0.39 0.35 0.36 0.34 0.36 Table 3: Average word cosine similarity scores on test set. Context Window (CW) 4.3 Learning Transformation matrix We employ PLS regression to learn bilingual word embeddings using a English-Hindi bilingual dictionary 3 . We have used 150"
W15-4005,P06-1102,0,0.0165025,"Missing"
W15-4005,D14-1162,0,0.0851402,"Missing"
W15-4005,D11-1014,0,0.0219562,", 2013b) linear transformation works well for language pairs which are closely related, however in this work we experiment with PLS regression which also establishes a linear relationship between words but is much more efficient than the simple least squares regression (explained in 4.2). The current research community has shown special interest towards vector space models by organizing various dedicated workshops in top rated conferences. Word representations have been used in many NLP applications like information extraction (Pas¸ca et al., 2006; Manning et al., 2008), sentiment prediction (Socher et al., 2011) and paraphrase detection (Huang, 2011). In the past various methodologies have been suggested to learn bilingual word embeddings for various natural language related tasks. (Mikolov et al., 2013b) and (Zou et al., 2013) have shown significant improvements by using bilingual word embeddings in context of machine translation experiments. The former applies linear transformation to bilingual dictionary while the latter uses word alignments knowledge. Zhang (2014) proposed an auto-encoder based approach to learn phrase embeddings from the word vectors and showed improvements by using semantic sim"
W15-4005,D14-1015,0,0.0399321,"Missing"
W15-4005,P14-1011,0,0.0495565,"Missing"
W15-4005,D13-1141,0,0.0265035,"cient than the simple least squares regression (explained in 4.2). The current research community has shown special interest towards vector space models by organizing various dedicated workshops in top rated conferences. Word representations have been used in many NLP applications like information extraction (Pas¸ca et al., 2006; Manning et al., 2008), sentiment prediction (Socher et al., 2011) and paraphrase detection (Huang, 2011). In the past various methodologies have been suggested to learn bilingual word embeddings for various natural language related tasks. (Mikolov et al., 2013b) and (Zou et al., 2013) have shown significant improvements by using bilingual word embeddings in context of machine translation experiments. The former applies linear transformation to bilingual dictionary while the latter uses word alignments knowledge. Zhang (2014) proposed an auto-encoder based approach to learn phrase embeddings from the word vectors and showed improvements by using semantic similarity score in MT experiments. The phrase vector is generated by recursively combining the two children vector into a same dimensional parent vector using the method suggested by (Socher et al., 2011). The work of (Gao"
W15-4005,jha-2010-tdil,0,\N,Missing
W15-5951,N03-1017,0,0.02201,"arallel corpus size for English and IL pairs. Hence, we explore a rule based approach for reordering taking insights from P¯an.inian Grammar (PG). The available reordering approaches are discussed in Section 2. Our reordering approach is described in Section 3. Section 4 talks about major divergences between English and Hindi. Reordering rule formation is described in Section 5 and Section 6 presents experiments and results. Section 7 does error analysis and Section 8 concludes the paper. 2 Related Work There are many approaches to handle TL word ordering. Some of them are described below: 1. Koehn et al (2003) perform reordering by using relative distortion probability distribution model trained from joint probability distribution model φ(¯e,¯f). Their model relies on the language model to produce words in right order (Koehn, 2009). 2. Kunchukuttan et al (2014) developed a phrase based system for English-Indian Language (henceforth En-IL) pairs that uses two extensions- (i) source reordering for En-IL translation using source side reordering rules developed by (Patel et al., 2013) and (ii) describing untranslated words for Indian-IL translation by using transliteration procedure. D S Sharma, R Sang"
W15-5951,P09-5002,0,0.0271585,"ch is described in Section 3. Section 4 talks about major divergences between English and Hindi. Reordering rule formation is described in Section 5 and Section 6 presents experiments and results. Section 7 does error analysis and Section 8 concludes the paper. 2 Related Work There are many approaches to handle TL word ordering. Some of them are described below: 1. Koehn et al (2003) perform reordering by using relative distortion probability distribution model trained from joint probability distribution model φ(¯e,¯f). Their model relies on the language model to produce words in right order (Koehn, 2009). 2. Kunchukuttan et al (2014) developed a phrase based system for English-Indian Language (henceforth En-IL) pairs that uses two extensions- (i) source reordering for En-IL translation using source side reordering rules developed by (Patel et al., 2013) and (ii) describing untranslated words for Indian-IL translation by using transliteration procedure. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 357–366, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) 3. A statistical translation model introduced by Yama"
W15-5951,kunchukuttan-etal-2014-shata,0,0.0143877,"n Section 3. Section 4 talks about major divergences between English and Hindi. Reordering rule formation is described in Section 5 and Section 6 presents experiments and results. Section 7 does error analysis and Section 8 concludes the paper. 2 Related Work There are many approaches to handle TL word ordering. Some of them are described below: 1. Koehn et al (2003) perform reordering by using relative distortion probability distribution model trained from joint probability distribution model φ(¯e,¯f). Their model relies on the language model to produce words in right order (Koehn, 2009). 2. Kunchukuttan et al (2014) developed a phrase based system for English-Indian Language (henceforth En-IL) pairs that uses two extensions- (i) source reordering for En-IL translation using source side reordering rules developed by (Patel et al., 2013) and (ii) describing untranslated words for Indian-IL translation by using transliteration procedure. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 357–366, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) 3. A statistical translation model introduced by Yamada and Knight (2001) incorpora"
W15-5951,W13-2807,0,0.0218177,"des the paper. 2 Related Work There are many approaches to handle TL word ordering. Some of them are described below: 1. Koehn et al (2003) perform reordering by using relative distortion probability distribution model trained from joint probability distribution model φ(¯e,¯f). Their model relies on the language model to produce words in right order (Koehn, 2009). 2. Kunchukuttan et al (2014) developed a phrase based system for English-Indian Language (henceforth En-IL) pairs that uses two extensions- (i) source reordering for En-IL translation using source side reordering rules developed by (Patel et al., 2013) and (ii) describing untranslated words for Indian-IL translation by using transliteration procedure. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 357–366, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) 3. A statistical translation model introduced by Yamada and Knight (2001) incorporates features based on syntax and converts SL parse tree according to TL with the help of probabilistic operations at required nodes using expectation-maximization algorithm. This model accepts parse trees as an input on whic"
W15-5951,W14-4006,1,0.872849,"Missing"
W15-5951,P01-1067,0,0.136907,"009). 2. Kunchukuttan et al (2014) developed a phrase based system for English-Indian Language (henceforth En-IL) pairs that uses two extensions- (i) source reordering for En-IL translation using source side reordering rules developed by (Patel et al., 2013) and (ii) describing untranslated words for Indian-IL translation by using transliteration procedure. D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 357–366, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) 3. A statistical translation model introduced by Yamada and Knight (2001) incorporates features based on syntax and converts SL parse tree according to TL with the help of probabilistic operations at required nodes using expectation-maximization algorithm. This model accepts parse trees as an input on which it performs child node reordering according to the TL. 4. Costa-Juss`a and Fonollosa (2009) used an Ngram-based Reordering (NbR) method that uses SMT techniques to generate reordering graph, which utilizes word classes and NbR model for reordering. It produces an intermediate representation of source corpora where word-order of SL is represented more closely to"
W16-1716,W09-3036,1,0.819471,"odifications etc. Both the Kāraka and Non-kāraka relations in the scheme are represented in Figure 1; glosses of these relations are given in Table 2 . The purpose of choosing a hierarchical model for relation types is to have the possibility of underspecifying certain relations. The Two Schemes Hindi Dependency Treebank and Computational Paninian Grammar The Hindi Treebank contains text from news articles and heritage domain. It consists of 434,856 tokens in 20,783 sentences with an average of 20.92 words per sentence as can be seen in Table 2. It is multi-layered and multi-representational (Bhatt et al., 2009; Xia et al., 2009; Palmer et al., 2009; Bhat et al., 2014). It contains three layers of annotation namely dependency structure (DS) for annotation of modified-modifier relations, PropBankstyle annotation for predicate-argument structure, and an independently motivated phrase-structure annotation. Each layer has its own framework, annotation scheme, and detailed annotation guidelines. Dependency Structure−the first layer in these treebanks−involves dependency analysis based on the Pān inian Grammatical framework (Bharati et al., 1995; Begum et al., 2008). Pān ini was an Indian grammarian who"
W16-1716,W13-2308,0,0.0605404,"Missing"
W16-1716,de-marneffe-etal-2014-universal,0,0.0369653,"Missing"
W16-1716,W10-1411,0,0.0471831,"Missing"
W16-1716,E14-4028,0,0.0182597,"the ongoing efforts in this direction, our work is a volunteer effort to harmonize the Hindi Dependency Treebank according to the UD formalism, making it a more available resource for multilingual parsing. In doing so, we have converted the dependency relations in Pān inian framework and the POS tag set followed by Hindi to the Universal Dependency scheme. This conversion had its challenges, as many language specific phenomena had to be addressed in the process. However, there was no requirement to develop a new, language specific UD-scheme, unlike some other treebanks, for instance Russian (Lipenkova and Soucek, 2014). The rest of the paper is organized as follows: In Section 2, we describe the annotation scheme used for the Indian language treebanking and Universal Dependency treebanking. Section 3 talks about the granularity of the Pān inian scheme. In Section 4, we elaborate upon the differences in design between the two schemes, how existing dependency scheme and POS tags for Hindi map onto the universal taxonomy, the issues that were faced Universal Dependencies (UD) are gaining much attention of late for systematic evaluation of cross-lingual techniques for crosslingual dependency parsing. In this p"
W16-1716,I08-2099,1,0.598253,"lti-layered and multi-representational (Bhatt et al., 2009; Xia et al., 2009; Palmer et al., 2009; Bhat et al., 2014). It contains three layers of annotation namely dependency structure (DS) for annotation of modified-modifier relations, PropBankstyle annotation for predicate-argument structure, and an independently motivated phrase-structure annotation. Each layer has its own framework, annotation scheme, and detailed annotation guidelines. Dependency Structure−the first layer in these treebanks−involves dependency analysis based on the Pān inian Grammatical framework (Bharati et al., 1995; Begum et al., 2008). Pān ini was an Indian grammarian who is credited with writing 2.2 Universal Dependencies As mentioned by (Nivre et al., 2016) and also discussed by (Johannsen et al., 2015), the driving principles of UD formalism are: 1. Content over function: Content words form the backbone of the syntactic representation. Giving priority to dependency relations between content words increases the probability of finding parallel structures across languages, since function words in one language often correspond to morphological inflection (or nothing at all) in other languages. Functional heads are instead"
W16-1716,J95-3006,0,0.662785,"n in Table 2. It is multi-layered and multi-representational (Bhatt et al., 2009; Xia et al., 2009; Palmer et al., 2009; Bhat et al., 2014). It contains three layers of annotation namely dependency structure (DS) for annotation of modified-modifier relations, PropBankstyle annotation for predicate-argument structure, and an independently motivated phrase-structure annotation. Each layer has its own framework, annotation scheme, and detailed annotation guidelines. Dependency Structure−the first layer in these treebanks−involves dependency analysis based on the Pān inian Grammatical framework (Bharati et al., 1995; Begum et al., 2008). Pān ini was an Indian grammarian who is credited with writing 2.2 Universal Dependencies As mentioned by (Nivre et al., 2016) and also discussed by (Johannsen et al., 2015), the driving principles of UD formalism are: 1. Content over function: Content words form the backbone of the syntactic representation. Giving priority to dependency relations between content words increases the probability of finding parallel structures across languages, since function words in one language often correspond to morphological inflection (or nothing at all) in other languages. Function"
W16-1716,P13-2017,0,0.0302716,"rough conversion of Pān inian Dependencies to UD for the Hindi Dependency Treebank (HDTB). We discuss the differences in annotation in both the schemes, present parsing experiments for both the formalisms and empirically evaluate their weaknesses and strengths for Hindi. We produce an automatically converted Hindi Treebank conforming to the international standard UD scheme, making it useful as a resource for multilingual language technology. 1 Introduction Universal Dependencies is a project undertaken to develop an inventory of languages that have treebanks annotated in a consistent scheme (McDonald et al., 2013). The UD annotation has evolved by reconstruction of the Standford Dependencies (De Marneffe and Manning, 2008) and it uses a slightly extended version of Google universal tag set for part of speech (POS) (Petrov et al., 2011). This is done with the motivation to facilitate the efforts in building of cross-linguistic tools such as parsers, translation systems, search engines, etc. The efforts in building similarly structured or annotated treebanks have invoked a lot intreset from researchers around the world. The first release of UD treebanks included six languages where English and Swedish we"
W16-1716,petrov-etal-2012-universal,0,0.103354,"Missing"
W16-1716,W15-1821,0,0.202018,"Missing"
W16-1716,zeman-etal-2012-hamledt,0,0.030311,"2015). Several treebanks have also been created using manual annotation procedures. For languages where a treebank is already available, automatic conversion process is more suitable than manual annotation which is expensive and time consuming. It should be noted here that while for some languages conversion between the original and the UD representations can be accurate, for others it may introduce too much noise. There have been few attempts that have tried to convert the annotation scheme used for Indian languages to other schemes such as the annotation style of Prague Dependency Treebank (Zeman et al., 2012). Our work, instead, aims to convert HDTB annotation scheme to UD. Keeping in line with the ongoing efforts in this direction, our work is a volunteer effort to harmonize the Hindi Dependency Treebank according to the UD formalism, making it a more available resource for multilingual parsing. In doing so, we have converted the dependency relations in Pān inian framework and the POS tag set followed by Hindi to the Universal Dependency scheme. This conversion had its challenges, as many language specific phenomena had to be addressed in the process. However, there was no requirement to develop"
W16-1716,W08-1301,0,\N,Missing
W16-1716,P09-3002,1,\N,Missing
W16-1716,L16-1262,0,\N,Missing
W16-5201,cassidy-etal-2014-alveo,0,0.0278398,"nd Vinoski, 2010) inspired packaging system, is again something which we believe can help with large scale adoption of a system like Kathaa. It paves the way for a public contributed repository of NLP components, all of which can be mashed together in any desired combination. The ability to optionally package individual services using Docker Containers 8 also helps make a strong case when pitching for the possibility of a large public contributed repository of NLP components. These are a few things which set Kathaa apart from already existing systems like LAPPS Grid (Ide et al., 2014), ALVEO (Cassidy et al., 2014) where the easy extensibility of the system is a major bottleneck in its large scale adoption. The inter-operability between existing systems is also of key importance, and the design of Kathaa accommodates for its easy adaptation to be used along with other similar system. The assumption, of course, is that a wrapper Kathaa Module has to be designed for each target system using the Kathaa Module Definition API. The wrapper modules would be completely decoupled from the Kathaa Core codebase, and hence can be designed and implemented by anyone just like any other Kathaa Module. A demonstration"
W16-5201,A97-2017,0,0.0978366,"mplementation layers of Natural Language Processing systems, and efficiently pack every component into consistent and reusable black-boxes which can be made to interface with each other through an intuitive visual interface, irrespective of the software environment in which the components reside, and irrespective of the technical proficiency of the user using the system. Kathaa builds on top of numerous ideas explored in the academia around Visual Programming Languages in general (Green and Petre, 1996) (Shu, 1988) (Myers, 1990), and also on Visual Programming Languages in the context of NLP (Cunningham et al., 1997). In a previous demonstration (Mohanty et al., 2016) at NAACL-HLT-2016, we showcased many of the features of Kathaa, and because of the general interest in Kathaa, we are now making an attempt to more formally model Kathaa in this paper. This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 Proceedings of WLSI/OIAF4HLT, pages 1–10, Osaka, Japan, December 12 2016. Licence details: http:// Figure 1: Example of a Hindi-Panjabi Machine Translation System, visually implemented using Kathaa. 2 Kathaa Modules Kathaa Modules are th"
W16-5201,ide-etal-2014-language,0,0.0280175,"Missing"
W16-5201,N16-3019,1,0.481557,"tems, and efficiently pack every component into consistent and reusable black-boxes which can be made to interface with each other through an intuitive visual interface, irrespective of the software environment in which the components reside, and irrespective of the technical proficiency of the user using the system. Kathaa builds on top of numerous ideas explored in the academia around Visual Programming Languages in general (Green and Petre, 1996) (Shu, 1988) (Myers, 1990), and also on Visual Programming Languages in the context of NLP (Cunningham et al., 1997). In a previous demonstration (Mohanty et al., 2016) at NAACL-HLT-2016, we showcased many of the features of Kathaa, and because of the general interest in Kathaa, we are now making an attempt to more formally model Kathaa in this paper. This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 1 Proceedings of WLSI/OIAF4HLT, pages 1–10, Osaka, Japan, December 12 2016. Licence details: http:// Figure 1: Example of a Hindi-Panjabi Machine Translation System, visually implemented using Kathaa. 2 Kathaa Modules Kathaa Modules are the basic units of computation in the proposed Visual"
W16-5201,1983.tc-1.13,0,0.29015,"Missing"
W17-6309,D14-1181,0,0.0045489,"Missing"
W17-6309,E17-2052,1,0.825244,"ers in transitive sentences in the evaluation set. Unlike their distribution in the HTB training data, the word orders in the evaluation set are relatively less skewed. S.No. 1 2 3 4 5 6 Order SOV OSV OVS SVO VOS VSO Percentage 33.07 23.62 17.32 14.17 9.45 2.36 Most of the movie scripts available online and the tweets are written in Roman script instead of the standard Devanagari script, requiring backtransliteration of the sentences in the evaluation set before running experiments. We also need normalization of non-standard word forms prevalent in tweets. We followed the procedure adapted by Bhat et al. (2017a) to learn a single backtransliteration and normalization system. We also performed sentence-level decoding to resolve homograph ambiguity in Romanized Hindi vocabulary. 4 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Kiperwasser and Goldberg, 2016). The monolingual models are trained on training files of HTB which uses the P¯aninian Grammar framework (PG) (Bharati et al., 1995), while the multilingual models are trained on Universal Dependency Treebanks of Hindi and English release"
W17-6309,Q16-1023,0,0.0478143,"instead of the standard Devanagari script, requiring backtransliteration of the sentences in the evaluation set before running experiments. We also need normalization of non-standard word forms prevalent in tweets. We followed the procedure adapted by Bhat et al. (2017a) to learn a single backtransliteration and normalization system. We also performed sentence-level decoding to resolve homograph ambiguity in Romanized Hindi vocabulary. 4 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Kiperwasser and Goldberg, 2016). The monolingual models are trained on training files of HTB which uses the P¯aninian Grammar framework (PG) (Bharati et al., 1995), while the multilingual models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our underlying parsing method is based on the arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For"
W17-6309,W03-3017,0,0.116751,"vocabulary. 4 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Kiperwasser and Goldberg, 2016). The monolingual models are trained on training files of HTB which uses the P¯aninian Grammar framework (PG) (Bharati et al., 1995), while the multilingual models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our underlying parsing method is based on the arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arceager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce. We use a non-"
W17-6309,Q16-1035,0,0.14756,", 2017. 2017 Association for Computational Linguistics target distributions benefited parsing of noisy, conversational data (Van der Plas et al., 2009; Foster, 2010). Order SOV OSV OVS SVO VOS VSO Percentage 91.83 7.80 0.19 0.19 0.0 0.0 Table 1: The table shows theoretically possible orders of Subject (S), Object (O) and Verb (V) in transitive sentences in the HTB training data with their percentages of occurrence. 95 90 85 80 75 70 2 Figure 2: Learning curves plotted against data size on the X axis and LAS score on the Y axis. Sampling Argument Scrambling via Syntactic Transformations Unlike Wang and Eisner (2016), we do not choose one word-order for a verbal projection based on a target distribution, but instead generate all of its theoretically possible orders. For each dependency tree, we alter the linear precedence relations between arguments of a verbal projection in ‘n!’ ways, while keeping their dominance relations intact. However, simply permuting all the nodes of verbal projections can lead to an overwhelming number of trees. For example, a data set of ‘t’ syntactic trees each containing an average of 10 nodes would allow around t × 10! i.e., 3 million possible permutations for our training da"
W17-6309,N09-2032,0,0.0840276,"Missing"
W17-6529,J95-3006,0,0.963742,"mber, person, case, vibhakti, TAM (tense, aspect and modality) label in case of verbs, or postposition in case of nouns), chunking information and syntacticosemantic dependency relations. There has been a shift from the Anncorra POS tags (Bharati et al., 2006) that were initially used for Indian Languages to the new common tagset for all Indian languages which we would refer to as the Bureau of Indian Standards (BIS) tagset (Choudhary and Jha, 2011). This new POS tagging scheme is finer than the previous scheme. The dependency relations are marked following the Computational Paninian Grammar (Bharati et al., 1995; Begum 1 vibhakti is a generic term for postposition and suffix that represent case marking 2 a chunk is a set of adjacent words which are in dependency relation with each other, and are connected to the rest of the words by a single incoming arc to the chunk 256 3 The organizations involved in this project are Jadavpur University-Kolkata (Bengali), MIT-Manipal (Kannada), CDIT,Trivandrum (Malayalam), IIT-Bombay (Marathi), IIITHyderabad (Hindi) Types Tokens Chunks Sentences Kannada 36778 188040 143400 16551 Malayalam 20107 65996 54818 5824 Telugu BIS 4079 11338 8203 2173 Telugu Ann. 4582 13477"
W17-6529,C16-1039,1,0.847665,"process. 1 Introduction Over the years there have been several successful attempts in building data driven dependency parsers using rich feature templates (K¨ubler et The last decade has seen quite a few attempts at parsing Indian languages Hindi, Telugu and Bengali (Bharati et al., 2008a; Nivre, 2009; Mannem, 2009; Kolachina et al., 2010; Ambati et al., 2010a). The research in this direction majorly focused on data driven transition-based parsing using MALT (Nivre et al., 2007), MST parser (McDonald et al., 2005) or constraint based method (Bharati et al., 2008b; Kesidi, 2013). Only recently Bhat et al. (2016a) have used neural network based non-linear parser to learn syntactic representations of Hindi and Urdu. Following their efforts, we present a similar parser for parsing five Indian Languages namely Bengali, 255 Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages 255-265, Pisa, Italy, September 18-20 2017 Marathi, Telugu, Kannada, Malayalam. These languages belong to two major language families, Indo-Aryan and Dravidian. The Dravidian languages - Telugu, Kannada and Malayalam are highly agglutinative. The rich morphological nature of a language c"
W17-6529,W09-3036,1,0.780508,"cts of inclusion of rich morpho-syntactic features on different languages and back the results with linguistic reasoning. In Section 6 we conclude and talk about future directions of research our work paves the way for. 2 2.1 Data and Background Dependency Treebanks There have been several efforts towards developing robust data driven dependency parsing techniques in the last decade (K¨ubler et al., 2009). The efforts, in turn, initiated a parallel drive for building dependency annotated treebanks (Tsarfaty et al., 2013). Development of Hindi and Urdu multi-layered and multi-representational (Bhatt et al., 2009; Xia et al., 2009; Palmer et al., 2009) treebanks was a concerted effort in this direction. In line with these efforts, treebanks for Kannada, Malayalam, Telugu, Marathi and Bengali are being developed as a part of the Indian Languages - Treebanking Project. The process of treebank annotation for various languages took place at different institutes3 . These treebanks are manually annotated and span over various domains, like that of newswire articles, conversational data, agriculture, entertainment, tourism and education, thus making our models trained on them robust. The treebanks are annota"
W17-6529,D14-1082,0,0.168243,"high parsing accuracies, they were computationally expensive to extract and also posed the problem of data sparsity. To address the problem of discrete representations of words, distributional representations became a critical component of NLP tasks such as POS tagging (Collobert et al., 2011), constituency parsing (Socher et al., 2013) and machine translation (Devlin et al., 2014). The distributed representations are shown to be more effective in non-linear architectures compared to the traditional linear classifier (Wang and Manning, 2013). Keeping in line with this trend, Chen and Manning (Chen and Manning, 2014) introduced a compact neural network based classifier for use in a greedy, transition-based dependency parser that learns using dense vector representations not only of words, but also of part-of-speech (POS) tags, dependency labels, etc. In our task of parsing Indian languages, a similar transition-based parser based on their model has been used. This model handles the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014). This paper presents our work to apply non linear neural network for parsing five r esource p oor I ndian L anguages belonging to tw"
W17-6529,P14-1129,0,0.0421072,"tion Technology, Hyderabad (IIIT-H) Gachibowli, Hyderabad, India juhi.tandon@research.iiit.ac.in dipti@iiit.ac.in Abstract al., 2009) requiring a lot of feature engineering expertise. Though these indicative features brought enormously high parsing accuracies, they were computationally expensive to extract and also posed the problem of data sparsity. To address the problem of discrete representations of words, distributional representations became a critical component of NLP tasks such as POS tagging (Collobert et al., 2011), constituency parsing (Socher et al., 2013) and machine translation (Devlin et al., 2014). The distributed representations are shown to be more effective in non-linear architectures compared to the traditional linear classifier (Wang and Manning, 2013). Keeping in line with this trend, Chen and Manning (Chen and Manning, 2014) introduced a compact neural network based classifier for use in a greedy, transition-based dependency parser that learns using dense vector representations not only of words, but also of part-of-speech (POS) tags, dependency labels, etc. In our task of parsing Indian languages, a similar transition-based parser based on their model has been used. This model"
W17-6529,W09-3819,0,0.011789,". Therefore we experiment to see how far the chunk information helps us in this setting. 3.5 Gender, Number, Person We want to capture the agreement between verb and its arguments in all languages by the addition of other morphological features such as gender, number and person ( GNP ) for each node. The verb agrees in GNP with the highest available karaka k1 usually. But agreement rules can be complex, it may sometimes take default feature or agree with karaka k2 in some cases. The problem worsens when there is a complex verb. Similar problems with agreement features have also been noted by (Goldberg and Elhadad, 2009). So we experiment to see if the parser can learn selective agreement pattern for different languages. Kannada and Malayalam have a three gender system - gender marking is based on semantics. Human males and females are masculine and feminine gender respectively, whereas all things and animals are neuter gender. Telugu also has a threegender system but human females are grouped with neuter nouns in singular, and human males in plural. The verb in Malayalam is not marked for number, gender person. Similarly in Bengali, the verb changes according to the person information 6 finite, non-finite, i"
W17-6529,goldhahn-etal-2012-building,0,0.0289992,"meters, thus providing an efficient solution to the problem of data sparsity. Moreover since word embeddings are assumed to capture semantic and syntactic aspects of a word, they can also improve the correlation between words and dependency labels. The same representations are also used in the POS tagger. The monolingual corpora of all the languages are used to learn their respective word embeddings. The data is collected from various sources such as Wikipedia dump7 , ILCI - health, tourism agriculture and entertainment data (Jha, 2010), raw corpus from EMILLE / CIIL (Xiao et al., 2004), LCC (Goldhahn et al., 2012), part of Opensubtitles corpus (Tiedemann, 2009), to train rich domain independent word-embeddings so that our parsing model is not biased. We use the Skipgram model with negative sampling implemented in the open-source word2vec toolkit (Mikolov et al., 2013) to learn word representations. The context window size was kept to 1, as shorter context captures more syntactic relatedness compared to longer contexts that capture semantic and topical similarity. The word embedding size was experimented with and embeddings of dimension 64 gave the best results. 4.4 Representation of POS, Chunk and GNP"
W17-6529,N12-1032,0,0.014346,"two major language families, Indo-Aryan and Dravidian. The Dravidian languages - Telugu, Kannada and Malayalam are highly agglutinative. The rich morphological nature of a language can prove challenging for a statistical parser as is noted by (Tsarfaty et al., 2010). For morphologically rich, free word order languages high performance can be achieved using vibhakti1 and information related to tense, aspect, modality (TAM). Syntactic features related to case and TAM marking have been found to be very useful in previous works on dependency parsing of Hindi (Ambati et al., 2010b; Hohensee, 2012; Hohensee and Bender, 2012; Bhat et al., 2016b). We decided to experiment with these features for other Indian languages too as they follow more or less the same typology, all being free order and ranging from being moderate to very morphologically rich. We propose an efficient way to incorporate this information in the aforementioned neural network based parser. In our model, these features are included as suffix (last 4 characters) embeddings for all nodes. Lexical embeddings of case and TAM markers occurring in all the chunk are also included. We also include chunk tags and gender, number, person information as feat"
W17-6529,jha-2010-tdil,0,0.0211902,"words that are closer in the embedding space to share the model parameters, thus providing an efficient solution to the problem of data sparsity. Moreover since word embeddings are assumed to capture semantic and syntactic aspects of a word, they can also improve the correlation between words and dependency labels. The same representations are also used in the POS tagger. The monolingual corpora of all the languages are used to learn their respective word embeddings. The data is collected from various sources such as Wikipedia dump7 , ILCI - health, tourism agriculture and entertainment data (Jha, 2010), raw corpus from EMILLE / CIIL (Xiao et al., 2004), LCC (Goldhahn et al., 2012), part of Opensubtitles corpus (Tiedemann, 2009), to train rich domain independent word-embeddings so that our parsing model is not biased. We use the Skipgram model with negative sampling implemented in the open-source word2vec toolkit (Mikolov et al., 2013) to learn word representations. The context window size was kept to 1, as shorter context captures more syntactic relatedness compared to longer contexts that capture semantic and topical similarity. The word embedding size was experimented with and embeddings"
W17-6529,W12-3607,1,0.872852,"Missing"
W17-6529,P05-1012,0,0.1151,"ng morphological features. Part of speech taggers and chunkers for all languages are also built in the process. 1 Introduction Over the years there have been several successful attempts in building data driven dependency parsers using rich feature templates (K¨ubler et The last decade has seen quite a few attempts at parsing Indian languages Hindi, Telugu and Bengali (Bharati et al., 2008a; Nivre, 2009; Mannem, 2009; Kolachina et al., 2010; Ambati et al., 2010a). The research in this direction majorly focused on data driven transition-based parsing using MALT (Nivre et al., 2007), MST parser (McDonald et al., 2005) or constraint based method (Bharati et al., 2008b; Kesidi, 2013). Only recently Bhat et al. (2016a) have used neural network based non-linear parser to learn syntactic representations of Hindi and Urdu. Following their efforts, we present a similar parser for parsing five Indian Languages namely Bengali, 255 Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages 255-265, Pisa, Italy, September 18-20 2017 Marathi, Telugu, Kannada, Malayalam. These languages belong to two major language families, Indo-Aryan and Dravidian. The Dravidian languages - Tel"
W17-6529,D11-1006,0,0.0331358,"ing them one by one to observe their effects. It is a known fact that language specific features play a crucial role in robust dependency parsing, but their generation may require expensive tools. 3.1 Part of Speech Tags POS tags are very important for dependency parsing, as a purely lexical parser may lead to sparseness but adding POS tags provides a coarser grammatical category. This generalization of words 5 The complete set of dependency relation types can be found in (Bharati et al., 2009) help as words belonging to the same part-ofspeech are expected to have the same syntactic behavior. McDonald et al. (2011) have shown in their delexicalised parser that most of the information is captured in POS tags and just using them as features provides high unlabeled attachment score (UAS). However, for labeled dependency parsing, especially for semantic-oriented dependencies like Paninian dependencies these non-lexical features are not predictive enough. 3.2 Word It is an indispensable unit for labeled dependency parsing. It is important for resolving ambiguous relationships for dependency parsing. But lexical units are sparse and difficult to learn given a limited training data set. This sparsity is observ"
W17-6529,W03-3017,0,0.0240331,"ults using predicted POS and chunk tags obtained from the models discussed below. We report auto accuracy of the parsing model on the same training, development and testing sets that are used for parsing with gold tags. 4.1 Parsing Model We have used a non-linear neural network greedy transition-based parser, similar in structure to (Chen and Manning, 2014). A few new features 259 have been introduced in the input layer of the model as described below. Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = φ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): 1) Shift, 2) Left-Arc, 3) Right-Arc, and 4) Reduce. We"
W17-6529,J08-4003,0,0.0175292,"order to parse in more realistic settings, we also show parsing results using predicted POS and chunk tags obtained from the models discussed below. We report auto accuracy of the parsing model on the same training, development and testing sets that are used for parsing with gold tags. 4.1 Parsing Model We have used a non-linear neural network greedy transition-based parser, similar in structure to (Chen and Manning, 2014). A few new features 259 have been introduced in the input layer of the model as described below. Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = φ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of tran"
W17-6529,P13-1045,0,0.0126191,"tems (KCIS) International Institute of Information Technology, Hyderabad (IIIT-H) Gachibowli, Hyderabad, India juhi.tandon@research.iiit.ac.in dipti@iiit.ac.in Abstract al., 2009) requiring a lot of feature engineering expertise. Though these indicative features brought enormously high parsing accuracies, they were computationally expensive to extract and also posed the problem of data sparsity. To address the problem of discrete representations of words, distributional representations became a critical component of NLP tasks such as POS tagging (Collobert et al., 2011), constituency parsing (Socher et al., 2013) and machine translation (Devlin et al., 2014). The distributed representations are shown to be more effective in non-linear architectures compared to the traditional linear classifier (Wang and Manning, 2013). Keeping in line with this trend, Chen and Manning (Chen and Manning, 2014) introduced a compact neural network based classifier for use in a greedy, transition-based dependency parser that learns using dense vector representations not only of words, but also of part-of-speech (POS) tags, dependency labels, etc. In our task of parsing Indian languages, a similar transition-based parser b"
W17-6529,W16-1716,1,0.806919,"d information and sanitychecked. The Telugu treebank data corresponding to BIS tagset is still being built so we used the data from ICON10 parsing contest (Husain et al., 2010). It was cleaned and appended with some more sentences. We automatically converted this data from Anncorra tagset to BIS tagset against some word lists and rules. Since 149 sentences are lost in automatic conversion we report results on both the datasets. The statistics of the treebank data in this work can be found in the Table 1. Previous work has been done to convert the Hindi Treebank to Universal Dependencies (UD) (Tandon et al., 2016). These new treebanks which are built on the same underlying principle, could also be converted to UD by the same process as a future work. 2.2 Both the K¯araka and Non-k¯araka relations in the scheme are given in Table 2. The * in the gloss name signifies that the relation can be more granular in function and branches to different types. 5 Relation k1 k2* k3 k4* k5 k7* rt rh ras k*u k*s r6 relc rs adv adj Meaning Agent / Subject / Doer Theme / Patient / Goal Instrument Recipient / Experiencer Source Spatio-temporal Purpose Cause Associative Comparative (Predicative) Noun / Adjective Complemen"
W17-6529,W10-1401,0,0.0778965,"Missing"
W17-6529,J13-1003,0,0.0428532,"Missing"
W17-6529,P16-3006,1,0.847602,"Missing"
W17-6529,I13-1183,0,0.0144584,"engineering expertise. Though these indicative features brought enormously high parsing accuracies, they were computationally expensive to extract and also posed the problem of data sparsity. To address the problem of discrete representations of words, distributional representations became a critical component of NLP tasks such as POS tagging (Collobert et al., 2011), constituency parsing (Socher et al., 2013) and machine translation (Devlin et al., 2014). The distributed representations are shown to be more effective in non-linear architectures compared to the traditional linear classifier (Wang and Manning, 2013). Keeping in line with this trend, Chen and Manning (Chen and Manning, 2014) introduced a compact neural network based classifier for use in a greedy, transition-based dependency parser that learns using dense vector representations not only of words, but also of part-of-speech (POS) tags, dependency labels, etc. In our task of parsing Indian languages, a similar transition-based parser based on their model has been used. This model handles the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014). This paper presents our work to apply non linear neural"
W17-7503,D09-1120,0,0.229744,"fine-tuned model is employed to generate a synthetic corpus again, on which we perform Coarse Learning for the next training iteration. Thus, this is a cyclical process, which is stopped when further increase in accuracy is observed to be negligible. The following sections explain the three phases in detail : 4 We train our own SMT model since the training, validation and testing sets used by Sata-Anuvadak are 17 unavailable to us. 4.1 Coarse Learning Coarse Learning is a form of weak supervision, which is a machine learning paradigm where the model learns from noisy data or prior knowledge. (Haghighi and Klein, 2009) used rich syntactic and semantic features to induce prior knowledge for the task of coreference resolution. (Ratner et al., 2016) uses an ensemble of weak learners using rules to identify biomedical entities from medical documents. Figure 2: Three-phase approach to improve robustness and accuracy. The entire cycle is repeated until the increase in accuracy is minimal. We conduct three self-training iterations. Large annotated parallel corpora are not easy to obtain for Indian languages. However, it is easier to use an existing MT system to generate a sub-optimal translation of a monolingual c"
W17-7503,jawaid-etal-2014-tagged,0,0.0183022,"igned across eleven Indian languages. We employed manual preprocessing to eliminate misalignments - the resultant dataset has a size of 47,382 sentences. These are split randomly into training set, validation set and test set containing 44,000, 1382 and 2000 sentences respectively. hin pan tam guj ben urd* hin ben guj hin pan guj tam ben urd tel kon eng mar mal Vocabulary 39170 849679 62780 86462 50553 36738 86997 70030 35134 77057 101869 The statistics for the ILCI corpus are given in Table 1. We use the EMILLE monolingual corpora (McEnery et al., 2000) for five languages and the UrMonoCorp (Jawaid et al., 2014) for Coarse Learning detailed in Section 4.2 . These statistics are given in Table 2. In addition to these, we extract samples from the EMILLE (McEnery et al., 2000) parallel corpus for the Housing and Legal domains. These datasets are used as test sets to show coverage of our NMT model. Details are given in Table 3. 1 This corpus is available on request from TDIL : https://goo.gl/VHYST 2 We extract a sample containing 500,000 sentences 15 from UrMonoCorp Tokens 11986152 14285063 17170697 12766111 2671369 8744825 Vocabulary 321356 272771 1285031 660465 243531 157133 Table 3: Parallel Corpus St"
W17-7503,D13-1176,0,0.0562164,"iding high quality translations despite the lack of large parallel corpora. In this paper, we demonstrate a three-phase integrated approach which combines weakly supervised and semisupervised learning with NMT techniques to build a robust model using a limited amount of parallel data. We conduct experiments for five language pairs (thereby generating ten systems) and our results show a substantial increase in translation quality over a baseline NMT model trained only on parallel data. 1 Introduction Neural Machine Translation (NMT) is an emerging technique which utilizes deep neural networks (Kalchbrenner and Blunsom, 2013), (Sutskever et al., 2014), (Bahdanau et al., 2014) to generate end-to-end translation. NMT has shown promising results for various language pairs and has been consistently performing better than Phrase based SMT, the state-of-the-art MT paradigm until a few years back. A major benefit in NMT 13 which makes it so popular is its ability to Dipti Misra Sharma LTRC IIIT Hyderabad use deep neural networks and learn linguistic information from the parallel data itself without being fed any learning features. This makes it a conceptually simple method which provides significantly better translations"
W17-7503,P17-4012,0,0.119364,"of the source hidden states is tracked and reference is done to the relevant ones when needed. This increases the translation quality for longer sentences. Further, local and global attention mechanism was proposed by (Luong et al., 2015). We employ encoder-decoder system with LSTM units trained to optimize maximum-likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). We also use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention plus feed-input model (Luong et al., 2015). We use OpenNMT-Lua (Klein et al., 2017) for building the models. The learning rate is set to 1 for both coarse learning and fine tuning. Our primary motive for the coarse learning stage is to learn only the general features from the synthetic corpus, thereby making it easier to fine tune the model. Hence, the decay rate is set to be 0.9 and 0.97 for coarse learning and fine tuning respectively, which results in significantly faster convergence for the former. Due to the same reason, the dropout ratio is kept higher for coarse learning (0.5) as compared to fine tuning (0.3). As the decay rate is higher for coarse learning, we run it"
W17-7503,P07-2045,0,0.0168831,"strate that we are able to build a robust NMT model which produces quality translation and delivers promising results, significantly better than a baseline NMT model. 2 Related Work NMT methods are data hungry. Efficient NMT for Indian languages is a challenging problem, owing to multiple reasons including morphological complexity and diversity, in addition to a lack of resources for many languages. Advances in the recent past mainly employ statistical and rule based methods for MT. (Kunchukuttan et al., 2014) uses statistical phrase based machine translation for Indian Languages using Moses (Koehn et al., 2007) for phrase extraction as well as lexicalized reordering. Sampark (Anthes, 2010) is a transfer based system for translation between 18 Indian language pairs, which uses 14 a common lexical transfer engine, whereas minimum structural transfer is required between Indian languages. (Kunchukuttan and Bhattacharyya, 2016) use orthographic features along with SMT to reach state of the art results in SMT for related languages. The use of monolingual data to improve translation accuracy in NMT was first proposed by (Gulcehre et al., 2015). Monolingual models were trained independently and then were in"
W17-7503,D16-1196,0,0.0208319,"cluding morphological complexity and diversity, in addition to a lack of resources for many languages. Advances in the recent past mainly employ statistical and rule based methods for MT. (Kunchukuttan et al., 2014) uses statistical phrase based machine translation for Indian Languages using Moses (Koehn et al., 2007) for phrase extraction as well as lexicalized reordering. Sampark (Anthes, 2010) is a transfer based system for translation between 18 Indian language pairs, which uses 14 a common lexical transfer engine, whereas minimum structural transfer is required between Indian languages. (Kunchukuttan and Bhattacharyya, 2016) use orthographic features along with SMT to reach state of the art results in SMT for related languages. The use of monolingual data to improve translation accuracy in NMT was first proposed by (Gulcehre et al., 2015). Monolingual models were trained independently and then were integrated to decoder module either through rescoring of the beam (shallow fusion), or by adding the recurrent hidden state of the language model to the decoder state of the encoder-decoder network, with an additional controller mechanism that controls the magnitude of the LM signal (deep fusion). (Sennrich et al., 201"
W17-7503,kunchukuttan-etal-2014-shata,0,0.338432,"um performance and conduct experiments on different Indian language pairs using the proposed approach. We demonstrate that we are able to build a robust NMT model which produces quality translation and delivers promising results, significantly better than a baseline NMT model. 2 Related Work NMT methods are data hungry. Efficient NMT for Indian languages is a challenging problem, owing to multiple reasons including morphological complexity and diversity, in addition to a lack of resources for many languages. Advances in the recent past mainly employ statistical and rule based methods for MT. (Kunchukuttan et al., 2014) uses statistical phrase based machine translation for Indian Languages using Moses (Koehn et al., 2007) for phrase extraction as well as lexicalized reordering. Sampark (Anthes, 2010) is a transfer based system for translation between 18 Indian language pairs, which uses 14 a common lexical transfer engine, whereas minimum structural transfer is required between Indian languages. (Kunchukuttan and Bhattacharyya, 2016) use orthographic features along with SMT to reach state of the art results in SMT for related languages. The use of monolingual data to improve translation accuracy in NMT was f"
W17-7503,D15-1166,0,0.0545507,"|y<j , x, s) (1) The entire model is jointly trained to maximize the (conditional) log-likelihood of the parallel training corpus: max θ N 1 ∑ log pθ (y (n) |x(n) ) N n=1 (2) where (y (n) , x(n) ) represents the nth sentence in parallel corpus of size N and θ denotes the set of all tunable parameters. (Bahdanau et al., 2014) proposed an atten16 tion mechanism so that the memory of the source hidden states is tracked and reference is done to the relevant ones when needed. This increases the translation quality for longer sentences. Further, local and global attention mechanism was proposed by (Luong et al., 2015). We employ encoder-decoder system with LSTM units trained to optimize maximum-likelihood (via a softmax layer) with back-propagation through time (Werbos, 1990). We also use an attention mechanism that allows the target decoder to look back at the source encoder, specifically the local attention plus feed-input model (Luong et al., 2015). We use OpenNMT-Lua (Klein et al., 2017) for building the models. The learning rate is set to 1 for both coarse learning and fine tuning. Our primary motive for the coarse learning stage is to learn only the general features from the synthetic corpus, thereby"
W17-7503,N06-1020,0,0.0687764,"languages having similar structure, like Fr ←→ En (French - English) showed better improvement in performance as compared to other languages having little similarity, like Uz ←→ En (Uzbek - English). Our approach is based on the intuition that transfer learning between the same language pair should perform better than its multilingual counterpart. The experimental results described in Section 5 demonstrate that the above intuition stands correct. During fine-tuning, the change in weights in each epoch learnt through transfer learning allows the model to align more towards the correct model. (McClosky et al., 2006) proposed using self-training for the task of parsing. We have experimented with its use in Neural machine Table 2: Monolingual Corpora statistics EMILLE and *UrMonoCorp translation. 3 Experimental Setup 3.1 Datasets We employ a small parallel corpus and large monolingual corpora for training. For the former, we use the multilingual Indian Language Corpora Initiative (ILCI) corpus 1 , which contains 50,000 sentences from the health and tourism domains aligned across eleven Indian languages. We employed manual preprocessing to eliminate misalignments - the resultant dataset has a size of 47,382"
W17-7503,2000.bcs-1.11,0,0.201032,"tains 50,000 sentences from the health and tourism domains aligned across eleven Indian languages. We employed manual preprocessing to eliminate misalignments - the resultant dataset has a size of 47,382 sentences. These are split randomly into training set, validation set and test set containing 44,000, 1382 and 2000 sentences respectively. hin pan tam guj ben urd* hin ben guj hin pan guj tam ben urd tel kon eng mar mal Vocabulary 39170 849679 62780 86462 50553 36738 86997 70030 35134 77057 101869 The statistics for the ILCI corpus are given in Table 1. We use the EMILLE monolingual corpora (McEnery et al., 2000) for five languages and the UrMonoCorp (Jawaid et al., 2014) for Coarse Learning detailed in Section 4.2 . These statistics are given in Table 2. In addition to these, we extract samples from the EMILLE (McEnery et al., 2000) parallel corpus for the Housing and Legal domains. These datasets are used as test sets to show coverage of our NMT model. Details are given in Table 3. 1 This corpus is available on request from TDIL : https://goo.gl/VHYST 2 We extract a sample containing 500,000 sentences 15 from UrMonoCorp Tokens 11986152 14285063 17170697 12766111 2671369 8744825 Vocabulary 321356 272"
W17-7503,rapp-vide-2006-example,0,0.0806444,"Missing"
W17-7503,P16-1009,0,0.0343594,"Bhattacharyya, 2016) use orthographic features along with SMT to reach state of the art results in SMT for related languages. The use of monolingual data to improve translation accuracy in NMT was first proposed by (Gulcehre et al., 2015). Monolingual models were trained independently and then were integrated to decoder module either through rescoring of the beam (shallow fusion), or by adding the recurrent hidden state of the language model to the decoder state of the encoder-decoder network, with an additional controller mechanism that controls the magnitude of the LM signal (deep fusion). (Sennrich et al., 2016) proposed use of synthetic data, a parallel data corpus generated using back-translation along with parallel corpus to increase the translation accuracy. Our method differs from them since it is three-phased. In the first phase, we train our model over a synthetic corpus generated using a suboptimal MT technique, and then fine tune it further on gold data. This allows better control over training during various stages - leading to better translation quality for Indian languages. Our second phase is inspired from (Zoph et al., 2016). They use transfer learning to increase translation quality be"
W17-7503,D16-1163,0,0.0904093,"at controls the magnitude of the LM signal (deep fusion). (Sennrich et al., 2016) proposed use of synthetic data, a parallel data corpus generated using back-translation along with parallel corpus to increase the translation accuracy. Our method differs from them since it is three-phased. In the first phase, we train our model over a synthetic corpus generated using a suboptimal MT technique, and then fine tune it further on gold data. This allows better control over training during various stages - leading to better translation quality for Indian languages. Our second phase is inspired from (Zoph et al., 2016). They use transfer learning to increase translation quality between resource scarce language pairs by incorporating the weights learnt during training for high resource language pairs. It was also found that languages having similar structure, like Fr ←→ En (French - English) showed better improvement in performance as compared to other languages having little similarity, like Uz ←→ En (Uzbek - English). Our approach is based on the intuition that transfer learning between the same language pair should perform better than its multilingual counterpart. The experimental results described in Sec"
W17-7505,D16-1025,0,0.0265739,"Missing"
W17-7505,J95-3006,0,0.674163,"Missing"
W17-7505,D09-1120,0,0.102267,"Missing"
W17-7505,D10-1092,0,0.0219809,"e both collected human judgement data to evaluate a wide spectrum of metrics. However, the problem of reordering has not been addressed much so far. The primary evalutaion metrics which exist currently for scoring translations are BLEU, METEOR, RIBES and NIST. BLEU (Papineni et al., 2002) measures the number of overlapping n-grams in a given translation when compared to a reference translation, giving higher scores to sequential words. METEOR (Lavie and Denkowski, 2009) scores translations using alignments based on exact, stem, synonym, and paraphrase matches between words and phrases. RIBES (Isozaki et al., 2010) is based on rank correlation coefficients modified with precision. NIST (Doddington, 2002) is a variation of BLEU; where instead of treating all n-grams equally, weightage is given on how informative a particular n-gram is. We report the BLEU score as a measure to test accuracy for the 110 NMT systems to maintain brevity. However, for the language-pair English -&gt; Hindi; we report all of the above scores. We also describe the challenges in evaluating MT accuracy keeping this language pair in consideration, however it should be noted that the same or similar challenges are faced when dealing wi"
W17-7505,jha-2010-tdil,0,0.0662979,"Missing"
W17-7505,P17-4012,0,0.0255427,"Missing"
W17-7505,P07-2045,0,0.00913893,"Missing"
W17-7505,kunchukuttan-etal-2014-shata,0,0.0371447,"Missing"
W17-7505,2015.iwslt-evaluation.11,0,0.106487,"Missing"
W17-7505,D15-1166,0,0.0192625,"Missing"
W17-7505,P02-1040,0,0.109825,"nsuming. There has been significant inter35 est in this area, both in terms of development as well as evaluation of MT metrics. The Workshop on Statistical Machine Transla- tion (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison- Burch et al., 2009) and the NIST Metrics for Machine Translation 2008 Evaluation 1 have both collected human judgement data to evaluate a wide spectrum of metrics. However, the problem of reordering has not been addressed much so far. The primary evalutaion metrics which exist currently for scoring translations are BLEU, METEOR, RIBES and NIST. BLEU (Papineni et al., 2002) measures the number of overlapping n-grams in a given translation when compared to a reference translation, giving higher scores to sequential words. METEOR (Lavie and Denkowski, 2009) scores translations using alignments based on exact, stem, synonym, and paraphrase matches between words and phrases. RIBES (Isozaki et al., 2010) is based on rank correlation coefficients modified with precision. NIST (Doddington, 2002) is a variation of BLEU; where instead of treating all n-grams equally, weightage is given on how informative a particular n-gram is. We report the BLEU score as a measure to te"
W17-7505,2006.amta-papers.25,0,0.166058,"Missing"
W17-7505,D16-1163,0,0.0358978,"Missing"
W17-7507,D10-1056,0,0.0221925,"Missing"
W17-7507,W02-1001,0,0.55191,"Missing"
W17-7507,P11-1061,0,0.0532715,"Missing"
W17-7507,H01-1035,0,0.0787926,"Missing"
W17-7561,S07-1002,0,0.0373376,"ith small/moderate set of untagged corpus without requiring knowledge structures and linguistic resources. To handle the WSD task related challenges of resource-poor languages some specific methods have been proposed. For Chinese language, Yang and Huang (2012) propose handling data sparsity issue by using synonyms for expansion of context, their first method regards synonyms as topic contextual feature to train Bayesian model and second method treats context words made up of synonyms as pseudo training data. Baskaya and Jurgens (2016) propose a Word Sense Induction and Disambiguation (WSID) (Agirre and Soroa, 2007) model in which they combine a small amount of sense-tagged data with information obtained from word sense induction (a fully unsupervised technique that automatically learns the different senses of a word based on how it is used). Yu et al. (2011), Khapra et al. (2011b), Khapra et al. (2011a) and Bhingardive et al. (2013) propose methods to use one language to help other language by means of multilingual parallel corpora, multilingual dictionary, translation and bilingual bootstrapping. Mancini et al. (2016) and Bhingardive et al. (2015a) propose to use word and sense embeddings derived from"
W17-7561,W15-5908,0,0.228221,"uch knowledge structures is a costly and time taking process which requires extensive amount of domain resources and linguistic expertise. Along with this, domain expertise is also needed to create and select hand crafted features and rules from the training data which are required in the automated methods. These requirements make it difficult to design a WSD algorithm for (6500+) (Nakov and Ng, 2009) “resourcepoor” languages. The existing literature on WSD methods report that the naive Most Frequent Sense (MFS) baseline obtained from a sense-tagged corpus is very hard to beat (Navigli, 2009; Bhingardive et al., 2015b). When (Preiss et al., 2009) tried to refine the selection of most frequent sense by using supplementary linguistic resources like POS tagger and Lemmatizer of the concerned language they found that performance of such a system is limited by the performance of used linguistic resources. This observation shows that for resourcepoor languages use of other linguistic resources is not much beneficial in WSD task, since their performances are also dependent on the availability of tagged/knowledge corpus. This inspires us to explore methods for WSD which do not rely on other linguistic resources a"
W17-7561,N15-1132,0,0.0706702,"Missing"
W17-7561,N09-1004,0,0.0239705,"g corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995; Khapra et al., 2011b), neural network (Taghipour and Ng, 2015; Yuan et al., 2016) and word sense induction (Baskaya and Jurgens, 2016). All types of WSD algorithms require knowledge structures and resources like, WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2"
W17-7561,P16-1085,0,0.0142113,"ed work. Section 3 describes our proposed approach. Section 4 presents and discusses the results and Section 5 concludes the paper and mentions future work directions. 2 Related Work Generally, all the existing WSD techniques can be categorized into one of the following approaches (Navigli, 2009; Pal and Saha, 2015): i) Knowledge based approach, which uses knowledge structures like, WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2002) or machine readable dictionaries (Lesk, 1986), ii) Supervised approach, which uses machine learning (K˚ageb¨ack and Salomonsson, 2016) and statistical methods (Iacobacci et al., 2016) on manually created sensetagged training corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource"
W17-7561,W02-0808,0,0.105889,"lomonsson, 2016) and statistical methods (Iacobacci et al., 2016) on manually created sensetagged training corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995; Khapra et al., 2011b), neural network (Taghipour and Ng, 2015; Yuan et al., 2016) and word sense induction (Baskaya and Jurgens, 2016). All types of WSD alg"
W17-7561,jha-2010-tdil,0,0.0307791,"If test word, Wt , present in sense-tagged text collection and is also present in Single Sense Word List then output associated sense ID. Else, find its context (Wtl , Wtr ) from test sentence and apply Algo 1. 6. If test word, Wt , is not present in sensetagged text collection then find its context (Wtl , Wtr ) from test sentence and apply Algo 2. 4 Results and Discussion We have used publicly available Health and Tourism domain sense-tagged corpus of Hindi and Marathi languages created by IIT Mumbai1 (Khapra et al., 2010) and Hindi language raw untagged Health and Tourism domain ILCI data (Jha, 2010). Table 2 gives the dataset details. Table 1 shows average 4-fold cross validation results obtained by our algorithm for polysemous test words which are not present in the sense-tagged training set. Table 3 presents the average 4-fold cross validation results obtained for polysemous test words along with Random Baseline and MFS baseline results. 1 Available at http://www.cfilt.iitb.ac.in/ wsd/annotated_corpus/ The results are presented in terms of Precision, Recall and F-Score accuracy measures as defined below (Navigli, 2009): No. of correctly predicted test words Total No. of predicted test"
W17-7561,W16-5307,0,0.0449277,"Missing"
W17-7561,P10-1155,0,0.0199201,"08 ity of words in sense-tagged training set and takes Word Sense Disambiguation Method 5. If test word, Wt , present in sense-tagged text collection and is also present in Single Sense Word List then output associated sense ID. Else, find its context (Wtl , Wtr ) from test sentence and apply Algo 1. 6. If test word, Wt , is not present in sensetagged text collection then find its context (Wtl , Wtr ) from test sentence and apply Algo 2. 4 Results and Discussion We have used publicly available Health and Tourism domain sense-tagged corpus of Hindi and Marathi languages created by IIT Mumbai1 (Khapra et al., 2010) and Hindi language raw untagged Health and Tourism domain ILCI data (Jha, 2010). Table 2 gives the dataset details. Table 1 shows average 4-fold cross validation results obtained by our algorithm for polysemous test words which are not present in the sense-tagged training set. Table 3 presents the average 4-fold cross validation results obtained for polysemous test words along with Random Baseline and MFS baseline results. 1 Available at http://www.cfilt.iitb.ac.in/ wsd/annotated_corpus/ The results are presented in terms of Precision, Recall and F-Score accuracy measures as defined below (Na"
W17-7561,I11-1078,0,0.271496,"asily obtainable even for resource-poor languages. The obtained results show that our method performs well even with very small sized sensetagged training data for Hindi and Marathi languages and its performance is better than the Random Baseline (Navigli, 2009) which selects a random sense for each polysemous test word, comparable to the Most Frequent Sense (MFS) baseline 504 that selects the most frequent sense available in the sense-tagged training corpus for each polysemous word and at par with the reported results on the used datasets (Bhingardive et al., 2015a; Bhingardive et al., 2013; Khapra et al., 2011a; Khapra et al., 2011b; Khapra et al., 2008). Rest of the paper is organized as follows: Section 2 presents related work. Section 3 describes our proposed approach. Section 4 presents and discusses the results and Section 5 concludes the paper and mentions future work directions. 2 Related Work Generally, all the existing WSD techniques can be categorized into one of the following approaches (Navigli, 2009; Pal and Saha, 2015): i) Knowledge based approach, which uses knowledge structures like, WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2002) or machine readable dictionaries (Lesk, 1986),"
W17-7561,P11-1057,0,0.135119,"asily obtainable even for resource-poor languages. The obtained results show that our method performs well even with very small sized sensetagged training data for Hindi and Marathi languages and its performance is better than the Random Baseline (Navigli, 2009) which selects a random sense for each polysemous test word, comparable to the Most Frequent Sense (MFS) baseline 504 that selects the most frequent sense available in the sense-tagged training corpus for each polysemous word and at par with the reported results on the used datasets (Bhingardive et al., 2015a; Bhingardive et al., 2013; Khapra et al., 2011a; Khapra et al., 2011b; Khapra et al., 2008). Rest of the paper is organized as follows: Section 2 presents related work. Section 3 describes our proposed approach. Section 4 presents and discusses the results and Section 5 concludes the paper and mentions future work directions. 2 Related Work Generally, all the existing WSD techniques can be categorized into one of the following approaches (Navigli, 2009; Pal and Saha, 2015): i) Knowledge based approach, which uses knowledge structures like, WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2002) or machine readable dictionaries (Lesk, 1986),"
W17-7561,P98-2127,0,0.52769,"and Pedersen, 2002) or machine readable dictionaries (Lesk, 1986), ii) Supervised approach, which uses machine learning (K˚ageb¨ack and Salomonsson, 2016) and statistical methods (Iacobacci et al., 2016) on manually created sensetagged training corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995; Khapra et al"
W17-7561,S07-1086,0,0.0425557,"eated sensetagged training corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995; Khapra et al., 2011b), neural network (Taghipour and Ng, 2015; Yuan et al., 2016) and word sense induction (Baskaya and Jurgens, 2016). All types of WSD algorithms require knowledge structures and resources like, WordNet (Fellbaum, 1998; Baner"
W17-7561,W97-0322,0,0.115652,"(Fellbaum, 1998; Banerjee and Pedersen, 2002) or machine readable dictionaries (Lesk, 1986), ii) Supervised approach, which uses machine learning (K˚ageb¨ack and Salomonsson, 2016) and statistical methods (Iacobacci et al., 2016) on manually created sensetagged training corpus. It also requires domain expertise for creating and selecting features and rules to be used for preprocessing and transforming the training data into the form required for designing the algorithm (Navigli, 2009; Iacobacci et al., 2016), iii) Unsupervised approach, which uses large amount of raw untagged training corpus (Pedersen and Bruce, 1997; Lin, 1998) to find word clusters which discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995;"
W17-7561,W09-2403,0,0.0273464,"ly and time taking process which requires extensive amount of domain resources and linguistic expertise. Along with this, domain expertise is also needed to create and select hand crafted features and rules from the training data which are required in the automated methods. These requirements make it difficult to design a WSD algorithm for (6500+) (Nakov and Ng, 2009) “resourcepoor” languages. The existing literature on WSD methods report that the naive Most Frequent Sense (MFS) baseline obtained from a sense-tagged corpus is very hard to beat (Navigli, 2009; Bhingardive et al., 2015b). When (Preiss et al., 2009) tried to refine the selection of most frequent sense by using supplementary linguistic resources like POS tagger and Lemmatizer of the concerned language they found that performance of such a system is limited by the performance of used linguistic resources. This observation shows that for resourcepoor languages use of other linguistic resources is not much beneficial in WSD task, since their performances are also dependent on the availability of tagged/knowledge corpus. This inspires us to explore methods for WSD which do not rely on other linguistic resources and can take advantage of conte"
W17-7561,N15-1035,0,0.0248399,"h discriminates the senses of the words in different clusters, or use multilingual parallel corpora (Ide et al., 2002; Bhingardive et al., 2013), a knowledge resource like WordNet (Patwardhan et al., 2007; Chen et al., 2009; Bhingardive et al., 2015b; Bhingardive et al., 2015a) or multilingual dictionary (Khapra et al., 2011a), and iv) Semi-supervised approach, that uses both sense-tagged and untagged data in different proportions with different methods like, co-training with multilingual parallel corpora (Yu et al., 2011), bootstrapping (Yarowsky, 1995; Khapra et al., 2011b), neural network (Taghipour and Ng, 2015; Yuan et al., 2016) and word sense induction (Baskaya and Jurgens, 2016). All types of WSD algorithms require knowledge structures and resources like, WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2002), machine readable dictionaries (Lesk, 1986), sensetagged training corpus (Navigli, 2009), parallel corpora and lage untagged raw corpus. Creation of such knowledge structures and resources is a costly and time taking process which requires extensive amount of domain resources and linguistic expertise. Due to this, for resource-poor languages, special methods are needed which can handle data"
W17-7561,C12-2137,0,0.0187644,"rpora and lage untagged raw corpus. Creation of such knowledge structures and resources is a costly and time taking process which requires extensive amount of domain resources and linguistic expertise. Due to this, for resource-poor languages, special methods are needed which can handle data sparsity issue present in sense-tagged training data and can work with small/moderate set of untagged corpus without requiring knowledge structures and linguistic resources. To handle the WSD task related challenges of resource-poor languages some specific methods have been proposed. For Chinese language, Yang and Huang (2012) propose handling data sparsity issue by using synonyms for expansion of context, their first method regards synonyms as topic contextual feature to train Bayesian model and second method treats context words made up of synonyms as pseudo training data. Baskaya and Jurgens (2016) propose a Word Sense Induction and Disambiguation (WSID) (Agirre and Soroa, 2007) model in which they combine a small amount of sense-tagged data with information obtained from word sense induction (a fully unsupervised technique that automatically learns the different senses of a word based on how it is used). Yu et"
W17-7561,H93-1052,0,0.70213,", Sti = hW1 /SIDi , W2 /SIDj . . . Wn /SIDk , Wi is a word and SIDi is a sense ID from Γ and (3) raw untagged sentences set RD = {RS1 , RS2 . . . RSM }, where RSi = hW1 W2 . . . Wm i, build a WSD model Θ which outputs the best sense ID sequence hSID1 SID2 . . . SIDl i for an input sequence of words hW1 W2 . . . Wl i. Here, we propose a semi-supervised WSD method which uses the concept of context based list (Rani et al., 2016) to build the WSD model from a set of sense-tagged and raw untagged training corpus. Our proposed method is also influenced by the one sense per collocation hypothesis of Yarowsky (1993) which tells that the sense of a word in a document is effectively determined by its context (Yarowsky, 1995). Our approach takes help of raw untagged data and expands the notions of context and context based list (Rani et al., 2016) to tackle the data sparsity issue. Our method does not require any preprocessing such as, stop/noncontent word removal and feature generation and selection from the sense-tagged training corpus. It works without using any additional knowledge structure like dictionary etc., other than the small sense-tagged corpus and moderate sized raw untagged data. This is easi"
W17-7561,P95-1026,0,0.896022,"sentences set RD = {RS1 , RS2 . . . RSM }, where RSi = hW1 W2 . . . Wm i, build a WSD model Θ which outputs the best sense ID sequence hSID1 SID2 . . . SIDl i for an input sequence of words hW1 W2 . . . Wl i. Here, we propose a semi-supervised WSD method which uses the concept of context based list (Rani et al., 2016) to build the WSD model from a set of sense-tagged and raw untagged training corpus. Our proposed method is also influenced by the one sense per collocation hypothesis of Yarowsky (1993) which tells that the sense of a word in a document is effectively determined by its context (Yarowsky, 1995). Our approach takes help of raw untagged data and expands the notions of context and context based list (Rani et al., 2016) to tackle the data sparsity issue. Our method does not require any preprocessing such as, stop/noncontent word removal and feature generation and selection from the sense-tagged training corpus. It works without using any additional knowledge structure like dictionary etc., other than the small sense-tagged corpus and moderate sized raw untagged data. This is easily obtainable even for resource-poor languages. The obtained results show that our method performs well even"
W17-7561,D09-1141,0,0.0369429,"umans to WSD algorithms via knowledge structures like WordNet (Fellbaum, 1998; Banerjee and Pedersen, 2002), machine readable dictionaries (Lesk, 1986) and sense-tagged training corpus (Navigli, 2009). Creation of such knowledge structures is a costly and time taking process which requires extensive amount of domain resources and linguistic expertise. Along with this, domain expertise is also needed to create and select hand crafted features and rules from the training data which are required in the automated methods. These requirements make it difficult to design a WSD algorithm for (6500+) (Nakov and Ng, 2009) “resourcepoor” languages. The existing literature on WSD methods report that the naive Most Frequent Sense (MFS) baseline obtained from a sense-tagged corpus is very hard to beat (Navigli, 2009; Bhingardive et al., 2015b). When (Preiss et al., 2009) tried to refine the selection of most frequent sense by using supplementary linguistic resources like POS tagger and Lemmatizer of the concerned language they found that performance of such a system is limited by the performance of used linguistic resources. This observation shows that for resourcepoor languages use of other linguistic resources i"
W19-4020,bhatia-etal-2010-empty,1,0.856783,"Missing"
W19-4020,W09-3036,1,0.928838,"indi-English code mixed tweets (1460) Identify verbs Identify Arguments Background and Related Work Complex Predicate The release of large corpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb Absent frame ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to Propbank mapping Annotate Figure 1: Data Creation workflow for gold annotation of the data 179 We built our corpus on syntactic information obtained from dependency labels. This allows us to annotate explicitly on the syntactic tree which e"
W19-4020,bonial-etal-2014-propbank,0,0.433599,". Tagset Label ARGA ARG0 ARG1 ARG2 ARG2 ATTR ARG2 LOC ARG2 GOL ARG2 SOU ARG3 ARGM DIR ARGM LOC ARGM MNR ARGM EXT ARGM TMP ARGM REC ARGM PRP ARGM CAU ARGM DIS ARGM ADV ARGM NEG ARGM PRX Frame File Creation 2. This frame file is used to annotate roles for any occurrence of the said verb to maintain consistency. Description Causer Agent or Experiencer or Doer Theme or Patient Benificiary Attribute or Quality Physical Location Destination or Goal Source Instrument Direction Location Manner Extent or Comparison Temporal Reciprocal Purpose Cause or Reason Discourse Adverb Negative Complex Predicate Bonial et al (2014) present a lexicon of frame-sets for English Propbank annotation. Vaidya et al. (2013) present Hindi Propbank frame files for simple verb constructions as well as for nominal-verb constructions. As Hindi Propbank is built on syntactic information from Dependency Treebank and we build our model on dependency labelled HindiEnglish code-mixed data, we use these frame files extensively for annotation of our corpus. We also refer to the English frame files to label the roles for simple English verbs in the corpus. Frame file for baca Roleset id: baca.01: to remain ARG1 Thing left Roleset id: baca.0"
W19-4020,W16-5801,0,0.201935,"Missing"
W19-4020,ahmed-etal-2012-reference,0,0.0406055,"Missing"
W19-4020,duran-aluisio-2012-propbank,0,0.0437077,"Missing"
W19-4020,P98-1013,0,0.618548,"annotation scheme in section 3. In section 4, we propose a baseline rule based system for manual annotation of the data using dependency label information. Section 5 talks about the results and working of our baseline system. We analyse cases of high errors in classification and explore reasons for the same. In Section 6 we shed light on future scope and conclude the paper. 2 3 Data Creation Hindi-English code mixed tweets (1460) Identify verbs Identify Arguments Background and Related Work Complex Predicate The release of large corpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGeral"
W19-4020,D15-1112,0,0.0456281,"Missing"
W19-4020,W14-3914,0,0.0664846,"Missing"
W19-4020,J02-3001,0,0.428484,"mation. Section 5 talks about the results and working of our baseline system. We analyse cases of high errors in classification and explore reasons for the same. In Section 6 we shed light on future scope and conclude the paper. 2 3 Data Creation Hindi-English code mixed tweets (1460) Identify verbs Identify Arguments Background and Related Work Complex Predicate The release of large corpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb Absent frame ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to"
W19-4020,W14-3902,0,0.0258848,"for Computational Linguistics 2015), deep learning methods (He et al., 2018b; Tan et al., 2018), joint prediction of predicates and its arguments (Toutanova et al., 2008; He et al., 2018a; Swayamdipta et al., 2018). Bali et. al (2014) analysed social media, Facebook in particular, and looking at the extent of Hindi-English code-mixed data available online, emphasise the need to develop NLP tools for codemixed social media data. Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline for Hindi-English codemixed data. Gupta et al. (2014) introduced the concept of Mixed-Script Information Retrieval and the problems posed by transliterated content such as spelling variations etc. There has been a surge of data set creation for code-mixed data (Bhat et al., 2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis ("
W19-4020,W13-3820,0,0.0283256,"lassification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) and so on. A Proposition Bank (Propbank) is a corpus of annotated semantic predicate-argument labels (Palmer et al., 2005). This is done with the help of verb frame files and the Proposition Bank tagset. The frame files contain the semantic roles needed for each verb and all the possible context variations of each verb (sense of the verb). To annotate, one must first identify the ‘sense id’ (Roleset id) of the verb present according to its usage, and then mark the corresponding labels present in its frame file. We follow exactly this proc"
W19-4020,E17-2052,1,0.849048,". Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline for Hindi-English codemixed data. Gupta et al. (2014) introduced the concept of Mixed-Script Information Retrieval and the problems posed by transliterated content such as spelling variations etc. There has been a surge of data set creation for code-mixed data (Bhat et al., 2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surde"
W19-4020,P18-2058,0,0.0189621,"h means ‘time’ in Hindi, or “wicket” in the domain of cricket. As we have the context of the whole tweet and world knowledge about Umesh Yadav who is an Indian cricketer, we are able to disambiguate the usage of the token ‘wkt’, though this may not always be the case. In this paper, we present a data set of HindiEnglish code-mixed tweets labelled with semantic roles. These labels provide us with information of 178 Proceedings of the 13th Linguistic Annotation Workshop, pages 178–188 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2015), deep learning methods (He et al., 2018b; Tan et al., 2018), joint prediction of predicates and its arguments (Toutanova et al., 2008; He et al., 2018a; Swayamdipta et al., 2018). Bali et. al (2014) analysed social media, Facebook in particular, and looking at the extent of Hindi-English code-mixed data available online, emphasise the need to develop NLP tools for codemixed social media data. Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Be"
W19-4020,N16-1159,1,0.936825,"of predicates and its arguments (Toutanova et al., 2008; He et al., 2018a; Swayamdipta et al., 2018). Bali et. al (2014) analysed social media, Facebook in particular, and looking at the extent of Hindi-English code-mixed data available online, emphasise the need to develop NLP tools for codemixed social media data. Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline for Hindi-English codemixed data. Gupta et al. (2014) introduced the concept of Mixed-Script Information Retrieval and the problems posed by transliterated content such as spelling variations etc. There has been a surge of data set creation for code-mixed data (Bhat et al., 2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a giv"
W19-4020,P18-1192,0,0.0226487,"h means ‘time’ in Hindi, or “wicket” in the domain of cricket. As we have the context of the whole tweet and world knowledge about Umesh Yadav who is an Indian cricketer, we are able to disambiguate the usage of the token ‘wkt’, though this may not always be the case. In this paper, we present a data set of HindiEnglish code-mixed tweets labelled with semantic roles. These labels provide us with information of 178 Proceedings of the 13th Linguistic Annotation Workshop, pages 178–188 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2015), deep learning methods (He et al., 2018b; Tan et al., 2018), joint prediction of predicates and its arguments (Toutanova et al., 2008; He et al., 2018a; Swayamdipta et al., 2018). Bali et. al (2014) analysed social media, Facebook in particular, and looking at the extent of Hindi-English code-mixed data available online, emphasise the need to develop NLP tools for codemixed social media data. Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Be"
W19-4020,D07-1002,0,0.186251,"tion (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) and so on. A Proposition Bank (Propbank) is a corpus of annotated semantic predicate-argument labels (Palmer et al., 2005). This is done with the help of verb frame files and the Proposition Bank tagset. The frame files contain the semantic roles needed for each verb and all the possible context variations of each verb (sense of the verb). To annotate, one must first identify the ‘sense id’ (Roleset id) of the verb present according to its usage, and then mark the corresponding labels present in its frame file. We follow exactly this process for the manual annotation of our corpus. The struct"
W19-4020,W10-1810,0,0.0296521,"ic information present. The Hindi Propbank was built as Table 2 shows a frame file for the Hindi verb ‘baca’. The rolesets in the frame file give us the senses of the predicate and the different arguments 180 it may take depending on the context in which it is used. In certain cases, we had to create new frame files for novel occurrences of verbs and absence of the relevant frame file. We also created frame files for inter-language complex predicate formations and noted the dependency label to Propbank label mapping. ation of lexical resources for annotation of complex predicates for English (Hwang et al., 2010) and Hindi (Vaidya et al., 2013) in the form of frame files. In our corpus, we observe complex predicate formations within the same language (intra-language) as well as between the two languages (inter-language or code-mixed). We have 462 unique complex predicates in our corpus. Table 4 gives the distribution of these in our data. Most of these complex predicates are nounverb constructions, also known as light verb constructions. Light verbs in Hindi are highly productive and can entirely change the meaning of the predicate. For instance, ‘hona’ (to be) and ‘karna’ (to do) are two Hindi light"
W19-4020,W18-2405,0,0.0161549,"s onto roman script. Barman et al. (2014) addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline for Hindi-English codemixed data. Gupta et al. (2014) introduced the concept of Mixed-Script Information Retrieval and the problems posed by transliterated content such as spelling variations etc. There has been a surge of data set creation for code-mixed data (Bhat et al., 2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) an"
W19-4020,kingsbury-palmer-2002-treebank,0,0.662605,"section 4, we propose a baseline rule based system for manual annotation of the data using dependency label information. Section 5 talks about the results and working of our baseline system. We analyse cases of high errors in classification and explore reasons for the same. In Section 6 we shed light on future scope and conclude the paper. 2 3 Data Creation Hindi-English code mixed tweets (1460) Identify verbs Identify Arguments Background and Related Work Complex Predicate The release of large corpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb A"
W19-4020,D08-1110,0,0.255713,"Missing"
W19-4020,P15-2036,0,0.020115,"ations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb Absent frame ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to Propbank mapping Annotate Figure 1: Data Creation workflow for gold annotation of the data 179 We built our corpus on syntactic information obtained from dependency labels. This allows us to annotate explicitly on the syntactic tree which enables consistency between Propbank structure and dependency structure. Dependency labels provide us with rich syntactic-semantic relations which facilitates mapping betwee"
W19-4020,P03-1002,0,0.145559,"2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) and so on. A Proposition Bank (Propbank) is a corpus of annotated semantic predicate-argument labels (Palmer et al., 2005). This is done with the help of verb frame files and the Proposition Bank tagset. The frame files contain the semantic roles needed for each verb and all the possible context variations of each verb (sense of the verb). To annotate, one must first identify the ‘sense id’ (Roleset id) of the verb present according to its usage, and then mark th"
W19-4020,palmer-etal-2008-pilot,0,0.0249029,". 2 3 Data Creation Hindi-English code mixed tweets (1460) Identify verbs Identify Arguments Background and Related Work Complex Predicate The release of large corpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb Absent frame ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to Propbank mapping Annotate Figure 1: Data Creation workflow for gold annotation of the data 179 We built our corpus on syntactic information obtained from dependency labels. This allows us to annotate explicitly on the sy"
W19-4020,J08-2002,0,0.0475373,"f the whole tweet and world knowledge about Umesh Yadav who is an Indian cricketer, we are able to disambiguate the usage of the token ‘wkt’, though this may not always be the case. In this paper, we present a data set of HindiEnglish code-mixed tweets labelled with semantic roles. These labels provide us with information of 178 Proceedings of the 13th Linguistic Annotation Workshop, pages 178–188 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2015), deep learning methods (He et al., 2018b; Tan et al., 2018), joint prediction of predicates and its arguments (Toutanova et al., 2008; He et al., 2018a; Swayamdipta et al., 2018). Bali et. al (2014) analysed social media, Facebook in particular, and looking at the extent of Hindi-English code-mixed data available online, emphasise the need to develop NLP tools for codemixed social media data. Vyas et al.(2014) worked on building a POS tagger for Hindi-English codemixed data and noted the difficulty posed by transliteration of Hindi tokens onto roman script. Barman et al. (2014) addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline f"
W19-4020,J05-1004,0,0.892493,"ment with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) and so on. A Proposition Bank (Propbank) is a corpus of annotated semantic predicate-argument labels (Palmer et al., 2005). This is done with the help of verb frame files and the Proposition Bank tagset. The frame files contain the semantic roles needed for each verb and all the possible context variations of each verb (sense of the verb). To annotate, one must first identify the ‘sense id’ (Roleset id) of the verb present according to its usage, and then mark the corresponding labels present in its frame file. We follow exactly this process for the manual annotation of our corpus. The structure of this paper is as follows. Section 2 talks about relevant work in the domains of Semantic Role Labelling and code-mix"
W19-4020,W11-0403,0,0.852241,"ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to Propbank mapping Annotate Figure 1: Data Creation workflow for gold annotation of the data 179 We built our corpus on syntactic information obtained from dependency labels. This allows us to annotate explicitly on the syntactic tree which enables consistency between Propbank structure and dependency structure. Dependency labels provide us with rich syntactic-semantic relations which facilitates mapping between dependency labels and Propbank labels. This would largely reduce annotation effort (Vaidya et al., 2011). We explore this in the working of our baseline model (Section 4). We present a Hindi-English code-mixed Twitter data set comprising 1460 tweets labelled with semantic roles according to the Hindi Propbank tagset. We use the corpus used by (Bhat et al., 2018) in which tweets are labelled with Paninian Dependency labels. Our corpus consists of simple verb constructions, in both Hindi and English, and also complex predicates which have been dealt with separately. These can be within the same language or across the two languages. Figure 1 shows the workflow for the gold annotation of the data. 3"
W19-4020,C16-1234,0,0.0177218,"addressed the problem of language identification on Bengali-Hindi-English Facebook comments. Sharma et al. (2016) built a shallow parsing pipeline for Hindi-English codemixed data. Gupta et al. (2014) introduced the concept of Mixed-Script Information Retrieval and the problems posed by transliterated content such as spelling variations etc. There has been a surge of data set creation for code-mixed data (Bhat et al., 2017; Gupta et al., 2016) and application based tools such as question classification (Raghavi et al., 2015), named-entity recognition (Singh et al., 2018), sentiment analysis (Prabhu et al., 2016; Ghosh et al., 2017) and so on. the role played by an argument with respect to a verb in a given sentence. We seek to gain semantic information irrespective of the syntactic variation a sentence or an utterance may have. Semantic Role Labelling for code-mixed data will aid in better understanding of these texts and further the research of any understanding based tasks such as information retrieval (Surdeanu et al., 2003; Moschitti et al., 2003), document classification (Bastianelli et al., 2013), questioning answering systems (Shen and Lapata, 2007) and so on. A Proposition Bank (Propbank) is"
W19-4020,vaidya-etal-2012-empty,0,0.0209357,". This doesn’t include empty categories for pronoun dropping but includes empty categories for Table 7: Frame file for the simple English verb ‘sleep’. The token for complex predicate is marked with the label ‘ARGM_PRX’according to the Propbank tagset. In the frame file for the verb ‘sleep’, given in Table 7, we can see possible rolesets or senses 182 3.3.2 dropped nouns, conjunctions, verbs etc. Empty categories were introduced in the Hindi Propbank to include core arguments missing from the predicate-argument structure after addition of the empty categories in the Hindi Dependency Treebank (Vaidya et al., 2012). Code-mixed language refers to the usage of linguistic units of one language in a sentence of another language. One fairly common preliminary step while annotating code-mixed data is Language Identification (Vyas et al., 2014; Sharma et al., 2016). The tokens present in the corpus are marked ‘hi’, for Hindi, or ‘en’, for English, or ‘ne’ for Named Entities. This assumes that codemixing doesn’t occur at sub-lexical levels. However, in our corpus, we came across a few cases where new lexical items are formed by mixing the two languages and modifying the morphology of the individual languages. O"
W19-4020,J08-2005,0,0.0555371,"rpora with semantic annotations like the FrameNet (Lowe, 1997; Baker et al., 1998) and Propbank (Kingsbury and Palmer, 2002) have enabled the training and testing of classifiers for automated annotation models. Gildea and Jurafsky (2002) initiated the work on 2001 release of the English Propbank with statistical classifiers and linguistic features. Since then, Propbanks have been created for different languages (Xue and Palmer, 2009; Palmer et al., 2008; Bhatt et al., 2009; Duran and Alu´ısio, 2012) and several advances have been made towards automating the process of Semantic Role Labelling (Punyakanok et al., 2008; Kshirsagar et al., 2015) using neural networks (FitzGerald et al., 2015; Zhou and Xu, Simple Verb Absent frame ﬁle Create verb Frame Hindi verb Refer verb frame and identify sense English Verb Mark Dependency to Propbank mapping Annotate Figure 1: Data Creation workflow for gold annotation of the data 179 We built our corpus on syntactic information obtained from dependency labels. This allows us to annotate explicitly on the syntactic tree which enables consistency between Propbank structure and dependency structure. Dependency labels provide us with rich syntactic-semantic relations which"
W19-4020,W13-1018,0,0.574417,"ARGM LOC ARGM MNR ARGM EXT ARGM TMP ARGM REC ARGM PRP ARGM CAU ARGM DIS ARGM ADV ARGM NEG ARGM PRX Frame File Creation 2. This frame file is used to annotate roles for any occurrence of the said verb to maintain consistency. Description Causer Agent or Experiencer or Doer Theme or Patient Benificiary Attribute or Quality Physical Location Destination or Goal Source Instrument Direction Location Manner Extent or Comparison Temporal Reciprocal Purpose Cause or Reason Discourse Adverb Negative Complex Predicate Bonial et al (2014) present a lexicon of frame-sets for English Propbank annotation. Vaidya et al. (2013) present Hindi Propbank frame files for simple verb constructions as well as for nominal-verb constructions. As Hindi Propbank is built on syntactic information from Dependency Treebank and we build our model on dependency labelled HindiEnglish code-mixed data, we use these frame files extensively for annotation of our corpus. We also refer to the English frame files to label the roles for simple English verbs in the corpus. Frame file for baca Roleset id: baca.01: to remain ARG1 Thing left Roleset id: baca.02: to avoid ARG0 person avoiding ARG1 Thing avoided Table 1: Hindi PropBank Tagset Tab"
W19-4020,D14-1105,0,0.0270095,"to the Propbank tagset. In the frame file for the verb ‘sleep’, given in Table 7, we can see possible rolesets or senses 182 3.3.2 dropped nouns, conjunctions, verbs etc. Empty categories were introduced in the Hindi Propbank to include core arguments missing from the predicate-argument structure after addition of the empty categories in the Hindi Dependency Treebank (Vaidya et al., 2012). Code-mixed language refers to the usage of linguistic units of one language in a sentence of another language. One fairly common preliminary step while annotating code-mixed data is Language Identification (Vyas et al., 2014; Sharma et al., 2016). The tokens present in the corpus are marked ‘hi’, for Hindi, or ‘en’, for English, or ‘ne’ for Named Entities. This assumes that codemixing doesn’t occur at sub-lexical levels. However, in our corpus, we came across a few cases where new lexical items are formed by mixing the two languages and modifying the morphology of the individual languages. One way of doing this is to add affixes from one language to a word of the other language. These constructions are used widely in day to day usage. We treat these cases as ‘Special Constructions’. T5: “Tore my calendar kyunki w"
W19-4020,P15-1109,0,0.052468,"Missing"
W19-5316,P07-2045,0,0.0106393,"the data used is provided in Table 1. where e(.) is the word embedding operation. Popular choices for mapping f are Long-Short-Term Memory (LSTM) units and Gated Recurrent Units (GRU), the former of which we use in our models. An NMT model is typically trained under the maximum log-likelihood objective: max J(θ) = max E(x,y)∼D [log Pθ (y|x)] Experimental setup Dataset Sentences Tokens IITB Hi-En Train Gu-En Train Gu-En Dev Gu-En Test 15,28,631 1,55,767 1,997 1,998 21.5M / 20.3M 1.68M / 1.58M 51.3K / 47.4K 51.5K / 47.5K 4.2 Multilingual Neural Machine Translation Data Processing We used Moses (Koehn et al., 2007) toolkit for tokenization and cleaning the English side of the data. Gujarati and Hindi sides of the data is first normalized with Indic NLP library1 followed by tokenization with the same library. As our preprocessing step, we removed all the sentences of length greater than 80 from our training corpus. Most of the practical applications in Machine Translation have focused on individual language pairs because it was simply too difficult to build a single system that translates to and from many language pairs. But Neural Machine Translation was shown to be an end-to-end learning approach and w"
W19-5316,J82-2005,0,0.669842,"Missing"
W19-5316,D15-1166,0,0.74163,"ormer models. This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the Gujarati→English news translation shared task of WMT19. Our system is based on encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English. 1 Dipti Misra Sharma IIIT Hyderabad dipti@iiit.ac.in Introduction Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2014; Johnson et al., 2017; Wu et al., 2017; Vaswani et al., 2017) has been receiving considerable attention in the recent years, given its superior performance without the demand of heavily hand crafted engineering efforts. NMT often outperforms Statistical Machine Translation (SMT) techniques but it still struggles if the parallel data is insufficient like in the case of Indian languages. The bulk of research on low resource NMT has focused on exploiting monolingual data or parallel data from other language pairs. Some recent methods to improve NMT models that exploit mono"
W19-5316,P15-1166,0,0.0240256,"i and Hindi sides of the data is first normalized with Indic NLP library1 followed by tokenization with the same library. As our preprocessing step, we removed all the sentences of length greater than 80 from our training corpus. Most of the practical applications in Machine Translation have focused on individual language pairs because it was simply too difficult to build a single system that translates to and from many language pairs. But Neural Machine Translation was shown to be an end-to-end learning approach and was quickly extended to multilingual machine translation in several ways. In Dong et al. (2015), the authors modify the attention-based encoderdecoder approach by introducing separate decoder 4.3 Subword Segmentation for NMT Neural Machine Translation relies on first mapping each word into the vector space, and tradi1 192 https://anoopkunchukuttan.github.io/indic nlp library/ model with Global Attention mechanism. We used an LSTM based Bi-directional encoder and a unidirectional decoder. We kept 4 layers in both the encoder & decoder with embedding size set to 512. The batch size was set to 64 and a dropout rate of 0.3. We used Adam optimizer (Kingma and Ba, 2014) for our experiments. O"
W19-5316,I17-2050,0,0.0377165,"ruggles if the parallel data is insufficient like in the case of Indian languages. The bulk of research on low resource NMT has focused on exploiting monolingual data or parallel data from other language pairs. Some recent methods to improve NMT models that exploit monolingual data ranges from back-translation (Sennrich et al., 2015a), dual NMT (He et al., 2016) to Unsupervised MT models (Lample et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Transfer Learning is also a promising approach for low resource NMT which exploits parallel data from other language pairs (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Typically it is achieved by training a parent model in a high resource language pair, then using some of the trained weights as the initialization for a child 2 Neural MT Architecture Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in (Luong et al., 2015). The model directly estimates the posterior distribution Pθ (y|x) of translating a source sentence x = (x1 , .., xn ) to a target sentence y = (y1 , .., ym ) as: Pθ (y|x) = m Y Pθ (yt |y1 , y2 , .., yt−1 , x) (1) t=1 191 Proceedings of the Fourth Conf"
W19-5316,N16-1101,0,0.035854,"tribution Pθ (y|x) of translating a source sentence x = (x1 , .., xn ) to a target sentence y = (y1 , .., ym ) as: Pθ (y|x) = m Y Pθ (yt |y1 , y2 , .., yt−1 , x) (1) t=1 191 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 191–195 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics and attention mechanism for each target language. In Zoph and Knight (2016), multi-source translation was proposed where the model has different encoders and different attention mechanisms for different source languages. In Firat et al. (2016), the authors proposed a multi-way multilingual NMT model using a single shared attention mechanism but with multiple encoders/decoders for each source/target language. In this paper, we adopted the approach proposed in Johnson et al. (2017), where a single NMT model is used for multilingual machine translation. We used HindiEnglish as our assisting language pair and combined it with Gujarati-English parallel data to form a multi source translation system. Each of the local posterior distribution P (yt |y1 ,2 , .., yt−1 , x) is modeled as a multinomial distribution over the target language voc"
W19-5316,P02-1040,0,0.105252,"chniques. For our baseline system, we learn separate vocabularies for Hindi and English each with 32k merge operations. For our multilingual model, we learn a joint vocabulary for Hindi and Gujarati & a separate vocabulary for English. With the help of BPE, the vocabulary size is reduced drastically and we no longer need to prune the vocabularies. After the translation, we do an extra post processing step to convert the target language subword units back to normal words. We found this approach to be very helpful in handling rare word representations. 4.4 5 In this section, we report the BLEU (Papineni et al., 2002) scores on the test sets provided in WMT19. Our simple NMT model which is an attention-based LSTM encoder-decoder model achieves a BLEU score of 6.2 on the test set. Our multilingual model which is trained with the help of Hindi-English parallel corpus attains a BLEU score of 9.8, showing a gain of +3.6 BLEU points on the same test set. Script Conversion India is a linguistically rich country having 22 constitutional languages, written in different scripts. Indian languages are highly inflectional with a rich morphology, default sentence structure as subject object verb (SOV) and relatively fr"
W19-5316,P16-1009,0,0.0489943,"Missing"
W19-5316,N16-1004,0,0.0304732,"onsists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in (Luong et al., 2015). The model directly estimates the posterior distribution Pθ (y|x) of translating a source sentence x = (x1 , .., xn ) to a target sentence y = (y1 , .., ym ) as: Pθ (y|x) = m Y Pθ (yt |y1 , y2 , .., yt−1 , x) (1) t=1 191 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 191–195 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics and attention mechanism for each target language. In Zoph and Knight (2016), multi-source translation was proposed where the model has different encoders and different attention mechanisms for different source languages. In Firat et al. (2016), the authors proposed a multi-way multilingual NMT model using a single shared attention mechanism but with multiple encoders/decoders for each source/target language. In this paper, we adopted the approach proposed in Johnson et al. (2017), where a single NMT model is used for multilingual machine translation. We used HindiEnglish as our assisting language pair and combined it with Gujarati-English parallel data to form a mult"
W19-5316,D16-1163,0,0.0305769,"ues but it still struggles if the parallel data is insufficient like in the case of Indian languages. The bulk of research on low resource NMT has focused on exploiting monolingual data or parallel data from other language pairs. Some recent methods to improve NMT models that exploit monolingual data ranges from back-translation (Sennrich et al., 2015a), dual NMT (He et al., 2016) to Unsupervised MT models (Lample et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Transfer Learning is also a promising approach for low resource NMT which exploits parallel data from other language pairs (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Typically it is achieved by training a parent model in a high resource language pair, then using some of the trained weights as the initialization for a child 2 Neural MT Architecture Our NMT model consists of an encoder and a decoder, each of which is a Recurrent Neural Network (RNN) as described in (Luong et al., 2015). The model directly estimates the posterior distribution Pθ (y|x) of translating a source sentence x = (x1 , .., xn ) to a target sentence y = (y1 , .., ym ) as: Pθ (y|x) = m Y Pθ (yt |y1 , y2 , .., yt−1 , x) (1) t=1 191 Proce"
W19-7724,W13-2322,0,0.0131473,"fsky (2002). The major problem with semantic roles is the difficulty involved in coming up with a standard set of roles and formal definitions of thematic roles. As a consequence, PropBank uses verb specific semantic roles as well as generalised semantic roles. Framenet uses semantic roles that are specific to a frame. There are also efforts to transform the syntactic dependency analysis to Logical Form (Reddy et al., 2016) for semantic parsing. There are also efforts to use Abstract Meaning Representation extending the existing relations in Propbank for the development of Semantic databanks (Banarescu et al., 2013). Given this background, now we highlight some of the salient features of a dependency tagset based on the Pāṇinian grammar framework. Bharati et al. (1991) proposed a computational grammar for processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sha"
W19-7724,C90-3005,0,0.831416,"ical Form (Reddy et al., 2016) for semantic parsing. There are also efforts to use Abstract Meaning Representation extending the existing relations in Propbank for the development of Semantic databanks (Banarescu et al., 2013). Given this background, now we highlight some of the salient features of a dependency tagset based on the Pāṇinian grammar framework. Bharati et al. (1991) proposed a computational grammar for processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sharma, 2017). Ramakrishnamacharyulu (2009) compiled a list of relations used in Indian Grammatical Tradition. A rule-based parser for Sanskrit has been developed using these dependency relations (Kulkarni, 2013; Kulkarni et al., 2010; Kulkarni and Ramakrishnamacharyulu, 2013; Kulkarni, 2019b). There are efforts to analyse English through the Pāṇinian framework. Bhatt (1993) and Bharati et al. (199"
W19-7724,P93-1015,0,0.901295,"Missing"
W19-7724,J95-3006,0,0.239462,"that kartṛ in the first sentence is an agent, in the second sentence an instrument and in the third it is the theme. Kartṛ, therefore, can be roughly translated as ‘doer’ which need not be animate. 4.3 What is the semantics associated with the kartṛ? Pāṇini defines kartṛ 10 as ‘the independent participant in the activity’. An activity typically involves more than one participants. The underlying verb expresses the complex activity which consists of subactivities of each of the participants involved. For example, in the case of opening of a lock, three subactivities are very clearly involved (Bharati et al., 1995) , viz. 1. the insertion of a key by an agent, 2. pressing of the levers of the lock by an instrument (key), and 3. moving of the latch and opening of the lock. Though in practice, to a large extent all the three subactivities 1 through 3 together constitute the activity ‘opening a lock’, sometimes the subactivities 2 and 3 together are also referred to as ‘opening a lock’, as noticed above in the second example, and the activity 3 alone is also referred to as ‘opening a lock’, as we see in the third sentence. Let us call them open1 , open2 and open3 , respectively. Pāṇini draws our attention"
W19-7724,W13-3705,1,0.84673,"fforts to use Abstract Meaning Representation extending the existing relations in Propbank for the development of Semantic databanks (Banarescu et al., 2013). Given this background, now we highlight some of the salient features of a dependency tagset based on the Pāṇinian grammar framework. Bharati et al. (1991) proposed a computational grammar for processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sharma, 2017). Ramakrishnamacharyulu (2009) compiled a list of relations used in Indian Grammatical Tradition. A rule-based parser for Sanskrit has been developed using these dependency relations (Kulkarni, 2013; Kulkarni et al., 2010; Kulkarni and Ramakrishnamacharyulu, 2013; Kulkarni, 2019b). There are efforts to analyse English through the Pāṇinian framework. Bhatt (1993) and Bharati et al. (1997) extend the notion of case suffixes (vibhakti pratyaya) to accou"
W19-7724,de-marneffe-etal-2006-generating,0,0.522849,"Missing"
W19-7724,W13-3721,0,0.0458909,"Missing"
W19-7724,de-marneffe-etal-2014-universal,0,0.0606402,"Missing"
W19-7724,J02-3001,0,0.358202,"tent treebanks, that can facilitate multilingual parser development (Nivre, 2015; Nivre et al., 2016). All these various lists of relations mentioned above are syntactic in nature. Several NLP tasks such as database query, robot instructions, information extraction, etc. need semantic representations of sentences. Two major efforts viz. Framenet (Fillmore and Baker, 2000) and Propbank(Kingsbury and Palmer, 2002; Kingsbury and Palmer, 2003) concentrated on the development of semantically tagged lexicon and corpus respectively. The first automatic semantic role labelling system was developed by Gildea and Jurafsky (2002). The major problem with semantic roles is the difficulty involved in coming up with a standard set of roles and formal definitions of thematic roles. As a consequence, PropBank uses verb specific semantic roles as well as generalised semantic roles. Framenet uses semantic roles that are specific to a frame. There are also efforts to transform the syntactic dependency analysis to Logical Form (Reddy et al., 2016) for semantic parsing. There are also efforts to use Abstract Meaning Representation extending the existing relations in Propbank for the development of Semantic databanks (Banarescu e"
W19-7724,W03-2401,0,0.285535,"uage Processing (NLP) community have recognised the importance of dependency parsing. For English, several parsers producing dependency style output were developed. In the initial stages, there was no consensus among the dependency parser developers on the number of dependency relations and their names. The link parser (Sleator and Temperley, 1993) used 106 relations, while Minipar (Lin, 1998) which was based on Chomsky’s minimalism and produced dependency parse used only 59 dependency relations. de Marneffe et al. (2006) modified the dependency relations proposed by Carroll et al. (1999) and King et al. (2003). These relations, known as Stanford Dependencies, were originally developed for English. They proposed a universal taxonomy with a total of 42 relations, which are supported across many languages. This set of relations then was adapted for several other languages. With the development of parsers for several languages, a need was felt to arrive at a single coherent standard, and this led to the development of universal dependencies that can be used for developing cross-linguistically consistent treebanks, that can facilitate multilingual parser development (Nivre, 2015; Nivre et al., 2016). Al"
W19-7724,kingsbury-palmer-2002-treebank,0,0.057565,"ent of parsers for several languages, a need was felt to arrive at a single coherent standard, and this led to the development of universal dependencies that can be used for developing cross-linguistically consistent treebanks, that can facilitate multilingual parser development (Nivre, 2015; Nivre et al., 2016). All these various lists of relations mentioned above are syntactic in nature. Several NLP tasks such as database query, robot instructions, information extraction, etc. need semantic representations of sentences. Two major efforts viz. Framenet (Fillmore and Baker, 2000) and Propbank(Kingsbury and Palmer, 2002; Kingsbury and Palmer, 2003) concentrated on the development of semantically tagged lexicon and corpus respectively. The first automatic semantic role labelling system was developed by Gildea and Jurafsky (2002). The major problem with semantic roles is the difficulty involved in coming up with a standard set of roles and formal definitions of thematic roles. As a consequence, PropBank uses verb specific semantic roles as well as generalised semantic roles. Framenet uses semantic roles that are specific to a frame. There are also efforts to transform the syntactic dependency analysis to Logic"
W19-7724,W13-3718,1,0.873699,"processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sharma, 2017). Ramakrishnamacharyulu (2009) compiled a list of relations used in Indian Grammatical Tradition. A rule-based parser for Sanskrit has been developed using these dependency relations (Kulkarni, 2013; Kulkarni et al., 2010; Kulkarni and Ramakrishnamacharyulu, 2013; Kulkarni, 2019b). There are efforts to analyse English through the Pāṇinian framework. Bhatt (1993) and Bharati et al. (1997) extend the notion of case suffixes (vibhakti pratyaya) to account for the notions of subject and object which have fixed positions in a sentence. Bharati and Kulkarni (2011) argues further that the concept of subject in English is the same as the concept of abhihita (expressed), and how by assigning a fixed position for Subject, and thereby doing away with the accusative marker English gains in the econo"
W19-7724,P05-1072,0,0.172634,"Missing"
W19-7724,I08-2099,1,0.817416,"ing. There are also efforts to use Abstract Meaning Representation extending the existing relations in Propbank for the development of Semantic databanks (Banarescu et al., 2013). Given this background, now we highlight some of the salient features of a dependency tagset based on the Pāṇinian grammar framework. Bharati et al. (1991) proposed a computational grammar for processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sharma, 2017). Ramakrishnamacharyulu (2009) compiled a list of relations used in Indian Grammatical Tradition. A rule-based parser for Sanskrit has been developed using these dependency relations (Kulkarni, 2013; Kulkarni et al., 2010; Kulkarni and Ramakrishnamacharyulu, 2013; Kulkarni, 2019b). There are efforts to analyse English through the Pāṇinian framework. Bhatt (1993) and Bharati et al. (1997) extend the notion of case suffixes (vibh"
W19-7724,Q16-1010,0,0.060845,"Missing"
W19-7724,1993.iwpt-1.22,0,0.803694,"mar for Sanskrit. The important feature of this list is that most of the relations represent well defined semantics that can be extracted from the surface string without any extra-linguistic information. 1 Introduction In the last two decades the researchers in the Natural Language Processing (NLP) community have recognised the importance of dependency parsing. For English, several parsers producing dependency style output were developed. In the initial stages, there was no consensus among the dependency parser developers on the number of dependency relations and their names. The link parser (Sleator and Temperley, 1993) used 106 relations, while Minipar (Lin, 1998) which was based on Chomsky’s minimalism and produced dependency parse used only 59 dependency relations. de Marneffe et al. (2006) modified the dependency relations proposed by Carroll et al. (1999) and King et al. (2003). These relations, known as Stanford Dependencies, were originally developed for English. They proposed a universal taxonomy with a total of 42 relations, which are supported across many languages. This set of relations then was adapted for several other languages. With the development of parsers for several languages, a need was"
W19-7724,W17-6529,1,0.845789,"et al., 2013). Given this background, now we highlight some of the salient features of a dependency tagset based on the Pāṇinian grammar framework. Bharati et al. (1991) proposed a computational grammar for processing Indian languages based on the Pāṇinian framework. A dependency tagset based on the Pāṇini’s grammar is being used for the development of treebanks for Indian languages (Bharati and Sangal, 1990; Bharati et al., 2002; Rafiya et al., 2008; Chaudhry et al., 2013; Chaudhry and Sharma, 2011). These tagsets are also used for the development of dependency parsers for Indian languages (Tandon and Sharma, 2017). Ramakrishnamacharyulu (2009) compiled a list of relations used in Indian Grammatical Tradition. A rule-based parser for Sanskrit has been developed using these dependency relations (Kulkarni, 2013; Kulkarni et al., 2010; Kulkarni and Ramakrishnamacharyulu, 2013; Kulkarni, 2019b). There are efforts to analyse English through the Pāṇinian framework. Bhatt (1993) and Bharati et al. (1997) extend the notion of case suffixes (vibhakti pratyaya) to account for the notions of subject and object which have fixed positions in a sentence. Bharati and Kulkarni (2011) argues further that the concept of"
W19-7810,D16-1250,0,0.034554,"ent, we train the CM stacking model with 1448 Hindi-English CM data (Gold-HE) as provided by Bhat et al. (2018) in addition to our 140 Gold-BE sentences. In order to fully capture the Hindi syntactic information in the CM data, we fortify the bilingual source model with the Hindi treebank resulting in a trilingual source model. We try to reduce the differences in data representations belonging to Hindi and Bengali by using: 1. Cross Lingual Word Embeddings for Hindi and Bengali by projecting the word2vec embeddings for the two languages into the same space by using the projection algorithm of Artetxe et al. (2016) and using a bilingual lexicon from ILCI parallel corpora. 2. WX notation8 to represent words from the two languages and using a common 32-dimensional character embedding space. 6 https://github.com/UniversalDependencies 7 Developed as a part of the Indian Languages Treebanking Project by Jadavpur University 8 http://wiki.apertium.org/wiki/WX_notation Embeddings Monolingual Crosslingual Crosslingual + WX notation POS 84.86 85.62 87.43 UAS 71.32 71.94 74.42 LAS 56.93 57.41 60.04 Table 3: Effect of embeddings on POS and Parser results for the Trilingual + Gold-(HE + BE) model Stacking Models (Bi"
W19-7810,W14-3902,0,0.0234587,"ndaries and within one lexical unit, the syntax of only one language is maintained. Since the more recent works have not focused on the differences between the two phenomena, we will use these two terms interchangeably. Recently, code-mixing which was often only observed in speech, has pervaded almost all forms of communication due to the growing popularity and usage of social media platforms by multilingual speakers (Rijhwani et al., 2017). Therefore, there has been considerable effort in building CM NLP systems such as language identification (Nguyen and Dogruoz, 2013; Solorio et al., 2014; Barman et al., 2014; Rijhwani et al., 2017), normalization and back-transliteration (Dutta et al., 2015). Part-ofspeech (POS) and chunk tagging for code-mixing data for various South Asian languages with English have been attempted with promising results (Sharma et al., 2016; Nelakuditi et al., 2016). Ammar et al. (2016) developed a single multilingual parser trained on multilingual set of treebanks that outperformed monolingually-trained parsers for several target languages. In the CoNLL 2018 shared task, several participating teams developed multilingual dependency parsers that integrated cross-lingual learnin"
W19-7810,I08-2099,1,0.684116,"encodes the input sentence into word and character embeddings and passes it to the shared bidirectional LSTM (Bi-LSTM). Bhat et al. (2018) demonstrates augmenting the final multilayer perceptron (MLP) layer of a bilingual model trained on Hindi and English treebanks (bilingual source model) into the MLP layer of the model trained on Hindi-English CM data (CM model) achieves state-of-the-art results for Hindi English code-mixing. 4 Experiments Our models are trained on English and Hindi UD-v2 treebanks.6 Due to the absence of a Bengali UD treebank, we converted the Paninian annotation scheme (Begum et al., 2008) present in the Bengali treebank7 to UD by slightly modifying the rules (Tandon et al., 2016) for Hindi. The characters are represented by 32-dimensional character embeddings while the words in each language are represented by 64 dimensional word2vec vectors (Mikolov et al., 2013) learned using the skip-gram model. The hidden dimensions and learning hyperparameters are consistent with those in Bhat et al. (2018). For our baseline model, we train the neural stacking model (Bhat et al., 2018) for Bengali-English by training the source model on both Bengali and English treebanks and stacking it o"
W19-7810,N18-1090,1,0.701512,"ali-English code-mixing is found in abundance as Bengali is widely spoken in India and Bangladesh. It is the second most widely spoken language in India after Hindi (Bhatia, 1982). Because of inherent structural and semantic similarity between Bengali and Hindi, we observe a close proximity between Bengali-English and Hindi-English code-mixing as well. Both of these language pairs deal with the challenges of mixing different typologically diverse languages; SOV word order1 for Hindi/Bengali and SVO word order for English. A dependency parser for Hindi-English code-mixing has been presented by Bhat et al. (2018). In comparison, Bengali-English code-mixing is left relatively unexplored barring significant works on language identification (Das and Gambäck, 2014) and POS tagging (Jamatia et al., 2015) which serve as preliminary tasks for more advanced parsing applications down the pipeline. The main hindrance to the development of parsing technologies for Bengali-English stem from the lack of annotated resources for the code-mixing of this language pair. In this paper, we try to utilize the preexisting resources for widely available monolingual Bengali, Hindi and English as well as Hindi-English code-mi"
W19-7810,D16-1070,0,0.0281554,"both HE (in blue) and B (in red). Since the sentences in BE, HE and B are essentially parallel, we get one-to-one mapping and do not need to introduce any dummy nodes. We select 3643 completely annotated trees for our Syn-BE. 3 Dependency Parsing We adapt the neural dependency parser by Bhat et al. (2018) which is based on a transition-based parser (Kiperwasser and Goldberg, 2016) and enhanced by neural stacks to incorporate monolingual syntactic knowledge with the CM model. The model jointly learns POS-tagging as well as parsing by adapting feature level neural stacks (Zhang and Weiss, 2016; Chen et al., 2016). The input layer for both the 5 https://github.com/irshadbhat/csnlp root root nmod amod amod case aparishkara haath obj advcl nmod case era byabohar ediye chalun dirty hands ADJ NOUN ADP NOUN VERB VERB [Bengali] of use avoid dirty hands go ke of obl case use se bache from save ADJ NOUN ADP NOUN ADP VERB [Hindi-English] root amod nmod case dirty hands era of obj use advcl ediye chalun avoid go ADJ NOUN ADP NOUN VERB VERB [Bengali-English] Figure 3: An example of annotation projection from Bengali and Hindi-English to Bengali-English tagger and the parser encodes the input sentence into word an"
W19-7810,W14-5152,0,0.0154613,"India after Hindi (Bhatia, 1982). Because of inherent structural and semantic similarity between Bengali and Hindi, we observe a close proximity between Bengali-English and Hindi-English code-mixing as well. Both of these language pairs deal with the challenges of mixing different typologically diverse languages; SOV word order1 for Hindi/Bengali and SVO word order for English. A dependency parser for Hindi-English code-mixing has been presented by Bhat et al. (2018). In comparison, Bengali-English code-mixing is left relatively unexplored barring significant works on language identification (Das and Gambäck, 2014) and POS tagging (Jamatia et al., 2015) which serve as preliminary tasks for more advanced parsing applications down the pipeline. The main hindrance to the development of parsing technologies for Bengali-English stem from the lack of annotated resources for the code-mixing of this language pair. In this paper, we try to utilize the preexisting resources for widely available monolingual Bengali, Hindi and English as well as Hindi-English code-mixing and adapt them for Bengali-English dependency parsing. We also propose a rule based system to synthetically generate Bengali-English code-mixing d"
W19-7810,dey-fung-2014-hindi,0,0.0760113,"Missing"
W19-7810,P02-1050,0,0.136644,"s sentences with word repetitions due to inconsistent chunking of multi-word expressions. We try to mitigate those errors in the post-processing step by carefully removing repeated words at code-mixing points. We attain this by calculating cosine similarity between the words represented by their cross lingual embeddings (see section 4). Eg: chiniyukta (“sugared”) sugared gums → sugared gum 2.3 Synthetic Bengali-English Treebank Cross-lingual annotation projection makes use of parallel data to project annotations from the source language to the target language through automatic word alignment. Hwa et al. (2002) proposed some basic projection heuristics to deal with different kinds of word alignments. Tiedemann (2014) proposed improvements in the annotation scheme by adding heuristics to remove unnecessary dummy nodes that are introduced in the target treebank to deal with problematic word alignments. We investigate the utility of annotation projection from the Hindi-English CM treebank (HE) and the Bengali monolingual treebank (B) to Bengali-English (BE). HE is created by parsing the Hindi-English CM data generated in the section 2.2 using the neural stacking dependency parser for Hindi-English by B"
W19-7810,R15-1033,0,0.0188579,"e of inherent structural and semantic similarity between Bengali and Hindi, we observe a close proximity between Bengali-English and Hindi-English code-mixing as well. Both of these language pairs deal with the challenges of mixing different typologically diverse languages; SOV word order1 for Hindi/Bengali and SVO word order for English. A dependency parser for Hindi-English code-mixing has been presented by Bhat et al. (2018). In comparison, Bengali-English code-mixing is left relatively unexplored barring significant works on language identification (Das and Gambäck, 2014) and POS tagging (Jamatia et al., 2015) which serve as preliminary tasks for more advanced parsing applications down the pipeline. The main hindrance to the development of parsing technologies for Bengali-English stem from the lack of annotated resources for the code-mixing of this language pair. In this paper, we try to utilize the preexisting resources for widely available monolingual Bengali, Hindi and English as well as Hindi-English code-mixing and adapt them for Bengali-English dependency parsing. We also propose a rule based system to synthetically generate Bengali-English code-mixing data. An attempt has been made to genera"
W19-7810,jha-2010-tdil,0,0.0501813,"Missing"
W19-7810,C82-1023,0,0.688787,"64.11 Table 2: POS and parsing results of neuralstacking model for different languages partially or in entirety (Dey and Fung, 2014). Sinha and Thakur (2005) had previously discussed CM constraints for Hindi-English and came to the conclusion that the phenomenon of code-mixing for this language pair is not entirely arbitrary. In our code-mixing method, we will be closely following the Closed Class Constraint which states that the matrix language elements within the closed class of grammar (possessives, ordinals, determiners, pronouns) are not allowed in code-mixing (Sridhar and Sridhar, 1980; Joshi, 1982). (1) Bengali: (Apnar “your” PRON) (chokher “of eyes” NOUN) (dekhashonar “care” VERB) (jonye “for” ADP) (aapni “you” PRON) (kotota “how much” DET) (icchuk “willing” ADJ) ? (2) English: (How ADV) (aware ADJ) (are VERB) (you PRON) (about ADP) (the DET) (care NOUN) (of ADP) (your PRON) (eyes NOUN) ? (3) Incorrect CS: (*Your PRON) (chokher “of eyes” NOUN) (*about ADP) (*the DET) (dekhashona “care” NOUN) (*you PRON) (*how ADV) (icchuk “willing” ADJ)? (4) Correct CS: (Apnar “your” PRON) (eyes NOUN) (er “of” ADP) (care NOUN) (er “of” ADP) ( jonno “for” ADP) (aapni “you” PRON) (kotota “how much” DET)"
W19-7810,Q16-1023,0,0.0330504,"ind the aligned Hindi node in HE. If the cosine similarity between the two is above a certain threshold (0.5), project annotations from HE to BE. In Figure 3, we demonstrate this with an example where the annotation for the BE tree is generated by both HE (in blue) and B (in red). Since the sentences in BE, HE and B are essentially parallel, we get one-to-one mapping and do not need to introduce any dummy nodes. We select 3643 completely annotated trees for our Syn-BE. 3 Dependency Parsing We adapt the neural dependency parser by Bhat et al. (2018) which is based on a transition-based parser (Kiperwasser and Goldberg, 2016) and enhanced by neural stacks to incorporate monolingual syntactic knowledge with the CM model. The model jointly learns POS-tagging as well as parsing by adapting feature level neural stacks (Zhang and Weiss, 2016; Chen et al., 2016). The input layer for both the 5 https://github.com/irshadbhat/csnlp root root nmod amod amod case aparishkara haath obj advcl nmod case era byabohar ediye chalun dirty hands ADJ NOUN ADP NOUN VERB VERB [Bengali] of use avoid dirty hands go ke of obl case use se bache from save ADJ NOUN ADP NOUN ADP VERB [Hindi-English] root amod nmod case dirty hands era of obj"
W19-7810,P03-1054,0,0.0366393,"for adpositions (prepositions and postpositions). We note that the example (4) results in an acceptable code-mixed sentence as the closed class elements from the matrix language Bengali are retained. The Code-Mixing Process The pipeline for our code-mixing script is as shown in Figure 2. The script takes shallow-parsed English and Bengali parallel corpora as inputs. Consistency across chunks in parallel sentences is imperative for direct replacement of chunks for code-mixing. However, there are various structural differences in constituency parsing obtained for English by the Stanford Parser (Klein and Manning, 2003) and shallow parsing obtained for Bengali by the Shallow Parser by TDIL Program, Department Of IT Govt. Of India.4 . The first module, chunk harmonizer handles the issue of structural differences in English and 4 http://ltrc.iiit.ac.in/analyzer/bengali/ Code-Mixing English Parallel Corpus Stanford Constituency Parser Chunk Harmonizer Bengali Parallel Corpus Shallow Parser Rule-based Chunk Replacement Synthetic BengaliEnglish CM Corpus Figure 2: Schematic diagram of the Code-Mixing process Bengali chunks by modifying the English chunks based on the following set of rules: 1. Separate the coordi"
W19-7810,J93-2004,0,0.0849337,"Missing"
W19-7810,W16-5811,0,0.0312566,"eech, has pervaded almost all forms of communication due to the growing popularity and usage of social media platforms by multilingual speakers (Rijhwani et al., 2017). Therefore, there has been considerable effort in building CM NLP systems such as language identification (Nguyen and Dogruoz, 2013; Solorio et al., 2014; Barman et al., 2014; Rijhwani et al., 2017), normalization and back-transliteration (Dutta et al., 2015). Part-ofspeech (POS) and chunk tagging for code-mixing data for various South Asian languages with English have been attempted with promising results (Sharma et al., 2016; Nelakuditi et al., 2016). Ammar et al. (2016) developed a single multilingual parser trained on multilingual set of treebanks that outperformed monolingually-trained parsers for several target languages. In the CoNLL 2018 shared task, several participating teams developed multilingual dependency parsers that integrated cross-lingual learning for resource-poor languages and were evaluated on monolingual treebanks belonging to 82 unique languages (Zeman et al., 2018). However, none of these multilingual parsers have been evaluated on code-mixed data or adapted specifically for CM parsing. The Bengali-English code-mixin"
W19-7810,D13-1084,0,0.0151793,"sentential, however there are strict phrasal boundaries and within one lexical unit, the syntax of only one language is maintained. Since the more recent works have not focused on the differences between the two phenomena, we will use these two terms interchangeably. Recently, code-mixing which was often only observed in speech, has pervaded almost all forms of communication due to the growing popularity and usage of social media platforms by multilingual speakers (Rijhwani et al., 2017). Therefore, there has been considerable effort in building CM NLP systems such as language identification (Nguyen and Dogruoz, 2013; Solorio et al., 2014; Barman et al., 2014; Rijhwani et al., 2017), normalization and back-transliteration (Dutta et al., 2015). Part-ofspeech (POS) and chunk tagging for code-mixing data for various South Asian languages with English have been attempted with promising results (Sharma et al., 2016; Nelakuditi et al., 2016). Ammar et al. (2016) developed a single multilingual parser trained on multilingual set of treebanks that outperformed monolingually-trained parsers for several target languages. In the CoNLL 2018 shared task, several participating teams developed multilingual dependency pa"
W19-7810,J03-1002,0,0.0131214,"Missing"
W19-7810,P18-1143,0,0.0184733,"applications down the pipeline. The main hindrance to the development of parsing technologies for Bengali-English stem from the lack of annotated resources for the code-mixing of this language pair. In this paper, we try to utilize the preexisting resources for widely available monolingual Bengali, Hindi and English as well as Hindi-English code-mixing and adapt them for Bengali-English dependency parsing. We also propose a rule based system to synthetically generate Bengali-English code-mixing data. An attempt has been made to generate code-mixing data for the Spanish-English language pair (Pratapa et al., 2018) but none for the HindiEnglish or Bengali-English language pair as these pairs pose special challenges due to their different word orders which commonly violate most code-mixing theories (Sinha and Thakur, 2005). We further present a method to project dependency annotations to our Bengali-English CM data from monolingual Bengali and Hindi-English CM treebank and generate a synthetic treebank for Bengali-English (Syn-BE) which helps improve the accuracy of our dependency parser. For evaluation purpose, we present a dataset of 500 Bengali-English tweets annotated under Universal Dependencies sch"
W19-7810,P17-1180,0,0.0189344,"ech extracts belonging to two different grammatical systems (Gumperz, 1982). The occurrence can be both inter-sentential or intra-sentential, however there are strict phrasal boundaries and within one lexical unit, the syntax of only one language is maintained. Since the more recent works have not focused on the differences between the two phenomena, we will use these two terms interchangeably. Recently, code-mixing which was often only observed in speech, has pervaded almost all forms of communication due to the growing popularity and usage of social media platforms by multilingual speakers (Rijhwani et al., 2017). Therefore, there has been considerable effort in building CM NLP systems such as language identification (Nguyen and Dogruoz, 2013; Solorio et al., 2014; Barman et al., 2014; Rijhwani et al., 2017), normalization and back-transliteration (Dutta et al., 2015). Part-ofspeech (POS) and chunk tagging for code-mixing data for various South Asian languages with English have been attempted with promising results (Sharma et al., 2016; Nelakuditi et al., 2016). Ammar et al. (2016) developed a single multilingual parser trained on multilingual set of treebanks that outperformed monolingually-trained p"
W19-7810,N16-1159,1,0.937134,"n only observed in speech, has pervaded almost all forms of communication due to the growing popularity and usage of social media platforms by multilingual speakers (Rijhwani et al., 2017). Therefore, there has been considerable effort in building CM NLP systems such as language identification (Nguyen and Dogruoz, 2013; Solorio et al., 2014; Barman et al., 2014; Rijhwani et al., 2017), normalization and back-transliteration (Dutta et al., 2015). Part-ofspeech (POS) and chunk tagging for code-mixing data for various South Asian languages with English have been attempted with promising results (Sharma et al., 2016; Nelakuditi et al., 2016). Ammar et al. (2016) developed a single multilingual parser trained on multilingual set of treebanks that outperformed monolingually-trained parsers for several target languages. In the CoNLL 2018 shared task, several participating teams developed multilingual dependency parsers that integrated cross-lingual learning for resource-poor languages and were evaluated on monolingual treebanks belonging to 82 unique languages (Zeman et al., 2018). However, none of these multilingual parsers have been evaluated on code-mixed data or adapted specifically for CM parsing. The"
W19-7810,W16-1716,1,0.858298,"idirectional LSTM (Bi-LSTM). Bhat et al. (2018) demonstrates augmenting the final multilayer perceptron (MLP) layer of a bilingual model trained on Hindi and English treebanks (bilingual source model) into the MLP layer of the model trained on Hindi-English CM data (CM model) achieves state-of-the-art results for Hindi English code-mixing. 4 Experiments Our models are trained on English and Hindi UD-v2 treebanks.6 Due to the absence of a Bengali UD treebank, we converted the Paninian annotation scheme (Begum et al., 2008) present in the Bengali treebank7 to UD by slightly modifying the rules (Tandon et al., 2016) for Hindi. The characters are represented by 32-dimensional character embeddings while the words in each language are represented by 64 dimensional word2vec vectors (Mikolov et al., 2013) learned using the skip-gram model. The hidden dimensions and learning hyperparameters are consistent with those in Bhat et al. (2018). For our baseline model, we train the neural stacking model (Bhat et al., 2018) for Bengali-English by training the source model on both Bengali and English treebanks and stacking it on a CM model trained on 140 Bengali-English CM (Gold-BE) sentences in our training set. Even"
W19-7810,C14-1175,0,0.0235903,"those errors in the post-processing step by carefully removing repeated words at code-mixing points. We attain this by calculating cosine similarity between the words represented by their cross lingual embeddings (see section 4). Eg: chiniyukta (“sugared”) sugared gums → sugared gum 2.3 Synthetic Bengali-English Treebank Cross-lingual annotation projection makes use of parallel data to project annotations from the source language to the target language through automatic word alignment. Hwa et al. (2002) proposed some basic projection heuristics to deal with different kinds of word alignments. Tiedemann (2014) proposed improvements in the annotation scheme by adding heuristics to remove unnecessary dummy nodes that are introduced in the target treebank to deal with problematic word alignments. We investigate the utility of annotation projection from the Hindi-English CM treebank (HE) and the Bengali monolingual treebank (B) to Bengali-English (BE). HE is created by parsing the Hindi-English CM data generated in the section 2.2 using the neural stacking dependency parser for Hindi-English by Bhat et al. (2018).5 BE is generated by parsing the parallel Bengali sentences using the same neural stacking"
W19-7810,K18-2001,0,0.0338415,"Missing"
W19-7810,P16-1147,0,0.0261457,"E tree is generated by both HE (in blue) and B (in red). Since the sentences in BE, HE and B are essentially parallel, we get one-to-one mapping and do not need to introduce any dummy nodes. We select 3643 completely annotated trees for our Syn-BE. 3 Dependency Parsing We adapt the neural dependency parser by Bhat et al. (2018) which is based on a transition-based parser (Kiperwasser and Goldberg, 2016) and enhanced by neural stacks to incorporate monolingual syntactic knowledge with the CM model. The model jointly learns POS-tagging as well as parsing by adapting feature level neural stacks (Zhang and Weiss, 2016; Chen et al., 2016). The input layer for both the 5 https://github.com/irshadbhat/csnlp root root nmod amod amod case aparishkara haath obj advcl nmod case era byabohar ediye chalun dirty hands ADJ NOUN ADP NOUN VERB VERB [Bengali] of use avoid dirty hands go ke of obl case use se bache from save ADJ NOUN ADP NOUN ADP VERB [Hindi-English] root amod nmod case dirty hands era of obj use advcl ediye chalun avoid go ADJ NOUN ADP NOUN VERB VERB [Bengali-English] Figure 3: An example of annotation projection from Bengali and Hindi-English to Bengali-English tagger and the parser encodes the input s"
Y09-2020,I08-2099,1,0.904025,"n-noun genitive relation, infinitive-verb relation, infinitive-noun relation, adjective-noun, adverb-verb relations, etc. In the 2nd stage it then tries to handle more complex relations such as conjuncts, relative clause, etc. What this essentially means is a 2-stage resolution of dependencies, where the parser selectively resolves the dependencies of various lexical heads at their appropriate stage, for 2 All the relations marked by the parser are syntactico-semantic labels. For a detailed analysis see Bharati et al. (1995). Many relations shown in the diagrams of this paper are described in Begum et al. (2008a). For the complete tagset description, see http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-guidelines/DS-guidelines-ver2-28-05-09.pdf 3 A clause is a group of word such that the group contains a single finite verb chunk. 616 example verbs in the 1st stage and conjuncts and inter-verb relations in the 2nd stage. The key ideas are: (1) There are two layers (stages), (2) the 1st stage handles intra-clausal relations, and the 2nd stage handles inter-clausal relations, (3) the output of each layer is a linguistically valid partial parse that becomes, if necessary, the input to the next layer,"
Y09-2020,P93-1015,1,0.823579,"Missing"
Y09-2020,J95-3006,0,0.90757,"guage parsing, Constraint based parsing. 1 Introduction Due to the availability of annotated corpora for various languages since the past decade, data driven parsing has proved to be immensely successful. Unlike English, however, most of the parsers for morphologically rich free word order (MoR-FWO) languages (such as Czech, Turkish, Hindi, etc.) have adopted the dependency grammatical framework. It is well known that for MoR-FWO languages, dependency framework provides ease of linguistic analysis and is much better suited to account for their various structures (Shieber, 1975; Mel'Cuk, 1988; Bharati et al., 1995). The state of the art parsing accuracy for many MoR-FWO languages is still low compared to that of English. Parsing experiments (Nivre et al., 2007; Hall et al., 2007) for these languages have pointed towards various reasons for this low performance. For Hindi1, (a) difficulty in extracting relevant linguistic cues, (b) non-projectivity, (c) lack of explicit cues, (d) long distance dependencies, (e) complex linguistic phenomena, and (f) less corpus size, have been suggested (Bharati et al., 2008) for low performance. The approach proposed in this paper shows how one can minimize these adverse"
Y09-2020,D07-1122,0,0.0288596,"mented by a controlled statistical strategy to achieve high performance and robustness. The overall task of dependency parsing is attacked using modularity, wherein specific tasks are broken down into smaller linguistically motivated sub-tasks. Figure 1 above shows the output of each of these sub-tasks. 2.1 Background Data driven parsing is usually a single stage process wherein a sentence is parsed at one go. Many attempts have, however, tried to divide the overall task into sub-task. One trend has been to first identify dependencies and then add edge labels over them (McDonald et al., 2005, Chen et al., 2007). The other trend has been towards performing smaller linguistically relevant tasks as a precursor to complete parsing (Abney, 1997; Bharati et al., 1995; Attardi and Dell’Orletta, 2008; Peh and Ting, 1996). In our approach we divide the task of parsing into the following sub-tasks (layers): 1. POS tagging, chunking (POSCH), 2. Constraint based hybrid parsing (CBHP), 3. Intra-chunk dependencies (IRCH) identification. (a) POSCH is treated as pre-processing to the task of parsing. A bag represents a set of adjacent words which are in dependency relations with each other, and are connected to the"
Y09-2020,W04-1510,0,0.0483247,"ies, (e) complex linguistic phenomena, and (f) less corpus size, have been suggested (Bharati et al., 2008) for low performance. The approach proposed in this paper shows how one can minimize these adverse effects and argues that a hybrid approach can prove to be a better option to parsing such languages. There have been, in the past, many attempts to parsing using constraint based approaches. Some of the constraint based parsers known in the literature are Karlsson et al. (1995), Maruyama (1990), Bharati et al. (1993, 2002), Tapanainen and Järvinen (1998), Schröder (2002), and more recently, Debusmann et al. (2004). Some attempts at parsing Hindi using data driven approach have been (Bharati et al., 2008b; Husain et al., 2009). Later in Section 4, we’ll compare the results of data-driven Hindi parsing with that of our approach. The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of Copyright 2009 by Akshar Bharati, Samar Husain, Meher Vijay, Kalyan Deepak, Dipti Misra Sharma, an"
Y09-2020,D07-1097,0,0.0360415,"roved to be immensely successful. Unlike English, however, most of the parsers for morphologically rich free word order (MoR-FWO) languages (such as Czech, Turkish, Hindi, etc.) have adopted the dependency grammatical framework. It is well known that for MoR-FWO languages, dependency framework provides ease of linguistic analysis and is much better suited to account for their various structures (Shieber, 1975; Mel'Cuk, 1988; Bharati et al., 1995). The state of the art parsing accuracy for many MoR-FWO languages is still low compared to that of English. Parsing experiments (Nivre et al., 2007; Hall et al., 2007) for these languages have pointed towards various reasons for this low performance. For Hindi1, (a) difficulty in extracting relevant linguistic cues, (b) non-projectivity, (c) lack of explicit cues, (d) long distance dependencies, (e) complex linguistic phenomena, and (f) less corpus size, have been suggested (Bharati et al., 2008) for low performance. The approach proposed in this paper shows how one can minimize these adverse effects and argues that a hybrid approach can prove to be a better option to parsing such languages. There have been, in the past, many attempts to parsing using const"
Y09-2020,P90-1005,0,0.49743,"extracting relevant linguistic cues, (b) non-projectivity, (c) lack of explicit cues, (d) long distance dependencies, (e) complex linguistic phenomena, and (f) less corpus size, have been suggested (Bharati et al., 2008) for low performance. The approach proposed in this paper shows how one can minimize these adverse effects and argues that a hybrid approach can prove to be a better option to parsing such languages. There have been, in the past, many attempts to parsing using constraint based approaches. Some of the constraint based parsers known in the literature are Karlsson et al. (1995), Maruyama (1990), Bharati et al. (1993, 2002), Tapanainen and Järvinen (1998), Schröder (2002), and more recently, Debusmann et al. (2004). Some attempts at parsing Hindi using data driven approach have been (Bharati et al., 2008b; Husain et al., 2009). Later in Section 4, we’ll compare the results of data-driven Hindi parsing with that of our approach. The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identific"
Y09-2020,H05-1066,0,0.358226,"iven approach is complemented by a controlled statistical strategy to achieve high performance and robustness. The overall task of dependency parsing is attacked using modularity, wherein specific tasks are broken down into smaller linguistically motivated sub-tasks. Figure 1 above shows the output of each of these sub-tasks. 2.1 Background Data driven parsing is usually a single stage process wherein a sentence is parsed at one go. Many attempts have, however, tried to divide the overall task into sub-task. One trend has been to first identify dependencies and then add edge labels over them (McDonald et al., 2005, Chen et al., 2007). The other trend has been towards performing smaller linguistically relevant tasks as a precursor to complete parsing (Abney, 1997; Bharati et al., 1995; Attardi and Dell’Orletta, 2008; Peh and Ting, 1996). In our approach we divide the task of parsing into the following sub-tasks (layers): 1. POS tagging, chunking (POSCH), 2. Constraint based hybrid parsing (CBHP), 3. Intra-chunk dependencies (IRCH) identification. (a) POSCH is treated as pre-processing to the task of parsing. A bag represents a set of adjacent words which are in dependency relations with each other, and"
Y09-2020,W06-1616,0,0.0229355,"ce will inevitably increase with the improvement of the prioritizer. There are various reasons why we think that the proposed approach is better suited to parsing MoR-FWO: (1) Complex linguistic cues can easily be encoded as part of various constraints. For example, it has been shown by Bharati et al. (2008b) that, for Hindi, complex agreement patterns, though present in the data, are not being learnt by data driven parsers. Such patterns along with other idiosyncratic language properties can be easily incorporated as constraints, (2) IP formulation allows for handling non-projective parsing (Riedel and Clarke, 2006), (3) Use of H-constraints and S-constraints together reflect the grammar of a language. The rules in the form of H-constraints are complemented by the weights of S-constraints learnt from the annotated corpus, (4) Two-stage parsing lends itself seamlessly to parsing complex sentences by modularizing the task of overall parsing, (5) The problem of label bias faced by the data driven Hindi parsers (Bharati et al., 2008b) for some cases does not arise here as contextually similar entities are disambiguated by tapping in hard to learn features, (6) Use of clauses as basic parsing units reduces th"
Y09-2020,A97-1011,0,0.200668,"Missing"
Y12-1042,W09-3036,1,0.839554,"s, null pronouns, gap, ellipsis because identification of referent in these cases is relatively difficult. Reference relations can also be classified on the basis of directionality i.e. anaphora as backward reference and cataphora as forward reference. In current annotation, while anaphoric references are annotated within and across sentences, only those cataphoric pronouns are annotated which have referent in the same sentence. 3 Hindi Dependency TreeBank The ‘Hindi/Urdu Dependency Treebank’ is being developed as a part of the Multi-Representational and Multi-Layered Treebank for Hindi/Urdu (Bhatt et al., 2009). It is a rich corpus with various linguistic information like POS-tag, dependency relation,morphological features in the Treebank. In order to further enrich the corpus with anaphoric reference information, we intend to annotate anaphora relations as a layer on top of the dependency layer. In the representation format of the Treebank(SSF)(Bharati et al., 2007), the information on the node is of attribute-value type, where the features are represented as values of some pre-defined attributes (e.g. name, morph, dependency relation etc.). Since Dependency relations are inherently modifier-modifi"
Y12-1042,W04-0209,0,0.0231287,"1997) and other MUC based schemes, which are used for co-reference annotation via markup tags. The MATE/GNOME project has another important scheme suitable for different types of dialogue annotations (Poesio and Artstein, 2008). Kucova and Hajicova (2005) is also a notable work toCopyright 2012 by Praveen Dakwale, Himanshu Sharma, and Dipti M Sharma 26th Pacific Asia Conference on Language,Information and Computation pages 391–400 wards annotating co-reference relations in a dependency TreeBank (Czech, PragueDT). Some other proposed schemes are, in Spanish and Catalan (Recasens et al., 2007; Navarro et al., 2004) and in Basque (Aduriz et al., 2004) for 3LB corpus. A known attempt for Hindi is, for demonstrative pronouns in EMILLE corpus (Sinha, 2002).The above mentioned schemes are used for anaphora annotation in English and various other languages. The motivation behind proposing a new scheme is that some of the challenges like annotation of distributed referent span, annotation of multiple constituents, and identification of head and modifiers are difficult to handle in above mentioned schemes. Such challenges, though faced in various languages, are more frequent in Hindi. In this paper these issues"
Y12-1042,passonneau-2004-computing,0,0.133729,"distinction of Concrete, Abstract and Other types of reference; we separate out the concrete references in the above used data for the comparative study of the proposed scheme with MUC. We consider agreement over those pronouns in Experiment 2, for which all the annotators have a perfect agreement in concrete category. 5.2.2 Experiment 2 In the second experiment the inter-annotator analysis is conducted for the concrete pronouns separated in Experiment 1. Krippendorff’s alpha (Krippendorff, 2004) was then used as a statistical measure to obtain the inter-annotator agreement. As suggested in (Passonneau, 2004) and (Poesio and Artstein, 2005) Krippendorff’s alpha is a better metrics for calculating agreement for co-reference/anaphora annotation as compared to other metrics because it considers degrees of disagreement and in anaphora it is difficult to define discrete categories. Similar to (Passonneau, 2004) we consider co-reference chain as discrete categories. Experiment (2) also invloved the same data and the same raters who carries out annotation in experiment (1). Krippendorff’s alpha is defined as follows : 399 Do De X (2) α=1− Do = X 1 i∈I i ∗ c(c − 1) k ∈K X k 0 ∈K 0 nik nik0 dkk0 (3) X X 1"
Y12-1042,poesio-artstein-2008-anaphoric,0,0.0237843,"w the structure of the Dependency Tree-Bank, it is convertible to other formats of annotation as well. In recent years, due to increasing interest in development of statistical systems for anaphora resolution, there have been significant attempts for creation of anaphora annotated corpora and annotation schemes. The most well known among these are MUC-7 annotation scheme (Hirschman and Chinchor, 1997) and other MUC based schemes, which are used for co-reference annotation via markup tags. The MATE/GNOME project has another important scheme suitable for different types of dialogue annotations (Poesio and Artstein, 2008). Kucova and Hajicova (2005) is also a notable work toCopyright 2012 by Praveen Dakwale, Himanshu Sharma, and Dipti M Sharma 26th Pacific Asia Conference on Language,Information and Computation pages 391–400 wards annotating co-reference relations in a dependency TreeBank (Czech, PragueDT). Some other proposed schemes are, in Spanish and Catalan (Recasens et al., 2007; Navarro et al., 2004) and in Basque (Aduriz et al., 2004) for 3LB corpus. A known attempt for Hindi is, for demonstrative pronouns in EMILLE corpus (Sinha, 2002).The above mentioned schemes are used for anaphora annotation in En"
Y12-1042,J00-4005,0,0.0310341,"Missing"
Y18-1053,D14-1058,0,0.208151,"MT & NLP Lab, KCIS LTRC, IIIT-Hyderabad Vasudeva Varma IREL Lab, KCIS LTRC, IIIT-Hyderabad knowledge, co-reference resolution and a large vocabulary. Simple keyword or pattern matching is less equipped to take up such a challenge (Bakman, 2007). There are 112 short trees and 119 tall trees currently in the park . Park workers will plant 105 short trees today . How many short trees will the park have when the workers are finished ? Answer : 112 + 105 Table 1: Mathematical Word Problem Example Previous systems either rely heavily on specific set of problem abstractions based on verb categories (Hosseini et al., 2014) or learning equations from pre-defined set of templates (Kushman et al., 2014). Deep neural solvers (Wang et al., 2017) proposed a combination of sequence-to-sequence model and information retrieval system. However, an ideal equation generation system for a word problem should be able to identify components of the equation and form the equation in an orderly fashion independent of syntax or vocabulary of the sentences. In this work, we introduce EquGener - an equation generator using a memory network with an equation decoder. Intuitively, a human math solver collects relevant details from the"
Y18-1053,P16-1084,0,0.0122247,"r. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the help of some hand-written rules and modified the softmax function on the decoder side. There have been a concerted effort to create large scale and diversified datasets. Huang et al. (2016) analyzed the existing datasets, and created a large-scale dataset from community question answering web pages. Most of the neural network frameworks suffer from lack of performance in presence of a limited training dataset. (Weston et al., 2014) introduced long term memory components in neural networks for better reasoning called “memory networks”. The memories can be retrieved and written multiple times and can be used for prediction in multiple tasks. (Sukhbaatar et al., 2015) came up with an end-to-end memory network which required less supervision in giving answers to a question asked fro"
Y18-1053,D17-1084,0,0.0129782,"rb categorization, CFG rules can be used to solve word problems. Liang et al. (2018) suggested that there exists mainly two kinds of approaches for solving MWPs - one involving understanding and the other without understanding. Kushman et al. (2014) system was a joint log linear distribution over the full set of equations and alignments between the variables and text. The number of equations was dependent on the number of training equation templates. The number slots were filled by the numbers present in the text while the unknown or variable slot were filled by the nouns in the problem text. Huang et al. (2017) also extracted relevant templates and did fine grained inferencing to solve word problems. Illinois Math Solver (Roy and Roth, 2016a) used two modules to solve any arithmetic word problem. The first module was a CFG based Semantic Parser and other module solved the problem by decomposing it into a series of classification problems (Roy and Roth, 2016b) with formation of an expression tree through constrained inference. Hosseini et al. (2014) ‘s system used verb categorization for identifying relevant variables, their values and mapping them into a set of linear equations which can be easily s"
Y18-1053,P18-1039,0,0.0112001,"ed an arithmetic word problem solver that learned how to use formulas to solve simple addition and subtraction problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techniques. Huang et al. (2018) used intermediate meaning representation to generate equations. There has been attempts (Ling et al., 2017) to generate answer rationales for arriving at the final answer. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the"
Y18-1053,Q15-1042,0,0.0420806,"Missing"
Y18-1053,N16-1136,0,0.013987,"anguage, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 from the encoder. The decoder also uses an LSTM to predict the next output. The decoder predicts the output distribution using teacher forcing (Lamb et al., 2016). The hidden and cell states are computed according to the LSTM equations defined in section 2.2. In Figure 1, the output tokens are referred to as Op1, Op2 and Opr which stand for the operands and the operator of the equation. 3 Experimental Setup 3.1 Data We used 1314 arithmetic problems with a single operation present in MAWPS Koncel-Kedziorski et al. (2016) as our training set. The operations include all basic mathematical operation addition (+), subtraction (-), multiplication (*) and division (/). The two benchmark datasets for evaluation are MA1 and IXL dataset Mitra and Baral (2016) which are subsets of the AI2 dataset Hosseini et al. (2014) released as a part of project Euclid 2 . We chose problems with only single operation from these two datasets - 103 from MA1 and 81 from IXL dataset. 3.2 Setting We used publicly available glove pre-trained embeddings 3 that were trained on Common Crawl containing 42 billion tokens, with a vocabulary siz"
Y18-1053,N18-1060,0,0.0130832,"Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 asked about the rulers. The verb “place” appears in the context of the rulers, so it also receives higher weights. 7 Previous Work Mukherjee and Garain (2008) explained different techniques used for word problem solving in their survey paper. Bobrow (1964) represented word problems in terms of a relational model. Bakman (2007) touched upon understanding of word problems involving extraneous information. Multiple approaches like template alignment, verb categorization, CFG rules can be used to solve word problems. Liang et al. (2018) suggested that there exists mainly two kinds of approaches for solving MWPs - one involving understanding and the other without understanding. Kushman et al. (2014) system was a joint log linear distribution over the full set of equations and alignments between the variables and text. The number of equations was dependent on the number of training equation templates. The number slots were filled by the numbers present in the text while the unknown or variable slot were filled by the nouns in the problem text. Huang et al. (2017) also extracted relevant templates and did fine grained inferenci"
Y18-1053,P17-1015,0,0.0227863,"n problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techniques. Huang et al. (2018) used intermediate meaning representation to generate equations. There has been attempts (Ling et al., 2017) to generate answer rationales for arriving at the final answer. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the help of some hand-written rules and modified the softmax function on the decoder side. There have been a con"
Y18-1053,D15-1166,0,0.00783601,"re compared to arrive at the alignment. at = align(ht , h˜s ) exp(ht T .h˜s ) = Σ 0 exp(h T .h˜0 ) s t s The context vector ct is computed as the weighted combination of the hidden states from the word sequence: Approach ct = Σt at × ht Base model The encoder-decoder architecture (Sutskever et al., 2014) has proved beneficial in many applications. But, this architecture has its limitation in handling long input sequences. This limitation results from a fixed size internal representation of the encoded sequence where the target sequence is decoded from this representation. Attention mechanism (Luong et al., 2015) has been widely used where the network learns the relative importance on which parts to attend to. In this architecture, the input sequence is encoded as a sequence of vectors and the decoder has access to all these vectors instead of a single vector. We modeled the input sequence as a sequence of word vectors. Each word vector is a concatenated vector representation of pre-trained glove (Pennington et al., 2014) embeddings and the embeddings learned by the network from the training corpus. The equation generation for a word problem requires the identification of words which indicate the pres"
Y18-1053,P16-1202,0,0.0144985,"oy and Roth, 2016a) used two modules to solve any arithmetic word problem. The first module was a CFG based Semantic Parser and other module solved the problem by decomposing it into a series of classification problems (Roy and Roth, 2016b) with formation of an expression tree through constrained inference. Hosseini et al. (2014) ‘s system used verb categorization for identifying relevant variables, their values and mapping them into a set of linear equations which can be easily solved. The system identified 7 kinds of verbs used in the problems which was predicted by support vector machines. Mitra and Baral (2016) created an arithmetic word problem solver that learned how to use formulas to solve simple addition and subtraction problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techni"
Y18-1053,D14-1162,0,0.0819292,"ut sequences. This limitation results from a fixed size internal representation of the encoded sequence where the target sequence is decoded from this representation. Attention mechanism (Luong et al., 2015) has been widely used where the network learns the relative importance on which parts to attend to. In this architecture, the input sequence is encoded as a sequence of vectors and the decoder has access to all these vectors instead of a single vector. We modeled the input sequence as a sequence of word vectors. Each word vector is a concatenated vector representation of pre-trained glove (Pennington et al., 2014) embeddings and the embeddings learned by the network from the training corpus. The equation generation for a word problem requires the identification of words which indicate the presence of operands and operators. So an attention based encoder-decoder has been used as a baseline for our equation generation system. Both the encoder and decoder employ Long-term short-term memory (LSTM) to represent the input and target sequence respectively. hj = f(hj−1 , sj ) (1) The j th hidden state of the encoder is computed as equation 1 using an LSTM. The decoder is initialized with the hidden and cell st"
Y18-1053,Y18-1000,0,0.263852,"Missing"
Y18-1053,D17-1088,0,0.0554704,"n and a large vocabulary. Simple keyword or pattern matching is less equipped to take up such a challenge (Bakman, 2007). There are 112 short trees and 119 tall trees currently in the park . Park workers will plant 105 short trees today . How many short trees will the park have when the workers are finished ? Answer : 112 + 105 Table 1: Mathematical Word Problem Example Previous systems either rely heavily on specific set of problem abstractions based on verb categories (Hosseini et al., 2014) or learning equations from pre-defined set of templates (Kushman et al., 2014). Deep neural solvers (Wang et al., 2017) proposed a combination of sequence-to-sequence model and information retrieval system. However, an ideal equation generation system for a word problem should be able to identify components of the equation and form the equation in an orderly fashion independent of syntax or vocabulary of the sentences. In this work, we introduce EquGener - an equation generator using a memory network with an equation decoder. Intuitively, a human math solver collects relevant details from the problem description which equip her to solve the problem (Dellarosa, 1986). Driven by this intuition, we learn a dense"
yeka-etal-2014-benchmarking,bojar-etal-2010-data,0,\N,Missing
yeka-etal-2014-benchmarking,W12-3152,0,\N,Missing
yeka-etal-2014-benchmarking,P02-1040,0,\N,Missing
yeka-etal-2014-benchmarking,P07-2045,0,\N,Missing
yeka-etal-2014-benchmarking,W03-0301,0,\N,Missing
yeka-etal-2014-benchmarking,P09-1090,0,\N,Missing
yeka-etal-2014-benchmarking,kolachina-kolachina-2012-parsing,1,\N,Missing
yeka-etal-2014-benchmarking,W10-3809,0,\N,Missing
yeka-etal-2014-benchmarking,I08-1067,0,\N,Missing
yeka-etal-2014-benchmarking,P03-1021,0,\N,Missing
