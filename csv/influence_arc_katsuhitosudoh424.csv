2007.iwslt-1.16,N07-1008,0,0.0150061,"parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with the ASR’s n-best list’s confidence measures. This paper is organized as follows: The overview of our decoder is presented in Section 2. We will describe the feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems s"
2007.iwslt-1.16,P03-1021,0,0.0149653,"e feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems share the same online training algorithm, but differ in that the decoder’s parameters are updated based on the dynamically generated candidate list, whereby the reranking training is based on a fixed translation candidate list. Section 4 presents the results for the evaluation campaign of IWSLT 2007. 2. Machine Translation System We use a linear feature combination approach [8] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of ca"
2007.iwslt-1.16,P05-1033,0,0.107755,"by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic str"
2007.iwslt-1.16,N03-1017,0,0.00424325,"te k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method [1] which performed better than a phrase-based system in the last year’s evaluation[2]. This method is able to generate translations efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-t"
2007.iwslt-1.16,P98-2230,0,0.0172839,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,W06-3119,0,0.0336527,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,2004.iwslt-evaluation.13,0,0.0146149,"nd intersected with an n-gram. 2.2. Features 2.2.1. Baseline Features The hierarchical phrase-based translation system employs standard real valued value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in both directions, h(γ|¯bβ) and h(¯bβ|γ), estimated by relative counts, count(γ, ¯bβ) [9]. • Word-based lexically weighted models of hlex (γ|¯bβ) and hlex (¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of"
2007.iwslt-1.16,W06-3108,0,0.0191772,"(¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. X1 X2 f j−1 fj f j+1 f j+3 X3 f j+2 Figure 2: Example hierarchical features. word alignments are observed with the same source and target sides, only the most frequently observed word alignment is kept to reduce the grammar size. Using the word alignment structure inside hierarchical phrases, we employs following feature set. • Word pair features directly capture the source/target word co"
2007.iwslt-1.16,P06-1098,1,0.894326,"parse binary features — of the order of millions — are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both sys"
2007.iwslt-1.16,P04-1007,0,0.0263178,"(ei−1 , fj−1 ), (ei , fj+1 )). • Insertion/deletion features are integrated in which no word alignment is associated in the target/source side. Inserted words are associated with all the words in the source sentence, such as (ei+1 , f1 ), ..., (ei+1 , fJ ) for the non-aligned word ei+1 with the source sentence f1J in Figure 1. In the same way, we use hierarchical phrase-wise deletion features by associating each inserted source word in a phrase to all the target words in the same phrase. • Target bigram features are also included to directly capture the fluency as in the n-gram language model [15], such as (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... in Figure 1. • Hierarchical features capture dependencies the source words in a parent phrase to the source words in child phrases, such as (fj−1 , fj ), (fj−1 , fj+1 ), (fj+3 , fj ), (fj+3 , fj+1 ), (fj , fj+2 ) and (fj+1 , fj+2 ) as indicated by the arrows in Figure 2. The hierarchical features are extracted only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt"
2007.iwslt-1.16,E99-1010,0,0.0399514,"ed only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt=1 i=0 for n = 1, ..., N do for t = 1, ..., T do C t ← bestk (f t ; wi ) Ot ← oraclem (Ot ∪ C t ; et ) wi+1 = update wi using C t w.r.t. Ot i=i+1 end for end for PN T i w return i=1 NT In order to achieve the generalization capability, we introduce normalized tokens for each surface form [3]. • Word class/part-of-speech/named entity. Words are clustered by mkcls [16]. The part-of-speech (POS) and named entity (NE) tags are also integrated to capture linguistic characteristics when taggers are available. A unique word class is assigned to each surface form. However, multiple POS/NE are potentially assigned to each surface word. In our approach, we do not disambiguate labels, but simply collect a surface word to multiple tags dictionary. Those tags are integrated by first running a tagger on the training data. Then, a surface form to POS/NE dictionary is generated by collecting all possible tags for each word. • Synsets from WordNet. In order to represent s"
2007.iwslt-1.16,D07-1080,1,0.562399,"ken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that"
2007.iwslt-1.16,P06-1091,0,0.0676827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P05-1012,0,0.28923,"WSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a s"
2007.iwslt-1.16,P06-2098,0,0.0623452,"sed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with"
2007.iwslt-1.16,P06-1096,0,0.119827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P02-1040,0,0.07649,"rmalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores to each reference translation. Therefore, possible oracle translations are mai"
2007.iwslt-1.16,W04-3201,0,0.0272233,"Liang et at. [18] presented a similar updating strategy in which parameters were updated toward an oracle translation found in C t , but ignored potentially better translations discovered in the past iterations. A new wi+1 is computed using the k-best list C t with respect to the oracle translations Ot (line 5). After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9). When updating parameters in line 5, we use the Margin Infused Relaxed Algorithm (MIRA) [4] which is an online version of the large-margin training algorithm for structured classification [20] that has been successfully used for dependency parsing [5] and joint-labeling/chunking [6]. Line 5 of the weight vector update procedure in Algorithm 1 is reAlgorithm 2 Online Training Algorithm for Reranker placed by the solution of: X 1 ˆ i+1 = argmin ||wi+1 − wi ||2 + C ξ(ˆ e, e′ ) w wi+1 2 ′ eˆ,e subject to si+1 (f t , eˆ) − si+1 (f t , e′ ) + ξ(ˆ e, e′ ) ≥ L(ˆ e , e′ ; et ) ′ ξ(ˆ e, e ) ≥ 0 ∀ˆ e ∈ Ot , ∀e′ ∈ C t (3)  ⊤ where si (f t , e) = wi · h(f t , e). ξ(·) is a non-negative slack variable and C ≥ 0 is a constant to control the influence to the objective function. A larger C implies"
2007.iwslt-1.16,P02-1034,0,0.0176638,"ain the best oracle translations O1T = eˆ1 , ..., eˆT that is treated as a “bed” document. The approximated BLEU for a hypothesized translation e′ for the training instance (f t , et ) is computed over the bed O1T except for eˆt , which is replaced by e′ : BLEU({ˆ e1 , ..., eˆt−1 , e′ , eˆt+1 , ..., eˆT }; E) The loss computed by the approximated BLEU measures the document-wise loss of substituting the correct translation eˆt Our reranking system is basically identical to the system presented in the last year’s IWSLT 2006 evaluation [2] that is based on the parse reranking method explained in [21]. We first generate n-best lists of candidate translations from the decoder, then train reranking model using the development set with additional features by ranking voted perceptron. Finally, during the testing, we rerank the k-best list of test data from the decoder by the parameters for the reranking. A separately trained reranking model is used for the ASR’s nbest list. The reranker selects the best translation out of the merged k · n-best list generated by translating all the sentences in the n-best list. 3.1. Features The reranking system employs a slightly different feature set from the"
2007.iwslt-1.16,takezawa-etal-2002-toward,0,0.068604,"n line 8 is based on an perceptron algorithm with the update amount scaled by the loss function L(·).  wn = wn + L(Rj , Ri ; et ) · h(f t , Rj ) − h(f t , Ri ) (5) As our loss function, we employed the difference of the approximated BLEU in Section 2.4, but used a set of 1-best translations from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-Englis"
2007.iwslt-1.16,2005.mtsummit-papers.11,0,0.00566131,"ions from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists large"
2007.iwslt-1.16,P03-1010,0,0.0124565,"ata for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decode"
2007.iwslt-1.16,H05-1059,0,0.0194319,"data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, mi"
2007.iwslt-1.16,W03-1506,0,0.274806,"d texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, miscellaneous texts and lexicons. Their characteristics are very different from the style in the IWSLT development and test conditions. Table 2 shows the development/test set perplexity of the source side language computed by the trigram of t"
2007.iwslt-1.16,N06-1014,0,0.0286367,"Missing"
2007.iwslt-1.16,2006.iwslt-evaluation.14,1,\N,Missing
2007.iwslt-1.16,C98-2225,0,\N,Missing
2008.iwslt-evaluation.13,D07-1080,1,0.906376,"Missing"
2008.iwslt-evaluation.13,2006.iwslt-evaluation.14,1,0.898254,"ted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document"
2008.iwslt-evaluation.13,2007.iwslt-1.16,1,0.843688,"entences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of sparse binary features for reranking, as well as a real-valued feature (decoder score). • Lexical translation probabilities in phrase pairs 3.3.1. Word alignment features • Word-based insertion/deletion penalties We use source-target word pairs extracted by separately running IBM Model 1 in both direction [4]. In addition to source-target word unigram pairs, we used pairs of targetside word bigram and their corresponding source-side words in terms of the word alignment. We also include POS-based features, target-side word surfaces are replaced with their POS tags in the word alignment features above. Target-side (English) POS tags are automatically annotated by Brill Tagger. • Word 5-gram language model scores • Reordering penalties • Length penalties on both words and hierarchical phrases 3. Reranking Component Our reranking component is based on Ranking SVMs [1]. Each decoder k-best translation"
2008.iwslt-evaluation.13,P03-1021,0,0.0231724,"2 - work in the experiments, because they failed to capture useful context information in the current condition. We discuss these features using distinctive examples between reranker selections and decoder 1-bests. This paper is organized as follows: Section 2 brieﬂy describes our SMT decoder. Section 3 describes our reranking component and sparse features for reranking. Section 4 presents the results for the evaluation campaign of IWSLT 2008, followed by discussion in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hie"
2008.iwslt-evaluation.13,J07-2003,0,0.0606697,"on in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by"
2008.iwslt-evaluation.13,N03-1017,0,0.0076853,"ased on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by"
2008.iwslt-evaluation.13,J03-1002,0,0.00281422,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,J04-4002,0,0.0283074,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,P02-1040,0,0.0764379,"in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document-wise calculation and is not suitable for sentence-level reranking. Given 1-best translation outputs for T input sentences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of spar"
2008.iwslt-evaluation.13,W03-1506,0,0.0710488,"Missing"
2008.iwslt-evaluation.13,C08-1137,0,\N,Missing
2008.iwslt-evaluation.13,2007.iwslt-1.8,0,\N,Missing
2008.iwslt-evaluation.13,P07-1002,0,\N,Missing
2008.iwslt-evaluation.13,W06-3110,0,\N,Missing
2008.iwslt-evaluation.13,P06-1066,0,\N,Missing
2008.iwslt-evaluation.13,P07-2045,0,\N,Missing
2008.iwslt-evaluation.13,N04-1022,0,\N,Missing
2008.iwslt-evaluation.13,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.13,W06-3119,0,\N,Missing
2008.iwslt-evaluation.13,I08-1066,0,\N,Missing
2008.iwslt-evaluation.13,2006.iwslt-evaluation.22,0,\N,Missing
2010.iwslt-evaluation.19,2005.iwslt-1.19,0,\N,Missing
2010.iwslt-evaluation.19,W09-0424,0,\N,Missing
2010.iwslt-evaluation.19,P05-1056,0,\N,Missing
2010.iwslt-evaluation.19,2009.iwslt-evaluation.5,0,\N,Missing
2010.iwslt-evaluation.19,2005.eamt-1.19,0,\N,Missing
2010.iwslt-evaluation.19,zhang-etal-2004-interpreting,0,\N,Missing
2010.iwslt-evaluation.19,2010.iwslt-papers.5,1,\N,Missing
2010.iwslt-papers.5,D10-1044,0,\N,Missing
2010.iwslt-papers.5,W08-0334,0,\N,Missing
2010.iwslt-papers.5,W09-0424,0,\N,Missing
2010.iwslt-papers.5,D09-1040,0,\N,Missing
2010.iwslt-papers.5,J07-3002,0,\N,Missing
2010.iwslt-papers.5,D09-1074,0,\N,Missing
2010.iwslt-papers.5,D08-1090,0,\N,Missing
2010.iwslt-papers.5,W10-1759,0,\N,Missing
2010.iwslt-papers.5,J04-4002,0,\N,Missing
2010.iwslt-papers.5,P06-1002,0,\N,Missing
2010.iwslt-papers.5,D07-1036,0,\N,Missing
2010.iwslt-papers.5,2005.mtsummit-papers.11,0,\N,Missing
2010.iwslt-papers.5,W10-1713,0,\N,Missing
2010.iwslt-papers.5,2009.mtsummit-papers.5,0,\N,Missing
2011.mtsummit-papers.11,P06-1002,0,0.0218152,"s et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusi"
2011.mtsummit-papers.11,W09-0432,0,0.0169878,"ng (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A"
2011.mtsummit-papers.11,J07-2003,0,0.0432673,"archical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in"
2011.mtsummit-papers.11,W07-0722,0,0.0256665,", and (b) if so, which step. Following the training pipeline in Section 1, we briefly survey the approaches in prior work: 1) Word alignment model training: Assume a probabilistic model of alignment, where the parameters (e.g. lexical translation probabilities in the IBM Models) are estimated from a mix of in-domain 119 and out-of-domain data. One method is to interpolate separate sets of parameters estimated from indomain and out-domain data. Wu et. al.(2005) sets the interpolation weights to be proportional to the relative frequency of observances in in-domain and out-of-domain data, while (Civera and Juan, 2007) treats it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual cor"
2011.mtsummit-papers.11,P11-1043,0,0.0359624,"Missing"
2011.mtsummit-papers.11,P09-2058,0,0.0240098,"ed from indomain and out-domain data. Wu et. al.(2005) sets the interpolation weights to be proportional to the relative frequency of observances in in-domain and out-of-domain data, while (Civera and Juan, 2007) treats it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that al"
2011.mtsummit-papers.11,2010.iwslt-papers.5,1,0.759658,"data. Outof-domain data is heterogeneous, consisting of Hansards, broadcast conversations, weblogs, etc. The data statistics are shown in Table 1. We compare 3 phrasal SMT systems: • in-domain model: Step1: Train word alignment model on in-domain bitext (M in ). Step 2-to-4: Alignment inference on in-domain text, followed by phrase extraction and scoring. • general-domain model: Step1: Train word alignment model on concatenated in-domain and out-of-domain bitext (M gen ). Step2-to-4: same as in-domain model. This simple approach is a strong adaptation baseline competitive in many tasks, c.f. (Duh et al., 2010). • bayes: Step1-to-2: Algorithm 1. Step3-to-4: same as in-domain model. Note that out-of-domain information is used only up to step 2 and excluded in further steps of the 4 www.itl.nist.gov/iad/mig/tests/mt/doc/ 118 pipeline. This clarifies the analysis: if we were to include out-of-domain bitext for phrase extraction, our SMT system might acquire new phrases, which reduces out-of-vocabulary rate and confounds the analysis of alignment inference results. Further, from preliminary experiments, we found that adding out-of-domain bitext to all four steps actually degrade results sometimes, due t"
2011.mtsummit-papers.11,W08-0334,0,0.0154527,"weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment inference, and view the latter as a sequential Bayesian update problem. The advantages of our approach are: 1. Its modularity enables the use of any model training algorithm for word alignment, as long as it outputs N-best lists or posteriors. 2. It gives consistent improvements over a multitude of datasets (2 tasks and 11 language pairs). We have shown how alignment inf"
2011.mtsummit-papers.11,D10-1044,0,0.0138276,"oldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marc"
2011.mtsummit-papers.11,N04-1035,0,0.0363765,"l we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contribution of this paper i"
2011.mtsummit-papers.11,2005.eamt-1.19,0,0.0322357,"the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment i"
2011.mtsummit-papers.11,W07-0733,0,0.0180875,"nslation performance, some work have shown weak correlation between the two (Ayan and Dorr, 2006; Fraser and Marcu, 2009). There are also adaptation methods that do not target a particular step in the training pipeline. For example, the information retrieval approach (Hildebrand et al., 2005) begins by identifying a subset of out-of-domain bitext most similar to in-domain; this data subset can be used for any (or all) steps of the training pipeline. An alternative approach is to train separate translation models for in-domain and out-of-domain data, then combine the final models log-linearly (Koehn and Schroeder, 2007) or dynamically (Finch and Sumita, 2008; L¨u et al., 2007). 5 Conclusions and Future Work We proposed a flexible and efficient method for domain adaptation in machine translation. The idea is to decompose the word alignment process into model training and alignment inference, and view the latter as a sequential Bayesian update problem. The advantages of our approach are: 1. Its modularity enables the use of any model training algorithm for word alignment, as long as it outputs N-best lists or posteriors. 2. It gives consistent improvements over a multitude of datasets (2 tasks and 11 language"
2011.mtsummit-papers.11,N03-1017,0,0.00872408,"d to particular model formalisms (e.g. phrase vs. hierarchical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively b"
2011.mtsummit-papers.11,2005.mtsummit-papers.11,0,0.0204779,"ple is counted equally. It may be beneficial to have a parameter if it can be tuned well without of overfitting, but we do not consider it here. 3 Experiments 3.1 Datasets and Setup We evaluate our proposed method under two tasks: The EMEA task involves the translation of medical texts from the European Medical Agency (Tiedemann, 2009). We test on ten language pairs–Danish (da), German (de), Greek (el), Spanish (es), Finnish (fi), French (fr), Italian (it), Dutch (nl), Portuguese (pt), and Swedish (sv)–all translating into English. The out-of-domain data are parliamentary texts from Europarl (Koehn, 2005). The NIST task involves translating newswire text using Chinese-to-English NIST OpenMT 2008 data. da in-domain 45.3 general 46.1 bayes 47.1* de 35.5 36.1 36.4* el 41.3 41.6 42.5* es 45.0 46.9 46.2 EMEA fi fr 33.6 46.8 34.0 47.8 34.6* 47.9 it 47.7 49.2 49.3* nl 45.6 46.1 46.2* pt 46.3 47.0 47.5* sv 45.3 45.2 45.9* NIST mt06 mt08 27.7 24.4 28.7 24.6 28.7 25.0* Table 2: Main Results: Test BLEU for EMEA (da,de,...,sv) and NIST (mt06,08): Best results are in bold-font. Statistical significant improvement over general-domain model is indicated by asterisk (*). Language Pair In-domain #sent train #w"
2011.mtsummit-papers.11,D09-1106,0,0.0176991,"m might acquire new phrases, which reduces out-of-vocabulary rate and confounds the analysis of alignment inference results. Further, from preliminary experiments, we found that adding out-of-domain bitext to all four steps actually degrade results sometimes, due to increased ambiguity of additional translation options on the target side. For all systems, we use the Moses decoder, adapted SRILM 3gram (EMEA) and 4gram (NIST), MERT for weight optimization, and GIZA++ (Model4) as the underlying word alignment training tool. Phrase tables are extracted from alignment matrices using the method of (Liu et al., 2009). 3.2 Main Results Table 2 summarizes all our main results. We see that bayes gives robust improvements in testset BLEU. For example, for Danish-to-English translation, using in-domain data by itself achieves 45.3 BLEU (in-domain). This can be improved to 46.1 BLEU by concating out-of-domain data (general). The proposed method, however, further improves the result to 47.1 BLEU (bayes). For the NIST dataset, we see that bayes improves upon general and indomain for the MT08 testset, and ties with general for the MT06 testset. On average, bayes improves over in-domain model by 1.1 BLEU points. Fu"
2011.mtsummit-papers.11,D07-1036,0,0.0487968,"Missing"
2011.mtsummit-papers.11,D09-1040,0,0.0212628,"s it as a hidden parameter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to"
2011.mtsummit-papers.11,D09-1074,0,0.0179112,"on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between the two (Ayan and Dorr,"
2011.mtsummit-papers.11,P08-1023,0,0.0144565,"ndard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contribution of this paper is two-fold: • We i"
2011.mtsummit-papers.11,J04-4002,0,0.0486609,"l formalisms (e.g. phrase vs. hierarchical rules) required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertain"
2011.mtsummit-papers.11,P05-1034,0,0.039829,"on/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and general-domain bitexts. The contrib"
2011.mtsummit-papers.11,W10-1759,0,0.0106112,"e translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about the latter two papers, in particular, is that they are able to incorporate supervised information (likelihood or expected TER on the dev set) for score adaptation. We emphasize that our contribution is orthogonal to previous work: alignment inference adaptation can be combined with any adaptation method in other parts of the pipeline. It remains to be seen whether the improvements are additive: while our results are positive in both alignment and final translation performance, some work have shown weak correlation between"
2011.mtsummit-papers.11,D08-1090,0,0.0212051,"meter in a mixture model. These methods are similar to ours in that the motivation is to improve alignments, but differs in that the focus is on training (not inference). 2) Alignment inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation invol"
2011.mtsummit-papers.11,P07-1004,0,0.0168471,"ent inference: To the best of our knowledge, there is no previous work in this area. The model training of Step 1 is related but not the same. Instead, alignment combination works (Deng and Zhou, 2009) may give some insights. 3) Phrase extraction: Out-of-domain text may contain unseen phrases useful for in-domain data. One approach attempts to discover paraphrases from large monolingual corpora (Marton et al., 2009; Snover et al., 2008). Another is a self-training approach that translates source in-domain text and re-trains the translation model on synthetic data (Bertoldi and Federico, 2009; Ueffing et al., 2007). 4) Scoring: Adaptation in the scoring step is the most direct way to improve results since it is the step closest to the final translation model. In fact, one could argue that all the previous steps are simply pre-processing to narrow down the size of ruleset/phrasetable; if scores are well-tuned, good translations can be achieved even if the ruleset is infinite in size. Recent approaches to score adaptation involve combining in-domain and out-of-domain scores at either the sentence or the phrase level (Shah et al., 2010; Matsoukas et al., 2009; Foster et al., 2010). A promising aspect about"
2011.mtsummit-papers.11,P05-1058,0,0.0309943,"3, except we use scores from the general model M gen rather than M in ; βij is calculated analogously to Eq. 4:  βij = A ∈N 2.4 lgen (A )δ(Aij = 0)/Z (8) I J gen (e1 ,f1 ) Summary and Caveat The pseudocode for the overall algorithm is presented in Algorithm 1. Basically, the alignment matrix posteriors are computed for each sentence pair by combining statistics from in-domain and generaldomain models. It is worth noting two caveats: • Our Bayesian view is that there is a prior alignment matrix, which is updated by in-domain model statistics. This differs from previous work in Step 1, e.g. (Wu et al., 2005), which adopts a prior for alignment model parameters. The distinction between adapting inference results and model parameters is an important one, and this is what gives us a flexible generalpurpose method. • Eq. 6 does not contain a tuning parameter between likelihood aij and prior αij . This arises from the sequential Bayesian update perspective, where each additional sample is counted equally. It may be beneficial to have a parameter if it can be tuned well without of overfitting, but we do not consider it here. 3 Experiments 3.1 Datasets and Setup We evaluate our proposed method under two"
2011.mtsummit-papers.11,J97-3002,0,0.110319,") required for the extraction/scoring steps. All we need are the standard toolsets for the training pipeline, and a pre-existing word alignment model that can generate n-best lists. Therefore, adaptation in Step 2 has wide applicability. Although Step 1 and Step 2 are sometimes considered as a single process, we will show that the decomposition into two steps is quite beneficial and opens up new possibilities. Virtually all SMT systems can be interpreted as consisting of the aforementioned 4-stage pipeline (e.g. phrase-based (Koehn et al., 2003; Och and Ney, 2004), hierarchical (Chiang, 2007; Wu, 1997), and tree-based (Quirk et al., 2005; Galley et al., 2004; Mi et al., 2008)). Our approach can be briefly summarized as follows: First, we train a word alignment model on a large general-domain dataset, then predict the alignment points for an in-domain bitext. The n-best list of predictions are used to compute a Bayesian prior indicating the a priori belief of any two words being aligned. Then, alignment inference of in-domain bitext is viewed as a sequential Bayesian update on weighted alignment matrices. The idea is to effectively balance the uncertainty in alignment from both in-domain and"
2011.mtsummit-papers.11,zhang-etal-2004-interpreting,0,0.0273166,"LEU. For example, for Danish-to-English translation, using in-domain data by itself achieves 45.3 BLEU (in-domain). This can be improved to 46.1 BLEU by concating out-of-domain data (general). The proposed method, however, further improves the result to 47.1 BLEU (bayes). For the NIST dataset, we see that bayes improves upon general and indomain for the MT08 testset, and ties with general for the MT06 testset. On average, bayes improves over in-domain model by 1.1 BLEU points. Further, in 9 of 12 cases, bayes also outperforms general-domain model by statistically significant margins, p <0.05 (Zhang et al., 2004). We thus conclude that the proposed method is robust under adaptation scenarios. 3.3 Analyses of Alignments We are also interested in checking if BLEU improvements correlate with quantifiable alignment improvements. This evaluation is possible since the NIST dataset contains some manual alignment annotations (LDC2006E93). We identified 892 sentence-pairs in our in-domain bitext that have manual alignments. Note that this supervised information is never used in any part of our method. Figure 1 shows alignment precision/recall. The curve is computed by thresholding the estimated weighted aligme"
2011.mtsummit-papers.34,2008.iwslt-papers.1,0,0.0222347,"presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions of translation systems between these language pairs suffers greatly on the scarce resource, such as parallel data. We introduced the idea of compatibility, where all languages can be mapped to the same semantic mea"
2011.mtsummit-papers.34,J93-2003,0,0.0230862,"f each sentence pair and choose a certain percentage of the best scored sentences for training. In order to include information from various resources, the quality of a sentence pair is measured using a log-linear model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and e"
2011.mtsummit-papers.34,2002.tmi-tutorials.2,0,0.0607427,"n the idea of compatibility. Generally speaking, the quality of compatible predictions provided by multiple systems is more reliable. For simple classiﬁcation problems, it is reasonable to take a prediction as good which the multiple systems agree on. This idea is widely used in ensemble learning and semi-supervised learning. Take Bootstrap aggregating, a meta-algorithm for ensemble learning as an example, multiple models are separately trained on randomly generated sub-samples, and then vote to achieve ﬁnal predictions. Another example closely related to our method is co-training such as in (Callison-Burch, 2002). One way to select automatic predictions for re-training in co-training is to choose the agreed ones. Different from simple classiﬁcation problems, even complex structured prediction problems such as parsing, the output of MT is in human languages, which may be the most complicated way to represent the meaning of another human language. It is too strict to ask multiple systems to provide exactly the same translated sentence for an input. We extend the agreement idea to the compatibility idea. Informally, two sentences are called compatible if they express the same meaning to some extent. We c"
2011.mtsummit-papers.34,P07-1092,0,0.0141798,"language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited."
2011.mtsummit-papers.34,P08-1010,0,0.0304425,"Missing"
2011.mtsummit-papers.34,2008.eamt-1.6,0,0.0108565,"d Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by"
2011.mtsummit-papers.34,W09-0431,0,0.013434,"e, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical b"
2011.mtsummit-papers.34,P07-2045,0,0.00524468,"d using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). E"
2011.mtsummit-papers.34,2005.mtsummit-papers.11,0,0.0225646,"ll scarce resourced language pairs. The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on l"
2011.mtsummit-papers.34,D07-1005,0,0.0133387,"Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation sys"
2011.mtsummit-papers.34,2010.iwslt-papers.12,0,0.0117409,"les obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including rule based, statistical based or even human translation. The approach introduced in (Leusch et al., 2010) can combine the translation output of a test set produced by any pivot MTs per different languages, however the individual systems are not improved and novel training data is not exploited. Bertoldi et al. (2008) evaluated several methods of pivot languages but did apply the global corpus ﬁltering i.e. compatibility measure to control the quality of data. Our purpose is not only to improve the translation quality but also to provide useful linguistic resources for other NLP tasks. 5 Conclusion and Future Work Thousands of human languages are recognized in the world, and building up millions o"
2011.mtsummit-papers.34,N01-1020,0,0.0187021,"rget languages. Callison-Burch (2002) presented a co-training method for SMT, the agreement of multiple translation systems is explored to ﬁnd the best translation for re-training. We applied compatibility instead of agreement based approach, detailed description on the difference between compatibility and agreement is referred to Section 1 and Section 2.3. Uefﬁng et al. (2009) explored model adaptation methods to use the monolingual data from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 20"
2011.mtsummit-papers.34,N04-1034,0,0.0782549,"Missing"
2011.mtsummit-papers.34,J03-1002,0,0.00497387,"model combining different sub-models. Let (f1J , eI1 ) be a bilingual sentence, the evaluation is performed using the following Equation: H(f1J , eI1 ) = M  λm hm (f1J , eI1 ) m=1 409 hm (f1J , eI1 ) is a score evaluated on this sentence pair using sub-model m. Each model m is assigned with a feature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a s"
2011.mtsummit-papers.34,P02-1040,0,0.0841189,"Missing"
2011.mtsummit-papers.34,N10-1063,0,0.039881,"Missing"
2011.mtsummit-papers.34,W11-2100,0,0.0454475,"The generated virtual parallel corpus can not only be applied into MT but also other NLP tasks. 2 2.1 Generating Virtual Parallel Data Background and Motivation There are only a few parallel corpora publicly available for some languages we work on. The JRC-Acquis(JRC) is a huge collection of European Union legislative documents translated into more than twenty ofﬁcial European languages (Steinberger et al., 2006). The European Parliament Proceedings Parallel Corpus (Europarl corpus) was extracted from the proceedings of the European Parliament (1996-today) (Koehn, 2005). News Commentary(NC) (SMT, 2011) and SETimes (SETIMES, 2011) are corpora collected from the news domains. In this paper, we are concerned with generating high-quality, virtual parallel data for machine translation. To do this, we exploit multiple parallel corpora in different language pairs. In particular, we generate parallel corpora for scarce resourced languages, taking Romanian to German as a case study for simplicity. We can also take German to Romanian or other language directions. In order to ﬁnd out the gap between the translation quality on better studied language pairs and that on less studied language pairs, we co"
2011.mtsummit-papers.34,steinberger-etal-2006-jrc,0,0.0697896,"Missing"
2011.mtsummit-papers.34,C96-2141,0,0.280846,"ature weight λm . For simplicity, we only include the negative logarithm of IBM model 1 (Brown et al., 1993) in normal and inverse direction as submodels. We combine the IBM model 1 in both directions in the log-linear model with an equal weight for each direction. We use the training software GIZA++ (Och and Ney, 2003) to obtain the lexicon probability. 3 Experimental Results 3.1 MT Setup We apply Moses (Koehn et al., 2007) as our baseline translation system and train standard alignment models in both directions with GIZA++ (Och and Ney, 2003) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993) which brings us the optimal translation performance and efﬁciency based on empirical evaluations. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The language model is a statistical 5-gram model with modiﬁed Kneser-Ney smoothing estimated using SRI-LM toolkit (Stolcke, 2002). Each language model is trained with the target side of the parallel data. We do not apply any zmert tuning in EMS because it does not improve our translation results on the evaluation set. Imp"
2011.mtsummit-papers.34,P07-1108,0,0.0135407,"from the source language, while their learning and application are constrained in a bilingual way without introducing any information from a third language. Mann and Yarowsky (2001) presented a method to induce translation lexicon based on transduction 412 models of cognate pairs via bridge language. The cognate string edit distance was applied instead of a general MT system, so that the vocabulary learning is limited to mostly European languages. For bridge or pivot languages in MT, Kumar et al. (2007) described a method to improve word alignment quality using multiple bridge languages. In (Wu and Wang, 2007) and (Habash and Hu, 2009) phrase translation tables are improved using the phrase tables obtained from pivot languages in different ways, and in (Eisele et al., 2008) a hybrid method combining RBMT and SMT systems is introduced to ﬁll up the data gap for pivot translation. Cohn and Lapata (2007) presented a method to obtain more reliable translation estimates from small data sets using multi-parallel data. Different from the previous approaches, we work on a black-boxed translation system, which means generation of the virtual data can be performed on any kind of translation systems including"
2011.mtsummit-papers.36,2009.mtsummit-posters.1,0,0.0193042,"rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing. 3 Proposed method This section presents the propos"
2011.mtsummit-papers.36,J93-2003,0,0.0259478,"ainder of this paper is organized as follows. Section 2 brieﬂy reviews related studies on the reordering problem and another related technology called post-editing. Section 3 presents the proposed method in detail taking Japanese-to-English translation as a test case. Section 4 reports our experiments and discusses the results. Section 5 concludes this paper with our prospects for future work. 2 Related Work Reordering is a both theoretically and practically challenging problem in SMT. In the early period of SMT studies, reordering is modeled by distancebased constraints in translation model (Brown et al., 1993; Koehn et al., 2003). This reordering model is easy to compute and also works well in relatively similar language pair like French-to-English. The distance-based reordering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but m"
2011.mtsummit-papers.36,J07-2003,0,0.106973,"Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up"
2011.mtsummit-papers.36,P05-1066,0,0.719105,"Missing"
2011.mtsummit-papers.36,W06-1609,0,0.0517088,"Missing"
2011.mtsummit-papers.36,W07-0732,0,0.0604907,"k for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in th"
2011.mtsummit-papers.36,N10-1128,0,0.0291627,"ore phrase translation options efﬁciently. The pre-ordering is based on syntactic parse and can be regarded as a sub-problem of tree-to-string translation. On the other hand, there are several studies on pre-ordering without syntactic parsing. Costa-juss`a and Fonollosa (2006) tackled the pre-ordering problem as SMT, using reordering tables derived from phrase tables. Tromble and Eisner (2009) applied linear ordering models to preordering. Their techniques can be applied to any language pairs but rely on noisy automatic word alignment results as the reference of the reordering model training. Dyer and Resnik (2010) advanced such a pre-ordering-based translation to a novel uniﬁed approach of long-distance pre-ordering and decoding, with discriminative context-free reordering and ﬁnite-state phrase translation. In this paper, we reverse the pre-ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into t"
2011.mtsummit-papers.36,2007.mtsummit-wpt.4,0,0.0178607,"lish translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete translation process in the post-editing"
2011.mtsummit-papers.36,N04-1035,0,0.0641016,"rdering constraint is not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT b"
2011.mtsummit-papers.36,C10-1043,0,0.193651,"ed English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with"
2011.mtsummit-papers.36,N04-1014,0,0.0802119,"not reasonable in some language pair such as English-toJapanese, because they have very different word ordering and appropriate reordering distances of words and phrases highly depend on their syntactic roles and contexts. Tillmann (2004) proposed a lexicalized reordering model that models orientation of phrases by monotone, swap, and discontinuous. This can directly model reordering of adjacent phrases but may not work for long distance reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is con"
2011.mtsummit-papers.36,W10-1736,1,0.779822,"d into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase"
2011.mtsummit-papers.36,N03-1017,0,0.171751,"is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-E"
2011.mtsummit-papers.36,P07-1091,0,0.280387,"rst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering."
2011.mtsummit-papers.36,J08-1002,0,0.0320343,"node. The determiners “the” and “a” are eliminated by the rules, and a pseudo-particle “ va0” is inserted after the subject. 4 • Japanese tokenizer: Mecab3 (with ipadic-2.7.0) Experiment We investigated the advantage of our post-ordering method by the following Japanese-to-English translation experiment with the post-ordering and baseline SMTs. 4.1 Setup We used NTCIR-9 PatentMT (NTCIR-9, 2011) English and Japanese dataset for this experiment. Some statistics of this dataset are shown in Table 1. We preprocessed the dataset by the following softwares: • English syntactic (HPSG) parser: Enju2 (Miyao and Tsujii, 2008) • English tokenizer: stepp (included in Enju package) 2 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 320 Word alignment was automatically estimated using MGIZA++4 using bitexts of 64 or less words in the training set to avoid a problematic underﬂow. Language models are word 5-gram models of English and HFE, trained with SRILM5 . 4.2 Compared methods We compared the proposed post-ordering with three baseline SMTs: a standard phrase-based SMT (PBMT) with lexicalized reordering, a hierarchical phrase-based SMT (HPBMT), and a string-to-tree syntax-based SMT (SBMT), included in Moses6 . 3"
2011.mtsummit-papers.36,P03-1021,0,0.0766118,"DQGDVHFRQGOHQV Figure 5: An example of the post-ordering translation. English parse trees used in SBMT were identical to the ones used for generating HFE sentences in the post-ordering. The post-ordering used two Moses phrase-based decoders, one for Japanese-to-HFE and the other for HFE-to-English. The models for these decoders were trained in the standard manner with Moses, grow-diag-final-and heuristics for symmetric word alignment, msd-bidirectional-fe lexicalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared me"
2011.mtsummit-papers.36,P02-1040,0,0.102487,"icalized reordering (in PBMT and the postordering). The parameter values are optimized by minimum error rate training (MERT) (Och, 2003) with mert-moses.pl. One difference among conﬁgurations of the decoders was distortion limit. The Japanese-to-HFE decoder did not require long distance reordering, so we compared two conditions with the values of 0 (monotone) and 6. The HFE-to-English and PBMT decoders had to drastically reorder phrases so we used the values of 12 and 20. In the HPBMT and SBMT decoders, we used 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE transl"
2011.mtsummit-papers.36,N07-1064,0,0.0204361,"ordering SMT framework for Japanese-to-English translation using English reordering rules on syntactic parse trees. There are a lot of pre-ordering studies, but this is the ﬁrst work of post-ordering to our knowledge. The problem can be regarded as a variant of string-to-tree SMT, from Japanese sentences to English trees. We divide the string-to-tree problem into two simpliﬁed problems, which can be solved efﬁciently with less computational cost than a string-to-tree SMT. Post-ordering is also highly related to post-editing technologies, which aim to correct errors in a rulebased translation (Simard et al., 2007; Dugast et al., 2007; Ehara, 2007) or a different type of SMT (Aikawa and Ruopp, 2009). There is a major difference of the post-ordering from such an post-editing framework; in the post-editing framework, the preceding translation process is a complete source-totarget translation, and the post-editing itself works as an additional process to ﬁx errors. In contrast, the post-ordering framework divides the whole translation process into two sub-processes focusing on translation and reordering. It has an advantage that the sub-processes are simpliﬁed and easy to solve compared to a complete tran"
2011.mtsummit-papers.36,N04-4026,0,0.440185,"source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translatio"
2011.mtsummit-papers.36,D09-1105,0,0.249521,"English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel techn"
2011.mtsummit-papers.36,D07-1077,0,0.0514298,"Missing"
2011.mtsummit-papers.36,J97-3002,0,0.389798,"e reordering, because discontinuous supplies few constraints for reordering. On the other hand, syntaxbased SMT (Yamada and Knight, 2001; Galley et al., 2004; Graehl and Knight, 2004) is a theoretically good solution. Reordering in syntax-based SMT is modeled in a similar manner as reordering of tree nodes in the same level (siblings), regardless of their reordering distance. Although this approach have some shortcomings with parse errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by"
2011.mtsummit-papers.36,C04-1073,0,0.779624,"in the opposite direction and proposes post-ordering; foreign sentences are ﬁrst translated into foreign-ordered English, and then reordered into correctly-ordered English. The experiments on Japanese-to-English patent translation show the signiﬁcant advantage of postordering over baseline phrase-based, hierarchical phrase-based, and syntax-based translation methods by 1.56, 0.76, and 2.77 points in BLEU, respectively. 1 A recently attractive approach for this challenge is called pre-ordering, which reorders source language sentences into the target language word order prior to SMT decoding (Xia and McCord, 2004; Collins et al., 2005; Costa-juss`a and Fonollosa, 2006; Li et al., 2007; Wang et al., 2007; Tromble and Eisner, 2009; Isozaki et al., 2010; Genzel, 2010). The pre-ordering approach is able to reorder source language words in long distance by some reordering rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of t"
2011.mtsummit-papers.36,P06-1066,0,0.0589586,"Missing"
2011.mtsummit-papers.36,I08-1066,0,0.0191332,"e errors and its too strong constraints, syntactic information is expected to be effective in some language pairs. Another syntactic approach, originally proposed by Wu (1997), uses formally-syntactic structure between source and target language sentences. This framework was extended as the hierarchical phrase-based SMT by Chiang (2007) and is convincing alternative in recent SMT research. The reordering models mentioned above are applied in SMT decoding and solved simultaneously with phrase translation. Xiong et al. extended the hierarchical SMT by lexicalized reordering (Xiong et al., 2006; Xiong et al., 2008). However, the integrated search requires a large computational cost both in time and space. To keep the search tractable, we constrain reordering search by its reordering distance, as so-called distortion limit (or maximum span in tree-based decoder). It effectively reduces the computational cost but it also give up long distance reordering exceeding the speciﬁed distortion limit. A novel alternative to the reordering problem, called pre-ordering, has been studied over recent years (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Genzel, 2010). Xia and McCord (2004) proposed auto"
2011.mtsummit-papers.36,P01-1067,0,0.583016,"ng rules or models. This effectively solves the complex reordering problem and achieves good translation performance especially in language pairs with very different word ordering. A crucial issue on the pre-ordering is to develop good reordering methods in the source language. Introduction Statistical Machine Translation (SMT) consists of two major problems, translation of words or phrases and their reordering. Recent research efforts developed novel technologies such as phrase-based SMT with phrase reordering models (Koehn et al., 2003; Tillmann, 2004), and tree-based (or syntax-based) SMT (Yamada and Knight, 2001; Galley et al., 316 In contrast, what can we do in the translation in the opposite direction? This is a non-trivial problem because the pre-ordering techniques are usually language dependent. Even if we have a good preordering technique in A-to-B translation such as reordering rules for syntactic parse trees, it cannot be used directly in B-to-A translation. Developing Bto-A pre-ordering is a different problem from A-toB, which may require a syntactic parser and/or linguistic insights. For example in Japanese-to-English translation, pre-ordering of Japanese parse trees into English word order"
2011.mtsummit-papers.36,zhang-etal-2004-interpreting,0,0.0319715,"ed 15 for its max-chart-span option. The other 321 Table 2 shows the results in BLEU (Papineni et al., 2002) in case-insensitive evaluation and average decoding times7 (on a Xeon 7460 2.66GHz computer) with the compared methods. The proposed post-ordering translation (with monotone Japaneseto-HFE translation) achieved 0.2963 in BLEU, better than the best HPBMT baseline (0.2887) by 0.76 points and the standard PBMT baseline (0.2806) by 1.57 points. The differences were statistically signiﬁcant according to the bootstrap sampling test (p &lt; 0.05 with HPBMT and p &lt; 0.01 with PBMT, 1,000 samples) (Zhang et al., 2004), and it was consistent among all post-ordering conditions. In the Japanese-to-HFE translation, the monotone conﬁguration was slightly better than the reordering with the distortion limit of 6 but the difference was not signiﬁcant. In the HFE-to-English translation, the difference in the distortion limit did not affect the ﬁnal results. Among the baseline methods, HPBMT was better than other baselines by 0.5 points. 4.4 Discussion The proposed post-ordering method was consistently better than the baseline methods in the experiment. To investigate the results in detail, we analyzed the Japanese"
2013.iwslt-evaluation.12,N03-1017,0,0.0375042,"an compounds; and system combination of different types of SMT systems based on generalized minimum Bayes risk (GMBR) framework. This paper presents details of our systems and reports the results in German-English and English-German MT tasks in the evaluation campaign. 2. Translation Methods The main feature of our system for this evaluation is that we perform translation using three different translation models and combine the results through system combination. Each of the three methods is described briefly below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine tra"
2013.iwslt-evaluation.12,J07-2003,0,0.106892,"below. 2.1. Phrase-based Machine Translation Phrase-based machine translation (PBMT; [2]) models the translation process by splitting the source sentence into phrases, translating the phrases into target phrases, and reordering the phrases into the target language order. PBMT is currently the most widely used method in SMT as it is robust, does not require the availability of linguistic analysis tools, and achieves high accuracy, particularly for languages with similar syntactic structure. 2.2. Hierarchical Phrase-based Machine Translation Hierarchical phrase-based machine translation (Hiero; [3]) expands the class of translation rules that can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translatio"
2013.iwslt-evaluation.12,P06-1077,0,0.131208,"at can be used in phrase-based machine translation by further allowing rules with gaps that can be filled in a hierarchical fashion. Hiero is generally considered to be more accurate than PBMT on language pairs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based o"
2013.iwslt-evaluation.12,D08-1022,0,0.0240076,"irs that are less monotonic, but also requires a significantly larger amount of memory and decoding time. As the German-English pair has a significant amount of reordering, particularly with movement of verbs, we can expect that Hiero will be able to handle these reorderings more appropriately in some cases. 2.3. Forest-to-string Machine Translation Tree-to-string machine translation (T2S; [4]) performs translation by first syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the ac"
2013.iwslt-evaluation.12,N04-1014,0,0.0445956,"st syntactically parsing the source sentence, then translating from sub-structures of the parse to a string in the target language. Forest-to-string machine translation (F2S; [5]) generalizes this framework, making it possible to not only translate the single one-best syntactic parse, but a packed forest that encodes many possible parses, helping to pass along some of the ambiguity of parsing to be resolved during translation. While there are a number of proposed methods for incorporating source-side syntax into the translation process, here we use a method based on tree-to-string transducers [6]. Syntax-driven methods such as T2S and F2S are particularly useful for language pairs with extremely large amounts of reordering, as the syntactic parse can help guide the accurate re-ordering of entire phrases or clauses. On the other hand, these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both s"
2013.iwslt-evaluation.12,P13-2119,1,0.927042,"these methods are highly dependent on parsing accuracy, and also have limits on the rules that can be extracted, and are somewhat less robust than the previous two methods. different syntactic parser that did not provide this information. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training t"
2013.iwslt-evaluation.12,D11-1033,0,0.0655671,"mation. 3. SMT Technologies 3.2.2. English-to-German 3.1. Training data selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data1 ). To address this domain adaption problem, we performed adaptation training data selection using the method of [7].2 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [8]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. By taking a sub-sample (same size as the target-domain data), we reduce training time and avoid training and testing language models on the same general-domain data. Similarly, INF (f ) and GENF (f ) are the cross-entropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those w"
2013.iwslt-evaluation.12,P05-1066,0,0.146142,") are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [7]. 3.2. Syntactic Rule-based Pre-ordering Preordering is a method that attempts to first re-order the source sentence into a word order that is closer to the target. As German and English have significantly different word order, we can imagine that this will help our accuracy for this language pair. 3.2.1. German-to-English We applied the clause restructuring method of Collins et al. [9] for German pre-ordering. The method is mainly based on moving German verbs in the end of clause structures towards the beginning of the clause. We re-implemented the method for German parse trees created using the Berkeley parser trained on TIGER corpus. We ignored some additional syntactic information such as subject markers and heads implemented in the original method of [9], because we used a 1 To give a sense of the domain difference, a 4-gram LM trained with Kneser-Ney smoothing on TED data gives a perplexity of 355 on the general domain data, compared to a perplexity of 99 on held-out T"
2013.iwslt-evaluation.12,E03-1076,0,0.18382,"ional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 hidden layers, 300 classes, and 4 steps of back-propogation through time. 3.4. German compound word splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [11]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). For simplicity, we did not experiment with splitting into three or more sub-words as done in the compound-splitter.perl script distributed with the Moses package. The unigram frequencies for the subwords and whole word is computed from the German part of the bitext. This s"
2013.iwslt-evaluation.12,I11-1153,1,0.737728,"vial to handle recombination of German split words after reordering and translation. To ensure that the uniform hypotheses space gives the same decision as the original loss in the true space p(e|f ), we use a small development set to tune the parameter θ as follows. For any two hypotheses e1 , e2 , and a reference translation er (possibly not in N (f )) we first compute the true loss: L(e1 |er ) and L(e2 |er ). If L(e1 |er ) &lt; L(e2 |er ), then we would want θ such that: K ∑ ∑ 3.5. GMBR system combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [12], which has been successfully applied to different types of SMT systems for patent translation [13]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. θk Lk (e1 |e) &lt; e∈N (f ) k=1 Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ a"
2013.iwslt-evaluation.12,P10-1017,0,0.0236144,"akes the problem amendable to solutions in “learning to rank” literature [15]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/"
2013.iwslt-evaluation.12,D12-1077,1,0.836426,"Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.6. What Didn’t Work Immediately We also tried several other methods that did not have a clear positive effect and were thus omitted from the final system. For example, we attempted to improve alignment accuracy using the discriminative alignment method proposed by [16] training on the 300 hand-aligned sentences.4 However, while this provided small gains in alignment accuracy on a held-out set, the gains were likely not enough, and MT results were inconclusive. We also attempted to use the reordering method of [17] as implemented in lader,5 again trained on the same 300 hand-aligned sentences, but increases in reordering accuracy on a held-out set were minimal. We believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-or"
2013.iwslt-evaluation.12,P13-4016,1,0.84637,"believe that both of these techniques are promising, but require a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of comp"
2013.iwslt-evaluation.12,P07-2045,0,0.0147657,"e a larger set of hand-aligned data to provide gains large enough to appear in MT results. 4 http://user.phil-fak.uni-duesseldorf.de/ tosch/ ˜ downloads.html 5 http://phontron.com/lader 4. Experiments 4.1. Setup 4.1.1. System overview We used three individual SMT systems for each language pairs: forest-to-string (F2S), hierarchical phrase-based (Hiero), and phrase-based with pre-ordering (Preorder). In some of our comparisons we also use simple phrase-based translation without preordering (PBMT). F2S was implemented with Travatar [18] and Preorder, PBMT, and Hiero were implemented using Moses [19]. For the Moses models, we generally used the default settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All s"
2013.iwslt-evaluation.12,P02-1040,0,0.0871245,"t settings, but with Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training dat"
2013.iwslt-evaluation.12,D10-1092,1,0.836115,"Good-Turing phrase table smoothing. For F2S translation we used Egret6 as a parser, and created forests using dynamic pruning including all edges that occurred in the 100-best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank7 for English, and TIGER corpus [20] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences"
2013.iwslt-evaluation.12,W04-3250,0,0.0816664,"lt settings for Travatar were used, with the exception of changing the number of composed rules to 6 and using Kneser-Ney rule table smoothing. All systems were evaluated using the standard BLEU score [21] and also RIBES [22], a metric designed specifically to show whether reordering is being performed properly. All systems were optimized towards BLEU score. We measure statistical significance between results with bootstrap resampling with p &gt; 0.05. Bold numbers in each table indicate the best system, and all systems that do not show a statistically significant difference from the best system [23]. All words were lowercased prior to translation, and finally recased by a SMT-based recaser as implemented in Moses. 4.1.2. Translation models We trained the translation models using WIT3 training data (138,499 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in 3.1. 4.1.3. Language models We used two types of word n-gram language models of German and English: interpolated 6-gram and Google 5-gram. The interpolated 6-gram LMs were from linear interpolation of several 6-gram LMs on different data sources (WIT3"
2013.iwslt-evaluation.12,P12-1001,1,0.853229,"at the F2S system did a significantly better job of accurately generating verbs at the end of the German sentence, demonstrating its superior capability for reordering. For German-English, on the other hand, F2S achieved a somewhat counter-intuitive low score on the reordering-based measure RIBES. Upon an analysis of the results, we found that the F2S system was largely getting the reordering right, but occasionally making big changes in reordering large clauses that were not reflected in the German reference. It is likely that if we optimized towards RIBES, or a combination of BLEU and RIBES [25] we might get better results. 4.4. Translation Method Comparison 4.5. Effect of Compound Splitting In this section, we provide a brief comparison of the three translation methods mentioned in Section 2 on tst2010 data. For all systems we used the TED data and 1M selected sentences for training, and used the language model described Next, we examine the effect of compound splitting for German-English translation. From the results in Table 7, we can see that compound splitting provides a gain for all systems, and particularly so for F2S translation. PBMT Hiero F2S en-de n-gram +RNNLM 23.11 23.81"
2013.iwslt-evaluation.12,federico-etal-2012-iwslt,0,\N,Missing
2014.amta-researchers.18,W08-0336,0,0.177985,"ey reported the improvement in word segmentation, and did not report its effect on the patent MT. Their work can be seen an application of a semisupervised learning method (Sun and Xu, 2011) to the domain adaptation. Such an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the do"
2014.amta-researchers.18,P08-1115,0,0.0286095,"omain adaptation. Such an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliterati"
2014.amta-researchers.18,J04-1004,0,0.0347997,"ses B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Segmentation Adaptation using Accessor Variety Sun and Xu (2011) and Guo et al. (2012) used Accessor Variety (AV) (Feng et al., 2004) derived from unlabeled corpora as word segmentation features. AV is a word extraction criterion from un-segmented corpora, focusing on the number of distinct characters appearing around a string. The AV of a string xn is defined as AV (xn ) = min {AVL (xn ), AVR (xn )} , where AVL (xn ) is the left AV (the number of distinct predecessor characters) and AVR (xn ) is the right AV (the number of distinct successor characters). The AV-based word extraction is based on an intuitive assumption; a word appears in many different context so that there is a large variation of its accessor characters. I"
2014.amta-researchers.18,N04-1035,0,0.0159661,"© The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentat"
2014.amta-researchers.18,P06-1085,0,0.0412818,"s Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and X"
2014.amta-researchers.18,I13-1147,1,0.845609,"translation experiments. 2 Related Work The patent MT between Japanese and English has been studied actively on shared tasks in NTCIR (Fujii et al., 2008, 2010; Goto et al., 2011, 2013). Recent important achievements in these studies are on the reordering problem especially in English-to-Japanese direction. Isozaki et al. (2010) proposed a very simple but effective rule-based syntactic pre-ordering method called Head Finalization. It is very effective for the long distance reordering. On the other hand, the Japanese-to-English direction is more difficult due to the lack of such simple rules. Hoshino et al. (2013) proposed an effective rule-based syntactic pre-ordering based on predicate-argument structures. Sudoh et al. (2013b) proposed a different approach called postordering for the Japanese-to-English patent MT, and achieved high translation performance by an efficient syntax-based translation. Our system uses the latter approach based on English syntax rather than the former one based on Japanese syntax. This is because our word segmentation adaptation can be applied directly to it without the Japanese parser adaptation as described earlier. General-purpose Japanese parsers do not work well in the"
2014.amta-researchers.18,W10-1736,1,0.907546,"erence from general-purpose MT. This work focuses on statistical MT (SMT) for patents, from Japanese to English. It is more difficult than English-to-Japanese in: 1) long distance reordering, and 2) word segmentation and lexical translation of domain-specific terms. The first problem is due to large syntactic differences between Japanese and English. Although the reordering in the English-to-Japanese direction can be solved effectively by very simple heuristics called Head Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), wh"
2014.amta-researchers.18,P06-2056,0,0.0397128,"Missing"
2014.amta-researchers.18,W99-0702,0,0.0703583,"ings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a w"
2014.amta-researchers.18,W04-3250,0,0.0288226,"and the large-scale unlabeled patent corpus with the BE and PD features • KyTea, MeCab, and JUMAN10 : publicly available Japanese morphological analyzers We also compared the results by the post-ordering with those by standard SAMT and PBMT. The search space parameters of the standard SAMT were set to the same value as the HFE-toEnglish SAMT, to compare the performance with similar computation time11 . 5.2.3 Results and Discussion Table 4 shows the translation performance in BLEU and TER (Snover et al., 2006) with the results of statistical significance tests (p=0.05) by bootstrap resampling (Koehn, 2004), in which our overall system resulted in the best. The table also shows the results of intermediate Japanese-to-HFE translation. The advantage of our system can be attributed to three techniques included in the system: domain adaption of word segmentation, katakana unknown word transliteration, and post-ordering. 9 It exceeded the maximum sentence length in the development and test sets. 10 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 11 Actually the post-ordering needs the time for the first monotone PBMT but it ran very fast and did not affect so much (Sudoh et al., 2013b). Al-Onaizan"
2014.amta-researchers.18,P11-2093,1,0.789822,"e monolingual corpora. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ f"
2014.amta-researchers.18,P03-1021,0,0.0420166,"implemented with Moses-chart and trained using the HFE sentences and the corresponding English parse trees. Its reordering parameter max-chart-span was set to 200 to allow arbitrary distance reordering for accurate Japanse-toEnglish translation9 . The search space parameter cube-pruning-pop-limit was set to 32 for efficiency, according to Sudoh et al. (2013b). Their language models were word 6-gram models trained using a large-scale English patent corpus with more than 300 million sentences. Model weights were optimized in BLEU (Papineni et al., 2002) using Minimum Error Rate Training (MERT) (Och, 2003). We chose the best weights among ten individual runs of MERT. The katakana transliteration was implemented as a Moses-based monotone PBMT in the character level, trained using transliteration pairs mined from the Japanese-English phrase table entries whose Japanese part consisted of katakana only. Its character-level language model was character 9-gram models trained using the large-scale English patent corpus which is used for the word-level language models described above. It was used to replace katakana words remained in the intermediate results in HFE with their transliteration results. 5"
2014.amta-researchers.18,P02-1040,0,0.0899612,"strain adjacent phrase translations. The HFE-to-English SAMT was implemented with Moses-chart and trained using the HFE sentences and the corresponding English parse trees. Its reordering parameter max-chart-span was set to 200 to allow arbitrary distance reordering for accurate Japanse-toEnglish translation9 . The search space parameter cube-pruning-pop-limit was set to 32 for efficiency, according to Sudoh et al. (2013b). Their language models were word 6-gram models trained using a large-scale English patent corpus with more than 300 million sentences. Model weights were optimized in BLEU (Papineni et al., 2002) using Minimum Error Rate Training (MERT) (Och, 2003). We chose the best weights among ten individual runs of MERT. The katakana transliteration was implemented as a Moses-based monotone PBMT in the character level, trained using transliteration pairs mined from the Japanese-English phrase table entries whose Japanese part consisted of katakana only. Its character-level language model was character 9-gram models trained using the large-scale English patent corpus which is used for the word-level language models described above. It was used to replace katakana words remained in the intermediate"
2014.amta-researchers.18,C04-1081,0,0.0601299,"al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Seg"
2014.amta-researchers.18,P12-1049,0,0.166158,"nsidering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features derived from a large-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical tr"
2014.amta-researchers.18,2006.amta-papers.25,0,0.0160653,"tation experiments above • Proposed: the patent-adapted segmenter using the labeled general-domain corpus and the large-scale unlabeled patent corpus with the BE and PD features • KyTea, MeCab, and JUMAN10 : publicly available Japanese morphological analyzers We also compared the results by the post-ordering with those by standard SAMT and PBMT. The search space parameters of the standard SAMT were set to the same value as the HFE-toEnglish SAMT, to compare the performance with similar computation time11 . 5.2.3 Results and Discussion Table 4 shows the translation performance in BLEU and TER (Snover et al., 2006) with the results of statistical significance tests (p=0.05) by bootstrap resampling (Koehn, 2004), in which our overall system resulted in the best. The table also shows the results of intermediate Japanese-to-HFE translation. The advantage of our system can be attributed to three techniques included in the system: domain adaption of word segmentation, katakana unknown word transliteration, and post-ordering. 9 It exceeded the maximum sentence length in the development and test sets. 10 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN 11 Actually the post-ordering needs the time for the firs"
2014.amta-researchers.18,D13-1021,1,0.223791,"ms is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features derived from a large-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical translation step and t"
2014.amta-researchers.18,D11-1090,0,0.0334295,"Missing"
2014.amta-researchers.18,I05-3027,0,0.0216655,"are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In this work we employ four classes B, M, E (beginning/middle/end of a word), and S (single-character word)2 , as Sun and Xu (2011). Our baseline features follow the work of Japanese word segmentation by Neubig et al. (2011): label bigrams, character n-grams (n=1, 2), and character type n-grams (n=1, 2, 3). We use the n-gram features within [i-2, i+2] for classifying the word at the position i. The character types are kanji, katakana, hiragana, digits, roman characters, and others. 4.2 Conventional Method: Word Segmentation Adaptation"
2014.amta-researchers.18,C08-1113,1,0.738883,"are trained using the monolingual corpora. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 237 4 Domain Adaptation of Japanese Word Segmentation for Patents We aim to improve the Japanese-to-English translation performance further by the word segmentation adaptation for patent-specific words and technical terms. We use the large-scale monolingual Japanese patent corpora for the domain adaptation, by the semi-supervised approach as Sun and Xu (2011) and Guo et al. (2012). There are also active learning-based supervised domain adaptation (Tsuboi et al., 2008; Neubig et al., 2011) and unsupervised word segmentation (Kempe, 1999; Kubota Ando and Lee, 2003; Goldwater et al., 2006, and many others) approaches, but the semi-supervised approach is expected to be effective; the active learning method is not easy to utilize for such large-scale corpora and the unsupervised method is not so accurate as existing supervised word segmenters. 4.1 Baseline Word Segmentation based on Conditional Random Fields We use a character-based word segmenter based on CRFs (Peng et al., 2004; Tseng et al., 2005). It solves a character-based sequential labeling problem. In"
2014.amta-researchers.18,2007.mtsummit-papers.63,0,0.0507407,"s an extended transliteration mining method for Japanese compound words (Sudoh et al., 2013a). 3 System Overview Our Japanese-to-English patent SMT is based on large-scale language resources in the patent domain. This work uses NTCIR PatentMT dataset (Goto et al., 2011, 2013) including a Japanese-English parallel corpus of 3.2 million sentences and monolingual corpora of more than 300 million sentences of Japanese and English. The parallel corpus was developed by an automatic sentence alignment over patent documents in the Japan Patent Office and the United States Patent and Trademark Office (Utiyama and Isahara, 2007). The workflow of our SMT system is illustrated in Figure 1. The translation is divided into the following four processes. 1. Japanese word segmentation using a patent-adapted word segmentation model 2. Translation into an intermediate language, Head Final English (HFE), by a monotone phrase-based SMT 3. Transliteration of untranslated Japanese katakana words (i.e. unknown words in the previous process) into English words, by a monotone phrase-based SMT in the character level 4. Post-ordering into English by a syntax-based SMT Here, HFE is Japanese-ordered English, which was proposed by Isozak"
2014.amta-researchers.18,C04-1073,0,0.0524449,"arge-scale patent corpus. We also incorporate machine transliteration for the unknown Japanese words written in katakana (Japanese phonograms), bootstrapped from the parallel corpus (Sajjad et al., 2012; Sudoh et al., 2013a). Our SMT system integrates these techniques with a post-ordering framework (Sudoh et al., 2013b), which divides the SMT problem explicitly into two sub-problems of the lexical translation and the reordering. In the post-ordering framework, the lexical translation precedes the reordering, different from pre-ordering in which the reordering precedes the lexical translation (Xia and McCord, 2004; Isozaki et al., 2010). An advantage of the post-ordering is that it is easy to integrate the domain-adapted word segmentation and the unknown word transliteration in its lexical translation step and that the reordering can use the improved lexical translation results. If we are to do the same thing in the pre-ordering, we need domain adaptation of its Japanese syntactic parser in addition to the word segmenter, and have to integrate the transliteration process with the SMT decoder as Durrani et al. (2014). Our system shows better translation accuracy in BLEU and TER than baseline methods in"
2014.amta-researchers.18,C08-1128,0,0.022744,"ir domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliteration pairs for training. Sajjad et al. (2012) proposed an unsupervised transliteration mining from standard parallel corpora for bootstrapping machin"
2014.amta-researchers.18,P02-1039,0,0.0604915,"Researchers Vancouver, BC © The Authors 234 Finalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method f"
2014.amta-researchers.18,W08-0335,0,0.0231886,"an approach is appropriate for the patent domain where a huge number of patent documents are publicly available. We extend their domain adaptation by more effective and easy-to-use features, and also incorporate additional Japanese-oriented features to improve the Japanese word segmentation in the patent domain. With respect to the relation between word segmentation and SMT, Chang et al. (2008) reported consistency and granularity of word segmentation is important in Chinese-to-English MT and modified their Chinese word segmenter to optimize the translation performance. Dyer et al. (2008) and Zhang et al. (2008) used multiple word segmentation results to overcome the problem of different word segmentation standards. Xu et al. (2008) optimized Chinese word segmentation for Chinese-to-English SMT using an extended Bayesian word segmentation method with bilingual correspondence. These studies aim to optimize word segmentation using bilingual correspondence and are different from the domain adaptation. Machine transliteration is an important problem for translating names and other imported words (Knight and Graehl, 1998). Conventional methods need to prepare parallel transliteration pairs for training. S"
2014.amta-researchers.18,Y06-1012,0,0.0222236,"proportional to the corpus size in general. Previous studies use several frequency classes with corresponding threshold values tuned according to the corpus, but it is not straightforward to determine appropriate classes and threshold values. Sun and Xu (2011) used the following features based on the left and right AVs of character n-grams for classifying xi , which imply word boundaries around xi , as illustrated in Figure 4. • Left AV of n-gram starting from xi : AVL (xi , ..., xi+n−1 ) 2 Guo et al. (2012) used six classes including B2, B3 (second and third character in a word) proposed by Zhao et al. (2006) for Chinese word segmentation. This paper uses the four classes, because the six classes did not improve the word segmentation accuracy in our pilot test. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 238 と前記複数の の前後方向で と前記ノード 直前に、その と前後で交差 predecessors: と: 3 の: 1 直: 1 Left AV of 前: 3 Right AV of 前: 3 Left BE of 前: 3 3 log 5 5 successors: 記: 2 後: 2 に: 1 2⇥ 1 1 log = 1.057 5 5 Right BE of 前: 2⇥ 2 3 log 5 5 1 1 log = 1.308 5 5 Figure 3: Example of accessor variety (AV) and branching entropy (BE) for a character “前”. character to be classi"
2014.amta-researchers.18,W06-3119,0,0.01628,"nalization (Isozaki et al., 2010), the reordering in Japanese-to-English is not so straightforward. The second problem results from the Japanese orthography in which there are no explicit word boundaries. General-purpose word segmenters often fail to segment the domain-specific words and those words are translated incorrectly or remain untranslated. Some domain-specific words cannot be translated as unknown words even if they are segmented correctly, due to limited SMT training data. The first problem has been addressed by a syntax-based approach (Yamada and Knight, 2002; Galley et al., 2004; Zollmann and Venugopal, 2006), while most previous studies did not deal with the second problem. Since the lexical translation also affects the reordering based on a lexicalized reordering model and an n-gram language model, considering both problems is important for an overall SMT system. The goal of this work is to improve the Japanese-to-English patent SMT by tackling both problems at the same time. The domain-specific words have important roles in the patents and should be translated carefully for meaningful translations. We propose a novel domain adaptation method for the word segmentation, using effective features d"
2014.amta-researchers.18,E14-4029,0,\N,Missing
2014.amta-researchers.18,J98-4003,0,\N,Missing
2014.iwslt-evaluation.18,P08-1023,0,0.0240241,"in the F2S system, and compare it to the more traditional method of cube pruning. The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven me"
2014.iwslt-evaluation.18,P06-1077,0,0.0368401,"The second is that this year we attempted to extract pre-ordering rules automatically from parallel corpora, as opposed to hand-designing preordering rules based on linguistic intuition. This paper presents details of our systems and reports the official results together with some detailed discussions on contributions of the techniques involved. 2.1. Forest-to-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT"
2014.iwslt-evaluation.18,W14-7002,1,0.835139,"o-String Machine Translation In our previous year’s submission to IWSLT, we achieved promising results using the forest-to-string machine translation (F2S; [3]) framework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of p"
2014.iwslt-evaluation.18,P14-2024,1,0.898714,"ework. F2S is a generalization of treeto-string machine translation (T2S; [4]) that performs translation by first syntactically parsing the source sentence, then translating from sub-structures of a packed forest of potential parses to a string in the target language. We have previously found that F2S produces highly competitive results for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hyperg"
2014.iwslt-evaluation.18,J07-2003,0,0.082081,"for language pairs with large divergence in syntax such as Japanese-English or Japanese-Chinese [5]. However, we have also found that there are several elements that must be appropriately handled to achieve high translation accuracy using syntax-driven methods [6], one of which is search. In the F2S component of our submission to IWSLT this year, we experimented with two different search algorithms to measure the effect that search has on the GermanEnglish and English-German pairs. As the first algorithm, we use the standard method for search in tree-based methods of translation: cube pruning [7]. For each edge to be expanded, cube pruning sorts the child hypotheses in descending order of probability, and at every step pops the highest-scoring hypothesis off the stack, calculates its language model scores, and adds the popped, scored edge to the hypergraph. It should be noted that the LM scores are not calculated until after the edge is popped, and thus the order of visiting edges is based on only an LMfree approximation of the true edge score, resulting in search errors. In our F2S system this year, we test a new method of hypergraph search [2], which aims to achieve better search ac"
2014.iwslt-evaluation.18,P05-1066,0,0.0488395,"Lake Tahoe, December 4th and 5th, 2014 ally, refining the probability estimates until the limit on number of stack pops is reached. In our previous work [6] we have found that hypergraph search achieved superior results to cube pruning, and we hypothesize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (d"
2014.iwslt-evaluation.18,C04-1073,0,0.0293599,"esize that these results will carry over to German-English and English-German as well. 2.2. Syntax-based Pre-ordering Pre-ordering is a method that attempts to first reorder the source sentence into a word order that is closer to the target, then translate using a standard method such as PBMT. We used hand-crafted German-English pre-ordering rules [8] in our submission last year. This year’s system uses an automatic method to extract domain-dependent pre-ordering rules, avoiding the time-consuming effort required for creating hand-crafted rules. The pre-ordering method is basically similar to [9], but is limited to reordering of child nodes in syntactic parse trees rather than rewriting and word insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation"
2014.iwslt-evaluation.18,N04-1035,0,0.060307,"rd insertion. Since the pre-ordering does not work perfectly in all cases, we allow for further reordering in the PBMT system that translates the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-order"
2014.iwslt-evaluation.18,D07-1078,0,0.0251997,"es the preordered sentences. The reordering limit of this system is chosen experimentally using held-out data (dev. set BLEU in this paper). 2.2.1. Reordering Pattern Extraction A reordering pattern represents a reordering of child nodes in a source language parse tree, determined by word alignment. The reordering pattern is similar to a tree-based translation pattern called frontier graph fragments, which form the most basic unit in tree-based translation [10], but only holds reordering information on the non-terminal child nodes. A reordering pattern can be extracted from an admissible node [11] in the parse tree that covers a distinct contiguous spans in the corresponding target language sentences. Since such a reordering pattern only is constrained by the syntactic labels on the parent and child nodes, we consider several attributes of reordering patterns: syntactic labels of its grand-parent, left and right siblings of the parent, and surface forms of its child nodes (only when the child is a part-of-speech node). 2.2.2. Deterministic Pre-ordering In order to make the pre-ordering deterministic, we use reordering rules from dominant reordering patterns that agree with more than 75"
2014.iwslt-evaluation.18,N03-1017,0,0.058736,"Missing"
2014.iwslt-evaluation.18,P13-2119,1,0.840496,"icularly for languages with similar syntactic structure. 3. Additional System Enhancements Here we review techniques that were used in our submission last year [1] and also describe some of our new attempts that were not effective in our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM."
2014.iwslt-evaluation.18,D11-1033,0,0.0295922,"our pilot test and not included in the final system. 3.1. Training Data Selection The target TED domain is different in both style and vocabulary from many of the other bitexts, e.g. Europarl, CommonCrawl (which we collectively call “general-domain” data). To address this domain adaption problem, we performed adaptation training data selection using the method of [13].1 The intuition is to select general-domain sentences that are similar to in-domain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an general-domain sentence pair (e, f ) as [14]: [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized cross-entropy of e on the English in-domain LM. GENE (e) is the lengthnormalized cross-entropy of e on the English general-domain LM, which is built from a sub-sample of the general-domain text. Similarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural"
2014.iwslt-evaluation.18,E03-1076,0,0.0546193,"milarly, INF (f ) and GENF (f ) are the crossentropies of f on Foreign-side LM. Finally, sentence pairs are ranked according to Eq. 1 and those with scores lower than some empirically-chosen threshold are added together with the in-domain bitext for translation model training. Here, the LMs are Recurrent Neural Network Language Models (RNNLMs), which have been shown to outperform n-gram LMs in this problem [13]. 3.2. German Compound Word Splitting German compound words present sparsity challenges for machine translation. To address this, we split German words following the general approach of [15]. The idea is to split a word if the geometric average of its subword frequencies is larger than whole word frequency. In our implementation, for each word, we searched for all possible decompositions into two sub-words, considering the possibility of deleting common German fillers “e”, “es”, and “s” (as in ”Arbeit+s+tier”). The unigram frequencies for the subwords and 1 Code/scripts available at http://cl.naist.jp/∼kevinduh/a/acl2013 128 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 whole word is computed from the German p"
2014.iwslt-evaluation.18,I11-1153,1,0.863598,"e the RNNLM log probabilities and add them as an additional feature to each translation hypothesis. We then re-run a single MERT optimization to find ideal weights for this new feature, and then extract the 1-best result from the 10,000-best list for the test set according to these new weights. The parameters for RNNLM training are tuned on the dev set to maximize perplexity, resulting in 300 nodes in the hidden layer, 300 classes, and 4 steps of back-propagation through time. 3.4. GMBR System Combination We used a system combination method based on Generalized Minimum Bayes Risk optimization [17], which has been successfully applied to different types of SMT systems for patent translation [18]. Note that our system combination only picks one hypothesis from an N-best list and does not generate a new hypothesis by mixing partial hypotheses among the N-best. 3.4.1. Theory Minimum Bayes Risk (MBR) is a decision rule to choose hypotheses that minimize the expected loss. In the task of SMT from a French sentence (f ) to an English sentence (e), the MBR decision rule on δ(f ) → e′ with the loss function L over the possible space of sentence pairs (p(e, f )) is denoted as: ∑ L(δ(f )|e)p(e|f"
2014.iwslt-evaluation.18,2011.mtsummit-papers.36,1,0.801283,"e improvements from the baseline 1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Jou"
2014.iwslt-evaluation.18,D13-1139,1,0.77246,"1-best in our pilot test, but they were much smaller than those resulting from RNNLM, and when the NNJM was combined with RNNLM we saw no significant gains. One possible reason is the small training data size; the model is very sparse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treeban"
2014.iwslt-evaluation.18,P13-4016,1,0.849598,"parse and needs large training data because of its large contexts of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S m"
2014.iwslt-evaluation.18,P07-2045,0,0.00853439,"s of fourteen (eleven source and three target) words. The affiliation is very important to predict the target word correctly but it was determined by automatic word alignment (such as GIZA++) and may not always be good enough in our experiments. We also tried post-ordering [22] by shift-reduce reordering [23] for German-to-English. It was not effective in our pilot test even in the first-pass lexical translation, probably due to less effective English-to-German pre-ordering rules. 4. Experiments F2S was implemented with Travatar3 [24] and the phrasebased MT systems were implemented with Moses [25]. For the Travatar rule tables, we used a modified version of Egret4 as a syntactic parser, and created forests using dynamic pruning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method"
2014.iwslt-evaluation.18,W06-1607,0,0.0253342,"uning including all edges that occurred in the 100best hypotheses. We trained the parsing model using the Berkeley parser over the Wall Street Journal section of the Penn Treebank5 for English, and TIGER corpus [26] for German. For model training, the default settings for Travatar were used, with the exception of changing the number of composed rules to 6 with Kneser-Ney smoothing. For search in the F2S models, we used the previously described hypergraph search method. For the Moses phrase tables, we used standard training settings with Kneser-Ney smoothing of phrase translation probabilities [27]. 4.1.2. Translation Models We trained the translation models using WIT3 training data (178,526 sentences) and 1,000,000 sentences selected over other bitexts (Europarl, News Commentary, and Common Crawl) by the method described in Section 3.1. 4.1.3. Language Models We used word 5-gram language models of German and English that were linearly interpolated from several word 5-gram language models trained on different data sources (WIT3 , Europarl, News Commentary, and Common Crawl). The interpolation weights were optimized to minimize perplexity on the development set, using interpolate-lm.perl"
2014.iwslt-evaluation.18,P11-1132,0,0.0600041,"Missing"
2014.iwslt-evaluation.18,P14-1129,0,0.0304397,"where a difference exists in the true loss. Then we optimize θ in a formulation similar to a Ranking SVM [19]. The pair-wise nature of Eqs. 6 and 7 makes the problem amendable to solutions in “learning to rank” literature [20]. We used BLEU as the objective function and the subcomponents of BLEU as features (system identity feature was not used). There is one regularization hyperparameter for the Ranking SVM, which we set by cross-validation over the development set (dev2010). 3.5. What Didn’t Work Immediately This year we tried to include a state-of-the-art Neural Network Joint Model (NNJM) [21] to improve the accuracy of translation probability estimation. The model is used to predict a target language word using its three preceding target language words and eleven source language words surrounding its affiliation (the non-NULL source language word aligned to the target language word to be predicted). We used top 16,000 source and target vocabularies in the model and mapped the other words into a single OOV symbol, while the original paper[21] used part-of-speech classes. Although the original paper presented a method for integrating the model with decoding, we used the NNJM for rer"
2014.iwslt-evaluation.18,N13-1116,0,\N,Missing
2014.iwslt-evaluation.18,2013.iwslt-evaluation.12,1,\N,Missing
2020.acl-main.327,W05-0909,0,0.440604,"stitute human evaluation in machine translation development because it is low-cost, handy, and stable to use. Popular automatic MTE metrics such as BLEU (Papineni et al., 2002) calculate the evaluation score based on a surface-level similarity of a paired 1-to-1 reference and translated hypothesis sentences. BLEU particularly evaluates the sentence similarity with the ngram word matching rate between a reference and hypothesis. However, the evaluation score drops when a reference and hypothesis are dissimilar in the surface even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al."
2020.acl-main.327,W17-4755,0,0.173914,"l, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as"
2020.acl-main.327,D18-2029,0,0.0753115,"Missing"
2020.acl-main.327,D17-1070,0,0.0508202,"et al. (2018) proposed an MTE framework called RUSE (Regressor Using Sentence Embeddings), which uses sentence3553 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3553–3558 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Table 1: Available corpus size annotated with human judgments in WMT-2017 Metrics Shared Task (to-English) WMT-2015 WMT-2016 WMT-2017 ALL cs-en 500 560 560 1620 de-en 500 560 560 1620 fi-en 500 560 560 1620 lv-en 560 560 ro-en 560 560 level embeddings obtained by a large-scale pretrained model like InferSent (Conneau et al., 2017), Quick Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018). Its regressor takes sentence vectors for a reference and translation hypothesis as inputs and returns a score, which is trained to correlate well with human evaluation (Graham et al., 2015). RUSE achieved the best correlation score with human judgments in the WMT-2017 Metrics Shared Task (Bojar et al., 2017). BERT regressor (Shimanaka et al., 2019) is a simple MTE metric based on BERT (Devlin et al., 2019) encoder. It is composed of BERT encoder and a multi-layer perceptron (MLP) regressor attached t"
2020.acl-main.327,P02-1040,0,0.111256,"at our proposed method using Crosslingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance. 1 Introduction Automatic machine translation evaluation (MTE) has been studied to substitute human evaluation in machine translation development because it is low-cost, handy, and stable to use. Popular automatic MTE metrics such as BLEU (Papineni et al., 2002) calculate the evaluation score based on a surface-level similarity of a paired 1-to-1 reference and translated hypothesis sentences. BLEU particularly evaluates the sentence similarity with the ngram word matching rate between a reference and hypothesis. However, the evaluation score drops when a reference and hypothesis are dissimilar in the surface even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity"
2020.acl-main.327,W15-4915,0,0.35177,"E, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as another pseudo reference, since the source and reference sentences should be semantically equivalent. The proposed method uses Cross-lingual Language Model (XLM) (Lample and Conneau, 2019) to handle source and target languages in a shared sentence embedding space. The proposed method with XLM trained with a translation language modeling (TLM) objective showed a higher correlation with human judgments than a baseline method using hypothesis and reference sentences. 2 Relate"
2020.acl-main.327,D18-1269,0,0.022276,"o encode both source and target language sentences into an embedding vector. XLM has three additional techniques to BERT: language independent subword based on Byte Pair Encoding (Sennrich et al., 2016), a language embedding layer, and a translation language modeling (TLM) objective that predicts masked words from surrounding words or a paired translation. The brief architecture of XLM is shown in Figure 1. (Lample and Conneau, 2019) reported that XLM trained with TLM objective obtains better performance than multilingual BERT(Devlin et al., 2019) on the XNLI cross-lingual classification task(Conneau et al., 2018). The experiments were conducted with a corpus of all language pairs to English translation from segment-level WMT2017 Metrics Shared Task (Bojar et al., 2017). We split sentences in WMT15 and WMT16 to training and development data with the ratio of 9:1 and whole sentences in WMT17 are used for evaluation of MTE methods. The corpus size for each language pair is shown in Table 1. We used two different models from all available XLM family models2 : XLM15 pretrained by MLM and TLM, and XLM100 pretrained only by MLM. XLM15 is expected to perform better by the paired bilingual training of TLM, but"
2020.acl-main.327,P16-1162,0,0.0127871,"arned to represent the quality of the translation hypothesis given two correct sentences aligned aside. 4 Experiments We conducted experiments to evaluate the performance of the proposed method in MTE by comparing with some existing methods. 4.1 Setting We propose an MTE method using source language sentences as additional pseudo references. We use cross-lingual language models called XLM (Lample and Conneau, 2019) to encode both source and target language sentences into an embedding vector. XLM has three additional techniques to BERT: language independent subword based on Byte Pair Encoding (Sennrich et al., 2016), a language embedding layer, and a translation language modeling (TLM) objective that predicts masked words from surrounding words or a paired translation. The brief architecture of XLM is shown in Figure 1. (Lample and Conneau, 2019) reported that XLM trained with TLM objective obtains better performance than multilingual BERT(Devlin et al., 2019) on the XNLI cross-lingual classification task(Conneau et al., 2018). The experiments were conducted with a corpus of all language pairs to English translation from segment-level WMT2017 Metrics Shared Task (Bojar et al., 2017). We split sentences i"
2020.acl-main.327,N19-1423,0,0.532221,"ning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2"
2020.acl-main.327,W18-6456,0,0.350446,"ce even if they share the same meaning. To counter this problem, METEOR (Banerjee and Lavie, 2005) is proposed to mitigate the word matching of synonyms with a synonym dictionary. Yet still, with mitigation of word matching, surfacelevel similarity cannot fully compensate for semantics, thus word representation instead of word symbols is used in Word Mover’s Distance (Kusner et al., 2015) and bleu2vec (T¨attar and Fishel, 2017). Besides, sentence representation is known to be an efficient feature instead of word representation because sentence vectors can represent more global meanings. RUSE (Shimanaka et al., 2018) and BERT (Devlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations a"
2020.acl-main.327,N12-1017,0,0.0185447,"evlin et al., 2019) based MTE, BERT regressor (Shimanaka et al., 2019), utilized sentence representation and performed well on WMT17 Metric Shared Task (Bojar et al., 2017). The metrics mentioned above compare a hypothesis translation to a reference. However, a reference translation represents only one possible translation and those MTE metrics are unlikely to correctly evaluate all candidates that share the same meanings of the reference or have fatally different meanings due to a few translation errors. This problem can be mitigated by the use of multiple reference translations as argued by Dreyer and Marcu (2012) and Qin and Specia (2015), but preparing such multiple references is costly. Hereby, we propose a method to incorporate source sentence into MTE as another pseudo reference, since the source and reference sentences should be semantically equivalent. The proposed method uses Cross-lingual Language Model (XLM) (Lample and Conneau, 2019) to handle source and target languages in a shared sentence embedding space. The proposed method with XLM trained with a translation language modeling (TLM) objective showed a higher correlation with human judgments than a baseline method using hypothesis and ref"
2020.acl-main.327,N15-1124,0,0.0273411,"ics Table 1: Available corpus size annotated with human judgments in WMT-2017 Metrics Shared Task (to-English) WMT-2015 WMT-2016 WMT-2017 ALL cs-en 500 560 560 1620 de-en 500 560 560 1620 fi-en 500 560 560 1620 lv-en 560 560 ro-en 560 560 level embeddings obtained by a large-scale pretrained model like InferSent (Conneau et al., 2017), Quick Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018). Its regressor takes sentence vectors for a reference and translation hypothesis as inputs and returns a score, which is trained to correlate well with human evaluation (Graham et al., 2015). RUSE achieved the best correlation score with human judgments in the WMT-2017 Metrics Shared Task (Bojar et al., 2017). BERT regressor (Shimanaka et al., 2019) is a simple MTE metric based on BERT (Devlin et al., 2019) encoder. It is composed of BERT encoder and a multi-layer perceptron (MLP) regressor attached to the last layer of BERT. This BERT encoder is a 12 layers bi-directional language model, referring to BERTbase (uncased)1 , trained with masked language model (MLM) and next sentence prediction (NSP). BERT regressor surpassed RUSE on the WMT-2017 data. 3 Proposed method: Automatic e"
2020.acl-main.327,W17-4771,0,0.0354707,"Missing"
2020.acl-srw.8,Q17-1010,0,0.0505016,"explicit knowledge of the given words in its prediction. The contribution of this work is two-fold: (1) We propose a word attribute transfer method that obtains a vector with an inverted binary attribute without explicit knowledge. (2) The proposed method demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into quee"
2020.acl-srw.8,P19-1160,0,0.0274677,"Missing"
2020.acl-srw.8,P19-1601,0,0.0149893,"6.9 29.5 0.0 26.7 29.7 0.0 33.5 36.7 0.0 25.7 36.6 100.0 100.0 0.1 100.0 100.0 0.5 100.0 100.0 1.2 100.0 100.0 4.6 5.5 The theory of analogic relations in word embeddings has been widely discussed (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019; Linzen, 2016). In our work, we focus on the analogic relations in a word embedding space and propose a novel framework to obtain a word vector with inverted attributes. The style transfer task (Niu et al., 2018; Prabhumoye et al., 2018; Logeswaran et al., 2018; Jain et al., 2019; Dai et al., 2019; Lample et al., 2019) resembles ours. In style transfer, the text style of the input sentences is changed. For instance, Jain et al. (2019) transferred from formal to informal sentences. These style transfer tasks use sentence pairs; our word attribute transfer task uses word pairs. Style transfer changes sentence styles, but our task changes the word attributes. Soricut and Och (2015) studied morphological transformation based on character information. Our work aims for more general attribute transfer, such as gender transfer and antonym, and is not limited to morphological transformation. S"
2020.acl-srw.8,P19-1315,0,0.0699055,"hat have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel 2 Word Attribute Transfer Task In this task, we focus on modeling t"
2020.acl-srw.8,W14-1618,0,0.437667,"ethod demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel"
2020.acl-srw.8,P17-1007,0,0.0651237,"e transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can be used to transfer a certain attribute of a word, such as changing king into queen by transferring its gender. This transfer can be applied to perform data augmentation; for example, rewriting He is a boy to She is a girl. It can be used to generate negative examples for natural language inference, for example. We tackle a novel 2 Word Attribute Transfer Task In this tas"
2020.acl-srw.8,N16-2002,0,0.0129596,"r datasets (in number of word pairs) Val X where V is the vocabulary of the word embedding model and cos(vy , vk ) is the cosine similarity meav ·v sure, defined as: cos(vy , vk ) = kvyykkvkk k . We evaluated the performance of the word attribute transfer using data with four different attributes. We used 300-dimensional word2vec and GloVe as the pre-trained word embedding. We used four different datasets of word pairs with four binary attributes: Male-Female, Singular-Plural, CapitalCountry, and Antonym (Table 1). These word pairs were collected from analogy test sets (Mikolov et al., 2013a; Gladkova et al., 2016) and the Internet. Noun antonyms were taken from the literature (Nguyen et al., 2017). For non-attribute dataset N , we sampled words from the vocabulary of word embedding. We sampled from 4 to 50 words for training and 1000 for the test (|Ntest |= 1000). Train |Atest | Stability = Experiment Dataset A 1 M EAN D IFF Analogy-based word attribute trans¯ d ¯ = fer withPa mean difference vector d: 1 (mi ,wi ,z)∈Atrain (vmi −vwi ). We de|Atrain | ¯ to vx termined whether to add or subtract d based on the explicit knowledge (Eq. 4). For proposed methods, we used the Adam optimizer (Kingma and Ba, 20"
2020.acl-srw.8,W16-2503,0,0.0720254,"ain the following formulas: (1) ∀(m, w, z) ∈ A, vm = fz ( fz (vm ) ), (8) ∀(m, w, z) ∈ A, vw = fz ( fz (vw ) ), (9) ∀(u, z) ∈ N , vu = fz ( fz (vu ) ). (10) Hence, the ideal transfer function is a mapping that becomes an identity mapping when we apply it twice for any v. Such a mapping is called involution in geometry. For example, f : v 7→ −v is one example of an involution. 4.2 Analogy is a general idea that can be used for word attribute transfer. PMI-based word embedding, such as SGNS and GloVe, captures analogic relations, including Eq. 2 (Mikolov et al., 2013c; Levy and Goldberg, 2014a; Linzen, 2016). By rearranging Eq. 2, Eq. 3 is obtained: Reflection Reflection Ref a,c is an ideal function because this mapping is an involution: ∀v ∈ Rn , v = Ref a,c ( Ref a,c (v) ). (11) Reflection reverses the location between two vectors in a Euclidean space through an hyperplane called a mirror. Reflection is different from inverse mapping. When m and w are paired words, reflection can transfer vm and vw each other with identical reflection mapping as in Eqs. 5 and 6, but an inverse mapping cannot. Given vector v in Euclidean space Rn , the formula for the reflection in the mirror is given: (2) ≈ vki"
2020.acl-srw.8,N13-1090,0,0.742739,"work for a word attribute transfer based on reflection that does not require explicit knowledge of the given words in its prediction. The contribution of this work is two-fold: (1) We propose a word attribute transfer method that obtains a vector with an inverted binary attribute without explicit knowledge. (2) The proposed method demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes. Introduction Word-embedding methods handle word semantics in natural language processing (Mikolov et al., 2013a,b; Pennington et al., 2014; Vilnis and McCallum, 2015; Bojanowski et al., 2017). Such wordembedding models as skip-gram with negative sampling (SGNS; Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) capture such analogic relations −−→ −→ −−−−−→ −−−→ as king− − man+ woman ≈ queen. Previous work (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019) offers theoretical explanation based on Pointwise Mutual Information (PMI; Church and Hanks, 1990) for maintaining analogic relations in word vectors. These relations can b"
2020.acl-srw.8,D18-1521,0,0.0198579,"= MLPθ1 ([z; vx ]), actress heroine Figure 3: Reflection with parameterized mirrors Reflection with a mirror by Eqs. 13 and 14 assumes a single mirror that only depends on z. Previous discussion assumed pairs that share a stable pair, such as king and queen. However, since gendered words often do not come in pairs, gender is not stable enough to be modeled by a single mirror. For example, although actress is exclusively feminine, actor is clearly neutral in many cases. Thus, actor is not obviously a masculine counterpart like king. In fact, bias exists in gender words in the embedding space (Zhao et al., 2018; Kaneko and Bollegala, 2019). This phenomenon can occur not only with gender attributes but also with other attributes. The single mirror assumption forces the The following property must be satisfied in word attribute transfer: (1) words with attribute z are transferred and (2) words without it are not transferred. Thus, loss L(θ1 , θ2 ) is defined: L(θ1 , θ2 ) = 1 |A| 1 + |N | 53 X (vy − vt )2 (18) (vy − vx )2 , (19) (x,t,z)∈A X (x,z)∈N where Eq. 18 is a term that draws target word vector vti closer to corresponding transferred vector vyi and Eq. 19 is a term that prevents words without a t"
2020.acl-srw.8,C18-1086,0,0.0392076,"Missing"
2020.acl-srw.8,D14-1162,0,0.0779148,"Missing"
2020.acl-srw.8,P18-1080,0,0.0292195,"0.0 100.0 0.4 100.0 99.9 1.0 100.0 99.9 5.2 AN R EF R EF+PM MLP 0.0 26.9 29.5 0.0 26.7 29.7 0.0 33.5 36.7 0.0 25.7 36.6 100.0 100.0 0.1 100.0 100.0 0.5 100.0 100.0 1.2 100.0 100.0 4.6 5.5 The theory of analogic relations in word embeddings has been widely discussed (Levy and Goldberg, 2014b; Arora et al., 2016; Gittens et al., 2017; Ethayarajh et al., 2019; Allen and Hospedales, 2019; Linzen, 2016). In our work, we focus on the analogic relations in a word embedding space and propose a novel framework to obtain a word vector with inverted attributes. The style transfer task (Niu et al., 2018; Prabhumoye et al., 2018; Logeswaran et al., 2018; Jain et al., 2019; Dai et al., 2019; Lample et al., 2019) resembles ours. In style transfer, the text style of the input sentences is changed. For instance, Jain et al. (2019) transferred from formal to informal sentences. These style transfer tasks use sentence pairs; our word attribute transfer task uses word pairs. Style transfer changes sentence styles, but our task changes the word attributes. Soricut and Och (2015) studied morphological transformation based on character information. Our work aims for more general attribute transfer, such as gender transfer and"
2020.acl-srw.8,N15-1186,0,0.0479293,"Missing"
2020.coling-main.234,N03-2003,0,0.0401264,"been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and Rahim, 2006; Yoshino et al., 2013) were widely used as criteria to select appropriate sentences for the training data augmentation. Such a selective approach using large-scale Web data ha"
2020.coling-main.234,P16-2006,0,0.0261972,"dsourcing (J is the number of queries assigned to intent fi ). We calculate similarities between each qi,j and ck , which is a question sentence extracted from the knowledge community website, for finding similar sentence cˆk to qi,j . cˆk which will be assigned as an agumented training sample of fi . Converting sentences to vector representations is an common approach to calculate similarities: vector space model (Salton et al., 1975), means of distributed representation of words (Mikolov et al., 2013; Le and Mikolov, 2014) and bi-directional long short-term memory neural networks (Bi-LSTM) (Cross and Huang, 2016; Yang et al., 2019). Recently, Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is known as a better sentence encoder, which is based on masked word prediction in surrounding sentences. We used the BERT model trained using Japanese Wikipedia (Sakata et al., 2019) on the task of masked word prediction because we would like to extract semantically similar sentences to seed queries. The task of masked word prediction is based on the distributional hypothesis (Harris, 1954); thus, the resultant model trained in the task can embed semantically similar sentences"
2020.coling-main.234,H94-1010,0,0.686349,"ebsite. We investigated the effects of the proposed data augmentation method in SLU task, even with small seed data. In particular, the proposed architecture augmented more than 120,000 samples to improve SLU accuracies. 1 Introduction Recent advances in speech applications running on smartphones and smart speakers increase the importance of spoken language understanding (SLU). SLU is a task to predict an appropriate system function with its arguments, given a user request written or spoken in natural language. Various SLU benchmarks have been proposed: Air Travel Information Services (ATIS) (Dahl et al., 1994), restaurant information navigation (Williams et al., 2014), and other speech applications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation"
2020.coling-main.234,N19-1423,0,0.0173428,"hich is a question sentence extracted from the knowledge community website, for finding similar sentence cˆk to qi,j . cˆk which will be assigned as an agumented training sample of fi . Converting sentences to vector representations is an common approach to calculate similarities: vector space model (Salton et al., 1975), means of distributed representation of words (Mikolov et al., 2013; Le and Mikolov, 2014) and bi-directional long short-term memory neural networks (Bi-LSTM) (Cross and Huang, 2016; Yang et al., 2019). Recently, Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is known as a better sentence encoder, which is based on masked word prediction in surrounding sentences. We used the BERT model trained using Japanese Wikipedia (Sakata et al., 2019) on the task of masked word prediction because we would like to extract semantically similar sentences to seed queries. The task of masked word prediction is based on the distributional hypothesis (Harris, 1954); thus, the resultant model trained in the task can embed semantically similar sentences into close points on the latent space. We note the vector of sentence qi,j as qi,j . Because both vectors qi,j and c"
2020.coling-main.234,W18-5708,0,0.0276546,"10). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and Rahim, 2006; Yoshino et al., 2013) were widely used as criteria to select appropriate sentences for the training data augmentation. Such a selective approach using large-scale Web data has been applied to the data augmentation of dialogue systems (Du and Black, 2018; Henderson et al., 2019). This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. Licence details: http: 2606 Proceedings of the 28th International Conference on Computational Linguistics, pages 2606–2612 Barcelona, Spain (Online), December 8-13, 2020 Crowdsourcing is a common way to collect human-annotated data at low-cost (Zhao et al., 2011; Mozafari et al., 2014). However, accurate SLU systems based on neural networks require a large-scale dataset. It is not easy to collect sufficient amount of training data only using c"
2020.coling-main.234,C18-1105,0,0.0209126,"lications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web text"
2020.coling-main.234,P18-1156,0,0.0128664,"arning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray et al., 2018). On the other hand, collecting text data from the Web is a widely used approach to building language models of automatic speech recognition systems (Bulyko et al., 2003; Sarikaya et al., 2005; Ng et al., 2005; Tsiartas et al., 2010). Web texts are expected to be more natural than generated pseudo sentences because users handcraft most of them. However, Web texts contain diverse domain texts; thus, we need some criteria to select appropriate texts to be used for the augmented training data. Test-set perplexity (Misu and Kawahara, 2006) or semantic similarity (Hakkani-Tur and"
2020.coling-main.234,P19-1078,0,0.0171223,"s arguments, given a user request written or spoken in natural language. Various SLU benchmarks have been proposed: Air Travel Information Services (ATIS) (Dahl et al., 1994), restaurant information navigation (Williams et al., 2014), and other speech applications (Hori et al., 2019). Adaptation of SLU to newly defined tasks is an important problem (Henderson et al., 2014). The number of training data directly affects the SLU accuracy because most of the recent SLU systems are based on statistical machine learning approaches. Some existing work tackled this problem based on transfer learning (Wu et al., 2019), which uses a pre-trained model on different domain data. However, it is still challenging to make the SLU accurate with no or fewer data. Data augmentation has been applied to solve this problem, which generates pseudo training samples (Hou et al., 2018; Yoo et al., 2019). However, such methods often generate unnatural training samples that will decrease SLU accuracy. Another problem is the ambiguity of user utterances; it is difficult to generate such ambiguous examples with generative approaches. Some existing work tackled this problem by using paraphrasing models (Saha et al., 2018; Ray e"
2020.coling-main.319,P17-4012,0,0.0407213,"ediction We need length estimates when we use LRPE and LDPE in inferences. Instead of using the input lengths like Lakew et al. (2019), we propose using an output length prediction based on a pre-trained BERT model in the source language. We used the [CLS] vector in the last layer of the BERT encoder to predict the output length through an output layer as a regression problem. 4 Experiments To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017). 4.1 Setup Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test. All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based le"
2020.coling-main.319,D18-2012,0,0.01921,"regression problem. 4 Experiments To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017). 4.1 Setup Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test. All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based lengths. 3581 Hyperparameters Our hyperparameter settings came from OpenNMT-py FAQ1 and are used commonly for all the compared methods described later in this section. We conducted five independent training runs with different random seeds and chose the best runs and training epochs in the devtest set to determine the models for the final evaluation. Evaluation We u"
2020.coling-main.319,P02-1040,0,0.115462,"ared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based lengths. 3581 Hyperparameters Our hyperparameter settings came from OpenNMT-py FAQ1 and are used commonly for all the compared methods described later in this section. We conducted five independent training runs with different random seeds and chose the best runs and training epochs in the devtest set to determine the models for the final evaluation. Evaluation We used BLEU (Papineni et al., 2002) for our evaluation metric given by multi-bleu.perl and also investigated the length ratio (LR) of the input and output sentences (LR = tgt len/ref len). BLEU was calculated on translation results re-tokenized by MeCab (Kudo, 2005) after merging the subwords. We also calculated the variance of the length difference between the translation results and the references (VAR) to investigate the effects of the output length constraints, following Takase and Okazaki (2019). The variance of the length differences on a test set consisting of n sentences is given by: V AR = 4.2 n 1X |li − ref leni |2 n"
2020.coling-main.319,N19-1401,0,0.220731,"slation (NMT), a decoder generates one token at a time, and each output token depends on the output tokens generated so far. The decoder’s prediction of the end of the sentence determines the length of the output sentence. This prediction is sometimes made too early– before all of the input information is translated–causing a so-called under-translation. Transformer has sinusoidal positional encoding to incorporate the token position information in the sequence into its encoder and decoder (Vaswani et al., 2017). There are some previous studies for controlling an output length in Transformer. Takase and Okazaki (2019) proposed two variants of length-aware positional encodings called length-ratio positional encoding (LRPE) and length-difference positional encoding (LDPE) to control the output length based on the given length constraints in automatic summarization. Lakew et al. (2019) applied LDPE and LRPE to NMT. They trained an NMT model using output length constraints based on LDPE and LRPE along with special tokens representing length ratio classes between input and output sentences, while they used the input sentence length at the inference time. However, the length of an input sentence is not a reliabl"
2020.iwslt-1.21,D19-1166,0,0.0202838,"res for Disfluent Spanish to Fluent English. NMT models used Fisher’s disfluent references for training. System Fisher UNCorpus + Fisher Fisher-like UNCorpus + Fisher 3.3.2 As shown above, some generated sentences lost the meaning of the sentence due to missing phrases. As a result, the quality of the parallel data decreased and the final translation performance was also degraded. One of the causes of this problem is style transfer constraints are too strong. Thus, it may be mitigated by a model that could control the tradeoff between style transfer and content preservation (Niu et al., 2017; Agrawal and Carpuat, 2019; Lample et al., 2019). Further improvement can be expected by preventing changes in the meaning of sentences and converting only the style. Fisher/Test 11.6 15.2 15.6 Results Tables 3 and 4 show the BLEU scores of the systems evaluated with single fluent references. In Table 3, “Fisher”, “UNCorpus” and “Fisher-like UNCorpus” are models trained on a single training data. “UNCorpus + Fisher” and “Fisher-like UNCorpus + Fisher” are models that were pre-trained on UNCorpus and Fisher-like UNCorpus and then fine-tuned on Fisher/Train, respectively. The models in Table 4 did not use Fisher’s fluent"
2020.iwslt-1.21,D18-1549,0,0.0125757,"m disfluent Spanish to fluent English, as illustrated in Figure 1. First, we transferred fluent Spanish in out-of-domain data into disfluent Spanish (Section 2.1). Then we trained the NMT model leveraging both out-of-domain parallel data as well as in-domain parallel data (Section 2.2). then fine-tuned on true in-domain data. 2.1 3 Unsupervised Style Transfer We employed an unsupervised learning method for the style transfer of Spanish of out-of-domain data. This is because there is no parallel corpus of fluent and disfluent Spanish and it is not possible to adapt supervised learning methods. Artetxe et al. (2018); Lample et al. (2018a,b) proposed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nation"
2020.iwslt-1.21,D11-1033,0,0.0517251,"d the decoder. We chose Adam (Kingma and Ba, 2014) with a learning rate of 0.0001, β1 = 0.9, β2 = 0.999 as the optimizer. Each mini-batch contained 16 sentences. In order to gain robustness to the content of the sentence, we first pre-trained the model using only UNCorpus/Train. During pre-training, early stopping was applied on the BLEU score between source sentences and back-translated sentences of the UNCorpus/Dev with a patience of 10 iterations, and the model with the highest score was stored. After that, additional training of 1 iteration using the Fisher/Train was performed. Evaluation Axelrod et al. (2011) used a language model of in-domain data for out-of-domain data selection in domain adaptation. Following this study, we estimated the similarity between domains by measuring the perplexity (P P L) of the training set W of the out-of-domain data using a 3-gram language model M made from the in-domain data (Equation 1). P P L = 10H(W |M ) (1) H(W |M ) is the entropy, defined as the average of the negative log-likelihood per token, as shown in the following equation: H(W |M ) = 1 1 X − log10 P (s|M ) |W |s∈W Table 2: Perplexity and the number of unknown words (# UNK) for Fisher/train in the 3-gr"
2020.iwslt-1.21,P17-2061,0,0.0491093,"Missing"
2020.iwslt-1.21,C18-1111,0,0.0174526,"tween in-domain and out-of-domain data affects the translation accuracy significantly (Koehn and Knowles, 2017). A domain can be defined by any property of the training data such as topic and style. We expect that the domain similarity comes from these properties. Introduction Neural Machine Translation (NMT) has significantly improved the quality of Machine Translation (MT) (Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). However, domain-specific translation is still difficult in low-resource scenarios, although high performance can be achieved in resource-rich scenarios (Chu and Wang, 2018). Another major problem is the difficulty in translating noisy input sentences including filler, hesitation, etc. Belinkov and Bisk (2017) suggests the difficulty in learning to translate noisy sentences compared to clean ones. The translation of noisy sentences is very important for spoken language translation. In the IWSLT 2020 Conversational Speech Translation Task, we are going to tackle these two problems. The task includes speech-to-text and textto-text translation from disfluent Spanish speeches/transcripts to fluent English text. We chose the text-to-text subtask for our challenge task"
2020.iwslt-1.21,W17-3204,0,0.0919082,"onal speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed. 1 In domain adaptation, the “similarity” between in-domain and out-of-domain data affects the translation accuracy significantly (Koehn and Knowles, 2017). A domain can be defined by any property of the training data such as topic and style. We expect that the domain similarity comes from these properties. Introduction Neural Machine Translation (NMT) has significantly improved the quality of Machine Translation (MT) (Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). However, domain-specific translation is still difficult in low-resource scenarios, although high performance can be achieved in resource-rich scenarios (Chu and Wang, 2018). Another major problem is the difficulty in translating noisy input sentences including fil"
2020.iwslt-1.21,D15-1166,0,0.14652,"Missing"
2020.iwslt-1.21,D17-1299,0,0.0623377,"Missing"
2020.iwslt-1.21,P02-1040,0,0.106503,"ng and punctuation removal were applied only to the source language. Model We used OpenNMT-py3 . The NMT model was based on Transformer. The hyperparameters of the model almost follow the transformer base settings (Vaswani et al., 2017). Note that in the Fisher-only experiment without domain adaptation, the batch size was halved to 2048 tokens. The model was trained for 20,000 iterations using out-of-domain data, and then fine-tuned for 1,000 iterations using in-domain data. The model parameters saved every 100 iterations. Evaluation To evaluate the performance, we calculated the BLEU scores (Papineni et al., 2002) with sacreBLEU4 . (2) 2 http://www.speech.sri.com/projects/srilm/ https://github.com/OpenNMT/OpenNMT-py 4 https://github.com/mjpost/sacreBLEU 3 https://github.com/facebookresearch/UnsupervisedMT 174 Table 3: BLEU scores of trained NMT models for Disfluent Spanish to Fluent English. System Fisher UNCorpus Fisher-like UNCorpus UNCorpus + Fisher Fisher-like UNCorpus + Fisher Fisher/Test 14.8 7.8 6.7 18.3 18.5 Table 4: BLEU scores for Disfluent Spanish to Fluent English. NMT models used Fisher’s disfluent references for training. System Fisher UNCorpus + Fisher Fisher-like UNCorpus + Fisher 3.3.2"
2020.iwslt-1.21,2013.iwslt-papers.14,0,0.0279375,"panish and it is not possible to adapt supervised learning methods. Artetxe et al. (2018); Lample et al. (2018a,b) proposed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain train"
2020.iwslt-1.21,P16-1009,0,0.0177075,"Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain training or only out-ofdomain training (Dakwale and Monz, 2017). In this method, an NMT is pre-trained on a resource rich out-of-domain data until convergence, and then its parameters are fine-tuned on a low-resource indomain data. In this study, we pre-trained the NMT model on the pseudo in-domain data generated in 2.1, and 4. English disfluent translations 5. English fluent translations When training, we used (3) as input and (4) or (5) as output. UNCorpus consists of manually translated UN documents o"
2020.iwslt-1.21,P16-1162,0,0.0253932,"Missing"
2020.iwslt-1.21,C16-1295,0,0.0524319,"Missing"
2020.iwslt-1.21,L16-1561,0,0.0134775,"osed Unsupervised Neural Machine Translation (UNMT) that learns the translation using monolingual corpora of two languages. In this system, we built a fluent-todisfluent style transfer model based on UNMT with out-of-domain fluent data and in-domain disfluent data. 2.2 Fisher/Train Dev Test UNCorpus/Train Dev Test # sentences 138,720 3,977 3,641 1,000,000 4,000 4,000 Results 3.1 Datasets We used the LDC Fisher Spanish speech (disfluent) with new English translations (fluent) (Post et al., 2013; Salesky et al., 2018) as parallel in-domain data and the United Nations Parallel Corpus (UNCorpus) (Ziemski et al., 2016) as parallel out-ofdomain data. Fisher has the following multi-way parallel data distributed by the task organizer: 1. Spanish disfluent speech 2. Spanish disfluent transcripts (gold) Domain Adaptation 3. Spanish disfluent transcripts (ASR output) For the challenge task, we apply fine-tuning, which is one of the conventional domain adaptation methods of MT (Sennrich et al., 2016a). The fine-tuning can result in significant improvements compared to both only in-domain training or only out-ofdomain training (Dakwale and Monz, 2017). In this method, an NMT is pre-trained on a resource rich out-of"
2021.humeval-1.5,J96-2004,0,0.787758,"diction &lt; Serious &lt; Incomprehensible &lt; Unrelated &lt; Fair &lt; Good &lt; Excellent for adequacy14 . Tables 9 and 10 show the label statistics on the training, development, and test sets after applying the heuristics. 5.1.2 Automatic Evaluation Method We used a simple sentence-level automatic MT evaluation framework, which takes hypothesis and reference sentences as the input and predicts the label. Since the task in the experiments was classification, the evaluation model was trained with Inter-annotator Agreement We also measured pairwise agreement among the three annotators using the κ coefficient (Carletta, 1996) and label concordance rate. The results are shown in Table 8. The inter-annotator agreement was not high enough but κ values are also com13 Note that we had three annotators who evaluated all the sentences. 14 We used this heuristic order because of the importance of content errors suggested by Contradiction and Serious. 51 Fluency Incomprehensible Poor Fair Good Excellent A -0.644 (0.371) -0.421 (0.408) -0.079 (0.478) 0.165 (0.479) 0.428 (0.524) B -0.692 (0.356) -0.420 (0.399) 0.019 (0.474) 0.408 (0.485) 0.644 (0.465) C -0.649 (0.378) -0.400 (0.418) -0.129 (0.449) 0.122 (0.467) 0.427 (0.521)"
2021.humeval-1.5,2010.amta-papers.20,0,0.204684,"comprehension results in the early 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables ex"
2021.humeval-1.5,N19-1423,0,0.00475106,"on corpus will be publicly available at https://github.com/ ksudoh/wmt15-17-humaneval. 1 Introduction Most machine translation (MT) studies still evaluate their results using BLEU (Papineni et al., 2002) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Comput"
2021.humeval-1.5,C16-1294,0,0.123701,"al., 2015). Sellam et al. (2020) proposed BLEURT that incorporates auxiliary task signals into the pre-training of a BERT-based sentence-level regression model. These methods aim to evaluate a translation hypothesis using the corresponding reference with a high correlation to human judgment. The evaluation of this kind of MT evaluation, often called meta-evaluation, is usually based on some benchmarks. The meta-evaluation in the recent studies uses the WMT Metrics task dataset consisting of human judgment on MT results. The human judgment is given in the form of Human Direct Assessment (DA) (Graham et al., 2016), a 100-point rating scale. The Human DA results are standardized into z-scores (human DA scores, hereinafter) and used as the evaluation and optimization objective of regression-based MT evaluation methods. Recent MT evaluation methods achieved more than 0.8 in Pearson correlation on WMT 2017 test set1 . However, Takahashi et al. (2020) reported a weaker correlation in low human DA score ranges. Such a finding suggests the difficulty of the MT evaluation on low-quality results. In this work, we focus on the problem in the evaluation of low-quality translations that cause serious misunderstand"
2021.humeval-1.5,D16-1134,0,0.0223488,"al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work analyzed the differences between comprehensibility and adequacy in machine translation outputs. The"
2021.humeval-1.5,U12-1010,0,0.0239653,"arly 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation inst"
2021.humeval-1.5,W15-3044,0,0.0454957,"Missing"
2021.humeval-1.5,2020.eamt-1.39,0,0.0281184,"focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work analyzed the differences between comprehensibility and adequacy in machine translation outputs. The human annotations in her work are major and minor errors in comprehensibility and adequacy on words and phrases. These fine-grained annotations are helpful for detailed translation error detection. The focus of our work is different; we are going to develop sentence-level MT evaluation through simpler human and automatic evaluation schemes. In this work, we suggest revisiting the classification-based evaluation with fluency and 2015. 3 http://producthelp.sdl.com/SDL_"
2021.humeval-1.5,2020.acl-main.704,0,0.163006,"t translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by professional translators. There are several metrics for translation quality assessment (QA) proposed in the translators’ community, such as LISA QA Metric3 and Multidimentional Quality Metrics (MQM)4"
2021.humeval-1.5,W18-6456,0,0.0229894,"02) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by p"
2021.humeval-1.5,2021.ccl-1.108,0,0.0313368,"Missing"
2021.humeval-1.5,H93-1040,0,0.712365,"Missing"
2021.humeval-1.5,P11-1023,0,0.0383942,"gs there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et al. (2016) proposed HUME based on a semantic representation called UCCA. This kind of fine-grained semantic evaluation requires some linguistic knowledge for annotators but enables explainable evaluation instead. However, the meaning of the sentence can be changed by small changes, as discussed later in section 3. Looking at sub-structures and using their coverage in the MT evaluation may suffer from this problem. One recent approach has been proposed by Popovi`c (Popovic, 2020; Popovi´c, 2020). Her work"
2021.humeval-1.5,2006.amta-papers.25,0,0.204382,"ed Work MT evaluation has evolved along with the advance of MT technologies. White et al. (1994) reviewed some attempts of human evaluation and presented adequacy, fluency, and comprehension results in the early 1990s. The Quality Panel approach presented in their paper was motivated by the evaluation of human translations, but it was finally abandoned due to human evaluation difficulties. CallisonBurch et al. (2007) presented meta-evaluation of the MT evaluation in WMT shared tasks. According to the findings there, the WMT shared tasks had employed ranking-based human evaluation for a while. Snover et al. (2006) defined Human-targeted Translation Edit Rate (HTER) that measures the translation quality by the required number of postedits on a translation hypothesis. Denkowski and Lavie (2010) and Graham et al. (2012) discussed the differences among those human evaluation approaches. Graham et al. (2016) proposed human DA for the MT evaluation, and DA has been used as standard human evaluation in recent WMT Metrics tasks. There is another line of human MT evaluation studies focusing on semantics. Lo and Wu (2011) proposed MEANT and its human evaluation variant HMEANT based on semantic frames. Birch et a"
2021.humeval-1.5,2020.acl-main.327,1,0.753728,"on, often called meta-evaluation, is usually based on some benchmarks. The meta-evaluation in the recent studies uses the WMT Metrics task dataset consisting of human judgment on MT results. The human judgment is given in the form of Human Direct Assessment (DA) (Graham et al., 2016), a 100-point rating scale. The Human DA results are standardized into z-scores (human DA scores, hereinafter) and used as the evaluation and optimization objective of regression-based MT evaluation methods. Recent MT evaluation methods achieved more than 0.8 in Pearson correlation on WMT 2017 test set1 . However, Takahashi et al. (2020) reported a weaker correlation in low human DA score ranges. Such a finding suggests the difficulty of the MT evaluation on low-quality results. In this work, we focus on the problem in the evaluation of low-quality translations that cause serious misunderstanding. Judging erroneous translations in the 100-point rating scale would be very difficult and unstable, because the extent of errors cannot be mapped easily into a one-dimensional space. Suppose we are evaluating a translation hypothesis, (1) It is our duty to remain at his sides with its reference, It is not our duty to remain at his si"
2021.humeval-1.5,W18-6450,0,0.0150766,"allenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shimanaka et al., 2018). Zhang et al. (2020) proposed BERTScore based on hard token-level 1 The correlation got worse in the newer WMT datasets (Ma et al., 2018, 2019) due to noise in human judgement (Sellam et al., 2020). 2 This example is taken from the Metrics dataset of WMT 46 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 46–55 Online, April 19, 2021. ©2021 Association for Computational Linguistics difference in this example is just in one missing word not in the hypothesis, but it may cause a serious misunderstanding. Such translation errors are considered as critical ones by professional translators. There are several metrics for translation quality assessment (QA) proposed in the translators’ community, such a"
2021.humeval-1.5,1994.amta-1.25,0,0.699241,"Missing"
2021.humeval-1.5,W19-5302,0,0.0420376,"Missing"
2021.humeval-1.5,P02-1040,0,0.116701,"tical machine translation evaluation in an age of neural machine translation. We have made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available at https://github.com/ ksudoh/wmt15-17-humaneval. 1 Introduction Most machine translation (MT) studies still evaluate their results using BLEU (Papineni et al., 2002) because of its simple, language-agnostic, and model-free methodology. Recent remarkable advances in neural MT (NMT) have cast an important challenge in its evaluation; NMT usually generates a fluent translation that cannot always be evaluated precisely by simple surface-based evaluation metrics like BLEU. A recent trend in the MT evaluation is to use a large-scale pre-trained model like BERT (Devlin et al., 2019). Shimanaka et al. (2019) proposed BERT Regressor based on sentence-level regression using a fine-tuned BERT model, as an extension of their prior study using sentence embeddings (Shi"
2021.humeval-1.5,D19-1053,0,0.0399997,"Missing"
2021.humeval-1.5,2020.coling-main.444,0,0.0779522,"Missing"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.24,N18-1008,0,0.0413439,"Missing"
2021.iwslt-1.24,W19-6612,0,0.0356742,"Missing"
2021.iwslt-1.24,N19-1202,0,0.0304475,"Missing"
2021.iwslt-1.24,P16-5005,0,0.029923,"Missing"
2021.iwslt-1.24,P84-1044,0,0.731782,"Missing"
2021.iwslt-1.24,2020.acl-demos.34,0,0.0278197,"Missing"
2021.iwslt-1.24,D16-1139,0,0.0291103,"perber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence (Kullback and Leibler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequence generated by the teacher model using beam search. In our experiments, sequence-level KD outperformed word-level one, and Kim and Rush (2016b) showed similar trends. Therefore, in our experiments, we call it KD. The KD technique is prevalent in many applications of machine learning, including MT (non-autoregressive machine translation (Gu et al., 2017), simultaneous translation (Ren et al., 2020), etc.). Typically, it is used to distill knowledge from a larger teacher model to a smaller or faster student model. Recent works (Furlanello et al., 2018; Yang et al., 20"
2021.iwslt-1.24,2013.iwslt-papers.14,0,0.0343778,"rained by FT. Specifically, (1) the student model is trained ˆ and LM T , inheriting the parameters with X of the teacher model. Then (2) fine-tune the ˆ and LKD . model with X 5 we used MuST-C (Di Gangi et al., 2019a), a multilingual ST corpus built from TED talks. It contains triplets of about 250K segments of English speeches, transcripts, and Italian translations. We used audio and transcript pairs to train the ASR. To train the MT model, we used transcripts as clean input and ASR outputs as noisy input. For Spanish-English, we used LDC Fisher Spanish speech with new English translations (Post et al., 2013; Salesky et al., 2018). It has the following roughly 140K segments of multi-way parallel data: Experiments Dataset We conducted experiments for English to Italian and Spanish to English NMT. For English-Italian, 200 1 2 https://github.com/mjpost/sacreBLEU https://github.com/pytorch/fairseq ST Type End-to-end Cascade System ST + ASR-PT (Di Gangi et al., 2019b)1 ST + ASR-PT (ESPnet)2 ST ST + ASR-PT MTclean (Di Gangi et al., 2019b)1 MTclean MTasr MTasr + FT MTasr + KD MTasr + FT + KD MTasr + KD → FT MTasr + FT → KD ASR-based input Clean input 16.8 21.5 17.0 21.4 18.9 22.4 29.7 22.1 27.2 23.2 29."
2021.iwslt-1.24,2020.acl-main.350,0,0.0217861,"bler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequence generated by the teacher model using beam search. In our experiments, sequence-level KD outperformed word-level one, and Kim and Rush (2016b) showed similar trends. Therefore, in our experiments, we call it KD. The KD technique is prevalent in many applications of machine learning, including MT (non-autoregressive machine translation (Gu et al., 2017), simultaneous translation (Ren et al., 2020), etc.). Typically, it is used to distill knowledge from a larger teacher model to a smaller or faster student model. Recent works (Furlanello et al., 2018; Yang et al., 2018) have shown that the student model’s accuracy exceeds that of the teacher model, even if its size is identical as the student model. KD has also been applied to ST. Gaido et al. (2020) applied KD to an end-to-end ST using an MT model based on clean transcriptions as the teacher of the end-toend ST model. Our work focuses on the application of KD to a cascade ST using a teacher model based on clean transcripts for the stud"
2021.iwslt-1.24,2020.acl-main.217,0,0.0298296,"ST consist of two components: automatic speech recognition (ASR) and machine translation (MT). In the cascade ST, the error propagation from ASR to MT seriously degrades the ST performance. On the other hand, a new ST system called end-to-end or direct ST uses a single model to directly translate the source language speech into target language text (B´erard et al., 2016). Such an end-to-end approach is a new paradigm in ST and is attracting much research attention. However, a naive end-toend ST without additional training, such as ASR tasks, remains inferior to a cascade ST (Liu et al., 2018; Salesky and Black, 2020). Additionally, it requires parallel data of the source language speech and the target language text, which cannot be obtained easily in practice. Recent ST studies have incorporated the techniques of cascade ST to end-to-end STs. Multitask training with an ASR subtask has been used During the training of an MT model for a cascade ST, we can use clean human transcripts for the source language speech as input. However, since the MT in a cascade ST always receives ASR output during inferences, ASR errors should be propagated to the MT model to cause translation errors. What if we use erroneous s"
2021.iwslt-1.24,P16-1009,0,0.237125,"int use of KD and FT To most effectively exploit both clean input X and ˆ we introduce two training ASR-based input X, techniques: KD and fine-tuning. In KD, the student ˆ by minimizing loss LKD . model is trained using X As shown in Fig. 1, LKD is the loss between Y 0 = 0 0 (y1 , ..., yM ) and Yˆ = (yˆ1 , ..., yˆN ), where Y 0 (1 ≤ m ≤ M ) and Yˆ (1 ≤ n ≤ N ) are the outputs of the teacher and student models. We use the sequence-level KD so that LKD is calculated by replacing L with M and l with m in Eq. 1. On the other hand, fine-tuning (FT) has been widely used for domain adaptation in MT (Sennrich et al., 2016a). Di Gangi et al. (2019c) showed that a model fine-tuned with ASR-based input becomes robust to erroneous ASR input while maintaining high performance for clean input. Following this finding, we employ FT for MT training. In FT, the ˆ which inherits the paramstudent model with X, eters of the teacher model with X, is trained by minimizing LM T (Fig. 1). In addition to the independent use of KD and FT, we examined their possible combinations: • FT+KD. Apply these techniques at the same time. Unlike regular FT, we use loss LKD instead of LM T . Specifically, (1) the teacher model is trained wi"
2021.iwslt-1.24,P16-1162,0,0.263516,"int use of KD and FT To most effectively exploit both clean input X and ˆ we introduce two training ASR-based input X, techniques: KD and fine-tuning. In KD, the student ˆ by minimizing loss LKD . model is trained using X As shown in Fig. 1, LKD is the loss between Y 0 = 0 0 (y1 , ..., yM ) and Yˆ = (yˆ1 , ..., yˆN ), where Y 0 (1 ≤ m ≤ M ) and Yˆ (1 ≤ n ≤ N ) are the outputs of the teacher and student models. We use the sequence-level KD so that LKD is calculated by replacing L with M and l with m in Eq. 1. On the other hand, fine-tuning (FT) has been widely used for domain adaptation in MT (Sennrich et al., 2016a). Di Gangi et al. (2019c) showed that a model fine-tuned with ASR-based input becomes robust to erroneous ASR input while maintaining high performance for clean input. Following this finding, we employ FT for MT training. In FT, the ˆ which inherits the paramstudent model with X, eters of the teacher model with X, is trained by minimizing LM T (Fig. 1). In addition to the independent use of KD and FT, we examined their possible combinations: • FT+KD. Apply these techniques at the same time. Unlike regular FT, we use loss LKD instead of LM T . Specifically, (1) the teacher model is trained wi"
2021.iwslt-1.24,D17-1145,0,0.0211509,"ion improved the robustness against ASR errors and that the knowledge distillation after the fine-tuning provided more significant improvement. 198 Proceedings of the 18th International Conference on Spoken Language Translation, pages 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence ("
2021.iwslt-1.24,Q19-1020,0,0.0339238,"Missing"
2021.iwslt-1.24,2020.aacl-demo.6,0,0.0861949,"Missing"
2021.iwslt-1.24,2020.autosimtrans-1.3,0,0.0350951,"s 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al., 2015) is a method of transferring knowledge from a teacher to a student model. Typically, the student model is trained by minimizing the KL-divergence (Kullback and Leibler, 1951) loss between the output probability distributions of the teacher and student models (wordlevel KD). Sequence-level knowledge distillation (sequence-level KD) (Kim and Rush, 2016a) targets the token-sequ"
2021.iwslt-1.24,C04-1168,0,0.116584,"ranscriptions. We also investigate the joint use of knowledge distillation and fine-tuning. Experimental results revealed that the knowledge distillation improved the robustness against ASR errors and that the knowledge distillation after the fine-tuning provided more significant improvement. 198 Proceedings of the 18th International Conference on Spoken Language Translation, pages 198–205 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related work (a) MT89:;< Some ST studies have tackled the problem of ASR error propagation. N-best hypotheses (Zhang et al., 2004; Quan et al., 2005), confusion networks (Bertoldi and Federico, 2005; Bertoldi et al., 2007), and lattices (Matusov and Ney, 2010; Sperber et al., 2017a) were used to include ASR ambiguity in the ST process. Osamura et al. (2018) used the weighted sum of embedding vectors for ASR word hypotheses based on their posterior probabilities. Sperber et al. (2017b) and Xue et al. (2020) showed that translation accuracy against erroneous speech transcriptions can be improved by introducing pseudo ASR errors in the training data of MT. Knowledge distillation (KD) (Buciluˇa et al., 2006; Hinton et al.,"
2021.iwslt-1.27,2021.iwslt-1.29,0,0.0229024,"foreign guest speakers from politicians to business representatives. The press conferences are video-recorded and available online3 . For some of them, transcripts are provided on its website. Translation Studies In translation studies, SI characteristics have typically been investigated from the aspects of latency, quality, and word order. For evaluating latency by human interpreters, Ear-Voice Span (EVS) is commonly used as a metric. EVS denotes the lag between the original utterances and the corresponding SIs. The analysis of quality often relies on a manual evaluation of the corpus data (Fantinuoli and Prandi, 2021). Ino and Kawahara (2008), for example, investigated SI faithfulness based on manual annotation of the data. SI aims to translate a source speech with low latency and high quality, where the two factors are in a trade-off relationship. However, previous studies (e.g., Lee, 2002) argued that a longer latency negatively affects SI quality. Word order has also been intensively studied in the field. Recent research by Cai et al. (2020) demonstrated a statistical study based on SIDB and compared word order between translation and SI. Material Our corpus consists of the SIs of four kinds of material"
2021.iwslt-1.27,2021.naacl-main.150,0,0.0126031,"amounts of experience, as in Shimizu et al. (2014), which enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translation"
2021.iwslt-1.27,P19-1126,0,0.0259571,"Missing"
2021.iwslt-1.27,D15-1276,0,0.0178831,"Missing"
2021.iwslt-1.27,2020.acl-main.253,0,0.015926,"egments that correspond to multiple English sentences: divide where it corresponds to the boundary of English sentences. Mark XXXXX for end/start times of Japanese segments. • English segments that consist of multiple sentences: divide at sentence boundary. Mark XXXXX for end/start times of segments. An example of the data aligned at the sentence level is shown in Fig. 2. Each sentence is delimited by one blank line. 4.3 Quality: To evaluate the SI quality, we calculated two metrics8 . The first one was BERTScore (Zhang et al., 2019), which is also used to evaluate machine translations (e.g., Edunov et al., 2020). It is based on contextualized subword embeddings and is expected to capture meanings rather than surface forms like BLEU (Papineni et al., 2002). It would be appropriate for evaluating the aspects of SIs used by interpreters, including anticipation, summarization, and generalization. BERTScores were calculated between SIs (candidates) and offline translations (references) for each sentence. The other quality metric was the bunsetsu-level semantic preservation score (BSPS), which evaluated the faithfulness of the SIs against the translations. An example is shown in Fig. 3. Similar to Ino and"
2021.iwslt-1.27,P14-2090,1,0.790449,"h enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation"
2021.iwslt-1.27,P02-1040,0,0.111383,"t times of Japanese segments. • English segments that consist of multiple sentences: divide at sentence boundary. Mark XXXXX for end/start times of segments. An example of the data aligned at the sentence level is shown in Fig. 2. Each sentence is delimited by one blank line. 4.3 Quality: To evaluate the SI quality, we calculated two metrics8 . The first one was BERTScore (Zhang et al., 2019), which is also used to evaluate machine translations (e.g., Edunov et al., 2020). It is based on contextualized subword embeddings and is expected to capture meanings rather than surface forms like BLEU (Papineni et al., 2002). It would be appropriate for evaluating the aspects of SIs used by interpreters, including anticipation, summarization, and generalization. BERTScores were calculated between SIs (candidates) and offline translations (references) for each sentence. The other quality metric was the bunsetsu-level semantic preservation score (BSPS), which evaluated the faithfulness of the SIs against the translations. An example is shown in Fig. 3. Similar to Ino and Kawahara (2008), each bunsetsu that appeared in the translation was considered a unit of ideas. Then we counted the number of bunsetsus in the SI"
2021.iwslt-1.27,shimizu-etal-2014-collection,1,0.825935,"quality. 1 Language En↔Jp En↔Es En↔Jp Zh→En En↔Jp Hours 182 217 22 68 304.5 Table 1: Existing SI corpora and ours or transcripts; for SI corpora, human interpreters actually do SI. SI corpora are useful not only for the construction of automatic SI systems but also for translation studies. To facilitate research in the field of SI, we are constructing a new large-scale English↔Japanese SI corpus1 . We recorded the SIs of lectures and press conferences and amassed over 300 hours of such data. Some lectures have SI data generated by three interpreters with different amounts of experience, as in Shimizu et al. (2014), which enables comparisons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), includin"
2021.iwslt-1.27,D18-2010,0,0.0128757,"by the first author to fairly compare the data of the interpreters of each rank. Since the segments in the SI transcripts were based on the interpreters’ utterances, they did not necessarily match among the interpreters. Thus, we gave sentence alignments based on the sentences of the English transcripts segmented using the following rules: 228 4 A bunsetsu is a basic unit of dependency in Japanese that consists of one or more content words and the following zero or more function words (Kawahara and Kurohashi, 2006). 5 We used Juman++ ver.1.02 rather than the development version of Juman++ V2 (Tolmachev et al., 2018). in our data, we separately calculated EVS at the beginning and the end of a sentence7 : EN_0177 469789 471829 I&apos;ve got two questions for you. JA_0116 XXXXXX 473315 二つの質問がありますよ。 EN_0178 471829 473469 (Laughter) JA_0000 XXXXXX XXXXXX __null__ EV Sstart = start timeJP − start timeEN EN_0179 473469 476069 You know what&apos;s coming now, right? JA_0117 474778 476197 質問分かってるんですね。 EV Send = end timeJP − end timeEN . Figure 2: Example of sentence-level alignment • segments ending with a period (.) or a period + a double quotation mark (.”) • segments ending with a question mark (?) or a question mark +"
2021.iwslt-1.27,2021.autosimtrans-1.5,0,0.0767654,"e after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translation is based on complete audio data In this paper, we describe the construction of a new corpus and present the results of its analysis. Its design follows the framework of Shimizu et al. (2014). The analysis was conducted on a subset of lectu"
2021.iwslt-1.27,2020.emnlp-main.178,0,0.0147252,"roduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translation is based on complete audio data In this pap"
2021.iwslt-1.27,P19-1582,0,0.0131896,"ons of SI differences based on experience. Introduction Simultaneous interpretation (SI) is a task of translating speech from a source language into a target language in real-time. Unlike consecutive translation, where the translation is done after the speaker pauses, in SI the translation process starts while the speaker is still talking. With recent developments in machine translation and speech processing, various studies have been conducted aiming at automatic speech translation (Pino et al., 2020; Wu et al., 2020; Inaguma et al., 2021; Bahar et al., 2021), including SI (Oda et al., 2014; Zheng et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020; Nguyen et al., 2021), based on speech corpora. Existing speech corpora can be classified into Speech Translation corpora or Simultaneous Interpretation corpora, as defined by Zhang et al. (2021). Table 1 lists publicly-available SI corpora. Although a large number of Speech Translation corpora have been published, the number of SI corpora remains very limited. Both types of corpora are comprised of audio data and their corresponding translations, although how the translations are generated is different. For Speech Translation corpora, a translati"
2021.iwslt-1.3,P19-1126,0,0.0284687,"Missing"
2021.iwslt-1.3,N12-1048,0,0.010074,"pon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automa"
2021.iwslt-1.3,N19-1423,0,0.0161391,"が 起き て も 何 が 起き て も 何 が 起き て も 何 が 起き て 人間 は 何 が 間違っ て いる の か を 考える の が 得意 です 新しい こと を 試し て み て も いい です よ ね 昇給 を 求める という よう な 何 か 新しい こと を 試みよ う という とき 人 は どう まずい こと に なり 得る か 考える こと に 長け て い ます Table 3: Translation examples by wait-k baseline and wait-k with chunk shuffling (pr = 0.02). System wait-10 + CShuflow wait-20 + SKDmedium wait-30high BLEU 14.41 16.20 16.19 train 2,762,408 AL 7.21 11.54 13.83 translation for partially-observed input, using a multi-label classifier based on linear SVMs (Fan et al., 2008). Motivated by this study, we used a neural network-based classifier using BERT (Devlin et al., 2019) for NCLP. The problem of NCLP is defined as the label prediction of a syntactic constituent coming next to a given word subsequence in the pre-order tree traversal. In this work, we used 1-lookahead prediction, so the problem was relaxed into the prediction of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the"
2021.iwslt-1.3,2020.emnlp-demos.19,0,0.0245443,"mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sentences from IWSLT 2017. During fine-tuning, usually generates literal translati"
2021.iwslt-1.3,J93-2004,0,0.0770268,"on of a label of a syntactic constituent given its preceding words and the first word composing it. A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime"
2021.iwslt-1.3,E17-1099,0,0.0202073,"refix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due to the large language difference in addition to data scarsity. We developed an automatic text-to-text simultaneous translation system for this shared task. We applied some extensions to a standard wait-k NMT i"
2021.iwslt-1.3,2014.iwslt-papers.16,0,0.0132413,". A predicted constituent label was inserted at the corresponding position in the input word sequence, immediately after its preceding word. That doubled the length of input sequences. For subword-based NMT, we applied BPE only onto words in the input sequences and put dummy labels after subwords other than end-of-word ones, to order the input in an alternating way. We used Huggingface transformers (Wolf et al., 2020) for our implementation of NCLP with bert-base-uncased. We used Penn Treebank 3 (Marcus et al., 1993) for the NCLP training and development sets, and NAIST-NTT TED Talk Treebank (Neubig et al., 2014) for the NCLP evaluation set. Table 5 shows the number of training, development, and evaluation instances extracted from the datasets. Note that we can extract many instances from a single parse tree. Table 6 shows the results of the 5 most frequent labels in the NCLP training data. NP and VP are chunk shuffling may work as a perturbation, and we need further investigation. Official results on the test set Table 4 shows BLEU and AL results on the test set. The system with the medium latency regime (wait-20 + SKD) worked relatively well; it achived a comparable BLEU result with wait-30. However"
2021.iwslt-1.3,P15-1020,1,0.837341,"tempt: Incremental Next Constituent Label Prediction We tried another technique described below in the shared task, but it was not included in our primary submission because it did not outperform the baseline. Here, we also describe this for further investigation in future. For simultaneous machine translation, deciding how long to wait for input before translation is important. Predicting what kind of phrase comes next is a part of useful information in determining the timing. In this study, we tried incremental Next Constituent Label Prediction (NCLP). In SMT-based simultaneous translation, Oda et al. (2015) proposed a method to predict unseen syntactic constituents to determine when to start 42 natural than baseline like these examples. However, many sentences are not informative and missing details compared to the baseline. We’ll investigate a more effective way to use NCLP in our future work. 18 16 wait-k wait-k+NCLP BLEU 14 32 12 k=10 56 52 40 60 64 48 28 44 7 In this paper, we described our English-to-Japanese text-to-text simultaneous translation system. We extended the baseline wait-k with the knowledge distillation to encourage literal translation and targetside chunk shuffling to relax t"
2021.iwslt-1.3,D16-1139,0,0.0183701,"o the length of T . Then, we choose to shuffle or keep the chunks in T¯ with a probability pr , defined as a hyperparameter. We tried only the random shuffling with the fixed chunk size of k in this time; More linguistically-motivated chunk reordering would be worth trying as future work. q(y = k|x; θT ) × k=1 log p(y = k|x; θ) (3) where θT parameterizes the teacher distribution. Sequence-level Knowledge Distillation (SKD), which gives the student model the output of the teacher model as knowledge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X"
2021.iwslt-1.3,N19-4009,0,0.0238387,"ge, propagates a wide range of knowledge to the student model and trains it to mimic its knowledge (Kim and Rush, 2016). The teacher distribution q(Y |X) is approximated by its mode q(Y |X) ≈ 1{Y = argmax q(Y |X)}, X∈T and the loss objectives as follows: LSKD = −Ex∼data X Target-side chunk shuffling q(Y |X) log p(Y |X) Y ∈T ≈ −EX∼data,Yb =argmax q(Y |X) [log p(y = Yb |X)] (4) 5 Primary system Y ∈T 5.1 Implementation where p(Y |X) is the sequence-level distribution, Our system implementation was based on the ofand Y ∈ T is the space of possible target sentences. ficial baseline1 using fairseq (Ott et al., 2019) and SKD can be implemented simply by training the SimulEval (Ma et al., 2020). student model using (X, Yb ), where Yb is derived from the teacher model outputs for the source lan- 5.2 Setup guage portion of the training corpus. Data All of the models were based on TransWe use SKD for reduction of colloquial expres- former, trained using 17.9 million Englishsions in the spoken language corpus. Such col- Japanese parallel sentences from WMT20 news loquial expressions are highly dependent on lan- task and fine-tuned using 223 thousand parallel guages and difficult to translate by NMT, which sent"
2021.iwslt-1.3,P06-2088,0,0.0507834,"slate X to Y incrementally. In other words, each output prediction of Y is made upon partial input observations of X. Suppose an output prefix subsequence Y1j = y1 , y2 , ..., yj has already been predicted from prefix observations of the input X1i = x1 , x2 , ..., xi . When we predict the next Introduction Automatic simultaneous translation is an attractive research field that aims to translate an input before observing its end for real-time translation similar to human simultaneous interpretation. Starting from early attempts using rule-based machine translation (Matsubara and Inagaki, 1997; Ryu et al., 2006) and statistical methods using statistical machine translation (Bangalore et al., 2012; Fujita et al., 2013), recent studies successfully applied neural machine translation (NMT) into this task (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019). The simultaneous translation shared task in the IWSLT evaluation campaign started on 2020 with English-to-German (Ansari et al., 2020) speechto-text and text-to-text tasks, and a new language pair of English-to-Japanese has been included on 2021 only in text-to-text task. English-to-Japanese is much more challenging than English-to-German due"
2021.iwslt-1.3,P16-1162,0,0.0452263,"IWSLT 2017. During fine-tuning, usually generates literal translations. Here, we we examined the effectiveness of knowledge distilfirstly train a teacher, Transformer-based offline lation and chunk shuffling with several hyperparamNMT model using the training corpus and use it to eter settings and reported the results by the models obtain pseudo-reference translations in the target that resulted in the higher BLEU on IWSLT 2021 language. Then, we train a student, Transformer- development set. The text was preprocessed by based simultaneous NMT model using the pseudo- Byte Pair Encoding (BPE) (Sennrich et al., 2016) parallel corpus with the original source language 1 https://github.com/pytorch/fairseq/ sentences and the corresponding translation re- blob/master/examples/simultaneous_ sults by the teacher model. The pseudo-references translation/docs/enja-waitk.md 40 BLEU 16.8 AL - 18 16 18 11.8 14.69 15.57 7.27 11.47 13.7 14 BLEU System offline Baseline wait-10 wait-20 wait-30high Proposed wait-10 + CShuflow wait-10 + SKD wait-20 + SKDmedium wait-30 + SKD 16 12 12 k=10 12 18 14 14 20 24 22 26 28 30 32 28 30 24 22 32 26 16 10 13.77 13.5 15.22 15.21 7.29 7.28 11.48 13.71 wait-k wait-k+SKD 8 6 6 8 10 12 14"
2021.sigdial-1.9,N19-1423,0,0.00516984,"raining data. Cx+i and Cx−i are, respectively, the set of the positive example action categories associated with the user request xi and the set of the action categories without any annotation. rj is the rank predicted by the model for the positive category j and L(rj ) is the weight function satisfying 81 bels (categories) as follows: ( ) d(xi , xj ) 70 · . sij = exp − 69 d¯ 4.1 Model Configuration PyTorch (Paszke et al., 2019) is used to implement the models. We used the Japanese BERT model (Shibata et al., 2019), which was pre-trained on Wikipedia articles. Both BASE and LARGE model sizes (Devlin et al., 2019) were used for the experiments. We used Adam (Kingma and Ba, 2015) to optimize the model parameters and set the learning rate to 1e−5. For m in Eq. (3) and κ in Eq. (1), we set m = −0.8, κ = 5 according to the literature (Cevikalp et al., 2020). We used the distributed representations output by BERT as the vector xi in the label propagation. Since the parameters of BERT are also optimized during the training, we reran the label propagation every five epochs. We pre-trained the model by PN learning before we applied PU learning. Similarity score sij of (PU, nearest) is also scaled by Eq. (6) as"
2021.sigdial-1.9,J13-3008,0,0.0716535,"Missing"
2021.sigdial-1.9,W17-5526,0,0.0430423,"Missing"
2021.sigdial-1.9,W17-5514,0,0.0497338,"Missing"
2021.sigdial-1.9,W17-5506,0,0.0139152,"stem assumes that the user’s intention is not clear. In the corpus collected by Cohen and Lane (2012), which assumes a car navigation dialogue agent, the agent responds to user requests classified as Q1, such as suggesting a stop at a gas station when the user is running out of gasoline. Our study collected a variation of ambiguous user utterances to cover several situations in sightseeing. Table 12: Ratios of false positive in label propagation 5.1 Task-Oriented Dialogue Corpus Many dialogue corpora for task-oriented dialogue have been proposed, such as Frames (El Asri et al., 2017), In-Car (Eric et al., 2017), bAbI dialog (Bordes and Weston, 2016), and MultiWOZ (Budzianowski et al., 2018). These corpora assume that the user requests are clear, as in Q3 in Table 1 defined by Taylor (1962, 1968), and do not assume that user requests are ambiguous, as is the case in our study. The corpus collected in our study assumes cases where the user requests are ambiguous, such as Q1 and Q2 in Table 1. Ohtake et al. (2009); Yoshino et al. (2017) tackled sightseeing dialogue domains. The corpus collected by Ohtake et al. (2009) consisted of dialogues by a tourist and guide for making a oneday plan to sightsee in"
2021.sigdial-1.9,D18-1547,0,0.0349231,"Missing"
2021.sigdial-1.9,W12-1635,0,0.0462243,"Missing"
2021.sigdial-1.9,C02-1086,0,0.195053,"Missing"
2021.sigdial-1.9,J98-1005,0,0.370326,"Missing"
2021.sigdial-1.9,W19-5931,0,0.0132541,"king them to these categories. There are 70 categories in total. The functions and categories are defined heuristically acThoughtful System Action to Ambiguous User Request Existing task-oriented dialogue systems assume that user intentions are clarified and uttered in an explicit manner; however, users often do not know what they want to request. User requests in such cases are ambiguous. Taylor (1962, 1968) categorizes user states in information search into four levels according to their clarity, as shown in Table 1. Most of the existing task-oriented dialogue systems (Madotto et al., 2018; Vanzo et al., 2019) convert explicit user requests (Q3) into machine readable expressions (Q4). Future dialogue systems need to take appropriate actions even in situations such as Q1 and Q2, where the users are not able to clearly verbalize their requests 1 The dataset is available at https://github.com/ahclab/arta_corpus. 78 Function spot search cording to Web sites for Kyoto sightseeing. “Spot search” is a function to search for specific spots and is presented to the user in the form of an action such as “Shall I search for an art museum around here?” “Restaurant search” is a function to search for specific re"
2021.sigdial-1.9,N10-1055,0,0.0451359,"Missing"
2021.sigdial-1.9,P18-1136,0,0.0133131,"s are generated by linking them to these categories. There are 70 categories in total. The functions and categories are defined heuristically acThoughtful System Action to Ambiguous User Request Existing task-oriented dialogue systems assume that user intentions are clarified and uttered in an explicit manner; however, users often do not know what they want to request. User requests in such cases are ambiguous. Taylor (1962, 1968) categorizes user states in information search into four levels according to their clarity, as shown in Table 1. Most of the existing task-oriented dialogue systems (Madotto et al., 2018; Vanzo et al., 2019) convert explicit user requests (Q3) into machine readable expressions (Q4). Future dialogue systems need to take appropriate actions even in situations such as Q1 and Q2, where the users are not able to clearly verbalize their requests 1 The dataset is available at https://github.com/ahclab/arta_corpus. 78 Function spot search cording to Web sites for Kyoto sightseeing. “Spot search” is a function to search for specific spots and is presented to the user in the form of an action such as “Shall I search for an art museum around here?” “Restaurant search” is a function to s"
2021.sigdial-1.9,P19-1078,0,0.017233,"mes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive/negative (PN) learning method to classify thoughtful actions given an ambiguous user request. 1 Introduction Task-oriented dialogue systems satisfy user requests by using pre-defined system functions (Application Programming Interface (API) calls). Natural language understanding, a module to bridge user requests and system API calls, is an important technology for spoken language applications such as smart speakers (Wu et al., 2019). Although existing spoken dialogue systems assume that users give explicit requests to the system (Young et al., 2010), users may not always be able to define and verbalize the content and conditions of their own requests clearly (Yoshino et al., 77 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 77–88 July 29–31, 2021. ©2021 Association for Computational Linguistics Level Q1 Q2 Q3 Q4 Definition The actual, but unexpressed request The conscious, within-brain description of the request The formal statement of the request The request as pres"
2021.sigdial-1.9,P19-1081,0,0.0164307,"logues by a tourist and guide for making a oneday plan to sightsee in Kyoto. However, it was difficult for the developed system to make particular recommendations for conversational utterances or monologues. Yoshino et al. (2017) developed a dialogue agent that presented information with a proactive dialogue strategy. Although the situation is similar to our task, their agent does not have clear natural language understanding (NLU) systems to bridge the user requests to a particular system action. Some dialogue corpora are proposed to treat user requests that are not always clear: OpenDialKG (Moon et al., 2019), ReDial (Li et al., 2018), and RCG (Kang et al., 2019). They assume that the system makes recommendations even if the user does not have a specific request, in particular, dialogue domains such as movies or music. In our study, we focus on conversational utterance and monologue during sightseeing, which can be a trigger of thoughtful actions from the system. 84 6 Conclusion straints. In 3rd International Conference on Computer Vision Theory and Applications (VISAPP ’ 08), pages 489–496. We collected a dialogue corpus that bridges ambiguous user requests to thoughtful system actions while focu"
2021.sigdial-1.9,W17-5542,1,0.848867,"d system actions. We defined a problem to train a model on the incompletely annotated data and tested on the completely annotated data1 . In order to train the model on the incomplete training data, we applied the positive/unlabeled (PU) learning method (Elkan and Noto, 2008; Cevikalp et al., 2020), which assumes that some of the data are annotated as positive and the rest are not. The experimental results show that the proposed classifier based on PU learning has higher classification performances than the conventional classifier, which is based on general positive/negative (PN) learning. 2 (Yoshino et al., 2017). We used crowdsourcing to collect ambiguous user requests and link them to appropriate system actions. This section describes the data collection. 2.1 Corpus Collection We assume a dialogue between a user and a dialogue agent on a smartphone application in the domain of tourist information. The user can make ambiguous requests or monologues, and the agent responds with thoughtful actions. Figure 1 shows an example dialogue between a user and a dialogue agent. The user utterance “I love the view here!” is not verbalized as a request for a specific function. The dialogue agent responds with a t"
2021.sigdial-1.9,W09-3405,1,0.687342,"tios of false positive in label propagation 5.1 Task-Oriented Dialogue Corpus Many dialogue corpora for task-oriented dialogue have been proposed, such as Frames (El Asri et al., 2017), In-Car (Eric et al., 2017), bAbI dialog (Bordes and Weston, 2016), and MultiWOZ (Budzianowski et al., 2018). These corpora assume that the user requests are clear, as in Q3 in Table 1 defined by Taylor (1962, 1968), and do not assume that user requests are ambiguous, as is the case in our study. The corpus collected in our study assumes cases where the user requests are ambiguous, such as Q1 and Q2 in Table 1. Ohtake et al. (2009); Yoshino et al. (2017) tackled sightseeing dialogue domains. The corpus collected by Ohtake et al. (2009) consisted of dialogues by a tourist and guide for making a oneday plan to sightsee in Kyoto. However, it was difficult for the developed system to make particular recommendations for conversational utterances or monologues. Yoshino et al. (2017) developed a dialogue agent that presented information with a proactive dialogue strategy. Although the situation is similar to our task, their agent does not have clear natural language understanding (NLU) systems to bridge the user requests to a"
C10-1050,P06-1090,0,0.0477622,"anguage model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to efficiently calculate the score of the reor"
C10-1050,P06-1067,0,0.0327337,"nd the right hand side of Figure 1, allowing us to score both global and local word reorderings. ′ ′ To add γ to rules, we permuted γ into γ after rule extraction based on Grow-diag-final (Koehn et al., 2005) alignment by GIZA++ (Och and Ney, 2003). To do this permutation on rules, we applied two methods. One is the same algorithm as Tromble and Eisner (2009), which reorders aligned source terminals and nonterminals in the same order as that of target side and moves unaligned source terminals to the front of aligned terminals or nonterminals (move-to-front). The other is the same algorithm as AI-Onaizan and Papineni (2006), which differs from Tromble and Eisner’s approach in attaching unaligned source terminals to the closest prealigned source terminals or nonterminals (attach). This extension of ′ adding γ does not increase the number of rules. Table 1 shows a Japanese-to-English example of the representation of rules for our proposed system. Japanese words are romanized. Suppose that source-side string is (X1 wa jinsei no X2 da) and target-side string is (X1 is X2 of life) and their word alignments are a=((jinsei , life) , (no , of) , (da , is)). Source-side aligned words and nonterminal symbols are sorted in"
C10-1050,J93-2003,0,0.0272701,"o handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally p"
C10-1050,N09-1025,0,0.0662338,"Missing"
C10-1050,niessen-etal-2000-evaluation,0,0.0539958,"o preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor scores. 4.4 Discussion Table 6 displays examples showing the cause of the improvements of our system with reordering model (attach) comparing to baseline system. We can see that the outputs of our sys"
C10-1050,P02-1038,0,0.548257,"Missing"
C10-1050,J03-1002,0,0.012804,"equation: ( ) ′ ′ Tˆ = (Sˆ , Tˆ) = (S , T ) argmax w(D) . (6) D:S(D)=S Our system generates the reordered source sen′ tence S as well as target sentence T . Figure 2 ′ shows the generated reordered source sentence S when translating the example of Figure 1. Note ′ that the structure of S is the same as that of target sentence T . The decoder generates both Figure 2 and the right hand side of Figure 1, allowing us to score both global and local word reorderings. ′ ′ To add γ to rules, we permuted γ into γ after rule extraction based on Grow-diag-final (Koehn et al., 2005) alignment by GIZA++ (Och and Ney, 2003). To do this permutation on rules, we applied two methods. One is the same algorithm as Tromble and Eisner (2009), which reorders aligned source terminals and nonterminals in the same order as that of target side and moves unaligned source terminals to the front of aligned terminals or nonterminals (move-to-front). The other is the same algorithm as AI-Onaizan and Papineni (2006), which differs from Tromble and Eisner’s approach in attaching unaligned source terminals to the closest prealigned source terminals or nonterminals (attach). This extension of ′ adding γ does not increase the number"
C10-1050,P05-1066,0,0.562071,"les of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it d"
C10-1050,P03-1021,0,0.214019,"X XXX System Baseline (Hiero) Preprocessing Hiero + move-to-front Hiero + attach 28.09 17.32 28.85 29.25 poorness, our proposed method effectively utilize this reordering model in contrast to preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor"
C10-1050,2005.iwslt-1.8,0,0.460908,"n model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to efficiently calculate"
C10-1050,W08-0402,0,0.0145236,"on; incorpolated into sorted rules for the proposed implementation. To reveal the effectiveness of integrating the reordering model into decoder, we compared the following setups: Data Training • baseline: a standard hierarchical phrasebased machine translation (Hiero) system. Development Test • preprocessing: applied Tromble and Eisner’s approach, then translate by Hiero system. Word. 2.4M 2.3M 10.3K 9.8K 14.2K 13.5K Avg. leng 12.0 11.5 10.3 9.8 14.2 13.5 Table 4: The Data statistics • Hiero system + reordering model: integrated reordering model into Hiero system. We used the Joshua Decoder (Li and Khudanpur, 2008) as the baseline Hiero system. This decoder uses a log-linear model with seven features, which consist of N -gram language model PLM (T ), lexical translation model Pw (γ|α), Pw (α|γ), rule ja en ja en ja en Sent. 200.8K 200.8K 1.0K 1.0K 1.0K 1.0K For experiments we used a Japanese-English basic travel expression corpus (BTEC). Japanese word order is linguistically very different from English and we think Japanese-English pair is a very good test bed for evaluating reordering model. 443 XXX XXX Metrics BLEU XX XXX System Baseline (Hiero) Preprocessing Hiero + move-to-front Hiero + attach 28.09"
C10-1050,P07-1091,0,0.0761436,"rase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitl"
C10-1050,2001.mtsummit-papers.68,0,0.0370106,"s reordering model in contrast to preprocessing approach. PER 39.68 45.27 39.89 39.43 5 Table 5: BLEU and PER scores on the test set. Our training corpus contains about 200.8k sentences. Using the training corpus, we extracted hierarchical phrase rules and trained 4-gram language model and word-based reordering model. Parameters were tuned over 1.0k sentences (development data) with single reference by minimum error rate training (MERT) (Och, 2003). Test data consisted of 1.0k sentences with single reference. Table 4 shows the condition of corpus in detail. 4.3 Results Table 5 shows the BLEU (Papineni et al., 2001) and PER (Niesen et al., 2000) scores obtained by each system. The results clearly indicated that our proposed system with word-based reordering model (move-to-front or attach) outperformed baseline system on BLEU scores. In contrast, there is no significant improvement from baseline on PER. This suggests that the improvement of BLEU mainly comes from reordering. In our experiment, preprocessing approach resulted in very poor scores. 4.4 Discussion Table 6 displays examples showing the cause of the improvements of our system with reordering model (attach) comparing to baseline system. We can s"
C10-1050,P08-1066,0,0.063341,"ur approach is similar to preprocessing approach (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009) in that it reorders source sentence in target order. The difference is this sentence reordering is done in decoding rather than in preprocessing. A lot of studies on lexicalized reordering (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) focus on the phrase-based model. These works cannnot be directly applied to hierarchical phrase-based model because of the difference between normal phrases and hierarchical phrases that includes nonterminal symbols. Shen et al. (2008,2009) proposed a way to integrate dependency structure into target and source side string on hierarchical phrase rules. This approach is similar to our approach in extending the formalism of rules on hierarchical phrase-based model in order to consider the constraint of word order. But, our approach differs from (Shen et al., 2008; Shen et al., 2009) in that syntax annotation is not necessary. 6 Conclusion and Future Work We proposed a method to integrate word-based reordering model into hierarchical phrase-based ′ machine translation system. We add γ into the hiero rules, but this does not i"
C10-1050,D09-1008,0,0.0590956,"l., 2005; Nagata et al., 2006) focus on the phrase-based model. These works cannnot be directly applied to hierarchical phrase-based model because of the difference between normal phrases and hierarchical phrases that includes nonterminal symbols. Shen et al. (2008,2009) proposed a way to integrate dependency structure into target and source side string on hierarchical phrase rules. This approach is similar to our approach in extending the formalism of rules on hierarchical phrase-based model in order to consider the constraint of word order. But, our approach differs from (Shen et al., 2008; Shen et al., 2009) in that syntax annotation is not necessary. 6 Conclusion and Future Work We proposed a method to integrate word-based reordering model into hierarchical phrase-based ′ machine translation system. We add γ into the hiero rules, but this does not increase the number of rules. So, this extension itself does not affect the search space of decoding. In this paper we used Tromble and Eisner’s reordering model for our method, but various reordering model can ′ be incorporated to our method, for example S N -gram language model. Our experimental results on Japanese-to-English task showed that our sys"
C10-1050,N04-4026,0,0.216112,"the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based translation to constrain word order. In this paper, we adopt the reordering model originally proposed by Tromble and Eisner (2009) for the preprocessing approach in phrase-based translation. To integrate the word-based reordering model, we added a reordered source string into the right-hand-side of SCFG’s rules. By this extension, our system can generate the reordered source sentence as well as target sentence and is able to e"
C10-1050,D09-1105,0,0.74067,"to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering mode"
C10-1050,P06-1098,1,0.822492,"than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global reordering by SCFG, it does not explicitly introduce reordering model to constrain word order. In contrast, lexicalized reordering models (Tillman, 2004; Koehn et al., 2005; Nagata et al., 2006) are extensively used This paper proposes a method that incorporates word-based reordering model into hierarchical phrase-based transl"
C10-1050,C04-1073,0,0.441205,"ontext-free grammar rules of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system. 1 To handle global reordering in phrase-based translation, various preprocessing approaches have been proposed, where the source sentence is reordered to target language order beforehand (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Tromble and Eisner, 2009). However, preprocessing approaches cannot utilize other information in the translation model and target language model, which has been proven helpful in decoding. Introduction Hierarchical phrase-based machine translation (Chiang, 2007; Watanabe et al., 2006) is one of the promising statistical machine translation approaches (Brown et al., 1993). Its model is formulated by a synchronous context-free grammar (SCFG) which captures the syntactic information between source and target languages. Although the model captures global re"
C10-1050,W02-1001,0,\N,Missing
C10-1050,P02-1040,0,\N,Missing
C10-1050,J07-2003,0,\N,Missing
C16-1021,J08-1001,0,0.801524,"nguages without much efforts. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies"
C16-1021,H01-1065,0,0.106884,"to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based featu"
C16-1021,N10-1099,0,0.0837394,"s. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it con"
C16-1021,N13-1136,0,0.275446,"ion. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relations between sentences (both inter and intra-document relations). Christensen et al. (2013) reported ROUGE scores lower than some baselines. This is because that, they claim, ROUGE is salience-focused and fails to notice the improvement in coherence. In a further human evaluation, they reported improvements in readability. These discourse-based methods without exception have discourse analysis as a prere"
C16-1021,P02-1057,0,0.0628032,"Missing"
C16-1021,W04-1017,0,0.029919,"of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativeness. The first one is based on global coherence and the second one local coherence. The global coheren"
C16-1021,W07-2321,0,0.0377804,"to the grammatical roles of Entity j in Sentence i. M (T ) is referred to as the Entity Grid of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativ"
C16-1021,P15-1056,0,0.0147264,"Hence the summarization systems need to identify important information and keep as much of it as possible. Most existing research follows such a guideline and takes salience as its sole focus. Salience-focused systems cannot guarantee the readability of the generated text as they fail to take coherence into consideration. Sentence reordering, as a post processing task has began to develop. Apparently, it cannot make up for the flaws of salience-focused systems because it is simply a reorganization of sentences. Besides, it also faces problems when dealing with temporal text (Yan et al., 2011; Ge et al., 2015). A better solution is to consider coherence when selecting sentences. Such comprehensive models have been proposed. Most of them are discourse driven and sacrifice informativeness for coherence. In this sense, our model is novel in dealing with coherence without discourse analysis. 219 5.1 Salience-Focused Method As stated, the summarization systems need to identify the important information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to sel"
C16-1021,W09-1802,0,0.11699,"07; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a c"
C16-1021,J95-2003,0,0.89001,"ted improvements in readability. These discourse-based methods without exception have discourse analysis as a prerequisite. As we all know, discourse analysis is still under development thus preventing the expected improvement. Furthermore, languages other than English do not enjoy plenty of ready-to-use discourse analysis tools. This also limits the usage of these discourse-based methods. Is it possible to consider coherence in summarization without discourse analysis? Before answering this question, we need to find out what is the key to coherence in text. According to the centering theory (Grosz et al., 1995; Walker et al., 1998), the coherence of text is to a large extent maintained by entities and the relations between them. This indicates that discourse analysis is not a must to preserve coherence; we can directly take advantages of entities and their relations to generate coherence text. Based on this point, we design a novel graph-based model for multi-document summarization that eliminates the effort of conducting discourse relation analysis (inter or intra document) and generates informative and readable summaries. We formulate the document set as a graph whose nodes corresponds to sentenc"
C16-1021,D13-1158,1,0.937019,"uation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relatio"
C16-1021,hong-etal-2014-repository,0,0.0620989,"multi-document summarization systems using ROUGE and human evaluation. The former aims to evaluate informativeness and the latter targets readability. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (20"
C16-1021,P14-1002,0,0.0177133,"s viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse analysis and hence can be"
C16-1021,D14-1218,0,0.019942,"piration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it contains and how their"
C16-1021,P13-2099,0,0.0150176,"d methods face some major challenges. One is informativeness, which means we need to maintain the important information of source documents in summaries. This is the focus of almost all research on summarization. Another challenge is presentation, namely that the extracted text should be well presented, i.e., it should contain little redundancy and be coherent so as to be readily understandable. Previous work has addressed the problem of redundancy, and some successful solutions like Maximum Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) have been proposed and widely adopted (e.g., (Li and Li, 2013)), but very few try to deal with coherence. Therefore the generated summaries generally suffer as regards readability and are very difficult to use for practical applications. In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons."
C16-1021,D14-1220,0,0.0209026,"et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse"
C16-1021,P11-1052,0,0.0900013,"., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coher"
C16-1021,W04-1013,0,0.0375736,". In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ 1 The baseline they used is the lead paragraph method and summaries are evaluated by human and ROUGE (Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013"
C16-1021,P14-5010,0,0.00489084,"Missing"
C16-1021,W98-1124,0,0.0172051,"s for sentence selection/compression. The problem is that these discourse based features usually play secondary roles, because the models all try to improve information coverage, which are evaluated by ROUGE. And ROUGE, as is commonly known, is not sensitive to coherence. Some others work directly on discourse analysis results, and they usually try to derive a passage from a given parse tree. The problem of summarization is regarded as finding a text T so that T = arg max F (T |T r) for a given tree T r. Here F is the objective function. Early representative work of this kind includes that of Marcu (1998) and that of Daum´e III and Marcu (2002). Recently, Hirao et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingl"
C16-1021,C04-1108,0,0.024334,"ing diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based features for sentence selection/compression. The pro"
C16-1021,radev-etal-2004-mead,0,0.346146,"y. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (2009). Here we follow the same steps with Takamura and Okumura (2009). 5 218 our systems, only nouns and pronouns are regarded as entities. There are"
C16-1021,C10-1111,0,0.0287562,"he length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from"
C16-1021,E09-1089,0,0.42513,"we assign a cost score, which is the number of words the corresponding sentence contains. To each path in the directed graph, we assign a gain score. The gain score is a comprehensive evaluation of the informativeness and coherence of the sequence of sentences represented by the path. The problem of extracting a good summary becomes the problem of extracting the best path. Note that it is an asymmetric graph. Gain scores for A → B → C and C → B → A are different. The direction determines the positions of corresponding sentences in the generated text. 2 In some previous work on summarization (Takamura and Okumura, 2009; Hirao et al., 2013), concepts are used to measure informativeness. Concepts can be used to refer any non functional words, including adjectives, adverbs. All the entities can be regarded as concepts, but some concept words (non-nominal words) are not entities. Entity is a subset of Concept. 215 One more thing to consider is the redundancy. Instead of formulating redundancy explicitly, we remove edges connecting similar sentences to turn the complete graph into an incomplete graph. This ensures that similar sentences do not occupy adjacent positions in the generated summaries and thus reduce"
C16-1021,P07-1070,0,0.0269097,"ant information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to select sentences that are most relevant but not too similar to the already selected ones. It tries to keep a balance between relevance and redundancy. MMR is also widely employed to avoid redundancy in summarization systems. Among existing research, one popular kind is the ranking method (e.g., Textrank (Mihalcea and Tarau, 2004), Lexrank (Erkan and Radev, 2004) and its variants (Wan et al., 2007; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gi"
C16-1021,W04-3252,0,\N,Missing
D10-1092,W05-0909,0,0.242163,"Missing"
D10-1092,W10-1749,0,0.0898134,"on of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. 951 Table 3: NTCIR-7 √ meta-evaluation: Effects of square root (b(x) = 1 − 1 − x) √ NKT NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as “A because B” vs. “B because A.” To penal"
D10-1092,E06-1032,0,0.0197708,"a-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an auto"
D10-1092,W07-0718,0,0.0467588,"f sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. 949 edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ⇒ English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 4.1 Results Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WE"
D10-1092,I05-2014,0,0.0369692,"l by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associat"
D10-1092,2007.mtsummit-papers.21,0,0.0340785,"eni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers"
D10-1092,W10-1736,1,0.720649,") was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s ρ and Kendall’s τ (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s τ to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. It is not clear how well τ works as an automatic evaluation metric of translation quality. Moreover, Spearman’s ρ might work better than Kendall’s τ . As we discuss later, τ considers only the direction of the rank change, whereas ρ considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second objec"
D10-1092,N03-1020,0,0.0120162,"tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundar"
D10-1092,P02-1040,0,0.100666,"ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A"
D10-1092,2006.amta-papers.25,0,0.103847,"evity Penalty (BP) min(1, exp(1 − r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP × (p1 p2 p3 p4 )1/4 . Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0×(11/11×9/10×6/9×4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks"
D10-1092,N03-2021,0,\N,Missing
D13-1021,2010.iwslt-papers.7,0,0.33004,"data (in case of sample-wise noise, we assume the noise is in the beginning). This assumption derives a constraint between signal and noise parts that helps to avoid a welter of transliteration and non-transliteration parts. It also has a shortcoming that it is generally noise t h エ noise e sp e ッ t c チ ン グ h i n sp g マ sp ス m ク a s t noiseB t noiseB noise k s s t noiseB t noiseB s s s t signal t signal Figure 2: Example of many-to-many alignment with partial noise in the beginning and end. “noise” stands for the noise symbol and “sp” stands for a white space. s 3.3 Constrained Gibbs sampling Finch and Sumita (2010) used a blocked Gibbs sampling algorithm with forward-filtering backwardsampling (FFBS) (Mochihashi et al., 2009). We extend their algorithm for our noise-aware model using a state-based calculation over the three states: non-transliteration part in the beginning (noiseB), transliteration part (signal), non-transliteration part in the end (noiseE). Figure 3 illustrates our FFBS steps. At first in the forward filtering, we begin with transition to noiseB and signal. The calculation of forward probabilities itself is almost the same as Finch and Sumita (2010) except for state transition constrai"
D13-1021,P09-1012,0,0.0328726,"nt between signal and noise parts that helps to avoid a welter of transliteration and non-transliteration parts. It also has a shortcoming that it is generally noise t h エ noise e sp e ッ t c チ ン グ h i n sp g マ sp ス m ク a s t noiseB t noiseB noise k s s t noiseB t noiseB s s s t signal t signal Figure 2: Example of many-to-many alignment with partial noise in the beginning and end. “noise” stands for the noise symbol and “sp” stands for a white space. s 3.3 Constrained Gibbs sampling Finch and Sumita (2010) used a blocked Gibbs sampling algorithm with forward-filtering backwardsampling (FFBS) (Mochihashi et al., 2009). We extend their algorithm for our noise-aware model using a state-based calculation over the three states: non-transliteration part in the beginning (noiseB), transliteration part (signal), non-transliteration part in the end (noiseE). Figure 3 illustrates our FFBS steps. At first in the forward filtering, we begin with transition to noiseB and signal. The calculation of forward probabilities itself is almost the same as Finch and Sumita (2010) except for state transition constraints: from noiseB to signal, from signal to noiseE. The backward-sampling traverses a path by probabilitybased sam"
D13-1021,P02-1040,0,0.0872961,"Missing"
D13-1021,P12-1049,0,0.282575,"ration in patent domain using patent bilingual corpora. 1 Introduction Transliteration is used for providing translations for source language words that have no appropriate counterparts in target language, such as some technical terms and named entities. Statistical machine transliteration (Knight and Graehl, 1998) is a technology to solve it in a statistical manner. Bilingual dictionaries can be used to train its model, but many of their entries are actually translation but not transliteration. Such non-transliteration pairs hurt the transliteration model and should be eliminated beforehand. Sajjad et al. (2012) proposed a method to identify such non-transliteration pairs, and applied it successfully to noisy word pairs obtained from automatic word alignment on bilingual corpora. It enables the statistical machine transliteration to be bootstrapped from bilingual corpora. This approach is beneficial because it does not require carefullydeveloped bilingual transliteration dictionaries and it can learn domain-specific transliteration patterns This paper proposes a novel transliteration mining method for such partial transliterations. The method uses a noise-aware character alignment model that distingu"
D13-1021,W10-2402,0,\N,Missing
D13-1021,J98-4003,0,\N,Missing
D13-1139,P05-1033,0,0.11802,"uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for in"
D13-1139,P04-1015,0,0.056235,"wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expands Y and Z in a reverse order, and the leftmost word of X# is set to wlef t0 of Z, and the rightmost word of X# is set to wright1 of Y. Variable a is set to a0 of Z. We use a linear model that is discriminatively trained with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English traini"
D13-1139,P05-1066,0,0.27903,"Missing"
D13-1139,D11-1018,0,0.218258,"30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features a"
D13-1139,P06-1121,0,0.202823,"tax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction g"
D13-1139,P12-2061,0,0.351117,"arget Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when usin"
D13-1139,N04-1014,0,0.0387663,"ction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce"
D13-1139,P10-1110,0,0.0256258,"j, S|s0 ⟩ : π where s′0 is {X, h, wlef t , wright , a} and s0 is {X, h, wlef t , wright , x} (i ≤ h, lef t, right < j). The side condition prevents the parser from inserting articles into phrase X more than twice. During parsing, articles are not explicitly inserted into the input string: they are inserted into it when backtracking to generate a reordered string after parsing. The reduce-MR-X action has a deduction rule: X→YZ∈P ∧q ∈π z q }| { : ⟨k, i, S|s′2 |s′1 ⟩ : π ′ ℓ : ⟨i, j, S|s′1 |s′0 ⟩ : π ℓ + 1 : ⟨k, j, S|s′2 |s0 ⟩ : π ′ 1 Since our notion of predictor states is identical to that in (Huang and Sagae, 2010), we omit the details here. 1384 s0 = {X, h0 , wlef t1 , wright0 , a1 }. New nonterminal X is lexicalized with head word wh0 of right nonterminal Z. This action expands Y and Z in a straight order. The leftmost word of phrase X is set to leftmost word wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expa"
D13-1139,P07-2045,0,0.0104477,"ift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word r"
D13-1139,J08-1002,0,0.0206303,"with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used"
D13-1139,N07-1051,0,0.0337815,"articles “ga” “wo” “wa” into English sentences. We privilege the nonterminals of a phrase modified by a deleted article to determine which “the” “a/an” or “no articles” should be inserted at the front of the phrase. Note that an original English sentence can be recovered from its HFE tree by using # symbols and annotated articles and deleting Japanese particles. As well as Goto et al. (2012), we solve postordering by a parser whose model is trained with a set of HFE trees. The main difference between Goto et al. (2012)’s model and ours is that while the former simply used the Berkeley parser (Petrov and Klein, 2007), our shift-reduce parsing model can use such non-local task specific features as the N -gram words of reordered strings without sacrificing efficiency. Our method integrates postediting (Knight and Chander, 1994) with reordering and inserts articles into English translations by learning an additional “insert” action of the parser. Goto et al. (2012) solved the article generation problem by using an 1383 N -gram language model, but this somewhat complicates their approach. Compared with other parsers, one advantage of the shift-reduce parser is to easily define such additional operations as “i"
D13-1139,2011.mtsummit-papers.36,1,0.941844,". 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered str"
D13-1139,J97-3002,0,0.358339,"chieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when using these non-local features, the complexity of the shift-reduce parser does not increase at all due to give up achieving an optimal solution. Therefore, it works much more efficient. In our experiments, we apply our proposed method to postordering for J-to-E patent tasks becaus"
D19-5601,W18-2716,0,0.0593982,"Missing"
D19-5601,W04-1013,0,0.0851055,"-text NLG and MT along two axes: • MT+NLG: RotoWire, WMT19, Monolingual RotoWire refers to the RotoWire dataset (Wiseman et al., 2017) (train/valid), WMT19 refers to the set of parallel corpora allowable by the WMT 2019 English-German task, and Monolingual refers to monolingual data allowable by the same WMT 2019 task, pre-trained embeddings (e.g., GloVe (Pennington et al., 2014)), pre-trained contextualized embeddings (e.g., BERT (Devlin et al., 2019)), pre-trained language models (e.g., GPT-2 (Radford et al., 2019)). Textual Accuracy Measures: We used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for texutal accuracy compared to reference summaries. Content Accuracy Measures: We evaluate the fidelity of the generated content to the input data using relation generation (RG), content selection (CS), and content ordering (CO) metrics (Wiseman et al., 2017). 2 model for both languages together, using a shared BPE vocabulary obtained from target game summaries and by prefixing the target text with the target language indicator. For MT and MT+NLG tracks, they mined the in-domain data by extracting basketball-related texts from Newscrawl when one of the following conditions are m"
D19-5601,D14-1162,0,0.0824718,"Missing"
D19-5601,D15-1044,0,0.0588225,"types of inputs. The results of the shared task are summarized in Sections 3 and 4. Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 3rd Workshop on Neural Machine Translation and Generation (WNGT 2019) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: this year we continued to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this Summary of Research Contributions We published a call for long papers, extended abstracts for pre"
D19-5601,W18-6502,0,0.0159592,"ocument-level Generation and Translation # documents Avg. # tokens (En) Avg. # tokens (De) Vocabulary size (En) Vocabulary size (De) The first shared task at the workshop focused on document-level generation and translation. Many recent attempts at NLG have focused on sentencelevel generation (Lebret et al., 2016; Gardent et al., 2017). However, real world language generation applications tend to involve generation of much larger amount of text such as dialogues or multisentence summaries. The inputs to NLG systems also vary from structured data such as tables (Lebret et al., 2016) or graphs (Wang et al., 2018), to textual data (Nallapati et al., 2016). Because of such difference in data and domain, comparison between different methods has been nontrivial. This task aims to (1) push forward such document-level generation technology by providing a testbed, and (2) examine the differences between generation based on different types of inputs including both structured data and translations in another language. In particular, we provided the following 6 tracks which focus on different input/output requirements: Valid Test 242 323 320 4163 5425 240 328 324 - 241 329 325 - Table 1: Data statistics of Roto"
D19-5601,D13-1176,0,\N,Missing
D19-5601,D15-1199,0,\N,Missing
D19-5601,P02-1040,0,\N,Missing
D19-5601,P10-2041,0,\N,Missing
D19-5601,W14-3302,0,\N,Missing
D19-5601,D17-1239,0,\N,Missing
D19-5601,W14-7001,0,\N,Missing
D19-5601,W17-4717,0,\N,Missing
D19-5601,W17-3518,0,\N,Missing
D19-5601,N19-1423,0,\N,Missing
I11-1004,J08-1002,0,0.059746,"show that our proposal can significantly improve BLEU scores of 2.47∼3.15 points compared with using the original English sentences. We finally conclude this paper by summarizing our proposal and the experiment results. Specially, for English-to-Japanese translation, Isozaki et al. (2010b) proposed to move syntactic or semantic heads to the end of corresponding phrases or clauses so that to yield head finalized English (HFE) sentences which follow the word order of Japanese. The head information of an English sentence is detected by a head-driven phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bi"
I11-1004,J03-1002,0,0.00568597,"ese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree. Note that the identifiers that start with ‘c’ 2 These word alignments are gained by running GIZA++ (Och and Ney, 2003) on the original parallel sentences. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 30 c0 &lt;tok id=t0 cat=SC pos=WRB base=when lexentry=[when] pred=conj_arg12 arg1=c16 arg2=c3&gt; c0 c1 c1 c3 &lt;cons id=c16 cat=S xcat= head=c18 sem_head=c18 schema=mod_head&gt; t0 &lt;cons id=c3 cat=S xcat= head=c13 sem_head=c13 schema=subj_head&gt; c18 c6 c21 c8 c13 c10 c7 c5 c3 c2 c16 c4 c2 c16 c9 c11 t0 t1 t2 t3 when the fluid pressure c12 t4 cylinder t6 31 is c23 c15 c17 c20 c14 t5 c19 t7 t8 t9 used , c22 c24 c25 t10 t11 t12 is gradually applied fluid . 流体 圧 シリンダ 31 の 場合 は 流体 が 徐々に 排出 さ れる こと と なる 。 0 1 fluid pressu"
I11-1004,P05-1033,0,0.0598702,"otone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu"
I11-1004,P03-1021,0,0.0213618,"topological order do if n is a terminal node then n.srcPhrase ← E[n.srcSpan[0]] else if n.srcPhrase = NULL then n.srcPhrase ← C ONNECT(n.children().srcPhrase) end if end for 3 Experiments 3.1 Setup We test our proposal by translating from English to Japanese. We use the NTCIR-9 English-Japanese patent corpus4 as our experiment set. Since the reference set of the official test set has not been released yet, we instead split the original development set averagely into two parts, named dev.a and dev.b. In our experiments, we first take dev.a as our development set for minimum-error rate tuning (Och, 2003) and then report the final translation accuracies on dev.b. For direct comparison with other systems in the future, we use the configuration of the official baseline system5 : • Moses6 (Koehn et al., 2007): revision = “3717” as the baseline decoder. Note that we also train Moses using HFE sentences (Isozaki et al., 2010b) and the English sentences pre-ordered by PASs; Algorithm 2 sketches the algorithm for applying pre-ordering rules to a given HPSG tree TE . The algorithm contains three parts: rule matching (Lines 4-12), bottom-up rule applying (Lines 13-19), and sentence collecting (Lines 20"
I11-1004,P05-1066,0,0.180454,"jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to"
I11-1004,P02-1040,0,0.0821751,"Missing"
I11-1004,N04-1035,0,0.311955,"mmunication Science Laboratories, NTT Corporation 2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan {wu.xianchao,sudoh.katsuhito,kevin.duh,tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract orientations (monotone or reordering) with probabilities for each bilingual phrase from the training data. For example, by taking lexical information as features, a maximum entropy phrase reordering model was proposed by Xiong et al. (2006). Second, syntax-based models attempt to solve the word ordering problem by employing syntactic structures. For example, linguistically syntaxbased approaches (Galley et al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et"
I11-1004,C10-1043,0,0.299656,"syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-S"
I11-1004,2007.tmi-papers.21,0,0.143543,"Missing"
I11-1004,D10-1092,1,0.904154,"Missing"
I11-1004,N04-4026,0,0.272773,"Missing"
I11-1004,W10-1736,1,0.754264,", in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-aligned HPSG-tree-tostring"
I11-1004,C96-2141,0,0.339493,"al., 2004; Liu et al., 2006) first parse source and/or target sentences and then learn reordering templates from the subtree fragments of the parse trees. In contrast, hierarchical phrase based translation (Chiang, 2005) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic b"
I11-1004,D07-1077,0,0.403469,"e far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), and English-to-SOV languages (Xu et al., 2009). Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word"
I11-1004,P10-1034,1,0.926742,"dering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content which corresponds to PASs. In order to record the relative positions among a predicate word and its argument phrases, we propose a linear-time algorithm to extract preordering rules from word-ali"
I11-1004,P07-2045,0,0.0692904,"eline SMT systems. 1 Introduction Statistical machine translation (SMT) suffers from an essential problem for translating between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) i"
I11-1004,2006.iwslt-evaluation.11,1,0.751145,"is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena: • predicate words and argument phrases respectively record reordering phenomena in a lexicalized level and an abstract level; • PASs provide a fine-grained classification of the reordering phenomena since they include factored representations of syntactic features of the predicate words and their argument phrases. 2 Pre-ordering Rule Extraction and Application 2.1 An example The idea of using PASs for pre-ordering follows (Komachi et al., 2006). Several reordering operations were manually designed by Komachi et al. (2006) to pre-ordering Japanese sentences into SVO-style English sentences. For comparison, our proposal 1) makes use of not only PASs but also the source syntactic tree structures for preordering rule matching, 2) extracts pre-ordering 1 Figure 1 shows a word-aligned HPSG-tree-tostring pair for English-to-Japanese translation. PASs among lexical nodes and their argument nodes in this HPSG tree are described by arrows in thick-lines. For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG tree."
I11-1004,C04-1073,0,0.419443,"phrase structure grammar (HPSG) parser, Enju1 (Miyao and Tsujii, 2008). In addition, transformation rules were manually written for appending particle seed words, refining POS tags to be used before parsing, and deleting English determiners. Due to the usage of the same parser, we take this HFE approach as one of our baseline systems. The goal in this paper, however, is to learn preordering rules from parallel data in an automatic way. Under this motivation, pre-ordering rules can be extracted in a language-independent manner. A number of researches follow this automatic way. For example, in (Xia and McCord, 2004), a variety of heuristic rules were applied to bilingual parse trees to extract pre-ordering rules for French-English translation. Rottmann and Vogen (2007) learned reordering rules based on sequences of part-of-speech (POS) tags, instead of parse trees. Dependency trees were used by Genzel (2010) to extract source-side reordering rules for translating languages from SVO to SOV, etc.. The novel idea expressed in this paper is that, predicate-argument structures (PASs) are introduced to extract fine-grained pre-ordering rules. PASs have the following merits for describing reordering phenomena:"
I11-1004,P06-1066,0,0.17735,"Missing"
I11-1004,H05-1021,0,0.0214174,"between languages with substantial structural differences, such as between English which is a subject-verbobject (SVO) language and Japanese which is a typical subject-object-verb (SOV) language. Numerous approaches have been consequently proposed to tackle this word-order problem, such as lexicalized reordering methods, syntax-based models, and pre-ordering ways. First, in order to overcome the shortages of traditional distance based distortion models (Brown et al., 1993; Koehn et al., 2007), phrase dependent lexicalized reordering models were proposed by several researchers (Tillman, 2004; Kumar and Byrne, 2005). Lexicalized reordering models learn local 29 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 29–37, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP rules in an automatic way, and 3) use factored representations of syntactic features to refine the preordering rules. Following (Wu et al., 2010a; Isozaki et al., 2010b), we use the HPSG parser Enju to generate the PASs of English sentences. HPSG (Pollard and Sag, 1994) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure"
I11-1004,N09-1028,0,0.247032,"05) is a formally syntax-based approach which can automatically extract hierarchical ordering rules from aligned string-string pairs without using additional parsers. These approaches have been proved to be both algorithmically appealing and empirically successful. However, most of current syntax-based SMT systems use IBM models (Brown et al., 1993) and hidden Markov model (HMM) (Vogel et al., 1996) to generate word alignments. These models have a penalty parameter associated with long distance jumps, and tend to misalign words which move far from the window sizes of their expected positions (Xu et al., 2009; Genzel, 2010). The third type tackles the word-order problem in pre-ordering ways. Through the usage of a sequence of pre-ordering rules, the word order of an original source sentence is (approximately) changed into the word order of the target sentence. Here, the pre-ordering rules can be manually or automatically extracted. For manual extraction of pre-ordering rules, linguistic background and expertise are required for predetermined language pairs, such as for GermanEnglish (Collins et al., 2005), Chinese-to-English (Wang et al., 2007), Japanese-to-English (KatzBrown and Collins, 2007), a"
I11-1004,J93-2003,0,\N,Missing
I11-1153,N09-2019,0,0.0494692,"Missing"
I11-1153,P09-1064,0,0.033943,"4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s sco"
I11-1153,C10-1036,0,0.0320563,"Missing"
I11-1153,P07-2026,0,0.0184181,"extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate surrogate for the true posterior distribution p(e|f ). The second difficulty poses a particular problem for system combination. Although the assumption in Eq. 5 is reasonable for single-system MT, it becomes P unclear how to compare the model scores i λi hi (e, f ) in a multi-system setting. To illustrate, consider two MT systems, their 2-best lists, and corresponding model scores: • System A: e1 , score=7; e2 , score=3; • System B: e3 , score=90; e4 , score=10; It is unclear what is the ranking of post"
I11-1153,E06-1005,0,0.124381,"Missing"
I11-1153,P08-1023,0,0.0352485,"satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reordering, distortion=6 (Koehn and others, 2007) • Forest-to-string system (Mi et al., 2008) • Weighted finite-state Transducer (WFST) (Zhou et al., 2006) with rule-based reordering as preprocessing (Isozaki et al., 2010b). Each system generates a 100-best list, so our system combination task involves hypothesis selection out of 300 hypotheses. As evaluation measure, we focus on BLEU, Normalized Kendall’s Tau (NKT), a metric that has been shown to correlate well with humans on this language pair (Isozaki et al., 2010a)2 , and a combination thereof. The loss function used for MBR is therefore the sum of BLEU and NKT. For GMBR, the sub-components of this loss function are derived from"
I11-1153,N07-1029,0,0.0560152,"Missing"
I11-1153,D08-1065,0,0.0343379,"BR) decision rule. Eq. 4 is the N-best approximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming t"
I11-1153,D08-1011,0,0.0385247,"k e Lk (e k=1 θk Ck (e ), where P|e) =′ Ck (e′ ) = L (e |e) represents the come k ′ bined loss for e . So we first compute Ck (·) for all hypotheses, for an O(|N (f )|2 ) run-time. To find the GMBR P decision then requires a ′ search arg mine′ ∈N (f ) K k=1 θk Ck (e ). So in test, GMBR is on the same order as conventional MBR. To tune θ, we first extract all pairs of hypotheses where a difference exists in the true loss, then optimize θ in a formulation similar to RankSVM (Joachims, 2006). The pair-wise nature of Eq. 10 makes the problem amenable to solutions in “learning to rank” literature (He et al., 2008a). The pseudocode is shown in Algorithm 1. The RankSVM (line 8) tries to satisfy the relations (Eq. 10) in its constraints while allowing for some slack ξ, whose amount depends on hyperparameter c. 1358 4 Experiments We experiment with the NTCIR-9 (2011) Englishto-Japanese Patent Translation task1 . This includes 3 million sentences for training individual MT systems; the official dev set is split into 1000 sentences for MERT of individual systems, 500 for system combination optimization (MBR, GMBR), and 500 for final evaluation. We combine three systems: • Phrase-based Moses with lexical reo"
I11-1153,D10-1092,1,0.905756,"Missing"
I11-1153,W10-1736,1,0.870667,"Missing"
I11-1153,P07-2045,0,0.00573936,"Missing"
I11-1153,N04-1022,0,0.0385613,"e loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality. 1 Introduction Minimum Bayes Risk (MBR) is a theoreticallyelegant decision rule that has been used for singlesystem decoding and system combination in machine translation (MT). MBR arose in Bayes decision theory (Duda et al., 2000) and has since been applied to speech recognition (Goel and Byrne, 2000) and machine translation (Kumar and Byrne, 2004). The idea is to choose hypotheses that minimize Bayes Risk as oppose to those that maximize posterior probability. This enables the use of taskspecific loss functions (e.g BLEU). However, the definition of Bayes Risk depends critically on the posterior probability of hypotheses. In single-system decoding, one could approximate this probability using model scores. However, for system combination, the various systems 2 The Difficulty with MBR Consider the task of translation from a French sentence (f ) to an English sentence (e). Our goal is to find a decision rule δ(f ) → e′ , which takes f as"
I11-1153,P09-1019,0,0.0167692,"oximation commonly used in practice: N (f ) contains the set of hypotheses in the N-best list, and the argmin and sum is only performed within this finite set. There are two difficulties with Eq. 4: 1. The N-best approximation is much smaller than the true space of all English hypotheses in the argmin and sum of Eq. 3. The approximation in the argmin causes search errors, while the approximation in the sum introduces bias. This problem can be somewhat mitigated by increasing the N-best list size or extending this space using lattices and hypergraphs (Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009). We do not address this issue here. 2. The posterior probability p(e|f ) in Eq.3 and Eq. 4 refers to the true posterior probability arising from Ep(e,f ) [·] in the derivation of Eq. 2. In practice, this can only be estimated from the MT decoder’s model scores: P (exp i λi hi (e, f ))α P p(e|f ) ≈ P ′ α e′ ∈N (f ) (exp i λi hi (e , f )) (5) where hi (e, f ) are features, λi are feature weights, and α is a scaling factor that determines the flatness of the posterior distribution (Ehling et al., 2007). It is important to emphasize that we are assuming that the decoder’s score is an accurate sur"
I13-1147,P05-1066,0,0.15332,"2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reordering"
I13-1147,W08-0509,0,0.0229865,"million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-orderi"
I13-1147,P11-1081,0,0.0681856,"used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Tabl"
I13-1147,D10-1092,1,0.935952,"Missing"
I13-1147,W10-1736,1,0.956068,"statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reorderings. Our claims in this paper are summarized as follows: 1. The inter-chunk pre-ordering"
I13-1147,P03-1021,0,0.0888984,"en-PatentMT.html 4 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 5 Since (unlike English) the Japanese language does not utilize spaces to delineate word boundaries, MeCab was used to perform the required Japanese tokenization. 6 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP 7 http://code.google.com/p/cabocha/ 8 http://www.cl.cs.titech.ac.jp/~ryu-i/ syncha/ Baseline Proposed BLEU 15.03 16.12 RIBES 62.71 69.30 Table 4: Results within a News Domain. 9 the following configurations are used in the system: 6gram for language modeling, msd-bidirectional-fe for reordering, and MERT (Och, 2003) for tuning. After reviewing our preliminary findings, distortion limits were set to 20 for the baseline and (Komachi et al., 2006), and 10 for others. 10 http://www.phontron.com/lader/ 11 Only 10,000 sampled lines were used for training due to its computational complexity: During the training process, it consumed 120 GB of memory space for almost entire month. 1064 improvement in RIBES by adding Rule 2 to Rule 1-2 and Rule 1-3, even thought this combination yields better translations for native speakers. This phenomenon can be explained by the characteristic difference between BLEU and RIBES."
I13-1147,P02-1040,0,0.094191,"e sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-ordering), Komachi et al. (2006), Katz-Brown and Collins (2008),Neubig et al. (2012), our pr"
I13-1147,I11-1085,0,0.0287568,"ents 4.1 Experimental Setup In order to compare pre-ordering methods, we conducted Japanese-to-English translation experiments on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2"
I13-1147,P03-1010,0,0.132275,"Missing"
I13-1147,C04-1073,0,0.317275,"ab.ntt.co.jp Abstract 2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially"
I13-1147,P07-2045,0,0.00751715,"ng, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-"
I13-1147,2006.iwslt-evaluation.11,1,0.39014,"ng that relies on PAS analysis contributes to improvements in translation quality. 2. The intra-chunk pre-ordering which converts postpositional phrases into prepositional phrases further improves translation quality. 3. Thus, our two-stage framework is more effective than other pre-ordering methods. Japanese-to-English is challenging because the grammatical forms of the two languages are totally dissimilar. For instance, English is a head-initial language, and utilizes subject-verb-object (SVO) word orders, while Japanese is a pure head-final language, and utilizes subject-object-verb (SOV). Komachi et al. (2006) proposed a rule-based pre-ordering method to convert SOV into SVO via a PAS analyzer. This method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via th"
I13-1147,W02-2016,0,0.24869,"nts on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use"
I13-1147,D12-1077,0,0.131501,"is method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via the dependency analyzer. In the second stage, each chunk word order is naively reversed2 . Neubig et al. (2012) proposed a statistical model that was capable of learning how to pre-order word sequences from human annotated or automatically generated alignment data. However, this method has very large computational complexity to model long distance reordering. 3 Two-stage Pre-ordering Method Here, we describe a new pre-ordering method which employs heuristic rules in two-stages. In the first stage, we reorganize and extend the rules described in (Komachi et al., 2006; Katz-Brown and Collins, 2008). In the second stage, we propose a new rule to consider chunk internal word orders. More precisely, we appl"
N15-1030,P11-2037,0,0.406359,"trube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in cap"
N15-1030,D10-1062,0,0.0133031,"”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parse"
N15-1030,N06-1024,0,0.165919,"on Chinese Treebank prove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and"
N15-1030,P14-1136,0,0.0985971,"features. The distributed representations of the words would be placed at the corresponding positions in the feature vector and the remaining are set to 0. Previous work usually involves lots of syntactic and semantic features. In the work of (Xue and Yang, 2013), 6 kinds of features are used, including those derived from constituency parse trees, dependency parse trees, semantic roles and others. Here we use only the dependency parse trees for the feature extraction. The words in dependency paths we use have proven their potential in representing the meanings of text in frame identification (Hermann et al., 2014). 1. The head word (except the dummy root node). Suppose words are represented using d dimension vectors, we need d elements to represent this feature. The distributed representations of the head word would be placed at the corresponding positions. Take the OP in the sentence shown in Fig. 3 for example. For the OP, its head word is “的”, its following word is “告别” and its nephews are “NULL” and “NULL” (ECs are invisible). 2. The following word in the text. This feature is extracted using the same method with head words. −−−−→ 的/DE −−−−−→ OP 3. “Nephews”, the sons of the following word. We choo"
N15-1030,P11-1081,0,0.146437,"eatures but does not reflect the grammatic constraints of languages. For example, simple sentences in Chinese contain one and only one subject, whether it is an EC or not. If it is decided there is an EC as a subject in a certain place, there should be no more ECs as subjects in the same sentence. But such an important property is not reflected in these classification models. Methods that adopt parsing techniques take the whole parse tree as input and output a parse tree with EC anchored. So we can view the sentence as a whole and deal with ECs with regarding to all the words in the sentence. Iida and Poesio (2011) also take the grammar constraints into consideration by formulating EC detection as an ILP problem. But they usually yield poor performances compared with classification methods partly because the methods they use can not fully explore the syntactic and semantic features. 5 Related Work Empty category is a complex problem (Li and Hovy, 2015). Existing methods for EC detection mainly explores syntactic and semantic features using classification models or parsing techniques. Johnson (2002) proposes a simple pattern based algorithm to recover ECs, both the positions and their antecedents in phra"
N15-1030,P02-1018,0,0.110192,"ements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates t"
N15-1030,D13-1028,0,0.0125519,"ction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue,"
N15-1030,D10-1086,0,0.159883,"EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations"
N15-1030,D14-1218,0,0.0167932,"arsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations and neural networks in predicting the locations and types of ECs. We formulate the EC detection as an annotation 263 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263–271, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics task, to assign predefined labels (EC types) to given contexts. Recently, Weston et al. (2011) proposed a system taking advantages of the hidden representations of neural networks for image"
N15-1030,D14-1220,0,0.0146701,"features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper demonstrates the advantages of distributed representations and neural networks in predicting the locations and types of ECs. We formulate the EC detection as an annotation 263 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263–271, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics task, to assign predefined labels (EC types) to given contexts. Recently, Weston et al. (2011) proposed a system taking advantages of the hidden representations of neural networks for image annotation which i"
N15-1030,D15-1278,0,0.0213416,"Missing"
N15-1030,J93-2004,0,0.0493905,"tic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferab"
N15-1030,N06-1025,0,0.0166719,"rformance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al.,"
N15-1030,P13-1081,0,0.0120588,"ical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these featur"
N15-1030,N13-1125,0,0.0747831,"onstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of the EC (to which element the EC is linked to if plausible). Existing research mainly focuses on the first problem which is referred to as EC detection (Cai et al., 2011; Yang and Xue, 2010), and so is this paper. As ECs are words or phrases inferable from their context, previous work mainly designs features mining the contexts of ECs and then trains classification models or parsers using these features (Xue and Yang, 2013; Johnson, 2002; Gabbard et al., 2006; Kong and Zhou, 2010). One problem with these human-developed features are that they are not fully capable of representing the semantics and syntax of contexts. Besides, the feature engineering is also time consuming and labor intensive. Recently neural network models have proven their superiority in capturing features using low dense vector compared with traditional manually designed features in dozens of NLP tasks (Bengio et al., 2006; Collobert and Weston, 2008; Socher et al., 2010; Collobert et al., 2011; Li and Hovy, 2014; Li et al., 2014). This paper"
N15-1030,C10-2158,0,0.29338,"ove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB. 1 Introduction The empty category (EC) is an important concept in linguistic theories. It is used to describe nominal words that do not have explicit phonological forms (they are also called “covert nouns”). This kind of grammatical phenomenons is usually caused by the omission or dislocation of nouns or pronouns. Empty categories are the “hidden” parts of text and are essential for syntactic parsing (Gabbard et al., 2006; Yang and Xue, 2010). As a basic problem in NLP, the resolution of ECs also has a huge impact on lots of downstream tasks, such as co-reference resolution (Ponzetto and Strube, 2006; Kong and Ng, 2013), long distance dependency relation analysis (Marcus et al., 1993; Xue et al., 2005). Research also uncovers the important role of ECs in machine translation. Some recent work (Chung and Gildea, 2010; Xiang et al., 2013) demonstrates the improvements they manage to obtain through EC detection in Chinese-English translation. To resolve ECs, we need to decide 1) the position and type of the EC and 2) the content of th"
N15-1030,W03-1730,0,0.01828,"ian 军队/troops 31 日/31rd 举行/hold 了/past-tense-marker 告别/farewell 德 国/Germany 的/DE 最后/final 仪式/ceremony 。 Figure 3: ECs in a Dependency Tree File #pro #PRO #OP #T #RNR #* #Others Total Train 81-325, 400-454 500-554, 590-596 600-885, 900 1023 1089 2099 1981 91 22 0 6305 Dev 41-80 Test 1-40 901-931 166 210 301 287 15 0 0 979 297 298 575 527 32 19 0 1748 is learned using the word2vec toolkit (Mikolov et al., 2013). We train the model on a large Chinese news copora provided by Sogou2 , which contains about 1 billion words after necessary preprocessing. The text is segmented into words using ICTCLAS(Zhang et al., 2003)3 . 3.2 Experiment Settings Initialization WA is initialized according to unif orm[− din +d24hidden , din +d24hidden ]. And WB is initialized using 24 24 unif orm[− dhidden +dout , dhidden +dout ]. Here din , dhidden and dout are the dimensions of the input layer, the hidden space and the label space. Table 1: Data Division and EC Distribution 3 3.1 Experiments on CTB Data The proposed method can be applied to various kinds of languages as long as annotated corpus are available. In our experiments, we use a subset of Chinese Treebank V7.0. We split the data set into three parts, training, deve"
P06-1078,W03-0430,0,0.0527793,"Missing"
P06-1078,H01-1034,0,0.251074,"Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan {sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp Abstract expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while o"
P06-1078,C00-2167,0,0.0542981,"Missing"
P06-1078,W98-1120,0,0.0374624,", especially in precision, compared to simply applying text-based NER to the ASR results. 2 Manual transcription Speech data Transcriptions ASR ASR results NE labeling NE-labeled transcriptions Setting ASR confidence feature to 1 SVM-based NER Text-based training data NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is class"
P06-1078,W03-0423,0,0.163739,"to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word"
P06-1078,N04-4010,0,0.0913895,"on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results. Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words m"
P06-1078,H05-1062,0,0.023325,"Missing"
P06-1078,C02-1054,1,\N,Missing
P12-1001,P07-1111,0,0.0175966,"tion of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunabi"
P12-1001,W11-2103,0,0.162556,"These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good t"
P12-1001,N10-1080,0,0.0600026,"in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint"
P12-1001,N09-1025,0,0.0389566,", so the method not only learns to discriminate pareto vs. non-pareto but also also learns to discriminate among competing non-pareto points. Also, like other MT works, in line 5 the N-best list is concatenated to N-best lists from previous iterations, so {h} is a set with i · N elements. General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques. For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMO-MERT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-B"
P12-1001,W09-0426,0,0.266662,"ts. In this case, we recommend the following trick: Set up a multi-objective problem where one metric is BLEU and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogona"
P12-1001,I08-1042,0,0.0478355,"Missing"
P12-1001,D11-1138,0,0.0324528,"auser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limitations We introduce a new approach (PMO) for training MT systems on multiple metrics. Leveraging the diverse perspectives of different evaluation metrics ha"
P12-1001,2009.mtsummit-posters.8,0,0.158214,"and the other is 3/4BLEU+1/4RIBES. This encourages PMO to explore the joint metric space but avoid solutions that sacrifice too much BLEU, and should also outperform Linear Combination that searches only on the (3/4,1/4) direction. 5 How many Pareto points? The number of pareto 7 Related Work Multi-objective optimization for MT is a relatively new area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). I"
P12-1001,D11-1125,0,0.225156,"Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluat"
P12-1001,D10-1092,1,0.861646,"nce & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective optimization method that avoids “overfitting to a single metric”. We want to build a MT system"
P12-1001,P07-2045,0,0.0130597,"Missing"
P12-1001,W07-0734,0,0.0337259,"s BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is"
P12-1001,P06-1096,0,0.0438141,"RT4 . Alternatively, by using the largemargin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality. 4 Experiments 4.1 Evaluation Methodology We experiment with two datasets: (1) The PubMed task is English-to-Japanese translation of scientific 4 A difference with traditional MERT is the necessity of sentence-BLEU (Liang et al., 2006) in line 6. We use sentenceBLEU for optimization but corpus-BLEU for evaluation here. 5 abstracts. As metrics we use BLEU and RIBES (which demonstrated good human correlation in this language pair (Goto et al., 2011)). (2) The NIST task is Chinese-to-English translation with OpenMT08 training data and MT06 as devset. As metrics we use BLEU and NTER. • BLEU = BP × (Πprecn )1/4 . BP is brevity penality. precn is precision of n-gram matches. 1/4 • RIBES = (τ + 1)/2 × prec1 , with Kendall’s τ computed by measuring permutation between matching words in reference and hypothesis5 . • NTER=max(1−TER,"
P12-1001,N07-1006,0,0.0190011,"w area. Linear-combination of BLEU/TER is the most common technique (Zaidan, 2009), sometimes achieving good results in evaluation campaigns (Dyer et al., 2009). As far as we known, the only work that directly proposes a multi-objective technique is (He and Way, 2009), which modifies MERT to optimize a single metric subject to the constraint that it does not degrade others. These approaches all require some setting of constraint strength or combination weights {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question"
P12-1001,D11-1035,0,0.262905,"a choice between picking the best weight according to BLEU (BLEU=.265,RIBES=.665) vs. another weight with higher RIBES but poorer BLEU, e.g. (.255,.675). Nevertheless, both the PMO and Linear-Combination with various (p1 , p2 ) samples this joint-objective space broadly. 3. Interestingly, a multi-objective approach can sometimes outperform a single-objective optimizer in its own metric. In Figure 2, singleobjective PRO focusing on optimizing RIBES only achieves 0.68, but PMO-PRO using both BLEU and RIBES outperforms with 0.685. The third observation relates to the issue of metric tunability (Liu et al., 2011). We found that RIBES can be difficult to tune directly. It is an extremely non-smooth objective with many local optima–slight changes in word ordering causes large changes in RIBES. So the best way to improve RIBES is to 6 0.694 0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.164 bleu Figure 3: NIST Results not to optimize it directly, but jointly with a more tunable metric BLEU. The learning curve in Figure 4 show that single-objective optimization of RIBES quickly falls into local optimum (at iteration 3) whereas PMO can zigzag and sacrifice RIBES in intermediate iterations (e.g. iter"
P12-1001,D08-1076,0,0.0416377,"f combination weights. Further we observe that multiobjective approaches can be helpful for optimizing difficult-to-tune metrics; this is beneficial for quickly introducing new metrics developed in MT evaluation into MT optimization, especially when good {pk } are not yet known. We conclude by drawing attention to some limitations and opportunities raised by this work: Limitations: (1) The performance of PMO is limited by the size of the Pareto set. Small N-best lists lead to sparsely-sampled Pareto Frontiers, and a much better approach would be to enlarge the hypothesis space using lattices (Macherey et al., 2008). How to compute Pareto points directly from lattices is an interesting open research question. (2) The binary distinction between pareto vs. non-pareto points ignores the fact that 2nd-place non-pareto points may also lead to good practical solutions. A better approach may be to adopt a graded definition of Pareto optimality as done in some multi-objective works (Deb et al., 2002). (3) A robust evaluation methodology that enables significance testing for multi-objective problems is sorely needed. This will make it possible to compare multi-objective methods on more than 2 metrics. We also nee"
P12-1001,mauser-etal-2008-automatic,0,0.138462,"ts {pk }. Recent work in MT evaluation has examined combining metrics using machine learning for better correlation with human judgments (Liu and Gildea, 2007; Albrecht and Hwa, 2007; Gimnez and M`arquez, 2008) and may give insights for setting {pk }. We view our Pareto-based approach as orthogonal to these efforts. The tunability of metrics is a problem that is gaining recognition (Liu et al., 2011). If a good evaluation metric could not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011)"
P12-1001,P03-1021,0,0.424427,"e diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 1 Introduction Weight optimization is an important step in building machine translation (MT) systems. Discriminative optimization methods such as MERT (Och, 2003), MIRA (Crammer et al., 2006), PRO (Hopkins and May, 2011), and Downhill-Simplex (Nelder and Mead, 1965) have been influential in improving MT systems in recent years. These methods are effective because they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While ma"
P12-1001,W07-0714,0,0.0433798,"Missing"
P12-1001,P02-1040,0,0.0840642,"e for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that"
P12-1001,2010.iwslt-evaluation.1,0,0.022396,"ecause they tune the system to maximize an automatic evaluation metric such as BLEU, which serve as surrogate objective for translation quality. However, we know that a single metric such as BLEU is not enough. Ideally, we want to tune towards an automatic metric that has perfect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. T"
P12-1001,2006.amta-papers.25,0,0.0542521,"ect correlation with human judgments of translation quality. ∗ *Now at Nara Institute of Science & Technology (NAIST) While many alternatives have been proposed, such a perfect evaluation metric remains elusive. As a result, many MT evaluation campaigns now report multiple evaluation metrics (Callison-Burch et al., 2011; Paul, 2010). Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR (Lavie and Agarwal, 2007) allows for stem/synonym matching and incorporates recall. TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order. Syntax (Owczarzak et al., 2007) and semantics (Pado et al., 2009) also help. Arguably, all these metrics correspond to our intuitions on what is a good translation. The current approach of optimizing MT towards a single metric runs the risk of sacrificing other metrics. Can we really claim that a system is good if it has high BLEU, but very low METEOR? Similarly, is a high-METEOR low-BLEU system desirable? Our goal is to propose a multi-objective o"
P12-1001,D11-1117,0,0.0468041,"not be used for tuning, it would be a pity. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al., 2011). (Mauser et al., 2008; Cer et al., 2010) report similar observations, in addition citing WER being difficult and BLEU-TER being amenable. One unsolved question is whether metric tunability is a problem inherent to the metric only, or depends also on the underlying optimization algorithm. Our positive results with PMO suggest that the choice of optimization algorithm can help. Multi-objective ideas are being explored in other NLP areas. (Spitkovsky et al., 2011) describe a technique that alternates between hard and soft EM objectives in order to achieve better local optimum in grammar induction. (Hall et al., 2011) investigates joint optimization of a supervised parsing objective and some extrinsic objectives based on downstream applications. (Agarwal et al., 2011) considers using multiple signals (of varying quality) from online users to train recommendation models. (Eisner and Daum´e III, 2011) trades off speed and accuracy of a parser with reinforcement learning. None of the techniques in NLP use Pareto concepts, however. 6 Opportunities and Limit"
P12-2020,W06-2920,0,0.0329395,"ee of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003)"
P12-2020,J07-4004,0,0.0657609,"tsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency struc"
P12-2020,P97-1003,0,0.248621,"typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features in a leaf node to"
P12-2020,W07-2416,0,0.0959405,"ng. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predica"
P12-2020,P07-2045,0,0.00633941,"icate types to the gold-standard grammatical relations can be found in Table 13 in (Clark and 102 Curran, 2007). The post-processing is like that described for HPSG parsing, except we greedily use the MST’s sentence root when we can not determine it based on the CCG parser’s PASs. 3 Experiments 3.1 Setup We re-implemented the string-to-dependency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gi"
P12-2020,P95-1037,0,0.136107,"2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerman (1995) and Collins (1997). Similar approach has been originally used by Shen et al. (2008). 2.4 HPSG parsing In the Enju English HPSG grammar (Miyao et al., 2003) used in this paper, the semantic content of 3 4 http://nlp.cs.lth.se/software/treebank converter/ http://www.cs.columbia.edu/ mcollins/papers/heads 101 a sentence/phrase is represented by a PAS. In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. The arguments of a predicate are designated by the arrows from the argument features"
P12-2020,H94-1020,0,0.0614407,"Missing"
P12-2020,J11-1007,0,0.0363269,"age side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al., 2011; Ma and Matsoukas, 2011). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011). It is known that dependency-style structures can be transformed from a number of linguistic struc∗ † Now at Baidu Inc. Now at Nara Institute of Science & Technology (NAIST) 2 Gaining Dependency Structures 2.1 Dependency tree We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable"
P12-2020,P05-1012,0,0.0580344,"_ aux_ verb_ punct_ noun_ arg0 arg1 arg12 arg12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head find"
P12-2020,J08-1002,0,0.0193457,"o, Soraku-gun Kyoto 619-0237 Japan wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp, kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp Abstract tures. For example, using the constituent-todependency conversion approach proposed by Johansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using"
P12-2020,W03-3017,0,0.0491047,"g12 arg1 when the fluid pressure cylinder 31 * c19 c13 c22 c24 c25 t10 t11 t12 aux_ arg12 adj_ arg1 verb_ arg12 is gradually applied . Figure 1: HPSG tree of an example sentence. ‘*’/ ‘+’=syntactic/semantic heads. Arrows in red (upper)= PASs, orange (bottom)=word-level dependencies generated from PASs, blue=newly appended dependencies. both during rule extracting and target dependency language model (LM) training. 2.2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. The MST parser (McDonald et al., 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). Here, we compare them on the SMT tasks through parsing the real-world SMT data. 2.3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). The head finding rules4 are according to Magerm"
P12-2020,J03-1002,0,0.00290707,"pendency decoder described in (Shen et al., 2008). Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Ba"
P12-2020,P02-1040,0,0.0827304,"Missing"
P12-2020,P11-1027,0,0.01549,"as target dependency LMs. For intuitive comparison, an outside SMT system is Moses (Koehn et al., 2007). For Chinese-to-English translation, we use the parallel data from NIST Open Machine Translation Evaluation tasks. The training data contains 353,796 sentence pairs, 8.7M Chinese words and 10.4M English words. The NIST 2003 and 2005 test data are respectively taken as the development and test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) to obtain word alignments. The Berkeley Language Modeling Toolkit, berkeleylm1.0b35 (Pauls and Klein, 2011), was employed to train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). 3.2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. Basically, we investigate (1) if two dependency graphs of one sentence share the same root word and (2) if the head of one word in one sentence are identical in two dependency graph"
P12-2020,P08-1066,0,0.377419,"le trees. A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al., 2003) parser, Enju1 (Miyao and Tsujii, 2008) and (2) a state-of-the-art CCG parser2 (Clark and Curran, 2007). The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al., 2008). This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation a"
P12-2020,N07-1051,0,\N,Missing
P13-2119,D10-1092,1,0.436516,"Missing"
P13-2119,W12-3139,0,0.0208124,"Missing"
P13-2119,N06-2001,0,0.028358,"Missing"
P13-2119,W04-3250,0,0.464764,"Missing"
P13-2119,W12-2703,0,0.060629,"Missing"
P13-2119,D11-1033,0,0.191828,"vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large g"
P13-2119,P10-2041,0,0.175474,"language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 1 2 Data Selection Method We employ the data selection method of (Axelrod et al., 2011), which builds upon (Moore and Lewis, 2010). The intuition is to select general-domain sentences that are similar to indomain text, while being dis-similar to the average general-domain text. To do so, one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentenc"
P13-2119,C90-3038,0,0.351908,"with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007; Nakamura et al., 1990). Both types of neural LMs have seen many improvements recently, in terms of computational scalability (Le et al., 2011) and modeling power (Arisoy et al., 2012; Wu et al., 2012; Alexandrescu and Kirchhoff, 2006). We focus on recurrent networks here since there are fewer hyper-parameters and its ability to model infinite context using recursion is theoretically attractive. But we note that feedforward networks are just as viable. 2 The recurrent states are unrolled for several time-steps, then stochastic gradient descent is applied. 679 en-de en-es In-domain Training Set #sentence 129k 140k #t"
P13-2119,2012.eamt-1.60,0,0.0186427,"ontext as an identity (n-gram hit-or-miss) function on [w(t − 1), w(t − 2), . . .], neural LMs summarize the context by a hidden state vector s(t). This is a continuous vector of dimension |S |whose elements are predicted by the previous word w(t − 1) and previous state s(t − 1). This is robust to rare contexts because continuous representations enable sharing of statistical strength between similar contexts. Bengio (2009) shows that such representations are better than multinomials in alleviating sparsity issues. 3 Experiment Setup We experimented with four language pairs in the WIT3 corpus (Cettolo et al., 2012), with English (en) as source and German (de), Spanish (es), French (fr), Russian (ru) as target. This is the in-domain corpus, and consists of TED Talk transcripts covering topics in technology, entertainment, and design. As general-domain corpora, we collected bitext from the WMT2013 campaign, including CommonCrawl and NewsCommentary for all 4 languages, Europarl for de/es/fr, UN for es/fr, Gigaword for fr, and Yandex for ru. The indomain data is divided into a training set (for SMT 1 Another major type of neural LMs are the so-called feed-forward networks (Bengio et al., 2003; Schwenk, 2007"
P13-2119,2012.amta-papers.19,0,0.122261,"Missing"
P13-2119,2010.iwslt-papers.5,1,0.84743,"one defines the score of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure"
P13-2119,P02-1040,0,0.0858846,"Missing"
P13-2119,W12-2702,0,0.0173223,"Missing"
P13-2119,W12-3154,0,0.0129056,"core of an generaldomain sentence pair (e, f ) as: Introduction A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data. Analyses have shown that this augmented data can lead to better statistical estimation or word coverage (Duh et al., 2010; Haddow and Koehn, 2012). [INE (e) − GENE (e)] + [INF (f ) − GENF (f )] (1) where INE (e) is the length-normalized crossentropy of e on the English in-domain LM. GENE (e) is the length-normalized cross-entropy 678 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678–683, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Now, given state vector s(t), we can predict the probability of the current word. Figure 1 is expressed formally in the following equations: w(t) = [w0 (t), . . . , wk (t), . . . w|W |(t)]  Figure 1: Recurrent neural LM. w"
P13-2119,2006.amta-papers.25,0,0.0371213,"Missing"
P13-2119,C12-1173,0,0.0163768,"Missing"
P13-2119,I08-2088,0,0.0198878,"Missing"
P13-2119,D10-1044,0,\N,Missing
P15-2023,W10-1749,0,0.0237746,"r to train such a classifier, we need an oracle label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this"
P15-2023,P05-1066,0,0.109678,"dering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We def"
P15-2023,W08-0509,0,0.0154767,"BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2"
P15-2023,P12-2061,0,0.705339,"to our oracle labels, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.1"
P15-2023,D13-1139,1,0.949755,"ls, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30."
P15-2023,I13-1147,1,0.859223,"ng s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30.20 30.69 33.04 29.43 32.59 test10 BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate th"
P15-2023,D10-1092,1,0.885359,"Missing"
P15-2023,W10-1736,1,0.961659,"heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . •"
P15-2023,N07-1051,0,0.0573074,"which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score,"
P15-2023,D11-1046,0,0.173982,"Missing"
P15-2023,W04-3250,0,0.55929,"Missing"
P15-2023,2011.mtsummit-papers.36,1,0.933737,"Missing"
P15-2023,W11-2102,0,0.013815,"label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this y-th word in the target sentence for simplic"
P15-2023,W04-3230,0,0.165103,"Missing"
P15-2023,C04-1073,0,0.176348,"rms a rule-based preordering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea"
P15-2023,P14-2091,0,0.0664321,"bstantial gain in RIBES, we attained a rather comparable gain in BLEU. The investigation of our translation suggests that insufficient generation of English articles caused a significant degradation in the BLEU score. Previous systems listed in Table 5 incorporated article generation and demonstrated its positive effect (Goto et al., 2012; Hayashi et al., 2013). While we achieved state-ofthe-art accuracy without language-specific techniques, it is also a promising direction to integrate our preordering method with language-specific techniques such as article generation and subject generation (Kudo et al., 2014). 3.2 Result Table 4 shows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses"
P15-2023,P12-1096,0,0.368513,"sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accuracy in Japanese-to-English translation with a simple method without language-specific heuristics. 1 It is also possible to use n-ary trees (Li et al., 2007; Yang et al., 2012), but we ke"
P15-2023,D13-1049,0,0.0380846,"suke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple discriminative preordering model as described in Section 2.1. They employed heuristics that utilize Giza to align their training sentences, then sort source words to resemble target word indices. After that, sorted source sentences without overlaps are used to train the model. They gained BLEU +1.54 improvement in Chinese-to-English evaluation. Our proposal follows their model, while we do not rely on their heuristics for preparing training data. Lerner and Petrov (2013) proposed another discriminative preordering model along dependency trees, which classifies whether the parent of each node should be the head in target language. They reported BLEU +3.7 improvement in English-to-Japanese translation. Hoshino et al. (2013) proposed a similar but rule-based method for Japanese-to-English dependency preordering. Yang et al. (2012) proposed a method to produce oracle reordering in the discriminative preordering model along dependency trees. Their idea behind is to minimize word alignment crossing recursively, which is essentially the same reordering objective as"
P15-2023,P07-1091,0,0.642315,"or accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accu"
P15-2023,P14-2024,0,0.0127024,"hows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses-chart; nevertheless, Neubig and Duh (2014) reported that their forestto-string system on the same corpus, which is unfortunately evaluated on the different testing data (test7), showed RIBES +6.19 (75.94) and BLEU +2.93 (33.70) improvements. Although not directly comparable, our method achieves a comparable or superior improvement. This testing data is excluded from latter experiments. 142 4 Related Work Acknowledgments We would like to thank Kevin Duh, Atsushi Fujita, Taku Kudo, Shinsuke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple disc"
P15-2023,J03-1002,0,0.00843586,"ments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score, while Nile achieved F1 86.9 score, for word alignment task. addition, we achieved modest improvements even with DL=0 (no distortion allowed), which indicates the monotonicity of our reordered sentences. Table 5 shows a comparison of the proposed method with a ru"
P15-2023,P02-1040,0,0.092262,"6.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the train"
P15-2023,P07-2045,0,\N,Missing
W10-1736,P07-1091,0,0.326335,"Missing"
W10-1736,2006.amta-papers.16,0,0.0396519,"Missing"
W10-1736,J03-1002,0,0.00303273,", 3, 2, 6] from this alignment. When the same word (or derivative words) appears twice or more in a single English sentence, two or more non-consecutive words in the English sentence are aligned to a single Japanese word: 3.1 Rough evaluation of reordering rate of change of speed NULL ({}) sokudo ({5}) henka ({3}) no ({2 4}) wariai ({1}) First, we examined rank correlation between Head Final English sentences produced by the Head Finalization rule and Japanese reference sentences. Since we do not have handcrafted word alignment data for an English-to-Japanese bilingual corpus, we used GIZA++ (Och and Ney, 2003) to get automatic word alignment. Based on this automatic word alignment, we measured Kendall’s τ for the word order between HFE sentences and Japanese sentences. Kendall’s τ is a kind of rank correlation measure defined as follows. Suppose a list of integers such as L = [2, 1, 3, 4]. The number of all integer pairs in this list is 4 C2 = 4 × 3/(2 × 1) = 6. The number of increasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4), and (3, 4). Kendall’s τ is defined by τ= We excluded the ambiguously aligned words (2 from the calculation of τ . We use only [5, 3, 1] and get τ = −1.0. The exclusion"
W10-1736,P05-1022,0,0.0389233,"Missing"
W10-1736,P02-1040,0,0.107213,"ecided to stop swapping them at coordination nodes, which are indicated cat and xcat attributes of the Enju output. We call this the coordination exception rule. In addition, we avoid Enju’s splitting of numerical expressions such as “12,345” and “(1)” because this splitting leads to inappropriate word orders. 246 3 Experiments John va1 a ball va2 hit . NULL ({3}) jon ({1}) wa ({2}) bohru ({4}) wo ({5}) utta ({6}) . ({7}) In order to show how closely our Head Finalization makes English follow Japanese word order, we measured Kendall’s τ , a rank correlation coefficient. We also measured BLEU (Papineni et al., 2002) and other automatic evaluation scores to show that Head Finalization can actually improve the translation quality. We used NTCIR7 PAT-MT’s Patent corpus (Fujii et al., 2008). Its training corpus has 1.8 million sentence pairs. We used MeCab (http:// mecab.sourceforge.net/) to segment Japanese sentences. Then, we get [1, 2, 4, 5, 6, 7] and τ = 1.0. We use τ or the average of τ over all training sentences to observe the tendency. Sometimes, one Japanese word corresponds to an English phrase: John went to Costa Rica . NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5}) ni ({3}) itta ({2}) . ({6}) We"
W10-1736,P05-1066,0,0.769816,"Missing"
W10-1736,P05-1034,0,0.166265,"Missing"
W10-1736,de-marneffe-etal-2006-generating,0,0.00411069,"Missing"
W10-1736,2006.amta-papers.25,0,0.0277013,"it (http:// 5 Discussion Our method used an HPSG parser, which gives rich information, but it is not easy to build such a parser. It is much easier to build word dependency parsers and Penn Treebank-style parsers. In order use these parsers, we have to add some heuristic rules. www.mibel.cs.tsukuba.ac.jp/norimatsu/ bleu kit/) following the PATMT’s overview paper (Fujii et al., 2008). The table shows that dl=6 gives the best result, and even dl=0 (no reordering in Moses) gives better scores than the organizers’ Moses. Table 2 also shows Word Error Rates (WER) and Translation Error Rates (TER) (Snover et al., 2006). Since they are error rates, smaller is better. Although the improvement of BLEU is not very impressive, the score of WER is greatly reduced. This difference comes from the fact that BLEU measures only local word order, while WER mea5.1 Word Dependency Parsers At first, we thought that we could substitute a word dependency parser for Enju by simply rephrasing a head with a modified word. Xu et al. (2009) used a semantic head-based dependency parser for a similar purpose. Even when we use a syntactic head-based dependency parser instead, we encountered their ‘excessive movement’ problem. A str"
W10-1736,N07-1007,0,0.0105596,"ad c4 appears before c5, so c4 and c5 are swapped. The lower picture shows the swapped result. Then we get John a ball hit, which has the same word order as its Japanese translation jon wa bohru wo utta except for the functional words a, wa, and wo. We have to add Japanese particles wa (topic marker) or ga (nominative case marker) for John and wo (objective case marker) for ball to get an acceptable Japanese sentence. It is well known that SMT is not good at generating appropriate particles from English, whitch does not have particles. Particle generation was tackled by a few research groups (Toutanova and Suzuki, 2007; Hong et al., 2009). Here, we use Enju’s output to generate seeds Figure 1: Enju’s XML output (some attributes are removed for readability). c0 ? c1 ? c2 ? t0 John c3 ? c6 ? t2 a c4 ? t1 hit c0 c1 ? c2 ? t0 John jon (wa) Original English ? c5 ? c5 ? c7 ? t3 ball Head Final English c3 ? c7 c6 c4 ? ? ? t3 t2 t1 a ball hit – bohru (wo) utta Figure 2: Head Finalization of a simple sentence (? indicates a head). 245 0 Original English ? ? 4 ? 1? 2 John 5 went 6 ? 7 to 8 9 the 0 13 3 ? 10 police 11 ? 12 because 13 14 ? 15 Mary ? 16 ? 19 his 17 lost 18 ? 20 wallet Head Final English ? 3 11 ? ? ? 16"
W10-1736,C04-1073,0,0.835753,"Missing"
W10-1736,N09-1028,0,0.772582,"ral level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rulebased preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al."
W10-1736,P09-2059,0,0.540137,"ider part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Why do we think this works? The reason is simple: Japanese is a typical head-final language. That is, a syntactic head word comes after nonhead (dependent) words. SOV is just one aspect of head-final languages. In order to implement this idea, we need a parser that outputs syntactic heads. Enju is such a parser from the University of Tokyo (http://www-tsujii.is.s. u-tokyo.ac.jp/enju). We discuss other parsers in section 5. There is another kind of head: semantic heads. Hong et al. (2009) used Stanford parser (de Marneffe et al., 2006), which outputs semantic headbased dependencies; Xu et al. (2009) also used the same representation. The use of syntactic heads and the number of dependents are essential for the simplicity of English is a typical SVO (Subject-VerbObject) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently,"
W10-1736,P01-1067,0,0.267073,"Missing"
W10-1757,W06-1615,0,0.0749079,"for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show consistent statistically significant improvements. From the Bayesian view, multitask formulation of N-best"
W10-1757,W09-2201,0,0.0321418,"r better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsi"
W10-1757,N09-1025,0,0.167697,"tanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail. Our goal here is to address this situation. 3 Proposed Reranking Framework h(e, f ) =  1       0 if foreign word “Monsieur” and English word “Mr.” co-occur in e,f otherwise In the following, we first give an intuitive comparison between single vs. multiple task learning (Section 3.1), before presenting the general metaalgorithm (Section 3.2) and particular instantiations (Section 3.3). (2) One can imagine that such features are sparse because it may only fire for input sentences that contain the word “Monsieur”. For all other input sentences, it is an useless, inactive featur"
W10-1757,J07-2003,0,0.0561179,"Missing"
W10-1757,J05-1003,0,0.264845,"f features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the 3. We demonstrate that"
W10-1757,N09-1068,0,0.0391788,"gularizer to ensure that the learned functions of related tasks are close to each other. The popular ℓ1 /ℓ2 objective can be optimized by various methods, such as boosting (Obozinski et al., 2009) and convex programming (Argyriou et al., 2008). Yet another regularizer is the ℓ1 /ℓ∞ norm (Quattoni et al., 2009), which replaces the 2-norm with a max. One could also define a regularizer to ensure i that each task-specific to some average P wi is close parameter, e.g. i ||w − wavg ||2 . If we interpret wavg as a prior, we begin to see links to Hierarchical Bayesian methods for multitask learning (Finkel and Manning, 2009; Daume, 2009). 2. Shared Subspace: This approach assumes that there is an underlying feature subspace that is common to all tasks. Early works on multitask learning implement this by neural networks, where different tasks have different output layers but share the same hidden layer (Caruana, 1997). Another method is to write the weight vector as two parts w = [u; v] and let the task-specific function be uT · h(e, f ) + vT · Θ · h(e, f ) (Ando and Zhang, 2005). Θ is a D ′ × D matrix that maps the original features to a subspace common to all tasks. The new feature representation is computed by"
W10-1757,W08-0804,0,0.0274275,"res are shared: Wa : » – 4 0 0 4 0 3 3 0 4 4 3 3 → 14 Wb : » – 4 0 3 4 0 3 0 0 4 5 3 0 → 12 2 In MT, evaluation metrics like BLEU do not exactly decompose across sentences, so for some training algorithms this loss is an approximation. [optional] RandomHashing({Hi }) W = MultitaskLearn({(Hi , yi )}) hc = ExtractCommonFeature(W) {Hic } = RemapFeature({Hi }, hc ) wc = ConventionalReranker({(Hic , yi )}) The first step, random hashing, is optional. Random hashing is an effective trick for reducing the dimension of sparse feature sets without suffering losses in fidelity (Weinberger et al., 2009; Ganchev and Dredze, 2008). It works by collapsing random subsets of features. This step can be performed to speed-up multitask learning later. In some cases, the original feature dimension may be so large that hashed representations may be necessary. The next two steps are key. A multitask learning algorithm is run on the N-best lists, and a common feature space shared by all lists is extracted. For example, if one uses the multitask objective of Eq. 5, the result of step 2 is a set of weights W. ExtractCommonFeature(W) then returns the feature id’s (either from original or hashed representation) that receive nonzero"
W10-1757,P09-1114,0,0.0254945,"help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfor"
W10-1757,P05-1024,1,0.783566,"engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jo"
W10-1757,N04-1022,0,0.0383443,"Missing"
W10-1757,P06-1096,0,0.0716939,"., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within machine learning. There has already been some applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on p"
W10-1757,P05-1012,0,0.149052,", 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al., 2005). The division into two research focuses is convenient, but may be suboptimal if the training algorithm and features do not match well together. Our work can be seen as re-connecting the two focuses, where the training algorithm is explicitly used to help discover better features. Multitask learning is currently an active subfield 381 within m"
W10-1757,N04-1021,0,0.102794,"Missing"
W10-1757,P02-1040,0,0.0791554,"Missing"
W10-1757,2009.iwslt-evaluation.1,0,0.0128431,"feature sets, with corresponding feature size and train/test BLEU/PER. All multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone. method. Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Evaluation campaigns like WMT (Callison-Burch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. 2. Designing better training algorithms: Nbest reranking can be seen as a subproblem of structured prediction, so many general structured prediction algorithms (c.f. (Bakir et al., 2007)) can be applied. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al., 2005) and MT (Watanabe et al., 2007) uses iterative sets of N-best lists in its training process. Other training algorithms include perceptron-style algorithms (Liang et al., 2006), MaxEnt (Ch"
W10-1757,P08-1098,0,0.0306633,"me applications in NLP: For example, (Collobert and Weston, 2008) uses a deep neural network architecture for multitask learning on partof-speech tagging, chunking, semantic role labeling, etc. They showed that jointly learning these related tasks lead to overall improvements. (Deselaers et al., 2009) applies similar methods for machine transliteration. In information extraction, learning different relation types can be naturally cast as a multitask problem (Jiang, 2009; Carlson et al., 2009). Our work can be seen as following the same philosophy, but applied to N-best lists. In other areas, (Reichart et al., 2008) introduced an active learning strategy for annotating multitask linguistic data. (Blitzer et al., 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We expect that more novel applications of multitask learning will appear in NLP as the techniques become scalable and standard. 6 Discussion and Conclusion N-best reranking is a beneficial framework for experimenting with large feature sets, but unfortunately feature sparsity leads to overfitting. We addressed this by re-casting N-best lists as multitask learning data. Our MT experiments show con"
W10-1757,N04-1023,0,0.65709,"nvolving millions of features. 1. We introduce the idea of viewing N-best reranking as a multitask learning problem. This view is particularly apt to any general reranking problem with sparse feature sets. 2. We propose a simple meta-algorithm that first discovers common feature representations across N-bests (via multitask learning) before training a conventional reranker. Thus it is easily applicable to existing systems. 1 Introduction Many natural language processing applications, such as machine translation (MT), parsing, and language modeling, benefit from the N-best reranking framework (Shen et al., 2004; Collins and Koo, 2005; Roark et al., 2007). The advantage of N-best reranking is that it abstracts away the complexities of first-pass decoding, allowing the researcher to try new features and learning algorithms with fast experimental turnover. In the N-best reranking scenario, the training data consists of sets of hypotheses (i.e. N-best lists) generated by a first-pass system, along with their labels. Given a new N-best list, the goal is to rerank it such that the best hypothesis appears near the top of the list. Existing research have focused on training a single reranker directly on the"
W10-1757,P09-1054,0,0.143227,"Missing"
W10-1757,D07-1080,1,0.936069,") is a D-dimensional feature vector, w is the weight vector to be trained, and N (f ) is the set of likely translations of f , i.e. the N-best list. The feature h(e, f ) can be any quantity defined in terms of the sentence pair, such as translation model and language model probabilities. Here we are interested in situations where the feature definitions can be quite sparse. A common methodology in reranking is to first design feature templates based on linguistic intuition and domain knowledge. Then, numerous features are instantiated based on the training data seen. For example, the work of (Watanabe et al., 2007) defines feature templates based on bilingual word alignments, which lead to extraction of heavilylexicalized features of the form: 2. The input (f ) has high variability (e.g. large vocabulary size), so that features for different inputs are rarely shared. 3. The N-best list output also exhibits high variability (e.g. many different word reorderings). Larger N may improve reranking performance, but may also increase feature sparsity. When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009)"
W10-1757,zhang-etal-2004-interpreting,0,0.0226462,"“de”) or special characters (such as numeral symbol and punctuation). These are features that can be expected to be widely applicable, and it is promising that multitask learning is able to recover these from the millions of potential features. 10 3. All three multitask methods obtained features that outperformed the baseline. The BLEU scores are 28.8, 28.9, 29.1 for Unsupervised Feature Selection, Joint Regularization, and Shared Subspace, respectively, which all outperform the 28.6 baseline. All improvements are statistically significant by bootstrap sampling test (1000 samples, p &lt; 0.05) (Zhang et al., 2004). 300 4. Shared Subspace performed the best. We conjecture this is because its feature projection can create new feature combinations that is more expressive than the feature selection used by the two other methods. Bootstrap samples 250 50 0 −0.2 Wabbit 1.2 5 Related Work in NLP Previous reranking work in NLP can be classified into two different research focuses: 1. Engineering better features: In MT, (Och and others, 2004) investigates features extracted from a wide variety of syntactic representations, such as parse tree probability on the outputs. Although their results show that the propo"
W10-1757,W09-0401,0,\N,Missing
W10-1757,P05-1022,0,\N,Missing
W10-1757,W09-0438,0,\N,Missing
W10-1762,J93-2003,0,0.011344,"ated studies on reordering. Section 3 describes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007)"
W10-1762,J07-2003,0,0.366382,"et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and Our"
W10-1762,P05-1066,0,0.134623,"Missing"
W10-1762,P98-1070,0,0.140627,"ur method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difﬁcult to decode long multi-clause sentences as described above. Our approach is also related to sentence simpliﬁcation and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related ﬁeld is clause identiﬁcation (Tjong et al., 2001). The proposed method is not limited to a speciﬁc clause identiﬁcation method and any method can be employed, if their clause deﬁnition matches the proposed method where clauses are independently translated. 3 Bilingual source Corpus (Training) target parse & clause segmentation Source Sentences (clause-segmented) word alignment Wor"
W10-1762,P02-1040,0,0.0837809,"est sentences are multi-clause sentences. Training Corpus Type Parallel (no-clause-seg.) Parallel (auto-aligned) (oracle-aligned) Dictionary Development Corpus Type Parallel (oracle-aligned) Test Corpus Type Parallel (clause-seg.) E J E J J E J #words 690,536 942,913 135,698 183,043 183,147 263,175 291,455 #words E 34,417 J 46,480 E J #words 34,433 45,975 decoders employed two language models: a word 5-gram language model from the Japanese sentences in the parallel corpus and a word 4-gram language model from the Japanese entries in the dictionary. The feature weights were optimized for BLEU (Papineni et al., 2002) by MERT, using the development sentences. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically signiﬁcant according to the bootstrap sampling test ("
W10-1762,N04-1035,0,0.0349554,"many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reorderi"
W10-1762,2006.amta-papers.25,0,0.0712603,"Missing"
W10-1762,N04-1014,0,0.0158515,"typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed s"
W10-1762,N04-4026,0,0.0472133,"ur thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically"
W10-1762,W01-0708,0,0.0411713,"Missing"
W10-1762,D09-1105,0,0.0200396,"nguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between languages with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pai"
W10-1762,J97-3002,0,0.192686,"sed SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse"
W10-1762,N03-1017,0,0.0275575,"ribes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be ext"
W10-1762,C04-1073,0,0.0479303,"Missing"
W10-1762,2005.iwslt-1.8,0,0.0209809,"uture studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering ove"
W10-1762,N09-1028,0,0.052652,"Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between l"
W10-1762,P01-1067,0,0.0503465,"ce-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previou"
W10-1762,P07-2045,0,0.00356861,"inal symbol s0 with the second clause and obtain the Japanese sentence: watashi wa tom ga kino susume ta zasshi o kat ta . 4 Experiment We conducted the following experiments on the English-to-Japanese translation of research paper abstracts in the medical domain. Such technical documents are logically and formally written, and sentences are often so long and syntactically complex that their translation needs long distance reordering. We believe that the medical domain is suitable as regards evaluating the proposed method. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210 . Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11 . Both 4.1 Resources Our bilingual resources were taken from the medical domain. The parallel corpus consisted of research paper abstracts in English taken from PubMed4 and the corresponding Japanese translations. The training port"
W10-1762,zhang-etal-2004-interpreting,0,0.0423294,"Missing"
W10-1762,P07-1091,0,0.327459,"ally follows the Penn Treebank II scheme but also includes SINV, SQ, SBAR. See http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enjuoutput-spec.html#correspondence for details. 418 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418–427, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem ove"
W10-1762,P06-1077,0,0.0402884,"related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-"
W10-1762,P06-1004,0,0.0185365,"in our corpora. 421 John lost the book that was borrowed ... clause(1) clause(2) p(that |kara) + p(was |kara ) + ... p(John |john ) + p(lost |john ) + ... john John k|fm ) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k ′ for each fm by ﬁnding k ′ = arg maxk t(lm = k|fm ). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japanese sentence into K clauses based on a dynamic programming algorithm proposed by Malioutov and Barzilay (2006). We deﬁne an M × M similarity matrix S = [sij ] with sij = exp(−||li −lj ||) where li is (K + i)-th row vector in the label matrix L. sij represents the similarity between the i-th and j-th Japanese words with respect to their clause alignment score distributions; if the score distributions are similar then sij is large. The details of this algorithm can be found in (Malioutov and Barzilay, 2006). The clause segmentation gives us contiguous Japanese clauses f˜1 , f˜2 , ..., f˜K , thus minimizing inter-segment similarity and maximizing intra-segment similarity. Second, we determine the clause"
W10-1762,J08-1002,0,0.0605287,"Missing"
W10-1762,W01-1408,0,0.038533,"Missing"
W10-1762,C98-1067,0,\N,Missing
W10-1762,J08-3004,0,\N,Missing
W12-4207,W08-0336,0,0.0779511,"Missing"
W12-4207,P05-1066,0,0.323609,"Missing"
W12-4207,C10-1043,0,0.0945607,"oached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training"
W12-4207,D10-1092,1,0.926453,"Missing"
W12-4207,W10-1736,1,0.0700138,"me Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a tra"
W12-4207,N03-1017,0,0.0605813,"ranslation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozak"
W12-4207,P07-2045,0,0.0344953,"Missing"
W12-4207,J08-1002,0,0.110732,"h languages with different phrase structures like English and Japanese. Head Finalization is a successful syntax-based reordering method designed to reorder sentences from a head-initial language to resemble the word order in sentences from a headfinal language (Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word de"
W12-4207,J03-1002,0,0.0108902,"reebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quali"
W12-4207,J04-4002,0,0.081366,"ally improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation"
W12-4207,P03-1021,0,0.0679623,"2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision"
W12-4207,P02-1040,0,0.0870379,"w-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the parallel corpora by using the modified Kneser-Ney smoothing (Chen and Goodman, 1999) implemented in the SRILM (Stolcke, 2002) toolkit. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. The effectiveness of the reorderings proposed in Section 3.3 was assessed by using two precision metrics and two error metrics on translation quality. The first evaluation metric is BLEU (Papineni et al., 2002), a very common accuracy metric in SMT that measures N -gram precision, with a penalty for too short sentences. The second evaluation metric was RIBES (Isozaki et al., 2010a), a recent precision metric used to evaluate translation quality between structurally different languages. It uses notions on rank correlation coefficients and precision measures. The third evaluation metric is TER (Snover et al., 2006), another error metric that computes the minimum number of edits required to convert translated sentences into its corresponding references. Possible edits include insertion, deletion, subst"
W12-4207,2006.amta-papers.25,0,0.130258,"(Isozaki et al., 2010b). The essence 58 of this rule is to move the syntactic heads to the end of its dependency by swapping child nodes in a phrase structure tree when the head child appears before the dependent child. Isozaki et al. (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. The score results from several mainstream evaluation methods indicated that the translation quality had been improved; the scores of Word Error Rate (WER) and Translation Edit Rate (TER) (Snover et al., 2006) had especially been greatly reduced. 2.2 Chinese Deep Parsing Syntax-based reordering methods need parsed sentences as input. Isozaki et al. (2010b) used Enju, an HPSG-based deep parser for English, but they also discussed using other types of parsers, such as word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reorderi"
W12-4207,N04-4026,0,0.0537842,"cCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preordering that significantly improves word alignments and leads to a better translation quality. Al59 though the method is limited to translation where the target language is head-final, it requires neither training data nor fine-tuning. To our knowledge, HF is the best method to reorder languages when translat"
W12-4207,N03-1033,0,0.00850002,"and extended CWMT Chinese-Japanese corpus. Dev. stands for Development, OoV for “Out of Vocabulary” words, K for thousands of elements, and M for millions of elements. Data statistics were collected after tokenizing. methods. Detailed Corpus statistics can be found in Table 6. To parse Chinese sentences, we used Chinese Enju (Yu et al., 2010), an HPSG-based parser trained with the Chinese HPSG treebank converted from Penn Chinese Treebank. Chinese Enju requires segmented and POS-tagged sentences to do parsing. We used the Stanford Chinese segmenter (Chang et al., 2008) and Stanford POStagger (Toutanova et al., 2003) to obtain the segmentation and POS-tagging of the Chinese side of the training, development, and test sets. The baseline system was trained following the instructions of recent SMT evaluation campaigns (Callison-Burch et al., 2010) by using the MT toolkit Moses (Koehn et al., 2007) in its default configuration. Phrase pairs were extracted from symmetrized word alignments and distortions generated by GIZA++ (Och and Ney, 2003) using the combination of heuristics “grow-diagfinal-and” and “msd-bidirectional-fe”. The language model was a 5-gram language model estimated on the target side of the p"
W12-4207,D07-1077,0,0.101287,"uhito Sudoh‡ Xianchao Wu‡∗ Kevin Duh‡† Hajime Tsukada‡ Masaaki Nagata‡ + The Graduate University For Advanced Studies, Tokyo, Japan ‡ NTT Communication Science Laboratories, NTT Corporation + handan@nii.ac.jp, ∗ wuxianchao@baidu.com, † kevinduh@is.naist.jp ‡ {sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp Abstract In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules. 1 Introduction In state-of-the-art Statistical Machine Translation (SMT) systems, bilingual phrases are th"
W12-4207,I11-1004,1,0.873372,"02; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu et al., 2011; Isozaki et al., 2010b). The advantages of this strategy are two fold. The first advantage is at the decoding stage, since it enables the translation to be constructed almost monotonically. The second advantage is at the training stage, since automatically estimated word-to-word alignments are likely to be more accurate and symmetrization matrices reveal more evident bilingual phrases, leading to the extraction of better quality bilingual phrases and cleaner phrase tables. In this work, we focus on Chinese-to-Japanese translation, motivated by the increasing interaction between these two coun"
W12-4207,C04-1073,0,0.304241,"ute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4” id=“c3”> means that the node that has id=“c4” is the syntactic head of the node that has id=“c3”. Figure 1: An XML output for a Chinese sentence from Chinese Enju. For clarity, we only draw information related to the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2"
W12-4207,N09-1028,0,0.298949,"the phrase structure and the heads. 2.3 Related Work Reordering is a popular strategy for improving machine translation quality when source and target languages are structurally very different. Researchers have approached the reordering problem in multiple ways. The most basic idea is preordering (Xia and McCord, 2004; Collins et al., 2005), that is, to do reordering during preprocessing time, where the source side of the training and development data and sentences from a source language that have to be translated are first reordered to ease the training and the translation, respectively. In (Xu et al., 2009), authors used a dependency parser to introduce manually created preordering rules to reorder English sentences when translating into five different SOV(Subject-ObjectVerb) languages. Other authors (Genzel, 2010; Wu et al., 2011) use automatically generated rules induced from parallel data. Tillmann (2004) used a lexical reordering model, and Galley et al. (2004) followed a syntactic-based model. In this work, however, we are centered in the design of manual rules inspired by the Head Finalization (HF) reordering (Isozaki et al., 2010b). HF reordering is one of the simplest methods for preorde"
W12-4207,C10-2162,0,0.0590977,"Missing"
W12-4207,W11-2907,0,0.172115,"word dependency parsers and Penn Treebankstyle parsers. However, to use word dependency parsers, they needed an additional heuristic rule to recover phrase structures, and Penn Treebank-style parsers are problematic because they output flat phrase structures (i.e. a phrase may have multiple dependents, which causes a problem of reordering within a phrase). Consequently, compared to different types of parsers, Head-Final English performs the best on the basis of English Enju’s parsing result. In this paper, we follow their observation, and use the HPSG-based parser for Chinese (Chinese Enju) (Yu et al., 2011) for Chinese syntactic parsing. Since Chinese Enju is based on the same parsing model as English Enju, it provides rich syntactic information including phrase structures and syntactic/semantic heads. Figure 1 shows an example of an XML output from Chinese Enju for the sentence “wo (I) qu (go to) dongjing (Tokyo) he (and) jingdu (Kyoto).” The label &lt;cons> and &lt;tok> represent the non-terminal nodes and terminal nodes, respectively. Each node is identified by a unique “id” and has several attributes. The attribute “head” indicates which child node is the syntactic head. In this figure, &lt;head=“c4”"
W12-4207,2002.tmi-tutorials.2,0,0.0338415,"atistical Machine Translation (SMT) systems, bilingual phrases are the main building blocks for constructing a translation given a sentence from a source language. To extract those bilingual phrases from a parallel corpus, the first step is to discover the implicit wordto-word correspondences between bilingual sentences (Brown et al., 1993). Then, a symmetrization matrix is built (Och and Ney, 2004) by using word-to-word alignments, and a wide variety ∗ Now at Baidu Japan Inc. Now at Nara Institute of Science and Technology (NAIST) † of heuristics can be used to extract the bilingual phrases (Zens et al., 2002; Koehn et al., 2003). This method performs relatively well when the source and the target languages have similar word order, as in the case of French, Spanish, and English. However, when translating between languages with very different structures, as in the case of English and Japanese, or Japanese and Chinese, the quality of extracted bilingual phrases and the overall translation quality diminishes. In the latter scenario, a simple but effective strategy to cope with this problem is to reorder the words of sentences in one language so that it resembles the word order of another language (Wu"
W12-4207,J93-2003,0,\N,Missing
W12-4207,D08-1076,0,\N,Missing
W12-4213,W10-1736,1,0.905882,"Missing"
W12-4213,P02-1018,0,0.0219674,"inserts ‘anata wa (you)’ when the predicate is a verb, and ‘sore wa (it)’ when the predicate is a adjective or a copula. These inserted position is the beginning of the sentence. In the case that the sentence is imperative, the system does not solve the zero pronouns (Fig. 3). 4 Experiments 4.1 Experimental Setting devset5 7 500 test 16 489 evaluation of the performances, comparing the system outputs with the English references of test data. Using only BLEU score is not adequate for evaluation of pronoun translation (Hardmeier et al., 2010). We were inspired empty node recovery evaluation by (Johnson, 2002) and defined antecedent Precision (P), Recall (R) and F-measure (F) as follows, P = |G ∩ S| |S| R= |G ∩ S| |G| 2P R P +R Here, S is the set of each pronoun in English translated by decoder, G is the set of the gold standard zero pronoun. We evaluated the effect of performance of every case among completed sentences by human, ones by the baseline system, and the original sentences. F = 4.3 Experimental Result Fig. 4 shows the outline of the procedure of our experiment. We used Moses (Koehn et al., 2007) for the training of the translation and language models, tuning with MERT (Och, 2003) and th"
W12-4213,C88-2159,0,\N,Missing
W12-4213,2007.iwslt-1.1,0,\N,Missing
W12-4213,P02-1040,0,\N,Missing
W12-4213,P07-2045,0,\N,Missing
W12-4213,2010.iwslt-papers.10,0,\N,Missing
W12-4213,W97-0114,0,\N,Missing
W12-4213,D08-1076,0,\N,Missing
W13-2806,P07-1091,0,0.0205674,"reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsuji"
W13-2806,W08-0336,0,0.0552701,"Missing"
W13-2806,J08-1002,1,0.869913,"Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical e"
W13-2806,W09-2307,0,0.0211337,"d sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25–33, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics limited the extensibility of their method. Our approach follows the idea of using dependency tree structures and POS tags, but we discard the information on dependency labels since we did not find them informative to guide our reordering strategies in our preliminary experiments, partly due to Chinese showing less dependencies and a larger label variability (Chang et al., 2009). volve either Chinese or Japanese, and explain how our method builds upon them. From a linguistic perspective, we describe in section 3 our observations of reordering issues between Chinese and Japanese and detail how our framework solves those issues. In section 4 we assess to what extent our pre-reordering method succeeds in reordering words in Chinese sentences to resemble the order of Japanese sentences, and measure its impact on translation quality. The last section is dedicated to discuss our findings and point to future directions. 2 3 Methodology In Subject-Verb-Object (SVO) languages"
W13-2806,J03-1002,0,0.00360351,"es are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods. 1 Introduction Translation between Chinese and Japanese languages gains interest as their economic and political relationship intensifies. Despite their linguistic influences, these languages have different syntactic structures and phrase-based statistical machine translation (SMT) systems do not perform well. Current word alignment models (Och and Ney, 2003) account for local differences in word order between bilingual sentences, but fail at capturing long distance word alignments. One of the main problems in the search of the best word alignment is the combinatorial explosion of word orders, but linguistically-motivated heuristics can help to guide the search. This work explores syntax-informed prereordering for Chinese; that is, we obtain syntactic structures of Chinese sentences, reorder the words to resemble the Japanese word order, and then translate the reordered sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedi"
W13-2806,P03-1021,0,0.0334338,"010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framewor"
W13-2806,W08-0509,0,0.0596627,"nju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framewor"
W13-2806,P02-1040,0,0.0874926,"ses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framework consists in three steps. First, we identify verbal blocks, which consist of Chinese words that will move all together as a block without altering their relative inner order. Second, we identify the right-"
W13-2806,2007.mtsummit-papers.29,0,0.039146,"vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifi"
W13-2806,P06-1055,0,0.00949413,"and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectio"
W13-2806,W12-4207,1,0.839206,"saaki}@lab.ntt.co.jp Abstract have difficulties in extracting reliable syntactic information, mainly because Chinese has a loose word order and few syntactic clues such as inflection and function words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of p"
W13-2806,I11-1136,1,0.782054,"l sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the"
W13-2806,D07-1077,0,0.0192007,"e verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers"
W13-2806,D10-1092,1,0.903219,"Missing"
W13-2806,I11-1004,1,0.842316,"heir positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al."
W13-2806,W10-1736,1,0.962597,"their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in s"
W13-2806,C04-1073,0,0.237416,"grammatical particles in the original vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based"
W13-2806,P07-2045,0,0.00976568,"Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensit"
W13-2806,N09-1028,0,0.465198,"ion words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of phrase-based SMT systems by pre-reordering the source language using unlabeled parse trees augmented with constituent information derived from Part-of-Speech tags. Specifically, we propose a f"
W13-2806,W11-2907,1,0.651863,"l., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical example for the lan"
W13-2806,W00-1303,0,0.708324,"proposed DPC method obtained p-values 0.002 and 0.0, which indicates significant improvements over the phrase-based system. Table 3: Evaluation of translation quality of two test sets when CWMT, News and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering st"
W13-2806,D08-1076,0,\N,Missing
W15-5012,W15-5001,0,0.0351006,"ndirect or rhetorical expressions. Due to this aspect, patent documents are good candidates for literal translation, which most machine translation (MT) approaches aim to do. One technical challenge for patent machine translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for the word order in distant languages. Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation. In this year’s WAT evaluation campaign (Nakazawa et al., 2015), we tackle long distance reordering by syntactic pre-ordering based on Chinese dependency structures (Sudoh et al., 2014) in a Chinese-to-Japanese patent translation task. Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering with hand-written rules or a learning-to-rank model; and a standard phrase-based statistical MT. This paper describes our system’s details and discusses our evaluation results. 2 System Overview Figure 1 sh"
W15-5012,D09-1058,0,0.0879295,"al., 2012) for better Chinese word segmentation based on the POS tag sequences. The dependency parser produces un95 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 95‒98, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). typed dependency trees. The Chinese analysis models were trained using in-house Chinese treebanks in the patent domain (about 35,000 sentences) as well as the standard Penn Chinese Treebank dataset (Sudoh et al., 2014). The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 dependency trees and have to consider all the possible permutation over one head word and one or more modifier words. 5 Evaluation 5.1 Setup We trained a word n-gram language model and two different phrase-based translation models by the above different pre-ordering approaches. We used all of the supplied Chinese-Japanese bilingual training corpora of one million sentence pairs (except for long sentences over 64 words) for the MT models: phrase tables, lexicalized reordering tables, and word 5-gram language models using standard Moses and KenLM training parameters. We"
W15-5012,P12-1096,0,0.0602246,"BES and BLEU than the tree-to-string baseline, but the difference may not be significant. The performances of our systems were lower than the tree-to-string baseline in the Human evaluation. With respect to the difference in the pre-ordering approaches, the rule-based system outperformed the data-driven one. Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a learning-to-rank approach with Ranking SVMs (Yang et al., 2012), which reorders the head word and its modifier words in a dependency tree based on their ranks. The features resemble those by Yang et al. (2012); we did not use label-related ones because our dependency trees do not have labels. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). The only difference is the tree structure; Hoshino et al. (2015) used binary trees and just considered monotone or reverse for two child nodes of each tree node. But we use 5.3 Discussion One critical concern is the difference bet"
W15-5012,P11-1084,0,0.0464458,"Missing"
W15-5012,W06-3119,0,0.130237,"Missing"
W15-5012,W14-7001,0,\N,Missing
W15-5012,E14-1026,0,\N,Missing
W16-4607,P05-1066,0,0.0776155,"fluency and adequacy. As a result, the proposed method improved fluency (NRM:NRM+PT+WA = 17.5:20) but not adequacy (NRM:NRM+PT+WA = 19:14.5). Although the outputs of two methods are similar, the proposed method favored fluent translation and resulted in slight improvements in BLEU and RIBES. 6 Related Work There are several studies on phrase reordering of statistical machine translation. They are divided into three groups: in-ordering such as distance-based reordering (Koehn et al., 2003) and lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), pre-ordering (Collins et al., 2005; Isozaki et al., 2010b; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). In-ordering is performed during decoding, pre-ordering is performed as pre-processing before decoding and post-ordering is executed as post-processing after decoding. In this section, we explain other reordering methods other than lexicalized reordering. In early studies on PBSMT, a simple distance-based reordering penalty was used (Koehn et al., 2003). 101 It worked fairly well for some language pairs with similar word order such as English-French but is not appropriate for"
W16-4607,D08-1089,0,0.48787,"ults show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reorder"
W16-4607,P12-2061,0,0.0194978,"2010b) parse source sentences and reorder the words using hand-crafted rules. (2) Discriminative pre-ordering models (Tromble and Eisner, 2009; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015) learn whether children of each node should be reordered using (automatically) aligned parallel corpus. However, pre-ordering models cannot use the target language information in decoding. Therefore, optimizing phrase ordering using target-side features like phrase translation probability and word alignment is not possible, as done in our proposed method. Post-ordering methods (Sudoh et al., 2011; Goto et al., 2012) are sometimes used in Japanese-toEnglish translation. They first translate Japanese input into head final English texts, then reorder head final English texts into English word orders. Post-ordering methods have the advantage of being able to use syntactic features at low computational cost, but need an accurate parser on the target side. 7 Conclusion In this study, we improved a neural reordering model in PBSMT using phrase translation and word alignment. We proposed phrase translation and word alignment features to construct phrase vectors. The experimental results demonstrate that our prop"
W16-4607,P15-2023,1,0.884694,"d method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem"
W16-4607,D10-1092,1,0.878159,"Missing"
W16-4607,W10-1736,1,0.91174,"points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confirm significant differences in BLEU, RIBES and WER scores on Japanese-to-English and English-to-Japanese translations using bootstrap resampling. Unfortunately, the proposed method is not able to identify significant differences in comparison with NRM. The reordering accuracy does not necessarily relate to the translation accuracy because we m"
W16-4607,N03-1017,0,0.272666,"ambiguity, data sparseness and noises in a phrase table. Previous neural reordering model is successful to solve the first and second problems but fails to address the third one. Therefore, we propose new features using phrase translation and word alignment to construct phrase vectors to handle inherently noisy phrase translation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the w"
W16-4607,P07-2045,0,0.123911,"The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some"
W16-4607,D13-1054,0,0.0917624,"a distant language pairs like Japanese and English often contain the NULL alignment and mis-aligned words. On the 1 We experimented in the Kyoto Free Translation Task. 95 Figure 1: Four orientations, namely Monotone, Swap, Discontinuous-right and Discontinuous-left, are shown. Monotone means that the source phrases f ai , f ai−1 are adjoining and monotonic with respect to the target phrases ei , ei−1 . Swap means f ai , f ai−1 are adjoining and swapping. Discontinuous-right means f ai , f ai−1 are separated and monotonic, and Discontinuous-left means f ai , f ai−1 are separated and swapping. Li et al. (2013) proposed an NRM, which uses a deep neural network to address the problems of high ambiguity and data sparsity. We describe the NRM in the next section and propose our model to improve the NRM to address the problem of noisy phrases in Section 4. 3 Neural Reordering Model Li et al. (2013) tackled the ambiguity and sparseness problem by distributed representation of phrases. The distributed representation maps sparse phrases into a dense vector space where elements with similar roles are expected to be located close to each other. 3.1 Distributed Representation of Phrases Socher et al. (2011) p"
W16-4607,P06-1090,0,0.0230635,"rce phrases f = f a1 , . . . , f ai , . . . , f aI , we translate and reorder the phrases to generate a sequence of target phrases e = e1 , . . . , ei , . . . , eI . Here a = a1 , . . . , aI expresses the alignment between the source phrase f ai and the target phrase ei . The alignment a can be used to represent the phrase orientation o. Three orientations with respect to previous phrase (Monotone, Swap, Discontinuous) are typically used in lexicalized reordering models (Galley and Manning, 2008). However, because global phrase reordering appears frequently in Japanese-to-English translation, Nagata et al. (2006) proposed four orientations instead of three by dividing the Discontinous label. In Figure 1, we show four orientations, called Monotone (Mono), Swap, Discontinuous-right (Dright ) and Discontinuous-left (Dleft ). Using alignments ai and ai−1 , orientation oi respected to the target phrases ei , ei−1 follows:   Mono (ai − ai−1 = 1)    Swap (a − a i i−1 = −1) (1) oi = Dright (ai − ai−1 > 1)    D (ai − ai−1 &lt; −1) left If the reordering probability of every phrase is expressed as P (oi |f ai , ei ), that of the sentence can be approximated as I ∏ I P (a1 |e) = P (oi |f ai , ei ). (2) i"
W16-4607,P15-1021,0,0.079694,"th phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem of reordering amb"
W16-4607,J03-1002,0,0.0172476,"sion corresponding to the NULL word. Table 1 explains each dimension of W A. For example, the fourth dimension of W A of the word “日本 (Japan)” in Figure 2 is 1 because the aligned word “Japan” is located at the center of the phrase. 5 Experiment We conduct two kinds of experiments: intrinsic evaluation of reordering accuracy and extrinsic evaluation of MT quality. 5.1 Setting We use the Kyoto Free Translation Task2 (KFTT) for our experiment. It is a task for Japanese-to-English translation that focuses on Wikipedia articles. We use KyTea3 (ver.0.4.7) for Japanese word segmentation and GIZA++ (Och and Ney, 2003) with grow-diag-final-and for word alignment. We extract 70M phrase bigram pairs and automatically annotate the correct reordering orientation using Moses (Koehn et al., 2007). We filter out phrases that appear only once. We randomly divide the parallel corpus into training, development, and test. We retain 10K instances for development and test and use 1M instances for training. We experimented 15, 25, 50, and 100-dimensional word vectors; 25-dimensional word vectors are used in all experiments involving our model. Thus, we set the vector size of the recursive auto-encoder to 31, to include t"
W16-4607,P03-1021,0,0.0265921,"at the instances of Mono are not affected much by the NULL alignment, because they contain less NULL alignment (See the top row in Table 4). Overall, as compared with the NRM, our proposed method using phrase translation and word alignment improves the accuracy by 3.17 points (1.5 points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confirm significant differences in BLEU, RIBES and WER sco"
W16-4607,P02-1040,0,0.0949791,"t , whereas that of Mono is not improved. This result suggests that the instances of Mono are not affected much by the NULL alignment, because they contain less NULL alignment (See the top row in Table 4). Overall, as compared with the NRM, our proposed method using phrase translation and word alignment improves the accuracy by 3.17 points (1.5 points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confi"
W16-4607,D11-1014,0,0.0181379,"ping. Li et al. (2013) proposed an NRM, which uses a deep neural network to address the problems of high ambiguity and data sparsity. We describe the NRM in the next section and propose our model to improve the NRM to address the problem of noisy phrases in Section 4. 3 Neural Reordering Model Li et al. (2013) tackled the ambiguity and sparseness problem by distributed representation of phrases. The distributed representation maps sparse phrases into a dense vector space where elements with similar roles are expected to be located close to each other. 3.1 Distributed Representation of Phrases Socher et al. (2011) proposed the recursive autoencoder, which recursively compresses a word vector and generates a phrase vector with the same dimension as the word vector. We define a word vector of u dimension x ∈ Ru , an encoding weight matrix We ∈ Ru×2u , and a bias term be . A phrase vector p1:2 is constructed as follows: p1:2 = f (We [x1 ; x2 ] + be ) (3) f is an activation function such as tanh, which is used in our experiments. When a phrase consists of more than two words, we compute a phrase vector p1:n recursively from the phrase vector p1:n−1 and the word vector xn . p1:n = f (We [p1:n−1 ; xn ] + be"
W16-4607,2011.mtsummit-papers.36,1,0.922434,"ments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem of reordering ambiguity and data sparsity using a neural"
W16-4607,N04-4026,0,0.353693,"nslation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. How"
W16-4607,D09-1105,0,0.0217846,"for some language pairs with similar word order such as English-French but is not appropriate for distant language pairs including Japanese-English. Lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008) introduced lexical constraints of the phrase reordering and not just penalizing long-distance reordering. Pre-ordering methods can be divided into two types: (1) Rule-based preprocessing methods (Collins et al., 2005; Isozaki et al., 2010b) parse source sentences and reorder the words using hand-crafted rules. (2) Discriminative pre-ordering models (Tromble and Eisner, 2009; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015) learn whether children of each node should be reordered using (automatically) aligned parallel corpus. However, pre-ordering models cannot use the target language information in decoding. Therefore, optimizing phrase ordering using target-side features like phrase translation probability and word alignment is not possible, as done in our proposed method. Post-ordering methods (Sudoh et al., 2011; Goto et al., 2012) are sometimes used in Japanese-toEnglish translation. They first translate Japanese input into head final English texts, the"
W16-4607,I11-1004,1,0.876014,"that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014)"
W16-4621,P15-2023,1,0.730929,"-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We found that the pairwise-based method gave slightly better pre-ordering performance than the Ranking SVMs in our pilot test, as did Jehl et al. (2014). We also renewed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those described in Jehl et al. (2014). Since these features are very sparse, we chose those appearing more than twice in the training parallel data. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). We used the intersection of bidirectional automatic word alignment (Nakagawa, 2015). The pairwise formulation enables a simple solution to determine the oracles for which we choose a binary decision, obtaining higher Kendall’s τ wi"
W16-4621,E14-1026,0,0.228,"Missing"
W16-4621,D15-1166,0,0.0489628,"ning parameters. We applied modified Kneser-Ney phrase table smoothing with an additional phrase scoring option: --KneserNey. The model weights were optimized by standard Minimum Error Rate Training (MERT), but we compared five independent MERT runs and chose the best weights for the development test set. The distortion limit was 9 for both the baseline and pre-ordering conditions, chosen from 0, 3, 6, and 9 by comparing the results of the MERT runs. 5.3 Neural MT Setup We also tried a recent neural MT for comparison with a phrase-based MT. We used a sequence-tosequence attentional neural MT (Luong et al., 2015) implemented by the Harvard NLP group1 with a vocabulary size of 50,000 and a 2-layer bidirectional LSTM with 500 hidden units on both the encoder/decoder2 . The neural MT, which was word-based with the same tokenizer used in the phrase-based MT setting, did not employ recent subword-based or character-based methods. The training time of the neural MT was about two days (13 epochs with 3.5 hours/epoch) with a NVIDIA Tesla K80 GPU. The decoding employed a beam search with a beam size of five and dictionary-based unknown word mapping with the IBM-4 lexical translation table obtained by MGIZA++."
W16-4621,P15-1021,0,0.100791,"newed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those described in Jehl et al. (2014). Since these features are very sparse, we chose those appearing more than twice in the training parallel data. The reordering oracles were determined to maximize Kendall’s τ over automatic word alignment in a similar manner to Hoshino et al. (2015). We used the intersection of bidirectional automatic word alignment (Nakagawa, 2015). The pairwise formulation enables a simple solution to determine the oracles for which we choose a binary decision, obtaining higher Kendall’s τ with and without swapping every node pair. 5 Evaluation 5.1 Pre-ordering Setup The pre-ordering model for the data-driven method was trained over the MGIZA++ word alignment used for the phrase tables described later. We trained a logistic-regression-based binary classification model 212 using the reordering oracles over training data with LIBLINEAR (version 2.1). Hyperparameter c was set to 0.01, chosen by the binary classification accuracy on the de"
W16-4621,W15-5012,1,0.80914,"ne translation is the complex syntactic structure of patent documents, which typically have long sentences that complicate MT reordering, especially for word order in distant languages. Chinese and Japanese have similar word order in noun modifiers but different subject-verb-object order, requiring long distance reordering in translation. In the WAT 2016 evaluation campaign (Nakazawa et al., 2016), we participated in a Chinese-to-Japanese patent translation task and tackled long distance reordering by syntactic pre-ordering based on Chinese dependency structures, as in our last year’s system (Sudoh and Nagata, 2015). We also use a recent neural MT as the following MT implementation for comparison with a traditional phrase-based statistical MT. Our system basically consists of three components: Chinese syntactic analysis (word segmentation, part-of-speech (POS) tagging, and dependency parsing) adapted to patent documents; dependency-based syntactic pre-ordering with hand-written rules or a learning-to-rank model; and the following MT component (phrase-based MT or neural MT). This paper describes our system’s details and discusses our evaluation results. 2 System Overview Figure 1 shows a brief workflow of"
W16-4621,D09-1058,0,0.0285827,"inese dependency parsing Dep. models Chinese language resource (patent) Dependency-based syntactic pre-ordeirng Phrase-based or Neural MT Pre-ordering model MT models Supplied parallel text Japanese sentence Figure 1: Brief workflow of our MT system. Gray-colored resource is an in-house one. Chinese analysis models were trained using an in-house Chinese treebank of about 35,000 sentences in the patent domain (Sudoh et al., 2014) as well as the standard Penn Chinese Treebank dataset. The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 Syntactic Pre-ordering Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a pairwise-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We fo"
W16-4621,P12-1096,0,0.017906,"(Sudoh et al., 2014) as well as the standard Penn Chinese Treebank dataset. The training also utilized unlabeled Chinese patent documents (about 100 G bytes) for semi-supervised training (Suzuki et al., 2009; Sudoh et al., 2014). 4 Syntactic Pre-ordering Data-driven pre-ordering obtains the most probable reordering of a source language sentence that is monotone with the target language counterpart. It learns rules or models using reordering oracles over word-aligned bilingual corpora. We used a pairwise-classification-based model for pre-ordering (Jehl et al., 2014), instead of Ranking SVMs (Yang et al., 2012) that we used the last year. An advantage of pairwise classification is that we can use features defined on every node pair, while we can only use node-wise features with Ranking SVMs. We found that the pairwise-based method gave slightly better pre-ordering performance than the Ranking SVMs in our pilot test, as did Jehl et al. (2014). We also renewed the features for this year’s system. We used span-based features (word and part-ofspeech sequences over dependency sub-structures) like Hoshino et al. (2015), word and part-of-speech n-grams (n=2,3,4) including head word annotations, and those d"
W17-3208,P11-2093,1,0.641669,"Missing"
W17-3208,P02-1040,0,0.100862,"Missing"
W17-3208,D17-1151,0,0.053157,"conjectured that this is an effect of gathering the similar sentences in a mini-batch as we mentioned in Section 4.2.3. These results indicate that in the case of SGD it is acceptable to TRG SRC , which is the fastest method to process the whole corpus (see Table 3), for SGD. Recently, Wu et al. (2016) proposed a new learning paradigm, which uses Adam for the initial training, then switches to SGD after several iterations. If we use this learning algorithm, we may be able to train the model more effectively by using SHUFFLE or SRC sorting method for Adam, and TRG SRC for SGD. 4.3 5 Recently, Britz et al. (2017) have released a paper about exploring the hyper-parameters of NMT. This work is similar to our paper in the terms of finding the better hyper-parameters by doing a large number of experiments and deriving empirical conclusions. However, notably this paper fixed the mini-batch size to 128 sentences and did not treat mini-batch creation strategy as one of the hyper-parameters of the model. With our experimental results, we argue that the mini-batch creation strategies also have an impact on the NMT training, and thus having solid recommendations for how to adjust this hyper-parameter are also o"
W17-3208,C16-2064,0,0.0371884,"Missing"
W17-3208,P17-4012,0,0.0977649,"Missing"
W17-3208,W16-2301,0,\N,Missing
W17-4709,J93-2003,0,0.0967937,"The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, Septem"
W17-4709,J07-2003,0,0.827158,"(de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include E"
W17-4709,P07-1092,0,0.754293,"[X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and targ"
W17-4709,W08-0333,0,0.0164523,"n confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which paral1 Code to replicate the experiments can be found at https://github.com/akivajp/wmt2017 90 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 90–98 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics trained on direct parallel corpora. (Aho and Ullman, 1969; Chian"
W17-4709,N04-1014,0,0.0774748,"ssible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998). When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007). 2.2 rules, it has the tendency to extract very large translation rule tables and also tends to be less syntactically faithful in its derivations. 2.3 Explicitly Syntactic Rules An alternative to Hiero rules is the use of synchronous context-free grammar or synchronous tree-substitution grammar (Graehl and Knight, 2004) rules that explicitly take into account the syntax of the source side (tree-to-string rules), target side (string-to-tree rules), or both (tree-to-tree rules). Taking the example of tree-to-string (T2S) rules, these use parse trees on the source language side, and the head symbols of the synchronous rules are not limited to S or X, but instead use non-terminal symbols corresponding to the phrase structure tags of a given parse tree. For example, T2S rules could take the form of: Hierarchical Rules In this section, we specifically cover the rules used in Hiero. Hierarchical rules are composed"
W17-4709,P15-2094,1,0.752216,"d Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that do not include English. One effective"
W17-4709,W11-2123,0,0.0138784,"trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT"
W17-4709,P13-4016,1,0.850627,"ext with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using"
W17-4709,Q17-1024,0,0.0416509,"Missing"
W17-4709,P11-2093,1,0.727838,"s a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) n"
W17-4709,N15-3009,1,0.821989,"zer, that is although designed mainly for neural MT, we confirmed that it also helps to reduce training time and even improves translation accuracy in our Hiero model as well. We first trained a single shared tokenization model by feeding a total of 10M sentences from the data of all the 6 languages, set the maximum shared vocabulary size to be 16k, and tokenized all available text with the trained model. We used English raw text without tokenization for phrase structure analysis and for training Hiero and T2S TMs on the pivot side. To generate parse trees, we used the Ckylark PCFG-LA parser (Oda et al., 2015), and filtered out lines of length over 60 tokens from all the parallel data to ensure accuracy of parsing and alignment. About 7.6M lines remained. Since Hiero requires a large amount of computational resources for training and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used t"
W17-4709,P02-1040,0,0.0978157,"T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT ) is minimum cost of a sequence of operations (contract an edge, uncontract an edge, modify the label of an edge) needed to transform pˆS into pˆT (Klein, 1998). According to equatio"
W17-4709,P07-2045,0,0.008798,"ng and decoding, so we decided not to use all available training data but first 1M lines for training each TM. As a decoder, we use Travatar (Neubig, 2013), and train Hiero and T2S TMs with its rule extraction code. We train 5-gram LMs over the target side of the same parallel data used for training TMs using KenLM (Heafield, 2011). For testing and parameter tuning, we used the first 1,000 lines of the 4,000 lines test and dev sets respectively. For the evaluation of translation results, we first detokenize with the SentencePiece model and re-tokenized with the tokenizer of the Moses toolkit (Koehn et al., 2007) for Arabic, Spanish, French and Russian and re-tokenized Chinese text with Kytea tokenizer (Neubig et al., 2011), then evaluated using case-sensitive BLEU4 (Papineni et al., 2002). We evaluate 6 translation methods: w(pˆS , pˆT ) · max w(pˆS , pˆ) (21) ˆ PT w(pˆS , pˆ) p∈T p∈T ˆ PT w(pˆS , pˆT ) · max w(ˆ p, pˆT ) (22) ˆ SP p, pˆT ) p∈T p∈T ˆ SP w(ˆ  0 (f lat(pˆS ) = f lat(pˆT )) w(pˆS , pˆT ) = exp (−d (pˆS , pˆT )) (otherwise) d(pˆS , pˆT ) = T reeEditDistance(pˆS , pˆT ) (23) (24) where f lat(ˆ p) returns the symbol string of pˆ keeping non-terminals, and T reeEditDistance(pˆS , pˆT )"
W17-4709,N07-1061,0,0.283363,"o 2.3 BLEU points.1 1 [X2] [X1] 记录 [X1] dossier [X2] (a) Standard triangulation method matching phrases VP VP [X1] enregistrer [X2] TO [X1] 记录 [X2] VB NP record [X2] [X1] NP NP [X1] dossier [X2] DT [X1] [X2] [X1] 记录 NN NP record [X2] (b) Proposed triangulation method matching subtrees Figure 1: Example of disambiguation by parse subtree matching (Fr-En-Zh), [X1] and [X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences"
W17-4709,N03-1017,0,0.272216,"[X2] are non-terminals for sub-phrases. lel data with the source and target languages exists (de Gispert and Mariño, 2006). Among various methods using pivot languages, one popular and effective method is the triangulation method (Utiyama and Isahara, 2007; Cohn and Lapata, 2007), which first combines sourcepivot and pivot-target translation models (TMs) into a source-target model, then translates using this combined model. The procedure of triangulating two TMs into one has been examined for different frameworks of SMT and its effectiveness has been confirmed both in Phrase-Based SMT (PBMT) (Koehn et al., 2003; Utiyama and Isahara, 2007) and in Hierarchical Phrase-Based SMT (Hiero) (Chiang, 2007; Miura et al., 2015). However, word sense ambiguity and interlingual differences of word usage cause difficulty in accurately learning correspondences between source and target phrases, and thus the accuracy obtained by triangulated models lags behind that of models Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are"
W17-4709,L16-1561,0,0.20481,"used in tree-based machine translation frameworks (§2). After describing the baseline triangulation method (§3), which uses only the surface forms for performing triangulation, we propose two methods for triangulation based on syntactic matching (§4). The first places a hard restriction on exact matching of parse trees (§4.1) included in translation rules, while the second places a softer restriction allowing partial matches (§4.2). To investigate the effect of our proposed method on pivot translation quality, we perform experiments of pivot translation on the United Nations Parallel Corpus (Ziemski et al., 2016), which shows that our method indeed provide significant gains in accuracy (of up to 2.3 BLEU points), in almost all combinations of 5 languages with English as a pivot language (§5). In addition, as an auxiliary result, we compare pivot translation using the proposed method with zero-shot neural machine translation, and find that triangulation of symbolic translation models still significantly outperforms neural MT in the zero-resource scenario. In the triangulation method, source-pivot and pivot-target phrase pairs are connected as a sourcetarget phrase pair when a common pivot-side phrase e"
W17-5712,P11-2027,0,0.0225845,"https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and ensemble models, we also calc"
W17-5712,D10-1092,1,0.7921,"W P = 0.75. 2 Model Ensembling https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and"
W17-5712,W16-4601,0,0.192201,"The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture. 1 Introduction Pr(e|f ) = 2.1 Pr(et |e<t , f ), (1) t=1 Neural machine translation (NMT) methods became one of the main-stream techniques in current machine translation studies. Previous WAT campaign showed that NMT methods can achieve higher translation accuracy in spite of simple model configurations (Nakazawa et al., 2016a). In this year, we chose the NMT architecture as our translation systems submitted for WAT2017 English-Japanese Scientific Paper Translation Task (Nakazawa et al., 2017). The main translation model is constructed by an encoder-decoder model (Sutskever et al., 2014) enforced by an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). This paper describes the details of our system, including whole model architecture, training criteria, decoding strategy, and data preparation. Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT"
W17-5712,L16-1350,1,0.888778,"Missing"
W17-5712,P02-1040,0,0.104299,"ystem penalizes shorter sentences, and tends to generate longer sentences. Note that if the beam width is 1, there is no effect from word penalty, because the translation system can generate only 1-best results. Results We trained all translation systems varied by model/training/tokenization hyper-parameters described in previous sections, and performed a grid search to find an optimal set of hyper-parameters for this task. For the training data, we used top 2M sentences in ASPEC corpus (Nakazawa et al., 2016b) provided by the organizer. We chose the optimal model that achieves the best BLEU (Papineni et al., 2002) score over the dev corpus. For the optimal model, we also performed a grid search about decoding-time hyper-parameters. All the optimal hyper-parameters described in previous sections are found as the results of these searches. We submitted two results generated from the same optimal model: one-best results, i.e., the results with fixing BW = 1, and adjusted results, i.e., the results with optimal BW and W P deHyper-parameters In our decoding strategy, We have 2 hyper-parameters: beam width BW and word penalty factor W P . We varied BW from 1 to 128, and W P from 0 to 1.5, and finally chose B"
W18-2711,D15-1166,0,0.434437,"g multi-source NMT implementations without no special modifications. Experimental results with real incomplete multilingual corpora of TED Talks show that it is effective in allowing for multi-source NMT in situations where full multilingual corpora are not available, resulting in BLEU score gains of up to 2 points compared to standard bi-lingual NMT. (3) The method we base our work upon is largely similar to Zoph and Knight (2016), with the exception of a few details. Most notably, they used local-p attention, which focuses only on a small subset of the source positions for each target word (Luong et al., 2015). In this work, we used global attention, which attends to all words on the source side for each target word, as this is the standard method used in the great majority of recent NMT work. 93 Gating Network Es Encoder Decoder Fr Encoder Decoder Ar Encoder Decoder En Es Eso es verdad Fr C&apos;est vrai Ar &lt;NULL&gt; That is true En Figure 4: Multi-encoder NMT with a missing input sentence pre-train Figure 3: Mixture of NMT Experts 2.2 Specifically, we attempt to extend the methods in the previous section to use an incomplete multilingual corpus in this work. Mixture of NMT Experts Garmash and Monz (2016)"
W18-2711,2012.eamt-1.60,0,0.0155388,"vous remercie (a) A standard bilingual corpus English Hello Thank you French Bonjour Je vous remercie Spanish Hola Gracias (b) A complete multi-source corpus English Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet hav"
W18-2711,2001.mtsummit-papers.46,0,0.675579,", such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final pred"
W18-2711,P15-1166,0,0.0351747,"c a estudiar el modelo empresarial. Luego empec a mirar el modelo empresarial. Luego empec a ver el modelo de negocios. 0.266 0.266 0.726 Sometimes they agree. &lt;NULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT fur"
W18-2711,P02-1040,0,0.100903,"of NMT experts, and one-to-one NMT. We used global attention and attention feeding (Luong et al., 2015) for the NMT models and used a bidirectional encoder (Bahdanau et al., 2015) in their encoders. The number of units was 512 for the hidden and embedding layers. Vocabulary size was the most frequent 30,000 words in the training data for each source and target languages. The parameter optimization algorithm was Adam (Kingma and Ba, 2015) and gradient clipping was set to 5. The number of hidden state units in the gating network for the mixture of NMT experts experiments was 256. We used BLEU (Papineni et al., 2002) as the evaluation metric. We performed early stopping, saving parameter values that had the smallest log perplexities on the validation data and used them when decoding test data. 4.2 Es x Data We used UN6WAY (Ziemski et al., 2016) as the complete multilingual corpus. We chose Spanish (Es), French (Fr), and Arabic (Ar) as source languages and English (En) as a target language The training data in the experiments were the one million sentences from the UN6WAY corpus whose sentence lengths were less than or equal to 40 words. We excluded 200,000 sentences for each language for the pseudo-incomp"
W18-2711,N16-1101,0,0.0320253,"ULL&gt; &lt;NULL&gt; A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. A veces estn de acuerdo. 1.000 1.000 1.000 Table 6: Translation examples in {English, French, Brazilian Portuguese}-to-Spanish translation. 5 6 Related Work In this paper, we examined strategies for multisource NMT. On the other hand, there are there are other strategies for multilingual NMT that do not use multiple source sentences as their input. Dong et al. (2015) proposed a method for multitarget NMT. Their method is using one sharing encoder and decoders corresponding to the number of target languages. Firat et al. (2016) proposed a method for multi-source multi-target NMT using multiple encoders and decoders with a shared attention mechanism. Johonson et al. (2017) and Ha et al. (2016) proposed multi-source and multitarget NMT using one encoder and one decoder, and sharing all parameters with all languages. Notably, these methods use multilingual data to better train one-to-one NMT systems. However, our motivation of this study is to improve NMT further by the help of other translations that are available on the source side at test time, and thus their approaches are different from ours. Conclusion In this pa"
W18-2711,L16-1561,0,0.142902,"ench Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-sou"
W18-2711,C16-1133,0,0.437525,"2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of-NMT-experts” method). Es"
W18-2711,N16-1004,0,0.239969,"ean parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garmash and Monz, 2016). Multi-source translation takes in multiple inputs, and references all of them when deciding which sentence to output. Specifically, in the context of neural machine translation (NMT), there are several methods proposed to do so. For example, Zoph and Knight (2016) propose a method where multiple sentences are each encoded separately, then all referenced during the decoding process (the “multi-encoder” method). In addition, Garmash and Monz (2016) propose a method where NMT systems over multiple inputs are ensembled together to make a final prediction (the “mixture-of"
W18-2711,W04-3250,0,0.0543353,"idation and test data for these experiments were also incomplete. This is in contrast to the experiments on UN6WAY where the test and validation data were complete, and thus this setting is arguable of more practical use. Table 4: The percentage of data without missing sentences on TED data. pus, even if just through the simple modification of replacing missing sentences with &lt;NULL&gt;. 4.3.3 Results Table 5 shows the results in BLEU and BLEU gains with respect to the one-to-one results. All the differences are statistically significant (p &lt; 0.01) by significance tests with bootstrap resampling (Koehn, 2004). The multi-source NMTs achieved consistent improvements over the oneto-one baseline as expected, but the BLEU gains were smaller than those in the previous experiments using the UN6WAY data. This is possibly With respect to the difference between the multi-encoder NMT and mixture of NMT experts, the multi-encoder achieved much higher BLEU in Pseudo-incomplete (0.8M) and Complete (1M), but this was not the case in Complete (0.2M). One possible reason here is the model complexity; the multi-encoder NMT uses a large single model while one-to-one sub-models in the mixture of NMT experts can be tr"
W18-2711,2005.mtsummit-papers.11,0,0.129127,"h Hello Thank you French Spanish Hola × × Bonjour Je vous remercie (c) An incomplete multi-source corpus with missing data Figure 1: Example of type of corpora. languages involved in the translation process. For example, we may have an original document in English, that we want to translate into several languages such as French, Spanish, and Portuguese. Some examples of these scenarios are the creation of video captions for talks (Cettolo et al., 2012) or Movies (Tiedemann, 2009), or translation of official documents into all the languages of a governing body, such as the European parliament (Koehn, 2005) or UN (Ziemski et al., 2016). In these cases, we are very often faced with a situation where we already have good, manually cu92 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 92–99 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics rated translations in a number of languages, and we’d like to generate translations in the remaining languages for which we do not yet have translations. In this work, we focus on this sort of multilingual scenario using multi-source translation (Och and Ney, 2001; Zoph and Knight, 2016; Garma"
W19-4106,P14-1092,0,0.021216,"pan Science and Technology Agency {takana.shohei.tj7, koichiro, sudoh, s-nakamura}@is.naist.jp Abstract the relation between dialogue continuity and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for sel"
W19-4106,N06-1023,0,0.011883,"s also reported that a conversational model using event causality relations can generate diverse and coherent responses (Fujita et al., 2011). However, 2 Response Re-ranking Using Event Causality Relations Figure 1 shows an overview of the proposed method. The process consists of four parts. First, N -best response candidates are generated from 1 an NCM given a dialogue history (Figure 1 ; Section 2.1). Then, events (predicate-argument structures) are extracted by an event parser from both the dialogue history and the response candi2 dates (Figure 1 ). We used Kurohashi Nagao Parser (KNP)1 (Kawahara and Kurohashi, 2006; Sasano and Kurohashi, 2011) as the event parser. Next, the extracted events are converted to dis1 http://nlp.ist.i.kyoto-u.ac.jp/?KNP 51 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 51–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Neural conversational model+re-ranking using event causality; a response that has an event causality relation (“be exhausted” → “relax”) to the dialogue history is selected by the re-ranking. predicate 1 be stressed out argument 1 - predicate 2 relieve lif t 10.02 argument 2 stress Table 1: Exa"
W19-4106,D18-2012,0,0.0232194,"l., 2018) was 100. We used gated recurrent units (GRUs) (Cho et al., 2014; Chung et al., 2014) whose number of layers was 2 and hidden unit size was 256, for the encoder and decoder of the NCMs. The batch size was 100, the dropout probability was 0.1, and the teacher forcing rate was 1.0. We used Adam (Kingma and Ba, 2015) as the optimizer. The gradient clipping was 50, the learning rate for the encoder and the context RNN of HRED was 1e−4 , and the learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue hi"
W19-4106,N16-1154,0,0.0202521,"ology Agency {takana.shohei.tj7, koichiro, sudoh, s-nakamura}@is.naist.jp Abstract the relation between dialogue continuity and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse"
W19-4106,N16-1014,0,0.0329726,"encoder and the context RNN of HRED was 1e−4 , and the learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue history is defined as, 3.2 Diversity of Beam Search We investigated internal diversity of N -best response candidates generated from each dialogue model. It is expected that the higher diversity is, the more effective re-ranking is. Hence, we evaluated diversity on the test data by dist-1, 2 (Li et al., 2016). Beam width was set to 20; it is same in the PMI = 1 |response| |response| ∑ wr"
W19-4106,D16-1230,0,0.0260094,"LEU, NIST, and PMI values than those of EncDec models in all re-ranking methods, we conducted a human evaluation by comparing HRED model-based systems. 4 Discussion We analyzed an adequacy of re-ranking using event causality relations. Here are system response examples of our proposed method. “()” indicates original Japanese sentences, “[]” indicates event causality relations used for re-ranking, and “<&gt;” indicates responses before re-ranking. All examples are translated from Japanese to English. 3.4 Human Evaluation It is difficult to evaluate system performances only with automatic metrics (Liu et al., 2016). Hence, we compared a baseline model and our models in a human evaluation to confirm coherency and dialogue continuity of responses selected by our proposed methods. We compared baseline HRED model with our proposed models, re-ranked without embedding and with embedding using the last Conversation 1: User 1: Because of my fears, I have been stressed out. 55 (Mou fuan-na koto ga oosugite sutoresu ga tamatteku.) User 2 (System): Are you OK? Don’t work too hard. ß (Daijobu desuka muri shinaide kudasaine) [work too hard → be stressed out (muri wo suru → sutoresu ga tamaru)] <Are you OK? (Daijobu"
W19-4106,D15-1166,0,0.0287431,"Missing"
W19-4106,P02-1040,0,0.103893,"the training. The dialogue corpus was split into 2,509,836, 63,308, and 58,970 dialogues as training, validation, and testing data, respectively. following experiments. The result is shown in Table 2: Ave.dists are averages of dist computed internal N -best response candidates. The diversity of EncDec is higher than that of HRED. 3.3 Comparison in Automatic Metrics Table 3 shows the results of our evaluation using automatic metrics. We compared the results by referring to the ratio of responses different from the without re-ranking method (“re-ranked”), bilingual evaluation understudy (BLEU) (Papineni et al., 2002), NIST (Doddington, 2002), and vector extrema (Gabriel et al., 2014) (“extrema”) score. NIST is based on BLEU, but heavily weights less frequent N-grams to focus on content words. Vector extrema computes cosine similarity between sentence vectors of a reference and a generated response from a model. Each sentence vector es is computed by taking extrema of Skip-gram word vectors ew in each dimension d as, { maxw∈s ewd if ewd &gt; |minw′ ∈s ew′ d | esd = . minw∈s ewd otherwise (5) 3.1 Model Settings The hidden unit size of Skip-gram (Mikolov et al., 2013c,a,b), predicate embedding, and RFTM (Weber"
W19-4106,N13-1090,0,0.159864,"alogue with those in the event causality pair pool. Any events are embedded into fixed length vectors to calculate their similarities. 52 Figure 2: Model architecture of predicate embedding Figure 3: Event causality relation matching; the lif t of the event causality relation in which “be exhausted” precedes “relax,” is calculated from the lif t of the most similar event causality relation where “be stressed out” precedes “relieve stress.” We define an event with a single predicate or a pair of a predicate and arguments. Argument a of an event is embedded into vector as va by using Skip-gram (Mikolov et al., 2013c,a,b). Predicate p of an event is embedded into vector as vp by using predicate embedding which is based on case-unit Skip-gram. Figure 2 shows the model architecture of predicate embedding. The model learns predicate vector representations which are good at predicting its arguments. To get an event embedding for the pair of vp and va , we propose to use RFTM, which was proposed by Weber et al. (2018). The RFTM embeds a predicate and its arguments into vector e as, ∑ e= Wa T (vp , va ). (2) Ave.dist-1 0.44 0.33 EncDec HRED Ave.dist-2 0.56 0.42 Table 2: Diversity of N -best Response Candidates"
W19-4106,I11-1085,0,0.0125595,"ational model using event causality relations can generate diverse and coherent responses (Fujita et al., 2011). However, 2 Response Re-ranking Using Event Causality Relations Figure 1 shows an overview of the proposed method. The process consists of four parts. First, N -best response candidates are generated from 1 an NCM given a dialogue history (Figure 1 ; Section 2.1). Then, events (predicate-argument structures) are extracted by an event parser from both the dialogue history and the response candi2 dates (Figure 1 ). We used Kurohashi Nagao Parser (KNP)1 (Kawahara and Kurohashi, 2006; Sasano and Kurohashi, 2011) as the event parser. Next, the extracted events are converted to dis1 http://nlp.ist.i.kyoto-u.ac.jp/?KNP 51 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 51–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Neural conversational model+re-ranking using event causality; a response that has an event causality relation (“be exhausted” → “relax”) to the dialogue history is selected by the re-ranking. predicate 1 be stressed out argument 1 - predicate 2 relieve lif t 10.02 argument 2 stress Table 1: Example of event causality relat"
W19-4106,shibata-etal-2014-large,0,0.283568,"responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causal"
W19-4106,N10-1012,0,0.0222227,"e learning rate for the decoder was 5e−4 . The loss function was inverse token frequency (ITF) loss (Nakamura et al., 2019). We used sentencepiece (Kudo and Richardson, 2018) as the tokenizer, and the vocabulary size was 32,000. These settings were the same in all models. Repetitive suppression (Nakamura et al., 2019) and length normalization (Macherey et al., 2016) were used at the decoding step. Finally, λ of Eq. (1) and Eq. (4) was set to 1.0. esd and ewd are the dth dimensions of es and ew respectively. Additionally, we evaluated dist (Li et al., 2016), Pointwise Mutual Information (PMI) (Newman et al., 2010), and average response length (“length”). Dist and PMI are used to evaluate diversity and coherency respectively. PMI between a response and a dialogue history is defined as, 3.2 Diversity of Beam Search We investigated internal diversity of N -best response candidates generated from each dialogue model. It is expected that the higher diversity is, the more effective re-ranking is. Hence, we evaluated diversity on the test data by dist-1, 2 (Li et al., 2016). Beam width was set to 20; it is same in the PMI = 1 |response| |response| ∑ wr max PMI(wr, wh). wh (6) wr and wh are words in the respon"
W19-4106,I11-1115,0,0.409655,"and the coherency of system responses is still an underlying problem. In this paper, we propose a novel method to select an appropriate response from response candidates generated by NCMs. We define a score for re-ranking to select a response that has an event causality relation to a dialogue history. Re-ranking effectively improves response reliability in language generation tasks such as why-question answering and dialogue systems (Oh et al., 2013; Jansen et al., 2014; Bogdanova and Foster, 2016; Ohmura and Eskenazi, 2018). We used event causality pairs extracted from a large-scale corpus (Shibata and Kurohashi, 2011; Shibata et al., 2014). We also use distributed event representation based on the Role Factored Tensor Model (RFTM) (Weber et al., 2018) to realize a robust matching of event causality relations, even if these causalities are not included in the extracted event causality pairs. In human and automatic evaluations, the proposed method outperformed conventional methods in selecting coherent and diverse responses. We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational model"
W19-4106,P13-1170,0,0.0793544,"Missing"
Y13-1026,W10-1736,1,0.929613,"dering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of thi"
Y13-1026,D11-1017,0,0.019774,"pes of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, P"
Y13-1026,W08-0509,0,0.0238615,"of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities betw"
Y13-1026,W00-1303,0,0.0484713,"PC is limited to dependency structure and POS tags, for analysis on the causes of reordering errors, we examine parsing errors from these two linguistic categories. In this section, the value of Kendall’s tau measures the word order similarity between Gold-DPC and Auto-DPC. Figure 2: The distribution of Kendall’s tau values for 2, 236 bilingual sentences (Chinese-Japanese) in which the Chinese is from three systems of baseline, Auto-DPC, and Gold-DPC. file, ch-ja.A3.final. The comparison implies how monotonically the Chinese sentences have been reordered to align with Japanese. We use MeCab6 (Kudo and Matsumoto, 2000) to segment Japanese sentences and also filter out sentences with more than 64 tokens. There are 2, 236 valid Chinese-Japanese bilingual sentences in total. Figure 2 shows the distribution of Kendall’s tau from three systems in which the baseline is built up by using ordinary Chinese. In Figure 2, baseline system contains a large numbers of non-monotonic aligned sentences, whereas both Auto-DPC and Gold-DPC increased the amount of sentences that achieved high τ values. Reordering based on gold-tree reduced more percentage of low τ sentences than reordering based on automatically parsed trees."
Y13-1026,C10-1043,0,0.040679,"ord correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insig"
Y13-1026,D07-1013,0,0.0632143,"noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the benchmark in scenario 2 and"
Y13-1026,gimenez-marquez-2008-towards,0,0.0609847,"Missing"
Y13-1026,J08-1002,1,0.759309,"he work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the b"
Y13-1026,P11-3013,0,0.0255294,"ependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing erro"
Y13-1026,W06-1608,0,0.178502,"of parsing errors on reordering performance. In this analysis, we borrow this state-of-the-art pre-reordering model for our experiments since it is a rule-based pre-reordering method for a distant language pair based on dependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. B"
Y13-1026,W12-4207,1,0.931686,"ons of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of this work is two fold"
Y13-1026,I11-1035,0,0.0178476,"nce separately, we quantify the extent of parsing errors that 4 4.1 Preliminary Experiment Gold Data In order to build up gold parse tree sets for comparison, we used the annotated sentences from Chinese Penn Treebank ver. 7.0 (CTB-7) which is a well known corpus that consists of parsed text in five genres. They are Chinese newswire (NS), magazine news (NM), broadcast news (BN), broadcast conversation programs (BC), and web newsgroups, weblogs (NW). We first randomly selected 517 unique sentences (hereinafter set-1) from all five genres in development set of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out th"
Y13-1026,W13-2806,1,0.816096,"Missing"
Y13-1026,C04-1073,0,0.102245,"plore every possible word correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing er"
Y13-1026,N09-1028,0,0.0647854,"complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. T"
Y13-1026,D09-1121,1,0.844566,"sing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the genera"
Y13-1026,W11-2907,1,0.848779,"ubclasses according to the methodology of the reordering method. We then plot the distribution of these parsing errors for various reordering qualities. In Section 5.2, we illustrate these parsing errors with examples. comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models. At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks. Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English. There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues. Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship. On the other hand, Gim´enez and M`arquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants. H"
Y13-1026,I11-1136,1,0.828769,"ncomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities between manually reordered Chinese sentences and automatically reordered Chinese sentences from set-1. Table 3 shows the average τ value. For baseline system, the average τ value shows h"
Y13-1026,D07-1112,0,\N,Missing
Y18-3001,Y18-3013,1,0.888887,"Missing"
Y18-3001,Y18-3003,1,0.889659,"Missing"
Y18-3001,Y18-3005,0,0.0459044,"Missing"
Y18-3001,P17-4012,0,0.0426364,". 3 each participant’s system. That is, the specific baseline system was the standard for human evaluation. At WAT 2018, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system except for the IITB tasks. We used a phrasebased statistical machine translation (SMT) system, which is the same system as that at WAT 2017, as the baseline system for the IITB tasks. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.5 We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems. In addition to the NMT baseline systems, we have SMT baseline systems for the tasks that started at last year or before last year. The baseline systems are shown in Tables 8, 9, and 10. SMT baseline systems are described in the previous WAT overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online tra"
Y18-3001,Y18-3002,0,0.0375601,"Missing"
Y18-3001,P07-2045,0,0.0120426,"in frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeC"
Y18-3001,W04-3250,0,0.312277,"Missing"
Y18-3001,Y18-3007,0,0.0326187,"Missing"
Y18-3001,Y18-3014,0,0.0304859,"Missing"
Y18-3001,W14-7001,1,0.458489,"ion (WAT2018) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Zh↔Ja, K↔Ja, En↔Ja patent translation subtasks, Hi↔En, My↔En mixed domain subtasks and Bn/Hi/Ml/Ta/Te/Ur/Si↔En Indic languages multilingual subtasks. For the WAT2018, 17 teams participated in the shared tasks. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2017 (Nakazawa et al., 2014; Nakazawa et al., 2015; Nakazawa et al., 2016; Nakazawa et al., 2017), WAT2018 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 5th WAT, we adopted new translation subtasks with Myanmar ↔ EnSadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp glish mixed domain corpus1 and Bengali/Hindi/Malayalam/Tamil/Telugu/Urdu/Sinhalese ↔ English OpenSubtitles corpus2 in addition to the subtasks at WAT2017. WAT is the uniq"
Y18-3001,W16-4601,1,0.938773,"Missing"
Y18-3001,P11-2093,0,0.0504246,"leu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.14 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.15 For Korean segmentation, we 11 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 12 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/ 13 http://www.phontron.com/kytea/model. html 14 http://code.google.com/p/mecab/ downloads/detail?name=mecab-ipadic-2.7. 0-20070801.tar.gz 15 http://nlp.stanford.ed"
Y18-3001,Y18-3011,0,0.141475,"Missing"
Y18-3001,P02-1040,0,0.119424,"://bitbucket.org/anoopk/indic_nlp_ library 10 https://github.com/rsennrich/ subword-nmt • tgt vocab size = 100000 • src words min frequency = 1 • tgt words min frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanes"
Y18-3001,Y18-3010,0,0.0585011,"Missing"
Y18-3001,Y18-3012,0,0.0444067,"Missing"
Y18-3001,2007.mtsummit-papers.63,0,0.0425147,"itute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja↔en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja↔zh subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score that are calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document ali"
Y18-3001,Y18-3017,0,0.0591037,"Missing"
Y18-3001,Y18-3006,1,0.868302,"Missing"
