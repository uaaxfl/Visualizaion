2020.isa-1.4,kordjamshidi-etal-2010-spatial,0,0.0694431,"Missing"
2020.isa-1.4,L16-1730,0,0.0266795,"EM D ESCRIPTORS (TSD). Before the ISO models can be used in UIMA, they have to be transferred to TSD. This is the first step towards collaborative annotation in a visually supporting interface. The annotation can then be enriched by T EXT A NNOTATOR embedded into VA NNOTATO R. This enables spatial annotations with a 3D interface in VR. In addition, spatial entities can be directly linked to 3D objects via a large number of categorized objects from ShapeNet (Chang et al., 2015), the slightly deeper annotated objects from ShapeNetSem (Savva et al., 2015), objects annotated using VoxML notation (Pustejovsky and Krishnaswamy, 2016) (under development) or via abstract representations (as exemplified in Figure 1). Simply by placing the objects in space, conclusions can be drawn about the relationships between them (and thus also about QSLinks and OLinks) because the information bandwidth of annotation acts in VR is much larger than with pure text annotation. For example, if a book is placed on the desk in VR, the corresponding QSLink and OLink can be set automatically with their relevant attributes. Such concrete pictorial representations are not always unambiguous, but in conjunction with the corresponding sentence, clas"
2020.isa-1.4,pustejovsky-yocum-2014-image,0,0.0201022,"ustejovsky et al., 2011; ISO, 2014a). The focus is on spatial and spatial-temporal relations between (spatial) entities and the connection via motion events. Spatial Entities are marked and connected to each other via different spatial connections. QSLinks (Qualitative Spatial Links) are for topological relations, OLinks (Orientation Links) for non-topological relations and MoveLinks for movements of entities in space. This scheme was the basis of SpaceEval (Pustejovsky et al., 2015) and was successfully applied to image descriptions to differentiate between content and structural statements (Pustejovsky and Yocum, 2014). ISOSpace in particular is being further improved (ISO, 2019) and serves as a basis for more specialized models, such as SceneML (Gaizauskas and Alrashid, 2019) for scene descriptions. In addition, SemAF contains schemata such as Semantic Roles (ISO, 2014b), Dialog Acts (ISO, 2012b) and other modules are under development, e.g. QuantML (Bunt et al., 2018). Humans have a strong spatial perception. This is reflected not only in how well people can adapt to new spatial environments, but also in their language (Haun et al., 2011). In recent years there have been increased efforts to create a ling"
2020.isa-1.4,pustejovsky-etal-2010-iso,0,0.0463398,"ch in a transformed learning environment. In Linda Daniela, editor, New Perspectives on Virtual and Augmented Reality. Taylor & Francis. in press. Abrami, G., Stoeckel, M., and Mehler, A. (2020b). TextAnnotator: A UIMA based tool for simultaneous and collaborative annotation of texts. In Proc. of LREC 2020, LREC 2020. accepted. Ahmed, S., Stoeckel, M., Driller, C., Pachzelt, A., and Mehler, A. (2019). BIOfid Dataset: Publishing a german gold standard for named entity recognition in historical biodiversity literature. In Proc. of CoNLL 2019. Bateman, J. A., Hois, J., Ross, R., and Tenbrink, T. (2010). A linguistic ontology of space for natural language processing. Artificial Intelligence, 174(14):1027–1071. Bateman, J. A. (2010). Language and space: A two-level semantic approach based on principles of ontological engineering. Int J Speech Tech, (1):29–48. https://www.biofid.de/en/ 34 Bunt, H., Pustejovsky, J., and Lee, K. (2018). Towards an ISO standard for the annotation of quantification. In Proc. of LREC 2018. Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., and Yu, F. (2015). ShapeNet: An informatio"
2020.isa-1.4,P84-1044,0,0.335173,"Missing"
2020.isa-1.4,verhagen-etal-2006-annotation,0,0.0726905,"ect aims at, could significantly support these annotation tasks. 1 https://unity.com/ 33 Figure 2: Workflow for ISOSpace Annotation. Blue borders stand for the original annotation steps (Pustejovsky et al., 2015). Red filled for VR support and orange for machine learning support. Span tagging can be supported with a sequence labeling system. And the link inference engine learns through annotations. cases automatically predicted and attributed, e.g., by examining transitive relations. Such support has also been successfully applied to the annotation of the TimeML standard (Setzer et al., 2005; Verhagen et al., 2006; Verhagen, 2007). The underlying workflow is shown in Figure 2. A central challenge will be the underspecification of scene descriptions. Related issues concern descriptions containing negations. Though we do not yet have a solution to solve the problems involved, we assume that by combining spatial experience in VR with annotation services provided by annotators, for example, underspecified reference relations can be annotated by exploring additional information with regard to the annotators’ positions in relation to referred objects. In examples such as “There is no book on the table” a cor"
2020.lrec-1.112,L18-1212,1,0.849005,"f NLP. In order to preprocess new documents and convert them into the UIMA format, T EXTA NNOTATOR uses T EX T I MAGER (Hemati et al., 2016), which provides various pre-processing pipelines for a variety of languages. This results in document-based stand-off annotations that enrich the input texts on the basis of established pipelines. Since the focus of T EXTA NNOTATOR are is on NLP and because there are a multitude of automatic pre-processing routines (c.f. Eckart de Castilho and Gurevych (2014), Bethard et al. (2014), Hemati et al. (2016)) as well as a database solution for UIMA documents (Abrami and Mehler, 2018), UIMA builds the core environment for T EXTA NNOTATOR to ensure interchangeability and interoperability. However, the existence of interchangeability does not necessarily guarantee the reuse of the annotation schemes defined in UIMA, as demonstrated in (Rak and Ananiadou, 2013). The authors show that many projects develop their own schemes for the execution of annotations, although such schemes are already available through other projects as TSD. Even if this results in a valid type system, this is not yet really usable, since a separate analysis engine must be developed for each type system."
2020.lrec-1.112,bethard-etal-2014-cleartk,0,0.0297306,"managed within T EXTA NNOTATOR are based on UIMA, which is now a de facto standard in the area of NLP. In order to preprocess new documents and convert them into the UIMA format, T EXTA NNOTATOR uses T EX T I MAGER (Hemati et al., 2016), which provides various pre-processing pipelines for a variety of languages. This results in document-based stand-off annotations that enrich the input texts on the basis of established pipelines. Since the focus of T EXTA NNOTATOR are is on NLP and because there are a multitude of automatic pre-processing routines (c.f. Eckart de Castilho and Gurevych (2014), Bethard et al. (2014), Hemati et al. (2016)) as well as a database solution for UIMA documents (Abrami and Mehler, 2018), UIMA builds the core environment for T EXTA NNOTATOR to ensure interchangeability and interoperability. However, the existence of interchangeability does not necessarily guarantee the reuse of the annotation schemes defined in UIMA, as demonstrated in (Rak and Ananiadou, 2013). The authors show that many projects develop their own schemes for the execution of annotations, although such schemes are already available through other projects as TSD. Even if this results in a valid type system, this"
2020.lrec-1.112,N13-3004,0,0.0226909,"In addition, external automatic pre-processing methods can be used and documents and user access permissions can be managed; the latter in a hierarchical manner, since it allows for groupbased curations. Furthermore, users can collaborate by sharing results, documents, and workflows, but the collaboration is not simultaneous. Thus Argo fulfills the requirements (a), (b), (d) and partly also (e). By and large, this is a very flexible tool, but it lacks simultaneous use and the ability to annotate more complex text structures in graphical mode. Another browser-based annotation tool is Anafora (Chen and Styler, 2013). Anafora is designed to allow multiple users to annotate documents in the same way, but not simultaneously. The documents to be annotated are saved as XML files without any database support. Additionally, the annotation scheme is based on XML, the same format in which the annotations are stored. A human-readable and proprietary annotation schema is used, allowing easy understanding and manual porting, but preventing automatic and systematic migration due to missing standards. Thus, Anafora fails to meet or only partly meets most of the requirements enumerated in Section 1. Tool MAE2 Brat WebA"
2020.lrec-1.112,W14-5201,0,0.0417982,"Missing"
2020.lrec-1.112,W16-4011,0,0.0457477,"Missing"
2020.lrec-1.112,D17-1063,0,0.0192556,"results will of course be incorporated into the next development milestones, which will be presented in the next section. 7. Future Work T EXTA NNOTATOR will be further developed regarding various aspects: first of all, the feature to select and annotate discontinuous text segments will be extended. For all visualizations provided by T EXTA NNOTATOR, a LATEX (TiKZ) export will be provided to obtain customizable LATEX source files as well as high-quality vector graphics, thereby following the example of TreeAnnotator. In addition, T EX TA NNOTATOR will implement a generic active learning unit (Fang et al., 2017) that supports the annotators. An important issue in this respect is the easy inclusion of annotation schemes: currently, new annotation schemes can only be applied by creating new TSDs, which requires a restart of Conclusion In this paper, we presented the current development state of T EXTA NNOTATOR as a browser-based and collaborative text annotation tool that performs various linguistic annotation tasks within the same framework. By the multitude of annotation tools existing at the present time and referring to their functional heterogeneity, requirements have been defined which should be"
2020.lrec-1.112,L18-1308,1,0.892314,"Missing"
2020.lrec-1.112,C16-2013,1,0.943264,"he data with additional information and subsequently allow them to be processed according to the needs of the project at hand. For this, over the past years, many annotation tools have been developed for various tasks (for a recent process and feature oriented overview see (Finlayson and Erjavec, 2017); for an overview of multimodal tools see (Cassidy and Schmidt, 2017)). In addition to manual annotation tools, there is a proportionately larger number of automatic pre-processing tools. Despite the diversity of automated pre-processing applications (e.g. (Eckart de Castilho and Gurevych, 2014; Hemati et al., 2016)), a post-correction is often required because no complete coverage is achieved. For the efficient correction of errors in automatic pre-processing as well as for the generation of training data for machine learning and their efficient management, it is necessary to use a suitable annotation tool. This suitability of an annotation tool depends highly on the topic of the annotation task. In this context, it is rather unlikely to annotate, for example, spoken language, handwritten recordings, written text, videos and images with a single tool. However, if different tools are used, a schema langu"
2020.lrec-1.112,W04-0208,0,0.0390064,"vailable data from historical texts from the field of biology that can be used to investigate processes of climate change over long periods of time. To be flexible, all selectable visual components of QuickAnnotator are dynamically customizable, but currently not configurable via a separate graphical user interface. Figure 3: A snapshot of QuickAnnotator: The tokens of the input text are presented as interactive elements; on the right side one sees the classes to be annotated. TimeAnnotator TimeAnnotator enables the annotation of temporal structures in texts. The underlying annotation scheme (Mani and Pustejovsky, 2004) allows the annotation of a temporal structure as a relative sequence of events described in the input text and represented as a tree. To complement this, we additionally apply the approach of Stede (2007) so that points in time can also be annotated. In other words, each annotated node in a time tree managed by TimeAnnotator has the ability to receive temporal annotations in the form of concrete timestamps, time spans or relational expressions (Abrami et al., 2019). KnowledgeBaseLinker T EXTA NNOTATOR provides an annotation component for annotating named entities, the so-called KnowledgeBaseL"
2020.lrec-1.112,C14-2023,0,0.315718,"ructures, etc.) and to perform disambiguation 891 using the same tool. Ideally, this means that users can create dynamic annotation schemas on their own. (d) Multi-Platform System: A multi-platform system is a tool that provides an API that makes it possible to use it platform and software independent. (e) Multi-Analysis System: A multi-analysis system enables the further processing of annotation data and advanced data analyses independent of the annotation itself. This implies the possibility of instantaneous checking the annotation quality, for example, with inter-annotator agreement (c.f. (Meyer et al., 2014, IAA)) methods. In addition, annotation projects based on annotations already performed can be structured accordingly. In this way, it is possible, for example, to annotate only texts for which the IAA value of a selected class is too low. (f) Collaborative and simultaneous use: Dealing with complex annotation tasks requires that several users can simultaneously edit the same documents at the same time. Simultaneous use allows for a collaborative workflow and should significantly increase the efficiency of annotation. However, it should be possible to disable simultaneous use of the system fo"
2020.lrec-1.112,Q14-1019,0,0.0107219,"ABELFY6 . However, KBL supports the integration and linking of a wide range of knowledge databases based on configurations managed by T EXTA N NOTATOR . Designed to expand on existing tools, KBL combines already implemented NLP tools from T EXT I MAGER with an easy to use graphical interface. Each token is automatically represented by an annotation box, which holds references and quick overviews for each linked ontology, including images, hyperlinks and short descriptions (see Fig. 4). Currently, Wikidata, Wikipedia, Wiktionary, Geonames7 , the German National Library8 , GermaNet and Babelfy (Moro et al., 2014) as well as ontologies from die BIOfid project are accessible and can quickly be searched within T EX TA NNOTATOR . For a more fine-grained NE detection, we developed the TTL AB NAMED E NTITY T YPE system, which distinguishes 15 different NE types and 90 subtypes (see Nagel (2008), Debus (2012), Kamianets (2000), Brendler (2004), Vasil’eva (2011), Wiktionary, Urban Dictionary9 and the ICOS List of Key Onomastic Terms10 ). 4. All annotations are created using T EXTA NNOTATOR’s API and each annotation is connected with an active user who has the permission to modify the documents accordingly. Th"
2020.lrec-1.112,W13-2311,0,0.0805519,"ich the input texts on the basis of established pipelines. Since the focus of T EXTA NNOTATOR are is on NLP and because there are a multitude of automatic pre-processing routines (c.f. Eckart de Castilho and Gurevych (2014), Bethard et al. (2014), Hemati et al. (2016)) as well as a database solution for UIMA documents (Abrami and Mehler, 2018), UIMA builds the core environment for T EXTA NNOTATOR to ensure interchangeability and interoperability. However, the existence of interchangeability does not necessarily guarantee the reuse of the annotation schemes defined in UIMA, as demonstrated in (Rak and Ananiadou, 2013). The authors show that many projects develop their own schemes for the execution of annotations, although such schemes are already available through other projects as TSD. Even if this results in a valid type system, this is not yet really usable, since a separate analysis engine must be developed for each type system. This is still an unsolved problem that requires considerable research which will be an open task for the further development of UIMA and T EXTA NNOTATOR. The paper is structured as follows: After the overview of existing annotation systems in Section 2, Section 3 gives a detail"
2020.lrec-1.112,rak-etal-2012-collaborative,0,0.0333321,"). Another annotation tool which is based on BRAT is WebAnno (Eckart de Castilho et al., 2016). WebAnno supports simple document- and user management functionalities and uses BRAT for performing and visualizing annotations. In addition, graphically supported annotations can be performed and a function is available for comparing annotations (curator mode) and for calculating the IAA. 892 However, WebAnno does not support a remote API and the collaborative simultaneous annotation of documents is not possible. Thus, WebAnno fulfills the requirements (a), (c), (e), and (partially) also (b). Argo (Rak et al., 2012), an annotation editor based on UCompare (Kano et al., 2009), enables the automatic and manual annotation of texts based on UIMA. Argo offers a web-based interface and an API to connect custom services to execute task-related annotation operations. In addition, external automatic pre-processing methods can be used and documents and user access permissions can be managed; the latter in a hierarchical manner, since it allows for groupbased curations. Furthermore, users can collaborate by sharing results, documents, and workflows, but the collaboration is not simultaneous. Thus Argo fulfills the"
2020.lrec-1.112,E12-2021,0,0.106008,"Missing"
2020.lrec-1.4,D19-1588,0,0.0109481,"edent. 4.2. that directly dominates a pronominal anaphoric mention, we add a dependency link from this token to the nonpronominal antecedent of lowest index of this pronoun. If this antecedent consists of several tokens, the root node of the corresponding dependency subtree is used as the target of the link. More formally: for each anaphoric pronoun wk ∈ D−1 (w, T ) depending on token d(wk ) of lemma w = L(d(wk )) such that there exists a mention wk = mj ∈ M (pronominal mentions are one-place), we extend the set of dependents D−1 (w, T ) of w as follows: D˙ −1 (w, T ) We used Spanbert-Base of Joshi et al. (2019a) for coreference resolution. For the needed dependency features we used the AllenNLP’s (Gardner et al., 2018) implementation of Dozat and Manning (2016). For tokenization, lemmatization and POS tags, Spacy (Honnibal and Montani, 2017) was used. = D−1 (w, T ) ∪ 5. {r(tree(mi )) |∃mj ∈ M ∃wk ∈ D−1 (w, T ) : wk = mj ∧ mi  mj } 5.1. (8) 4.1. Evaluation Word Similarity The first analyses on the generated word vectors ran over various word similarity tasks. All results are listed in table 2. For evaluation, we used the benchmark tool of Jastrzebski et al. (2017)1 as it computes the accuracy for a"
2020.lrec-1.4,S12-1047,0,0.0680024,"Missing"
2020.lrec-1.4,N16-1175,0,0.097418,"res such as POS-tags, subword information, semantic relations and in-domain data on word embeddings have been analyzed (Rezaeinia et al., 2017; Wendlandt et al., 2018; Bojanowski et al., 2017; Boleda et al., 2017; Gupta et al., 2017) and improved results have been obtained. In this paper we complement this research and ask about the effects of CR on word embeddings. This is done by example of six methods of computing word embeddings: Cbow (Mikolov et al., 2013), Skip (Mikolov et al., 2013), Glove (Pennington et al., 2014), Wang (Ling et al., 2015), Levy (Levy and Goldberg, 2014) and Komninos (Komninos and Manandhar, 2016). . . . his poetry. 7→ . . . Edgar Allan Poe poetry. So far, our replacement procedure only considers pronouns. The reason is that we expect the greatest loss of information from not replacing them. In this way, we avoid problems that we would get if we replaced phrasal mentions (e.g. more complex noun phrases) with their phrasal antecedents. 3.2. Extending the informational scope of dependency-based embeddings For embeddings derived from dependency trees, we choose an approach that explores the underlying dependency relations. Let D(w, T ) = {d(wi1 ), . . . , d(wik )} (6) be the set of all pa"
2020.lrec-1.4,D17-1018,0,0.0331857,"Missing"
2020.lrec-1.4,N18-2108,0,0.0289163,"Missing"
2020.lrec-1.4,P14-2050,0,0.601754,"spect to future work is presented in Section 6. Many NLP systems use word embeddings as a fast to learn resource that captures important lexical information (Mikolov et al., 2013). Once trained, embeddings can be used in many different tasks, like Coreference Resolution (Lee et al., 2018), Emotion Detection (Felbo et al., 2017), Biomedical Natural Language Processing (Wang et al., 2018), Image Caption Generation (Vinyals et al., 2015) or Text Classification (Uslu et al., 2019). Most of them rely on local information delimited by context windows or dependency parents to predict word relations (Levy and Goldberg, 2014). This approach encounters problems wherever semantic relationships have to be captured, which are expressed by coreference, as the following example illustrates: Edgar Allan Poe was an American writer. Poe is best known for his poetry. Based on a context window-based approach of a maximum of five right neighbors, we get data to examine the relationship of Poe and writer and of his and poetry. But the model is not informed about a relationship between Poe and poetry when using a too small window. Obviously, the detour via the use of overly large window sizes (which would capture wanted as well"
2020.lrec-1.4,N15-1142,0,0.0767508,"Missing"
2020.lrec-1.4,P19-1441,0,0.0223345,"to extending window-based embeddings by means of CR is the following: For all pronominal mentions mi , for which first(mi ) is not pronominal, we replace: mi ← first(mi ) (5) This means that we replace each pronoun with its lowest index antecedent which in our case is represented by a corresponding lemma or multiword expression as shown in the following example: Only recently, new systems have been introduced which are trained on large contexts using LSTMs (Peters et al., 2018) or large neural attention systems (Transformers) based on more complex transfer-learning tasks (Devlin et al., 2018; Liu et al., 2019) and are therefore not limited to local information – but at the price of additional computational complexity. At the same time, the list of proposals for new embedding methods that are pre-trained on ever larger corpora from more and more areas (genres, registers etc.) of more and more languages is constantly growing (Grave et al., 2018; Bojanowski et al., 2017; Radford et al., 2019). In recent years, the impact of various features such as POS-tags, subword information, semantic relations and in-domain data on word embeddings have been analyzed (Rezaeinia et al., 2017; Wendlandt et al., 2018;"
2020.lrec-1.4,W13-3512,0,0.105416,"Missing"
2020.lrec-1.4,Q17-1010,0,0.117367,"Missing"
2020.lrec-1.4,E17-2013,0,0.0621266,"Missing"
2020.lrec-1.4,P15-1136,0,0.0666477,"Missing"
2020.lrec-1.4,D16-1245,0,0.049758,"Missing"
2020.lrec-1.4,D17-1169,0,0.0670658,"Missing"
2020.lrec-1.4,W18-2501,0,0.0119687,"to the nonpronominal antecedent of lowest index of this pronoun. If this antecedent consists of several tokens, the root node of the corresponding dependency subtree is used as the target of the link. More formally: for each anaphoric pronoun wk ∈ D−1 (w, T ) depending on token d(wk ) of lemma w = L(d(wk )) such that there exists a mention wk = mj ∈ M (pronominal mentions are one-place), we extend the set of dependents D−1 (w, T ) of w as follows: D˙ −1 (w, T ) We used Spanbert-Base of Joshi et al. (2019a) for coreference resolution. For the needed dependency features we used the AllenNLP’s (Gardner et al., 2018) implementation of Dozat and Manning (2016). For tokenization, lemmatization and POS tags, Spacy (Honnibal and Montani, 2017) was used. = D−1 (w, T ) ∪ 5. {r(tree(mi )) |∃mj ∈ M ∃wk ∈ D−1 (w, T ) : wk = mj ∧ mi  mj } 5.1. (8) 4.1. Evaluation Word Similarity The first analyses on the generated word vectors ran over various word similarity tasks. All results are listed in table 2. For evaluation, we used the benchmark tool of Jastrzebski et al. (2017)1 as it computes the accuracy for a lot of important Word Similarity and Analogy Tasks. We used: (MEN (Bruni et al., 2014), WS353 (Finkelstein et"
2020.lrec-1.4,L18-1550,0,0.0217384,"in the following example: Only recently, new systems have been introduced which are trained on large contexts using LSTMs (Peters et al., 2018) or large neural attention systems (Transformers) based on more complex transfer-learning tasks (Devlin et al., 2018; Liu et al., 2019) and are therefore not limited to local information – but at the price of additional computational complexity. At the same time, the list of proposals for new embedding methods that are pre-trained on ever larger corpora from more and more areas (genres, registers etc.) of more and more languages is constantly growing (Grave et al., 2018; Bojanowski et al., 2017; Radford et al., 2019). In recent years, the impact of various features such as POS-tags, subword information, semantic relations and in-domain data on word embeddings have been analyzed (Rezaeinia et al., 2017; Wendlandt et al., 2018; Bojanowski et al., 2017; Boleda et al., 2017; Gupta et al., 2017) and improved results have been obtained. In this paper we complement this research and ask about the effects of CR on word embeddings. This is done by example of six methods of computing word embeddings: Cbow (Mikolov et al., 2013), Skip (Mikolov et al., 2013), Glove (Pen"
2020.lrec-1.4,S17-1012,0,0.0570446,"Missing"
2020.lrec-1.4,J15-4004,0,0.110781,"Missing"
2020.lrec-1.4,D14-1162,0,0.084175,"Missing"
2020.lrec-1.4,N18-1202,0,0.0301996,"poetry/nmod:poss−1 his/nmod:poss, (poe/nmod:poss) 3.1. {i} (4) Extending the informational scope of window-based embeddings Our approach to extending window-based embeddings by means of CR is the following: For all pronominal mentions mi , for which first(mi ) is not pronominal, we replace: mi ← first(mi ) (5) This means that we replace each pronoun with its lowest index antecedent which in our case is represented by a corresponding lemma or multiword expression as shown in the following example: Only recently, new systems have been introduced which are trained on large contexts using LSTMs (Peters et al., 2018) or large neural attention systems (Transformers) based on more complex transfer-learning tasks (Devlin et al., 2018; Liu et al., 2019) and are therefore not limited to local information – but at the price of additional computational complexity. At the same time, the list of proposals for new embedding methods that are pre-trained on ever larger corpora from more and more areas (genres, registers etc.) of more and more languages is constantly growing (Grave et al., 2018; Bojanowski et al., 2017; Radford et al., 2019). In recent years, the impact of various features such as POS-tags, subword in"
2020.lrec-1.4,P09-5006,0,0.0629967,"Missing"
2020.lrec-1.4,N18-1190,0,0.021275,"2018; Liu et al., 2019) and are therefore not limited to local information – but at the price of additional computational complexity. At the same time, the list of proposals for new embedding methods that are pre-trained on ever larger corpora from more and more areas (genres, registers etc.) of more and more languages is constantly growing (Grave et al., 2018; Bojanowski et al., 2017; Radford et al., 2019). In recent years, the impact of various features such as POS-tags, subword information, semantic relations and in-domain data on word embeddings have been analyzed (Rezaeinia et al., 2017; Wendlandt et al., 2018; Bojanowski et al., 2017; Boleda et al., 2017; Gupta et al., 2017) and improved results have been obtained. In this paper we complement this research and ask about the effects of CR on word embeddings. This is done by example of six methods of computing word embeddings: Cbow (Mikolov et al., 2013), Skip (Mikolov et al., 2013), Glove (Pennington et al., 2014), Wang (Ling et al., 2015), Levy (Levy and Goldberg, 2014) and Komninos (Komninos and Manandhar, 2016). . . . his poetry. 7→ . . . Edgar Allan Poe poetry. So far, our replacement procedure only considers pronouns. The reason is that we exp"
2020.lrec-1.4,N16-1114,0,0.0409367,"Missing"
2020.lrec-1.650,W19-5512,0,0.0126694,"menter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence structure analysis, i.e. constituency and dependency parsers (Chen and Manning, 2014; Dyer et al., 2016a; Kiperwasser and Goldberg, 2016; Straka et al., 2016; Nguyen and Verspoor, 2018), operate on sentence units, the boundaries of which are recognized during preprocessing. However, current sentence boundary detectors (such as (Azzi et al., 2019; Gonz´alez-Gallardo and TorresMoreno, 2018)) operate sequentially; they detect sequences of sentence beginnings and ends without their dependencies. In contrast to this, natural language sentences are often more deeply structured: they can, for example, consist of a main clause and several subordinate clauses or other segments (e.g. inserts in parentheses). They can even embed entire sentences recursively and then contain several sentence beginnings and ends. This happens, for example, when embedding direct speech into indirect speech, where embedded sentences can even be interrupted and cont"
2020.lrec-1.650,D14-1082,0,0.013604,"by providing additional structural information. In this context, we experiment with German dependency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence structure analysis, i.e. constituency and dependency parsers (Chen and Manning, 2014; Dyer et al., 2016a; Kiperwasser and Goldberg, 2016; Straka et al., 2016; Nguyen and Verspoor, 2018), operate on sentence units, the boundaries of which are recognized during preprocessing. However, current sentence boundary detectors (such as (Azzi et al., 2019; Gonz´alez-Gallardo and TorresMoreno, 2018)) operate sequentially; they detect sequences of sentence beginnings and ends without their dependencies. In contrast to this, natural language sentences are often more deeply structured: they can, for example, consist of a main clause and several subordinate clauses or other segments (e.g. i"
2020.lrec-1.650,A00-2004,0,0.135407,"d a prospect on future work. 2. Related Work In the field of logical document structure analysis, there is already research in the form of Sentence Boundary Detection (SBD), where the task consists of recognizing the beginning and end of a sentence. For an approach to text classification that explores logical document structures, see (Mehler et al., 2007). Attempts were made to solve the task of SBD using different approaches and methods for different languages, including supervised and unsupervised methods. Among the su5282 pervised variants are maximum entropy (Reynar and Ratnaparkhi, 1997; Choi, 2000; Wang and Huang, 2003; Agarwal et al., 2005), rule-based (Wang and Huang, 2003), Markov Models (Jurish and W¨urzner, 2013), conditional random fields (Liu et al., 2005; Oba et al., 2006) and the ever increasingly popular neural networks (Xu et al., 2014; Treviso et al., 2017; Gonz´alez-Gallardo et al., 2018; Du et al., 2019; Schweter and Ahmed, 2019). Another class of methods for the SBD are the unsupervised methods (Kiss and Strunk, 2006; Strunk et al., 2006), which in contrast to the supervised variants do not require any additional training resources. The methods presented here have in com"
2020.lrec-1.650,W19-5513,0,0.0120365,"tructures, see (Mehler et al., 2007). Attempts were made to solve the task of SBD using different approaches and methods for different languages, including supervised and unsupervised methods. Among the su5282 pervised variants are maximum entropy (Reynar and Ratnaparkhi, 1997; Choi, 2000; Wang and Huang, 2003; Agarwal et al., 2005), rule-based (Wang and Huang, 2003), Markov Models (Jurish and W¨urzner, 2013), conditional random fields (Liu et al., 2005; Oba et al., 2006) and the ever increasingly popular neural networks (Xu et al., 2014; Treviso et al., 2017; Gonz´alez-Gallardo et al., 2018; Du et al., 2019; Schweter and Ahmed, 2019). Another class of methods for the SBD are the unsupervised methods (Kiss and Strunk, 2006; Strunk et al., 2006), which in contrast to the supervised variants do not require any additional training resources. The methods presented here have in common that they operate sequentially, whereby only the beginning and end of a sentence are determined and thus it is assumed that sentences have no hierarchy. It is neglected that natural language sentences can be deeply nested by dependent clauses. In this paper we approach this desideratum by developing a tagger that recogni"
2020.lrec-1.650,N16-1024,0,0.456225,"structural information. In this context, we experiment with German dependency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence structure analysis, i.e. constituency and dependency parsers (Chen and Manning, 2014; Dyer et al., 2016a; Kiperwasser and Goldberg, 2016; Straka et al., 2016; Nguyen and Verspoor, 2018), operate on sentence units, the boundaries of which are recognized during preprocessing. However, current sentence boundary detectors (such as (Azzi et al., 2019; Gonz´alez-Gallardo and TorresMoreno, 2018)) operate sequentially; they detect sequences of sentence beginnings and ends without their dependencies. In contrast to this, natural language sentences are often more deeply structured: they can, for example, consist of a main clause and several subordinate clauses or other segments (e.g. inserts in parenthes"
2020.lrec-1.650,C16-2013,1,0.882114,"Missing"
2020.lrec-1.650,K18-2013,0,0.0304229,"Missing"
2020.lrec-1.650,Q16-1023,0,0.0170972,"ion. In this context, we experiment with German dependency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence structure analysis, i.e. constituency and dependency parsers (Chen and Manning, 2014; Dyer et al., 2016a; Kiperwasser and Goldberg, 2016; Straka et al., 2016; Nguyen and Verspoor, 2018), operate on sentence units, the boundaries of which are recognized during preprocessing. However, current sentence boundary detectors (such as (Azzi et al., 2019; Gonz´alez-Gallardo and TorresMoreno, 2018)) operate sequentially; they detect sequences of sentence beginnings and ends without their dependencies. In contrast to this, natural language sentences are often more deeply structured: they can, for example, consist of a main clause and several subordinate clauses or other segments (e.g. inserts in parentheses). They can even embed entire s"
2020.lrec-1.650,J06-4003,0,0.0418328,"d methods for different languages, including supervised and unsupervised methods. Among the su5282 pervised variants are maximum entropy (Reynar and Ratnaparkhi, 1997; Choi, 2000; Wang and Huang, 2003; Agarwal et al., 2005), rule-based (Wang and Huang, 2003), Markov Models (Jurish and W¨urzner, 2013), conditional random fields (Liu et al., 2005; Oba et al., 2006) and the ever increasingly popular neural networks (Xu et al., 2014; Treviso et al., 2017; Gonz´alez-Gallardo et al., 2018; Du et al., 2019; Schweter and Ahmed, 2019). Another class of methods for the SBD are the unsupervised methods (Kiss and Strunk, 2006; Strunk et al., 2006), which in contrast to the supervised variants do not require any additional training resources. The methods presented here have in common that they operate sequentially, whereby only the beginning and end of a sentence are determined and thus it is assumed that sentences have no hierarchy. It is neglected that natural language sentences can be deeply nested by dependent clauses. In this paper we approach this desideratum by developing a tagger that recognizes recursively structured sentence structures. (Vogel and Fischer, 2019) retrain a Recurrent Neural Network Grammar"
2020.lrec-1.650,P03-1054,0,0.0263841,"s with the help of new training data without having to change their source code. This approach is taken up in this paper. Classical constituency parsers as given by Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016b) are trained with tree-like training data. Our newly created training corpus (see Section 3) also consists of tree-like structures. In order to obtain the desired segmenters for hierarchical sentence segment structures, it is thus essentially sufficient to retrain existing parsers. To this end, we experiment with three parsers: the (unlexicalized1 ) Stanford PCFG parser (Klein and Manning, 2003), the Stanford Shift Reduce parser (Zhu et al., 2013) and the RNNG parser (Dyer et al., 2016b) which was chosen as the most promising parser due to its F1-scores. In order to evaluate these parsers, we use the evaluation tool evalb (Sekine and Collins, 1997). Table 2 shows the corresponding results (training, validation and test sets were split according to the 60/20/20 rule): RNNG is obviously the best performing parser; but it is also most time-consuming in terms of training. The shift-reduce parser is the fastest in training and parsing, while RNNG is in the middle of the rating. From this"
2020.lrec-1.650,P05-1056,0,0.205447,"Missing"
2020.lrec-1.650,J93-2004,0,0.0712246,"Missing"
2020.lrec-1.650,K18-2008,0,0.0176783,"dency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence structure analysis, i.e. constituency and dependency parsers (Chen and Manning, 2014; Dyer et al., 2016a; Kiperwasser and Goldberg, 2016; Straka et al., 2016; Nguyen and Verspoor, 2018), operate on sentence units, the boundaries of which are recognized during preprocessing. However, current sentence boundary detectors (such as (Azzi et al., 2019; Gonz´alez-Gallardo and TorresMoreno, 2018)) operate sequentially; they detect sequences of sentence beginnings and ends without their dependencies. In contrast to this, natural language sentences are often more deeply structured: they can, for example, consist of a main clause and several subordinate clauses or other segments (e.g. inserts in parentheses). They can even embed entire sentences recursively and then contain several sen"
2020.lrec-1.650,J03-2003,0,0.439731,"n consist of a main sentence and several subordinate clauses as well as further segments (e.g. inserts in parentheses); they can even recursively embed whole sentences and then contain multiple sentence beginnings and ends. In this paper, we introduce a tool that segments sentences into tree structures to detect this type of recursive structure. To this end, we retrain different constituency parsers with the help of modified training data to transform them into sentence segmenters. With these segmenters, documents are mapped to sequences of sentence-related “logical document structures” (cf. (Power et al., 2003)). The resulting segmenters aim to improve downstream tasks by providing additional structural information. In this context, we experiment with German dependency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved. Keywords: sentence segmentation, logical document structure, constituency parsing, dependency parsing, CoNLL shared task 1. Introduction Tools for sentence str"
2020.lrec-1.650,A97-1004,0,0.642655,"ection 7 gives a conclusion and a prospect on future work. 2. Related Work In the field of logical document structure analysis, there is already research in the form of Sentence Boundary Detection (SBD), where the task consists of recognizing the beginning and end of a sentence. For an approach to text classification that explores logical document structures, see (Mehler et al., 2007). Attempts were made to solve the task of SBD using different approaches and methods for different languages, including supervised and unsupervised methods. Among the su5282 pervised variants are maximum entropy (Reynar and Ratnaparkhi, 1997; Choi, 2000; Wang and Huang, 2003; Agarwal et al., 2005), rule-based (Wang and Huang, 2003), Markov Models (Jurish and W¨urzner, 2013), conditional random fields (Liu et al., 2005; Oba et al., 2006) and the ever increasingly popular neural networks (Xu et al., 2014; Treviso et al., 2017; Gonz´alez-Gallardo et al., 2018; Du et al., 2019; Schweter and Ahmed, 2019). Another class of methods for the SBD are the unsupervised methods (Kiss and Strunk, 2006; Strunk et al., 2006), which in contrast to the supervised variants do not require any additional training resources. The methods presented here"
2020.lrec-1.650,L16-1680,0,0.059125,"Missing"
2020.lrec-1.650,E17-1030,0,0.0637897,"Missing"
2020.lrec-1.650,P13-1043,0,0.0175746,"e their source code. This approach is taken up in this paper. Classical constituency parsers as given by Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016b) are trained with tree-like training data. Our newly created training corpus (see Section 3) also consists of tree-like structures. In order to obtain the desired segmenters for hierarchical sentence segment structures, it is thus essentially sufficient to retrain existing parsers. To this end, we experiment with three parsers: the (unlexicalized1 ) Stanford PCFG parser (Klein and Manning, 2003), the Stanford Shift Reduce parser (Zhu et al., 2013) and the RNNG parser (Dyer et al., 2016b) which was chosen as the most promising parser due to its F1-scores. In order to evaluate these parsers, we use the evaluation tool evalb (Sekine and Collins, 1997). Table 2 shows the corresponding results (training, validation and test sets were split according to the 60/20/20 rule): RNNG is obviously the best performing parser; but it is also most time-consuming in terms of training. The shift-reduce parser is the fastest in training and parsing, while RNNG is in the middle of the rating. From this point of view, RNNG is a top candidate for our task."
2020.lt4hala-1.21,C18-1139,0,0.291179,"d as follows: Section 2 describes the data sets we used to train our word embeddings. Section 3 describes the training process of the taggers and how they were integrated into our system. In Section 4, we present and discuss our results, while Section 5 provides a summary of this study and prospects for future work. 2. Datasets This section gives a brief overview about the datasets supplied for EvaLatin as well as other corpora we used for the closed modality run of the POS task. Current state-of-the-art sequence labeling systems for POS tagging make use of word embeddings or language models (Akbik et al., 2018; Bohnet et al., 2018; Gleim et al., 2019, LMs). These tools are usually trained and evaluated on high-resource languages; making use of the availability of large unlabeled corpora to build feature-rich word em130 beddings. This leads to an ever-increasing ubiquitousness of embeddings for all kinds of languages. Unfortunately, the number of available, high-quality corpora for Latin is stretched thin; historically the Latin Wikipedia has often been used as a corpus for training word embeddings (Grave et al., 2018; Heinzerling and Strube, 2018). But the Latin Wikipedia is composed of modern text"
2020.lt4hala-1.21,N19-4010,0,0.267555,"to the length of the output sequence. There already exist approaches for POS tagging for Latin (Gleim et al., 2019; vor der Br¨uck and Mehler, 2016; Eger et al., 2016; Eger et al., 2015; Straka and Strakov´a, 2017; Kestemont and De Gussem, 2016; Kondratyuk and Straka, 2019; Manjavacas et al., 2019). These approaches mostly utilize the increasingly popular neural network based methods for POS-tagging – by example of Latin. Part of this contribution is to extend this work and to train state-of-theart neural network based sequence labeling tools (Straka and Strakov´a, 2017; Lample et al., 2016; Akbik et al., 2019a; Kondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpora and are more useful for neural network sequence labeling tools if the corpora are not only large but also from the same domain as the documents to be processed. Therefore another part of this contribution is to create word embeddings for Latin for different genres and epochs. Since Latin is a morphologically rich language, sub-word-embeddin"
2020.lt4hala-1.21,N19-1078,0,0.133362,"to the length of the output sequence. There already exist approaches for POS tagging for Latin (Gleim et al., 2019; vor der Br¨uck and Mehler, 2016; Eger et al., 2016; Eger et al., 2015; Straka and Strakov´a, 2017; Kestemont and De Gussem, 2016; Kondratyuk and Straka, 2019; Manjavacas et al., 2019). These approaches mostly utilize the increasingly popular neural network based methods for POS-tagging – by example of Latin. Part of this contribution is to extend this work and to train state-of-theart neural network based sequence labeling tools (Straka and Strakov´a, 2017; Lample et al., 2016; Akbik et al., 2019a; Kondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpora and are more useful for neural network sequence labeling tools if the corpora are not only large but also from the same domain as the documents to be processed. Therefore another part of this contribution is to create word embeddings for Latin for different genres and epochs. Since Latin is a morphologically rich language, sub-word-embeddin"
2020.lt4hala-1.21,P18-1246,0,0.0839404,"Missing"
2020.lt4hala-1.21,brants-2000-inter,0,0.059593,"Missing"
2020.lt4hala-1.21,W18-6004,0,0.0163552,"s we compiled a corpus of historical, Medieval Latin texts covering different epochs which is presented in the following section. 2.1. Historical Corpora An overview of the corpora used is shown in table 1. It lists each corpus together with its numbers of sentences, tokens and characters and provides a summary of the overall corpus with the total number and unique counts. In addition to the corpus published for EvaLatin, we added other publicly accessible corpora: the Universal Dependencies Latin (Nivre et al., 2016a, UD Latin) corpora UD Latin-PROIEL (Haug and Jøhndal, 2008), UD Latin-ITTB (Cecchini et al., 2018) and UD Latin-Perseus (Bamman and Crane, 2011a), the Capitularies (Mehler et al., 2015) and the Cassiodorus Variae (Variae, 2020). But the main bulk of text comes from the Latin text repository of the eHumanties Desktop (Gleim et al., 2009; Gleim et al., 2012) and the CompHistSem (Cimino et al., 2015) project comprising a large number of Medieval Latin texts.1 For all corpora we extracted the plain text without annotations and compiled a single corpus called Historical Latin Corpus (HLC). Corpus Sentences Tokens Chars UD-Perseus Cassiodor. Variae EvaLatin Capitularies UD-PROIEL UD-ITTB CompHis"
2020.lt4hala-1.21,W15-3716,1,0.887068,"Missing"
2020.lt4hala-1.21,L16-1239,1,0.88574,"ch as information retrieval, knowledge extraction or semantic analysis, POS tagging is a crucial pre-processing step. However, in morphologically rich languages such as Latin, this task is not trivial due to the variability of lexical forms. In order to perform POS tagging automatically, it has to be understood as a sequence labeling problem, where an output class is assigned to each input word so that the length of the input sequence corresponds to the length of the output sequence. There already exist approaches for POS tagging for Latin (Gleim et al., 2019; vor der Br¨uck and Mehler, 2016; Eger et al., 2016; Eger et al., 2015; Straka and Strakov´a, 2017; Kestemont and De Gussem, 2016; Kondratyuk and Straka, 2019; Manjavacas et al., 2019). These approaches mostly utilize the increasingly popular neural network based methods for POS-tagging – by example of Latin. Part of this contribution is to extend this work and to train state-of-theart neural network based sequence labeling tools (Straka and Strakov´a, 2017; Lample et al., 2016; Akbik et al., 2019a; Kondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikol"
2020.lt4hala-1.21,E09-2006,1,0.361377,"ts numbers of sentences, tokens and characters and provides a summary of the overall corpus with the total number and unique counts. In addition to the corpus published for EvaLatin, we added other publicly accessible corpora: the Universal Dependencies Latin (Nivre et al., 2016a, UD Latin) corpora UD Latin-PROIEL (Haug and Jøhndal, 2008), UD Latin-ITTB (Cecchini et al., 2018) and UD Latin-Perseus (Bamman and Crane, 2011a), the Capitularies (Mehler et al., 2015) and the Cassiodorus Variae (Variae, 2020). But the main bulk of text comes from the Latin text repository of the eHumanties Desktop (Gleim et al., 2009; Gleim et al., 2012) and the CompHistSem (Cimino et al., 2015) project comprising a large number of Medieval Latin texts.1 For all corpora we extracted the plain text without annotations and compiled a single corpus called Historical Latin Corpus (HLC). Corpus Sentences Tokens Chars UD-Perseus Cassiodor. Variae EvaLatin Capitularies UD-PROIEL UD-ITTB CompHistSem 2 260 3 129 14 009 15 170 18 526 19 462 2 608 730 29 078 135 352 258 861 477 247 215 175 349 235 79 136 129 1 444 884 748 477 1 528 538 2 432 482 1 157 372 1 771 905 384 199 772 Total Unique 2 665 840 80 129 332 971 839 389 576 106 43"
2020.lt4hala-1.21,L18-1550,0,0.274126,"ondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpora and are more useful for neural network sequence labeling tools if the corpora are not only large but also from the same domain as the documents to be processed. Therefore another part of this contribution is to create word embeddings for Latin for different genres and epochs. Since Latin is a morphologically rich language, sub-word-embeddings (Grave et al., 2018; Heinzerling and Strube, 2018) must be created to reflect its morphological peculiarities. The various sequence labeling tools provide different results, making it advisable to combine them in order to bundle their strengths. For this reason LSTMVoter (Hemati and Mehler, 2019) was used to create a conglomerate of the various tools and models (re-)trained here. To simplify the above mentioned process of training embeddings and sequence labeling tools on the one hand and creating an ensemble thereof, we developed a generic pipeline architecture which takes a labeled corpus in ConLLU format as i"
2020.lt4hala-1.21,L18-1473,0,0.508946,", 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpora and are more useful for neural network sequence labeling tools if the corpora are not only large but also from the same domain as the documents to be processed. Therefore another part of this contribution is to create word embeddings for Latin for different genres and epochs. Since Latin is a morphologically rich language, sub-word-embeddings (Grave et al., 2018; Heinzerling and Strube, 2018) must be created to reflect its morphological peculiarities. The various sequence labeling tools provide different results, making it advisable to combine them in order to bundle their strengths. For this reason LSTMVoter (Hemati and Mehler, 2019) was used to create a conglomerate of the various tools and models (re-)trained here. To simplify the above mentioned process of training embeddings and sequence labeling tools on the one hand and creating an ensemble thereof, we developed a generic pipeline architecture which takes a labeled corpus in ConLLU format as input, trains the different tagg"
2020.lt4hala-1.21,C16-2013,1,0.828975,"Missing"
2020.lt4hala-1.21,D19-1279,0,0.0373487,"pre-processing step. However, in morphologically rich languages such as Latin, this task is not trivial due to the variability of lexical forms. In order to perform POS tagging automatically, it has to be understood as a sequence labeling problem, where an output class is assigned to each input word so that the length of the input sequence corresponds to the length of the output sequence. There already exist approaches for POS tagging for Latin (Gleim et al., 2019; vor der Br¨uck and Mehler, 2016; Eger et al., 2016; Eger et al., 2015; Straka and Strakov´a, 2017; Kestemont and De Gussem, 2016; Kondratyuk and Straka, 2019; Manjavacas et al., 2019). These approaches mostly utilize the increasingly popular neural network based methods for POS-tagging – by example of Latin. Part of this contribution is to extend this work and to train state-of-theart neural network based sequence labeling tools (Straka and Strakov´a, 2017; Lample et al., 2016; Akbik et al., 2019a; Kondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpor"
2020.lt4hala-1.21,N16-1030,0,0.0767176,"sequence corresponds to the length of the output sequence. There already exist approaches for POS tagging for Latin (Gleim et al., 2019; vor der Br¨uck and Mehler, 2016; Eger et al., 2016; Eger et al., 2015; Straka and Strakov´a, 2017; Kestemont and De Gussem, 2016; Kondratyuk and Straka, 2019; Manjavacas et al., 2019). These approaches mostly utilize the increasingly popular neural network based methods for POS-tagging – by example of Latin. Part of this contribution is to extend this work and to train state-of-theart neural network based sequence labeling tools (Straka and Strakov´a, 2017; Lample et al., 2016; Akbik et al., 2019a; Kondratyuk and Straka, 2019) for Latin. These neural network based sequence labeling tools usually require pre-trained word embeddings (e.g. Mikolov et al. (2013a) or Pennington et al. (2014)). These word embeddings are trained on large unlabeled corpora and are more useful for neural network sequence labeling tools if the corpora are not only large but also from the same domain as the documents to be processed. Therefore another part of this contribution is to create word embeddings for Latin for different genres and epochs. Since Latin is a morphologically rich languag"
2020.lt4hala-1.21,N15-1142,0,0.0877689,"Missing"
2020.lt4hala-1.21,N19-1153,0,0.160856,"Missing"
2020.lt4hala-1.21,D13-1032,0,0.2534,"Missing"
2020.lt4hala-1.21,L16-1262,0,0.0888398,"Missing"
2020.lt4hala-1.21,D14-1162,0,0.0820589,"Missing"
2020.lt4hala-1.21,N18-1202,0,0.013813,") are composed of sub-word token embeddings. They utilize a vocabulary of character sequences which are induced from a large text corpus using a variant of bytepair encoding for textual data (Sennrich et al., 2016). We used the SentencePiece’s3 implementation of the byte-pair algorithm to encode the HLC (see Section 4). 3.1.3. FLAIR Language Model Current methods for sequence labeling use language models (LMs) trained on large unlabeled corpora to obtain contextualized embeddings, achieving state-of-the-art performance in POS tagging and named entity recognition for English, German and Dutch (Peters et al., 2018; Akbik et al., 2018). Some recent sequence labeling models with strong performance leverage FLAIR character language models (Akbik et al., 2018; Akbik et al., 2019b). These models are available through the FLAIR framework (Akbik et al., 2019a) which, since its first release, has been expanded with character language models for various languages by the NLP community, but none for Latin. Thus, we trained our own Latin character language model on the HLC of Section 2.1. 3.2. Taggers In the following sections we briefly describe the taggers we have selected for our evaluation. 3.2.1. MarMoT MarMo"
2020.lt4hala-1.21,E14-1078,0,0.0719312,"Missing"
2020.lt4hala-1.21,P16-1162,0,0.00565379,"e corpus rather than considering local samples of cooccurrences. 3.1.2. Sub-word Embeddings fastText embeddings (Grave et al., 2018) are trained on character n-grams of words rather than words themselves. They are able to capture character-based information which may be related to morphological information in addition to distributional information. Byte-Pair Embeddings (Heinzerling and Strube, 2018, BPEmb) are composed of sub-word token embeddings. They utilize a vocabulary of character sequences which are induced from a large text corpus using a variant of bytepair encoding for textual data (Sennrich et al., 2016). We used the SentencePiece’s3 implementation of the byte-pair algorithm to encode the HLC (see Section 4). 3.1.3. FLAIR Language Model Current methods for sequence labeling use language models (LMs) trained on large unlabeled corpora to obtain contextualized embeddings, achieving state-of-the-art performance in POS tagging and named entity recognition for English, German and Dutch (Peters et al., 2018; Akbik et al., 2018). Some recent sequence labeling models with strong performance leverage FLAIR character language models (Akbik et al., 2018; Akbik et al., 2019b). These models are available"
2020.lt4hala-1.21,2020.lt4hala-1.16,0,0.270529,"Missing"
2020.lt4hala-1.21,K17-3009,0,0.0802019,"Missing"
2020.lt4hala-1.21,P14-5003,0,0.0258567,"Missing"
2020.lt4hala-1.21,L16-1240,1,0.873171,"Missing"
C02-1063,J97-1003,0,0.0568921,"in definition (1): 3 Text Linkage Departing from ordinary list as well as cluster structures, we model the connotation of a text as a hierarchy, where each node represents a single connoted text (and not a set of texts as in case of agglomerative cluster analysis). In order to narrow down a solution for this task we need a linguistic criterion, which bridges between the linguistic knowledge represented in semantic spaces and the task of connotative text linkage. For this purpose we refer to the concept of lexical cohesion introduced by (Halliday and Hasan, 1976); see (Morris and Hirst, 1991; Hearst, 1997; Marcu, 2000) who already use this concept for text segmentation. According to this approach, lexical cohesion results from reiterating words, which are semantically related on the basis of (un-)systematic relations (e.g. synonymy or hyponymy). Unsystematic lexical cohesion results from patterns of contextual, paradigmatic similarity: “[. . . ] lexical items having similar patterns of collocation—that is, tending to appear in similar contexts—will generate a cohesive force if they occur in adjacent sentences.” (Halliday and Hasan, 1976, p. 286). Several factors influencing this cohesive force"
C02-1063,P98-2127,0,0.0107138,"t only of directly linked nodes, but of whole paths, thereby reflecting their syntagmatic order. Looking for a mathematical model of this optimality criterion, minimal spanning trees (MST) drop out, since they only optimize direct node-to-node similarities disregarding any path context. Furthermore, whereas we expect to yield different trees modeling the connotations of different texts, MSTs ignore this aspect dependency since they focus on a unique spanning tree of the underlying feature space. Another candidate is given by dependency trees (Rieger, 1984) which are equal to similarity trees (Lin, 1998): for a given root x, the nodes are inserted into its similarity tree (ST) in descending order of their similarity to x, where the predecessor of any node z is chosen to be the node y already inserted, to which z is most similar. Although STs already capture the aspect dependency induced by their varying roots, the path criterion is still not met. Thus, we generalize the concept of a ST to that of a cohesion tree as follows: First, we observe that the construction of STs uses two types of order relations: the first, let it call ≤1x , determines the order of the nodes inserted dependent on root"
C02-1063,J91-1002,0,0.0875014,"d connotation as defined in definition (1): 3 Text Linkage Departing from ordinary list as well as cluster structures, we model the connotation of a text as a hierarchy, where each node represents a single connoted text (and not a set of texts as in case of agglomerative cluster analysis). In order to narrow down a solution for this task we need a linguistic criterion, which bridges between the linguistic knowledge represented in semantic spaces and the task of connotative text linkage. For this purpose we refer to the concept of lexical cohesion introduced by (Halliday and Hasan, 1976); see (Morris and Hirst, 1991; Hearst, 1997; Marcu, 2000) who already use this concept for text segmentation. According to this approach, lexical cohesion results from reiterating words, which are semantically related on the basis of (un-)systematic relations (e.g. synonymy or hyponymy). Unsystematic lexical cohesion results from patterns of contextual, paradigmatic similarity: “[. . . ] lexical items having similar patterns of collocation—that is, tending to appear in similar contexts—will generate a cohesive force if they occur in adjacent sentences.” (Halliday and Hasan, 1976, p. 286). Several factors influencing this"
C02-1063,J98-1004,0,0.0322242,"peronym-based, their effects seem to be small (Scott and Matwin, 1999). More seriously (Riloff, 1995) argues that the bag of words model ignores morphological and syntactical information which she found to be essential for solving some categorization tasks. An alternative to the vector space model are semantic spaces, which have been proposed as a highdimensional format for representing relations of semantic proximity. Relying on sparse knowledge resources, they prove to be efficient in cognitive science (Kintsch, 1998; Landauer and Dumais, 1997), computational linguistics (Rieger, 1984; Sch¨ utze, 1998), and information retrieval. Although semantic spaces prove to be an alternative to the vector space model, they leave the question unanswered of how to explore and visualize similarities of signs mapped onto them. In case that texts are represented as points in semantic space, this question refers to the exploration of their implicit, content based relations. Several methods for solving this task have been proposed which range from simple lists via minimal spanning trees to cluster analysis as part of scatter/gahter algorithms (Hearst and Pedersen, 1996). Representing a sign’s environment in"
C02-1063,E99-1019,0,0.0197282,"similar lexical organizations differ texturally. If for example a short commentary connotes two equally similar texts, another commentary and a long report, the commentary should be preferred. Thus, in M2 the textual connotation of a text is not only seen to be structured on the basis of the criterion of similarity of lexical organization, but also by means of genre specific features modeled as quantitative text characteristics. This approach follows (Herdan, 1966), who programmatically asked, whether difference in style correlates with difference in frequency of use of linguistic forms. See (Wolters and Kirsten, 1999) who, following this approach, already used POS frequency as a source for genre classification, a task which goes beyond the scope of the given paper. On this background a compound text similarity measure can be derived as a linear model: σ(x, y) = 3 X ωi σi (x, y) ∈ [0, 1] (4) i=1 a. where σ1 (x, y) = σ(~x, ~y ) models lexical semantics of texts x, y according to M1; b. σ2 uses the Levenshtein metric for measuring the similarity of the text structure stings assigned to x and y; c. and σ3 measures, based on an Euclidean metric, the similarity of texts with respect to the quantitative features"
C02-1063,C98-2122,0,\N,Missing
C02-1063,P84-1062,0,\N,Missing
C16-1331,W13-3520,0,0.0245253,"raints, this model jointly optimizes monolingual and cross-lingual objectives similarly as in Klementiev et al. (2012): X X L` (w, h; θ` ) + λΩ(θe , θf ) L= `∈{e,f } w,h∈D` is minimized, where w and h are target words and their contexts, respectively, and θe , θf are embedding parameters for two languages. The terms L` encode the monolingual constraints and the term Ω(θe , θf ) encodes the cross-lingual constraints, enforcing similar words across languages (obtained from sentence aligned data) to have similar embeddings. 3 Data For our experiments, we use the Wikipedia extracts available from Al-Rfou et al. (2013)2 as monolingual data and Europarl (Koehn, 2005) as bilingual database. We consider two settings, one in which we take all 21 (All21) languages available in Europarl and one in which we focus on the 10 (Big10) largest languages. These languages are bg, cs, da, de , el, en, es, et, fi, fr, hu, it, lt, lv, nl, pl, pt, ro, sk, sl, sv (Big10 languages highlighted). To induce a comparable setting, we extract in the All21 setup: 195,842 parallel sentences from Europarl and roughly 835K (randomly extracted) sentences from Wikipedia for each of the 21 languages. In the Big10 setup, we extract 1,098,89"
C16-1331,W16-1208,0,0.22335,"wo languages in a joint semantic space. Thus, it may be less sensitive to varying polysemous associations across different languages (cf. our vir example in Section 1), and hence less adequate for capturing cross-lingual polysemy.8 In terms of language similarity, we mention that our approach is formally similar to approaches as in 8 Thus, we would also expect CCA to perform better in monolingual intrinsic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embedding p and `, which are a"
C16-1331,P14-2134,0,0.0318527,"6 Related work Besides the mono- and multilingual word vector representation research that forms the basis of our work and which has already been referred to, we mention the following three related approaches to language classification. Koehn (2005) compares down-stream task performance in SMT to language family relationship, finding positive correlation. Cooper (2008) measures semantic language distance via bilingual dictionaries, finding that French appears to be semantically closer to Basque than to German, supporting our arguments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest t"
C16-1331,P05-1074,0,0.0776918,"sic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embedding p and `, which are arguably best suited for this task. Our approach also differs from Eger et al. (2015) in that we do not apply a random-surfer process to our semantic graphs. We finally note that the linguistic problem of (semantic) language classification, as we consider, involves some vagueness as there is de facto no gold standard that we can compare to. Reasonably, however, languages should be semantically similar to a degre"
C16-1331,P14-1023,0,0.0126228,"tions between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 201"
C16-1331,P10-4002,0,0.011222,"similarity as measured by cosine distances in semantic spaces as in Figure 1 bottom right. Bilingual embedding models: We consider two approaches to constructing bilingual word embeddings. The first is the canonical correlation analysis (CCA) approach suggested in Faruqui and Dyer 3509 (2014). This takes independently constructed word vectors from two different languages and projects them onto a common vector space such that (one-best) translation pairs, as determined by automatic word alignments, are maximally linearly correlated. CCA relies on word level alignments and we use cdec for this (Dyer et al., 2010). The second approach we employ is called BilBOWA (BBA) (Gouws et al., 2015). Rather than separately training word vectors for two languages and subsequently enforcing cross-lingual constraints, this model jointly optimizes monolingual and cross-lingual objectives similarly as in Klementiev et al. (2012): X X L` (w, h; θ` ) + λΩ(θe , θf ) L= `∈{e,f } w,h∈D` is minimized, where w and h are target words and their contexts, respectively, and θe , θf are embedding parameters for two languages. The terms L` encode the monolingual constraints and the term Ω(θe , θf ) encodes the cross-lingual constr"
C16-1331,P16-2009,1,0.853758,"an, supporting our arguments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a lon"
C16-1331,S15-1014,1,0.843484,"ts for projecting two languages in a joint semantic space. Thus, it may be less sensitive to varying polysemous associations across different languages (cf. our vir example in Section 1), and hence less adequate for capturing cross-lingual polysemy.8 In terms of language similarity, we mention that our approach is formally similar to approaches as in 8 Thus, we would also expect CCA to perform better in monolingual intrinsic evaluations (as our experiments have partly confirmed) and BBA to perform better in multilingual intrinsic evaluations. We thank one reviewer for pointing this out. 3513 (Eger et al., 2015; Asgari and Mofrad, 2016) and others. Namely, we construct graphs, one for each language, and compare them to determine language distance. Compared to Eger et al. (2015), our approach differs in that they use translations in a second language ` to measure similarity between pivot language p words. This idea also underlies very well-known lexical semantic resources such as the paraphrase database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013); see also Eger and Sejane (2010). In contrast, we directly use bilingual embeddings for this similarity measurement by jointly embe"
C16-1331,E14-1049,0,0.459107,"duction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring se"
C16-1331,N15-1184,0,0.0214248,"s support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulti"
C16-1331,N13-1092,0,0.0734899,"Missing"
C16-1331,N15-1157,0,0.16569,"Missing"
C16-1331,P16-1141,0,0.0259237,"ments on contact as co-determining semantic language similarity. Bamman et al. (2014) and Kulkarni et al. (2015b) study semantic distance between dialects of English by comparing region specific word embeddings. Studying geographic variation of (different) languages is also closely related to studying temporal variation within one and the same language (Kulkarni et al., 2015a), with one crucial difference being the need to find a common representation in the former case. Word embeddings — in particular, monolingual ones — can also be used to address the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a long-standing tradition tha"
C16-1331,P14-1006,0,0.0187252,"derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring semantic similarity in one of"
C16-1331,J15-4004,0,0.0218517,"ord alignments obtained on the respective Europarl data pair. For BBA, we use the monolingual Wikipedias of ` and `0 for the monolinugal constraints, and the Europarl sentence alignments of ` and `0 for the bilingual constraints. We only consider words that occur at least 100 times in the respective data sets. 4.1 Monolingual semantic task (Q1) We first evaluate the obtained BBA and CCA embedding vectors on monolingual p = English evaluation tasks, for varying second language `. The tasks we consider are WS353 (Finkelstein et al., 2002), MTurk287 (Radinsky et al., 2011), MTurk771,4 SimLex999 (Hill et al., 2015), and MEN (Bruni et al., 2014), which are standard semantic similarity datasets for English, documented in an array of previous research. In addition, we include the SimLex999-de and SimLex999-it (Leviant and Reichart, 2015) for p = German and p = Italian, respectively. In each task, the goal is to determine the semantic similarity between two language p words, such as dog and cat (when p = English). For the tasks, we indicate average Spearman correlation coefficients δ = δp,` between 2 https://sites.google.com/site/rmyeid/projects/polyglot. All other parameters set to default values. 4 http:/"
C16-1331,P12-1092,0,0.0132942,"ic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown t"
C16-1331,C12-1089,0,0.238393,"eography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when usi"
C16-1331,2005.mtsummit-papers.11,0,0.124689,"This work is structured as follows. Section 2 introduces our approach of constructing graphs from bilingual word embeddings and its relation to the two questions outlined. Section 3 describes our data, 1 Our initial expectation was that bilingual word embeddings lead to better results in monolingual settings, at least for some second languages. However, this was not confirmed in any of our experiments. This may be related to our (small) data set sizes (see Section 3) or to other factors, but has no concern for the question (Q1) we are investigating. 3508 which is based on the Europarl corpus (Koehn, 2005). Section 4 details our experiments, which we discuss in Section 5. We relate to previous work in Section 6 and conclude in Section 7. 2 Model In this section, we formally outline our approach. Given N + 1 languages, choose one of them, p, as pivot language. Construct N weighted networks (p) (p) G` = (V (p) , E (p) , w` ) as follows: nodes V (p) are the words of language p, graphs are fully connected, (p) i.e., E (p) = V (p) × V (p) , and edge weights are w` (u, v) = sim(up,` , vp,` ). The similarity function sim is, e.g., cosine similarity, and up,` , vp,` ∈ Rd are bilingual word embeddings o"
C16-1331,N15-1028,0,0.0421879,"chitectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better performance even in a monolingual setting, e.g., when using them for measuring semantic similarity in one of the two languages involved (Faruqui an"
C16-1331,D14-1113,0,0.0212587,"ty is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate semantic similarity of words from different languages; see Figure 1 for an illustration. Moreover, the resulting word vectors have been shown to produce on-par or better"
C16-1331,D14-1162,0,0.0823932,"age classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact. 1 Introduction Word embeddings derived from context-predicting neural network architectures have become the stateof-the-art in distributional semantics modeling (Baroni et al., 2014). Given the success of these models and the ensuing hype, several extensions over the standard paradigm (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) have been suggested, such as retrofitting word vectors to semantic knowledge-bases (Faruqui et al., 2015), multi-sense (Huang et al., 2012; Neelakantan et al., 2014), and multi-lingual word vectors (Klementiev et al., 2012; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Lu et al., 2015; Gouws et al., 2015; Gouws ˇ and Søgaard, 2015; Huang et al., 2015; Suster et al., 2016). The models underlying the latter paradigm, which we focus on in the current work, project word vectors of two (or multiple) languages into a joint semantic space, thereby allowing to evaluate sema"
C16-1331,P14-1131,0,0.0666447,"Missing"
C16-1331,N16-1160,0,0.0241553,"Missing"
C16-1331,N16-1156,0,0.0328774,"the latter scenario (Eger and Mehler, 2016; Hamilton et al., 2016). In terms of classifying languages, the work that is closest to ours is that of Asgari and Mofrad (2016). A key difference between their approach and ours is that, in order to achieve a common representation between languages, they translate words. This has the disadvantage that translation pairs need to be known, which typically requires large amounts of parallel text. In contrast, bilingual word embeddings, which 3514 form the basis of our experiments, can be generated from as few as ten translation pairs, as demonstrated in Zhang et al. (2016). There is by now a long-standing tradition that compares languages via analysis of complex networks that encode their words and the (semantic) relationships between them (Cancho and Sol´e, 2001; Gao et al., 2014). These studies often only look at very abstract statistics of networks such as average path lengths and clustering coefficients, rather than analyzing them on a level of content of their nodes and edges. In addition, they often substitute co-occurrence as a proxy for semantic similarity. However, as Asgari and Mofrad (2016) point out, co-occurrence is a naive estimate of similarity;"
C16-2013,W14-5201,0,0.0791715,"Missing"
C16-2013,P10-4005,0,0.0216089,"NLP (Burghardt et al., 2014) and conTEXT (Burghardt et al., 2014) are web-based NLP tools including visualization components. In order to combine the best of both worlds, TextImager additionally subsumes the functionalities This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 59 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 59–63, Osaka, Japan, December 11-17 2016. of these tools. It also shares functionalities with WebLicht (Hinrichs et al., 2010). However, unlike WebLicht, TextImager is based on open UIMA and, thus, complies to an industrial standard of modeling text processing chains. 3 System Architecture of TextImager TextImager consists of two parts, front-end and back-end. The front-end is a web application that makes all functionalities and NLP processes available in a user-friendly way. It allows users for analyzing and visualizing unstructured texts and text corpora. The back-end is a highly modular, expandable, scalable and flexible architecture with parallel processing capabilities. 3.1 Back-end Figure 1 shows the architectu"
C16-2013,W16-3212,1,0.82163,"gramming skills. This is especially needed for scholars in digital humanities who are not trained in using script languages for modeling statistical procedures, but expect a versatile tool encapsulating this computational complexity. Thus, TextImager users can process input texts using R packages like LDA (see Figure 3b), network analysis or stylometrics (see Figure 3a) without the need to manipulate or to invoke any R script directly. All these R packages 3 https://www.d3js.org http://visjs.org 5 https://www.r-project.org 4 61 are given a single entrance point in the form of TextImager. See (Mehler et al., 2016) for a recent research study based on TextImager. 4 Future Work In already ongoing work, we extend the functionality of TextImager. This includes covering all features of tools like conTEXT. In contrast to many current frameworks, we will make TextImager’s source code open-source as soon as the framework reaches a stable and documented version. We are going to specify a comprehensive model for component specification. The model will contain specifications of general components and their dependency hierarchy. This model will help defining where new NLP components are settled within the NLP land"
C18-2031,S12-1059,0,0.0541246,"Missing"
C18-2031,C16-2013,1,0.826693,"ships between articles assigned to the DDC and articles for which these assignments do not exist. In this paper, we focus on Arabic, English, French, German, Spanish, and Turkish while performing a deeper analysis by example of the German corpus (#articles 15 136, #tokens per article 1 228, #classes 2nd level 98 and #classes 3rd level 641). Additionally we select more Wikipedias from the List of Wikipedias3 , where depth &gt;= 50 and #articles &gt;= 10 000, to be available through our LTV API. 2 Classification Model The architecture of the LTV framework consists of four steps: 1. We use TextImager (Hemati et al., 2016) for preprocessing (lemmatization, part of speech tagging) the German Wikipedia and perform Word Sense Disambiguation (WSD) by means of fastSense (Uslu et al., 2018a), a WSD tool that is trained on the entire German Wikipedia. Our approach is in line with (Pilehvar and Navigli, 2015) and, thus, disambiguates input words to obtain sense representations as input for calculating sense embeddings. 2. The disambiguated Wikipedia corpus is then used to create sense embeddings by means of word2vec (Mikolov et al., 2013) using all sentences as input. 3. The aim is to obtain disambiguated articles and"
C18-2031,L18-1168,1,0.842478,"and Turkish while performing a deeper analysis by example of the German corpus (#articles 15 136, #tokens per article 1 228, #classes 2nd level 98 and #classes 3rd level 641). Additionally we select more Wikipedias from the List of Wikipedias3 , where depth &gt;= 50 and #articles &gt;= 10 000, to be available through our LTV API. 2 Classification Model The architecture of the LTV framework consists of four steps: 1. We use TextImager (Hemati et al., 2016) for preprocessing (lemmatization, part of speech tagging) the German Wikipedia and perform Word Sense Disambiguation (WSD) by means of fastSense (Uslu et al., 2018a), a WSD tool that is trained on the entire German Wikipedia. Our approach is in line with (Pilehvar and Navigli, 2015) and, thus, disambiguates input words to obtain sense representations as input for calculating sense embeddings. 2. The disambiguated Wikipedia corpus is then used to create sense embeddings by means of word2vec (Mikolov et al., 2013) using all sentences as input. 3. The aim is to obtain disambiguated articles and sense embeddings for training a DDC classifier and thus generating nnDDC. For this we enrich the disambiguated Wikipedia articles with DDC information using Wikidat"
D19-5702,L18-1473,0,0.651931,"equence tagging tasks (eg. Akbik et al., 2018, 2019b). We follow the approach of Akbik et al., using FLAIR to stack (i.e. concatenate) character and word embeddings to improve recognition rates. We further expand this model by adding sub-word embeddings to the stacked embeddings. These stacked embeddings serves as input for a BiLSTM-CRF sequence tagger (Huang et al., 2015). For our best performing model, we used two different token-level embeddings, a WANG 2 VECbased embedding (Ling et al., 2015) and a FASTT EXT-based embedding (Bojanowski et al., 2017), a single byte-pair sub-word embedding (Heinzerling and Strube, 2018) and one context sensitive character-level language model (Akbik et al., 2019b). Figure 1 gives a visual depiction of our best performing model. The following paragraphs describe the used embeddings in more detail. Spanish Health Corpus In this section, we describe the Spanish Health Corpus, a collection of 7353 diverse Spanish health and science journal articles and papers. The corpus was obtained from SciELO4 by means of an automated crawler.5 The content of the articles in this corpus was downloaded as embedded text from the respective websites and stripped of any structural elements, like"
D19-5702,N19-4010,0,0.0202277,"Missing"
D19-5702,N19-1078,0,0.0230508,"Missing"
D19-5702,C18-1139,0,0.0553752,"en access Spanish medical publications and (according to the creators) shows properties of both biomedical and medical literature as well as clinical records. The SPACCC corpus is given in brat standoff format3 as two separate files per document, one containing the plain text, the other containing the annotations with character level offsets on the raw text. We converted the corpus into a CoNLL2003 compatible format, applying common whitespace tokenization and splitting tokens on non-alphanumeric characters, as this increased the performance of our model. sults for sequence tagging tasks (eg. Akbik et al., 2018, 2019b). We follow the approach of Akbik et al., using FLAIR to stack (i.e. concatenate) character and word embeddings to improve recognition rates. We further expand this model by adding sub-word embeddings to the stacked embeddings. These stacked embeddings serves as input for a BiLSTM-CRF sequence tagger (Huang et al., 2015). For our best performing model, we used two different token-level embeddings, a WANG 2 VECbased embedding (Ling et al., 2015) and a FASTT EXT-based embedding (Bojanowski et al., 2017), a single byte-pair sub-word embedding (Heinzerling and Strube, 2018) and one context"
D19-5702,N15-1142,0,0.16461,"tion and splitting tokens on non-alphanumeric characters, as this increased the performance of our model. sults for sequence tagging tasks (eg. Akbik et al., 2018, 2019b). We follow the approach of Akbik et al., using FLAIR to stack (i.e. concatenate) character and word embeddings to improve recognition rates. We further expand this model by adding sub-word embeddings to the stacked embeddings. These stacked embeddings serves as input for a BiLSTM-CRF sequence tagger (Huang et al., 2015). For our best performing model, we used two different token-level embeddings, a WANG 2 VECbased embedding (Ling et al., 2015) and a FASTT EXT-based embedding (Bojanowski et al., 2017), a single byte-pair sub-word embedding (Heinzerling and Strube, 2018) and one context sensitive character-level language model (Akbik et al., 2019b). Figure 1 gives a visual depiction of our best performing model. The following paragraphs describe the used embeddings in more detail. Spanish Health Corpus In this section, we describe the Spanish Health Corpus, a collection of 7353 diverse Spanish health and science journal articles and papers. The corpus was obtained from SciELO4 by means of an automated crawler.5 The content of the art"
D19-5702,Q17-1010,0,0.217138,"ters, as this increased the performance of our model. sults for sequence tagging tasks (eg. Akbik et al., 2018, 2019b). We follow the approach of Akbik et al., using FLAIR to stack (i.e. concatenate) character and word embeddings to improve recognition rates. We further expand this model by adding sub-word embeddings to the stacked embeddings. These stacked embeddings serves as input for a BiLSTM-CRF sequence tagger (Huang et al., 2015). For our best performing model, we used two different token-level embeddings, a WANG 2 VECbased embedding (Ling et al., 2015) and a FASTT EXT-based embedding (Bojanowski et al., 2017), a single byte-pair sub-word embedding (Heinzerling and Strube, 2018) and one context sensitive character-level language model (Akbik et al., 2019b). Figure 1 gives a visual depiction of our best performing model. The following paragraphs describe the used embeddings in more detail. Spanish Health Corpus In this section, we describe the Spanish Health Corpus, a collection of 7353 diverse Spanish health and science journal articles and papers. The corpus was obtained from SciELO4 by means of an automated crawler.5 The content of the articles in this corpus was downloaded as embedded text from"
D19-5702,goldhahn-etal-2012-building,0,0.0224533,"Missing"
D19-5702,P16-1162,0,0.00932557,"AST T EXT and WANG 2 VEC embeddings are trained on T tokens, and BPE MB uses S syllable input. The embeddings are stacked and serve as input for a BiLSTM-CRF Sequence Labeling Model. Byte-Pair Embeddings. Similar to FAST T EXT, Byte-Pair embeddings (Heinzerling and Strube, 2018, BPE MB) are trained on a pre-processed corpus that contains sub-word entities. But in contrast to FAST T EXT, words in the training corpus are represented as combinations of syllables instead of skip-grams. These syllables or subword units are learned from the corpus prior to the segmentation using Byte-Pair-Encoding (Sennrich et al., 2016) for a predefined number. In our experiments, we used pre-trained 300 dimensional Spanish Byte-Pair embeddings made available by Heinzerling and Strube (2018) with a syllable vocabulary size of 100 000.9 level WORD 2 VEC model of Mikolov et al. (2013). During training, WANG 2 VEC makes a prediction for each neighboring position of the target word instead of making a single prediction for all neighbours. Thus, the resulting embeddings are better at capturing syntactic, positional information (Ling et al., 2015). We trained 300 dimensional WANG 2 VEC-based embeddings based on 100 iterations usin"
E09-2006,J93-2004,0,\N,Missing
E17-3005,C16-2013,1,0.892468,"Missing"
gleim-mehler-2010-computational,J93-2004,0,\N,Missing
gleim-mehler-2010-computational,kunze-lemnitzer-2002-germanet,0,\N,Missing
gleim-mehler-2010-computational,P02-1022,0,\N,Missing
islam-mehler-2012-customization,J93-1004,0,\N,Missing
islam-mehler-2012-customization,C08-1118,0,\N,Missing
islam-mehler-2012-customization,2008.amta-papers.5,0,\N,Missing
islam-mehler-2012-customization,2005.mtsummit-papers.11,0,\N,Missing
K19-1081,C18-1139,0,0.0344642,"NLL) Our neural models consist of two separately trained components: a) foundational word embeddings, modeling the general knowledge from large unlabeled text corpora, and b) various taskspecific neural architectures, modeling the domain 7 Model Flair Wang2vec We further train a sequence labeling model using Flair10 . We build the model in http://www.wikidata.org/ 9 8 All results can be acquired with the following WikiData queries: http://w.wiki/3u3 and http://w.wiki/3ud 10 876 http://www.texttechnologylab.org/resources2018/ http://github.com/zalandoresearch/flair the same fashion as used by (Akbik et al., 2018) following the guide given by the authors for the task ”CoNLL-03 Named Entity Recognition (German)”, while keeping the pooled contextualized embeddings (PCE) and exchanging the GloVe embeddings employed by the authors with Wang2vec embeddings trained on the COW corpus. Flair ELMo In addition to the previous model, we train a Flair Sequence Tagging model by stacking an ELMo embedding layer on top of the Flair Wang2vec model. The ELMo embeddings were trained on a section of the Leipzig Corpora Collection (Goldhahn et al., 2012) containing 100,000 sentences from Wikipedia using default parameters"
K19-1081,benikova-etal-2014-nosta,0,0.116958,"d. In the same year, with the emergence of multilingual language models such as ELMo, Flair and BERT (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018), the performance of various NLP tasks, including NER, was notably improved. Hence, the task of German NER has benefited from these developments. However, with respect to the availability of a variety of resources, there has not been much progress made until now. Regarding the standard task of NER based on four categories (PERSON, LOCATION, ORGANIZATION, OTHER), the first choice of resources for German is still the GermEval dataset (Benikova et al., 2014), followed by the datasets of CoNLL and T¨uBa-D/Z (Tjong Kim Sang and De Meulder, 2003; Telljohann et al., 2012). However, their potential for purposes outside of theoretical ML is limited. These datasets do not contain any annotations for taxonomic and temporal entities which are of key interest for biodiversity researchers. For biological NER in the German language, there are no predecessor resources available to the knowledge of the authors; only an English counterpart exists, namely the Copious dataset (T.H. Nguyen et al., 2019), which has been reFigure 1: Flowchart showing the data cleani"
K19-1081,goldhahn-etal-2012-building,0,0.0573212,"Missing"
K19-1081,N16-1175,0,0.02585,"Missing"
K19-1081,W03-0419,0,0.192928,"Missing"
K19-1081,N16-1030,0,0.303692,"remainder of the paper is organized as follows: Section 2 reviews related work. Section 3 describes the source texts and the preprocessing pipeline. Section 4 describes the annotation guidelines, process and environment for producing the BIOfid dataset, and methods (n-gram-based sequence tagger, neural models) for evaluating the practical quality of our annotated dataset. Section 5 presents the experimental results. Finally, Section 6 draws a conclusion. 2 Related Work 2018 was a vital year for the task of German NER, following a saturation period from when the last major progress was made by Lample et al. (2016). With the grammar-specific morphological processing and resource-optimization presented by Ahmed and Mehler (2018), the gap between English and German NER was closed. In the same year, with the emergence of multilingual language models such as ELMo, Flair and BERT (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018), the performance of various NLP tasks, including NER, was notably improved. Hence, the task of German NER has benefited from these developments. However, with respect to the availability of a variety of resources, there has not been much progress made until now. Regardin"
K19-1081,P14-2050,0,0.013334,"ges after randomizing its order of sentences. These final data files are utilized for training and evaluating our models, which are described in the next section. 875 knowledge from the labeled training data. In this section, both components are presented briefly. 4.5 Methods For the evaluation of the BIOfid dataset, we use six different approaches and compare each others results: one classic rule-based model and five highperforming embedding-based models. 4.5.1 Word Embeddings The language model of continuous space word representations (word2vec) (Mikolov et al., 2013) and its variations by (Levy and Goldberg, 2014; Komninos and Manandhar, 2016) are the foundations of most ongoing research in NLP with neural networks. Based on the context, the model embeds words, phrases or sentences into high dimensional vector spaces. We use the model of Wang2vec (Ling et al., 2015) and its morphological extension (Ahmed and Mehler, 2018) which explores syntactic data specific for German and, thus, better suites the task of NER. We use the recently published German language word embeddings from the TTLab9 which are pre-trained with the morphological extension of the Wang2vec algorithm on the COW corpus (Sch¨afer, 2015"
K19-1081,N15-1142,0,0.0177281,". 4.5 Methods For the evaluation of the BIOfid dataset, we use six different approaches and compare each others results: one classic rule-based model and five highperforming embedding-based models. 4.5.1 Word Embeddings The language model of continuous space word representations (word2vec) (Mikolov et al., 2013) and its variations by (Levy and Goldberg, 2014; Komninos and Manandhar, 2016) are the foundations of most ongoing research in NLP with neural networks. Based on the context, the model embeds words, phrases or sentences into high dimensional vector spaces. We use the model of Wang2vec (Ling et al., 2015) and its morphological extension (Ahmed and Mehler, 2018) which explores syntactic data specific for German and, thus, better suites the task of NER. We use the recently published German language word embeddings from the TTLab9 which are pre-trained with the morphological extension of the Wang2vec algorithm on the COW corpus (Sch¨afer, 2015), the largest collection of German texts extracted from web documents with over 617 Mio. sentences. Out of the six published variants of embeddings, we opt for token-based embeddings (COW.lower.wang2vec), as they delivered the best results for German NER ac"
K19-1081,N18-1202,0,0.0305909,"Missing"
L16-1239,D12-1133,0,0.0245951,"ost recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutanova et al., 2003), Lapos (Tsuruoka et al., 2011), Mate (Bohnet and Nivre, 2012). We also include the OpenNLPTagger, an official Apache project.4 For these systems, we refer to the original works for descriptions. Among the most recent generation of taggers, we consider the MarMoT (M¨uller et al., 2013) tagger, which implements a higher order CRF with approximations such that it can deal with large output spaces. In addition, MarMoT can be trained to fire on the predictions of lexical resources as well as on word embeddings, real vectorvalued representations of words. FLORS (Schnabel and Sch¨utze, 2014) tags a given word by constructing a feature vector representation of"
L16-1239,A00-1031,0,0.867929,"ion retrieval, knowledge extraction, or semantic analysis. In morphologically rich languages such as German and Latin, both problems are non-trivial due to the variability of lexical forms. This results both in large tagsets for POS tagging — which list such inflectional categories as case, gender, degree, etc., besides coarse-grained POS labels — and a large number of (potentially unseen) forms associated with each lemma. In this work, we survey tagging and lemmatization techniques for the two languages mentioned. Our survey includes both older, such as the TreeTagger (Schmid, 1994) and TnT (Brants, 2000), and more modern approaches to tagging and lemmatization. Although our expectation is clearly that technology steadily improves with time, it is apropri not obvious how large the gap between older and more modern approaches is, and also what the ordering of the most recent generation of systems is. We test our systems under the following requirements:1 • Ideally, we would want a learned system to perform well on the distribution (a specific text genre, historical language variant, etc.) on which it has been trained (in-domain (ID)) but also to perform decently on corpora of similar but differ"
L16-1239,D08-1113,0,0.0149464,"ncies between POS tagging and lemmatization, which should substantially improve its performance relative to approaches where the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatizat"
L16-1239,N13-1138,0,0.0192683,"). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on prefix and suffix transformations. LemmaGen (Jursic et al., 2010) learns ‘ripple down rules’ (Compton and Jansen, 1988), that is, tree-like decision 2 Alternatively, le"
L16-1239,W15-3716,1,0.894242,"Missing"
L16-1239,P12-2072,0,0.357689,"ere the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on prefix and suffix transformations. LemmaGen (Jursic et al., 2010) learns ‘ripple down rules’ (Compton and Jansen"
L16-1239,D15-1025,0,0.0297568,"Missing"
L16-1239,L16-1677,1,0.87418,"Missing"
L16-1239,D13-1032,0,0.107869,"Missing"
L16-1239,D15-1272,0,0.313835,"Missing"
L16-1239,N15-1093,0,0.0185071,"ging and lemmatization, which should substantially improve its performance relative to approaches where the tasks are treated independently (M¨uller et al., 2015). • Finally, run times of systems may be of considerable interest for practitioners. Therefore, we include both training and testing time estimations of the different techniques. 2. Lemmatization We view lemmatization as the problem of transforming a word form into its canoncial form, or lemma. In a machine learning context,2 lemmatization has e.g. been considered as a character-level string transduction process (Dreyer et al., 2008; Nicolai et al., 2015; Eger, 2015), a prefix and suffix transformation problem (Jursic et al., 2010; Gesmundo and Samardzic, 2012) or as a pattern matching task (Durrett and DeNero, 2013; Ahlberg et al., 2014). While characterlevel string transducers may yield excellent results (Nicolai et al., 2015), particularly when trained and tested on lists of words randomly extracted from a lexicon (Eger, 2015), they tend to be slower to learn and typically consider the lemmatization problem in isolation, ignoring contextual word form cues.3 In this work, we experiment with two approaches to lemmatization, both based on pre"
L16-1239,W96-0213,0,0.596937,"nd the last character is replaced by en. This compact encoding allows to view lemmatization as a classification problem where the size of the output space is relatively small, on the orders of at most hundreds or thousands of labels. Moreover, lemmatization can then also be treated as a sequence labeling problem, where dependence between subsequent labels may be taken into account. 3. POS tagging POS tagging (or sequence labeling) has witnessed several milestones such as including dependencies between output labels (as in Markov models such as HMMs or CRFs), the broad use of lexical features (Ratnaparkhi, 1996; Toutanova et al., 2003), or the concept of the margin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000)"
L16-1239,Q14-1002,0,0.0446971,"Missing"
L16-1239,N03-1033,0,0.0203684,"er is replaced by en. This compact encoding allows to view lemmatization as a classification problem where the size of the output space is relatively small, on the orders of at most hundreds or thousands of labels. Moreover, lemmatization can then also be treated as a sequence labeling problem, where dependence between subsequent labels may be taken into account. 3. POS tagging POS tagging (or sequence labeling) has witnessed several milestones such as including dependencies between output labels (as in Markov models such as HMMs or CRFs), the broad use of lexical features (Ratnaparkhi, 1996; Toutanova et al., 2003), or the concept of the margin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutan"
L16-1239,W11-0328,0,0.0646816,"rgin introduced in SVMs. The most recent class of taggers is characterized by the availability to include word representations learned from unlabeled data, the possibility to apply feature-rich models to problems with large output spaces, and/or by making use of deep (rather than shallow) models such as neural networks that can in addition function without handcrafting features. In this work, we consider the following part-of-speech tagging systems, listed by the order of their year of publication: TreeTagger (Schmid, 1994), TnT (Brants, 2000), Stanford tagger (Toutanova et al., 2003), Lapos (Tsuruoka et al., 2011), Mate (Bohnet and Nivre, 2012). We also include the OpenNLPTagger, an official Apache project.4 For these systems, we refer to the original works for descriptions. Among the most recent generation of taggers, we consider the MarMoT (M¨uller et al., 2013) tagger, which implements a higher order CRF with approximations such that it can deal with large output spaces. In addition, MarMoT can be trained to fire on the predictions of lexical resources as well as on word embeddings, real vectorvalued representations of words. FLORS (Schnabel and Sch¨utze, 2014) tags a given word by constructing a fe"
L16-1239,D15-1155,0,0.0351593,"Missing"
L16-1240,N15-1055,0,0.0551392,"Missing"
L16-1240,N03-1033,0,0.394701,"agger that integrates a rule interpreter and a statistical model in the area of Latin texts. Rules are useful in cases of data sparseness which do not allow for defining sufficient training data. The source code of the tagger can be obtained from http://prepro. hucompute.org. 2. Related Work There is a multitude of statistical approaches for PoS tagging. Almost all of them rely on a statistical model that is trained on annotated examples and then applied to previously unseen texts. These models include decision trees (Schmid, 1994), Hidden Markov Models (Brants, 2000), Maximum Entropy Models (Toutanova et al., 2003), neural networks and structured SVMs. An exception is described by (Bellegarda, 2010) who employs learning per analogy together with latent semantic analysis (Landauer et al., 1998) (called latent analogy). While there exists a lot of freely available PoS taggers, so-called morphological taggers are still rare. An exception is (M¨uller and Sch¨utze, 2015) who provide a morphological tagger for several languages that is based on 3rd order CRFs (Lafferty et al., 2001). A hybrid morphological tagger for Czech is introduced by (Spoustov´a, 2008). The employed statistical model is a combination of"
L16-1240,W11-0328,0,0.0289382,"joint tagging, which can also be accomplished with older CRF applications like CRFsuite that are not able to handle huge amount of tags. For that, besides of an isolated tagging of morphological labels, we jointly tag certain label combinations (currently only case, gender, and number). In case of ties in the above described lexicon lookup, we prefer the lexicon entries that are compliant with the result of the joint prediction. 6.1. Evaluation We evaluate TLT-CRF with respect to lemmatization and morphological tagging using the Capitularies corpus (see above). We compare TLT-CRF with Lapos (Tsuruoka et al., 2011), TreeTagger (Schmid, 1994), StanfordTagger (Toutanova et al., 2003), OpenNLPTagger5 and MarMoT (M¨uller and Sch¨utze, 2015) by training and evaluating these taggers using the same data set. The results of our evaluation are shown in Table 2. According to these results, TLT-CRF achieves promising results in comparison to its state-of-the-art competitors. In our evaluation scenario, MarMoT is definitely the best performing tagger. However, TLT-CRF performs second best in the case of tagging PoS. MarMoT and TLT-CRF, which make use of the lexicon, perform best regarding PoS tagging. This stresses"
L16-1240,A00-1031,0,\N,Missing
L16-1240,W15-3716,1,\N,Missing
L16-1240,L16-1239,1,\N,Missing
L16-1677,petrov-etal-2012-universal,0,0.0327301,"1 3.2. Table 4: Overview: reliability assessment. Tagset PercAgree Kappa AC1 AC1 Conf. STTS UT 77.12 84.68 0.87 0.92 0.87 0.92 (0.85 – 0.89) (0.91 – 0.94) 2.3. Reliability In order to assess the reliability (Carmines and Zeller, 1979) of the part-of-speech annotation of TGermaCorp we calculated the interrater agreement of several annotators and different data. The main agreement study comprises five annotators’ STTS annotations of an extract of 555 words of Thomas Mann’s novel Der Tod in Venedig. Additionally, the STTS annotation has been mapped onto the 12 tags of the Universal Tagset (UT) (Petrov et al., 2012). Agreement has been measured by means of three coefficients: raw percentage agreement (“PercAgree”), Fleiss’ Kappa (Fleiss, 1971), and Gwet’s AC1 (Gwet, 2001). The respective results are collected in Table 4. The reliability results reach Krippendorff’s (Krippendorff, 1980) level of credible results (i.e., agreement coefficient &gt; 0.80), which, according to (Rietveld and van Hout, 1993) can even be regarded as “almost perfect”. We used the R environment for statistical computing (R Core Team, 2013) for all analyses and calculations. 3. TigerSmall and WikiMimikry Assessing the Lexical and Synta"
L16-1677,C12-2095,0,0.0196118,"of different measures and/or to motivate the choice of measure carefully. The question for the effect of measure has been raised with respect to various subfields various times, see for instance Cha (2007) on density functions providing a dendrogram of distance measures, Salleh et al. (2012) on geometrical shapes, Cerqueira-Silva et al. (2009) on molecular markers. The latter reported a highly significant Spearman correlation of 0.58 between the Mahalanobis distance and the Euclidian distance, making them the most distant measures for their data and distance set. In computational linguistics, Rama and Kolachina (2012) worked on typological distances. Jin and Barri`ere (2005) found in a preliminary study, that the Dice coefficient, most similar to the Jaccard index, correlated best with human similarity judgments. Given these considerations, the choice of the three applied measures allows for the assessment of different aspects of the data and allows generalisability on the other hand. 4. Discussion Given the different genres underlying TGermaCorp and the two comparison corpora, the quite similar results of the diversity measures applied above come as a surprise. However, those measures focus on the respect"
L18-1168,agerri-etal-2014-ixa,0,0.0312178,"define a label for any input text, while word2vec uses context windows of lexical units to predict single words or vice versa. We transpose fastText to word sense disambiguation in order to efficiently determine the meaning of ambiguous words even in cases in which we face big data. By this we mean scenarios in which hundreds of thousands of different words are ambiguous. fastSense is characterized by its simplicity, speed and quality. This distinguishes it from similar tools. For instance, (Mihalcea and Csomai, 2007; Ferragina and Scaiella, 2010; Ratinov et al., 2011b; Ratinov et al., 2011a; Agerri et al., 2014; Moro et al., 2014) present approaches to Entity Linking. More specifically, they link tokens in texts to knowledge databases such as DBpedia, Wikipedia or WordNet to identify instances of entities. These approaches are similar to ours, with the difference that we focus on ambiguous words, while the latter approaches also link words that have only one meaning. The disadvantage of these approaches is their speed. For large amounts of data, they may take weeks to produce an output (see Table 2 for an estimation of this time effort). (Mihalcea, 2007) uses a technique similar to the one presented"
L18-1168,C16-2013,1,0.809796,"With the same hardware, these tools take 6 to 188 days to process our test set. The effort was estimated based on a subset of elements as documented in Table 2. Obviously, fastSense outperforms these competitors. However, since these tools link to different resources (e.g., DBpedia, WordNet or Wikipedia), this comparison only holds for time effort. 4.2. Wikipedia-based Disambiguation In order to show that our approach allows for capturing large amounts of data, we created a corpus using the disambiguation pages of the whole German Wikipedia. For preprocessing this data we used the TextImager (Hemati et al., 2016) pipeline. Every word listed on a disambiguation page in Wikipedia corresponds to a different meaning of the corresponding lemma (page title). In total, we processed 221,965 disambiguation pages related to 825,179 senses. On average, this gives 3.72 senses Senseval and SemEval related Disambiguation SemCor (Mihalcea, 2016) provides texts with semantically annotated WordNet senses, which are automatically mapped to WordNet. We trained on SemCor 3.0 for performing Senseval and SemEval related tests. Because of the small amount of data provided by this corpus (234,136 disambiguated words), we wer"
L18-1168,P16-1085,0,0.0886293,"n et al., 2016) present two WSD algorithms, achieving the best results by means of a semisupervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity. (Tripodi and Pelillo, 2016) describe an approach to WSD based on evolutionary game theory, in which words tend to adapt senses of their neighborhood so that WSD is reformulated as a kind of constraint satisfaction. (Zhong and Ng, 2010) present a framework for English all-words WSD. It disambiguates each content word of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Chaplot et al., 2015) propose a graph based unsupervised WSD system which requires WordNet, a dependency parser and a POS-Tagger. They model WSD as a maximum-a-posteriori inference query operating on a Markov random field. (Raganato et al., 2017a) define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network (Hochreiter and Schmidhuber, 1997). (Melamud et al., 2016) present context2vec which is also based on bidirectional LSTMs for learni"
L18-1168,K16-1006,0,0.0237603,"of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Chaplot et al., 2015) propose a graph based unsupervised WSD system which requires WordNet, a dependency parser and a POS-Tagger. They model WSD as a maximum-a-posteriori inference query operating on a Markov random field. (Raganato et al., 2017a) define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network (Hochreiter and Schmidhuber, 1997). (Melamud et al., 2016) present context2vec which is also based on bidirectional LSTMs for learning disambiguating word contexts. Unlike these approaches, we present a method that can handle big data: in terms of the number of senses to be distinguished and in terms of the number of units to be disambiguated. On the one hand, knowledge driven approaches using, for example, WordNet and related resources are limited in terms of the number of senses distinguished by them. GermaNet, for example, distinguishes 33,630 senses of 13,445 ambiguous words – that is much less than considered by us. On the other hand, approaches"
L18-1168,C04-1162,0,0.191222,"ambiguous words, while the latter approaches also link words that have only one meaning. The disadvantage of these approaches is their speed. For large amounts of data, they may take weeks to produce an output (see Table 2 for an estimation of this time effort). (Mihalcea, 2007) uses a technique similar to the one presented here to build a sense-tagged Wikipedia corpus using the link structure of Wikipedia to match senses. However, this corpus has not been used to disambiguate ambiguous words according to Wikipedia’s disambiguation pages, but to com1042 pare them with the data of Senseval 2. (Mihalcea et al., 2004) use a PageRank algorithm operating on semantic networks to perform WSD. The underlying network is spanned by means of semantic relations of synsets, entailment and other WordNet relations. The PageRank algorithm assigns scores to words and chooses the disambiguating synset of highest score. (Yuan et al., 2016) present two WSD algorithms, achieving the best results by means of a semisupervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity. (Tripodi and Pelillo, 2016) describe an approach to WSD based on evolutionary game theory, i"
L18-1168,N07-1025,0,0.060642,"nov et al., 2011b; Ratinov et al., 2011a; Agerri et al., 2014; Moro et al., 2014) present approaches to Entity Linking. More specifically, they link tokens in texts to knowledge databases such as DBpedia, Wikipedia or WordNet to identify instances of entities. These approaches are similar to ours, with the difference that we focus on ambiguous words, while the latter approaches also link words that have only one meaning. The disadvantage of these approaches is their speed. For large amounts of data, they may take weeks to produce an output (see Table 2 for an estimation of this time effort). (Mihalcea, 2007) uses a technique similar to the one presented here to build a sense-tagged Wikipedia corpus using the link structure of Wikipedia to match senses. However, this corpus has not been used to disambiguate ambiguous words according to Wikipedia’s disambiguation pages, but to com1042 pare them with the data of Senseval 2. (Mihalcea et al., 2004) use a PageRank algorithm operating on semantic networks to perform WSD. The underlying network is spanned by means of semantic relations of synsets, entailment and other WordNet relations. The PageRank algorithm assigns scores to words and chooses the disa"
L18-1168,Q14-1019,0,0.0689181,"y input text, while word2vec uses context windows of lexical units to predict single words or vice versa. We transpose fastText to word sense disambiguation in order to efficiently determine the meaning of ambiguous words even in cases in which we face big data. By this we mean scenarios in which hundreds of thousands of different words are ambiguous. fastSense is characterized by its simplicity, speed and quality. This distinguishes it from similar tools. For instance, (Mihalcea and Csomai, 2007; Ferragina and Scaiella, 2010; Ratinov et al., 2011b; Ratinov et al., 2011a; Agerri et al., 2014; Moro et al., 2014) present approaches to Entity Linking. More specifically, they link tokens in texts to knowledge databases such as DBpedia, Wikipedia or WordNet to identify instances of entities. These approaches are similar to ours, with the difference that we focus on ambiguous words, while the latter approaches also link words that have only one meaning. The disadvantage of these approaches is their speed. For large amounts of data, they may take weeks to produce an output (see Table 2 for an estimation of this time effort). (Mihalcea, 2007) uses a technique similar to the one presented here to build a sen"
L18-1168,D17-1120,0,0.112303,"eighborhood so that WSD is reformulated as a kind of constraint satisfaction. (Zhong and Ng, 2010) present a framework for English all-words WSD. It disambiguates each content word of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Chaplot et al., 2015) propose a graph based unsupervised WSD system which requires WordNet, a dependency parser and a POS-Tagger. They model WSD as a maximum-a-posteriori inference query operating on a Markov random field. (Raganato et al., 2017a) define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network (Hochreiter and Schmidhuber, 1997). (Melamud et al., 2016) present context2vec which is also based on bidirectional LSTMs for learning disambiguating word contexts. Unlike these approaches, we present a method that can handle big data: in terms of the number of senses to be distinguished and in terms of the number of units to be disambiguated. On the one hand, knowledge driven approaches using, for example, WordNet and related resources are limited in terms of the number of"
L18-1168,E17-1010,0,0.10654,"eighborhood so that WSD is reformulated as a kind of constraint satisfaction. (Zhong and Ng, 2010) present a framework for English all-words WSD. It disambiguates each content word of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Chaplot et al., 2015) propose a graph based unsupervised WSD system which requires WordNet, a dependency parser and a POS-Tagger. They model WSD as a maximum-a-posteriori inference query operating on a Markov random field. (Raganato et al., 2017a) define WSD in terms of a sequence learning problem. This is done by means of a bidirectional LSTM-based neural network (Hochreiter and Schmidhuber, 1997). (Melamud et al., 2016) present context2vec which is also based on bidirectional LSTMs for learning disambiguating word contexts. Unlike these approaches, we present a method that can handle big data: in terms of the number of senses to be distinguished and in terms of the number of units to be disambiguated. On the one hand, knowledge driven approaches using, for example, WordNet and related resources are limited in terms of the number of"
L18-1168,P11-1138,0,0.192167,"c and fastText is that the latter requires to define a label for any input text, while word2vec uses context windows of lexical units to predict single words or vice versa. We transpose fastText to word sense disambiguation in order to efficiently determine the meaning of ambiguous words even in cases in which we face big data. By this we mean scenarios in which hundreds of thousands of different words are ambiguous. fastSense is characterized by its simplicity, speed and quality. This distinguishes it from similar tools. For instance, (Mihalcea and Csomai, 2007; Ferragina and Scaiella, 2010; Ratinov et al., 2011b; Ratinov et al., 2011a; Agerri et al., 2014; Moro et al., 2014) present approaches to Entity Linking. More specifically, they link tokens in texts to knowledge databases such as DBpedia, Wikipedia or WordNet to identify instances of entities. These approaches are similar to ours, with the difference that we focus on ambiguous words, while the latter approaches also link words that have only one meaning. The disadvantage of these approaches is their speed. For large amounts of data, they may take weeks to produce an output (see Table 2 for an estimation of this time effort). (Mihalcea, 2007)"
L18-1168,C16-1130,0,0.0344018,"presented here to build a sense-tagged Wikipedia corpus using the link structure of Wikipedia to match senses. However, this corpus has not been used to disambiguate ambiguous words according to Wikipedia’s disambiguation pages, but to com1042 pare them with the data of Senseval 2. (Mihalcea et al., 2004) use a PageRank algorithm operating on semantic networks to perform WSD. The underlying network is spanned by means of semantic relations of synsets, entailment and other WordNet relations. The PageRank algorithm assigns scores to words and chooses the disambiguating synset of highest score. (Yuan et al., 2016) present two WSD algorithms, achieving the best results by means of a semisupervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity. (Tripodi and Pelillo, 2016) describe an approach to WSD based on evolutionary game theory, in which words tend to adapt senses of their neighborhood so that WSD is reformulated as a kind of constraint satisfaction. (Zhong and Ng, 2010) present a framework for English all-words WSD. It disambiguates each content word of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al"
L18-1168,P10-4014,0,0.205994,"tic relations of synsets, entailment and other WordNet relations. The PageRank algorithm assigns scores to words and chooses the disambiguating synset of highest score. (Yuan et al., 2016) present two WSD algorithms, achieving the best results by means of a semisupervised algorithm combining labeled sentences with unlabeled ones and propagating labels based on sentence similarity. (Tripodi and Pelillo, 2016) describe an approach to WSD based on evolutionary game theory, in which words tend to adapt senses of their neighborhood so that WSD is reformulated as a kind of constraint satisfaction. (Zhong and Ng, 2010) present a framework for English all-words WSD. It disambiguates each content word of a given sentence using a linear kernel-based SVM (Joachims, 2002). (Iacobacci et al., 2016) show that the use of word embeddings achieves an improvement in WSD compared to standard features. (Chaplot et al., 2015) propose a graph based unsupervised WSD system which requires WordNet, a dependency parser and a POS-Tagger. They model WSD as a maximum-a-posteriori inference query operating on a Markov random field. (Raganato et al., 2017a) define WSD in terms of a sequence learning problem. This is done by means"
L18-1212,L18-1308,1,0.593089,"Missing"
L18-1212,C16-2013,1,0.666856,"n of UIMA DI into rights and resource management tools enables user and group-specific access to UIMA documents and provides data protection. Finally, UIMA documents can be made accessible for third party programs. UIMA DI, which we evaluate in relation to file system-based storage, is available under the GPLv3 license via GitHub. Keywords: UIMA, database interface, text annotation, Neo4J, MongoDB, Web Services, REST 1. Introduction NLP and automatic text analysis necessarily involve the annotation of natural language texts. In various projects (e.g. (Da Silva et al., 2006; Kano et al., 2009; Hemati et al., 2016)), especially in an NLP context (e.g. (Savova et al., 2010; Eckart de Castilho and Gurevych, 2014; Patterson et al., 2017; Kreimeyer et al., 2017; Rudzewitz et al., 2017; Kassner et al., 2017)), the Apache Unstructured Information Management applications (UIMA) (Ferrucci et al., 2009) is used as a standard architecture for text annotation. Accordingly, there are various tools for processing and producing UIMA compliant documents (e.g. (Ogren et al., 2008; Zeldes et al., 2009; Ferrucci et al., 2010; Hemati et al., 2016; Niekler et al., 2017)). UIMA documents are specified by means of UIMA Type"
L18-1212,W17-0305,0,0.0628666,"Missing"
L18-1308,W17-3602,0,0.013566,"uments – the standard format of many scientific publication organs –, but enables further modifications within the graphic’s source code. By this means, relevant features can be highlighted for publications – as illustrated in Fig. 2 –, or visualizations can be uncovered piecewise in presentations for talks. (c) Multiple Window View A well-known problem of RST annotations is that there may be several, equally justified but concurring rhetorical analyses and structures of text spans (Moore and Pollack, 1992). Since this pluralism of rhetorical analyses is perfectly legitimate according to RST (Das et al., 2017), it has to be accounted for in the annotation workflow. Accordingly, a multiple window view was developed, which allows users to simultaneously inspect multiple annotations of the same text side by side, see Fig. 3. In the current version, this is achieved by simply creating a copy of the document. In future releases, the different annotations will be stored in the same document by using different Subjects of Analysis, that is, specific UIMA objects3 , which will further open up the possibility of having different annotations even for fragments of a text. 2.3. Storing Annotations in TextAnnot"
L18-1308,C16-2013,1,0.537104,"otations: this has been evaluated in a study in comparison to the RSTTool. The components, annotation processes, and visualizations of T REE A NNOTATOR are described in Sec. 2. The usability study is reported in Sec. 3. Future developments are summarized in Sec. 4. 2. 2.1. TreeAnnotator Annotating Tree-like Text Structures in TreeAnnotator T REE A NNOTATOR is an up-to-date, browser-based and freely available device for discourse annotation. It is designed as a module of the still-developing annotation suite T EXT A NNOTATOR. T EXT A NNOTATOR in turn draws on the architecture of T EXT I MAGER (Hemati et al., 2016), an UIMA-based framework that offers a wide range of NLP and visualization tools through a user-friendly GUI. All NLP tools that are available via T EXT I MAGER can be utilized for preprocessing input documents, say, in terms of tokenization, lemmatization or pos tagging. T REE A NNO TATOR ’s front-end is based on the Ext JS framework1 and 1958 1 https://www.sencha.com/products/extjs/#overview 1-4 Interpretation 3-4 1-2 Elaboration 1: The administration must come to a decision, Disjunction 2: and fast. 4: or put priority on education. 3: Either save money at all costs Figure 1: T REE A NNOTAT"
L18-1308,J92-4007,0,0.0751414,"s that the right frontier of a text becomes visible: the right frontier constitutes the structural 1959 example2.rs3 example1.rs3 maz-17539.rs3 1-3 1-3 Motivation 1: Come home by 5:00. Condition 2-3 1-2 Condition Motivation 2: Then we can go to the hardware store before it closes. RST (Classical) 3: That way we can ﬁnish the bookshelves tonight. 3: That way we can ﬁnish the bookshelves tonight. 1: Come home by 5:00. RST (Dep. & Incl.) RST (Classical) 2: Then we can go to the hardware store before it closes. RST (Dep. & Incl.) Figure 3: Multiple window view, showing two concurring annotations (Moore and Pollack, 1992, p. 542 f.). Several collapsible annotation views can be visualized. boundary for anaphoric attachment sites and, hence, is of importance for any further co-reference annotation. (b) Graphics Output Besides screenshot-like graphics (PNG), T REE A NNOTATOR allows for generating LATEX output: RST trees are exported using the rst package (Reitter, 2002), DIHs are output to PGF/TikZ (Tantau, 2015). PGF/TikZ are formal languages that can be interpreted by TEX (Knuth, 1984), LATEX (Lamport, 1994) (including XELATEX), and ConTEXt (Hagen and Hoekwater, 2013) (including LuaTEX) to produce vector graph"
L18-1308,W04-0213,0,0.161432,"Missing"
L18-1308,E12-2021,0,0.243184,"Missing"
L18-1308,N16-3001,0,0.248865,"ly in terms of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), has not gained much attention for quite a long time. Most RST annotations, with the RST Discourse Treebank (Carlson et al., 2003)(Carlson et al., 2014) leading the way, are carried out with the original RST Annotation Tool (O’Donnell, 1997) or its extension, the ISI RST Annotation Tool (Marcu, 1999). While still usable, both tools, however, are not maintained any more and do not comply to current annotation frameworks. Due to such reasons, a third, browser-based RST annotation tool has been developed recently: rstWeb (Zeldes, 2016). Why, then, is there need for a new tool like T REE A NNO TATOR ? We highlight three reasons in the following: Firstly, T REE A NNOTATOR, unlike rstWeb, follows the UIMA framework, which supports current requirements of annotation pipelines (Wilcock, 2017). Note that also the Atomic/ANNIS frameworks (Druskat et al., 2014; Krause and Zeldes, 2016) are not based on UIMA, and hence T REE A NNOTATOR is a module following a technologically complementary approach, more related to the “NLP-affine” text annotation tool BRAT (Stenetorp et al., 2012) (which in turn is the technological and design sourc"
L18-1589,I13-1057,0,0.0162708,"based on the simple English Wikipedia with respect to the accuracy of link extraction, diachronic network analysis and the impact of using different Wikipedia frameworks to text analysis. Keywords: Wikipedia, Java Framework, Diachronic Text Analysis, Diachronic Network Analysis, Link Extraction 1. Introduction Wikis such as Wikipedia, Wiktionary, WikiSource or WikiNews constitute a popular form of collaborative writing and thus a valuable resource for computational linguistics. This relates to research exploiting wiki data as a resource, e.g. for word sense disambiguation (Uslu et al., 2018; Dandala et al., 2013), explicit semantic analysis (Gabrilovich and Markovitch, 2007) or training data for named entity recognition (Nothman et al., 2008). Furthermore, wikis are also considered as a research object on their own, for example with respect to measuring the impact of authors and edits (Priedhorsky et al., 2007), studies on discourse structure (Mehler et al., 2018) as well as on edit categories (Daxenberger and Gurevych, 2012) or dynamics of conflicts in Wikipedias (Yasseri et al., 2012). The Wiki principle is also used to publish research results like digital editions of texts as Wikiditions (Mehler e"
L18-1589,C12-1044,0,0.0173622,"tive writing and thus a valuable resource for computational linguistics. This relates to research exploiting wiki data as a resource, e.g. for word sense disambiguation (Uslu et al., 2018; Dandala et al., 2013), explicit semantic analysis (Gabrilovich and Markovitch, 2007) or training data for named entity recognition (Nothman et al., 2008). Furthermore, wikis are also considered as a research object on their own, for example with respect to measuring the impact of authors and edits (Priedhorsky et al., 2007), studies on discourse structure (Mehler et al., 2018) as well as on edit categories (Daxenberger and Gurevych, 2012) or dynamics of conflicts in Wikipedias (Yasseri et al., 2012). The Wiki principle is also used to publish research results like digital editions of texts as Wikiditions (Mehler et al., 2016). Generally speaking, researchers need efficient as well as manageable means to access wiki data. However, one has to face the challenge that this data is not limited to articles but also includes discussions, pages in other namespaces such as portals, rich metadata, revision histories, information about (anonymous or registered) writers as well as about the link-based networking of pages. For small querie"
L18-1589,P11-4017,0,0.0382277,"Missing"
L18-1589,P14-5010,0,0.00263614,"imple English Wikipedia (extracted on 2017-09-29, using the latest revision IDs of the dump used for WikiDragon and JWPL) resulting in 127,634.00 articles. We extract plain text from each article by the default methods provided by the frameworks. As normalization towards JWPL we strip the title header as well as the categories section from the WikiDragon output. From the reference Web API output we strip the section index which is automatically created for larger pages as well as the “edit”-labels which are generated for section titles. For tokenization we use PTBTokenizer of Stanford CoreNLP(Manning et al., 2014). Figure 4 shows a comparison of the type-token ratio distributions based on WikiDragon/XOWA (blue) and JWPL/Sweble (red) against the Web API as reference. The distributions are sorted in ascending order based on the Web API variant. As the plots indicate, both distributions deviate from the gold standard to some degree. In case of Sweble the distribution is scattered much more, indicating a significant difference to the original data. Accordingly, the correlation coefficients12 between XOWA and Web API are considerably higher (0.955, T=15,679, P &lt; 2.2e−16 ) than between Sweble and Web API (0."
L18-1589,U08-1016,0,0.00988737,"f using different Wikipedia frameworks to text analysis. Keywords: Wikipedia, Java Framework, Diachronic Text Analysis, Diachronic Network Analysis, Link Extraction 1. Introduction Wikis such as Wikipedia, Wiktionary, WikiSource or WikiNews constitute a popular form of collaborative writing and thus a valuable resource for computational linguistics. This relates to research exploiting wiki data as a resource, e.g. for word sense disambiguation (Uslu et al., 2018; Dandala et al., 2013), explicit semantic analysis (Gabrilovich and Markovitch, 2007) or training data for named entity recognition (Nothman et al., 2008). Furthermore, wikis are also considered as a research object on their own, for example with respect to measuring the impact of authors and edits (Priedhorsky et al., 2007), studies on discourse structure (Mehler et al., 2018) as well as on edit categories (Daxenberger and Gurevych, 2012) or dynamics of conflicts in Wikipedias (Yasseri et al., 2012). The Wiki principle is also used to publish research results like digital editions of texts as Wikiditions (Mehler et al., 2016). Generally speaking, researchers need efficient as well as manageable means to access wiki data. However, one has to fa"
L18-1589,L18-1168,1,0.814794,"luate the framework based on the simple English Wikipedia with respect to the accuracy of link extraction, diachronic network analysis and the impact of using different Wikipedia frameworks to text analysis. Keywords: Wikipedia, Java Framework, Diachronic Text Analysis, Diachronic Network Analysis, Link Extraction 1. Introduction Wikis such as Wikipedia, Wiktionary, WikiSource or WikiNews constitute a popular form of collaborative writing and thus a valuable resource for computational linguistics. This relates to research exploiting wiki data as a resource, e.g. for word sense disambiguation (Uslu et al., 2018; Dandala et al., 2013), explicit semantic analysis (Gabrilovich and Markovitch, 2007) or training data for named entity recognition (Nothman et al., 2008). Furthermore, wikis are also considered as a research object on their own, for example with respect to measuring the impact of authors and edits (Priedhorsky et al., 2007), studies on discourse structure (Mehler et al., 2018) as well as on edit categories (Daxenberger and Gurevych, 2012) or dynamics of conflicts in Wikipedias (Yasseri et al., 2012). The Wiki principle is also used to publish research results like digital editions of texts a"
L18-1589,zesch-etal-2008-extracting,0,0.0309842,"Missing"
menke-mehler-2010-ariadne,wittenburg-etal-2006-elan,0,\N,Missing
menke-mehler-2010-ariadne,E09-2006,1,\N,Missing
menke-mehler-2010-ariadne,wittenburg-etal-2002-metadata,0,\N,Missing
P16-2009,P14-1023,0,0.0544445,", several wellknown examples of meaning change in English have been documented. For example, the word gay’s meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20th century to its present meaning of homosexual (Kulkarni et al., 2015a). Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple. In our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art in distributional semantics modeling (Baroni et al., 2014). Our approach and results are partly independent of this representation, however, in that we take a structuralist approach: we derive new, ‘second-order embeddings’ by modeling the meaning of words by 52 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 52–58, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics t ∈ T . Here, T is a set of time indices. Denote an embedding of a word wi at time period t as wi (t) ∈ Rd . Since embeddings wi (s), wi (t) for two different time periods s, t are generally not comparable, a"
P16-2009,E12-1060,0,0.0110016,"just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear model of semantic change We postulate, and subsequently test, the following model of meaning dynamics which describes meaning change over time for words wi : wi (t) = p X X n=1 wj ∈V ∩N (wi ) αnwj"
P16-2009,D14-1110,0,0.00994504,"period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear mo"
P16-2009,S15-1014,1,0.664239,"We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, today’s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change. 1 Introduction Meaning is not uniform, neither across space, nor across time. Across space, different languages tend to exhibit different polysemous associations for corresponding terms (Eger et al., 2015; Kulkarni et al., 2015b). Across time, several wellknown examples of meaning change in English have been documented. For example, the word gay’s meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20th century to its present meaning of homosexual (Kulkarni et al., 2015a). Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple. In our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art"
P16-2009,P14-1096,0,0.0453411,"analysis. On the one hand, coarse-grained trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresp"
P16-2009,E14-1049,0,0.0216492,"s wj (t − n) and p ≥ 1 is the order of the model. The set N (wi ) ⊆ V denotes a set of ‘neighbors’ of word wi .2 This model says that the meaning of a word wi at some time t is determined by reference to the meanings of its ‘neighbors’ in previous time periods, and that the underlying functional relationship is linear. We remark that the model described by Eq. (2) is a time-series model, and, in particular, a vector-autoregressive (VAR) model with special 1 An alternative to our second-order embeddings is to project vectors from different time periods in a common space (Mikolov et al., 2013a; Faruqui and Dyer, 2014), which requires to find corresponding terms across time. Further, one could also consider a ‘core’ vocabulary of semantically stable words, e.g., in the spirit of Swadesh (1952), instead of using all vocabulary words as reference. 2 We also constrain the vectors wi (t), for all wi ∈ V , to contain non-zero entries only for words in N (wi ). 3 Graph models Let V = {w1 , . . . , w|V |} be the common vocabulary (intersection) of all words in all time periods 2 53 We lemmatize and POS tag the data and likewise only consider nouns, verbs and adjectives, making the same frequency constraints as in"
P16-2009,D14-1113,0,0.0323176,"Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 2015). 3.1 A linear model of semantic change We p"
P16-2009,P12-1092,0,0.0608619,"the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with similar meanings/roles in two time periods but potentially different lexical forms) (Zhang et al., 201"
P16-2009,P11-2053,0,0.0226989,"aches to meaning change analysis. On the one hand, coarse-grained trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that inte"
P16-2009,P15-1063,0,0.0417373,"d trend analyses compare the semantics of a word in one time period with the meaning of the word in the preceding time period (Jatowt and Duh, 2014; Kulkarni et al., 2015a). Such coarse-grained models, by themselves, do not specify in which respects a word has changed (e.g., semantic broadening or narrowing), but just aim at capturing whether meaning change has occurred. In contrast, more fine-grained analyses typically senselabel word occurrences in corpora and then investigate changes in the corresponding meaning distributions (Rohrdantz et al., 2011; Mitra et al., 2014; Plitz et al., 2015; Zhang et al., 2015). Sense-labeling may be achieved by clustering of the context vectors of words (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014) or by applying LDA-based techniques where word contexts take the roles of documents and word senses take the roles of topics (Rohrdantz et al., 2011; Lau et al., 2012). Finally, there are studies that test particular meaning change hypotheses such as whether similar words tend to diverge in meaning over time (according to the ‘law of differentiation’) (Xu and Kemp, 2015) and papers that intend to detect corresponding terms across time (words with simi"
pustylnikov-etal-2008-unified,nivre-etal-2006-talbanken05,0,\N,Missing
pustylnikov-etal-2008-unified,J93-2004,0,\N,Missing
pustylnikov-etal-2008-unified,bosco-etal-2000-building,0,\N,Missing
pustylnikov-etal-2008-unified,W05-1714,0,\N,Missing
pustylnikov-etal-2008-unified,boguslavsky-etal-2002-development,0,\N,Missing
pustylnikov-etal-2008-unified,W06-2920,0,\N,Missing
pustylnikov-etal-2008-unified,W05-0302,0,\N,Missing
pustylnikov-etal-2008-unified,dzeroski-etal-2006-towards,0,\N,Missing
rehm-etal-2008-towards,C94-2174,0,\N,Missing
rehm-etal-2008-towards,P97-1005,0,\N,Missing
S15-1014,P05-1074,0,0.143843,"ages, but this would have required 220 − 1 > 1 million comparisons. 5 Conclusion We have encoded lexical semantic spaces of different languages by means of the same pivot language in order to make the languages comparable. To this end, we introduced association networks in which links between words in the reference language depend on translations from the respective source language, weighted by probability of translation. Our methodology is closely related to analogous approaches in the paraphrasing community which interlink paraphrases by means of their translations in other languages (e.g., Bannard and Callison-Burch (2005), Kok and Brockett (2010)), but our application scenario is different and we also describe a principled manner to generate weighted links between lexical units from multilingual data. Using random walks to represent similarities among words in the association networks, we finally derived similarity values for pairs of languages. This allowed us to perform several cluster analyses to group the 20 source languages. Interestingly, in our data sample, semantic language classification appears to be almost perfectly correlated with genealogical relationships between languages. To the best of our kno"
S15-1014,P11-1041,0,0.0182122,"genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. In order to address this question, we provide a translation-based model of lexical semantic spaces. Our approach is to genera"
S15-1014,N12-1044,0,0.0232595,"nowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. In order to address this question, we provide a translation-based model of lexical semantic spaces. Our approach is to generate association networks in which"
S15-1014,D14-1112,0,0.0129445,"ply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the geMAN vir PERSON HUMAN MAN homo HUSBAND WARRIOR PERSON HUMAN heros mensch HUSBAND HERO GUY DEMIGOD"
S15-1014,S12-1015,1,0.904617,"Missing"
S15-1014,D09-1124,0,0.183374,"4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013))."
S15-1014,P14-1006,0,0.0406309,"rube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approa"
S15-1014,O97-1002,0,0.143652,"arget. The paper is structured as follows. Section 2 outlines related work. Section 3 presents our formal model and Section 4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic net128 work in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words"
S15-1014,2005.mtsummit-papers.11,0,0.0404005,"es in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the geMAN vir PERSON HUMAN MAN homo HUSBAND WARRIOR PERSON HUMAN heros mensch HUSBAND HERO GUY DEMIGOD MALE (a) English-Latin mann (b) English-German Figure 1: Excerpts of bilingual dictionaries as bipartite graphs with links between words if and only if one is a translation of the other. Data from www.latin-dictionary.net and dict.leo.org."
S15-1014,N10-1017,0,0.0753569,"ork generation, we first define the vector representation of node v k in graph Gk = (Vk , Wk ) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from v k and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk (α, β)](α,β)∈Vk ×Vk . Note that the higher Wk (α, β), the higher the likelihood that the surfer takes the transition from α to β. More precisely, we let the meaning [[v k ]] of node v k in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), k k (k) + (1 − d)v0k , vN +1 = dvN A k , for N ≥ 0, is a 1 × |Rvoc |vector, where each vN (k) A is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector v0k , which is a vector of zeros except for index 3 More correctly, one could define Pk [α|z] = f1z , whenever α is a translation of z, and Pk [α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4 This reasoning ignores cases of homonymy, which weaken the semant"
S15-1014,J03-1002,0,0.00404703,"ontrasting such networks may then allow for clustering languages due to shared lexical semantic associations. As mentioned above, we generalize the model outlined so far to the situation of probabilistic translation relationships derived from corpus data, rather than from bilingual dictionaries. Working on corpus data has both advantages and disadvantages compared to using human compiled and edited dictionaries. On the one hand, • the translation relations induced from corpus data are noisy since their estimation is partially inaccurate due to limitations of alignment toolkits such as GIZA++ (Och and Ney, 2003) as employed by us. Implications of this inaccuracy are outlined below. • By using unannotated corpora, we cannot straightforwardly distinguish between cases of polysemy and homonymy. The problem is that homonymy should (ideally) not contribute to generating lexical semantic association networks as considered here. However, homonymy is apparently a rather rare phenomenon, while polysemy, which we expect to underlie the structure of our networks, is abundant (cf. L¨obner (2002)). On the other hand, • classical dictionaries can be very heterogeneous in their scope and denomination of translation"
S15-1014,D08-1109,0,0.0329044,"the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we i"
vor-der-bruck-etal-2014-collex,P06-4018,0,\N,Missing
vor-der-bruck-etal-2014-collex,P13-2048,0,\N,Missing
vor-der-bruck-etal-2014-collex,kunze-lemnitzer-2002-germanet,0,\N,Missing
vor-der-bruck-etal-2014-collex,ide-suderman-2004-american,0,\N,Missing
vor-der-bruck-etal-2014-collex,W11-3302,0,\N,Missing
W06-1710,J03-3001,0,0.0397634,"enres and therefore may signal one of those genres when being observed. Consequently, corpus analysis requires, amongst others, a comparison of occurrences in a given text with typical occurrences in other texts of the same genre (Stubbs, 2001, p. 120). This raises the question how to judge the membership of texts, in which occurrences of linguistic items are observed, to the genres involved. Evidently, because of the size of the corpora involved, this question is only adequately answered by reference to the area of automatic classification. This holds all the more for web corpus linguistics (Kilgarriff and Grefenstette, 2003; Baroni and Bernardini, 2006) where large corpora of web pages, whose membership in webgenres is presently unknown, have to be analyzed. Consequently, web corpus linguistics faces two related task: 67 • We explore a further resource of reliably tagging web genres and registers, respectively, in the form of document structure. they solely map web units onto feature vectors by disregarding their structure. This includes linkage beyond pairwise linking as well as document internal structures according to the Document Object Model (DOM). A central pitfall of this approach is that it disregards th"
W06-2801,C94-2174,0,\N,Missing
W07-0210,boguslavsky-etal-2002-development,0,0.0639982,"Missing"
W07-0210,bosco-etal-2000-building,0,0.026149,"Missing"
W07-0210,mengel-lezius-2000-xml,0,0.0221939,"The treebanks We analyze seven treebanks each from a different language. Their features are summarized in Table 1. A comprehensive description of these and related banks is given by (Kakkonen, 2005). As explained by Kakkonen, one generally faces the problem of the heterogeneity not only of the annotation schemes, but also of the serialization formats used by them. Thus, we unified the various formats in order to get a single interface to the analysis of syntactic dependency networks derived thereof. Although there exists a representation format for syntactic annotations (i.e. TIGER-XML — cf. (Mengel and Lezius, 2000)) we decided to use the Graph eXchange Language (GXL) in order to solve the heterogeneity problem. The GXL has been proposed as a uniform format for data interchange (Holt et al., 2006). It allows representing attributed, directed, undirected, mixed, ordered, hierarchical graphs as well as hypergraphs. Its application-dependent attribution model concerns vertices, edges and graphs. Because of its expressiveness it was utilized in modeling constituency structures (Pustylnikov, 2006) as well as nonlinear document structures (Mehler, 2007b). We utilize it to map syntactic dependency structures. O"
W07-0210,nivre-etal-2006-talbanken05,0,0.0235624,"Missing"
W07-0210,W06-2801,1,0.936093,"cal properties of networks as different as technical, biological and social networks has grown tremendously. See (Barab´asi and Albert, 2002; Dorogovtsev and Mendes, 2002; Newman, 2003) for a review. Among them many kinds of linguistic networks have been studied: e.g., free word association networks (Steyvers and Tenenbaum, 2005), syllable networks (Soares et al., 2005), thesaurus networks (Sigman 65 TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 65–72, c Rochester, April 2007 2007 Association for Computational Linguistics and Cecchi, 2002), and document networks (Mehler, 2006). See (Mehler, 2007a) for a review of linguistic network studies. Here we focus on the so called global syntactic dependency networks (GSDN) (Ferrer i Cancho et al., 2004; Ferrer i Cancho, 2005). A GSDN is induced from a dependency treebank in two steps: 1. The vertices of the network are obtained from the word forms appearing as nuclei in the input treebank and from punctuation marks as far as they have been annotated and mapped onto dependency trees. The notion of a nucleus is adapted from Lucien Tesni`ere: a nucleus is a node of a dependency tree. Note that multipart nuclei may also occur."
W07-1523,P05-2010,0,0.0150479,"ties they experienced. The subjects were asked to read three texts – a wikipedia entry (137 words), a newspaper article (233 words), and an interview (306 words). They were then given a list of all nouns occurring in the articles (almost all chainers exclusively consider nouns as chaining candidates), which they had to rate with respect to their ’importance’ in understanding the text. On this basis they were asked to determine the semantic relations of every possible chaining candidate pair, thus chain the nouns and annotate the three texts. Just like previously reported case studies (Beigman Klebanov, 2005; Morris and Hirst, 2004; Morris and Hirst, 2005) aiming at the annotation of lexical chains, we found that the inter-annotator agreement was in general relatively low. Only the annotation of very prominent items in the three texts, which accounted for approximately one fifth of the chaining candidates, resulted in a satisfying agreement (that is: the majority of the subjects produced an identical or very similar annotation). However, all subjects complained about the task. They found it rather diffi143 cult to construct linearized or quasi-linearized structures, in short, chains. Instead, mos"
W07-1523,J91-1002,0,0.027243,"cedentIDRefs=&quot;de8 de10&quot; phorIDRef=&quot;de12&quot;/> &lt;/chs:semRel> &lt;/chs:chs> types that specify the subtype of the relation, e. g. ident or hypernym as secondary types of cospecLink or meronym or setMember as secondary types of bridgingLink. An example annotation of an indirect anaphoric relation (element bridgingLink, line 30) between the discourse entities de12 (lines 18 to 21) and de8 (lines 3 to 5) and de10 (lines 9 to 11) can be seen in Listing 1. 2.2 Lexical Chaining Motivation and Background Based on the concept of lexical cohesion (Halliday and Hasan, 1976), computational linguists (inter alia Morris and Hirst (1991)) developed a method to compute a partial text representation: lexical chains. These span over passages or even the complete text linking lexical items. The exemplary annotation in Figure 1 illustrates that lexical chaining is achieved by the selection of vocabulary and significantly accounts for the cohesive structure of a text passage. Items in a lexical chain are connected via semantic relations. Accordingly, lexical chains are computed on the basis of a lexical semantic resource such as WordNet (Fellbaum, 1998). Figure 1 also depicts 142 Figure 1: Chaining Example (adapted from Halliday et"
W07-1523,W04-2607,0,0.0424992,"enced. The subjects were asked to read three texts – a wikipedia entry (137 words), a newspaper article (233 words), and an interview (306 words). They were then given a list of all nouns occurring in the articles (almost all chainers exclusively consider nouns as chaining candidates), which they had to rate with respect to their ’importance’ in understanding the text. On this basis they were asked to determine the semantic relations of every possible chaining candidate pair, thus chain the nouns and annotate the three texts. Just like previously reported case studies (Beigman Klebanov, 2005; Morris and Hirst, 2004; Morris and Hirst, 2005) aiming at the annotation of lexical chains, we found that the inter-annotator agreement was in general relatively low. Only the annotation of very prominent items in the three texts, which accounted for approximately one fifth of the chaining candidates, resulted in a satisfying agreement (that is: the majority of the subjects produced an identical or very similar annotation). However, all subjects complained about the task. They found it rather diffi143 cult to construct linearized or quasi-linearized structures, in short, chains. Instead, most of the subjects built"
W07-1523,goecke-witt-2006-exploiting,1,0.846266,"Missing"
W07-1523,holler-etal-2004-exploiting,0,0.115977,"ied (cf. Müller and Strube (2003)). Second, the relation(s) between markables and their respective types and subtypes have to be defined. The markables form a basis for the annotation process and therefore have to be annotated in advance. Normally, for a domain under investigation, elements are denoted as being markables either via a specific element or via the use of a universal attribute. In our system, discourse entities are detected automatically on the basis of POS and parsing information. The annotation scheme for annotating anaphoric relations is an extension of the scheme presented by Holler et al. (2004) that has been developed for annotations in the context of text-to-hypertext conversion in the project B1 HyTex. We adopt the distinction between coreference and cospecification but we extend the annotation scheme for an explicit distinction between cospecification (direct anaphora) and bridging (associative or indirect anaphora). Thus, we add the primary relation type bridgingLink (denoting bridging) to the already existing one (cospecLink). Each primary relation type includes different secondary relation Listing 1: The annotation format for anaphoric relations. Shortened and manually revised"
W07-1523,ma-etal-2002-models,0,0.0665775,"work supporting the time-aligned handling of video and audio streams and, therefore, much effort has been spent on the design and development of tools, unimodal annotation has often been fulfilled by using ordinary XML editors which can be error-prone. Nevertheless, specialized annotation frameworks are available as well, e. g. MMAX can be used for multi-level annotation projects (cf. Müller and Strube (2001; 2003)). However, as annotation projects grow in size and complexity (often multiple annotation layers are generated), collaborative annotation and the use of annotation tools is vital. • Ma et al. (2002), for example, describe collaborative annotation in the context of the AGTK. But since most of the aforementioned applications have to be installed locally on a PC, working on a corpus and managing annotations externally can be difficult. • Another problem worth to be mentioned is data management. Having several annotators working on one text, unification and comparison of the markup produced is quite difficult. • Furthermore, annotation tools help to increase both the quality and quantity of the annotation process. Recent web technologies allow the design of webbased applications that resembl"
W07-1523,W01-1612,0,0.020198,"atic Coder). Besides, other tools such as ELAN2 or Anvil3 are available as well, as are tool kits such as the Annotation Graph Toolkit (AGTK)4 or the NITE XML Toolkit.5 While multimodal annotation demands a framework supporting the time-aligned handling of video and audio streams and, therefore, much effort has been spent on the design and development of tools, unimodal annotation has often been fulfilled by using ordinary XML editors which can be error-prone. Nevertheless, specialized annotation frameworks are available as well, e. g. MMAX can be used for multi-level annotation projects (cf. Müller and Strube (2001; 2003)). However, as annotation projects grow in size and complexity (often multiple annotation layers are generated), collaborative annotation and the use of annotation tools is vital. • Ma et al. (2002), for example, describe collaborative annotation in the context of the AGTK. But since most of the aforementioned applications have to be installed locally on a PC, working on a corpus and managing annotations externally can be difficult. • Another problem worth to be mentioned is data management. Having several annotators working on one text, unification and comparison of the markup produced"
W07-1523,W03-2117,0,0.0173735,"e Annotation Scheme for Anaphoric Relations Several annotation schemes for annotating anaphoric relations have been developed in the last years, e. g. the UCREL anaphora annotation scheme (Fligelstone, 1992; Garside et al., 1997), the SGML-based MUC annotation scheme (Hirschmann, 1997), and the MATE/GNOME Scheme (Poesio, 2004), amongst others. In order to annotate discourse relations – either anaphoric relations or lexical chains (cf. Section 2.2) – two types of information have to be specified. First, the markables, i. e. the elements that can be part of a relation, have to be specified (cf. Müller and Strube (2003)). Second, the relation(s) between markables and their respective types and subtypes have to be defined. The markables form a basis for the annotation process and therefore have to be annotated in advance. Normally, for a domain under investigation, elements are denoted as being markables either via a specific element or via the use of a universal attribute. In our system, discourse entities are detected automatically on the basis of POS and parsing information. The annotation scheme for annotating anaphoric relations is an extension of the scheme presented by Holler et al. (2004) that has bee"
W07-1523,poesio-etal-2002-acquiring,0,0.0140918,"underlying lemmata and the like). In order to evaluate different resource settings, decision 141 trees with varying sets of feature vectors are used for the process of anaphora resolution. Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e. g. rewriting rule sets. Both, training and evaluation as well as empirically based analysis of anaphora need an annotated reference corpus (Poesio et al., 2002). Scheme and annotation process are described in the following section. The Annotation Scheme for Anaphoric Relations Several annotation schemes for annotating anaphoric relations have been developed in the last years, e. g. the UCREL anaphora annotation scheme (Fligelstone, 1992; Garside et al., 1997), the SGML-based MUC annotation scheme (Hirschmann, 1997), and the MATE/GNOME Scheme (Poesio, 2004), amongst others. In order to annotate discourse relations – either anaphoric relations or lexical chains (cf. Section 2.2) – two types of information have to be specified. First, the markables, i."
W07-1523,P03-1022,0,0.0113227,"(1976)) is annotated as an XML element which holds a variety of attribute information. Each XML element is reinterpreted as a feature vector; pairs of discourse entities between which an anaphoric relation holds form a single feature vector with additional information relevant for anaphora resolution (e. g. distance information, identity of grammatical form, semantic relatedness of underlying lemmata and the like). In order to evaluate different resource settings, decision 141 trees with varying sets of feature vectors are used for the process of anaphora resolution. Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e. g. rewriting rule sets. Both, training and evaluation as well as empirically based analysis of anaphora need an annotated reference corpus (Poesio et al., 2002). Scheme and annotation process are described in the following section. The Annotation Scheme for Anaphoric Relations Several annotation schemes for annotating anaphoric relations have been developed in the last years, e"
W07-1523,W06-2715,1,0.820982,"erface is necessary due to different reasons: The resources used have often been developed independently from each other and a cascaded application of one resource to the output of another resource is not always possible. Furthermore, the output of different resources often cannot be encoded in a single structure without driving into incompatibilites (i. e. XML overlap). Therefore an architecture was developed which allows for the combination of the output structures of several linguistic resources into a single XML annotated document and which is described in detail in Witt et al. (2005) and Stührenberg et al. (2006) . 2.1 Anaphoric Relations Motivation and Background Resolving anaphoric relations needs a variety of different information (e. g. POS, distance information, grammatical function, semantic knowledge, see, for example, Mitkov (2002) for an overview). Several resources are applied to a corpus of 47 texts and the output structures are combined into a single XML document using the architecture mentioned above. In order not only to integrate but also evaluate resources for a given linguistic task formally in terms of precision and recall, it should be possible to either switch on or switch off a gi"
W07-1523,P04-1017,0,0.0141699,"r referent (cf. Karttunen (1976)) is annotated as an XML element which holds a variety of attribute information. Each XML element is reinterpreted as a feature vector; pairs of discourse entities between which an anaphoric relation holds form a single feature vector with additional information relevant for anaphora resolution (e. g. distance information, identity of grammatical form, semantic relatedness of underlying lemmata and the like). In order to evaluate different resource settings, decision 141 trees with varying sets of feature vectors are used for the process of anaphora resolution. Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e. g. rewriting rule sets. Both, training and evaluation as well as empirically based analysis of anaphora need an annotated reference corpus (Poesio et al., 2002). Scheme and annotation process are described in the following section. The Annotation Scheme for Anaphoric Relations Several annotation schemes for annotating anaphoric relations have been de"
W07-1523,W04-2327,0,\N,Missing
W07-1523,M98-1029,0,\N,Missing
W07-1523,C69-7001,0,\N,Missing
W07-1523,C69-6902,0,\N,Missing
W15-3716,P11-1089,0,0.0594503,"Missing"
W15-3716,W07-0905,0,0.0135909,"25 898 4 821 9 594 113 87 75 44 32 103 87 109 14 11 Table 2: Distribution of the lexicon entries over the different parts of speech. (x, y) pairs as indicated. To transduce/lemmatize a new input form, rules (and their exceptions) are ordered, and the first condition that is satisfied fires the corresponding rule. inform each other. Lee et al. (2011) show that tagging and dependency parsing may mutually inform each other in such a setup, too. Concerning lexical resources for Latin, to our knowledge, there are concurrently three freely available resources for Latin: Perseus (Smith et al., 2000; Bamman and Crane, 2007), Proiel (Haug and Jøhndal, 2008), and the Index Thomisticus (IT) (Busa, 1980; McGilivray et al., 2009). Perseus and Proiel cover the more classical Latin era, while IT focuses on the writings of Thomas Aquinas. All resources indicate lemma and various part-of-speech information for its tokens. IT in addition provides dependency information. Concerning size, Perseus is the smallest resource with roughly 3 500 sentences, and Proiel and IT each contain about 13 000–14 000 Latin sentences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined"
W15-3716,D12-1133,0,0.0241411,"erable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibit high performance for richly Lemmatization On our corpus, we learn a character-level string transducer as a component model of our tagger. This lemmatizer is trained on pairs of strings (x, y) where x is a full form (e.g., amavisse ‘have loved’) and y its corresponding lemma (e.g., amo ‘love’). Learning a statistical lemmatizer has the advantage that it can cope with OOV words and may adapt to the distribution of the corpus. Our lemmatization module is LemmaGen (Jurˇsiˇc et al., 201"
W15-3716,N15-1055,0,0.0650386,"Missing"
W15-3716,P13-1068,0,0.037986,"Missing"
W15-3716,D13-1032,0,0.0939245,"Missing"
W15-3716,A00-1031,0,0.575144,"ion provides dependency information. Concerning size, Perseus is the smallest resource with roughly 3 500 sentences, and Proiel and IT each contain about 13 000–14 000 Latin sentences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined are languageindependent and general-purpose taggers. The TreeTagger (Schmid, 1994) implements a tagger based on decision trees. Despite its simple architecture, it seems to enjoy considerable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibi"
W15-3716,W02-1001,0,0.250134,"e, and the TreeTagger for the category ‘PoS’ (similar curves for the other tagging subtasks). Apparently, the more recent tagger generation generalizes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we employ two lemm"
W15-3716,W96-0213,0,0.50231,"taggers Lapos, Mate, and the TreeTagger for the category ‘PoS’ (similar curves for the other tagging subtasks). Apparently, the more recent tagger generation generalizes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we"
W15-3716,P09-1055,0,0.0652057,"Missing"
W15-3716,N03-1033,0,0.124082,"izes substantially better than the older approaches, exhibiting much higher accuracies especially at small training set sizes. inflected languages, where there may be considerable dependence between morphology and syntax, as well as for more configurational languages like English. The OpenNLPTagger is an official Apache project and provides three different tagging methods: maximum entropy, perceptron and perceptron sequence (cf. (Ratnaparkhi, 1996; Collins, 2002)) for maximum/perceptron based entropy tagging). We evaluated the maximum entropy and the perceptron approach.9 The Stanford tagger (Toutanova et al., 2003) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features. We use default parametrizations for all taggers10 and trained all taggers on a random sample of our data of about 14 000 sentences and test them on the remainder of about 1 500 sentences. 6 6.2 Lemma accuracy is indicated in Table 4. As we mentioned, we employ two lemmatization strategies based on the taggers’ outputs: either the lemma is retrieved from the lexicon given the predicted part-of-speech and the morphological"
W15-3716,W11-0328,0,0.150195,"ntences. 4 5 Part-of-speech taggers Here, we briefly sketch the taggers we survey in Section 6. All taggers outlined are languageindependent and general-purpose taggers. The TreeTagger (Schmid, 1994) implements a tagger based on decision trees. Despite its simple architecture, it seems to enjoy considerable popularity up until recently. Concurrently, two freely available TreeTagger taggers for Latin are available.8 TnT (Brants, 2000) implements a trigram Hidden Markov tagger with a module for handling unknown words. It has been shown to perform similarly well as maximum entropy models. Lapos (Tsuruoka et al., 2011) is a ‘history based’ tagging model (this model class subsumes maximum entropy Markov model) incorporating a lookahead mechanism into its decision-making process. It has been reported to be competitive with globally optimized models such as CRFs and structured perceptrons. Mate (Bohnet and Nivre, 2012) implements a transition based system for joint part-of-speech tagging and dependency parsing reported to exhibit high performance for richly Lemmatization On our corpus, we learn a character-level string transducer as a component model of our tagger. This lemmatizer is trained on pairs of string"
W16-3212,W13-4032,0,0.0325579,"Missing"
W16-3212,W12-1610,0,0.07175,"Missing"
W16-3212,D13-1032,0,0.0379827,"Missing"
W16-3212,S10-1071,0,0.0199046,"Missing"
W16-3212,L16-1239,1,\N,Missing
Y12-1059,W10-1001,0,0.0522113,"ort Vector Machines (SVM) outperform traditional readability measures. Pitler and Nenkova (2008) also used a unigram language model and found that this feature is a strong predictor of readability. baseline system that uses three traditional readability formulas proposed by Gunning (1952) , Dale and Chall (1948; 1995) and Senter and Smith (1967). These traditional formulas are widely used in many readability classiﬁcation tools. POS-based Features: Parts of Speech (POS)based grammatical features were shown to be useful in readability classiﬁcation (Pitler and Nenkova, 2008; Feng et al., 2009; Aluisio et al., 2010; Feng et al., 2010). In the experiment of (Feng et al., 2010), these features outperform language-model-based features. The government agency National Curriculum and Textbook Board, Bangladesh1 makes available textbooks that are used in public schools in Bangladesh. The textbooks cover many different subjects, including Bangla Literature, Social Science, General Science and Religious Studies. These textbooks are for students from grade one to grade ten. All of the textbooks are in Portable Document Format (PDF). Some of them are made by scanning textbooks and some of them are converted from t"
Y12-1059,J08-1001,0,0.0662583,"CRBLPConverter3 is used to convert these non-standard Bangla texts to Unicode. To cope with the font of the text, the CRBLPConverter required some slight modiﬁcations. Text books not only contain descriptive texts but also contain questions, poems, religious hymns, texts from other languages (e.g., Arabic, Pali) and transcription of Arabic texts (e.g., Surah). Manual work was involved to clean these non-descriptive texts and extract each chapter as a document. Class two contains only one Syntax-based Features: Text readability is affected by syntactic constructions (Pitler and Nenkova, 2008; Barzilay and Lapata, 2008; Heilman et al., 2007; Heilman et al., 2008). In this line of research, Barzilay and Lapata (2008) show, for example, that multiple noun phrases in a single sentence require the reader to remember more items. Semantic-based Features: On the semantic level, a paragraph that refers to many entities burdens the reader since he has to keep track of these entities, their semantic representations and how these entities are related. Texts that refer to many entities are extremely difﬁcult to understand for people with intellectual disabilities (Feng et al., 2009). Noemie and Huenerfauth (2009) show"
Y12-1059,N04-1025,0,0.215223,"Missing"
Y12-1059,E09-1027,0,0.515525,"ce that uses the average number of characters in a word and the average number of words in a sentence. Many of the other readability formulas are summarized in (Dubay, 2004). English has a long history of readability research, but there is very little previous research in Bangla text readability. Das and Roychudhury (2004; 2006) show that readability formulas proposed by (Kincaid et al., 1975) and (Gunning, 1952) work well for Bangla text. The readability formulas were tested semiautomatically on seven documents, mostly novels. Obviously this data set is small. Petersen & Ostendorf (2009) and Feng et al. (2009) show that these traditional methods have signiﬁcant drawbacks. Longer sentences are not always syntactically complex and the syllable number of a single word does not correlate with its difﬁculty. With recent advancements of NLP tools, a new class of text features is now available. Language Model Based Features: CollinsThompson and Callan (2004), Schwarm and Ostendorf (2005), Alusio et al. (2010), Kate et al. (2010) and Eickhoff et al. (2011) use statistical language models to classify texts for their readability. They show that trigrams are more informative than bigram and unigram models. Co"
Y12-1059,C10-2032,0,0.388848,"VM) outperform traditional readability measures. Pitler and Nenkova (2008) also used a unigram language model and found that this feature is a strong predictor of readability. baseline system that uses three traditional readability formulas proposed by Gunning (1952) , Dale and Chall (1948; 1995) and Senter and Smith (1967). These traditional formulas are widely used in many readability classiﬁcation tools. POS-based Features: Parts of Speech (POS)based grammatical features were shown to be useful in readability classiﬁcation (Pitler and Nenkova, 2008; Feng et al., 2009; Aluisio et al., 2010; Feng et al., 2010). In the experiment of (Feng et al., 2010), these features outperform language-model-based features. The government agency National Curriculum and Textbook Board, Bangladesh1 makes available textbooks that are used in public schools in Bangladesh. The textbooks cover many different subjects, including Bangla Literature, Social Science, General Science and Religious Studies. These textbooks are for students from grade one to grade ten. All of the textbooks are in Portable Document Format (PDF). Some of them are made by scanning textbooks and some of them are converted from typed text. There is"
Y12-1059,P02-1026,0,0.207617,"Missing"
Y12-1059,C10-1062,0,0.0765792,"k well for Bangla text. The readability formulas were tested semiautomatically on seven documents, mostly novels. Obviously this data set is small. Petersen & Ostendorf (2009) and Feng et al. (2009) show that these traditional methods have signiﬁcant drawbacks. Longer sentences are not always syntactically complex and the syllable number of a single word does not correlate with its difﬁculty. With recent advancements of NLP tools, a new class of text features is now available. Language Model Based Features: CollinsThompson and Callan (2004), Schwarm and Ostendorf (2005), Alusio et al. (2010), Kate et al. (2010) and Eickhoff et al. (2011) use statistical language models to classify texts for their readability. They show that trigrams are more informative than bigram and unigram models. Combining information from statistical language models with other features using Support Vector Machines (SVM) outperform traditional readability measures. Pitler and Nenkova (2008) also used a unigram language model and found that this feature is a strong predictor of readability. baseline system that uses three traditional readability formulas proposed by Gunning (1952) , Dale and Chall (1948; 1995) and Senter and Sm"
Y12-1059,D08-1020,0,0.282187,"word does not correlate with its difﬁculty. With recent advancements of NLP tools, a new class of text features is now available. Language Model Based Features: CollinsThompson and Callan (2004), Schwarm and Ostendorf (2005), Alusio et al. (2010), Kate et al. (2010) and Eickhoff et al. (2011) use statistical language models to classify texts for their readability. They show that trigrams are more informative than bigram and unigram models. Combining information from statistical language models with other features using Support Vector Machines (SVM) outperform traditional readability measures. Pitler and Nenkova (2008) also used a unigram language model and found that this feature is a strong predictor of readability. baseline system that uses three traditional readability formulas proposed by Gunning (1952) , Dale and Chall (1948; 1995) and Senter and Smith (1967). These traditional formulas are widely used in many readability classiﬁcation tools. POS-based Features: Parts of Speech (POS)based grammatical features were shown to be useful in readability classiﬁcation (Pitler and Nenkova, 2008; Feng et al., 2009; Aluisio et al., 2010; Feng et al., 2010). In the experiment of (Feng et al., 2010), these featur"
Y12-1059,P05-1065,0,0.0875233,"ed by (Kincaid et al., 1975) and (Gunning, 1952) work well for Bangla text. The readability formulas were tested semiautomatically on seven documents, mostly novels. Obviously this data set is small. Petersen & Ostendorf (2009) and Feng et al. (2009) show that these traditional methods have signiﬁcant drawbacks. Longer sentences are not always syntactically complex and the syllable number of a single word does not correlate with its difﬁculty. With recent advancements of NLP tools, a new class of text features is now available. Language Model Based Features: CollinsThompson and Callan (2004), Schwarm and Ostendorf (2005), Alusio et al. (2010), Kate et al. (2010) and Eickhoff et al. (2011) use statistical language models to classify texts for their readability. They show that trigrams are more informative than bigram and unigram models. Combining information from statistical language models with other features using Support Vector Machines (SVM) outperform traditional readability measures. Pitler and Nenkova (2008) also used a unigram language model and found that this feature is a strong predictor of readability. baseline system that uses three traditional readability formulas proposed by Gunning (1952) , Dal"
Y12-1059,N07-1058,0,\N,Missing
