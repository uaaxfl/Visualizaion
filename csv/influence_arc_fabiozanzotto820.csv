2020.emnlp-main.18,D10-1115,0,0.0394419,"ded universal syntactic representations in neural networks. 1 Introduction Universal sentence embeddings (Conneau et al., 2018), which are task-independent, distributed sentence representations, are redesigning the way linguistic models in natural language processing are defined. These embeddings are usually created from scratch over large corpora without human supervision (Cho et al., 2014; Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) or are crafted with compositional distributional semantics methods (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010). Traditional task-independent, symbolic, humandefined syntactic interpretations for sentences, which may be referred to as universal syntactic interpretations, are losing their centrality in language understanding systems due to the success of transformer-based neural networks (Vaswani et al., 2017) that have boosted performances on a wide variety of linguistic tasks (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019). There is evidence that universal sentence embeddings store bits of universal syntactic interpretations. Even if not explicitly designed for encod"
2020.emnlp-main.18,D18-2029,0,0.0527712,"Missing"
2020.emnlp-main.18,D14-1179,0,0.0566724,"Missing"
2020.emnlp-main.18,P02-1034,0,0.708847,"rks applied to binary trees. Initially, these RecNNs have 257 For a long time, structural kernel functions have been the way to exploit syntactic information in learning but these functions cannot be used within neural networks. Kernel machines (Cristianini and Shawe-Taylor, 2000) exploit these, generally recursive, structural kernel functions that define a similarity measure between two trees counting common substructures. Hence, these structural kernel functions are built over a clear, although hidden, space of substructures. Structural kernels have been defined for both constituency-based (Collins and Duffy, 2002; Moschitti, 2006) and dependencybased parse trees (Culotta and Sorensen, 2004). As underlying spaces are well defined, it is even possible to extract back substructures that are relevant in each decision (Pighin and Moschitti, 2010). However, these structural kernel functions are generally recursive algorithms that hide the real underlying space of features. Thus, structures are never represented as vectors in the target representation spaces as these spaces are generally huge. It is generally impossible then to use these clear spaces in learning with neural networks. In the field of structur"
2020.emnlp-main.18,D17-1070,0,0.0279077,"ed with two state-of-the-art transformerbased universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks. 1 Introduction Universal sentence embeddings (Conneau et al., 2018), which are task-independent, distributed sentence representations, are redesigning the way linguistic models in natural language processing are defined. These embeddings are usually created from scratch over large corpora without human supervision (Cho et al., 2014; Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) or are crafted with compositional distributional semantics methods (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010). Traditional task-independent, symbolic, humandefined syntactic interpretations for sentences, which may be referred to as universal syntactic interpretations, are losing their centrality in language understanding systems due to the success of transformer-based neural networks (Vaswani et al., 2017) that have boosted performances on a wide variety of linguistic tasks (Devlin et al"
2020.emnlp-main.18,P18-1198,0,0.149552,"of large-scale textual representation learners. In this paper, we propose KERMIT (Kernelinspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformerbased universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks. 1 Introduction Universal sentence embeddings (Conneau et al., 2018), which are task-independent, distributed sentence representations, are redesigning the way linguistic models in natural language processing are defined. These embeddings are usually created from scratch over large corpora without human supervision (Cho et al., 2014; Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) or are crafted with compositional distributional semantics methods (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010). Traditional task-independent, symbolic, humandefined syntactic interpretat"
2020.emnlp-main.18,D19-1415,0,0.0277699,"sualizing Neural Network Activation on Syntactic Trees The definitions of the KERMIT encoder make it possibile to devise KERMITviz , which offers prediction interpretability (Jacovi et al., 2018) in the context of textual classification. We propose a clear causal relation for explaining (Lipton, 2016) classification decisions where syntax is important by defining heat parse trees and calculating the relevance of single subtrees with layer-wise relevance propagation (LRP) (Bach et al., 2015). LRP has already been used in the context of explaining decisions in natural language processing tasks (Croce et al., 2019b,a). Heat parse trees (HPTs), similarly to “heat trees” in biology (Foster et al., 2017), are heatmaps over 259 parse trees (see the colored tree in Fig. 1). The underlying representation is an active tree t, that is a tree where each node t = (r, vr , [t1 , ..., tk ]) has an activation value vr ∈ R associated. HPTs are graphical visualizations of active trees t where colors and sizes of nodes r depend on their activation values vr . In this way, HPTs highlight parts of parse trees relevant in final decisions. To draw HPTs, we compute activation value vr of nodes r in active tree t by using L"
2020.emnlp-main.18,P04-1054,0,0.232305,"me, structural kernel functions have been the way to exploit syntactic information in learning but these functions cannot be used within neural networks. Kernel machines (Cristianini and Shawe-Taylor, 2000) exploit these, generally recursive, structural kernel functions that define a similarity measure between two trees counting common substructures. Hence, these structural kernel functions are built over a clear, although hidden, space of substructures. Structural kernels have been defined for both constituency-based (Collins and Duffy, 2002; Moschitti, 2006) and dependencybased parse trees (Culotta and Sorensen, 2004). As underlying spaces are well defined, it is even possible to extract back substructures that are relevant in each decision (Pighin and Moschitti, 2010). However, these structural kernel functions are generally recursive algorithms that hide the real underlying space of features. Thus, structures are never represented as vectors in the target representation spaces as these spaces are generally huge. It is generally impossible then to use these clear spaces in learning with neural networks. In the field of structural kernels, distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) have o"
2020.emnlp-main.18,P19-1285,0,0.0645886,"tuency parse trees for KERMIT have been obtained by using Stanford’s CoreNLP probabilistic context-free grammar parser (Manning et al., 2014). Thirdly, the following transformer sub-networks have been used: (1) BERTBASE , used in the uncased setting with the pre-trained English model; (2) BERTLARGE , used with the same settings of BERTBASE ; and, (3) XLNet base cased. All the models were implemented using Huggingface’s transformers library (Wolf et al., 2019). The input text for BERT and XLNet has been preprocessed and tokenized as specified in respectively in Devlin et al. (2018) Yang et al. (2019). Fourthly, as the experiments are text classification tasks, the decoder layer of our KERMIT+Tranformer architecture is a fully connected layer with the softmax activation function applied to the concatenation of the KERMIT output and the final [CLS] token representation of the selected transformer model. Finally, the optimizer used to train the whole architecture is AdamW (Loshchilov and Hutter, 2019) with the learning rate set to 3e−5 . In the completely universal setting, KERMIT is composed only by the first lightweight encoder layer (grey layer in Figure 1) (KERMITENC ). In this setting,"
2020.emnlp-main.18,N19-1419,0,0.03053,"at universal sentence embeddings store bits of universal syntactic interpretations. Even if not explicitly designed for encoding syntax, these embeddings implicitly capture syntactic relations among words with different strategies. Transformers (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Dai et al., 2019) seem to capture syntactic relations among words by “focusing the attention”. Yet, to be sure that syntax is encoded, many syntactic probes (Conneau et al., 2018) for neural networks have been designed to test for specific phenomena (Kovaleva et al., 2019; Jawahar et al., 2019; Hewitt and Manning, 2019; Ettinger, 2019; Goldberg, 2019) or for full syntactic trees (Hewitt and Manning, 2019; Mareˇcek and Rosa, 2019). Indeed, some syntax is correctly encoded in these universal sentence embeddings. However, universal sentence embeddings encode syntax in a way that is opaque and not so universal. Firstly, and perhaps surprisingly, task-adapted universal sentence embeddings encode syntax better than general universal sentence embeddings (Jawahar et al., 2019). Secondly, even if these embeddings contains syntactic information and may be “just another way in which traditional syntactic models are en"
2020.emnlp-main.18,W18-5408,0,0.028762,"λ8 is the decay factor applied to the sample subtree with 8 nodes. Given the properties of the vectors E(r) ∼ N (0, √1d I) and the properties of the shuffled circular convolution ⊗, it is possible to empirically demonstrate that Γ(ti )T Γ(ti ) ≈ 1 and Γ(ti )T Γ(tj ) ≈ 0 (Plate, 1995; Zanzotto and Dell’Arciprete, 2012). Hence, this property can be used to interpret the behavior of the decision in the neural network. 3.3 Visualizing Neural Network Activation on Syntactic Trees The definitions of the KERMIT encoder make it possibile to devise KERMITviz , which offers prediction interpretability (Jacovi et al., 2018) in the context of textual classification. We propose a clear causal relation for explaining (Lipton, 2016) classification decisions where syntax is important by defining heat parse trees and calculating the relevance of single subtrees with layer-wise relevance propagation (LRP) (Bach et al., 2015). LRP has already been used in the context of explaining decisions in natural language processing tasks (Croce et al., 2019b,a). Heat parse trees (HPTs), similarly to “heat trees” in biology (Foster et al., 2017), are heatmaps over 259 parse trees (see the colored tree in Fig. 1). The underlying rep"
2020.emnlp-main.18,P19-1356,0,0.0495286,"Missing"
2020.emnlp-main.18,D19-1445,0,0.0189382,"019; Yang et al., 2019). There is evidence that universal sentence embeddings store bits of universal syntactic interpretations. Even if not explicitly designed for encoding syntax, these embeddings implicitly capture syntactic relations among words with different strategies. Transformers (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Dai et al., 2019) seem to capture syntactic relations among words by “focusing the attention”. Yet, to be sure that syntax is encoded, many syntactic probes (Conneau et al., 2018) for neural networks have been designed to test for specific phenomena (Kovaleva et al., 2019; Jawahar et al., 2019; Hewitt and Manning, 2019; Ettinger, 2019; Goldberg, 2019) or for full syntactic trees (Hewitt and Manning, 2019; Mareˇcek and Rosa, 2019). Indeed, some syntax is correctly encoded in these universal sentence embeddings. However, universal sentence embeddings encode syntax in a way that is opaque and not so universal. Firstly, and perhaps surprisingly, task-adapted universal sentence embeddings encode syntax better than general universal sentence embeddings (Jawahar et al., 2019). Secondly, even if these embeddings contains syntactic information and may be “just another"
2020.emnlp-main.18,2020.tacl-1.50,0,0.0430355,"oduced (colored trees). coder for embedding syntax parse trees in universalsyntax-encoding vectors by explicitly embedding subtrees in the representation space. KERMITviz is a visualizer to inspect how syntax is used in taking final decisions in specific tasks. We showed that KERMIT can effectively embed different syntactic information and KERMITviz can explain KERMIT’s decisions. Furthermore, paired with universal sentence embeddings, KERMIT outperforms state-of-the-art models - BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) - in three different downstream tasks, albeit findings in Kuncoro et al. (2020), showing that traditional syntactic information is not represented in universal sentence embeddings. 2 been used to parse sentences and not to include preexisting syntax in a final task (Socher et al., 2011). Then, these RecNNs have been used to encode preexisting syntax in the specific task of Sentiment Analysis (Socher et al., 2012, 2013). With the rise of Long Short-Term Memories (LSTMs), Tai et al. (2015); Zhu et al. (2015) and Zhang et al. (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following th"
2020.emnlp-main.18,P14-5010,0,0.00299859,"our experiments, the specific configurations adopted in the completely universal and task-specific settings, the used computational architecture and the datasets. The general experimental settings are described hereafter. Firstly, the core of our method KERMIT encoder has been tested on a distributed representation space Rd with d = 4000 with the penalizing factor λ set to λ = 0.4 as this has been considered a common value in previous works (Moschitti, 2006). Secondly, constituency parse trees for KERMIT have been obtained by using Stanford’s CoreNLP probabilistic context-free grammar parser (Manning et al., 2014). Thirdly, the following transformer sub-networks have been used: (1) BERTBASE , used in the uncased setting with the pre-trained English model; (2) BERTLARGE , used with the same settings of BERTBASE ; and, (3) XLNet base cased. All the models were implemented using Huggingface’s transformers library (Wolf et al., 2019). The input text for BERT and XLNet has been preprocessed and tokenized as specified in respectively in Devlin et al. (2018) Yang et al. (2019). Fourthly, as the experiments are text classification tasks, the decoder layer of our KERMIT+Tranformer architecture is a fully connec"
2020.emnlp-main.18,P08-1028,0,0.0930401,"ectively embedding human-coded universal syntactic representations in neural networks. 1 Introduction Universal sentence embeddings (Conneau et al., 2018), which are task-independent, distributed sentence representations, are redesigning the way linguistic models in natural language processing are defined. These embeddings are usually created from scratch over large corpora without human supervision (Cho et al., 2014; Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) or are crafted with compositional distributional semantics methods (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010). Traditional task-independent, symbolic, humandefined syntactic interpretations for sentences, which may be referred to as universal syntactic interpretations, are losing their centrality in language understanding systems due to the success of transformer-based neural networks (Vaswani et al., 2017) that have boosted performances on a wide variety of linguistic tasks (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019). There is evidence that universal sentence embeddings store bits of universal syntactic interpretations. Even if not"
2020.emnlp-main.18,E06-1015,0,0.220855,"es. Initially, these RecNNs have 257 For a long time, structural kernel functions have been the way to exploit syntactic information in learning but these functions cannot be used within neural networks. Kernel machines (Cristianini and Shawe-Taylor, 2000) exploit these, generally recursive, structural kernel functions that define a similarity measure between two trees counting common substructures. Hence, these structural kernel functions are built over a clear, although hidden, space of substructures. Structural kernels have been defined for both constituency-based (Collins and Duffy, 2002; Moschitti, 2006) and dependencybased parse trees (Culotta and Sorensen, 2004). As underlying spaces are well defined, it is even possible to extract back substructures that are relevant in each decision (Pighin and Moschitti, 2010). However, these structural kernel functions are generally recursive algorithms that hide the real underlying space of features. Thus, structures are never represented as vectors in the target representation spaces as these spaces are generally huge. It is generally impossible then to use these clear spaces in learning with neural networks. In the field of structural kernels, distri"
2020.emnlp-main.18,E17-1002,0,0.0197898,"013). With the rise of Long Short-Term Memories (LSTMs), Tai et al. (2015); Zhu et al. (2015) and Zhang et al. (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following the structure of a binary tree instead of following an input sequence. In semantic relatedness and in sentiment classification, TreeLSTM has outperformed RecNN (Tai et al., 2015) by using pre-existing syntactic information. TreeLSTM has also been used to induce task-specific trees while learning a novel task (Choi et al., 2018). Moreover, Munkhdalai and Yu (2017) have specialized LSTM for binary and n-ry trees with their Neural Tree Indexers and Strubell et al. (2018) have encoded syntactic information by using multi-head attention within a transformer architecture. However, there is a major problem with the methods for embedding syntactic structures in neural networks, it is unclear which parts of the parse trees are represented, and how. Hence, the behavior of neural networks that use these embeddings is obscure. It is then difficult to understand what kind of syntactic knowledge is encoded in the different layers and how this syntactic knowledge is"
2020.emnlp-main.18,W10-2926,0,0.0359233,"Kernel machines (Cristianini and Shawe-Taylor, 2000) exploit these, generally recursive, structural kernel functions that define a similarity measure between two trees counting common substructures. Hence, these structural kernel functions are built over a clear, although hidden, space of substructures. Structural kernels have been defined for both constituency-based (Collins and Duffy, 2002; Moschitti, 2006) and dependencybased parse trees (Culotta and Sorensen, 2004). As underlying spaces are well defined, it is even possible to extract back substructures that are relevant in each decision (Pighin and Moschitti, 2010). However, these structural kernel functions are generally recursive algorithms that hide the real underlying space of features. Thus, structures are never represented as vectors in the target representation spaces as these spaces are generally huge. It is generally impossible then to use these clear spaces in learning with neural networks. In the field of structural kernels, distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) have opened an interesting possibility. To reduce the computational cost of tree kernels, these distributed tree kernels embed the huge space of substructures i"
2020.emnlp-main.18,D13-1170,0,0.0297695,"Missing"
2020.emnlp-main.18,D18-1548,0,0.022693,". (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following the structure of a binary tree instead of following an input sequence. In semantic relatedness and in sentiment classification, TreeLSTM has outperformed RecNN (Tai et al., 2015) by using pre-existing syntactic information. TreeLSTM has also been used to induce task-specific trees while learning a novel task (Choi et al., 2018). Moreover, Munkhdalai and Yu (2017) have specialized LSTM for binary and n-ry trees with their Neural Tree Indexers and Strubell et al. (2018) have encoded syntactic information by using multi-head attention within a transformer architecture. However, there is a major problem with the methods for embedding syntactic structures in neural networks, it is unclear which parts of the parse trees are represented, and how. Hence, the behavior of neural networks that use these embeddings is obscure. It is then difficult to understand what kind of syntactic knowledge is encoded in the different layers and how this syntactic knowledge is used. Some initial attempts to clarify which syntactic parts are encoded in embedding vectors exist. Zhang"
2020.emnlp-main.18,P15-1150,0,0.055385,"l sentence embeddings, KERMIT outperforms state-of-the-art models - BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) - in three different downstream tasks, albeit findings in Kuncoro et al. (2020), showing that traditional syntactic information is not represented in universal sentence embeddings. 2 been used to parse sentences and not to include preexisting syntax in a final task (Socher et al., 2011). Then, these RecNNs have been used to encode preexisting syntax in the specific task of Sentiment Analysis (Socher et al., 2012, 2013). With the rise of Long Short-Term Memories (LSTMs), Tai et al. (2015); Zhu et al. (2015) and Zhang et al. (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following the structure of a binary tree instead of following an input sequence. In semantic relatedness and in sentiment classification, TreeLSTM has outperformed RecNN (Tai et al., 2015) by using pre-existing syntactic information. TreeLSTM has also been used to induce task-specific trees while learning a novel task (Choi et al., 2018). Moreover, Munkhdalai and Yu (2017) have specialized LSTM for binary and n-ry trees w"
2020.emnlp-main.18,S18-1076,1,0.865294,"Missing"
2020.emnlp-main.18,P19-3007,0,0.0192222,"ERMIT-based models behave better with less training. In fact, KERMIT-based models learned in the 1-epoch setting, outperform models learned in the 5-epoch setting. Plots in Figure 2 report the best 1-epoch setting model in the plots of the 5-setting model. This can be linked to the fact that KERMIT with more parameters overfits on training. In fact, KERMITENC +BERTBASE outperforms the funnel and diamond KERMIT-based systems. KERMITENC has fewer parameters than KERMIT. and KERMIT . Finally, we explored the interpretative power of KERMITviz comparing it with the transformer visualizer BERTviz (Vig, 2019). We focused on two examples of Yelp Reviews where the coordinating conjunction but plays an important role (see Fig. 3): (1) “Unique food, great atmosphere, pricey but worth a trip for special occasions.”; (2) “The boba drink was terrible, but the shaved ice was good.”. The two sentences have 4 and 3 as ratings, respectively. In fact, the but in the first sentence introduces a coordinated sentence that does not 263 change the rating. On the contrary, the but in the second sentence introduces a coordinated sentence but the shaved ice was good that radically changes the polarity. In the case of"
2020.emnlp-main.18,D12-1110,0,0.0812249,"and KERMITviz can explain KERMIT’s decisions. Furthermore, paired with universal sentence embeddings, KERMIT outperforms state-of-the-art models - BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) - in three different downstream tasks, albeit findings in Kuncoro et al. (2020), showing that traditional syntactic information is not represented in universal sentence embeddings. 2 been used to parse sentences and not to include preexisting syntax in a final task (Socher et al., 2011). Then, these RecNNs have been used to encode preexisting syntax in the specific task of Sentiment Analysis (Socher et al., 2012, 2013). With the rise of Long Short-Term Memories (LSTMs), Tai et al. (2015); Zhu et al. (2015) and Zhang et al. (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following the structure of a binary tree instead of following an input sequence. In semantic relatedness and in sentiment classification, TreeLSTM has outperformed RecNN (Tai et al., 2015) by using pre-existing syntactic information. TreeLSTM has also been used to induce task-specific trees while learning a novel task (Choi et al., 2018). Moreove"
2020.emnlp-main.18,D19-1002,0,0.0224282,"st sentence introduces a coordinated sentence that does not 263 change the rating. On the contrary, the but in the second sentence introduces a coordinated sentence but the shaved ice was good that radically changes the polarity. In the case of BERTviz, this causal relationship is extremely difficult to grasp from the visual representation. In fact, BERTviz is a good visualization mechanism for seeing how models assign weights to different input elements (Bahdanau et al., 2015; Belinkov and Glass, 2019), but it is extremely obscure in explaining causal relations in classification predictions (Wiegreffe and Pinter, 2019). Instead, KERMITviz with its tree heat maps show exactly that the but and the related syntactic structure is irrelevant in the first sentence and extremely relevant in the second. Hence, our heat parse trees can be useful to draw the causal relation between the decision and the information used. 5 Conclusions Universal syntactic interpretations are valuable language interpretations, which have been developed in years of study. In this paper, we introduced KERMIT to show that these interpretations can be effectively used in combination with universal sentence embeddings produced from scratch."
2020.emnlp-main.18,C10-1142,1,0.822741,"Missing"
2020.emnlp-main.18,D18-1294,0,0.0242753,"2018) have encoded syntactic information by using multi-head attention within a transformer architecture. However, there is a major problem with the methods for embedding syntactic structures in neural networks, it is unclear which parts of the parse trees are represented, and how. Hence, the behavior of neural networks that use these embeddings is obscure. It is then difficult to understand what kind of syntactic knowledge is encoded in the different layers and how this syntactic knowledge is used. Some initial attempts to clarify which syntactic parts are encoded in embedding vectors exist. Zhang et al. (2018) have encoded parse trees by means of paths connecting the root of parse trees with words. Yet, these attempts are still far from completely representing parse trees. Background and Related Work Embedding symbolic syntactic or structured information within neural networks is a very active research field given the impression that using preexisting syntactic knowledge in neural networks can be beneficial for many tasks. Initial attempts have tried to recursively encode structures in distributed representations to use them inside neural networks (Pollack, 1990; Goller and Kuechler, 1996). More re"
2020.emnlp-main.18,N16-1035,0,0.0178441,"state-of-the-art models - BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) - in three different downstream tasks, albeit findings in Kuncoro et al. (2020), showing that traditional syntactic information is not represented in universal sentence embeddings. 2 been used to parse sentences and not to include preexisting syntax in a final task (Socher et al., 2011). Then, these RecNNs have been used to encode preexisting syntax in the specific task of Sentiment Analysis (Socher et al., 2012, 2013). With the rise of Long Short-Term Memories (LSTMs), Tai et al. (2015); Zhu et al. (2015) and Zhang et al. (2016) independently proposed TreeLSTM as an adapted version of LTSM that may use syntactic information. In TreeLSTM, the LSTM is applied following the structure of a binary tree instead of following an input sequence. In semantic relatedness and in sentiment classification, TreeLSTM has outperformed RecNN (Tai et al., 2015) by using pre-existing syntactic information. TreeLSTM has also been used to induce task-specific trees while learning a novel task (Choi et al., 2018). Moreover, Munkhdalai and Yu (2017) have specialized LSTM for binary and n-ry trees with their Neural Tree Indexers and Strubell"
basili-etal-2000-tuning,1995.iwpt-1.15,0,\N,Missing
basili-etal-2000-tuning,C92-2070,0,\N,Missing
basili-etal-2004-a2q,W02-0404,0,\N,Missing
basili-etal-2004-a2q,P02-1040,0,\N,Missing
basili-etal-2004-a2q,P98-1013,0,\N,Missing
basili-etal-2004-a2q,C98-1013,0,\N,Missing
basili-etal-2004-similarity,P95-1026,0,\N,Missing
C02-1129,2000.iwpt-1.7,0,\N,Missing
C10-1142,A00-2018,0,0.022523,"Missing"
C10-1142,D08-1094,0,0.22877,"Missing"
C10-1142,P07-1028,0,0.0113016,"onal meaning of the word sequence eats mice is governed by the verb eats. Following this model, the distributional vector (s) can be written as: (s) ≈ (h, s) (7) where w ~ is the distributional vector of the word w, Rw is the set of the vectors representing the direct −1 is selectional preferences of the word w, and Rw the set of the vectors representing the indirect selectional preferences of the word w. Given a set of −1 syntactic relations R, the set Rw and Rw contain respectively a selectional preference vector Rw (r) and Rw (r)−1 for each r ∈ R. Selectional preferences are computed as in Erk (2007). If x is the semantic head of sequence s, then the model can be written as: r (s) = (x, x ← − y) = ~x Ry (r) (8) Otherwise, if y is the semantic head: r (s) = (y, x ← − y) = ~y Rx−1 (r) (9) is in both cases realised using BAM or BMM. We will call these models: basic additive model with selectional preferences (BAM-SP) and basic multiplicative model with selectional preferences (BMM-SP). Both Mitchell and Lapata (2008) and Erk and Pad´o (2008) experimented with few empirically estimated parameters. Thus, the general additive CDS model has not been adequately explored. Estimating Additive Compo"
C10-1142,W09-0209,1,0.774592,"l., 1990). Moore-Penrose pseudoinverse (Penrose, 1955) is computed in the following way. Let the original matrix Q have n rows and m columns and be of rank r. The SVD decomposition of the original matrix Q is Q = U ΣV T where Σ is a square diagonal matrix of dimension r. Then, the pseudoinverse matrix that minimises the equation 18 is: Q+ = V Σ+ U T (20) where the diagonal matrix Σ+ is the r × r transposed matrix of Σ having as diagonal elements the reciprocals of the singular values δ11 , δ12 , ..., δ1r of Σ. Using SVD to compute the pseudo-inverse matrix allows for different approximations (Fallucchi and Zanzotto, 2009). The algorithm for computing the singular value decomposition is iterative (Golub and Kahan, 1965). Firstly derived dimensions have higher singular value. Then, dimension k is more informative than dimension k 0 > k. We can consider different values for k to obtain different SVD for the approximations Q+ k of the original matrix Q+ in equation 20), i.e.: 1266 + T Q+ k = Vn×k Σk×k Uk×m (21) where Q+ k is a matrix n by m obtained considering the first k singular values. 4 Building positive and negative examples As explained in the previous section, estimating CDS models, needs a set of triples"
C10-1142,P09-2017,1,0.85318,"Missing"
C10-1142,J03-4004,0,0.00938077,"model for compositional distributional semantics. 1 Given the successful application to words, distributional semantics has been extended to word sequences. This has happened in two ways: (1) via the reformulation of DH for specific word sequences (Lin and Pantel, 2001); and (2) via the definition of compositional distributional semantics (CDS) models (Mitchell and Lapata, 2008; Jones and Mewhort, 2007). These are two different ways of addressing the problem. Introduction Lexical distributional semantics has been largely used to model word meaning in many fields as computational linguistics (McCarthy and Carroll, 2003; Manning et al., 2008), linguistics (Harris, 1964), corpus linguistics (Firth, 1957), and cognitive research (Miller and Charles, 1991). The fundamental hypothesis is the distributional hypothesis (DH): “similar words share similar contexts” (Harris, 1964). Recently, this hypothesis has been operationally defined in many ways in the fields of Lin and Pantel (2001) propose the pattern distributional hypothesis that extends the distributional hypothesis for specific patterns, i.e. word sequences representing partial verb phrases. Distributional meaning for these patterns is derived directly by"
C10-1142,P08-1028,0,0.797891,"xtracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics. 1 Given the successful application to words, distributional semantics has been extended to word sequences. This has happened in two ways: (1) via the reformulation of DH for specific word sequences (Lin and Pantel, 2001); and (2) via the definition of compositional distributional semantics (CDS) models (Mitchell and Lapata, 2008; Jones and Mewhort, 2007). These are two different ways of addressing the problem. Introduction Lexical distributional semantics has been largely used to model word meaning in many fields as computational linguistics (McCarthy and Carroll, 2003; Manning et al., 2008), linguistics (Harris, 1964), corpus linguistics (Firth, 1957), and cognitive research (Miller and Charles, 1991). The fundamental hypothesis is the distributional hypothesis (DH): “similar words share similar contexts” (Harris, 1964). Recently, this hypothesis has been operationally defined in many ways in the fields of Lin and P"
C10-1142,J07-2002,0,0.0675455,"Missing"
C10-1142,W01-0513,0,0.0186172,"~xi , ~yi ). Examples are positive in the sense that ~zi = (xy) for an ideal CDS. There are no available sets to contain such triples, with the exception of the set used in Mitchell and Lapata (2008) which is designed only for testing purposes. It contains similar and dissimilar pairs of sequences (s1 ,s2 ) where each sequence is a verb-noun pair (vi ,ni ). From the positive part of this set, we can only derive quadruples where (v1 n1 ) ≈ (v2 n2 ) but we cannot derive the ideal resulting vector of the composition (vi ni ). Sets used to test multiword expression (MWE) detection models (e.g., (Schone and Jurafsky, 2001; Nicholson and Baldwin, 2008; Kim and Baldwin, 2008; Cook et al., 2008; Villavicencio, 2003; Korkontzelos and Manandhar, 2009)) are again not useful as containing only valid MWE that cannot be used to determine the set of training triples needed here. As a result, we need a novel idea to build sets of triples to train CDS models. We can leverage on knowledge stored in dictionaries. In the rest of the section, we describe how we build the positive example set E and a control negative example set N E. Elements of the two sets are pairs (t,s) where t is a target word s is a sequence of words. t"
C10-1142,W03-1808,0,0.0193835,"ailable sets to contain such triples, with the exception of the set used in Mitchell and Lapata (2008) which is designed only for testing purposes. It contains similar and dissimilar pairs of sequences (s1 ,s2 ) where each sequence is a verb-noun pair (vi ,ni ). From the positive part of this set, we can only derive quadruples where (v1 n1 ) ≈ (v2 n2 ) but we cannot derive the ideal resulting vector of the composition (vi ni ). Sets used to test multiword expression (MWE) detection models (e.g., (Schone and Jurafsky, 2001; Nicholson and Baldwin, 2008; Kim and Baldwin, 2008; Cook et al., 2008; Villavicencio, 2003; Korkontzelos and Manandhar, 2009)) are again not useful as containing only valid MWE that cannot be used to determine the set of training triples needed here. As a result, we need a novel idea to build sets of triples to train CDS models. We can leverage on knowledge stored in dictionaries. In the rest of the section, we describe how we build the positive example set E and a control negative example set N E. Elements of the two sets are pairs (t,s) where t is a target word s is a sequence of words. t is the word that represent the distributional meaning of s in the case of E. Contrarily, t i"
C14-1068,J10-4006,0,0.150198,"Missing"
C14-1068,D10-1115,0,0.0904988,"Missing"
C14-1068,P02-1034,0,0.447351,"Missing"
C14-1068,W05-1203,0,0.254525,"Missing"
C14-1068,D11-1096,0,0.0587699,"Missing"
C14-1068,P13-4006,0,0.0695554,"Missing"
C14-1068,W13-3824,1,0.742802,"Missing"
C14-1068,D11-1129,0,0.0359997,"Missing"
C14-1068,W13-3214,0,0.0249302,"Missing"
C14-1068,N10-1146,1,0.926857,"Missing"
C14-1068,P08-1028,0,0.139182,"Missing"
C14-1068,S13-1011,0,0.0491239,"Missing"
C14-1068,D12-1110,0,0.0490028,"Missing"
C14-1068,W11-1302,1,0.863786,"Missing"
C14-1068,C10-1142,1,0.884005,"Missing"
C14-1068,S13-1004,0,\N,Missing
D09-1010,P02-1034,0,0.562754,"novel algorithm for computing the similarity in first-order rewrite rule feature spaces. Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function. 1 Introduction Natural language processing models are generally positive combinations between linguistic models and automatically learnt classifiers. As trees are extremely important in many linguistic theories, a large amount of works exploiting machine learning algorithms for NLP tasks has been developed for this class of data structures (Collins and Duffy, 2002; Moschitti, 2004). These works propose efficient algorithms for determining the similarity among two trees in tree fragment feature spaces. Yet, some NLP tasks such as textual entailment recognition (Dagan and Glickman, 2004; Dagan et al., 2006) and some linguistic theories such as HPSG (Pollard and Sag, 1994) require more general graphs and, then, more general algorithms for computing similarity among graphs. Unfortunately, algorithms for computing similarity among two general graphs in term of common subgraphs are still exponential (Ramon and G¨artner, 2003). In these cases, approximated al"
D09-1010,P04-1043,0,0.0756263,"ting the similarity in first-order rewrite rule feature spaces. Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function. 1 Introduction Natural language processing models are generally positive combinations between linguistic models and automatically learnt classifiers. As trees are extremely important in many linguistic theories, a large amount of works exploiting machine learning algorithms for NLP tasks has been developed for this class of data structures (Collins and Duffy, 2002; Moschitti, 2004). These works propose efficient algorithms for determining the similarity among two trees in tree fragment feature spaces. Yet, some NLP tasks such as textual entailment recognition (Dagan and Glickman, 2004; Dagan et al., 2006) and some linguistic theories such as HPSG (Pollard and Sag, 1994) require more general graphs and, then, more general algorithms for computing similarity among graphs. Unfortunately, algorithms for computing similarity among two general graphs in term of common subgraphs are still exponential (Ramon and G¨artner, 2003). In these cases, approximated algorithms have been"
D09-1010,P03-2041,0,0.0406402,"function L only applies to the subsets of nodes related to the two trees, i.e., L : Nt ∪ Ng → L. Nodes in the set A are not labeled. 92 In (Zanzotto and Moschitti, 2006), tripartite directed acyclic graphs are implicitly introduced and exploited to build first-order rule feature spaces. Yet, both in (Zanzotto and Moschitti, 2006) and in (Moschitti and Zanzotto, 2007), the model proposed has two major limitations: it can represent rules with less than 7 variables and the proposed kernel is not a completely valid kernel as it uses the max function. In machine translation, some methods such as (Eisner, 2003) learn graph based rewrite rules for generative purposes. Yet, the method presented in (Eisner, 2003) can model first-order rewrite rules only with a very small amount of variables, i.e., two or three variables. The explicit representation of the tDAG in Fig. 2 has been useful to show that the unification of a rule and a sentence pair is a graph matching problem. Yet, it is complex to follow. We will then describe a tDAG with an alternative and more convenient representation. A tDAG G = (N, E) can be seen as pair G = (τ, γ) of extended trees τ and γ where τ = (Nt ∪ A, Et ∪ EAt ) and γ = (Ng ∪"
D09-1010,H05-1049,0,0.0443679,"Missing"
D09-1010,P03-1005,0,0.0321356,"uch as textual entailment recognition (Dagan and Glickman, 2004; Dagan et al., 2006) and some linguistic theories such as HPSG (Pollard and Sag, 1994) require more general graphs and, then, more general algorithms for computing similarity among graphs. Unfortunately, algorithms for computing similarity among two general graphs in term of common subgraphs are still exponential (Ramon and G¨artner, 2003). In these cases, approximated algorithms have been proposed. For example, the one proposed in (G¨artner, 2003) counts the number of subpaths in common. The same happens for the one proposed in (Suzuki et al., 2003) that is applicable to a particular class of graphs, i.e. the hierarchical directed acyclic graphs. These algorithms do not compute the number of subgraphs 91 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 91–100, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP and it is a valid kernel function. The paper is organized as follows. In Sec. 2, we firstly describe tripartite directed acyclic graphs (tDAGs) to model first-order feature (FOR) spaces. In Sec. 3, we then present the related work. In Sec. 4, we introduce the similarity function for these"
D09-1010,P06-1051,1,0.907577,"sis H, e.g., whether or not “Farmers feed cows animal extracts” entails “Cows eat animal extracts” (T1 , H1 ). If we want to learn textual entailment classifiers, we need to exploit first-order rules hidden in training instances. To positively exploit the training instance “Pediatricians suggest women to feed newborns breast milk” entails “Pediatricians suggest that newborns eat breast milk” (T2 , H2 ) for classifying the above example, learning algorithms should learn that the two instances hide the first-order rule ρ = f eed Y Z → Y eat Z . The first-order rule feature space, introduced by (Zanzotto and Moschitti, 2006), gives high performances in term of accuracy for textual entailment recognition with respect to other features spaces. In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), that model first-order rule feature spaces and, using this class of graphs, we introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces. The possibility of explicitly representing the first-order feature space as subgraphs of tDAGs makes the derived similarity function a valid kernel. With respect to the algorithm proposed in (Moschitti"
D09-1010,W07-1401,0,\N,Missing
D11-1061,C10-2005,0,0.0434461,"final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy tweets in the same cluster by pe"
D11-1061,A00-2018,0,0.0511078,"Missing"
D11-1061,P02-1034,0,0.0114475,", t1 ) + cos(t2 , t2 ) where cos(·, ·) is the cosine similarity between the two vectors. The LEX feature space is simple and can be extremely effective in modeling the content of tweet pairs. Yet, in principle, it doesn’t model the relations among words in the tweet. Different content feature spaces are then needed to capture these relations. 4.4 Syntactic content model (SYNT) The SYNT model represents a tweet pair using pairs of syntactic tree fragments from t1 and t2 . Each feature is a pair < f r1 , f r2 >, where f r1 and f r2 are syntactic tree fragments (see figure below). As defined in (Collins and Duffy, 2002), a syntactic tree fragment f ri is active in ti when f ri is a subtree of the syntactic interpretation of ti . Therefore, these features represent ground rules connecting the left-hand sides and the right-hand sides of the tweet pair: each feature is active for a pair (t1 , t2 ) when the left-hand side f r1 is activated by the syntactic analysis of t1 and the right-hand side f r2 is activated by t2 . As an example consider the feature: that the two tweets are correctly syntactically analyzed). This feature space models the relations between words syntactically. Therefore it overcomes the limi"
D11-1061,W05-1203,0,0.0463576,"(Haghighi and Vanderwende, 2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE"
D11-1061,C10-2028,0,0.0177398,"; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy tweets in the same cluster by performing word alignment. In our paper we are also interested in aligning and grouping tweets, although our goal is to detect redundancy, not to perform SRL. On a different ground, Ritt"
D11-1061,C04-1051,0,0.415551,"Missing"
D11-1061,C10-1034,0,0.0542159,"Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance ca"
D11-1061,N09-1041,0,0.153402,"gure 1 shows an example of a Twitter search engine where redundant tweets are Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659–669, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction. present (left) and where they are discarded (right). Also, from a computational linguistic point of view, the high redundancy in micro-blogs gives the unprecedented opportunity to study classical tasks such as text summarization (Haghighi and Vanderwende, 2009), textual entailment recognition (Dagan et al., 2006) and paraphrase detection (Dolan et al., 2004) on very large corpora characterized by an original and emerging linguistic style, pervaded with ungrammatical and colloquial expressions, abbreviations, and new linguistic forms. The aim of this paper is to formally define, for the first time, the problem of redundancy in micro-blogs and to systematically approach the task of automatic redundancy detection. Note that we focus on linguistic redundancy, i.e. tweets that convey the same information with different wordings, and ignore the more trivi"
D11-1061,H05-1049,0,0.0186408,"re short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE techniques and features that are usually use"
D11-1061,O97-1002,0,0.00826592,"r example, consider the tweet pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, BOW would assign a low score, since many words are not shared across the two tweets. WBOW fixes this problem by matching ‘Oscar’‘Academy Awards’ and ‘forgot’-‘snubbed’ at the semantic level. To provide these matches, WBOW relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&Conrath similarity (Jiang and Conrath, 1997). In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. 4.3 Lexical content model (LEX) This model and the next ones (SYNT and FOR) explicitly model the content of a tweet pair P = (t1 , t2 ) as a whole. This is a radically different approach with respect to the similarity-based models explored so far, where the content of t1 and t2 were treated independently (i.e. each tweet with its own bag of words), and the SVM used as the single fea"
D11-1061,C10-1079,0,0.140281,"erienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; 659 Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill ski jump. #Vancouver2010” By performing an editorial study (described l"
D11-1061,N06-1006,0,0.015125,"Missing"
D11-1061,N04-3012,0,0.0128387,"Missing"
D11-1061,N10-1021,0,0.0990585,"Missing"
D11-1061,N10-1020,0,0.0987787,"ntial growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; 659 Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill ski jump. #Vancouver2010” By performing an editorial study (described later in the paper) we"
D11-1061,C00-2137,0,0.0280484,"Missing"
D11-1061,D09-1010,1,0.848831,"Missing"
D11-1061,P06-1051,1,0.580299,"in a kernel function. Given two pairs of tweets P (a) and P (b) , the SYNT kernel function is defined as follows: (a) h NP (a) (b) where K(·, ·) is the tree kernel function described in (Collins and Duffy, 2002). 4.5 Syntactic first-order rule content model (FOR) The FOR model overcomes the limitations of SYNT, by enriching the space with features representing first-order relations between the two tweets of a pair. Each feature represents a rule with variables, i.e. a first order rule that is activated by the tweet pairs if the variables are unified. This feature space has been introduced in (Zanzotto and Moschitti, 2006) and shown to improve over the ones above. Each feature < f r1 , f r2 > is a pair of syntactic tree fragments augmented with variables. The feature is active for a tweet pair (t1 , t2 ) if the syntactic interpretations of t1 and t2 can be unified with < f r1 , f r2 >. For example, consider the following feature: S S h NP X VP VBP bought S (b) KSY N T (P (a) , P (b) ) = K(t1 , t1 ) + K(t2 , t2 ) NP Y , NP X VP VBP NP Y i owns S VP VBP bought NP , N VP VBP NP i owns This feature is active for the pair of tweets (“GM bought Opel”,“GM owns Opel”) since the syntactic analysis of the pair matches th"
D11-1061,W07-1401,0,\N,Missing
fallucchi-etal-2010-generic,W01-0521,0,\N,Missing
fallucchi-etal-2010-generic,N03-1027,0,\N,Missing
fallucchi-etal-2010-generic,C92-2082,0,\N,Missing
fallucchi-etal-2010-generic,P04-3013,0,\N,Missing
fallucchi-etal-2010-generic,P07-1007,0,\N,Missing
fallucchi-etal-2010-generic,W09-0432,0,\N,Missing
fallucchi-etal-2010-generic,P06-1015,0,\N,Missing
fallucchi-etal-2010-generic,J08-4004,0,\N,Missing
fallucchi-etal-2010-generic,P06-1101,0,\N,Missing
fallucchi-etal-2010-generic,J96-2004,0,\N,Missing
fallucchi-etal-2010-generic,W06-1615,0,\N,Missing
fallucchi-etal-2010-generic,D09-1053,0,\N,Missing
fallucchi-etal-2010-generic,W09-0209,1,\N,Missing
fallucchi-etal-2010-generic,N04-4006,0,\N,Missing
fallucchi-zanzotto-2008-yet,J90-1003,0,\N,Missing
fallucchi-zanzotto-2008-yet,C04-1051,0,\N,Missing
fallucchi-zanzotto-2008-yet,W05-1209,0,\N,Missing
fallucchi-zanzotto-2008-yet,C92-2082,0,\N,Missing
fallucchi-zanzotto-2008-yet,P06-1015,0,\N,Missing
fallucchi-zanzotto-2008-yet,N04-3012,0,\N,Missing
fallucchi-zanzotto-2008-yet,P02-1006,0,\N,Missing
fallucchi-zanzotto-2008-yet,W04-3206,0,\N,Missing
guthrie-etal-2004-large,H93-1051,0,\N,Missing
guthrie-etal-2004-large,A97-1035,1,\N,Missing
guthrie-etal-2004-large,J01-3001,0,\N,Missing
guthrie-etal-2004-large,C96-1079,0,\N,Missing
J15-1010,W13-0112,0,0.179789,"= TALL boy, T T  cat  F = g(K1 ( f (tall), f ( )), K2 ( f (boy), f (cat))) = TALL I, boy (19) Dual Space Model. We have until now applied the CC to linear CDSMs with the dot product as the final comparison operator (what we called K). The CC also holds for the effective Dual Space model of Turney (2012), which assumes that each word has two distributional representations, wd in “domain” space and wf in “function” space. The similarity of two phrases is directly computed as the geometric average of the separate similarities between the first and second words in both spaces. Even though 2 Grefenstette et al. (2013) first framed the Lexical Function in terms of tensor contraction. 171 Computational Linguistics Volume 41, Number 1 there is no explicit composition step, it is still possible to put the model in CC form. Take x = (x1 , x2 ) and its trivial decomposition. Define, for a word w with  vector repT resentations wd and wf : f (w) = wd w f . Define also K1 ( f (x1 ), f (y1 )) =  f (x1 ), f (y1 )F , √  K2 ( f (x2 ), f (y2 )) =  f (x2 ), f (y2 )F and g(a, b) to be ab. Then g(K 1 ( f (x1 ), f (y1 )), K2 ( f (x2 ), f (y2 ))) =   = xd1 xf 1 T , yd1 yf 1 T F · xd2 xf 2 T , yd2 yf 2 T"
J15-1010,W10-2805,0,0.0383526,"Missing"
J15-1010,N10-1146,1,0.755896,"Missing"
J15-1010,P08-1028,0,0.52287,"Missing"
J15-1010,C10-1142,1,0.901092,"Missing"
N10-1146,N09-1003,0,0.0403911,"Missing"
N10-1146,D09-1110,0,0.0118203,"is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syn"
N10-1146,W05-0601,1,0.800255,"syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the"
N10-1146,H05-1079,0,0.00742673,"ee kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatica"
N10-1146,W07-1402,0,0.0153556,"wing example: T6 ⇒?H6 T6 “In 1956 JFK met Marilyn Monroe” H6 “Marilyn Monroe died in 1956” The problem is that the pairs hT2 , H2 i and hT4 , H4 i share more meaningful features than the rule ρ5 , which should make the difference with respect to the relation between the pairs hT2 , H2 i and hT6 , H6 i. Indeed, the word “kill” is more semantically related to “murdered” than to “meet”. Using this information, it is possible to derive more effective rules from training examples. There are several solutions for taking this information into account, e.g. by using FrameNet semantics (e.g., like in (Burchardt et al., 2007)), it is possible to encode a lexical-syntactic rule using the KILLING and the DEATH frames, i.e.: ρ3 = X killed Y → Y died ρ7 = along with such rules, the temporal information should be taken into consideration. Given the importance of lexical-syntactic rules in RTE, many methods have been proposed for their extraction from large corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)). Unfortunately, these unsupervised methods in general produce rules that can hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Mos"
N10-1146,A00-2018,0,0.0694397,"K, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We"
N10-1146,P02-1034,0,0.0260782,"important fragment from a semantically similar sentence, which cannot be matched by STK but it is matched by SSTK. account in the model definition. Since tree kernels have been shown to be very effective for exploiting syntactic information in natural language tasks, a promising idea is to merge together the two different approaches, i.e. tree kernels and semantic similarities. 4.1 Syntactic Tree Kernel (STK) Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. The standard definition of the STK, given in (Collins and Duffy, 2002), allows for any set of nodes linked by one or more entire production rules to be valid substructures. The formal characterization is given in (Collins and Duffy, 2002) and is reported hereafter: Let F = {f1 , f2 , . . . , f|F |} be the set of tree fragments and χi (n) be an indicator function, equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree kernel function over P T1 and T2 is defined as T K(T1 , T2 ) = P n1 ∈NT1 n2 ∈NT2 ∆(n1 , n2 ), where NT1 and NT2 are the sets of nodes in T1 and T2 , respectively and P|F | ∆(n1 , n2 ) = i=1 χi (n1 )χi (n2 ). ∆ function coun"
N10-1146,E09-1025,0,0.0237506,"describes lexical similarity approaches, which can serve the generalization purpose. Section 4 describes how to integrate lexical similarity in syntactic structures using syntactic/semantic tree kernels (SSTK) whereas Section 5 shows how to use SSTK in a kernel-based RTE system. Section 6 describes the experiments and results. Section 7 discusses the efficiency and accuracy of our system compared with other RTE systems. Finally, we draw the conclusions in Section 8. 2 Related work Lexical-syntactic rules are largely used in textual entailment recognition systems (e.g., (Bar-Haim et al., 2007; Dinu and Wang, 2009)) as they conveniently encode world knowledge into linguistic structures. For example, to decide whether the simple sentences are in the entailment relation: T2 ⇒?H2 T2 “In 1980 Chapman killed Lennon.” H2 “John Lennon died in 1980.” we need a lexical-syntactic rule such as: from examples in terms of complex relational features. This approach can easily miss some useful information and rules. For example, given the pair hT2 , H2 i, to derive the entailment value of the following case: T4 ⇒?H4 T4 “In 1963 Lee Harvey Oswald murdered JFK” H4 “JFK died in 1963” we can only rely on this relatively i"
N10-1146,W07-1401,0,0.0258766,"SSTK) Kernels. The latter according to different similarities distributional vs. Wordnet-based approaches. Second, we derive qualitative and quantitative properties, which justify the selection of one with respect to the other. For this purpose, we tested four different version of SSTK, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is als"
N10-1146,H05-1049,0,0.0076299,"w that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more ac"
N10-1146,O97-1002,0,0.0476717,"Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We built different LSA matrices from the British National Corpus (BNC) and Wikipedia (Wiki). The British National Corpus (BNC) is a balanced synchronic text corpus containing 100 million words with mor"
N10-1146,E06-1015,1,0.639162,"utomatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
N10-1146,N04-3012,0,0.553662,"n hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009), where lexical-syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan a"
N10-1146,W07-1418,0,0.0285501,"xploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain"
N10-1146,C08-1107,0,0.0297184,"oded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syntactic and semantic information in RTE, without requiring either large automatic rule acquisition or hand-coding. These models exploit lexical similarities to generalize lexical-syntactic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form o"
N10-1146,H05-1047,0,0.0318507,"s realized by means of tree kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic"
N10-1146,P94-1019,0,0.146531,"tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the following measures, that we use in this study, are based on path lengths between concepts in the Wordnet Hierarchy: Path the measure is equal to the inverse of the shortest path length (path length) between two synsets c1 and c2 in WordNet SimP ath = 1 path length(c1 , c2 ) (1) WUP the Wu and Palmer (Wu and Palmer, 1994) similarity metric is based on the depth of two given synsets c1 and c2 in the WordNet taxonomy, and the depth of their least common subsumer (lcs). These are combined into a similarity score: SimW U P = 2 × depth(lcs) depth(c1 ) + depth(c2 ) (2) Wordnet similarity measures on synsets can be extended to similarity measures between words as follows: κS (w1 , w2 ) = max(c1 ,c2)∈C1 ×C2 SimS (c1 , c2 ) (3) where S is Path or WUP and Ci is the set of the synsets related to the word wi . 3.2 Distributional Semantic Similarity Latent Semantic Analysis (LSA) is one of the corpus-based measure of distr"
N10-1146,C00-2137,0,0.0288195,"m with the standard accuracy and then we determine the statistical significance by using the model RTE2 +WOK RTE3 +WOK RTE5 +WOK STK 61.5 52.62 66.38 53.25 62.0 54.33 SSTK 61.12 52.75 66.5 54.5 62.0 57.33 maxSTK 63.88 61.25 66.5 62.25 64.83 63.33 maxSSTK 64.12 59.38 67.0 64.38 64.83 62.67 STK+maxSTK 63.12 61.25 66.88 63.12 65.5 61.83 SSTK+maxSSTK 63.50 58.75 67.25 63.62 66.5 62.67 ∅ 60.62 66.75 60.67 - Table 2: Comparing different lexico-syntactic kernels with Wiki-based semantic kernels of the j parameters, i.e. j = 1, which was not selected by our limited parameter validation. described in (Yeh, 2000) and implemented in (Pad´o, 2006). 6.2 Distributional vs. WordNet-based Semantics The first experiment compares the basic kernel, i.e. WOK+STK+maxSTK, with the new semantic kernel, i.e. WOK+SSTK+maxSSTK, where SSTK and maxSSTK encode four different kinds of similarities, BNC, WIKI, WUP and Path. The aim is twofold: understanding if semantic similarities can be effectively used to derive generalized lexicosyntactic rules and to determine the best similarity model. Table 1 shows the results according to No Semantics, Wiki, BNC, Path and WUP. The three pairs of rows represent the results over the"
N10-1146,P06-1051,1,0.887883,"actic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
P06-1051,H05-1079,0,0.0520553,"of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such nouns are both noun modifiers: insurance modifies companies and cash modifies dividends. A second class of methods can give a solution to the previous problem. These methods generally combine a similarity measure with a set of possible transformations T applied over syntactic and semantic interpretations. The entailment between T and H is detected when there is a transformation r ∈ T so that sim(r(T ), H) &gt; α. These transformations are logical rules in (Bos and Markert, 2005) or sequences of allowed rewrite rules in (de Salvo Braz et al., 2005). The disadvantage is that such rules have to be manually designed. Moreover, they generally model better positive implications than negative ones and they do not consider errors in syntactic parsing and semantic analysis. To consider structural and lexical relation similarity, we augment syntactic trees with placeholders which identify linked words. More in detail: - We detect links between words wt in T that are equal, similar, or semantically dependent on words wh in H. We call anchors the pairs (wt , wh ) and we associat"
P06-1051,A00-2018,0,0.00616774,"Missing"
P06-1051,P02-1034,0,0.122088,"cturally similar to T 00 and H 00 , respectively and (2) the lexical relations within the pair (T 0 , H 0 ) are compatible with those in (T 00 , H 00 ). Typically, T and H show a certain degree of overlapping, thus, lexical relations (e.g., between the same words) determine word movements from T to H (or vice versa). This is important to model the syntactic/lexical similarity between example pairs. Indeed, if we encode such movements in the syntactic parse trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in (Collins and Duffy, 2002). 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is the"
P06-1051,W05-1203,0,0.322917,"se trees of texts and hypotheses, we can use interesting similarity measures defined for syntactic parsing, e.g., the tree kernel devised in (Collins and Duffy, 2002). 2 Related work Although the textual entailment recognition problem is not new, most of the automatic approaches have been proposed only recently. This has been mainly due to the RTE challenge events (Dagan et al., 2005; Bar Haim et al., 2006). In the following we report some of such researches. A first class of methods defines measures of the distance or similarity between T and H either assuming the independence between words (Corley and Mihalcea, 2005; Glickman et al., 2005) in a bag-of-word fashion or exploiting syntactic interpretations (Kouylekov and Magnini, 2005). A pair (T, H) is then in entailment when sim(T, H) &gt; α. These approaches can hardly determine whether the entailment holds in the examples of the previous section. From the point of view of bag-of-word methods, the pairs (T1 , H1 ) and (T1 , H2 ) have both the same intra-pair similarity since the sentences of T1 and H1 as well as those of T1 and H2 differ by a noun, insurance and cash, respectively. At syntactic level, also, we cannot capture the required information as such"
P06-1051,C92-2082,0,0.0290809,"Missing"
P06-1051,O97-1002,0,0.0203333,"Missing"
P06-1051,E06-1015,1,0.509203,"2.0 (Miller, 1995) to extract both the verbs in entailment, Ent set, and the derivationally related words, Der set. - The wn::similarity package (Pedersen et al., 2004) to compute the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) as in (Corley and Mihalcea, 2005). This is one of the best figure method which provides a similarity score in the [0, 1] interval. We used it to implement the d(lw , lw0 ) function. - A selected portion of the British National Corpus2 to compute the inverse document frequency (idf ). We assigned the maximum idf to words not found in the BNC. - SVM-light-TK3 (Moschitti, 2006) which encodes the basic tree kernel function, KT , in SVMlight (Joachims, 1999). We used such software to implement Ks (Eq. 6), K1 , K2 (Eq. 5) and Ks + Ki kernels. The latter combines our new kernel with traditional approaches (i ∈ {1, 2}). Only the bold part of T supports the implication; the rest is useless and also misleading: if we used it to compute the similarity it would reduce the importance of the relevant part. Moreover, as we normalize the syntactic tree kernel (KT ) with respect to the size of the two trees, we need to focus only on the part relevant to the implication. The ancho"
P06-1051,N04-3012,0,0.0908262,"Missing"
P06-1051,W05-1206,0,0.0134069,"e have designed an effective way to automatically learn entailment rules from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art Recently, textual entailment recognition has been receiving a lot of attention. The main reason is that the understanding of the basic entailment processes will allow us to model more accurate semantic theories of natural languages (Chierchia and McConnell-Ginet, 2001) and design important applications (Dagan and Glickman, 2004), e.g., Question Answering and Information Extraction. However, previous work (e.g., (Zaenen et al., 2005)) suggests that determining whether or not a text T entails a hypothesis H is quite complex even when all the needed information is explicitly asserted. For example, the sentence T1 : “At the end of the year, all solid companies pay dividends.” entails the hypothesis H1 : “At the end of the year, all solid insurance companies pay dividends.” but it does not entail the hypothesis H2 : “At the end of the year, all solid companies pay cash dividends.” Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similari"
P06-1107,C92-2082,0,0.0221311,"(Sec. 4.1), we translate the verb selectional expectations in specific Subject-Verb lexico-syntactic patterns P(vt , vh ). Secondly (Sec. 4.2), we define a statistical measure S(vt , vh ) that captures the verb preferences. This measure describes how much the relations between target verbs (vt , vh ) are stable and commonly agreed. Our method to detect verb entailment relations is based on the idea that some point-wise assertions carry relevant semantic information. This idea has been firstly used in (Robison, 1970) and it has been explored for extracting semantic relations between nouns in (Hearst, 1992), where lexico-syntactic patterns are induced by corpora. More recently this method has been applied for structuring terminology in isa hierarchies (Morin, 1999) and for learning question-answering patterns (Ravichandran and Hovy, 2002). 4.1 Nominalized textual entailment lexico-syntactic patterns The idea described in Sec. 2 can be applied to generate Subject-Verb textual entailment lexicosyntactic patterns. It often happens that verbs can undergo an agentive nominalization, e.g., play vs. player. The overall procedure to verify if an entailment between two verbs (vt , vh ) holds in a pointwi"
P06-1107,J03-3005,0,0.0231506,"verb pairs where the nominalized pattern is applicable. The best pattern or the best combined method should be the one that gives the highest values of S to verb pairs in entailment relation, and the lowest value to other pairs. We need a corpus C over which to estimate probabilities, and two dataset, one of verb entailment pairs, the True Set (T S), and another with verbs not in entailment, the Control Set (CS). We use the web as corpus C where to estimate Smi and GoogleT M as a count estimator. The web has been largely employed as a corpus (e.g., (Turney, 2001)). The findings described in (Keller and Lapata, 2003) suggest that the count estimations we need in our study over Subject-Verb bigrams are highly correlated to corpus counts. 6.1 Se(t) = p((vh , vt ) ∈ T S|S(vh , vt ) > t) Sp(t) = p((vh , vt ) ∈ CS|S(vh , vt ) < t) The ROC curve (Se(t) vs. 1 − Sp(t)) naturally follows (see Fig. 1). Better methods will have ROC curves more similar to the step function f (1 − Sp(t)) = 0 when 1 − Sp(t) = 0 and f (1 − Sp(t)) = 1 when 0 < 1 − Sp(t) ≤ 1. The ROC analysis provides another useful evaluation tool: the AROC, i.e. the total area under the ROC curve. Statistically, AROC represents the probability that the"
P06-1107,P02-1006,0,0.0179325,"es. This measure describes how much the relations between target verbs (vt , vh ) are stable and commonly agreed. Our method to detect verb entailment relations is based on the idea that some point-wise assertions carry relevant semantic information. This idea has been firstly used in (Robison, 1970) and it has been explored for extracting semantic relations between nouns in (Hearst, 1992), where lexico-syntactic patterns are induced by corpora. More recently this method has been applied for structuring terminology in isa hierarchies (Morin, 1999) and for learning question-answering patterns (Ravichandran and Hovy, 2002). 4.1 Nominalized textual entailment lexico-syntactic patterns The idea described in Sec. 2 can be applied to generate Subject-Verb textual entailment lexicosyntactic patterns. It often happens that verbs can undergo an agentive nominalization, e.g., play vs. player. The overall procedure to verify if an entailment between two verbs (vt , vh ) holds in a pointwise assertion is: whenever it is possible to apply the agentive nominalization to the hypothesis vh , scan the corpus to detect those expressions in which the agentified hypothesis verb is the subject of a clause governed by the text ver"
P06-1107,W04-3205,0,0.468305,"exicosyntactic patterns. It often happens that verbs can undergo an agentive nominalization, e.g., play vs. player. The overall procedure to verify if an entailment between two verbs (vt , vh ) holds in a pointwise assertion is: whenever it is possible to apply the agentive nominalization to the hypothesis vh , scan the corpus to detect those expressions in which the agentified hypothesis verb is the subject of a clause governed by the text verb vt . Given a verb pair (vt , vh ) the assertion is for851 Lexico-syntactic patterns Pnom (vt , vh ) = nominalization Phb (vt , vh ) = happens-before (Chklovski and Pantel, 2004) probabilistic entailment (Glickman et al., 2005) Ppe (vt , vh ) = Fagent (v) = F (v) = additional sets Fall (v) = {“agent(vh )|num:sing vt |person:third,t:pres ”, “agent(vh )|num:plur vt |person:nothird,t:pres ”, “agent(vh )|num:sing vt |t:past ”, “agent(vh )|num:plur vt |t:past ”} {“vh |t:inf and then vt |t:pres ”, “vh |t:inf * and then vt |t:pres ”, “vh |t:past and then vt |t:pres ”, “vh |t:past * and then vt |t:pres ”, “vh |t:inf and later vt |t:pres ”, “vh |t:past and later vt |t:pres ”, “vh |t:inf and subsequently vt |t:pres ”, “vh |t:past and subsequently vt |t:pres ”, “vh |t:inf and ev"
P06-1107,P89-1010,0,0.0172529,"statistical analysis of the verb selectional preferences over a corpus. This assessment will validate point-wise entailment assertions. Before introducing the statistical entailment indicator, we provide some definitions. Given a corpus C containing samples, we will refer to the absolute frequency of a textual expression t in the corpus C with fC (t). The definition can be easily extended to a set of expressions T . Given a pair vt and vh we define the following entailment strength indicator S(vt , vh ). Specifically, the measure Snom (vt , vh ) is derived from point-wise mutual information (Church and Hanks, 1989): Measures to estimate the entailment strength Snom (vt , vh ) = log The above textual entailment patterns define pointwise entailment assertions. If pattern instances are found in texts, the related verb-subject pairs suggest but not confirm a verb selectional preference. p(vt , vh |nom) p(vt )p(vh |pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs. Probabilities are estimated using maximum-likelihood: 1 Agentive nominalization has been obtained adding “-er” to the verb root taking into accou"
P06-1107,J90-1003,0,\N,Missing
pazienza-etal-2006-mixing,W04-2704,0,\N,Missing
pazienza-etal-2006-mixing,P05-1016,0,\N,Missing
pazienza-etal-2006-mixing,kipper-etal-2004-extending,0,\N,Missing
R09-1016,C96-1005,0,0.10647,". We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 1 Introduction Taxonomies and, in general, networks of words connected with transitive relations are extremely important knowledge repositories for a variety of applications in natural language processing (NLP) and knowledge representation (KR). In NLP, taxonomies such as WordNet [17] are widely used in intermediate tasks such as word sense disambiguation (e.g. [1]) and selectional preference induction (e.g., [25]) as well as in final applications such as question answering (e.g., [4]) and textual entailment recognition (e.g. [5]). In KR, taxonomies as well as other word networks are the bulk of domain ontologies. To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending ex"
R09-1016,W05-1203,0,0.0229591,"of using SVD for feature selection positively affects performances. 1 Introduction Taxonomies and, in general, networks of words connected with transitive relations are extremely important knowledge repositories for a variety of applications in natural language processing (NLP) and knowledge representation (KR). In NLP, taxonomies such as WordNet [17] are widely used in intermediate tasks such as word sense disambiguation (e.g. [1]) and selectional preference induction (e.g., [25]) as well as in final applications such as question answering (e.g., [4]) and textual entailment recognition (e.g. [5]). In KR, taxonomies as well as other word networks are the bulk of domain ontologies. To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [1"
R09-1016,C92-2082,0,0.410362,"ain ontologies. To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21]. These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26]). The task is generally seen as a classification (e.g., [22, 27]) or a clustering (e.g., [3]) problem. This allows the use of machine learning models. Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27], are represented. These feature spaces are used to determine whether or not new word"
R09-1016,N04-1016,0,0.0699389,"Missing"
R09-1016,J04-2002,0,0.116265,"5]). In KR, taxonomies as well as other word networks are the bulk of domain ontologies. To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21]. These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26]). The task is generally seen as a classification (e.g., [22, 27]) or a clustering (e.g., [3]) problem. This allows the use of machine learning models. Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27], are represente"
R09-1016,P06-1015,0,0.134339,"ain ontologies. To be effectively used in NLP and KR applications, taxonomies and knowledge repositories have to be large or, at least, adapted to specific domains. Yet, even huge knowledge repositories such as WordNet [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21]. These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26]). The task is generally seen as a classification (e.g., [22, 27]) or a clustering (e.g., [3]) problem. This allows the use of machine learning models. Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27], are represented. These feature spaces are used to determine whether or not new word"
R09-1016,C02-1090,0,0.392106,"et [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21]. These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26]). The task is generally seen as a classification (e.g., [22, 27]) or a clustering (e.g., [3]) problem. This allows the use of machine learning models. Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27], are represented. These feature spaces are used to determine whether or not new word pairs coming from the text collection have to be included in existing knowledge repositories. Decision models are learnt ∗ DISP University Rome “Tor Vergata“ ∗ using existing knowledge repositories and then applied t"
R09-1016,P02-1006,0,0.0123512,"ome induced lexical-syntactic patterns (originally used in [26]). The distributional hypothesis is widely used in many approaches for taxonomy induction from texts. For example, it is used in [3] for populating lattices, i.e. graphs of a particular class, of formal concepts. Lexical syntactic patterns are also a source of relevant information for deciding whether or not a particular relation holds between two words. This approach has been widely used for detecting hypernymy relations such as in [13, 18], for other ontological relations such as in [21], or for more generic relations such as in [24, 28]. These learning models generally use the hypothesis that two words are related according to a particular relation if these often appear in specific text fragments. Despite the wide range of models for taxonomy learning, only very few exploit the structure of existing taxonomies. The task is seen as building taxonomies from scratch. In [3], for example, lattices and the related taxonomies are the target. Yet, existing taxonomies may be used to drive the process of building new taxonomies. In [19], WordNet [17] and WordNet glosses are used to drive the construction of domain specific ontologies"
R09-1016,P06-1101,0,0.393382,"et [17] are extremely poor when used in specific domains such as the medical domain (see [29]). Automatically creating, adapting, or extending existing knowledge repositories using domain texts is, then, a very important and active area. A large variety of methods have been proposed: ontology learning methods [16, 3, 19] in KR as well as knowledge harvesting methods in NLP such as [13, 21]. These learning methods use variants of the distributional hypothesis [12] or exploit some induced lexical-syntactic patterns (originally used in [26]). The task is generally seen as a classification (e.g., [22, 27]) or a clustering (e.g., [3]) problem. This allows the use of machine learning models. Yet, as any other machine learning problem, knowledge harvesting and ontology learning models exploit the above hypothesis to build feature spaces where instances, i.e., words as in [22] or word pairs as in [27], are represented. These feature spaces are used to determine whether or not new word pairs coming from the text collection have to be included in existing knowledge repositories. Decision models are learnt ∗ DISP University Rome “Tor Vergata“ ∗ using existing knowledge repositories and then applied t"
R09-1016,W04-3206,0,0.0283459,"ome induced lexical-syntactic patterns (originally used in [26]). The distributional hypothesis is widely used in many approaches for taxonomy induction from texts. For example, it is used in [3] for populating lattices, i.e. graphs of a particular class, of formal concepts. Lexical syntactic patterns are also a source of relevant information for deciding whether or not a particular relation holds between two words. This approach has been widely used for detecting hypernymy relations such as in [13, 18], for other ontological relations such as in [21], or for more generic relations such as in [24, 28]. These learning models generally use the hypothesis that two words are related according to a particular relation if these often appear in specific text fragments. Despite the wide range of models for taxonomy learning, only very few exploit the structure of existing taxonomies. The task is seen as building taxonomies from scratch. In [3], for example, lattices and the related taxonomies are the target. Yet, existing taxonomies may be used to drive the process of building new taxonomies. In [19], WordNet [17] and WordNet glosses are used to drive the construction of domain specific ontologies"
S13-2007,D10-1115,0,0.0186256,"ations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dict"
S13-2007,S13-2018,0,0.0375986,"Missing"
S13-2007,S13-2016,0,0.0556694,"Missing"
S13-2007,W10-2805,0,0.0217452,"3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are nat"
S13-2007,S13-2020,0,0.0615235,"Missing"
S13-2007,P08-1028,0,0.0683415,"s a core problem, since satisfactory performance in computing the similarity of full sentences depends on similarity computations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Diction"
S13-2007,P03-1048,0,0.0187643,"nts were allowed to use or ignore the training data, i.e. the systems could be supervised or unsupervised. Unsupervised systems were allowed to use the training data for development and parameter tuning. Since this is a core task, participating systems were not be able to use dictionaries or other prefabricated lists. Instead, they were allowed to use distributional similarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsuper"
S13-2007,S13-2019,0,0.0932092,"tional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 .825 .857 .797 .854 .881 .893 .937"
S13-2007,D11-1094,0,0.0586444,"Missing"
S13-2007,S13-2017,0,0.0241557,"Missing"
S13-2007,S13-2008,0,0.0216165,"milarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 ."
S13-2007,C10-1142,1,0.346352,"task is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are natural repositories of equivalences between words under definition and sequences of words used for defining them. Figure 1 presents t"
S13-2007,zesch-etal-2008-extracting,1,0.80164,"Missing"
S14-1013,D10-1115,0,0.0422631,"Missing"
S14-1013,P02-1034,0,0.0270801,"ers a word sequence. The first subtree represents the chunk covering the second NP and the node dominates the word sequence its:d final:n concert:n. The second subtree represents the structure of the whole sentence and one chunk, that is the first NP dominating the word sequence the:d rock:n band:n. The third subtree again represents the structure of the whole sentence split into two chunks without the verb. NP S HH NP VBZ NP where τ3 is the third subtree of Figure 2. Given a tree t, the set S(t) is defined as the set containing all the relevant CSSTs of the tree t. As for the tree kernels (Collins and Duffy, 2002), the set S(t) contains all CSSTs derived from the subtrees of t such that if a node n belongs to a subtree ts , all the siblings of n in t belongs to ts . In other words, productions of the initial subtrees are complete. A CSST is obtained by collapsing in a single terminal nodes a contiguous sequence of words dominated by a single non-terminal node. For example: S ``` ` XXX  X NP its:p final:j concert:n VP  X XXX Z  Z  the:d rock:n band:n VBZ NP S ``` ` VP PP XXX  X P  NP the:d rock:j band:n VBZ VP Z  Z NP ``` ` NP PP  P its:p final:j concert:n DT NN HH NN Figure 2: S"
S14-1013,W05-1203,0,0.0154392,"are different from these two. To build the final kernel to learn the classifier, we followed standard approaches (Dagan et al., 2013), that is, we exploited two models: a model with only a rewrite rule feature space (RR) and a model with the previous space along with a token-level similarity feature (RRTWS). The two models use our CSTKs and the standard TKs in the following way as kernel functions: (1) RR(p1 , p2 ) = κ(ta1 , ta2 ) + κ(tb1 , tb2 ); (2) RRT W S(p1 , p2 ) = κ(ta1 , ta2 ) + κ(tb1 , tb2 ) + (T W S(a1 , b1 ) · T W S(a2 , b2 ) + 1)2 where T W S is a weighted token similarity (as in (Corley and Mihalcea, 2005)). where Avn , Bvn are matrices used for verb and noun phrase interaction, and Aan , Ban are used for adjective, noun interaction. 3 3.1 Experimental Investigation Experimental set-up We experimented with the Recognizing Textual Entailment datasets (RTE) (Dagan et al., 2006). RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing. The dev/test distr"
S14-1013,D11-1096,0,0.0217978,"ons for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity. Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels (Haussler, 1999) to model both interpretations. Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning 2 Chunk-based Smoothed Tree Kernels This section describes the new class of kernels. We first introduce the notion of the chunk-based syntactic subtree. Then, we describe the recursive formulation of the class of kernels. Finally, we introduce the basic CDSMs we use and we introduce two instances of the class of kernels. 93 Proceedings of the Th"
S14-1013,P13-4006,1,0.846371,"ental Investigation Experimental set-up We experimented with the Recognizing Textual Entailment datasets (RTE) (Dagan et al., 2006). RTE is the task of deciding whether a long text T entails a shorter text, typically a single sentence, called hypothesis H. It has been often seen as a classification task (see (Dagan et al., 2013)). We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split between training and testing. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, and 600/600 T-H pairs. Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. We collected a 35K-by-35K matrix by counting co-occurrence of the 30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmas occurring in the datasets 3.2 Results Table 1 shows the results of the experiments, the table is organised as follows: columns 2-6 report the accuracy of the RTE systems based on rewrite rules (R"
S14-1013,D11-1129,0,0.0404321,"Missing"
S14-1013,W10-2805,0,0.095846,"ors x and y . For example, given the a1 a2 as: chunk-based subtree τ3 in Figure 2 and → → ADD(a1 , a2 ) = αa1 + β a2 where α and β weight the first and the second word of the pair. The basic additive model for word sequences s = w1 . . . wk is recursively defined as follows: (→ w1 if k = 1 fBA (s) = → αw1 + βfBA (w2 . . . wk ) if k > 1 S XXX  X NP VP  X XX τ4 =   X !!aa the:d orchestra:n VBZ NP PP P  its:p show:n the similarity KF (τ3 , τ4 ) is: hf (the:d orchestra:n), f (the:d rock:n band:n)i · hf (its:p show:n), f (its:p final:j concert:n)i. The Full Additive model (FA) (used in (Guevara, 2010) for adjective-noun pairs and (Zanzotto et al., 2010) for three different syntactic relations) → computes the compositional vector a of a pair using two linear tranformations AR and BR respectively applied to the vectors of the first and the second word. These matrices generally only depends on the syntactic relation R that links those two words. The operation follows: The recursive formulation of the Chunk-based Smoothed Tree Kernel (CSTK) is a bit more complex but very similar to the recursive formulation of the syntactic tree kernels: K(t1 , t2 ) = X C(n1 , n2 ) (2) → n1 ∈ N (t1 ) n2 ∈ N (t"
S14-1013,N10-1146,1,0.847374,"semantic interpretations for sentences is important for many high-level NLP tasks, such as question-answering, textual entailment recognition, and semantic textual similarity. Systems and models for these tasks often use classifiers or regressors that exploit convolution kernels (Haussler, 1999) to model both interpretations. Convolution kernels are naturally defined on spaces where there exists a similarity function between terminal nodes. This feature has been used to integrate distributional semantics within tree kernels. This class of kernels is often referred to as smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011), yet, these models only use distributional vectors for words. Compositional distributional semantics models (CDSMs) on the other hand are functions mapping text fragments to vectors (or higher-order tensors) which then provide a distributional meaning 2 Chunk-based Smoothed Tree Kernels This section describes the new class of kernels. We first introduce the notion of the chunk-based syntactic subtree. Then, we describe the recursive formulation of the class of kernels. Finally, we introduce the basic CDSMs we use and we introduce two instances of the class of kernels. 93"
S14-1013,P08-1028,0,0.0431133,"i , τj ) = δ(s(τi ), s(τj )) hf (a), f (b)i models (CDSMs). We use two CDSMs: the Baa ∈ P T (τi ) sic Additive model (BA) and teh Full Additive b ∈ P T (τj ) model (FA). We thus define two specific CSTKs: the CSTK+BA that is based on the basic additive model and the CSTK+FA that is based on the full where δ(s(τi ), s(τj )) is the Kroneker’s delta additive model. We describe the two CDSMs in function between the the structural part of two the following. chunk-based syntactic subtrees, P T (τ ) are the The Basic Additive model (BA) (introduced in nodes in τ directly covering a chunk or a word, (Mitchell and Lapata, 2008)) computes the disti→ → and h x , y i is the cosine similarity between the butional semantics vector of a pair of words a = → → two vectors x and y . For example, given the a1 a2 as: chunk-based subtree τ3 in Figure 2 and → → ADD(a1 , a2 ) = αa1 + β a2 where α and β weight the first and the second word of the pair. The basic additive model for word sequences s = w1 . . . wk is recursively defined as follows: (→ w1 if k = 1 fBA (s) = → αw1 + βfBA (w2 . . . wk ) if k > 1 S XXX  X NP VP  X XX τ4 =   X !!aa the:d orchestra:n VBZ NP PP P  its:p show:n the similarity KF (τ3 , τ4 ) is: hf"
S14-1013,C10-1142,1,0.848214,": chunk-based subtree τ3 in Figure 2 and → → ADD(a1 , a2 ) = αa1 + β a2 where α and β weight the first and the second word of the pair. The basic additive model for word sequences s = w1 . . . wk is recursively defined as follows: (→ w1 if k = 1 fBA (s) = → αw1 + βfBA (w2 . . . wk ) if k > 1 S XXX  X NP VP  X XX τ4 =   X !!aa the:d orchestra:n VBZ NP PP P  its:p show:n the similarity KF (τ3 , τ4 ) is: hf (the:d orchestra:n), f (the:d rock:n band:n)i · hf (its:p show:n), f (its:p final:j concert:n)i. The Full Additive model (FA) (used in (Guevara, 2010) for adjective-noun pairs and (Zanzotto et al., 2010) for three different syntactic relations) → computes the compositional vector a of a pair using two linear tranformations AR and BR respectively applied to the vectors of the first and the second word. These matrices generally only depends on the syntactic relation R that links those two words. The operation follows: The recursive formulation of the Chunk-based Smoothed Tree Kernel (CSTK) is a bit more complex but very similar to the recursive formulation of the syntactic tree kernels: K(t1 , t2 ) = X C(n1 , n2 ) (2) → n1 ∈ N (t1 ) n2 ∈ N (t2 ) → fF A (a1 , a2 , R) = AR a1 + BR a2 95 RR RRTWS"
S14-2049,P02-1034,0,0.0110699,"that can be considered a compositional distributional semantic model as it transforms sentences into matrices that are then used by the learner as feature vectors. Our model is called Distributed Smoothed Tree Kernel (Ferrone and Zanzotto, 2014) as it mixes the distributed trees (Zanzotto and Dell’Arciprete, 2012) representing syntactic information with distributional semantic vectors representing semantic information. The computation of the final matrix for each sentence is done compositionally. 3.2 We describe here the approach in a few sentences. In line with tree kernels over structures (Collins and Duffy, 2002), we introduce the set S(t) of the subtrees ti of a given lexicalized tree t. A subtree ti is in the set S(t) if s(ti ) is a subtree of s(t) and, if n is a node in ti , all the siblings of n in t are in ti . For each node of ti we only consider its syntactic label sn , except for the head h(ti ) for which we also consider its semantic component wn (see Fig. 1). The functions DSTs we define compute the following: X DST (t) = T = Ti S:booked::v ``` ` ` VP:booked::v XXX   X PRP:we::p V:booked::v NP:flight::n   PP  P NP:we::p We booked DT:the::d NN:flight::n the flight Figure 2: A lexicaliz"
S14-2049,D09-1010,1,0.866594,"Missing"
S14-2049,J03-4003,0,0.0151756,"Missing"
S14-2049,W05-1203,0,0.0240269,"each feature represents a rule with variables, i.e. a first order rule that is activated by the pairs if the variables are unified. This feature space has been introduced in (Zanzotto and Moschitti, 2006) and shown to improve over the one above. Each feature hf r1 , f r2 i is a pair of syntactic tree fragments augmented with variables. The feature is active for a pair (t1 , t2 ) if the syntactic interpretations of t1 and t2 can be unified with < f r1 , f r2 >. For example, consider the following feature: S S PP  P Weighted Token Similarity (WTS) h This similarity model was first defined bt Corley and Mihalcea (2005) and since then has been used by many RTE systems. The model extends a classical bag-of-word model to a Weighted-Bag-of-Word (wbow) by measuring similarity between the two sentences of the pair at the semantic level, instead of the lexical level. For example, consider the pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, a bag-of-word model would assign a low score, since many words are not shared across the two sentences. wbow fixes this problem by matching ‘Oscar’-‘Academy Awar"
S14-2049,P06-1051,1,0.766733,"d hyperonymy matches: in our experiments we specifically use Jiang&Conrath similarity (Jiang and Conrath, 1997). ing models, the task is framed as a multiclassification problem. The difficulty is to determine the best feature space on which to train the classifier. A full-fledged RTE systems based on machine learning that has to deal with natural occurring text is generally based on: • some within-pair features that model the similarity between the sentence a and the sentence b 2.2 • some features representing more complex information of the pair (a, b) such as rules with variables that fire (Zanzotto and Moschitti, 2006) In the following, we describe the within-pair feature and the syntactic rules with variable features used in the full-fledged RTE system. As the second space of features is generally huge, the full feature space is generally used in kernel machines where the final kernel between two instances p1 = (a1 , b1 ) and p2 = (a2 , b2 ) is: K(p1 , p2 ) = F R(p1 , p2 ) + + (W T S(a1 , b1 ) · W T S(a2 , b2 ) + 1)2 where F R counts how many rules are in common between p1 and p2 and W T S computes a lexical similarity between a and b. In the following sections we describe the nature of W T S and of F R 2."
S14-2049,P13-4006,0,0.0610506,"Missing"
S14-2049,C14-1068,1,0.841244,", whereas T represents the matrix of a tree t encoding structure and distributional meaning. The above full-fledged RTE system, although it may use distributional semantics, is not a model that applies a compositional distributional semantic model as it does not explicitly transform sentences in vectors, matrices, or tensors that represent their meaning. We here propose a model that can be considered a compositional distributional semantic model as it transforms sentences into matrices that are then used by the learner as feature vectors. Our model is called Distributed Smoothed Tree Kernel (Ferrone and Zanzotto, 2014) as it mixes the distributed trees (Zanzotto and Dell’Arciprete, 2012) representing syntactic information with distributional semantic vectors representing semantic information. The computation of the final matrix for each sentence is done compositionally. 3.2 We describe here the approach in a few sentences. In line with tree kernels over structures (Collins and Duffy, 2002), we introduce the set S(t) of the subtrees ti of a given lexicalized tree t. A subtree ti is in the set S(t) if s(ti ) is a subtree of s(t) and, if n is a node in ti , all the siblings of n in t are in ti . For each node"
S14-2049,W07-1401,0,0.0152416,"emantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods. 1 Introduction Recognizing Textual Entailment is a largely explored problem (Dagan et al., 2013). Past challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) explored methods and models applied in complex and natural texts. In this context, machine learning solutions show interesting results. The Shared Task #1 of SemEval instead wants to explore systems in a more controlled textual environment where the phenomena to model are clearer. The aim of the Shared Task is to study how RTE systems built upon compositional distributional semantic models behave 2 A Standard full-fledged Machine Learning Approach for RTE For now on, the task of recognizing textual entailment (RTE) is defined as the task to decide if a pair p = (a, b) like: (“Two children are"
S14-2049,O97-1002,0,0.225319,"ttribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/ 4.0/ is in entailment, in contradiction, or neutral. As in the tradition of applied machine learn300 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 300–304, Dublin, Ireland, August 23-24, 2014. these matches, wbow relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&Conrath similarity (Jiang and Conrath, 1997). ing models, the task is framed as a multiclassification problem. The difficulty is to determine the best feature space on which to train the classifier. A full-fledged RTE systems based on machine learning that has to deal with natural occurring text is generally based on: • some within-pair features that model the similarity between the sentence a and the sentence b 2.2 • some features representing more complex information of the pair (a, b) such as rules with variables that fire (Zanzotto and Moschitti, 2006) In the following, we describe the within-pair feature and the syntactic rules wit"
S14-2049,P03-1054,0,0.0125424,"count vectors were transformed where s(ti ) are distributed tree fragment into positive Pointwise Mutual Information (Zanzotto and Dell’Arciprete, 2012) for the → scores and reduced to 300 dimensions by subtree t and wh(ti ) is the distributional Singular Value Decomposition. This setup vector of the head of the subtree t. Diswas picked without tuning, as we found it tributed tree fragments have the property → → effective in previous, unrelated experiments. that s(ti )s(tj ) ≈ δ(ti , tj ). Thus, given the We parsed the sentence with the Stanford important property of the outer product Parser (Klein and Manning, 2003) and exthat applies in the Frobenius product: tracted the heads for use in the lexicalized →→> →→> → → → → h a w , b v iF = h a , b i · hw, v i. we have that trees with Collins’ rules (Collins, 2003). Equation 2 is satisfied as: Table 1 reports our results on the textual entailment classification task, together with the → → maximum, minimum and average score for the → → hTi , Tj iF = hs(ti ), s(tj )i · hwh(ti ) , wh(tj ) i challenge. The first observation is that the → → full-fledged RTE system is still definitely bet≈ δ(s(ti ), s(tj )) · hwh(ti ) , wh(tj ) i ter than our CDSM system. We belie"
S18-1076,C14-1068,1,0.838899,"Missing"
S18-1076,E17-2017,0,0.126585,"Missing"
S18-1076,S18-1003,0,0.29785,"etworks and, second, understanding if syntactic information is useful in modeling tweets for the specific task of emoji prediction. The architecture of SyntNN is then organized around two sub-networks (see Fig. 1): (1) a syntactic sub-network and (2) a semantic subnetwork. These two sub-networks are then merged to obtain the final classification layer. The rest of the section describes the two subnetworks and the merging layer. In this paper, we present SyntNN as a way to include traditional syntactic models in multilayer neural networks used in the task of Semeval Task 2 of emoji prediction (Barbieri et al., 2018). The model builds on the distributed tree embedder also known as distributed tree kernel (Zanzotto and Dell’Arciprete, 2012). Initial results are extremely encouraging but additional analysis is needed to overcome the problem of overfitting. 1 SyntNN: a Syntax-aware Multilayer Perceptron Introduction Syntactic models of language have always played a crucial role in many natural language processing tasks but, in recent years, it has been marginalized by the advent of neural networks and in particular long-short term memory (LSTM). These latter networks have had a tremendous impact on how lingu"
S18-1076,P02-1034,0,0.208097,"rent neural network. The key point is how to obtain the transformation of the symbolic representation of syntactic trees into tensor-based representations that have meaningful properties. The Distributed Tree Embedding Layer (DTE) (see Fig. 1) is the core component of the syntactic subnetwork. DTE is based on a technique to embed the structural information of syntactic tree into dense, low-dimensional vectors of real numbers (Zanzotto and Dell’Arciprete, 2012; Ferrone and Zanzotto, 2014; Zanzotto et al., 2015). This technique has been originated as a way to replace syntactic kernel functions (Collins and Duffy, 2002) in kernel machines (Cristianini and Shawe-Taylor, 2000) but it can be seen as a principled way to In this paper, we want to explore the use of “traditional” syntactic information within a neural network framework in the task of Semeval Task 2 of emoji prediction (Barbieri et al., 2018, 2017). We propose SyntNN as a way to include traditional syntactic models in multilayer neural networks. The model builds on the distributed tree embedder also known as distributed tree kernel (Zanzotto and Dell’Arciprete, 2012; Ferrone and Zanzotto, 2014; Zanzotto et al., 2015) that is a way to transpose synta"
shehata-zanzotto-2006-dependency,W00-1903,0,\N,Missing
shehata-zanzotto-2006-dependency,J93-2004,0,\N,Missing
shehata-zanzotto-2006-dependency,bosco-etal-2000-building,0,\N,Missing
shehata-zanzotto-2006-dependency,J03-4003,0,\N,Missing
W01-1013,2000.iwpt-1.7,0,0.155609,"Missing"
W01-1013,P00-1064,0,0.0912682,"Missing"
W01-1013,O98-4002,1,0.865745,"Missing"
W01-1013,W98-0705,0,0.144557,"Missing"
W01-1013,M95-1017,0,\N,Missing
W04-2510,A97-1012,0,0.0453747,"Missing"
W04-2510,M98-1007,0,0.0923267,"Missing"
W04-2510,W01-1205,0,0.0851294,"Missing"
W04-2510,P01-1037,0,\N,Missing
W04-2510,M95-1017,0,\N,Missing
W05-1207,P91-1034,0,0.256981,"Missing"
W05-1207,W04-3205,0,0.022223,"lations between verb pairs based on what we call textual entailment pattern. In this work we defined a first kernel of textual entailment patterns based on subject-verb relations. Potentials of the method are still high as different kinds of textual entailment patterns may be defined or discovered investigating relations between sentences and subsentences as done in (Lapata and Lascarides, 2004) for temporal relations or between near sentences as done in (Basili et al., 2003) for cause-effect relations between domain events. Some interesting and simple inter-sentential patters are defined in (Chklovski and Pantel, 2004). Moreover, with respect to anchorbased approaches, the method we presented here offers a different point of view on the problem of acquiring textual entailment relation prototypes, as textual entailment patterns do not depend on the repetition of “similar” facts. This practically independent view may open the possibility to experiment co-training algorithms (Blum and Mitchell, 1998) also in this area. Finally, the approach proposed can be useful to define better probability estimations in probabilistic entailment detection methods such as the one described in (Glickman et al., 2005). Referenc"
W05-1207,P89-1010,0,0.0823407,"ure is often positively used in terminology extraction (e.g., (Daille, 1994)). Secondly, another measure Smi (vt , vh ) related to point-wise mutual information (Fano, 1961) may be also used. Given the possibility of estimating the probabilities through maximum-likelihood principle, the definition is straightforward: Smi (vt , vh ) = log10 p(Ppers (vt , vh )) p(Fpers (vt ))p(F(vh )) where p(x) = fC (x)/fC (.). The aim of this measure is to indicate the relatedness between two elements composing a pair. Mutual information has been positively used in many NLP tasks such as collocation analysis (Church and Hanks, 1989), terminology extraction (Damerau, 1993), and word sense disambiguation (Brown et al., 1991). 3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge (the expected structure of the searched collocations, i.e., the textual entailment patterns) and partially on some probability distribution estimation. Only a positive combination of both these two ingredients can give good results when applying (and evaluating) the model. The aim of the experimental evaluation is then to understand, on the one side,"
W05-1207,C92-2082,0,0.065585,"property of playing). This limitation can be overcome when agentive nouns such as runner play subject roles in some sentences. Agentive nouns usually denote the “doer” or “performer” of some action a. This is exactly what is needed to make clearer the relevant property of the noun playing the logical subject role, in order to discover entailment. The action a will be the one entailed by the verb heading the sentence. For example, in “the player wins”, the action play evocated by the agentive noun player is entailed by win. 2.2 Textual entailment patterns As observed for the isa relations in (Hearst, 1992) local and simple inter-sentential patterns may carry relevant semantic relations. As we saw in the previous section, this also happens for entailment relations. Our aim is thus to search for an initial set of textual patterns that describe possible linguistic forms expressing entailment relations between two verbs (vt , vh ). By using these patterns, actual pointwise assertions of entailment can be detected or verified in texts. We call these prototypical patterns textual entailment patterns. The idea described in Sec. 2.1 can be straightforwardly applied to generate textual entailment patter"
W05-1207,J03-3005,0,0.0367205,"esults when applying (and evaluating) the model. The aim of the experimental evaluation is then to understand, on the one side, if the proposed textual entailment patterns are useful to detect entailment between verbs and, on the other, if a statistical measure is preferable with respect to the other. We will here evaluate the capability of our method to recognise entailment between given pairs of verbs. We carried out the experiments using the web as the corpus C where to estimate our two textual entailment measures (Sf and Smi ) and GoogleT M as a count estimator. The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. As test bed we used existing resources: a non trivial set of controlled verb entailment pairs is in fact contained in WordNet (Miller, 1995). There, the entailment relation is a semantic relation defined at the synset level, standing in the verb subhierarchy. Each between the True Set and the Control Set is empty, we are not completely sure that the Control Set does not contains any pair where the entailment relation holds. What we may assume is that this last"
W05-1207,N04-1020,0,0.0193677,"ncy in the corpus. Finally, in this case, mutual information appears to be a better indicator for the entailment relation with respect to the frequency. 4 Conclusions We have defined a method to recognise and extract entailment relations between verb pairs based on what we call textual entailment pattern. In this work we defined a first kernel of textual entailment patterns based on subject-verb relations. Potentials of the method are still high as different kinds of textual entailment patterns may be defined or discovered investigating relations between sentences and subsentences as done in (Lapata and Lascarides, 2004) for temporal relations or between near sentences as done in (Basili et al., 2003) for cause-effect relations between domain events. Some interesting and simple inter-sentential patters are defined in (Chklovski and Pantel, 2004). Moreover, with respect to anchorbased approaches, the method we presented here offers a different point of view on the problem of acquiring textual entailment relation prototypes, as textual entailment patterns do not depend on the repetition of “similar” facts. This practically independent view may open the possibility to experiment co-training algorithms (Blum and"
W05-1207,P02-1006,0,0.0321785,"ling of Semantic Equivalence and Entailment, pages 37–42, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics {Yahoo, Overture}, that are used to retrieve in the corpus text fragments where they co-occur, e.g. “Yahoo purchased Overture (July 2003).”, “Now that Overture is completely owned by Yahoo!...”. These retrieved text fragments are then considered good candidate for paraphrasing X bought Y. Anchor-based learning methods have been used to investigate many semantic relations ranging from very general ones as the isa relation in (Morin, 1999) to very specific ones as in (Ravichandran and Hovy, 2002) where paraphrases of question-answer pairs are searched in the web or as in (Szpektor et al., 2004) where a method to scan the web for searching textual entailment prototype relations is presented. These methods are mainly devoted to induce entailment pairs related to the first kind of textual entailment, that is, paraphrasing as their target is mainly to look for the same “fact” in different textual forms. Incidentally, these methods can come across strict entailment relations whenever specific anchors are used for both a fact ft and a strictly entailed fact fh . In this work we will investi"
W05-1207,W04-3206,0,0.0256276,"putational Linguistics {Yahoo, Overture}, that are used to retrieve in the corpus text fragments where they co-occur, e.g. “Yahoo purchased Overture (July 2003).”, “Now that Overture is completely owned by Yahoo!...”. These retrieved text fragments are then considered good candidate for paraphrasing X bought Y. Anchor-based learning methods have been used to investigate many semantic relations ranging from very general ones as the isa relation in (Morin, 1999) to very specific ones as in (Ravichandran and Hovy, 2002) where paraphrases of question-answer pairs are searched in the web or as in (Szpektor et al., 2004) where a method to scan the web for searching textual entailment prototype relations is presented. These methods are mainly devoted to induce entailment pairs related to the first kind of textual entailment, that is, paraphrasing as their target is mainly to look for the same “fact” in different textual forms. Incidentally, these methods can come across strict entailment relations whenever specific anchors are used for both a fact ft and a strictly entailed fact fh . In this work we will investigate specific methods to induce the second kind of textual entailment relations, that is, strict ent"
W05-1207,J90-1003,0,\N,Missing
W06-3806,A00-2018,0,0.00977345,"Missing"
W06-3806,W05-1203,0,0.0558672,"ching tecniques are not sufficient as these may only detect the structural similarity between sentences of textual entailment pairs. An extension is needed to consider also if two pairs show compatible relations between their sentences. In this paper, we propose to observe textual entailment pairs as pairs of syntactic trees with co-indexed nodes. This shuold help to cosider both the structural similarity between syntactic tree pairs and the similarity between relations among sentences within a pair. Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. We experimented with such kernel using Support Vector Machines on the Recognizing Textual Entailment (RTE) challenge test-beds. The comparative results show that (a) we have designed an effective way to automatically learn entailment rules 33 Workshop on TextGraphs, at HLT-NAACL 2006, pages 33–36, c New York City, June 2006. 2006 Association for Computational Linguistics from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-ofthe-art models. In the remainder of this paper, Sec. 2 introduces the cross-pair simila"
W06-3806,O97-1002,0,0.0604873,"etween T1 and H1 . These placeholders are then used to augment tree nodes. To better take into account argument movements, placeholders are propagated in the syntactic trees following constituent heads (see Fig. 1). In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dectors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivationally related form relation and the verb entailment relation in WordNet, and, fi34 nally, a WordNet-based similarity (Jiang and Conrath, 1997). Each of these detectors gives a different weight to the anchor: the actual computed similarity for the last and 1 for all the others. These weights will be used in the final kernel. 2.2 Similarity between pairs of co-indexed trees Pairs of syntactic trees where nodes are co-indexed with placeholders allow the design a cross-pair similarity that considers both the structural similarity and the intra-pair word movement compatibility. Syntactic trees of texts and hypotheses permit to verify the structural similarity between pairs of sentences. Texts should have similar structures as well as hyp"
W06-3806,P04-1043,1,0.874312,"Missing"
W06-3806,N04-3012,0,0.0623781,"Missing"
W07-1412,P02-1034,0,0.0394357,": E1 and E2 share the following subtrees: T3 “For my younger readers, Chapman killed John Lennon more than twenty years ago.” “John Lennon died more than twenty years ago.” H6 “Cows are vegetarian but, to save money on mass-production, farmers fed cows animal extracts.” “Cows have eaten animal extracts.” (E6 ) but it will clearly fail when used for: (R3 ) T7 This is the rewrite rule they have in common. Then, E2 can be likely classified as a valid entailment, as it shares the rule with the valid entailment E1 . The cross-pair similarity model uses: (1) a tree similarity measure KT (τ1 , τ2 ) (Collins and Duffy, 2002) that counts the subtrees that τ1 and τ2 have in common; (2) a substitution function t(·, c) that changes names of the placeholders in a tree according to a set of correspondences between placeholders c. Given C as the collection of all correspondences between the placeholders of (T 0 , H 0 ) and (T 00 , H 00 ), the cross-pair similarity is computed as: KS ((T 0 , H 0 ), (T 00 , H 00 )) = maxc∈C (KT (t(T 0 , c), t(T 00 , c)) + KT (t(H 0 , c), t(H 00 , c))) (1) The cross-pair similarity KS , used in a kernel-based learning model as the support vector machines, allows the exploitation of implici"
W07-1412,W05-1203,0,0.0239919,"Missing"
W07-1412,E06-1015,1,0.85746,"he computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS ((T 0 , H 0 ), (T 00 , H 00 )). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72–77, c Prague, June 2007. 2007 Association for Computational Linguistics"
W07-1412,P06-1051,1,0.786744,") we reduce the computational cost of the cross-pair similarity computation algorithm to allow the learning over larger training sets. The paper is organized as follows: in Sec. 2 we review the cross-pair similarity model and its limits; in Sec. 3, we introduce our model for typed anchors; in Sec. 4 we describe how we limit the computational cost of the similarity; in Sec. 5 we present the two submission experiments, and in Sec. 6 we draw some conclusions. 2 Cross-pair similarity and its limits 2.1 Learning entailment rules with syntactic cross-pair similarity The cross-pair similarity model (Zanzotto and Moschitti, 2006) proposes a similarity measure aiming at capturing rewrite rules from training examples, computing a cross-pair similarity KS ((T 0 , H 0 ), (T 00 , H 00 )). The rationale is that if two pairs are similar, it is extremely likely that they have the same entailment value. The key point is the use of placeholders to mark the relations between the sentence words. A placeholder co-indexes two substructures in the parse trees of text and hypothesis, 72 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72–77, c Prague, June 2007. 2007 Association for Computational Linguistics"
W08-2004,H05-1079,0,0.0150814,"ls for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 ⇒ H 1 T1 “Wanadoo bought KStones” H1 “Wanadoo owns KStones” where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Pro"
W08-2004,P02-1034,0,0.466751,"Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25–32 Manchester, August 2008 For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment"
W08-2004,W05-1203,0,0.0504424,"using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation. 1 Introduction Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out (Bar Haim et al., 2006). The aim is to detect implications between sentences like: T1 ⇒ H 1 T1 “Wanadoo bought KStones” H1 “Wanadoo owns KStones” where T1 and H1 stand for text and hypothesis, respectively. Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed (Corley and Mihalcea, 2005; Glickman and Dagan, 2004; de Salvo Braz et al., 2005; Bos and Markert, 2005). However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 ∗ V is larger than the actual space, which is the one of all possible subsequences with gaps, i.e. it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceeding"
W08-2004,P06-1117,1,0.782749,"eed to represent it in the learning algorithm; for this purpose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we re"
W08-2004,W04-2403,1,0.858821,"methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the set of fragments {f1 , f2 , ."
W08-2004,W06-2909,1,0.824703,"roach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and Duffy, 2002). Given the s"
W08-2004,P07-1098,1,0.823458,"pose, an interesting approach is based on kernel methods. Since the considered graphs are composed by only two trees, we can carried out a simplified computation of a graph kernel based on tree kernel pairs. VP VP  entailment and derivational morphology are applied. NP  n V VP NP , V NP = book D N , D N D N a flight NP NP a flight NP NP D N , DN , D N , D N a flight a o N , , ... flight flight Figure 1: A syntactic parse tree. 3.1 Tree Kernels 3 Kernels over Semantic Tree Pair-based Graphs Tree Kernels (e.g. see NLP applications in (Giuglea and Moschitti, 2006; Zanzotto and Moschitti, 2006; Moschitti et al., 2007; Moschitti et al., 2006; Moschitti and Bejan, 2004)) represent trees in terms of their substructures (fragments) which are mapped into feature vector spaces, e.g. ℜn . The kernel function measures the similarity between two trees by counting the number of their common fragments. For example, Figure 1 shows some substructures for the parse tree of the sentence ""book a flight"". The main advantage of tree kernels is that, to compute the substructures shared by two trees τ1 and τ2 , the whole fragment space is not used. In the following, we report the formal definition presented in (Collins and D"
W08-2004,P06-1051,1,0.769809,". it only contains all possible concatenations of words respecting their order. 25 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25–32 Manchester, August 2008 For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees. In (Zanzotto and Moschitti, 2006), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses. In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment pair were generated causing a remarkable slowdown. In this paper, we describe the feature space of all possible tree fragment pairs and we show that it can be evaluated with a much simpler kernel than the one used in previous work, both in terms"
W09-0209,C92-2082,0,0.401363,"ies 2 Unsupervised feature selection with for specific domains is then a very interesting Singular Value Decomposition area of research (O’Sullivan et al., 1995; Magnini Singular value decomposition (SVD) is one of the and Speranza, 2001; Snow et al., 2006). Autopossible factorization of a rectangular matrix that matic methods for learning taxonomies from corhas been largely used in information retrieval for pora often use distributional hypothesis (Harris, reducing the dimension of the document vector 1964) and exploit some induced lexical-syntactic space (Deerwester et al., 1990). patterns (Hearst, 1992; Pantel and Pennacchiotti, The decomposition can be defined as follows. 2006). In these models, within a very large set, Given a generic rectangular n × m matrix A, its candidate word pairs are selected as new word singular value decomposition is: pairs in hyperonymy and added to an existing taxonomy. Candidate pairs are represented in some A = U ΣV T feature space. Often, these feature spaces are where U is a matrix n × r, V T is a r × m and Σ huge and, then, models may take into considerais a diagonal matrix r × r. The two matrices U tion noisy features. and V are unitary, i.e., U T U = I a"
W09-0209,N04-1016,0,0.0321489,"0 0 0 0 1 0 2 0 2 1 0 0 1 0 0 0 0 0 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Concrete nouns boat bowl cat chicken corn cup duck elephant helicopter knife lion mushroom owl pear pencil pig potato scissors ship spoon telephone turtle Clas Artifact Artifact Animal Animal Vegetable Artifact Animal Animal Artifact Artifact Animal Vegetable Animal Vegetable Artifact Animal Vegetable Artifact Artifact Artifact Artifact Animal Sense 0 0 0 1 2 0 0 0 0 0 0 4 0 0 0 0 2 0 0 0 1 1 Table 2: Concrete nouns, Classes and senses selected in WordNet are good models for natural language (Lapata and Keller, 2004). As the focus of the paper is the analysis of the effect of the SVD feature selection, we used as feature spaces both n-grams and bag-of-words. Out of the T ∪ T , we selected only those pairs that appeared at a distance of at most 3 tokens. Using these 3 tokens, we generated three spaces: (1) 1-gram that contains monograms, (2) 2-gram that contains monograms and bigrams, and (3) the 3-gram space that contains monograms, bigrams, and trigrams. For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in"
W09-0209,P06-1015,0,0.151751,"Missing"
W09-0209,P06-1101,0,0.472076,"nfo.uniroma2.it Abstract In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances. 1 Introduction Fabio Massimo Zanzotto Disp, University “Tor Vergata” Rome, Italy zanzotto@info.uniroma2.it probabilistic taxonomy learning models. Given the probabilistic taxonomy learning model introduced by (Snow et al., 2006), we leverage on the computation of logistic regression to exploit singular value decomposition (SVD) as unsupervised feature selection. SVD is used to compute the pseudo-inverse matrix needed in logistic regression. To describe our idea, we firstly review how SVD can be used as unsupervised feature selection (Sec. 2). In Section 3 we then describe the probabilistic taxonomy learning model introduced by (Snow et al., 2006). We will then shortly review the logistic regression used to compute the taxonomy learning model to describe where SVD can be naturally used. We will describe our experiment"
W10-3504,W05-1209,0,0.028088,"tailment. A key issue for the future development of RTE is then the creation of datasets fulfilling two properties: (1) large size; (2) homogeneity wrt. existing RTE corpora. The task of creating large datasets is unfeasible for human annotators. Collaborative annotation environments such as the Amazon Mechanical Turk1 can help to annotate pairs of sentences in positive or negative entailment (Zaenen, submitted; Snow et al., 2008). Yet, these environments can hardly solve the problem of finding relevant pairs of sentences. Completely automatic processes of dataset creation have been proposed (Burger and Ferro, 2005; Hickl et al., 2006). Unfortunately, these datasets are not homogeneous wrt. to the RTE datasets, as they are In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora. 1"
W10-3504,A00-2018,0,0.0215194,"ogy. 4 Annotators were initially trained on a small development corpus of 200 pairs. The inter-annotator agreement on this set, computed using the Kappa-statistics (Siegel and Castellan, 1988), was 0.60 corresponding to substantial agreement, 33 Training Corpus RTE-2 RTE-1 RTE-3 wiki news RTE-2+RTE-1 RTE-2+RTE-3 RTE-2+news RTE-2+wiki RTE-1+wiki RTE-3+wiki The corpus has been randomly split in three equally numerous parts: development, training, and testing. We kept aside the development to design the features, while we used training and testing for the experiments. We use the Charniak Parser (Charniak, 2000) for parsing sentences, and SVM-light (Joachims, 1999) extended with the syntactic first-order rule kernels described in (Zanzotto and Moschitti, 2006; Moschitti and Zanzotto, 2007) for creating the FOSR feature space. 5.2 Accuracy 60.62 51.25 57.25 56.00 53.25 58.5 59.62 56.75 59.25 53.37 59.00 Table 1: Accuracy of different training corpora over RTE-2 test. Experimental Results In a second experiment we aim at checking if WIKI is homogeneous to the RTE challenge corpora – i.e. if it contains (T, H) pairs similar to those of the RTE corpora. If this holds, we would expect the performance of t"
W10-3504,J03-3005,0,0.0157567,"sing the size of labelled corpora. The algorithm can be applied only under a specific applicability condition: corpus’ instances must have two independent views, i.e. they can be modeled by two independent feature sets. We here adopt a slightly modified version of the Unlike the LCC corpus where negative and positive examples are clearly separated, the WIKI corpus mixes the two sets – i.e. it is unlabelled. In order to exploit the WIKI corpus in the RTE task, one should either manually annotate the corpus, 2 It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). 31 cotraining algorithm, as described in Fig.2. Under the applicability condition, instances are modeled on a feature space F = F1 × F2 × C, where F1 and F2 are the two independent views and C is the set of the target classes (in our case, true and false entailment). The algorithm starts with an initial set of training labelled examples L and a set of unlabelled examples U . The set L is copied in two sets L1 and L2 , used to train two different classifiers h1 and h2 , respectively using views F1 and F2 . The two classifiers are used to classify the unlabelled set U , obtaining two different"
W10-3504,W99-0613,0,0.0749473,"ify the unlabelled set U , obtaining two different classifications, U1 and U2 . Then comes the cotraining step: the k-best classified instances in U1 are added to L2 and feed the learning of a new classifier h2 on the feature space F2 . Similarly, the k-best instances in U2 are added to L1 and train a new classifier h1 on F1 . The procedure repeats until a stopping condition is met. This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). The final outcome of co-training is the new set of labelled examples L1 ∪ L2 and the two classifier h1 and h2 , obtained from the last iteration. 4.2 The MITRE corpus is extracted using two subsequent sentences, the title and the first paragraph. The LCC negative corpus is extracted using two correlated sentences or subsentences. Also in these two cases, it is very hard to find a view that is independent from the space of the sentence pairs. None of the existing RTE corpora can then be used for co-training. In the next section we show that this is not the case for the WIKI corpus. 4.3 Creati"
W10-3504,D08-1027,0,0.00524732,"Missing"
W10-3504,W07-1401,0,0.0278802,"Missing"
W10-3504,D09-1010,1,0.868289,"Missing"
W10-3504,P06-1051,1,0.857365,"mposed only by the textual fragments of T and H, i.e. the only information available are the two pieces of texts, from which it is difficult to extract completely independent sets of features, as linguistic features tend to be dependent. 4.3.1 Content-pair view The content-pair view is the classical view used in RTE. The original entry S1 represents the Text T , while the revision S2 is the Hypothesis H. Any feature space of those reported in the textual entailment literature could be applied. We here adopt the space that represents first-order syntactic rewrite rules (FOSR), as described in (Zanzotto and Moschitti, 2006). In this feature space, each feature represents a syntactic first-order or 32 5.1 grounded rewrite rule. For example, the rule: ρ = l → r= NP X In order to check the above claims, we need to experiment with both manually labelled and unlabelled corpora. As unlabelled corpora we adopt: S S VP VBP bought NP Y → NP Y VP VBP NP X owns wiki unlabelled: An unlabelled WIKI corpus of about 3,000 examples. The corpus has been built by downloading 40,000 Wikipedia pages dealing with 800 entries about politics, scientific theories, and religion issues. We extracted original entries and revisions from th"
W10-3504,W07-1428,0,0.0916708,"Missing"
W10-3504,P02-1046,0,\N,Missing
W11-1302,D10-1115,0,0.126327,"Missing"
W11-1302,P02-1034,0,0.809333,"of sentences plays a very important role. Understanding the relation between the structure and the meaning is needed for building distributional compositional models for sentences. Stemming from distributed representation theories, we investigate the interaction between distributed structure and distributional meaning. We propose a pure distributed tree (DT) and distributional distributed tree (DDT). DTs and DDTs are exploited for defining distributed tree kernels (DTKs) and distributional distributed tree kernels (DDTKs). We compare DTKs and DDTKs in two tasks: approximating tree kernels TK (Collins and Duffy, 2002); performing textual entailment recognition (RTE). Results show that DTKs correlate with TKs and perform in RTE better than DDTKs. Then, including distributional vectors in distributed structures is a very difficult task. 1 Research in Distributed Representations (DR) (Hinton et al., 1986) proposed models and methods for encoding data structures in vectors, matrices, or high-order tensors. Distributed Representations are oriented to preserve the structural information in the final representation. For this purpose, DR models generally use random and possibly orthogonal vectors for words and str"
W11-1302,W05-1203,0,0.405682,"om vectors as the other nodes. In DDTs, these vectors are distributional vectors obtained on a corpus with an LSA reduction (Deerwester et al., 1990). 12 (5) where K(·, ·) is a generic kernel. We will then experiment with different P K kernels obtained using: the original tree kernel function (TK) (Collins and Duffy, 2002), DTK, and DDTK. Along with the previous task specific kernels, we use a simpler feature (Lex) that is extremely effective in determining the entailment between T and H. This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). This feature, hereafter called Lex, encodes the similarity between T and H, i.e., sim(T, H). This feature is used alone or in combination with the previous kernels and it gives an important boost to their performances. In the task experiment, we will then also have: Lex+TK, Lex+DTK, and Lex+DDTK. 4 Experimental Evaluation In this section, we experiment with the distributed tree kernels (DTK) and the distributional distributed tree kernels (DDTK) in order to understand whether or not the syntactic structure and the distributional meaning can be easily encoded in the distributed trees. We will"
W11-1302,W07-1401,0,0.0105055,"d for the two settings (Sec. 4.1). Secondly, we report on the experimental results (Sec. 4.2). 4.1 Experimental Set-up We have the double aim of producing a direct comparison of how the distributed tree kernel (DTK) is approximating the original tree kernel (TK) and a task based comparison for assessing if the approximation is enough effective to similarly solve the task that is textual entailment recognition. For both experimental settings, we take the recognizing textual entailment sets ranging from the first challenge (RTE-1) to the fifth (RTE-5) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The distributional vectors used for DDTK have been obtained by an LSA reduction of the word-byword cooccurrence matrix generated on the UKWaC corpus (Ferraresi et al., 2008), using a context window of size 3. An appropriate size for the LSA reduction was deemed to be 250. Thus, in the experiments we used 250 dimensions both for distributional and random vectors, to allow a correct comparison between DTK and DDTK models. For the direct comparison, we used tree pairs derived from the RTE sets. Each pair is derived from a T-H pair where T and H are syntactically analyz"
W11-1302,W10-2805,0,0.0548485,"Missing"
W11-1302,N10-1146,1,0.913482,"Missing"
W11-1302,E06-1015,0,0.0152788,"γ is a value to ensure that the operation ⊗ approximate the property of vector module preservation. This function is not commutative and this guarantees that different trees t have different vectors ~t. It is possible to demonstrate that: ~ Te = X ~t 3.2.1 Entailment-specific Kernels Recognizing textual entailment (RTE) is a complex semantic task often interpreted as a classification task. Given the text T and the hypothesis H determine whether or not T entails H. For applying the previous kernels to this classification task, we need to define a specific class of kernels. As in (Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Zanzotto et al., 2009), we encode the text T and the hypothesis H in two separate syntactic feature spaces. Then, given two pairs of text-hypothesis P1 = (T1 , H1 ) and P2 = (T2 , H2 ), the prototypical kernel P K is written as follows: (3) t∈S(T ) P K(P1 , P2 ) = K(T1 , T2 ) + K(H1 , H2 ) where S(T ) is the set of the subtrees of T , t is one of its subtrees, and ~t is its distributed representation. The distributed kernel Tg K function over trees then easily follows as: ~ f ~ f Tg K(T1 , T2 ) = T 1 · T2 = X X t~1 · t~2 (4) t1 ∈S(T1 ) t2 ∈S(T2 ) If the different tree"
W11-1302,W07-1406,0,0.0136732,"ensure that the operation ⊗ approximate the property of vector module preservation. This function is not commutative and this guarantees that different trees t have different vectors ~t. It is possible to demonstrate that: ~ Te = X ~t 3.2.1 Entailment-specific Kernels Recognizing textual entailment (RTE) is a complex semantic task often interpreted as a classification task. Given the text T and the hypothesis H determine whether or not T entails H. For applying the previous kernels to this classification task, we need to define a specific class of kernels. As in (Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Zanzotto et al., 2009), we encode the text T and the hypothesis H in two separate syntactic feature spaces. Then, given two pairs of text-hypothesis P1 = (T1 , H1 ) and P2 = (T2 , H2 ), the prototypical kernel P K is written as follows: (3) t∈S(T ) P K(P1 , P2 ) = K(T1 , T2 ) + K(H1 , H2 ) where S(T ) is the set of the subtrees of T , t is one of its subtrees, and ~t is its distributed representation. The distributed kernel Tg K function over trees then easily follows as: ~ f ~ f Tg K(T1 , T2 ) = T 1 · T2 = X X t~1 · t~2 (4) t1 ∈S(T1 ) t2 ∈S(T2 ) If the different trees are orthogonal, Tg K(T"
W11-1302,P06-1051,1,0.816955,"he space; and γ is a value to ensure that the operation ⊗ approximate the property of vector module preservation. This function is not commutative and this guarantees that different trees t have different vectors ~t. It is possible to demonstrate that: ~ Te = X ~t 3.2.1 Entailment-specific Kernels Recognizing textual entailment (RTE) is a complex semantic task often interpreted as a classification task. Given the text T and the hypothesis H determine whether or not T entails H. For applying the previous kernels to this classification task, we need to define a specific class of kernels. As in (Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Zanzotto et al., 2009), we encode the text T and the hypothesis H in two separate syntactic feature spaces. Then, given two pairs of text-hypothesis P1 = (T1 , H1 ) and P2 = (T2 , H2 ), the prototypical kernel P K is written as follows: (3) t∈S(T ) P K(P1 , P2 ) = K(T1 , T2 ) + K(H1 , H2 ) where S(T ) is the set of the subtrees of T , t is one of its subtrees, and ~t is its distributed representation. The distributed kernel Tg K function over trees then easily follows as: ~ f ~ f Tg K(T1 , T2 ) = T 1 · T2 = X X t~1 · t~2 (4) t1 ∈S(T1 ) t2 ∈S(T2 ) If the different tree"
W11-1302,C10-1142,1,0.830581,"Missing"
W13-3205,J02-3001,0,0.0849595,"lic syntactic interpretations. Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information. 1 Introduction Syntactic processing is widely considered an important activity in natural language understanding (Chomsky, 1957). Research in natural language processing (NLP) exploits this hypothesis in models and systems. Syntactic features improve performance in high level tasks such as question answering (Zhang and Lee, 2003), semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Moschitti et al., 2008; Collobert et al., 2011), paraphrase detection (Socher et al., 2011), and textual entailment recognition (MacCartney et al., 2006; Wang and Neumann, 2007; Zanzotto et al., 2009). Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures. The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors. 40 Proceedings of the Workshop on Continuous Vector Space Models and their Composit"
W13-3205,J04-4004,0,0.0196538,"istical package (Mevik and Wehrens, 2007). 44 dim with λ values in the range 0–0.4. System Comparison We compared the DRPs against the existing way of producing distributed trees (based on the recent paper described in (Zanzotto and Dell’Arciprete, 2012)): distributed trees are obtained using the output of a symbolic parser (SP) that is then transformed into a distributed tree using the DT with the appropriate λ. We refer to this chain as the Distributed Symbolic Parser (DSP ). The DSP is then the chain DSP (s) = DT (SP (s)) (see Figure 1). As for the symbolic parser, we used Bikel’s version (Bikel, 2004) of Collins’ head-driven statistical parser (Collins, 2003). For a correct comparison, we used the Bikel’s parser with oracle partof-speech tags. We experimented with two versions: (1) a lexicalized method DSPlex , i.e., the natural setting of the Collins/Bikel parser, and (2) a fully non-lexicalized version DSPno lex that exploits only part-of-speech tags. We obtained this last version by removing words in input sentences and leaving only part-of-speech tags. We trained these DSP s on P Ttrain . 4096 8192 Model DRP3 DSPlex ; λ=0 0.7192 0.9073 λ = 0.2 0.6406 0.8564 λ = 0.4 0.0646 0.6459 Estima"
W13-3205,A00-2018,0,0.04181,"again explicit, through smaller but expressive vectors. 3 Distributed Representation Parsers In this section, first, we sketch the idea of Distributed Representation “Parsers” (DRPs). Then, we review the distributed trees as a way to represent trees in low dimensional vectors. Finally, we describe how to build DRPs by mixing a function that encodes sentences in vectors and a linear regressor that can be induced from training data. 3.1 The Idea The approach to using syntax in learning algorithms generally follows two steps: first, parse sentences s with a symbolic parser (e.g., (Collins, 2003; Charniak, 2000; Nivre et al., 2007)) and produce symbolic trees t; second, use an encoder to build syntactic feature vectors. Figure 1 sketches this idea ;when the final vectors are the distributed trees t ∈ Rd (Zanzotto and Dell’Arciprete, 2012)1 . In this case, the last step 1 To represent a distributed tree for a tree t, we use the ; notation t to stress that this small vector is an approximation of the original high-dimensional vector ~t in the space of tree 41 t S PP  P s “We booked the flight” → Symbolic Parser (SP) NP → ; VP !!aa PRP V NP Q  Q booked DT We → Distributed Tree Encoder (DT) NN →"
W13-3205,N06-1006,0,0.0634602,"Missing"
W13-3205,P02-1034,0,0.345728,"syntactic feature vectors. In this paper, we explore an alternative way to use syntax in feature spaces: the Distributed Representation Parsers (DRP). The core of the idea is straightforward: DRPs directly bridge the gap between sentences and syntactic feature spaces. DRPs act as syntactic parsers and feature extractors at the same time. We leverage on the distributed trees recently introduced by Zanzotto&Dell’Arciprete (2012) and on multiple linear regression models. Distributed trees are small vectors that encode the large vectors of the syntactic tree fragments underlying the tree kernels (Collins and Duffy, 2002). These vectors effectively represent the original vectors and lead to performances in NLP tasks similar to tree kernels. Multiple linear regression allows to learn linear DRPs from training data. We experiment with the Penn Treebank data set (Marcus et al., 1993). Results show that DRPs produce distributed trees significantly better than those obtained by existing methods, in the same non-lexicalized conditions, and competitive with those obtained by existing methods with lexical information. Finally, DRPs are extremely faster than existing methods. The rest of the paper is organized as follo"
W13-3205,J03-4003,0,0.140496,"sional vectors again explicit, through smaller but expressive vectors. 3 Distributed Representation Parsers In this section, first, we sketch the idea of Distributed Representation “Parsers” (DRPs). Then, we review the distributed trees as a way to represent trees in low dimensional vectors. Finally, we describe how to build DRPs by mixing a function that encodes sentences in vectors and a linear regressor that can be induced from training data. 3.1 The Idea The approach to using syntax in learning algorithms generally follows two steps: first, parse sentences s with a symbolic parser (e.g., (Collins, 2003; Charniak, 2000; Nivre et al., 2007)) and produce symbolic trees t; second, use an encoder to build syntactic feature vectors. Figure 1 sketches this idea ;when the final vectors are the distributed trees t ∈ Rd (Zanzotto and Dell’Arciprete, 2012)1 . In this case, the last step 1 To represent a distributed tree for a tree t, we use the ; notation t to stress that this small vector is an approximation of the original high-dimensional vector ~t in the space of tree 41 t S PP  P s “We booked the flight” → Symbolic Parser (SP) NP → ; VP !!aa PRP V NP Q  Q booked DT We → Distributed Tree Enco"
W13-3205,J93-2004,0,0.0459319,"DRPs act as syntactic parsers and feature extractors at the same time. We leverage on the distributed trees recently introduced by Zanzotto&Dell’Arciprete (2012) and on multiple linear regression models. Distributed trees are small vectors that encode the large vectors of the syntactic tree fragments underlying the tree kernels (Collins and Duffy, 2002). These vectors effectively represent the original vectors and lead to performances in NLP tasks similar to tree kernels. Multiple linear regression allows to learn linear DRPs from training data. We experiment with the Penn Treebank data set (Marcus et al., 1993). Results show that DRPs produce distributed trees significantly better than those obtained by existing methods, in the same non-lexicalized conditions, and competitive with those obtained by existing methods with lexical information. Finally, DRPs are extremely faster than existing methods. The rest of the paper is organized as follows. First, we present the background of our Classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors. In this paper, we explore an alternative path to use syntax in feature spaces: the Distributed Repr"
W13-3205,J08-2003,0,0.129019,"DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information. 1 Introduction Syntactic processing is widely considered an important activity in natural language understanding (Chomsky, 1957). Research in natural language processing (NLP) exploits this hypothesis in models and systems. Syntactic features improve performance in high level tasks such as question answering (Zhang and Lee, 2003), semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Moschitti et al., 2008; Collobert et al., 2011), paraphrase detection (Socher et al., 2011), and textual entailment recognition (MacCartney et al., 2006; Wang and Neumann, 2007; Zanzotto et al., 2009). Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures. The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors. 40 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 40–49, c Sofia, Bulgaria, Augu"
W13-3205,P04-1054,0,0.0583305,"ns and Duffy, 2002) appeared. Tree kernels gave the possibility to fully exploit feature spaces of tree fragments. Until then, learning algorithms could not treat these huge spaces. It is infeasible to explicitly represent that kind of feature vectors and to directly compute similarities through dot products. Tree kernels (Collins and Duffy, 2002), by computing similarities between two trees with tree comparison algorithms, exactly determine dot products of vectors in these target spaces. After their introduction, different tree kernels have been proposed (e.g., (Vishwanathan and Smola, 2002; Culotta and Sorensen, 2004; Moschitti, 2006)). Their use spread in many NLP tasks (e.g., (Zhou et al., 2007; Wang and Neumann, 2007; Moschitti et al., 2008; Zanzotto et al., 2009; Zhang and Li, 2009)) and in other areas like biology (Vert, 2002; Hashimoto et al., 2008) and computer security (D¨ussel et al., 2008; Rieck and Laskov, 2007; Bockermann et al., 2009). Tree kernels have played a very important role in promoting the use of syntactic information in learning classifiers, but this method obfuscated the fact that syntactic trees are ultimately used as vectors in learning algorithms. To work with the idea of direct"
W13-3205,P05-1072,0,0.0297136,"ns. Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information. 1 Introduction Syntactic processing is widely considered an important activity in natural language understanding (Chomsky, 1957). Research in natural language processing (NLP) exploits this hypothesis in models and systems. Syntactic features improve performance in high level tasks such as question answering (Zhang and Lee, 2003), semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Moschitti et al., 2008; Collobert et al., 2011), paraphrase detection (Socher et al., 2011), and textual entailment recognition (MacCartney et al., 2006; Wang and Neumann, 2007; Zanzotto et al., 2009). Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures. The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors. 40 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 40–49,"
W13-3205,D09-1073,0,0.0291244,". It is infeasible to explicitly represent that kind of feature vectors and to directly compute similarities through dot products. Tree kernels (Collins and Duffy, 2002), by computing similarities between two trees with tree comparison algorithms, exactly determine dot products of vectors in these target spaces. After their introduction, different tree kernels have been proposed (e.g., (Vishwanathan and Smola, 2002; Culotta and Sorensen, 2004; Moschitti, 2006)). Their use spread in many NLP tasks (e.g., (Zhou et al., 2007; Wang and Neumann, 2007; Moschitti et al., 2008; Zanzotto et al., 2009; Zhang and Li, 2009)) and in other areas like biology (Vert, 2002; Hashimoto et al., 2008) and computer security (D¨ussel et al., 2008; Rieck and Laskov, 2007; Bockermann et al., 2009). Tree kernels have played a very important role in promoting the use of syntactic information in learning classifiers, but this method obfuscated the fact that syntactic trees are ultimately used as vectors in learning algorithms. To work with the idea of directly obtaining rich syntactic feature vectors from sentences, we need some techniques to make these high-dimensional vectors again explicit, through smaller but expressive vec"
W13-3205,D07-1076,0,0.0217345,"paces of tree fragments. Until then, learning algorithms could not treat these huge spaces. It is infeasible to explicitly represent that kind of feature vectors and to directly compute similarities through dot products. Tree kernels (Collins and Duffy, 2002), by computing similarities between two trees with tree comparison algorithms, exactly determine dot products of vectors in these target spaces. After their introduction, different tree kernels have been proposed (e.g., (Vishwanathan and Smola, 2002; Culotta and Sorensen, 2004; Moschitti, 2006)). Their use spread in many NLP tasks (e.g., (Zhou et al., 2007; Wang and Neumann, 2007; Moschitti et al., 2008; Zanzotto et al., 2009; Zhang and Li, 2009)) and in other areas like biology (Vert, 2002; Hashimoto et al., 2008) and computer security (D¨ussel et al., 2008; Rieck and Laskov, 2007; Bockermann et al., 2009). Tree kernels have played a very important role in promoting the use of syntactic information in learning classifiers, but this method obfuscated the fact that syntactic trees are ultimately used as vectors in learning algorithms. To work with the idea of directly obtaining rich syntactic feature vectors from sentences, we need some techniqu"
W13-3205,W07-1406,0,0.201022,"sting methods with lexical information. 1 Introduction Syntactic processing is widely considered an important activity in natural language understanding (Chomsky, 1957). Research in natural language processing (NLP) exploits this hypothesis in models and systems. Syntactic features improve performance in high level tasks such as question answering (Zhang and Lee, 2003), semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Moschitti et al., 2008; Collobert et al., 2011), paraphrase detection (Socher et al., 2011), and textual entailment recognition (MacCartney et al., 2006; Wang and Neumann, 2007; Zanzotto et al., 2009). Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures. The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors. 40 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 40–49, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics A solution to the above problem stems from the recently revitalized research in Distributed Repr"
W13-3824,N10-1146,1,0.92572,"ted tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a strongest link between CDS models and convolution kernels. The rest of the paper is organized as follows. Section 2 focuses on the description of two basic binary operations for compositional distributional semantics, their recursive application to word sequences (or sentences) with a particular attention to their effect on the similarity measure. Section 3 describes the tree kernels (Collins and Duffy, 2002), the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012), and the smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011) to introduce links with similarity measures applied over compositionally obtained distributional vectors. Section 4 draws sketches the future work. In this paper, we want to start the analysis of the models for compositional distributional semantics (CDS) with respect to the distributional similarity. We believe that this simple analysis of the properties of the similarity can help to better investigate new CDS models. We show that, looking at CDS models from this point of view, these models are strictly related with convolution kernels (Haussler, 1999), e.g.: tree kernel"
W13-3824,J10-4006,0,0.050137,"espect to the distributional similarity. We believe that this simple analysis of the properties of the similarity can help to better investigate new CDS models. We show that, looking at CDS models from this point of view, these models are strictly related with convolution kernels (Haussler, 1999), e.g.: tree kernels (Collins and Duffy, 2002). We will then examine how the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 1 Introduction Distributional semantics (see (Turney and Pantel, 2010; Baroni and Lenci, 2010)) is an interesting way of “learning from corpora” meaning for words (Firth, 1957) and of comparing word meanings (Harris, 1964). A flourishing research area is compositional distributional semantics (CDS), which aims to leverage distributional semantics for accounting the meaning of word sequences and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011; Clark et al., 2008; Socher et al., 2011). The area proposes compositional operations to derive the meaning of word sequences using the distributional meaning"
W13-3824,D10-1115,0,0.0420737,"how the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 1 Introduction Distributional semantics (see (Turney and Pantel, 2010; Baroni and Lenci, 2010)) is an interesting way of “learning from corpora” meaning for words (Firth, 1957) and of comparing word meanings (Harris, 1964). A flourishing research area is compositional distributional semantics (CDS), which aims to leverage distributional semantics for accounting the meaning of word sequences and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011; Clark et al., 2008; Socher et al., 2011). The area proposes compositional operations to derive the meaning of word sequences using the distributional meanings of the words in the sequences. The first and more important feature of distributional semantics is to compare the meaning of different words, a way to compute their similar85 2 Compositional distributional semantics over sentences Generally, the proposal of a model for compositional distributional semantics stems from some basic vector combination operations and, t"
W13-3824,P08-1028,0,0.664603,"2002). We will then examine how the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 1 Introduction Distributional semantics (see (Turney and Pantel, 2010; Baroni and Lenci, 2010)) is an interesting way of “learning from corpora” meaning for words (Firth, 1957) and of comparing word meanings (Harris, 1964). A flourishing research area is compositional distributional semantics (CDS), which aims to leverage distributional semantics for accounting the meaning of word sequences and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011; Clark et al., 2008; Socher et al., 2011). The area proposes compositional operations to derive the meaning of word sequences using the distributional meanings of the words in the sequences. The first and more important feature of distributional semantics is to compare the meaning of different words, a way to compute their similar85 2 Compositional distributional semantics over sentences Generally, the proposal of a model for compositional distributional semantics stems from some basic vector"
W13-3824,P02-1034,0,0.877825,"important as the similarity is generally used even by machine learning models such as the kernel machines (Cristianini and ShaweTaylor, 2000). In this paper, we want to start the analysis of the models for compositional distributional semantics with respect to the similarity measure. We focus on linear CDS models. We believe that this simple analysis of the properties of the similarity can help to better investigate new CDS models. We show that, looking CDS models from this point of view, these models are strictly related with the convolution kernels (Haussler, 1999), e.g., the tree kernels (Collins and Duffy, 2002). We will then examine how the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a strongest link between CDS models and convolution kernels. The rest of the paper is organized as follows. Section 2 focuses on the description of two basic binary operations for compositional distributional semantics, their recursive application to word sequences (or sentences) with a particular attention to their effect on the similarity measure. Section 3 describes the tree kernels (Collins and Duffy, 2002), the distributed tree kernels (Zanzotto and Dell’Arciprete,"
W13-3824,C10-1142,1,0.890617,"nels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 1 Introduction Distributional semantics (see (Turney and Pantel, 2010; Baroni and Lenci, 2010)) is an interesting way of “learning from corpora” meaning for words (Firth, 1957) and of comparing word meanings (Harris, 1964). A flourishing research area is compositional distributional semantics (CDS), which aims to leverage distributional semantics for accounting the meaning of word sequences and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011; Clark et al., 2008; Socher et al., 2011). The area proposes compositional operations to derive the meaning of word sequences using the distributional meanings of the words in the sequences. The first and more important feature of distributional semantics is to compare the meaning of different words, a way to compute their similar85 2 Compositional distributional semantics over sentences Generally, the proposal of a model for compositional distributional semantics stems from some basic vector combination operations and, then, these operations a"
W13-3824,D11-1096,0,0.309757,"zotto and Dell’Arciprete, 2012) are an interesting result to draw a strongest link between CDS models and convolution kernels. The rest of the paper is organized as follows. Section 2 focuses on the description of two basic binary operations for compositional distributional semantics, their recursive application to word sequences (or sentences) with a particular attention to their effect on the similarity measure. Section 3 describes the tree kernels (Collins and Duffy, 2002), the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012), and the smoothed tree kernels (Mehdad et al., 2010; Croce et al., 2011) to introduce links with similarity measures applied over compositionally obtained distributional vectors. Section 4 draws sketches the future work. In this paper, we want to start the analysis of the models for compositional distributional semantics (CDS) with respect to the distributional similarity. We believe that this simple analysis of the properties of the similarity can help to better investigate new CDS models. We show that, looking at CDS models from this point of view, these models are strictly related with convolution kernels (Haussler, 1999), e.g.: tree kernels (Collins and Duffy,"
W13-3824,D11-1129,0,0.0872969,"Missing"
W13-3824,W10-2805,0,0.0956456,"’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 1 Introduction Distributional semantics (see (Turney and Pantel, 2010; Baroni and Lenci, 2010)) is an interesting way of “learning from corpora” meaning for words (Firth, 1957) and of comparing word meanings (Harris, 1964). A flourishing research area is compositional distributional semantics (CDS), which aims to leverage distributional semantics for accounting the meaning of word sequences and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011; Clark et al., 2008; Socher et al., 2011). The area proposes compositional operations to derive the meaning of word sequences using the distributional meanings of the words in the sequences. The first and more important feature of distributional semantics is to compare the meaning of different words, a way to compute their similar85 2 Compositional distributional semantics over sentences Generally, the proposal of a model for compositional distributional semantics stems from some basic vector combination operations and, then, these operations are recursively"
