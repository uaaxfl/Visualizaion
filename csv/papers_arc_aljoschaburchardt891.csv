2020.wmt-1.38,Fine-grained linguistic evaluation for state-of-the-art Machine Translation,2020,-1,-1,4,0.622197,5140,eleftherios avramidis,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements."
W18-6436,Fine-grained evaluation of {G}erman-{E}nglish Machine Translation based on a Test Suite,2018,11,5,3,1,12459,vivien macketanz,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The test suite has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low."
L18-1142,{TQ}-{A}uto{T}est {--} An Automated Test Suite for (Machine) Translation Quality,2018,0,2,3,1,12459,vivien macketanz,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
K17-3001,{C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies,2017,28,32,40,0,5828,daniel zeman,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems."
W16-6404,Deeper Machine Translation and Evaluation for {G}erman,2016,-1,-1,3,0.918559,5140,eleftherios avramidis,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2329,"{DFKI}{'}s system for {WMT}16 {IT}-domain task, including analysis of systematic errors",2016,16,3,2,0.918559,5140,eleftherios avramidis,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We are presenting a hybrid MT approach in the WMT2016 Shared Translation Task for the IT-Domain. Our work consists of several translation components based on rule-based and statistical approaches that feed into an informed selection mechanism. Additions to last yearxe2x80x99s submission include a WSD component, a syntactically-enhanced component and several improvements to the rule-based component, relevant to the particular domain. We also present detailed human evaluation on the output of all translation components, focusing on particular systematic errors."
L16-1001,Evaluating Machine Translation in a Usage Scenario,2016,0,4,2,1,17856,rosa gaudio,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this document we report on a user-scenario-based evaluation aiming at assessing the performance of machine translation (MT) systems in a real context of use. We describe a sequel of experiments that has been performed to estimate the usefulness of MT and to test if improvements of MT technology lead to better performance in the usage scenario. One goal is to find the best methodology for evaluating the eventual benefit of a machine translation system in an application. The evaluation is based on the QTLeap corpus, a novel multilingual language resource that was collected through a real-life support service via chat. It is composed of naturally occurring utterances produced by users while interacting with a human technician providing answers. The corpus is available in eight different languages: Basque, Bulgarian, Czech, Dutch, English, German, Portuguese and Spanish."
L16-1296,Tools and Guidelines for Principled Machine Translation Development,2016,2,1,3,0,20841,nora aranberri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development. Our toolkit for automatic evaluation showcases quick and detailed comparison of MT system variants through automatic metrics and n-gram feedback, along with manual evaluation via edit-distance, error annotation and task-based feedback."
W15-5702,Towards Deeper {MT} - A Hybrid System for {G}erman,2015,-1,-1,2,0.918559,5140,eleftherios avramidis,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-5705,Evaluating a Machine Translation System in a Technical Support Scenario,2015,-1,-1,2,1,17856,rosa gaudio,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-4914,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,4,0.314783,5059,maja popovic,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
W15-3004,{DFKI}{'}s experimental hybrid {MT} system for {WMT} 2015,2015,15,4,3,0.918559,5140,eleftherios avramidis,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"DFKI participated in the shared translation task of WMT 2015 with the GermanEnglish language pair in each translation direction. The submissions were generated using an experimental hybrid system based on three systems: a statistical Moses system, a commercial rule-based system, and a serial coupling of the two where the output of the rule-based system is further translated by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM1 models based on POS 4-grams (contrastive submission)."
2015.eamt-1.15,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,4,0.314783,5059,maja popovic,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
avramidis-etal-2014-taraxu,"The tara{X{\\\U}} corpus of human-annotated machine translations""",2014,7,2,2,0.869254,5140,eleftherios avramidis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. This paper describes the corpus developed as a result of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing."
2014.eamt-1.38,Using a new analytic measure for the annotation and analysis of {MT} errors on real data,2014,7,12,2,0,28494,arle lommel,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"This work presents the new flexible Multidimensional Quality Metrics (MQM) framework and uses it to analyze the performance of state-of-the-art machine translation systems, focusing on xe2x80x9cnearly acceptablexe2x80x9d translated sentences. A selection of WMT news data and xe2x80x9ccustomerxe2x80x9d data provided by language service providers (LSPs) in four language pairs was annotated using MQM issue types and examined in terms of the types of errors found in it. Despite criticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR."
2014.eamt-1.41,"Relations between different types of post-editing operations, cognitive effort and temporal effort",2014,-1,-1,3,0.404274,5059,maja popovic,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2013.tc-1.6,Multidimensional quality metrics: a flexible system for assessing translation quality,2013,-1,-1,1,1,13863,aljoscha burchardt,Proceedings of Translating and the Computer 35,0,None
2013.mtsummit-wptp.2,What can we learn about the selection mechanism for post-editing?,2013,-1,-1,3,0.479698,5059,maja popovic,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2013.mtsummit-posters.4,A {CCG}-based Quality Estimation Metric for Statistical Machine Translation Learning from Human Judgments of Machine Translation Output,2013,-1,-1,3,0.479698,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,None
2013.mtsummit-posters.5,Learning from Human Judgments of Machine Translation Output,2013,10,7,3,0.479698,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. Usually, human judgments come in the form of ranking outputs of different translation systems and recently, post-edits of MT output have come into focus. This paper describes the results of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing. Translation outputs from three domains and six translation directions generated by five distinct translation systems have been analysed with the goal of getting relevant insights for further improvement of MT quality and applicability."
2013.mtsummit-european.10,{MATECAT}: Machine Translation Enhanced Computer Assisted Translation {META} - Multilingual {E}urope Technology Alliance,2013,-1,-1,2,0,60,georg rehm,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.11,{META} - Multilingual {E}urope Technology Alliance,2013,-1,-1,2,0,60,georg rehm,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.17,{QTL}aunchpad,2013,-1,-1,5,0,40954,stephen doherty,Proceedings of Machine Translation Summit XIV: European projects,0,None
avramidis-etal-2012-involving,Involving Language Professionals in the Evaluation of Machine Translation,2012,17,12,2,0.757779,5140,eleftherios avramidis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraX{\""U} project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. In a consortium of research and industry partners, the project integrates human translators into the development process for rating and post-editing of machine translation outputs thus collecting feedback for possible improvements."
2012.eamt-1.14,Towards the Integration of {MT} into a {LSP} Translation Workflow,2012,-1,-1,3,0.274343,5800,david vilar,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
W11-2104,Evaluate with Confidence Estimation: Machine ranking of translation outputs using grammatical features,2011,20,22,4,0.757779,5140,eleftherios avramidis,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. The system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser."
W11-2109,Evaluation without references: {IBM}1 scores as evaluation metrics,2011,9,15,4,0.479698,5059,maja popovic,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. We propose a truly automatic evaluation metric based on ibm1 lexicon probabilities which does not need any reference translations. Several variants of ibm1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the ibm1 scores are competitive with the classic evaluation metrics, the most promising being ibm1 scores calculated on morphemes and pos-4grams."
2011.eamt-1.36,From Human to Automatic Error Classification for Machine Translation Output,2011,13,18,2,0.479698,5059,maja popovic,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"Future improvement of machine translation systems requires reliable automatic evaluation and error classification measures to avoid time and money consuming human classification. In this article, we propose a new method for automatic error classification and systematically compare its results to those obtained by humans. We show that the proposed automatic measures correlate well with human judgments across different error classes as well as across different translation outputs on four out of five commonly used error classes."
burchardt-pennacchiotti-2008-fate,{FATE}: a {F}rame{N}et-Annotated Corpus for Textual Entailment,2008,11,18,1,1,13863,aljoscha burchardt,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Several studies indicate that the level of predicate-argument structure is relevant for modeling prevalent phenomena in current textual entailment corpora. Although large resources like FrameNet have recently become available, attempts to integrate this type of information into a system for textual entailment did not confirm the expected gain in performance. The reasons for this are not fully obvious; candidates include FrameNetÂs restricted coverage, limitations of semantic parsers, or insufficient modeling of FrameNet information. To enable further insight on this issue, in this paper we present FATE (FrameNet-Annotated Textual Entailment), a manually crafted, fully reliable frame-annotated RTE corpus. The annotation has been carried out over the 800 pairs of the RTE-2 test set. This dataset offers a safe basis for RTE systems to experiment, and enables researchers to develop clearer ideas on how to effectively integrate frame knowledge in semantic inferenence tasks like recognizing textual entailment. We describe and present statistics over the adopted annotation, which introduces a new schema based on full-text annotation of so called relevant frame evoking elements."
I08-1051,"Formalising Multi-layer Corpora in {OWL} {DL} - Lexicon Modelling, Querying and Consistency Control",2008,15,16,1,1,13863,aljoscha burchardt,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers flexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus."
W07-1402,A Semantic Approach To Textual Entailment: System Evaluation and Task Analysis,2007,9,35,1,1,13863,aljoscha burchardt,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper discusses our contribution to the third RTE Challenge -- the SALSA RTE system. It builds on an earlier system based on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap. We evaluate their (combined) performance on various data sets. However, earlier observations that the combination of features improves the overall accuracy could be replicated only partly."
burchardt-etal-2006-salsa,The {SALSA} Corpus: a {G}erman Corpus Resource for Lexical Semantics,2006,23,144,1,1,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications."
burchardt-etal-2006-salto,{SALTO} - A Versatile Multi-Level Annotation Tool,2006,8,67,1,1,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control."
