2011.iwslt-evaluation.13,P07-2045,0,0.00313389,"the perplexity on the test2010 corpus. In spite of its small size, the low perplexity of the TED Talk corpus seems to indicate that it is the better suited for this task. This is not a reliable measure, of course, but it can give an early indication of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summariz"
2011.iwslt-evaluation.13,W10-1738,1,0.842521,"n of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summarized in Table 2. It can be seen that the choice of training 1 The BLEU scores are cases-sensitive and computed used preprocessed references, in the same way as the preprocessing of the original data. As such it may not fully agree with officially calc"
2011.iwslt-evaluation.13,D08-1064,0,0.0141918,"System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessarily produce the best complete hypothesis, but it should be a good enough indicator for our purposes and further discussion (see also 3.2). 99 Documents Systems 20.4 34.1 32.1 26.3 29.9 29.9 33.8 34.3 27.4 33.8 26.8 18.9 30.1 25.6 23.8 27.1 25.1 27.9 28.0 22.8 32.2 23.8 19.3 31.9 26.2 24.3 28.3 28.1 31.6 29.2 24.0 33.9 25.4 20.8 35.5 32.1 25.9 30.2 31.0 34.3 34.0 28.4 34.4 25.0 21.3 33.8 31.0 26.2 30.8 30.7 33.5 32.2 27.4 33.7 26.8 20.6 33.7 30.6 25.5 29.0 29.0 33.9 33.1 27.0 35.0 27.9 17.4 27.8 23.0 20.7 25.1 21.2 26.1 25.2 21.6 29.0 20.3 17.3 28.9 25.1 21.9"
2011.iwslt-evaluation.13,P02-1040,0,0.0903367,"which provides the best translation, using some automatic method akin to text classification. However this strategy showed not to be effective. Table 3 shows an overview of the BLEU scores of each of the documents composing the test2010 corpus, with the cells shaded to provide a more visual overview of the distribution of the scores. It can be observed that the best system is generally the same for most documents, and the difference in BLEU scores is not very large. Indeed, if we generate a new complete hypothesis by selecting the best system for 4.1. Cross System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessa"
2011.iwslt-evaluation.13,W11-2109,1,0.811438,"e source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experi"
2011.iwslt-evaluation.13,W06-3110,0,0.200778,"Missing"
2011.iwslt-evaluation.13,W11-2103,0,0.0127923,"nally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experimental and in development stage. Therefore we also built a more traditional system based on log-linear models trained on the BLEU score. 5.1. Based on BLEU Log-linear models are at the heart of most state-of-the-art statistical machine translation systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are"
2011.iwslt-evaluation.13,P07-2026,0,0.328387,"Missing"
2011.iwslt-evaluation.13,2008.amta-srw.3,0,0.0623263,"tion systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are the corresponding scaling factors, which we optimize with standard MERT training with respect to the BLEU score. This is also the usual approach used for rescoring n-best lists generated by a single system, and has been used previously for sentence selection purposes (see [20] which uses a very similar approach to our own). Note that no system dependent features like translation probabilities were computed, as we wanted to keep the system general. In fact, for the system combination task, only the single-best translation was provided, without additional information. Table 5 gives an overview of the effect of the different features used in this approach.5 It can be seen that the best performance is obtained when combining all the models. Language model scores alone are not powerful enough to give an improvement over the best single system, and the IBM1 scores even h"
2011.iwslt-evaluation.13,C04-1072,0,0.20734,"Missing"
2011.iwslt-evaluation.13,vilar-etal-2006-error,1,0.914641,"Missing"
2011.iwslt-evaluation.13,W11-2104,1,0.847176,"in this case (not negative!), this indicates that higher IBM1 scores are beneficial for the system. For the language models, the picture is mixed, the system tries to maximize the probability of some of them, but to minimize it for others. 5.2. Based on human ranking We considered employing a supervised machine learning approach trained over sentences evaluated by human judges, made available by the WMT evaluations. The system was trained based on human rankings of MT output and consequently used to replicate ranking for our sentence-level translation alternatives. According to this approach [21], ranking is decomposed into a set of pairwise decisions, where each translation output gets compared with each one of the other alternatives. For this purpose, a binary classifier is trained to learn to comFeature Sign Cross system BLEU (system level) Cross system BLEU (sentence level) Word penalty − − − EXTer hINFer hLEXer hRer MISer WER − − + + + + IBM1 − LM (Europarl) LM (Giga FrEn) LM (monolingual TED) LM (UN) LM (News) − − + + + Table 6: Sign of the scaling factors corresponding to the features. pare the output quality. The sentence that wins most of the pairwise comparisons (ranked firs"
2011.iwslt-evaluation.13,W08-0309,0,0.0621747,"Missing"
2011.iwslt-evaluation.13,P06-1055,0,0.0117871,"ng systems, optimizing the output for the highest probability of the consequent n-grams. On the other hand, automatic metrics are also based on n-grams matching with the reference translation. In order to avoid a possible overfitting on n-grams, but also to capture more complex phenomena (such as long distance structures and grammatical fluency) that are still important to quality output and may have been neglected by the statistical systems, we considered including features derived after parsing the systems’ output with Probabilistic Context Free Grammars (PCFG). For this the Berkeley Parser [16] was used. PCFG parsing allows the generation of n-best lists of trees, scored probabilistically, leading to the selection of the tree with the highest score. From this process, we extracted the number of distinct parsing trees of the sentence, after having allowed the generation of an n-best list of size n = 1000. A smaller number of trees could mean that there are less possible tree derivations, i.e. less parsing ambiguity. Parsing statistics are not only an indicator of the grammaticality of the sentence, but also of how complicated it is, assuming it is fully grammatical. Therefore, we rel"
2011.iwslt-evaluation.13,N07-2015,0,0.109556,"mmatical). For this reason, we parsed both the source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT ev"
2011.iwslt-evaluation.13,W10-1703,0,0.0767624,"Missing"
2011.iwslt-evaluation.13,E06-1032,0,\N,Missing
2011.iwslt-evaluation.13,J11-4002,1,\N,Missing
2011.iwslt-evaluation.13,W09-0401,0,\N,Missing
2011.iwslt-evaluation.13,2011.iwslt-evaluation.1,0,\N,Missing
2013.mtsummit-posters.4,2010.iwslt-papers.1,0,0.0149183,"h means that the verb read needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which hel"
2013.mtsummit-posters.4,2012.eamt-1.44,0,0.0167737,"ad needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which helps 225 to estimate its gr"
2013.mtsummit-posters.4,W11-2104,1,0.915377,"to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and tar"
2013.mtsummit-posters.4,J99-2004,0,0.0441188,"al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to boost their performance over the baseline system. 224 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags, cf. Bangalore and Joshi (1999)) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that specifies the local syntactic context of the word at the lexical level in the form of a set of arguments. CCG builds a parse tree for a sentence by combining CCG categories using a set of binary combinatory rules. Most of the CCG grammar is contained in the lexicon, which is why CCG has simpler rules compared to CFG productions. CCG categories are divided into atomic and complex categories. Examples of atomic categories are S (sentence), N (noun), NP (noun phrase), et"
2013.mtsummit-posters.4,C04-1046,0,0.0459933,"syntactic categories, we were able to extract grammaticality QE features based on recognising grammatical chunks and examining sequences of CCG categories in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum En"
2013.mtsummit-posters.4,W10-1703,0,0.0690335,"Missing"
2013.mtsummit-posters.4,W12-3102,0,0.0576051,"Missing"
2013.mtsummit-posters.4,W12-3110,0,0.0115413,"et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Specia (2012) compare the performance of a set of linguistic features extracted from source and target sentences constituency and dependency trees with the baseline system of the WMT 2012 evaluation campaign (Callison-Burch et al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to bo"
2013.mtsummit-posters.4,2011.eamt-1.32,0,0.0415548,"Missing"
2013.mtsummit-posters.4,P07-1037,0,0.0606055,"Missing"
2013.mtsummit-posters.4,D09-1123,0,0.0510684,"Missing"
2013.mtsummit-posters.4,W12-3117,0,0.0190571,"features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Spe"
2013.mtsummit-posters.4,2006.amta-papers.25,0,0.126803,"Missing"
2013.mtsummit-posters.4,2010.jec-1.5,0,0.0451371,"Missing"
2013.mtsummit-posters.4,2009.mtsummit-papers.16,0,0.0118631,"s in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic fea"
2013.mtsummit-posters.4,2011.mtsummit-papers.58,0,0.160181,"trate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a"
2013.mtsummit-posters.4,2011.eamt-1.12,0,0.0120636,"uk translation and sometimes from internal translation information output by the MT system. With the improvement of the quality of MT systems and their increasing use in real-world applications, MT QE has become increasingly more important. QE has been demonstrated to help in making the integration of MT systems in the translation pipeline more efficient. For example, using QE to filter out low-quality translations from the post-editing process has been shown to help in reducing post-editing time as low-quality translations might take more time to post-edit than to be translated from scratch (Specia, 2011). Furthermore, QE helps to enhance MT user experience by informing the user of the predicted quality of the translation produced by the MT system. Moreover, QE has been more and more used to enhance the quality of MT systems by integrating QE scores in n-best reranking and combining the translation of different MT systems. QE features estimate the quality of the translation by capturing the aspects which evaluate translation quality, namely fluency and adequacy, in addition to predicting the difficulty of the translation. Adequacy refers to the extent to which the meaning of the source sentenc"
2013.mtsummit-posters.4,P10-1062,0,0.0244835,"rk The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased"
2013.mtsummit-posters.5,2003.mtsummit-systems.1,0,0.0537752,"ence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the best general purpose MT engines, it has been included in order to allow us to assess the performance level of our SMT system and also to compare it directly with other MT ap"
2013.mtsummit-posters.5,W10-1703,0,0.0302962,"number of translators working on it. 2.1 Translation systems used The evaluated translation outputs presented in this work are produced by German-English, GermanFrench and German-Spanish machine translation Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranki"
2013.mtsummit-posters.5,W12-3102,0,0.0648542,"Missing"
2013.mtsummit-posters.5,vilar-etal-2006-error,1,0.893094,"Missing"
2013.mtsummit-posters.5,W10-1738,1,0.850961,"level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the be"
2013.mtsummit-posters.5,federmann-2010-appraise,0,0.0730217,"nslation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and tech"
2013.mtsummit-posters.5,P07-2045,0,0.0038879,"licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine"
2013.mtsummit-posters.5,W12-3123,0,0.0364801,"ent of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluation process has b"
2013.mtsummit-posters.5,2011.eamt-1.12,0,0.0194594,"rther improvement of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluati"
2013.mtsummit-wptp.2,2003.mtsummit-systems.1,0,0.0711019,"1 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), se"
2013.mtsummit-wptp.2,federmann-2010-appraise,0,0.0122073,"e sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), select the translation output which is easiest to post-edit and perform the editing. Post-edit all: for each source sentence in the selected subset (4070 sentences in total), postedit all four produced translation outputs. For both post-editing tasks, the translators"
2013.mtsummit-wptp.2,P10-1064,0,0.032193,"Missing"
2013.mtsummit-wptp.2,W12-3123,0,0.0120168,"proved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professi"
2013.mtsummit-wptp.2,2011.eamt-1.12,0,0.0122675,", five types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, si"
2013.mtsummit-wptp.2,2012.amta-wptp.9,0,0.0175217,"dding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by trans"
2013.mtsummit-wptp.2,W10-1738,1,0.708534,"n-de es-de fr-de Total News 1788 514 912 1744 101 1852 6911 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and p"
2014.eamt-1.38,W13-2201,0,0.100476,"Missing"
2014.eamt-1.38,W11-2107,0,0.017106,"n examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result of these limitations, there has been a"
2014.eamt-1.38,2010.eamt-1.12,0,0.209351,"Missing"
2014.eamt-1.38,1994.amta-1.9,0,0.935888,"lity Metric” MQM designed by the QTLaunchPad project (http://www.qt21.eu/launchpad). The metric was designed to facilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model,"
2014.eamt-1.38,P02-1040,0,0.0903186,"ticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result o"
2014.eamt-1.38,stymne-ahrenberg-2012-practice,0,0.494563,"cilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model, SAE J2450, TAUS DQF, ATA assessment, and various tool-specific metrics) that served as the basis for a descriptive framework f"
2014.eamt-1.38,vilar-etal-2006-error,0,0.85614,"Missing"
2014.eamt-1.41,2011.mtsummit-papers.17,0,0.162598,"– Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas removing additions ha"
2014.eamt-1.41,W12-3102,0,0.0667545,"Missing"
2014.eamt-1.41,2012.eamt-1.35,0,0.0190046,"(Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2012 ok 323 31 200 quality level edit+ edit edit1559 0 544 399 0 550 548 856 576 make the translation acceptable. Post-editing time is measured on the sentence level in a controlled way in order to isolate factors such as pauses between sentences. The technical effort is represented by following five types of edit operations: bad 99 20 74 Table 1: Corpus statistics: number of sentences assigned to each"
2014.eamt-1.41,W12-3123,0,0.0661415,"under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 191 More details about the technical effort can be obtained by analysing particular edit operations. (Blain et al., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al"
2014.eamt-1.41,J11-4002,1,0.862953,"Missing"
2014.eamt-1.41,specia-etal-2010-dataset,0,0.058975,"Missing"
2014.eamt-1.41,2011.eamt-1.12,0,0.111983,"szkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas"
2014.eamt-1.41,2009.mtsummit-posters.20,0,0.295234,"e Lommel, Aljoscha Burchardt, Eleftherios Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most c"
2014.eamt-1.41,2010.jec-1.6,0,0.0874217,"Missing"
2014.eamt-1.41,temnikova-2010-cognitive,0,0.116741,"Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most"
2014.eamt-1.41,2013.mtsummit-papers.15,0,0.0298537,"., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2"
2015.eamt-1.15,W05-0814,0,0.0385293,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.15,E03-1076,0,0.0450815,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.15,2005.mtsummit-papers.11,0,0.0120371,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.15,J11-4002,1,0.893851,"Missing"
2015.eamt-1.15,2011.eamt-1.12,0,0.0238497,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.15,E09-1087,0,0.0700919,"Missing"
2015.eamt-1.15,tiedemann-2012-parallel,0,0.0242552,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.15,W11-2103,0,\N,Missing
2020.wmt-1.38,N18-1118,0,0.0289868,"Missing"
2020.wmt-1.38,W18-6432,0,0.0950057,"Missing"
2020.wmt-1.38,W18-6433,0,0.104826,"Missing"
2020.wmt-1.38,W18-6434,0,0.0282019,"est suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various linguistic phenomena and supporting different language directions. These include work in grammatical contrasts (Cinkova and Bojar, 2018), discourse (Bojar et al., 2018), morphology (Burlot et al., 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). When compared to the vast majority of the previous test suites, the one presented here is the only one 346 Proceedings of the 5th Conference on Machine Translation (WMT), pages 346–356 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Lexical Ambiguity Er las gerne Novellen. He liked to read novels. He liked to read novellas. Phrasal verb Warum starben die Dinosaurier aus? Why did the dinosaurs die? Why did the dinos"
2020.wmt-1.38,L16-1100,0,0.0594885,"n ways that cannot be seen by generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite trac"
2020.wmt-1.38,W18-6435,0,0.0332352,"Missing"
2020.wmt-1.38,D17-1263,0,0.035982,"Missing"
2020.wmt-1.38,C90-2037,0,0.743908,"re relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various lingu"
2020.wmt-1.38,2020.wmt-1.12,0,0.168789,"Missing"
2020.wmt-1.38,W18-6436,1,0.851368,"a source sentence and a set of correct and/or incorrect MT outputs. At the evaluation time, the test items are given as input to the MT systems and it is tested on whether the respective MT output consists a correct translation. By observing the amount of the test items that are translated correctly, one can calculate the performance of the MT systems regarding the respective phenomenon. The evaluation presented in this paper is based on the DFKI Test Suite for MT on German to English, which has been presented in Burchardt et al. (2017) and applied extensively in the WMT shared task of 2018 (Macketanz et al., 2018) and 2019 (Avramidis et al., 2019). The current version includes 5,560 test items in order to control 107 phenomena organised in 14 categories. Some sample test items can be seen in Table 1 whereas a more detailed list of test sentences with correct and incorrect translations can be found on GitHub1 . 3.1 Application of the test suite The construction of the test suite has been thoroughly explained in the papers from the previous years (Avramidis et al., 2018, 2019) and depicted in Figure 1 (steps a-c). The test items of the test suite are given as input to the MT systems (step d). Their MT ou"
2020.wmt-1.38,2020.wmt-1.25,0,0.313684,"Missing"
2020.wmt-1.38,W18-6307,0,0.0467561,"Missing"
2020.wmt-1.38,P02-1040,0,0.106035,"Missing"
2020.wmt-1.38,2020.acl-main.442,0,0.0159125,"generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of"
2020.wmt-1.38,W18-6437,0,0.044788,"Missing"
2020.wmt-1.38,2020.wmt-1.30,0,0.0546219,"Missing"
2020.wmt-1.38,P19-1164,0,0.0404761,"asing interest on several natural language processing (NLP) tasks. Focusing on particular issues gives the possibility to analyse the automatic output in ways that cannot be seen by generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comp"
2020.wmt-1.38,1991.mtsummit-panels.5,0,0.636469,"o distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various linguistic pheno"
2020.wmt-1.38,2020.wmt-1.33,0,0.157472,"Missing"
2021.acl-srw.20,2020.wmt-1.38,1,0.77426,"pronouns under the co-reference phenomenon in the category of non-verbal agreement. 4 System name Experiment setup Test suite setup For the development and application of the test suite we used the tool TQ-AutoTest (Macketanz et al., 2018a). We created 10 sentences per phenomenon, resulting in a total of 585 sentences, examining 49 phenomena organised in 13 categories. The raw test items, as well as the translations evaluated can be found in our repository1 . The phenomena selected for this experiment are a subset of the ones of German→English MT, as described in Macketanz et al. (2018b) and Avramidis et al. (2020), adapted to the opposite language direction. An extract of the used sentences can be found in table 5. 4.2 Data The Europarl corpus ver. 10 (Koehn, 2005) with about 1,8 M sentences and the DGT 2019 corpus (Tiedemann, 2012) with approximately 5,2 M sentences were used, summing up to around 7 M parallel sentences for training. Newstest 2015 (Bojar et al., 2015) was used as a development (validation) set and newstest 2016 (Bojar et al., 2016) as a test set. We applied standard preprocessing including normalization, sentence filtering, tokenization and byte-pair encoding by using the default Mari"
2021.acl-srw.20,W19-5351,1,0.924593,"ator, despite its limitations. Fine-grained evaluation using test suites Despite the widespread usage of BLEU score, there have been critical voices from the translation community on its role. As stated by Callison-Burch et al. (2006), BLEU sometimes does not reflect improvement in the quality of the produced translations and therefore is not always a reliable metric to rate a system overall. They showed that BLEU score allows for a certain variance and is often unreliable or inconsistent compared to human analysis especially when one is examining linguistic phenomena on a fine grained level (Avramidis et al., 2019). To overcome the disadvantages and instabilities of the BLEU score, researches have suggested the utilisation of test suites. Such test suites can report scores either through manual (Ahrenberg, 2018; Koh et al., 2001) or semi-automatic evaluation. Semi-automatic evaluation uses certain metrics to be tested against, such as reference translations with specific tokens (Guillou and Hardmeier, 2016; Macketanz et al., 2018a). Another important aspect Methods Additionally, we build several systems with different architectures and corpus sizes to allow further comparisons. This being a student expe"
2021.acl-srw.20,2020.cl-1.1,0,0.0265589,"nt Research Workshop, pages 186–196 August 5–6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related work Interpreting NMT with regards to linguistic phenomena There have been several efforts to interpret the operation of NMT with regards to linguistic phenomena. These works mostly focus on identifying which parts of the neural topology are responsible for learning some particular linguistic aspects. For example they investigate the role of particular neurons (Bau et al., 2019), layers, major components such as the encoder and the decoder (Dalvi et al., 2017; Tang et al., 2019; Belinkov et al., 2020), or different architectures (Tang et al., 2020) with regards to word sense disambiguation and semantics, morphology, long range dependencies and syntax, etc. Contrary to these works, our consideration of the linguistic aspects is not focusing on the elements of the neural network, but on its timely development during the training process. Recognising the limitations of scoring with cross-entropy or BLEU score, two papers have proposed scoring based on more focused metrics, such as semantic similarity (Wieting et al., 2019) and adequacy (Kong et al., 2018). Here, we are not interested in findi"
2021.acl-srw.20,L18-1142,1,0.923187,"allows for a certain variance and is often unreliable or inconsistent compared to human analysis especially when one is examining linguistic phenomena on a fine grained level (Avramidis et al., 2019). To overcome the disadvantages and instabilities of the BLEU score, researches have suggested the utilisation of test suites. Such test suites can report scores either through manual (Ahrenberg, 2018; Koh et al., 2001) or semi-automatic evaluation. Semi-automatic evaluation uses certain metrics to be tested against, such as reference translations with specific tokens (Guillou and Hardmeier, 2016; Macketanz et al., 2018a). Another important aspect Methods Additionally, we build several systems with different architectures and corpus sizes to allow further comparisons. This being a student experiment, the computational and time restrictions allowed a limited number of models trained with an amount of data that is smaller than the state-of-the-art. However, that should serve as proof of concept. Despite the models not being state-of-the-art, our focus remains on the evolution of the linguistic performance, starting from the early steps of the training process. In our experiments we will have three systems: a s"
2021.acl-srw.20,E06-1032,0,0.12904,"hots, we intend to get insights with a linguistic perspective in the machine learning process. We can only evaluate particular snapshots, since the functioning of the test suite tool allows semi-automatic error annotation and there is still need to manually evaluate some uncertain decisions and edge cases. To decide which snapshots to pick, we relied on the use of BLEU score as a first indicator, despite its limitations. Fine-grained evaluation using test suites Despite the widespread usage of BLEU score, there have been critical voices from the translation community on its role. As stated by Callison-Burch et al. (2006), BLEU sometimes does not reflect improvement in the quality of the produced translations and therefore is not always a reliable metric to rate a system overall. They showed that BLEU score allows for a certain variance and is often unreliable or inconsistent compared to human analysis especially when one is examining linguistic phenomena on a fine grained level (Avramidis et al., 2019). To overcome the disadvantages and instabilities of the BLEU score, researches have suggested the utilisation of test suites. Such test suites can report scores either through manual (Ahrenberg, 2018; Koh et al"
2021.acl-srw.20,W18-6436,1,0.898882,"Missing"
2021.acl-srw.20,L16-1100,0,0.0204498,"They showed that BLEU score allows for a certain variance and is often unreliable or inconsistent compared to human analysis especially when one is examining linguistic phenomena on a fine grained level (Avramidis et al., 2019). To overcome the disadvantages and instabilities of the BLEU score, researches have suggested the utilisation of test suites. Such test suites can report scores either through manual (Ahrenberg, 2018; Koh et al., 2001) or semi-automatic evaluation. Semi-automatic evaluation uses certain metrics to be tested against, such as reference translations with specific tokens (Guillou and Hardmeier, 2016; Macketanz et al., 2018a). Another important aspect Methods Additionally, we build several systems with different architectures and corpus sizes to allow further comparisons. This being a student experiment, the computational and time restrictions allowed a limited number of models trained with an amount of data that is smaller than the state-of-the-art. However, that should serve as proof of concept. Despite the models not being state-of-the-art, our focus remains on the evolution of the linguistic performance, starting from the early steps of the training process. In our experiments we will"
2021.acl-srw.20,2005.mtsummit-papers.11,0,0.142487,"ion of the test suite we used the tool TQ-AutoTest (Macketanz et al., 2018a). We created 10 sentences per phenomenon, resulting in a total of 585 sentences, examining 49 phenomena organised in 13 categories. The raw test items, as well as the translations evaluated can be found in our repository1 . The phenomena selected for this experiment are a subset of the ones of German→English MT, as described in Macketanz et al. (2018b) and Avramidis et al. (2020), adapted to the opposite language direction. An extract of the used sentences can be found in table 5. 4.2 Data The Europarl corpus ver. 10 (Koehn, 2005) with about 1,8 M sentences and the DGT 2019 corpus (Tiedemann, 2012) with approximately 5,2 M sentences were used, summing up to around 7 M parallel sentences for training. Newstest 2015 (Bojar et al., 2015) was used as a development (validation) set and newstest 2016 (Bojar et al., 2016) as a test set. We applied standard preprocessing including normalization, sentence filtering, tokenization and byte-pair encoding by using the default Marian setting (Junczys-Dowmunt et al., 2018) with embedded SentencePiece (Kudo and Richardson, 2018). Concerning the length of the individual sentences, we f"
2021.acl-srw.20,2001.mtsummit-papers.35,0,0.314671,"l. (2006), BLEU sometimes does not reflect improvement in the quality of the produced translations and therefore is not always a reliable metric to rate a system overall. They showed that BLEU score allows for a certain variance and is often unreliable or inconsistent compared to human analysis especially when one is examining linguistic phenomena on a fine grained level (Avramidis et al., 2019). To overcome the disadvantages and instabilities of the BLEU score, researches have suggested the utilisation of test suites. Such test suites can report scores either through manual (Ahrenberg, 2018; Koh et al., 2001) or semi-automatic evaluation. Semi-automatic evaluation uses certain metrics to be tested against, such as reference translations with specific tokens (Guillou and Hardmeier, 2016; Macketanz et al., 2018a). Another important aspect Methods Additionally, we build several systems with different architectures and corpus sizes to allow further comparisons. This being a student experiment, the computational and time restrictions allowed a limited number of models trained with an amount of data that is smaller than the state-of-the-art. However, that should serve as proof of concept. Despite the mo"
2021.acl-srw.20,D18-2012,0,0.0237008,"ntences can be found in table 5. 4.2 Data The Europarl corpus ver. 10 (Koehn, 2005) with about 1,8 M sentences and the DGT 2019 corpus (Tiedemann, 2012) with approximately 5,2 M sentences were used, summing up to around 7 M parallel sentences for training. Newstest 2015 (Bojar et al., 2015) was used as a development (validation) set and newstest 2016 (Bojar et al., 2016) as a test set. We applied standard preprocessing including normalization, sentence filtering, tokenization and byte-pair encoding by using the default Marian setting (Junczys-Dowmunt et al., 2018) with embedded SentencePiece (Kudo and Richardson, 2018). Concerning the length of the individual sentences, we followed the general practice and limited the sentences to a maximum length of 100. 4.3 Training setup The NMT systems were trained using Marian ver. 1.9.0 (Junczys-Dowmunt et al., 2018). In order to follow the learning curve of the training process, 188 1 https://github.com/pstadler1990/nmt_ paper21_appendix we kept one checkpoint every 10,000 iterations. To do so, we disabled the overwrite option from the CLI call of Marian. As per default, cross entropy was used as a validation metric, whereas the training processes were run on a compu"
2021.acl-srw.20,P02-1040,0,0.109342,"has been done in order to investigate how the training process evolves with regards to measurable factors of translation quality, such as the rules of linguistic correctness (grammar, syntax, semantics). In particular, the training process performs several iterations through which the neural network weights are gradually adjusted to achieve the optimal performance for the training data seen at the moment. After several iterations, the performance of the model, with its current weights, is typically validated against a development set, using some automatic metrics (cross entropy or BLEU score; Papineni et al., 2002), which may also define whether This work is intended to provide NMT researchers and engineers with additional guidance on what to look for when evaluating and designing machine translation systems. This is a preliminary work towards this direction, aiming to investigate how the training process evolves with regards to linguistic performance for several phenomena. We do this by selecting snapshots of particular training epochs and evaluating these snapshots with test suites, which probe the translation of specific linguistic phenomena. As a result, we can observe the learning curve of those li"
2021.acl-srw.20,P16-1009,0,0.0755642,"Missing"
2021.acl-srw.20,D19-1149,0,0.0452774,"Missing"
2021.acl-srw.20,tiedemann-2012-parallel,0,0.0362767,"al., 2018a). We created 10 sentences per phenomenon, resulting in a total of 585 sentences, examining 49 phenomena organised in 13 categories. The raw test items, as well as the translations evaluated can be found in our repository1 . The phenomena selected for this experiment are a subset of the ones of German→English MT, as described in Macketanz et al. (2018b) and Avramidis et al. (2020), adapted to the opposite language direction. An extract of the used sentences can be found in table 5. 4.2 Data The Europarl corpus ver. 10 (Koehn, 2005) with about 1,8 M sentences and the DGT 2019 corpus (Tiedemann, 2012) with approximately 5,2 M sentences were used, summing up to around 7 M parallel sentences for training. Newstest 2015 (Bojar et al., 2015) was used as a development (validation) set and newstest 2016 (Bojar et al., 2016) as a test set. We applied standard preprocessing including normalization, sentence filtering, tokenization and byte-pair encoding by using the default Marian setting (Junczys-Dowmunt et al., 2018) with embedded SentencePiece (Kudo and Richardson, 2018). Concerning the length of the individual sentences, we followed the general practice and limited the sentences to a maximum l"
2021.acl-srw.20,W19-5355,0,0.0208881,"Missing"
2021.acl-srw.20,P19-1427,0,0.0212652,"s the encoder and the decoder (Dalvi et al., 2017; Tang et al., 2019; Belinkov et al., 2020), or different architectures (Tang et al., 2020) with regards to word sense disambiguation and semantics, morphology, long range dependencies and syntax, etc. Contrary to these works, our consideration of the linguistic aspects is not focusing on the elements of the neural network, but on its timely development during the training process. Recognising the limitations of scoring with cross-entropy or BLEU score, two papers have proposed scoring based on more focused metrics, such as semantic similarity (Wieting et al., 2019) and adequacy (Kong et al., 2018). Here, we are not interested in finding a linguistic metric to improve the training process, but to apply a fine-grained linguistic analysis to the several stages of the training process and make observations. 2.2 for using test suites instead of relying solely on automatic evaluation, is the domain-knowledge that only human judges can provide and is required to to assess the translation quality (Vojtˇechová et al., 2019). 3 We are interested in observing the learning curve of neural machine translation with regards to linguistic phenomena. Particularly, the a"
2021.mtsummit-at4ssl.8,P16-2027,0,0.0688439,"-word translation of Japanese to Japanese Sign Language (JSL) by finger spelling. By the beginning of the century, Elliott et al. (2000) specified the first framework for producing avatar based SL from text. They proposed the Signing Gesture Markup Language (SIGML; Elliott et al., 2004), an XML-formatted SL sequence description to drive the animation of an avatar in a web browser. It is built on the Hamburg Notation System (HamNoSys Hanke, 2004) that allows phonetic representations of signs. SiGML is still applied in recent sign generation concepts (Kaur and Singh, 2015; Verma and Kaur, 2015; Rayner et al., 2016; Sugandhi et al., 2020). It is used as input language for the SL animation system Java Avatar Signing (JASigning) (Elliott et al., 2010), the successor of SiGMLSigning (Elliott et al., 2004). This was applied by Rayner et al. (2016) for their open online SL translation application development platform and Sugandhi et al. (2020), who developed a system that produces Indian Sign Language (ISL) from English text while considering ISL grammar. For the correct grammar they created a HamNoSys database before converting the representations to SiGML. Another architecture was introduced by Heloir and"
avramidis-etal-2012-involving,eisele-chen-2010-multiun,0,\N,Missing
avramidis-etal-2012-involving,federmann-2010-appraise,1,\N,Missing
avramidis-etal-2012-involving,J11-4002,1,\N,Missing
avramidis-etal-2012-involving,P02-1040,0,\N,Missing
avramidis-etal-2012-involving,W10-1738,1,\N,Missing
avramidis-etal-2012-involving,P07-2045,0,\N,Missing
avramidis-etal-2012-involving,W11-2104,1,\N,Missing
avramidis-etal-2012-involving,W10-1703,0,\N,Missing
avramidis-etal-2012-involving,vilar-etal-2006-error,1,\N,Missing
avramidis-etal-2012-involving,W11-2100,0,\N,Missing
avramidis-etal-2012-involving,2011.eamt-1.36,1,\N,Missing
avramidis-etal-2012-involving,2010.amta-papers.27,0,\N,Missing
avramidis-etal-2012-richly,steinberger-etal-2006-jrc,0,\N,Missing
avramidis-etal-2012-richly,vandeghinste-etal-2008-evaluation,1,\N,Missing
avramidis-etal-2012-richly,W09-0424,0,\N,Missing
avramidis-etal-2012-richly,P07-2045,0,\N,Missing
avramidis-etal-2012-richly,C04-1072,0,\N,Missing
avramidis-etal-2012-richly,W08-0309,0,\N,Missing
avramidis-etal-2012-richly,2005.mtsummit-papers.11,0,\N,Missing
avramidis-etal-2012-richly,W10-1720,1,\N,Missing
avramidis-etal-2012-richly,P03-1021,0,\N,Missing
avramidis-etal-2014-taraxu,federmann-2010-appraise,0,\N,Missing
avramidis-etal-2014-taraxu,avramidis-etal-2012-involving,1,\N,Missing
avramidis-etal-2014-taraxu,W10-1738,1,\N,Missing
avramidis-etal-2014-taraxu,P07-2045,0,\N,Missing
avramidis-etal-2014-taraxu,2013.mtsummit-posters.5,1,\N,Missing
C12-1008,W11-2104,1,0.893006,"Missing"
C12-1008,C04-1046,0,0.0215429,"Evaluation. The applicability is even broader, as the approach presented is system-independent and relies on generic automatic analysis applied on any input containing sets of one source and several translation outputs. 2 Previous work Quality Estimation is a rather recent aspect in research on Machine Translation. As a field, it tries to provide quality assessment on the translation output without the availability of reference translations. Previous work includes statistical methods on predicting word-level confidence (Ueffing and Ney, 2005; Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order"
C12-1008,W08-0309,0,0.0634503,"d discordant counts from all segments (i.e. sentences) are gathered and the fraction is calculated with their sums • Average segment tau (τseg ) where tau is calculated on a segment level and then averaged over the number of sentences. This shows equal importance to each sentence, irrelevant of the number of alternative translations. 123 4 Experiment 4.1 Data sets Both our training and test data were extracted from human-annotated data containing comparisons of the outputs of several German-to-English MT systems, as a result of the evaluation tasks run by the Workshops on Machine Translation (Callison-Burch et al., 2008, 2009, 2010, 2011), which have been freely available for further research. In the development phase, our training set consisted of the human rankings of years 2008, 20101 , 2011 and the test-set from the year 2009. In order to re-assure that the system is not overfitting the development environment, the best systems were also tested upon a different set-up, where the human rankings of years 2008, 2009 and 2010 were used for training and the rankings from 2011 system combination task (2011c) were used for testing. The provided data-sets contain human judgments organized in rankings of at most"
C12-1008,W10-1703,0,0.127831,"Missing"
C12-1008,W08-0331,0,0.0196778,"rom many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranking in MT evaluation, by developing a machine learning approach to train on rank data, though these are using reference translations and are only evaluated by producing an overall corpus-level ranking. METEOR, one of the state-of-the-art evaluation metrics (Lavie and Agarwal, 2007) also gets its components tuned over human rankings. Avramidis et al. (2011) do MT evaluation without references based on learned ranking, by using parsing features and Parton et al. (2011) also show a configuration of their metric which achieves good correlation with human judgments"
C12-1008,D11-1125,0,0.0235809,"s. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranking in MT evaluation, by developing a machine learning approach to train on rank data, though these are using reference translations and are only evaluated by producing an overall corpus-level ranking. METEOR, one of the state-of-the"
C12-1008,2005.mtsummit-papers.11,0,0.00799171,"re aggregated into one ranking, and the ties on both pairwise and ranking level were removed. One should also notice that repetitive human rankings of the same systems often disagree with each other, which signifies the existence of some noise in our data. 4.2 Implementation PCFG parsing features were generated on the output of the Berkeley Parser, with the default grammars based on an English and a German treebank (Petrov and Klein, 2007). N-gram features were based on language models of order 5, built with the SRILM toolkit (Stolcke, 2002) on monolingual training material from the Europarl (Koehn, 2005) and the News (CallisonBurch et al., 2011) corpora. The Acrolinx IQ2 was used to annotate source and target with language checking suggestions and provide style, grammar and spelling scores. The annotation process was organized with the Ruffus library (Goodstadt, 2010) and the learning algorithms were executed using the Orange toolkit (Demšar et al., 2004). 4.3 Strategy The amount of features and learning options provide an exponential number of experiment parameters. However, in order to be able to draw a fair amount of conclusions in a decent amount of time, we followed an incremental approa"
C12-1008,P02-1040,0,0.101987,"ng style, grammar and terminology, summed up in relevant quality scores. • Language-model probabilities: Language models are also an indication of fluency, since they provide statistics on how likely the sequences of the words are for a particular language. Although Statistical MT systems are expected to already optimize over the language model, as mentioned above, other types of systems may still benefit from this features. This feature category includes the smoothed n-gram probability of the sentence. • Contrastive evaluation scores: Each translation is scored with an automatic metric (e.g. Papineni et al., 2002), using the competitive translations as references. This has shown to perform well as a feature in similar tasks (Soricut et al., 2012). Keeping the isomorphism assumption, an additional hint for the adequacy of the translation was applied for the features that are apparent in both source and target: The ratio of these features was calculated by diving the feature value of each one of the translation outputs with the respective feature value of the source. 3.5 Machine learning algorithms The modular approach of the pairwise classification allowed the use of several machine learning algorithms"
C12-1008,W11-2111,0,0.0178593,"selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranking in MT evaluation, by developing a machine learning approach to train on rank data, though these are using reference translations and are only evaluated by producing an overall corpus-level ranking. METEOR, one of the state-of-the-art evaluation metrics (Lavie and Agarwal, 2007) also gets its components tuned over human rankings. Avramidis et al. (2011) do MT evaluation without references based on learned ranking, by using parsing features and Parton et al. (2011) also show a configuration of their metric which achieves good correlation with human judgments without any reference information, using target features produced by a language correction software. Reported work has used various aspects of weighing or training over human ranking. Following 116 on a similar path, we are focusing on the ability of a mechanism to reproduce human preference rankings and compare its outcome with existing evaluation methods. 117 3 3.1 Methods Problem description As already introduced in Section 1, this work is aiming to a mechanism for ranking multiple translation ou"
C12-1008,P06-1055,0,0.0578312,"tability is the grammaticality of the generated sentences. This is intense in many statistical systems 120 (particularly the ones following the phrase-based approach) since they treat the generation process in a rather shallow way. Most often language models are used within the MT systems in order to optimize the output for the highest probability of the consequent ngrams. As an additional measure of quality which can capture more complex phenomena (such as grammatical fluency, long distance structures, etc.) we include features derived from Probabilistic Context Free Grammars (PCFG) parsing (Petrov et al., 2006). PCFG parsing operates by creating many possible tree parses for a given a sentence, forming an n-best list of parse hypotheses. These hypotheses are scored probabilistically, leading to the selection of the tree with the highest overall probability. We allowed an n-best list with a size of n=1000 and counted the number of trees generated. Although the n-best list reaches the limit for the majority of the sentences, some sentences have a smaller number of trees, which signifies less possible tree derivations, i.e. less parsing ambiguity, a feature which would be useful for our use. Additional"
C12-1008,N07-1051,0,0.0239019,"puts for one source sentence are often spanned along many 5-way rankings. For the test set, the multiple rankings of the same source sentence (produced by all available systems) were aggregated into one ranking, and the ties on both pairwise and ranking level were removed. One should also notice that repetitive human rankings of the same systems often disagree with each other, which signifies the existence of some noise in our data. 4.2 Implementation PCFG parsing features were generated on the output of the Berkeley Parser, with the default grammars based on an English and a German treebank (Petrov and Klein, 2007). N-gram features were based on language models of order 5, built with the SRILM toolkit (Stolcke, 2002) on monolingual training material from the Europarl (Koehn, 2005) and the News (CallisonBurch et al., 2011) corpora. The Acrolinx IQ2 was used to annotate source and target with language checking suggestions and provide style, grammar and spelling scores. The annotation process was organized with the Ruffus library (Goodstadt, 2010) and the learning algorithms were executed using the Orange toolkit (Demšar et al., 2004). 4.3 Strategy The amount of features and learning options provide an exp"
C12-1008,2009.eamt-1.15,0,0.0132974,"the fields of Hybrid MT, System Combination and MT Evaluation. The applicability is even broader, as the approach presented is system-independent and relies on generic automatic analysis applied on any input containing sets of one source and several translation outputs. 2 Previous work Quality Estimation is a rather recent aspect in research on Machine Translation. As a field, it tries to provide quality assessment on the translation output without the availability of reference translations. Previous work includes statistical methods on predicting word-level confidence (Ueffing and Ney, 2005; Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (20"
C12-1008,N07-1029,0,0.0232185,"the availability of reference translations. Previous work includes statistical methods on predicting word-level confidence (Ueffing and Ney, 2005; Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then"
C12-1008,2011.eamt-1.15,0,0.0116816,"Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranki"
C12-1008,W12-3121,0,0.0124971,"tive estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranking in MT evaluation, by developing a machine learning approach to train on rank data, though these are using reference translations and are only evaluated by producing an overall corpus-level ranking. METEOR, one of the state-of-the-art evaluation metrics (Lavie and Agarwal, 2007) also gets its components tuned over human rankings. Avramidis et al. (2011) do MT evaluation witho"
C12-1008,W12-3118,0,0.464085,"ication of fluency, since they provide statistics on how likely the sequences of the words are for a particular language. Although Statistical MT systems are expected to already optimize over the language model, as mentioned above, other types of systems may still benefit from this features. This feature category includes the smoothed n-gram probability of the sentence. • Contrastive evaluation scores: Each translation is scored with an automatic metric (e.g. Papineni et al., 2002), using the competitive translations as references. This has shown to perform well as a feature in similar tasks (Soricut et al., 2012). Keeping the isomorphism assumption, an additional hint for the adequacy of the translation was applied for the features that are apparent in both source and target: The ratio of these features was calculated by diving the feature value of each one of the translation outputs with the respective feature value of the source. 3.5 Machine learning algorithms The modular approach of the pairwise classification allowed the use of several machine learning algorithms as part of the system core. 121 • Naïve Bayes predicts the probability of a binary class c given a set of features p(c, f1 , . . . , f"
C12-1008,W12-3110,0,0.138463,"local maxima, but it should suffice if it can provide a functioning system confirming the original idea. 1 2 3 In all of the experiments we excluded the crowdsourced sentences contained in the set of 2010 http://www.acrolinx.com (proprietary) We tried to come as close to the original feature set when not all features were technically available 124 4.4 Results 4.4.1 Searching for the best system The search through different combinations of feature sets and classification methods is depicted in Table 1. Feature sets 2 - 5 derive from previous work (Soricut et al., 2012; Avramidis et al., 2011; Specia et al., 2012) and are explained in Table 3. Out of these, it appears that feature set 2 is the most successful one for this particular problem, providing a correlation which is acceptable to begin with. K-nn slightly outperforms Naïve Bayes. Consequently, extensions to feature set 2 are considered for further experimentation. Feature set 2.1 provides an improved combination with logistic regression: It derives from the same annotation as feature set 2, with the difference that the features of the target had not been not divided with the features of the source, in order to provide a fixed ratio as a feature"
C12-1008,2009.eamt-1.5,0,0.0205785,"ystem-independent and relies on generic automatic analysis applied on any input containing sets of one source and several translation outputs. 2 Previous work Quality Estimation is a rather recent aspect in research on Machine Translation. As a field, it tries to provide quality assessment on the translation output without the availability of reference translations. Previous work includes statistical methods on predicting word-level confidence (Ueffing and Ney, 2005; Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should"
C12-1008,H05-1096,0,0.0262757,"s in MT, as it touches the fields of Hybrid MT, System Combination and MT Evaluation. The applicability is even broader, as the approach presented is system-independent and relies on generic automatic analysis applied on any input containing sets of one source and several translation outputs. 2 Previous work Quality Estimation is a rather recent aspect in research on Machine Translation. As a field, it tries to provide quality assessment on the translation output without the availability of reference translations. Previous work includes statistical methods on predicting word-level confidence (Ueffing and Ney, 2005; Raybaud et al., 2009b), correctness of a sentence (Blatz et al., 2004) and has been recently evolved into a regression problem (Specia et al., 2009; Raybaud et al., 2009a) for estimating correctness scores or correctness probabilities. Whereas the aforementioned work has been focusing on estimating absolute measures of quality for a single output, our focus is on the comparative estimation of quality among several system outputs. In this direction, Rosti et al. (2007) perform sentence-level selection with generalized linear models, based on re-ranking N-best lists merged from many MT systems"
C12-1008,2011.iwslt-evaluation.13,1,0.888205,"Missing"
C12-1008,W07-0736,0,0.0285218,"st lists merged from many MT systems. Sánchez-Martínez (2011) uses only source-language information in order to build a classifier which chooses which machine translation system should be used in order to translate a sentence. As an application to statistical MT tuning, Hopkins and May (2011) improve the tuning MERT process by using the pairwise approach of ranking with a classifier. Others (Vilar et al., 2011; Soricut and Narsale, 2012) use machine learning for ranking the candidate translations and then selecting the highest-ranked translation as the final output. A couple of contributions (Ye et al., 2007; Duh, 2008) introduce the idea of using ranking in MT evaluation, by developing a machine learning approach to train on rank data, though these are using reference translations and are only evaluated by producing an overall corpus-level ranking. METEOR, one of the state-of-the-art evaluation metrics (Lavie and Agarwal, 2007) also gets its components tuned over human rankings. Avramidis et al. (2011) do MT evaluation without references based on learned ranking, by using parsing features and Parton et al. (2011) also show a configuration of their metric which achieves good correlation with huma"
C12-1008,W12-3102,0,\N,Missing
C12-1008,W09-0401,0,\N,Missing
C12-1008,W07-0734,0,\N,Missing
federmann-etal-2012-ml4hmt,vandeghinste-etal-2008-evaluation,1,\N,Missing
federmann-etal-2012-ml4hmt,federmann-2010-appraise,1,\N,Missing
federmann-etal-2012-ml4hmt,E06-1005,0,\N,Missing
federmann-etal-2012-ml4hmt,W09-0424,0,\N,Missing
federmann-etal-2012-ml4hmt,P02-1040,0,\N,Missing
federmann-etal-2012-ml4hmt,W02-1019,0,\N,Missing
federmann-etal-2012-ml4hmt,W05-0909,0,\N,Missing
federmann-etal-2012-ml4hmt,W08-0309,0,\N,Missing
federmann-etal-2012-ml4hmt,W10-1720,1,\N,Missing
federmann-etal-2012-ml4hmt,W11-2101,0,\N,Missing
L16-1296,W15-5705,1,0.838417,"Missing"
L16-1296,P02-1040,0,0.134922,"Missing"
N19-4006,N19-1423,0,0.0117602,"t with α = 0.001. 6 Future Work The inputs in Fig. 3 contain easily readable evidence. There is, however, also much evidence that is hard to read. In general, we can assume that with increasing architectural complexity, more complex class evidence can be uncovered, which may come at the cost of harder readability. In the future, it is worth exploring how different architectures and model choices affect the quality, complexity and readability of the uncovered evidence. For instance, one direction would be to to train the classifier on top of a pretrained language model (Howard and Ruder, 2018; Devlin et al., 2019) which could improve the classification performance. Furthermore, other explainability methods should also be tested. 7 Conclusion We presented a new approach to analyse and juxtapose translations. Furthermore, we also presented an implementation of the approach, DiaMaT. DiaMaT exploits the generalization power of neural networks to learn systematic differences between human and machine translations and then takes advantage of neural explainability methods to uncover these. It learns from corpora containing millions of translations but offers explanations on sentence level. In a stress test, D"
N19-4006,W17-5221,0,0.0681906,"Missing"
N19-4006,L18-1550,0,0.0315766,"Missing"
N19-4006,W18-5434,1,0.453082,"Missing"
N19-4006,berka-etal-2012-automatic,0,0.0570323,"Missing"
N19-4006,P18-1031,0,0.163826,"alizes well after training, it has learned to recognize 29 Proceedings of NAACL-HLT 2019: Demonstrations, pages 29–34 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics systematic or frequent differences between the two classes (herinafter also referred to as “class evidence”). Class evidence may be, for instance, style differences, overused n-grams but also errors. The text classifier can be implemented through various architectures, ranging from deep CNNs (Conneau et al., 2017) to recurrent classifiers built on top of pre-trained language models (Howard and Ruder, 2018). 2.2 have contributed for a given example to produce a decision (e.g. classification or regression).”1 In our case the interpretable domain is the plain text space. There exist several candidate explainability methods, one of which we present in the following as an example. Step 2: Sort In a second step, we suggest letting the trained classifier predict the labels of a test set which contains human and machine translations and then sort them by classification confidence. This is based on the assumption that if the classifier is very certain that a given translation was produced by a machine ("
N19-4006,J82-2005,0,0.640537,"Missing"
N19-4006,P02-1040,0,0.103341,"Missing"
N19-4006,P17-4012,0,\N,Missing
N19-4006,W18-6401,0,\N,Missing
N19-4006,E17-1104,0,\N,Missing
P08-1087,W07-0702,1,0.329256,"s with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vi"
P08-1087,H92-1022,0,0.0638621,"spectively, following the specifications made by the ACL 2007 2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), a"
P08-1087,E06-1032,1,0.454206,"Missing"
P08-1087,D07-1007,0,0.0280456,"features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supe"
P08-1087,W05-0620,0,0.0160883,"e “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method similar to Semantic Role Labelling (Carreras and Marquez, 2005; Surdeanu and Turmo, 2005). English, as morphologically poor language, usually follows a fixed word order (subject-verb-object), so that a syntax parser can be easily used for identifying the subject and the object of most sentences. Considering such annotation, a factored translation model is trained to map the word-case pair to the correct inflection of the target noun. Given the agreement restriction, all words that accompany the noun (adjectives, articles, determiners) must follow the case of the noun, so their likely case needs to be identified as well. For this purpose we use a syntax p"
P08-1087,P97-1003,0,0.128471,"ative paths are combined as following (fig. 5): hT (f |e) = X hTt(j) (ej , f j ) (3) News Commentary respectively, following the specifications made by the ACL 2007 2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1"
P08-1087,P05-1066,1,0.636806,"Missing"
P08-1087,W06-3102,0,0.0326234,"Missing"
P08-1087,D07-1116,0,0.0189629,"ested a post-processing system which uses morphological and syntactic features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in fac"
P08-1087,N06-2013,0,0.0164687,"itional statistical machine translation methods are based on mapping on the lexical level, which takes place in a local window of a few words. Hence, they fail to produce adequate output in many cases where more complex linguistic phenomena play a role. Take the example of morphology. Predicting the correct morphological variant for a target word may not depend solely on the source words, but require additional information about its role in the sentence. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). There has been less work on the opposite case, translating from English into morphologically richer languages. In a study of translation quality for languages in the Europarl corpus, Koehn (2005) reports that translating into morphologically richer languages is more difficult than translating from them. There are intuitive reasons why generating richer morphology from morphologically poor languages is harder. Take the example of translating noun phrases from English to Greek (or German, Czech, etc.). In English, a noun phrase is rendered the same if it is the subject or the object. However,"
P08-1087,2006.amta-papers.8,0,0.0112658,"mar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vilar et al. (2006) order to aid reordering (Yamada and Knight, 2001; Collins et al., 2005; Huang et al., 2006), but as a means for getting the “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method simil"
P08-1087,2005.mtsummit-papers.11,1,0.0189893,"ore complex linguistic phenomena play a role. Take the example of morphology. Predicting the correct morphological variant for a target word may not depend solely on the source words, but require additional information about its role in the sentence. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). There has been less work on the opposite case, translating from English into morphologically richer languages. In a study of translation quality for languages in the Europarl corpus, Koehn (2005) reports that translating into morphologically richer languages is more difficult than translating from them. There are intuitive reasons why generating richer morphology from morphologically poor languages is harder. Take the example of translating noun phrases from English to Greek (or German, Czech, etc.). In English, a noun phrase is rendered the same if it is the subject or the object. However, Greek words in noun phrases are inflected based on their role in the sentence. A purely lexical mapping of English noun phrases to Greek noun phrases suffers from the lack of information about its"
P08-1087,D07-1091,1,0.769511,"ed higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowledge. Habash et al. (2007) also investigated case determination in Arabic. Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags. The morphological factors are added on the morphologically rich side and scored with a 7-gram sequence model. Probabilistic models for using only source tags were investigated by Birch et al. (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses o"
P08-1087,P07-2045,1,0.0131712,"s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model was trained on 440,082 aligned sentences of Europarl v.3, tuned with Minimum Error Training (Och, 2003). It was tuned over a development set of 2,000 Europarl sentences and tested on two sets of 2,000 sentences each, from the Europarl and a 767 BLEU devtest test07 18.13 18.05 18.16 18.17 18.14 18.16 18.08 18.24 18.21 18.20 NIST devtest test07 5.218 5.279 5.224 5.316 5.259 5.316 5.258 5.340 5.285 5.340 Table 1: Translating English to Greek: Using a single translation table may cause sparse data problems, which are address"
P08-1087,P07-1017,0,0.170023,"chunks that are translated (see Figure 1 for an example). 764 In one of the first efforts to enrich the source in word-based SMT, Ueffing and Ney (2003) used partof-speech (POS) tags, in order to deal with the verb conjugation of Spanish and Catalan; so, POS tags were used to identify the pronoun+verb sequence and splice these two words into one term. The approach was clearly motivated by the problems occurring by a single-word-based SMT and have been solved by adopting a phrase-based model. Meanwhile, there is no handling of the case when the pronoun stays in distance with the related verb. Minkov et al. (2007) suggested a post-processing system which uses morphological and syntactic features, in order to ensure grammatical agreement on the output. The method, using various grammatical source-side features, achieved higher accuracy when applied directly to the reference translations but it was not tested as a part of an MT system. Similarly, translating English into Turkish (Durgar El-Kahlout and Oflazer, 2006) uses POS and morph stems in the input along with rich Turkish morph tags on the target side, but improvement was gained only after augmenting the generation process with morphotactical knowle"
P08-1087,P03-1021,0,0.00486831,"t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model was trained on 440,082 aligned sentences of Europarl v.3, tuned with Minimum Error Training (Och, 2003). It was tuned over a development set of 2,000 Europarl sentences and tested on two sets of 2,000 sentences each, from the Europarl and a 767 BLEU devtest test07 18.13 18.05 18.16 18.17 18.14 18.16 18.08 18.24 18.21 18.20 NIST devtest test07 5.218 5.279 5.224 5.316 5.259 5.316 5.258 5.340 5.285 5.340 Table 1: Translating English to Greek: Using a single translation table may cause sparse data problems, which are addressed using an alternative path to a second translation table We tested several various combinations of tags, while using a single translation component. Some combinations seem to"
P08-1087,2001.mtsummit-papers.68,0,0.0397069,"2nd Workshop on SMT1 . A Czech model was trained on 57,464 aligned sentences, tuned over 1057 sentences of the News Commentary corpus and and tested on two sets of 964 sentences and 2000 sentences respectively. The training sentences were trimmed to a length of 60 words for reducing perplexity and a standard lexicalised reordering, with distortion limit set to 6. For getting the syntax trees, the latest version of Collins’ parser (Collins, 1997) was used. When needed, part-of-speech (POS) tags were acquired by using Brill’s tagger (Brill, 1992) on v1.14. Results were evaluated with both BLEU (Papineni et al., 2001) and NIST metrics (NIST, 2002). j where each phrase j is translated by one translation table t(j) and each table i has a feature function hTi . as shown in eq. (2). 4.2 Results set baseline person pos+person person+case altpath:POS Figure 5: Decoding using an alternative path with different factorization 4 Experiments This preprocessing led to annotated source data, which were given as an input to a factored SMT system. 4.1 Experiment setup For testing the factored translation systems, we used Moses (Koehn et al., 2007), along with a 5-gram SRILM language model (Stolcke, 2002). A Greek model w"
P08-1087,W05-0635,0,0.00954772,"mation, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes advantage of syntax, following a method similar to Semantic Role Labelling (Carreras and Marquez, 2005; Surdeanu and Turmo, 2005). English, as morphologically poor language, usually follows a fixed word order (subject-verb-object), so that a syntax parser can be easily used for identifying the subject and the object of most sentences. Considering such annotation, a factored translation model is trained to map the word-case pair to the correct inflection of the target noun. Given the agreement restriction, all words that accompany the noun (adjectives, articles, determiners) must follow the case of the noun, so their likely case needs to be identified as well. For this purpose we use a syntax parser to acquire the syntax"
P08-1087,vilar-etal-2006-error,0,0.0119775,"Missing"
P08-1087,P01-1067,0,0.0900552,"models by having Combinatorial Categorial Grammar (CCG) supertags as factors on the input words, but in this case English was the target language. This paper reports work that strictly focuses on translation from English to a morphologically richer language. We go one step further than just using easily acquired information (e.g. English POS or lemmata) and extract target-specific information from the source sentence context. We use syntax, not in Figure 2: Classification of the errors on our EnglishGreek baseline system (ch. 4.1), as suggested by Vilar et al. (2006) order to aid reordering (Yamada and Knight, 2001; Collins et al., 2005; Huang et al., 2006), but as a means for getting the “missing” morphology information, depending on the syntactic position of the words of interest. Then, contrary to the methods that added only output features or altered the generation procedure, we used this information in order to augment only the source side of a factored translation model, assuming that we do not have resources allowing factors or specialized generation in the target language (a common problem, when translating from English into under-resourced languages). 2 Therefore, the followed approach takes ad"
P08-1087,P02-1040,0,\N,Missing
W11-2104,C04-1046,0,0.80923,"of every given sentence. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training"
W11-2104,W08-0309,0,0.121698,"ch metrics have been known as Confidence Estimation metrics and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that perfo"
W11-2104,W10-1703,0,0.363914,"s and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that performs similar to the human ranking: it is trained to rank sev"
W11-2104,W09-3712,0,0.0156559,"ng, the classifiers were used to perform ranking on a test set of 184 sentences which had been kept apart from the 2010 data, with the criterion that they do not contain contradictions among human judgments. In order to allow further comparison with other evaluation metrics, we performed an extended experiment: we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to per"
W11-2104,N07-1051,0,0.0080242,": we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to perform better given a smaller group of statistically independent features. Since exhaustive training/testing of all possible feature subsets was not possible, we performed feature selection based on the Relieff method (Kononenko, 1994; Kira and Rendell, 1992). Automatic ranking was performed based on the most"
W11-2104,P06-1055,0,0.00745077,"e pairwise comparison of system outputs ti and tj with respective ranks ri and rj , determined as:  1 ri &lt; rj c(ri , rj ) = −1 ri &gt; rj At testing time, after the classifier has made all the pairwise decisions, those need to be converted back to ranks. System entries are ordered, according to how many times each of them won in the pairwise comparison, leading to rank lists similar to the ones provided by human annotators. Note that this kind of decomposition allows for ties when there are equal times of winnings. 66 Acquiring features • Parsing: Processing features acquired from PCFG parsing (Petrov et al., 2006) for both source and target side include: – – – – parse log likelihood, number of n-best trees, confidence for the best parse, average confidence of all trees. Ratios of the above target features to their respective source features were included. • Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. In particular, NPs, VPs, PPs, NNs and punctuation occurences were counted. Then the ratio of the occurences of each tag in the target sentence by its occurences on the source sentence was also calculated. 2.4"
W11-2104,2009.eamt-1.15,0,0.0842697,"Missing"
W11-2104,N07-1029,0,0.0599919,"Missing"
W11-2104,2009.mtsummit-papers.16,0,0.0218978,"teria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating against this data. Evalu"
W11-2104,H05-1096,0,0.181409,"nce. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating aga"
W11-2104,W09-0401,0,\N,Missing
W11-2104,J06-4003,0,\N,Missing
W11-2109,W05-0909,0,0.0830542,"xplored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of ev"
W11-2109,J93-2003,0,0.0279455,"st be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of evaluating translation quality (e.g., completeness, ordering, domain fit, etc.) is transformed into a kind of paraphrase evaluation in the target language, which is a very difficult problem itself. In addition, the set of selected references always represents only a small subset of all good translations. To remedy these drawbacks, we propose a truly automatic evaluation metric which is based on the IBM 1 lexicon scores (Brown et al., 1993). The inclusion of IBM 1 scores in translation systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated"
W11-2109,W08-0309,0,0.0789113,"n experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P"
W11-2109,W10-1703,0,0.0538159,"have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P (f1J |eI1 ) = 3 Experiments on WMT 2008, WMT 2009 and WMT 2010 test data I J"
W11-2109,P02-1040,0,0.0908633,"BM 1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using refer"
W11-2109,E09-1087,0,0.0855911,"Missing"
W11-2109,W09-0401,0,\N,Missing
W11-2109,C04-1046,0,\N,Missing
W12-3108,W11-2104,1,0.886062,"Missing"
W12-3108,C04-1046,0,0.258703,"2.1 1 Introduction As Machine Translation (MT) gradually gains a position into production environments, the need for estimating the quality of its output is increasing. Various use cases refer to it as input assessment for Human Post-editing, as an extension for Hybrid MT or System Combination, or even a method for improving components of existing MT systems. With the current submission we are trying to address the problem of assigning a quality score to a single MT output per source sentence. Previous work includes regression methods for indicating a binary value of correctness (Quirk, 2001; Blatz et al., 2004; Ueffing and Ney, 2007), human-likeness (Gamon et al., 2005) or continuous scores (Specia et al., 2009). As we also work with continuous scores, we are making an effort to combine previous feature acquisition sources, Data and basic approach This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al., 2012). The organizers provided an English-to-Spanish development set and a test set of 1832 and 422 sentences respectively, derived from WMT09 and WMT10 datasets. For each source sentence of"
W12-3108,W12-3102,0,0.102295,"Missing"
W12-3108,2005.eamt-1.15,0,0.146392,"ns a position into production environments, the need for estimating the quality of its output is increasing. Various use cases refer to it as input assessment for Human Post-editing, as an extension for Hybrid MT or System Combination, or even a method for improving components of existing MT systems. With the current submission we are trying to address the problem of assigning a quality score to a single MT output per source sentence. Previous work includes regression methods for indicating a binary value of correctness (Quirk, 2001; Blatz et al., 2004; Ueffing and Ney, 2007), human-likeness (Gamon et al., 2005) or continuous scores (Specia et al., 2009). As we also work with continuous scores, we are making an effort to combine previous feature acquisition sources, Data and basic approach This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al., 2012). The organizers provided an English-to-Spanish development set and a test set of 1832 and 422 sentences respectively, derived from WMT09 and WMT10 datasets. For each source sentence of the development set, participants were offered one translatio"
W12-3108,W11-2111,0,0.0388945,"Missing"
W12-3108,N07-1051,0,0.0135473,"ncorrelated with each other; selection was done in two variations, greedy stepwise and best first. The data were discretised according to the algorithm requirements and features were scored in a 10fold cross-validation. Regression algorithms produce a model for directly predicting a quality score with continuous values. Experimentation here included Partial Least Squares Regression (Stone and Brooks, 1990), Multivariate Adaptive Regression Splines – MARS (Friedman, 1991), Lasso (Tibshirani, 1994) and Linear Regression. PCFG parsing features were generated on the output of the Berkeley Parser (Petrov and Klein, 2007), trained over an English and a Spanish treebank (Mariona Taul´e and Recasens, 2008). Ngram features have been generated with the SRILM toolkit (Stolcke, 2002). The Acrolinx IQ1 was used to parse the source side, whereas the Language Tool2 was applied on both sides. The feature selection and learning algorithms were implemented with the Orange (Demˇsar et al., 2004) and Weka (Hall et al., 2009) toolkits. 2.4 Machine Learning 3.2 We tried to approach the issue with two distinct modelling approaches, classification and regression. The methods explained in the previous section provide a wide rang"
W12-3108,P06-1055,0,0.0394275,"ed sentence (averaged for all words in the hypothesis - type/token ratio) • IBM1-model lookup: Average number of translations per source word in the sentence, unweighted or weighted by the inverse frequency of each word in the source corpus • Language modeling: Language model probability of the source and translated sentence • Corpus lookup: percentage of unigrams / bigrams / trigrams in quartiles 1 and 4 of frequency (lower and higher frequency words) in a corpus of the source language Additionally, the following linguistically motivated features were also included: 85 • Parsing: PCFG Parse (Petrov et al., 2006) loglikelihood, size of n-best tree list, confidence for the best parse, average confidence of all parse trees. Ratios of the mentioned target features to the corresponding source features. • Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. Additionally, the ratio of the occurences of each tag in the target sentence by the corresponding occurences on the source sentence. • Language quality check: Source and target sentences were subject to automatic rule-based language quality checking, providing a wid"
W12-3108,2011.eamt-1.15,0,0.0238042,"Missing"
W12-3108,2009.eamt-1.5,0,0.0704688,", the need for estimating the quality of its output is increasing. Various use cases refer to it as input assessment for Human Post-editing, as an extension for Hybrid MT or System Combination, or even a method for improving components of existing MT systems. With the current submission we are trying to address the problem of assigning a quality score to a single MT output per source sentence. Previous work includes regression methods for indicating a binary value of correctness (Quirk, 2001; Blatz et al., 2004; Ueffing and Ney, 2007), human-likeness (Gamon et al., 2005) or continuous scores (Specia et al., 2009). As we also work with continuous scores, we are making an effort to combine previous feature acquisition sources, Data and basic approach This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al., 2012). The organizers provided an English-to-Spanish development set and a test set of 1832 and 422 sentences respectively, derived from WMT09 and WMT10 datasets. For each source sentence of the development set, participants were offered one translation generated by a state-ofthe-art phrase-bas"
W12-3108,J07-1003,0,0.0310114,"s Machine Translation (MT) gradually gains a position into production environments, the need for estimating the quality of its output is increasing. Various use cases refer to it as input assessment for Human Post-editing, as an extension for Hybrid MT or System Combination, or even a method for improving components of existing MT systems. With the current submission we are trying to address the problem of assigning a quality score to a single MT output per source sentence. Previous work includes regression methods for indicating a binary value of correctness (Quirk, 2001; Blatz et al., 2004; Ueffing and Ney, 2007), human-likeness (Gamon et al., 2005) or continuous scores (Specia et al., 2009). As we also work with continuous scores, we are making an effort to combine previous feature acquisition sources, Data and basic approach This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al., 2012). The organizers provided an English-to-Spanish development set and a test set of 1832 and 422 sentences respectively, derived from WMT09 and WMT10 datasets. For each source sentence of the development set, par"
W12-3108,quirk-2004-training,0,\N,Missing
W12-3108,taule-etal-2008-ancora,0,\N,Missing
W13-2240,C12-1008,1,0.893937,"features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can take advantage of several powerful binary classification methods (Avramidis, 2012). We used logistic regression, which optimizes a logistic function to predict values in the range between zero and one (Cameron, 1998), given a feature set X: P (X) = 1 1+ e−1(a+bX) (1) Regression The sentence-ranking sub-task has provided training data for two language pairs, German-English and English-Spanish. For both sentence pairs, we train the systems using the provided annotated data sets WMT2010, WMT2011 and WMT2012, while the data set WMT2009 is used for the evaluation during the development phase. Data sets are analyzed with black-box feature generation. For each language pair, the t"
W13-2240,2005.mtsummit-papers.11,0,0.023686,"orithm to iteratively minimize the least squares error computed from training data (Miller, 2002). Experiments are repeated with two variations of Logistic Regression concerning internal features treatment: Stepwise Feature Set Selection (Hosmer, 1989) and L2-Regularization (Lin et al., 2007). Relieff is implemented for k=5 nearest neighbours sampling m=100 reference instances. Information gain is calculated after discretizing features into n=100 values N-gram features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). PCFG parsing features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can ta"
W13-2240,W11-2104,1,0.623947,"on of the position of the unknown words. Black-box features Features of this type are generated as a result of automatic analysis of both the source sentence and the MT output (when applicable), whereas many of them are already part of the baseline infrastructure. For all features we also calculate the ratios of the source to the target sentence. These features include: PCFG Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of VPs, NPs etc.), the parse log-likelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). Log probability (pC) and future cost estimate (c) of the phrases chosen as part of the best translation: minimum and maximum values and their position in the sentence averaged to the number of sentences, and also their average, variance, standard deviation; count of the phrases whose probability or future cost estimate is lower and higher than their standard deviation; the ratio of these phrases to the total number of phrases. Rule-based language correction is a result of hand-written controlled language rules, that indicate mistakes on several pre-defined error categories (Naber, 2003). We"
W13-2240,W07-0734,0,0.0259301,"1 Information gain Information gain (Hunt et al., 1966) estimates the difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher ranks (i.e. better transl"
W13-2240,W13-2202,0,0.0864643,"Missing"
W13-2240,P02-1040,0,0.0945526,"rch et al., 2012). 2.2.1 Information gain Information gain (Hunt et al., 1966) estimates the difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher"
W13-2240,P06-1055,0,0.0492042,"unknown to the phrase table, average number of unknown words first/last position of an unknown word in the sentence normalized to the number of tokens, variance and deviation of the position of the unknown words. Black-box features Features of this type are generated as a result of automatic analysis of both the source sentence and the MT output (when applicable), whereas many of them are already part of the baseline infrastructure. For all features we also calculate the ratios of the source to the target sentence. These features include: PCFG Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of VPs, NPs etc.), the parse log-likelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). Log probability (pC) and future cost estimate (c) of the phrases chosen as part of the best translation: minimum and maximum values and their position in the sentence averaged to the number of sentences, and also their average, variance, standard deviation; count of the phrases whose probability or future cost estimate is lower and higher than their standard deviation; the ratio of these phrases to the total number of"
W13-2240,N07-1051,0,0.0145822,"ations of Logistic Regression concerning internal features treatment: Stepwise Feature Set Selection (Hosmer, 1989) and L2-Regularization (Lin et al., 2007). Relieff is implemented for k=5 nearest neighbours sampling m=100 reference instances. Information gain is calculated after discretizing features into n=100 values N-gram features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). PCFG parsing features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can take advantage of several powerful binary classification methods (Avramidis, 2012). We used logistic regression, which optimizes a logistic function t"
W13-2240,W12-3118,0,0.172404,"difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher ranks (i.e. better translations) are ordered correctly, as these are more important than low"
W13-2240,taule-etal-2008-ancora,0,0.0896671,"Missing"
W13-2240,W12-3102,0,\N,Missing
W14-3337,W11-2104,1,0.901804,"Missing"
W14-3337,N07-1051,0,0.0302621,"Missing"
W14-3337,P06-1055,0,0.0482552,"all words in the hypothesis (type/token ratio). 3.2.2 Additional features Additionally to the baseline features, the following feature groups are considered: Klein, 2007) trained over an English and a Spanish treebank (Taul´e et al., 2008).2 Baseline features are extracted using Quest and HTER edits and scores are recalculated by modifying the original TERp code. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Scikit Learn toolkit (Pedregosa et al., 2011). Parsing Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of verb phrases, noun phrases etc.), the parse loglikelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). In order to reduce unnecessary noise, in some experiments we separate a group of “basic” parsing labels, which include only verb phrases, noun phrases, adjectives and subordinate clauses. 4.2 Evaluation All specific model parameters were tested with cross validation with 10 equal folds on the training data. Cross validation is useful as it reduces the possibility of overfitting, yet using the entire am"
W14-3337,2006.amta-papers.25,0,0.0247829,"ction As Machine Translation (MT) gets integrated into regular translation workflows, its use as base for post-editing is radically increased. As a result, there is a great demand for methods that can automatically assess the MT outcome and ensure that it is useful for the translator and can lead to more productive translation work. Although many agree that the quality of the MT output itself is not adequate for the professional standards, there has not yet been a widelyaccepted way to measure its quality on par with human translations. One such metric, the Human Translation Edit Rate (HTER) (Snover et al., 2006), is the focus of the current submission. HTER is highly relevant to the need of adapting HTER = #insertions + #dels + #subs + #shifts #reference words We notice that the metric is clearly based on four edit types that are seemingly independent of each other. This poses the question whether the existing 302 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 302–306, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics threshold are omitted. approach of learning the entire metric altogether introduces way too much complexity in the"
W14-3337,W12-3102,0,0.060835,"Missing"
W14-3337,taule-etal-2008-ancora,0,0.0639697,"Missing"
W14-3337,2005.mtsummit-papers.11,0,0.0174113,"c model parameters were tested with cross validation with 10 equal folds on the training data. Cross validation is useful as it reduces the possibility of overfitting, yet using the entire amount of data. The regression task is evaluated in terms of Mean Average Error (MAE). Experiment setup 4.1 Implementation 5 The open source language tool1 is used to annotate source and target sentences with automatically detected monolingual error tags. Language model features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl v7.0 (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). For the parsing parsing features we used the Berkeley Parser (Petrov and 1 Data In our effort to reproduce HTER in a higher granularity, we noticed that HTER scoring on the official data was reversed: the calculation was performed by using the MT output as reference and the human post-edition as hypothesis. Therefore, the denominator on the “official” scores is the number of tokens on the MT output. This makes the prediction even easier, as this number of tokens is always known. Apart from the data provided by the WMT14, we include additional"
W14-5104,avramidis-etal-2012-richly,1,0.867792,"Missing"
W14-5104,W14-3339,0,0.0137678,"x features in order to predict numerical indications of translation quality, such as postediting effort (Rubino et al., 2013; Hildebrand and Vogel, 2013) or post-editing time (Avramidis et al., 2013). Contrary to these works, we only predict specific error types, with the focus on understanding the contribution of the features. Prediction of specific error types was included in the shared tasks of the 8th and 9th Workshop on Statistical Machine Translation (Bojar et al., 2013; Bojar et al., 2014). Several participants contributed systems that predict error types (Besacier and Lecouteux, 2013; Bicici and Way, 2014; de Souza et al., 2014). In that case, prediction was done on the word level and contrary to our experiments, no glass-box features were used, therefore there was no connection of the ML with the decoding process. Guzm´an and Vogel (2012), in the work that is most related to ours, aim to identify the contribution of the features. Similar to several previously mentioned works, a multivariate linear regression model is trained in order to predict continuous quality values of complex metrics. Although the aim of this work is similar to ours, we work in a more fine-grained way: instead of modelli"
W14-5104,C04-1046,0,0.0295306,"utput (Section 4.1). D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decoding process. The first experiments on “Confidence Estimation” make use of a small number of Statistical Machine Translation (SMT) features in order to train a supervised model for predicting the quality of the Translation (Blatz et al., 2004). Later work, identified as “Quality Estimation”, defines such features as “glass-box” features (Specia et al., 2009). 54 glass-box features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use gla"
W14-5104,W07-0718,0,0.0157585,"e pair lang de-en en-de de-fr fr-de de-es es-de all sentences total p.e 1811 1139 1101 315 982 198 1051 122 543 543 931 931 6419 3248 reordering err. total p.e 1043 474 891 232 819 157 851 88 288 288 345 345 4237 1584 missing words total p.e 1079 570 671 151 597 80 691 76 322 322 333 333 3693 1532 extra words total p.e 869 454 722 208 630 147 621 66 186 186 339 339 3367 1400 Table 3: The size of the corpus per error category and language pair. p.e. indicates the number of sentences that were minimally post-edited by professional translators with MERT using the news corpus test set from WMT07 (Callison-Burch et al., 2007). The decoding features are extracted from Moses’ verbose output of level 2. Our target language model with an order of 5 is trained with SRILM toolkit (Stolcke, 2002), based on the respective monolingual training material. The Orange toolkit (Demˇsar et al., 2004) is used for processing and running the Logistic Regression algorithms. The Hjerson tool (Popovi´c, 2011b) was used in order to detect errors on the translation. 6 6.1 Results Model performance A necessary step is to check how well each model fits the data, since a well-fit model is required for drawing conclusions. For this purpose"
W14-5104,W14-3340,0,0.0335952,"Missing"
W14-5104,C12-1063,0,0.0330443,"Missing"
W14-5104,W13-2246,0,0.0308525,"features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use glass-box features in order to predict numerical indications of translation quality, such as postediting effort (Rubino et al., 2013; Hildebrand and Vogel, 2013) or post-editing time (Avramidis et al., 2013). Contrary to these works, we only predict specific error types, with the focus on understanding the contribution of the features. Prediction of specific error types was included in the shared tasks of the 8th and 9th Workshop on Statistical Machine Translation (Bojar et al., 2013; Bojar et al., 2014). Several participants contributed systems that predict error types (Besacier and Lecouteux, 2013; Bicici and Way, 2014; de Souza et al., 2014). In that case, prediction was done on the word level and contrary to our experiments, no glass-box features"
W14-5104,P07-1019,0,0.0351913,"aining a statistical model on these error categories.1 . In order to detect the errors on the translation output, we follow the automatic method by Popovi´c and Ney (2011), which has shown to correlate well with human error annotation. This method automatically detects errors based on the edit distance of the produced translation against a reference human translation. An example of how errors are detected can be seen in Figure 1. 4.2 Phrase-based SMT search graph The glass-box features are extracted from the decoding process of a phrase-based SMT system (Koehn et al., 2003) with cube-pruning (Huang and Chiang, 2007). The decoding process performs a search in various dimensions, calculating scores for many phrases and hypothesis expansions. Most scores are difficult to be interpreted as glass-box features in their initial form. The amount of scores calculated per sentence is not fixed, whereas the basic requirement for each feature is to have only one value that is valid on a sentence level, so that it can be used in the sentence error prediction model. For this purpose, we process the verbose output of the decoder and derive scores, counts and other statistics that can have this sentence-level interpreta"
W14-5104,N03-1017,0,0.00998221,"our data give sufficient amounts for training a statistical model on these error categories.1 . In order to detect the errors on the translation output, we follow the automatic method by Popovi´c and Ney (2011), which has shown to correlate well with human error annotation. This method automatically detects errors based on the edit distance of the produced translation against a reference human translation. An example of how errors are detected can be seen in Figure 1. 4.2 Phrase-based SMT search graph The glass-box features are extracted from the decoding process of a phrase-based SMT system (Koehn et al., 2003) with cube-pruning (Huang and Chiang, 2007). The decoding process performs a search in various dimensions, calculating scores for many phrases and hypothesis expansions. Most scores are difficult to be interpreted as glass-box features in their initial form. The amount of scores calculated per sentence is not fixed, whereas the basic requirement for each feature is to have only one value that is valid on a sentence level, so that it can be used in the sentence error prediction model. For this purpose, we process the verbose output of the decoder and derive scores, counts and other statistics t"
W14-5104,2005.mtsummit-papers.11,0,0.158089,"Missing"
W14-5104,W07-0734,0,0.0286154,"lly significant coefficients of the logistic function are used to analyze parts of the decoding process that are related to the particular errors. 1 Introduction Evaluating the output of Machine Translation (MT) has been in the focus since the first developments of the field. There have been several efforts to measure the translation performance, or to identify errors by defining manual and automatic metrics. Advanced automatic metrics and Quality Estimation methods have introduced machine learning (ML) techniques in order to predict indications about the quality of the produced translations (Lavie and Agarwal, 2007; Stanojevic and Sima’an, 2014). When compared to traditional automatic metrics, ML techniques allow acquiring knowledge about the quality of the translation out of a big amount of features. Such features are typically black-box features, generated by automatic analysis over the text of the source or the translations, or less often glass-box features, derived from the internal functioning of the translation mechanism. In this work, we focus on the glass-box fea20 tures. However, instead of focusing on the performance of a quality assessment mechanism, we look backwards into what happened durin"
W14-5104,P02-1040,0,0.091003,"a weight vector estimated by ML to minimize the error of the function ◦, given samples of X and Y . This vector contains coefficients for each one of the features. Given a well-fit model and a relevant statistical function, these coefficients can indicate the importance of each feature. Our aim is to use the β coefficients in order to explain several behaviours of the decoding process, relevant to the errors. The exact formulation of the statistical function ◦ is given in Section 4.3. Our intention is to not train the model using as a dependent variable a complex quality metric such as BLEU (Papineni et al., 2002) or WER, since this would increase complexity by capturing many issues in just one number. Instead, we choose a more fine-grained approach, by focusing onto specific type of errors that occur often in machine translation output (Section 4.1). D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decodi"
W14-5104,J11-4002,1,0.884579,"Missing"
W14-5104,2011.eamt-1.36,1,0.897659,"Missing"
W14-5104,2009.eamt-1.5,0,0.0157733,"essing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decoding process. The first experiments on “Confidence Estimation” make use of a small number of Statistical Machine Translation (SMT) features in order to train a supervised model for predicting the quality of the Translation (Blatz et al., 2004). Later work, identified as “Quality Estimation”, defines such features as “glass-box” features (Specia et al., 2009). 54 glass-box features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use glass-box features in order to predict numerical indications of translation quality, such as postediting effort (Rubino"
W14-5104,W14-3354,0,0.0276059,"Missing"
W14-5104,vilar-etal-2006-error,0,0.0885356,"Missing"
W14-5104,P07-2045,0,\N,Missing
W14-5104,W13-2248,0,\N,Missing
W14-5104,W10-1703,0,\N,Missing
W15-3004,W07-0726,0,0.0343284,"he QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17"
W15-3004,eisele-chen-2010-multiun,0,0.0119801,"t consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT sy"
W15-3004,W11-2104,1,0.900755,"Missing"
W15-3004,W12-3138,0,0.0126825,"t is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computat"
W15-3004,W10-1708,0,0.0233189,"1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015"
W15-3004,W12-0115,0,0.0126604,"bination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. 2.1.1 Empirical machine learning classifier (primary submission) The machine learning (ML) selection mechanism is based on encouraging results of previous projects including E URO M ATRIX + (Federmann and Hunsicker, 2011), META-NET (Federmann, 2012), QTL AUNCH PAD (Avramidis, 2013; Shah et al., 2013). It has been extended to include several features that can only be generated on a sentence level and would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In the architecture at hand, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source wit"
W15-3004,P02-1040,0,0.0935117,"Missing"
W15-3004,N07-1051,0,0.0411992,"l system combination produces a perfect translation. In this particular case, the machine translation is even better than the human reference (“W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus.”) as the latter is introducing a determiner for “table”, which is not justified by the source. 2.1 Feature sets We experimented with feature sets that performed well in previous experiments. In particular: • Basic syntax-based feature set: unknown words, count of tokens, count of alternative parse trees, count of verb phrases, PCFG parse log likelihood. The parsing was performed with the Berkeley Parser (Petrov and Klein, 2007) and features were extracted from both source and target. This feature set has performed well as a metric in WMT-11 metrics task (Avramidis et al., 2011). Sentence level selection • Basic feature set + 17 QuEst baseline features: this feature set combines the basic syntax-based feature set described above We present two methods for performing sentence level selection, one with pairwise classifier and one based on POS 4-gram IBM 1 models. 67 icality. Further analysis on this aspect may be required. with the baseline feature set of the QuEst toolkit (Specia et al., 2013) as per WMT-13 (Bojar et"
W15-3004,W11-2123,0,0.0411543,"o methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-"
W15-3004,W11-2110,1,0.867236,"Missing"
W15-3004,N07-1064,0,0.190898,"analysis phase, where the sourcelanguage text is parsed and a tree of the source language is constructed • the transfer phase, where the analysis tree is used for the transfer phase, where canonical forms and categories of the source are transferred into similar representations of the target language • the generation phase, where the target sentence is formed out of the transfered representations by employing inflection and agreement rules. LucyMoses As an alternative way of automatic post-editing of the transfer-based system, a serial transfer+SMT system combination is used, as described in (Simard et al., 2007). For building it, the first stage is translation of the source language part of the training corpus by the transfer-based system. In the second stage, an SMT system is trained using the transfer-based translation output as a source language and the target language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. In previous experiments, however, the method on its own could not outperform Moses trained on a large parallel corpus. The example in Figure 1 (taken from the QTL EAP corpus u"
W15-3004,P13-1135,0,0.0238802,"by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this pap"
W15-3004,P13-4014,0,0.0588687,"Missing"
W15-3004,J93-2003,0,\N,Missing
W15-3004,W05-0909,0,\N,Missing
W15-3004,W13-2201,0,\N,Missing
W15-3004,W11-2141,0,\N,Missing
W15-4914,W05-0814,0,0.0377031,"lish sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation output • ﬁrst four letters of the word (4let) The simplest way for word reduction is to use only its ﬁrst n letters. The choice of ﬁrst four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • ﬁrst two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more reﬁned method which splits words into stems and sufﬁxes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained whe"
W15-4914,E03-1076,0,0.0295937,"for lemmas, it would not be possible to detect any inﬂectional error thus setting the inﬂectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inﬂective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classiﬁcation results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The sufﬁx of each word is removed and only the stem is preserved. For calculation of stem and sufﬁx frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation o"
W15-4914,2005.mtsummit-papers.11,0,0.0288445,"” error rates. The best way for the assessment would be, of course, a comparison with human error classiﬁcation. Nevertheless, this has not been done for two reasons: ﬁrst, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable"
W15-4914,J11-4002,1,0.893667,"Missing"
W15-4914,2011.eamt-1.12,0,0.0230765,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using T"
W15-4914,E09-1087,0,0.0605874,"Missing"
W15-4914,tiedemann-2012-parallel,0,0.0606606,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenian"
W15-4914,W11-2103,0,\N,Missing
W15-5702,2003.mtsummit-systems.1,0,0.0185567,"(6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation occurs in three phases, namely analysis, transfer, and generation. All three phases consist of hand-coded linguistic rules which have shown to perform well for capturing the structural and semantic differences between German and other language"
W15-5702,W14-3302,0,0.0706052,"Missing"
W15-5702,eisele-chen-2010-multiun,0,0.0162457,"gure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we als"
W15-5702,W11-2141,0,0.122729,"dent target language structures. A RestAPI allows the different processing steps and/or intermediate results to be influenced. Deep features for empirical enhancement Although deep techniques indicate good coverage of a number of linguistic phenomena, each of the three phases may frequently encounter serious robustness issues and/or the inability to fully process a given sentence. Erroneous analysis from early phases may aggregate along the pipeline and cause further sub-optimal choices in later phases, thus severely deteriorating the quality of the produced translation. Preliminary analysis (Federmann and Hunsicker, 2011) has shown that such is the case for source sentences that are ungrammatical in the first place or that have a very shallow syntax with many specialized lexical entries. To tackle these issues, we combine the transfer-based component with our supportive SMT engine in the following two ways: (a) train a statistical machine translation to automatically post-edit the output of the transfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that perf"
W15-5702,W12-0115,0,0.0146068,"combination produces a perfect translation. In this particular case, the machine translation (W¨ahlen Sie im Einf¨ugen Men¨u Tabelle aus) is even better than the human reference (W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus) as the latter introduces a determiner for “table” that is not justified by the source. English Transfer-‐ based MT German* SMT German Figure 2: Serial System Combination en→de. 2.4 Parallel System Combination: Selection Mechanism The selection mechanism is based on encouraging results of previous projects including Euromatrix Plus (Federmann and Hunsicker, 2011), T4ME (Federmann, 2012), QTLaunchPad (Avramidis, 2013; Shah et al., 2013). It has been extended to include several deep features that can only be generated on a sentence level and that would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In System 1, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source with the st"
W15-5702,W11-2123,0,0.00929833,"l., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has sh"
W15-5702,popovic-ney-2006-pos,1,0.760166,"e pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation oc"
W15-5702,N07-1064,0,0.0303755,"ransfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that performs real-time analysis of the produced translations and automatically selects the output that is predicted to be of a better quality (Avramidis, 2011). Figure 1 shows the overall architecture of System 1 for en→de. 2.3 Serial System Combination: Lucy+Moses For automatic post-editing of the transfer-based system, a serial Transfer+SMT system combination is used, as described in (Simard et al., 2007) The first stage is translation of the source-language part of the training corpus by the transfer-based system. The second stage is training an SMT system with the transfer-based translation output as a source language and the target-language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. Figure 2 illustrates the architecture for translation direction en→de. Note that the notion of “German*” in the figure is meant to distinguish the input and output of the SMT system. “German*” is t"
W15-5702,P13-1135,0,0.0293718,"arts from each employed method. Figure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield,"
W16-2329,2003.mtsummit-systems.1,0,0.743848,"ve search with all possible combinations of the modification above and the most indicative automatic scores are shown in table 3. Although automatic scores have in the past shown low performance when evaluating RBMT systems, our proposed modifications have a lexical impact that can be adequately measured with ngram based metrics. Our investigation and discussion is performed on Batch 2. The best combination of the suggested modifications achieves an overall improvement of 0.51 points BLEU and 0.68 points METEOR over the baseline. In particular: Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its state-of-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. This year’s work on RBMT focuses on issues revealed through manual inspection of its performance on the development set: • Adding quotes around menu items resulted in a significant drop of the automatic scores, so it was not used;"
W16-2329,P02-1040,0,0.0961403,"Missing"
W16-2329,W15-3004,1,0.893586,"Missing"
W16-2329,N07-1051,0,0.0242083,"Missing"
W16-2329,W07-0718,0,0.055134,"spective phrase-based models, using the first 1.1M sentences of Europarl and ommiting the entire Commoncrawl. We experimented with four different settings concerning the translation path. These settings with the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn,"
W16-2329,E06-1032,0,0.0392743,"Missing"
W16-2329,I08-2089,0,0.142696,"he entire Commoncrawl. We experimented with four different settings concerning the translation path. These settings with the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and que"
W16-2329,W11-2123,0,0.0209094,"training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 system variants 2.3 Syntax-enhanced SMT Motivated by the importance of grammar in the translation between English and German, we developed a syntax-enhanced SMT system. The process is similar to that of our baseline, but this version includes syntax-aware phrase extraction. Phrase pairs in the baseline SMT system were augmented with linguistically-motivated phrase pairs. These phrases were extracted by generating constituency and dependency parse trees"
W16-2329,N07-1064,0,0.0988575,"t pairwise recomposition (Avramidis, 2013) to reduce ties between the systems. Due to technical reasons, the version of the selection mechanism that is submitted to this task is only a pilot version that includes WSDSMT (section 2.2), baseline RBMT (section 2.4) and RBMT→SMT (section 2.5). When ties occurred, despite the soft recomposition, the system was selected based on a predefined system priority (WSD-SMT, RBMT, RBMT→SMT). The preSerial RBMT post-editing with SMT As an alternative to automatic post-editing of the RBMT system, a serial RBMT+SMT system combination is used, as described in (Simard et al., 2007). For building it, the first stage is translation of the source language part of the training corpus by the RBMT system. In the second stage, a SMT system is trained using the RBMT translation output as a source language and the target language part as a target language. Later, the test set is first translated by the RBMT system, and the obtained translation is translated by the SMT system. 418 defined order of the systems needs to be further confirmed as part of the future work. 3 the investigated phenomena. SMT performs well on terminology, menu items and quotation marks, but seems to suffer"
W16-2329,P03-1054,0,0.00782516,"Missing"
W16-2329,2009.mtsummit-posters.18,1,0.80462,"nd German, we developed a syntax-enhanced SMT system. The process is similar to that of our baseline, but this version includes syntax-aware phrase extraction. Phrase pairs in the baseline SMT system were augmented with linguistically-motivated phrase pairs. These phrases were extracted by generating constituency and dependency parse trees for both the source and target languages, followed by nodealigning the parallel parse trees using a statistical tree aligner (Zhechev, 2009). The syntax-aware phrase extraction algorithm obtains surface-level chunks (syntax-aware) from the aligned subtrees (Srivastava and Way, 2009). Intermediate experiments were conducted by using either constituency parsing or dependency SMT with Word Sense Disambiguation The word-sense-disambiguated SMT system is a factored phrase-based statistical system with two decoding paths, one basic and one alternative. In the basic path, all nouns of the source language (English) have been annotated with a WSD system (Weissenborn et al., 2015) that assigns BabelNet senses to nouns and has recently shown improvements over state-of-the-art results on several corpora. The sense labels are estimated based on the disambiguation analysis on the sent"
W16-2329,W08-0318,0,0.10178,"the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based"
W16-2329,P15-1058,0,0.0431184,"Missing"
W16-2329,W07-0734,0,0.0528643,"put for every sentence. The architecture of the system is illustrated in figure 1. The core of the selection mechanism is a ranker which reproduces ranking by aggregating pairwise decisions by a binary classifier (Avramidis, 2013). Such a classifier is trained on binary comparisons in order to select the best one out of two different MT outputs given one source sentence at a time. As training material, we used the test-sets of WMT evaluation task (2008-2014). The rank labels for the training are automatically generated, after ordering the given MT outputs based on their sentence-level METEOR (Lavie and Agarwal, 2007) against the references. We have previously experimented with training on ranking provided by users, but experiments showed that for this task, ranks made out of sentence-level METEOR maximize all automatic scores on our development set, including other document-level ones, such as BLEU. output RBMT RBMT→SMT Figure 1: Architecture of the selection mechanism automatic scores, the difference does not seem significant, and manual inspection raised the concern that this may be because of the way this phrase has been translated in the references. We therefore conducted manual sentence selection on"
W16-2329,P03-1021,0,0.0374423,"the fact that different engines make different errors) solely based on the rough feedback provided by automatic scores. As scores like BLEU (Pap2 System components We hereby present the systems that appear in our submissions and our hybrid system: 2.1 Phrase-based SMT baseline The baseline system consists of a basic phrasebased SMT model, trained with the state-of-theart settings on both the generic and technical data. The translation table was trained on a concatenation of generic and technical data, filtering out the sentences longer than 80 words. Batch 1 was used as a tuning set for MERT (Och, 2003). One language model (monolingual) of order 5 was trained on the target side from both the 415 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 415–422, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics corpus Chromium browser Drupal Libreoffice help Libreoffice UI Ubuntu Saucy Europarl (mono) News (mono) Commoncrawl (parallel) Europarl (parallel) MultiUN (parallel) News Crawl (parallel) entries words 6.3K 4.7K 46.8K 35.6K 182.9K 55.1K 57.4K 1.1M 143.7K 1.6M 2.2M 89M 54.0M 1.7B 2.4M 1.9M 167.6K 201.3K 53.6M 50.1"
W16-2329,avramidis-etal-2012-involving,1,\N,Missing
W16-6404,2003.mtsummit-systems.1,0,0.0803587,"ns were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional work on RBMT focused on issues revealed through manual inspection of its performance on the QTLeap corpus (see also section 3): 2 http://metashare.metanet4u.eu/go2/qtleapcorpus 30 corpus Chromium browser Drupal Libreoffi"
W16-6404,W16-2329,1,0.864384,"have performed using a dedicated “test suite” that contains selected examples of relevant phenomena. While automatic scores show huge differences between the engines, the overall average number or errors they (do not) make is very similar for all systems. However, the detailed error breakdown shows that the systems behave very differently concerning the various phenomena. 1 Introduction This paper describes a hybrid Machine Translation (MT) system built for translating from English to German in the domain of technical documentation. The system builds upon the general architecture described in Avramidis et al. (2016), but in the current version several components have been improved or replaced. As detailed in the previous paper, the design of the system was driven by the assumptions that a) none of today’s common MT approaches, phrase-based statistical (PB-SMT) or rule-based (RBMT), is on its own capable of providing enough good translations to be useful in an outbound translation scenario without human intervention, and b) “deep” linguistic knowledge should help to improve translation quality. Instead of building a completely new system, our goal is to adjust and combine existing systems in a smart way u"
W16-6404,W14-4012,0,0.109025,"Missing"
W16-6404,W11-2123,0,0.0103712,"training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional w"
W16-6404,W08-0318,0,0.0344971,"s longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based"
W16-6404,W16-2361,0,0.029527,"Missing"
W16-6404,I08-2089,0,0.0251905,"cal data. The translation table was trained on a concatenation of generic and technical data, filtering out the sentences longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and que"
W17-4758,W09-0441,0,0.0356115,"Machine Translation (MT) output (Blatz et al., 2004). A commonly-used subtask of QE refers to the learning of automatic metrics. These metrics produce a continuous score based on the comparison between the MT output and a reference translation. When the reference is a minimal post-edition of the MT output, the quality score produced is intuitively more objective and robust as compared to other QE subtasks, where the quality score is assigned directly by the annotators. In that case, the score is a direct reflection of the changes that need to take place in order to fix the translation. HTER (Snover et al., 2009) is the most commonly used metric as it directly represents the least required post-editing effort. In order to predict the results of an automatic metric, QE approaches use machine learning to predict a model that associates a feature vector with the single quality score. In this case the statis2 Previous work The prediction of HTER first appeared as a means to estimate post-editing effort (Specia and Farzindar, 2010). Bypassing the direct calculation of HTER was shown by Kozlova et al. (2016), who had positive results by predicting BLEU instead of HTER. Predicting the HTER score with regards"
W17-4758,W14-3337,1,0.93025,"s to post-editing operations, such as re-ordering and lexical choices, has been done by adding the relevant features in the input (Sagemo and Stymne, 2016), whereas Tezcan et al. (2016) use the wordlevel quality estimation labels as a feature for predicting the sentence-level score. To the best of our knowledge, all previous work used a model to directly predict a single HTER score, in con534 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 534–539 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics trast to Avramidis (2014), which trained one separate model for every HTER component and used the 4 individual predictions to calculate the final score, albeit with no positive results. In our work we extend that, by employing a more elegant machine learning approach that predicts four separate labels for the HTER components but through a single model. 3 labels h1 y1 h2 y2 x1 x2 .. . .. . y3 xD Methods 3.1 hidden layer features hm y4 Machine Learning Figure 1: Network graph for the multi-layer perceptron which given the features x1...D can jointly predict the amount of the post-editing operations y1...4 The calculatio"
W17-4758,2010.jec-1.5,0,0.0404129,"e the quality score is assigned directly by the annotators. In that case, the score is a direct reflection of the changes that need to take place in order to fix the translation. HTER (Snover et al., 2009) is the most commonly used metric as it directly represents the least required post-editing effort. In order to predict the results of an automatic metric, QE approaches use machine learning to predict a model that associates a feature vector with the single quality score. In this case the statis2 Previous work The prediction of HTER first appeared as a means to estimate post-editing effort (Specia and Farzindar, 2010). Bypassing the direct calculation of HTER was shown by Kozlova et al. (2016), who had positive results by predicting BLEU instead of HTER. Predicting the HTER score with regards to post-editing operations, such as re-ordering and lexical choices, has been done by adding the relevant features in the input (Sagemo and Stymne, 2016), whereas Tezcan et al. (2016) use the wordlevel quality estimation labels as a feature for predicting the sentence-level score. To the best of our knowledge, all previous work used a model to directly predict a single HTER score, in con534 Proceedings of the Conferen"
W17-4758,P15-4020,0,0.0592115,"iment is focusing on machine learning, for German-English only the baseline features are used. For English-German, we additionally performed preliminary experiments with the feature-set from Avramidis (2017) including 94 features that improved QE performance for translating into German, generated with the software Qualitative (Avramidis, 2016). The addition of these features did not result into any improvements, so we are not reporting their results during the development phase (see Section 5 for more details). The code for training quality estimation models was based on the software Quest++ (Specia et al., 2015) and Scikit-learn (Pedregosa et al., 2011) ver. 1.18. method dev test SVM 4×SVM MLP MLP4 0.400 0.392 0.447* 0.476* 0.441 0.409 0.447 0.475** Table 1: Pearson rho correlation against golden labels concerning the 4 different approaches for predicting HTER for German-English. (*) indicates significant improvement (α = 0.05) over the SVM baseline (**) significant improvement over all models The approach of MLP4 achieves a small but significant improvement over the baseline and the 4×SVM on the development set. On the development set both MLP and MLP4 beat significantly the baseline, but MLP4 is no"
W17-4758,C04-1046,0,0.215649,"core, we suggest a model that jointly predicts the amount of the 4 distinct postediting operations, which are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. Without any feature exploration, a multi-layer perceptron with 4 outputs yields small but significant improvements over the baseline. 1 Introduction Quality Estimation (QE) is the evaluation method that aims at employing machine learning in order to predict some measure of quality given a Machine Translation (MT) output (Blatz et al., 2004). A commonly-used subtask of QE refers to the learning of automatic metrics. These metrics produce a continuous score based on the comparison between the MT output and a reference translation. When the reference is a minimal post-edition of the MT output, the quality score produced is intuitively more objective and robust as compared to other QE subtasks, where the quality score is assigned directly by the annotators. In that case, the score is a direct reflection of the changes that need to take place in order to fix the translation. HTER (Snover et al., 2009) is the most commonly used metric"
W17-4758,W16-2385,0,0.03826,"is a direct reflection of the changes that need to take place in order to fix the translation. HTER (Snover et al., 2009) is the most commonly used metric as it directly represents the least required post-editing effort. In order to predict the results of an automatic metric, QE approaches use machine learning to predict a model that associates a feature vector with the single quality score. In this case the statis2 Previous work The prediction of HTER first appeared as a means to estimate post-editing effort (Specia and Farzindar, 2010). Bypassing the direct calculation of HTER was shown by Kozlova et al. (2016), who had positive results by predicting BLEU instead of HTER. Predicting the HTER score with regards to post-editing operations, such as re-ordering and lexical choices, has been done by adding the relevant features in the input (Sagemo and Stymne, 2016), whereas Tezcan et al. (2016) use the wordlevel quality estimation labels as a feature for predicting the sentence-level score. To the best of our knowledge, all previous work used a model to directly predict a single HTER score, in con534 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 534–539"
W17-4758,W16-2390,0,0.018292,"s of an automatic metric, QE approaches use machine learning to predict a model that associates a feature vector with the single quality score. In this case the statis2 Previous work The prediction of HTER first appeared as a means to estimate post-editing effort (Specia and Farzindar, 2010). Bypassing the direct calculation of HTER was shown by Kozlova et al. (2016), who had positive results by predicting BLEU instead of HTER. Predicting the HTER score with regards to post-editing operations, such as re-ordering and lexical choices, has been done by adding the relevant features in the input (Sagemo and Stymne, 2016), whereas Tezcan et al. (2016) use the wordlevel quality estimation labels as a feature for predicting the sentence-level score. To the best of our knowledge, all previous work used a model to directly predict a single HTER score, in con534 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 534–539 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics trast to Avramidis (2014), which trained one separate model for every HTER component and used the 4 individual predictions to calculate the final score, albeit with"
W17-4758,W15-3043,0,\N,Missing
W18-2107,W18-2100,0,0.0724206,"Missing"
W18-2107,W12-3110,0,0.0219398,"put and not of QE predictions. Similar to MT output, predictions of sentencelevel QE have also been evaluated on test-sets consisting of randomly drawn texts and a single metric has been used to measure the performance over the entire text (e.g. Bojar et al., 2017). There has been criticism on the way the test-sets of the shared tasks have been formed with regards to the distribution of inputs (Anil and Fran, 2013), e.g. when they demonstrate a dataset shift (QuioneroCandela et al., 2009). Additionally, although there has been a lot of effort to infuse linguistically motivated features in QE (Felice and Specia, 2012), there has been no effort to evaluate their predictions from a linguistic perspective. To the best Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018 |Page 243 of our knowledge there has been no use of a Test Suite in order to evaluate sentence-level QE, or to inspect the predictions with regards to linguistic categories or specific error types. 3 Method The evaluation of QE presented in this paper is based on these steps: (1) construction of the Test Suite with respect to linguistic categories; (2) selection of suitable Test S"
W18-2107,D17-1263,0,0.0889841,"Missing"
W18-2107,C90-2037,0,0.224518,"rted is the generic domain of the text. In this paper we make an effort to demonstrate the value of using a linguistically-motivated controlled test-set (also known as a Test Suite) for evaluation instead of generic test-sets. We will focus on the sub-field of sentence-level Quality Estimation (QE) on MT and see how the evaluation of QE on a Test Suite can provide useful information concerning particular linguistic phenomena. 2 Related work There have been few efforts to use a broadlydefined Test Suite for the evaluation of MT, the first of them being during the early steps of the technology (King and Falkedal, 1990). Although the topic has been recently revived (Isabelle et al., 2017; Burchardt et al., 2017), all relevant research so far applies only to the evaluation of MT output and not of QE predictions. Similar to MT output, predictions of sentencelevel QE have also been evaluated on test-sets consisting of randomly drawn texts and a single metric has been used to measure the performance over the entire text (e.g. Bojar et al., 2017). There has been criticism on the way the test-sets of the shared tasks have been formed with regards to the distribution of inputs (Anil and Fran, 2013), e.g. when they"
W18-2107,2014.eamt-1.38,1,0.858464,"Missing"
W18-6436,W07-0718,0,0.207348,"Missing"
W18-6436,L16-1100,0,0.150074,") and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either b"
W18-6436,D17-1263,0,0.157443,"Missing"
W18-6436,L18-1142,1,0.707008,"Missing"
W18-6436,1991.mtsummit-panels.5,0,0.873131,"spects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low. 1 2 Related Work The use of test suites in the evaluation of NLP applications (Balkan et al., 1995) and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim a"
W18-6436,P02-1040,0,0.11164,"8). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium"
W18-6436,2006.amta-papers.25,0,0.0953688,"ented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium, Brussels, October 31"
W18-6436,C90-2037,0,\N,Missing
W18-6436,E17-2058,0,\N,Missing
W18-6436,W18-2107,1,\N,Missing
W19-5351,W18-6432,0,0.0473338,"Missing"
W19-5351,W18-6433,0,0.0862885,"Missing"
W19-5351,W07-0718,0,0.330509,"Missing"
W19-5351,W18-6437,0,0.0667606,"Missing"
W19-5351,W18-6434,0,0.0312015,"ion that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites (Guillou and Hardmeier, 2016; Isabelle et al., 2017). 2 Related Work Several test suites have been presented as part of the Test Suite track of the Third Conference of Machine Translation (Bojar et al., 2018a). Each test suite focused on a particular phenomenon, such as discourse (Bojar et al., 2018b), morphology (Burlot et al., 2018), grammatical contrasts (Cinkova and Bojar, 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). In contrast to the above test suites, our test suite is the only one that does such a systematic evaluation of more than one hundred phenomena. A direct comparison can be done with the latter related paper, since it focuses at the same language direction. Its authors use automated methods to extract text items, whereas in our test suite the test items are created manually. 445 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 445–454 c Florence, Italy"
W19-5351,L16-1100,0,0.142058,"nly focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort. Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites (Guillou and Hardmeier, 2016; Isabelle et al., 2017). 2 Related Work Several test suites have been presented as part of the Test Suite track of the Third Conference of Machine Translation (Bojar et al., 2018a). Each test suite focused on a particular phenomenon, such as discourse (Bojar et al., 2018b), morphology (Burlot et al., 2018), grammatical contrasts (Cinkova and Bojar, 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). In contrast to the above test suites, our test suite is the only one that does such a systematic evaluation of more than one hundred phenomena. A direct compa"
W19-5351,W09-0441,0,0.0494383,"lar expressions, followed by minimal human refinement (Section 3). The application of the suite allows us to form conclusions on the particular grammatical performance of the systems and perform several comparisons (Section 4). Introduction For decades, the development of Machine Translation (MT) has been based on either automatic metrics or human evaluation campaigns with the main focus on producing scores or comparisons (rankings) expressing a generic notion of quality. Through the years there have been few examples of more detailed analyses of the translation quality, both automatic (HTER (Snover et al., 2009), Hjerson (Popovi´c, 2011)) and human (MQM Lommel et al., 2014). Nevertheless, these efforts have not been systematic and they have only focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort. Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task (CallisonBurch et al., 2007). In order to"
W19-5351,W18-6435,0,0.057944,"Missing"
W19-5351,D17-1263,0,0.086885,"Missing"
W19-5351,2014.eamt-1.38,1,0.896199,"Missing"
W19-5351,L18-1142,1,0.718292,"Missing"
W19-5351,W18-6436,1,0.890415,"Missing"
