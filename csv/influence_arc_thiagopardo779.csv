2020.emnlp-main.123,L18-1157,1,0.860562,"Missing"
2020.emnlp-main.123,W13-2322,0,0.121798,"/ name “Barcelona” Figure 1: An example of AMR graph for the sentence “I live in Barcelona”. is not provided by manual annotations. Also, the available automatic aligners focus only on the English language (Pourdamghani et al., 2014; Flanigan et al., 2014; Liu et al., 2018), and they do not perform well for other languages. For Portuguese, for instance, the sentence “N˜ao era surpresa para mim” (It was no surprise to me), the JAMR aligner (Flanigan et al., 2014) produces alignment only between the token surpresa (surprise) and the node surpresa, as shown in Figure 2. Introduction According to Banarescu et al. (2013), Abstract Meaning Representation (AMR) is a semantic meaning representation, which may be encoded as a rooted Direct Acyclic Graph (DAG) where the nodes are concepts, and the edges are relations among them. This representation explicitly details semantics information, as depicted in Figure 1. In this figure, the live-01 node is the root of the graph and city node introduces a named entity. Moreover, :ARGx relations are predicates from the PropBank lexicon (Kingsbury and Palmer, 2002), which encode semantic information according to each PropBank sense. To parse a text into an AMR graph, most o"
2020.emnlp-main.123,J93-2003,0,0.132855,"Missing"
2020.emnlp-main.123,P13-2131,0,0.077425,"alignment method aligns the spans 0-1 and 2-3, which are the words Mas (But) and n˜ao (not), respectively, with nodes 0 and 0.0.0, which are contrast-01 and -, respectively (see Figure 5). Our alignment tool is available at http://github. com/rafaelanchieta/amr-aligner. In what follows, we detail our experiments with the aligner and the obtained results. 4 corpus of the Portuguese language (Anchiˆeta and Pardo, 2018a), which contains 1,274, 145, and 143 sentences for training, development, and testing, respectively. To compare the results of the parsers, we used the traditional Smatch metric (Cai and Knight, 2013) and the more recently proposed SEMA metric (Anchiˆeta et al., 2019). SEMA is a more robust metric that considers the parent of the nodes in the graph, producing fairer results than the Smatch metric. Table 2 shows the obtained results in the extrinsic evaluation. Experiments and Results Parser We performed two experiments, one intrinsic and another extrinsic. In the first, we randomly chose and manually aligned one hundred sentences with their respective AMRs from the Little Prince corpus (Anchiˆeta and Pardo, 2018a). Then, we compared the manual alignment with the alignments produced by Flan"
2020.emnlp-main.123,E17-1051,0,0.0607474,"them. This representation explicitly details semantics information, as depicted in Figure 1. In this figure, the live-01 node is the root of the graph and city node introduces a named entity. Moreover, :ARGx relations are predicates from the PropBank lexicon (Kingsbury and Palmer, 2002), which encode semantic information according to each PropBank sense. To parse a text into an AMR graph, most of the AMR parsers require alignment between the word (tokens) of the sentence and the nodes of the corresponding graph (see, for instance, (Flanigan et al., 2014; Wang et al., 2015; Zhou et al., 2016; Damonte et al., 2017). However, this anchoring # ::snt Não era surpresa para mim # ::alignments 2-3|0 (s / surpresa :polarity :domain (e / eu)) Figure 2: Alignment produced by JAMR aligner for the sentence “N˜ao era surpresa para mim” (It was no surprise to me). The JAMR aligned only the span 2-3, which is the token surpresa (surprise), with node 0, which is the root of the graph; - and eu nodes were not aligned. This wrong or bad alignments occur because of the JAMR aligner adopts a string-match strategy that is focused on the English language. Besides, these issues contribute to a decrease in the performance of"
2020.emnlp-main.123,P14-1134,0,0.424715,"sed on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language. 1 i/i l / live-01 :location :op1 :name c / city n / name “Barcelona” Figure 1: An example of AMR graph for the sentence “I live in Barcelona”. is not provided by manual annotations. Also, the available automatic aligners focus only on the English language (Pourdamghani et al., 2014; Flanigan et al., 2014; Liu et al., 2018), and they do not perform well for other languages. For Portuguese, for instance, the sentence “N˜ao era surpresa para mim” (It was no surprise to me), the JAMR aligner (Flanigan et al., 2014) produces alignment only between the token surpresa (surprise) and the node surpresa, as shown in Figure 2. Introduction According to Banarescu et al. (2013), Abstract Meaning Representation (AMR) is a semantic meaning representation, which may be encoded as a rooted Direct Acyclic Graph (DAG) where the nodes are concepts, and the edges are relations among them. This representation expl"
2020.emnlp-main.123,2020.acl-demos.35,0,0.0288989,"Mas (Peter) Pedro Word Embeddings Concepts WMD(W i ,C j ) responder # ::snt Mas Pedro não respondeu # ::alignments 0-1|0 3-4|0.0 2-3|0.0.0 1-2|0.0.1+0.0.1.0+0.0.1.0.0 5-6|0.0.2 6-7|0.0.2.0 (c / contrast-01 :ARG2 (r / responder-01 :polarity :ARG0 (p / person :name (n / name :op1 “Pedro”)))) In the next step, we mapped each concept to its respective position in the graph. One can see that we mapped the contrast-01 concept to the root of the graph 0, its child responder-01 to 0.0, and their children - and person to their respective positions 0.0.0 and 0.0.1. To do this, we used the Penman tool (Goodman, 2020). In the last step, we align the word tokens of the sentence with the concepts of the graph. The AMR language has two concept types: concrete and abstract (or special keywords) ones. The former are those that are explicitly present in the sentence, while the latter are not. In Figure 5, we can see that the responder-01 is a concrete concept, since it is in the sentence, while the contrast-01, person, and name concepts are abstract2 . To align concrete concepts, we use the Word Mover’s Distance (WMD)3 (Kusner et al., 2015) and the pre-trained GloVe embeddings of 100 dimensions. The WMD is a dis"
2020.emnlp-main.123,D14-1048,0,0.645287,"the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language. 1 i/i l / live-01 :location :op1 :name c / city n / name “Barcelona” Figure 1: An example of AMR graph for the sentence “I live in Barcelona”. is not provided by manual annotations. Also, the available automatic aligners focus only on the English language (Pourdamghani et al., 2014; Flanigan et al., 2014; Liu et al., 2018), and they do not perform well for other languages. For Portuguese, for instance, the sentence “N˜ao era surpresa para mim” (It was no surprise to me), the JAMR aligner (Flanigan et al., 2014) produces alignment only between the token surpresa (surprise) and the node surpresa, as shown in Figure 2. Introduction According to Banarescu et al. (2013), Abstract Meaning Representation (AMR) is a semantic meaning representation, which may be encoded as a rooted Direct Acyclic Graph (DAG) where the nodes are concepts, and the edges are relations among them. T"
2020.emnlp-main.123,2020.acl-demos.14,0,0.0180961,"e Portuguese language. Our method produces alignments in the JAMR aligner format since most of the AMR parsers adopt this alignment type. To support our method, we used the pre-trained GloVe1 embeddings of 100 dimensions for the Portuguese 1 We also experimented other pre-trained models with dimensions of 50, 100, and 300. 1596 language (Hartmann et al., 2017) and some lexical resources. We organized our method into three phases over input annotated sentences: preprocessing, mapping, and aligning. In the first step, we tokenize the sentences and lemmatize each token, applying the Stanza tool (Qi et al., 2020) trained for Portuguese. The Portuguese tokenization is slightly different from English. For example, some hyphenated words, as “via-me” and “ouvi-la” (translated for “saw me” and “hear her”), should be separated by the hyphen, whereas other words, as “segunda-feira” and “rec´em-casados” (translated for “Monday” and “newly married”), should not be separated. To detail the next steps, we will use Figure 5 as an example. words of another document. We used this distance function to evaluate a distance between the embedded word tokens in the sentence and the embedded concepts in the graph to produ"
2020.emnlp-main.123,W17-6615,0,0.0231967,"Missing"
2020.emnlp-main.123,D19-6313,1,0.895194,"Missing"
2020.emnlp-main.123,kingsbury-palmer-2002-treebank,0,0.305871,"only between the token surpresa (surprise) and the node surpresa, as shown in Figure 2. Introduction According to Banarescu et al. (2013), Abstract Meaning Representation (AMR) is a semantic meaning representation, which may be encoded as a rooted Direct Acyclic Graph (DAG) where the nodes are concepts, and the edges are relations among them. This representation explicitly details semantics information, as depicted in Figure 1. In this figure, the live-01 node is the root of the graph and city node introduces a named entity. Moreover, :ARGx relations are predicates from the PropBank lexicon (Kingsbury and Palmer, 2002), which encode semantic information according to each PropBank sense. To parse a text into an AMR graph, most of the AMR parsers require alignment between the word (tokens) of the sentence and the nodes of the corresponding graph (see, for instance, (Flanigan et al., 2014; Wang et al., 2015; Zhou et al., 2016; Damonte et al., 2017). However, this anchoring # ::snt Não era surpresa para mim # ::alignments 2-3|0 (s / surpresa :polarity :domain (e / eu)) Figure 2: Alignment produced by JAMR aligner for the sentence “N˜ao era surpresa para mim” (It was no surprise to me). The JAMR aligned only the"
2020.emnlp-main.123,N15-1040,0,0.0757867,"pts, and the edges are relations among them. This representation explicitly details semantics information, as depicted in Figure 1. In this figure, the live-01 node is the root of the graph and city node introduces a named entity. Moreover, :ARGx relations are predicates from the PropBank lexicon (Kingsbury and Palmer, 2002), which encode semantic information according to each PropBank sense. To parse a text into an AMR graph, most of the AMR parsers require alignment between the word (tokens) of the sentence and the nodes of the corresponding graph (see, for instance, (Flanigan et al., 2014; Wang et al., 2015; Zhou et al., 2016; Damonte et al., 2017). However, this anchoring # ::snt Não era surpresa para mim # ::alignments 2-3|0 (s / surpresa :polarity :domain (e / eu)) Figure 2: Alignment produced by JAMR aligner for the sentence “N˜ao era surpresa para mim” (It was no surprise to me). The JAMR aligned only the span 2-3, which is the token surpresa (surprise), with node 0, which is the root of the graph; - and eu nodes were not aligned. This wrong or bad alignments occur because of the JAMR aligner adopts a string-match strategy that is focused on the English language. Besides, these issues contr"
2020.emnlp-main.123,D18-1264,0,0.423958,"lly matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language. 1 i/i l / live-01 :location :op1 :name c / city n / name “Barcelona” Figure 1: An example of AMR graph for the sentence “I live in Barcelona”. is not provided by manual annotations. Also, the available automatic aligners focus only on the English language (Pourdamghani et al., 2014; Flanigan et al., 2014; Liu et al., 2018), and they do not perform well for other languages. For Portuguese, for instance, the sentence “N˜ao era surpresa para mim” (It was no surprise to me), the JAMR aligner (Flanigan et al., 2014) produces alignment only between the token surpresa (surprise) and the node surpresa, as shown in Figure 2. Introduction According to Banarescu et al. (2013), Abstract Meaning Representation (AMR) is a semantic meaning representation, which may be encoded as a rooted Direct Acyclic Graph (DAG) where the nodes are concepts, and the edges are relations among them. This representation explicitly details sema"
2020.emnlp-main.123,P18-1037,0,0.0542036,":polarity :domain (e / eu)) Figure 2: Alignment produced by JAMR aligner for the sentence “N˜ao era surpresa para mim” (It was no surprise to me). The JAMR aligned only the span 2-3, which is the token surpresa (surprise), with node 0, which is the root of the graph; - and eu nodes were not aligned. This wrong or bad alignments occur because of the JAMR aligner adopts a string-match strategy that is focused on the English language. Besides, these issues contribute to a decrease in the performance of AMR parsers. As a result, recent AMR parsing methods have focused on alignmentfree approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b). However, they require a large annotated 1595 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1595–1600, c November 16–20, 2020. 2020 Association for Computational Linguistics corpus, which is available only for English. In this context, aiming to bridge this lack of resources and tools for other languages, we propose an AMR aligner for Portuguese that focuses on a more semantically matched word-concept pair. For that, we use pre-trained word embeddings and the Word Mover’s Distance (WMD) function (Kusner et al., 2015) to m"
2020.emnlp-main.123,P19-1009,0,0.0276784,"Missing"
2020.emnlp-main.123,D19-1392,0,0.0215613,"Missing"
2020.emnlp-main.123,D16-1065,0,0.0128273,"are relations among them. This representation explicitly details semantics information, as depicted in Figure 1. In this figure, the live-01 node is the root of the graph and city node introduces a named entity. Moreover, :ARGx relations are predicates from the PropBank lexicon (Kingsbury and Palmer, 2002), which encode semantic information according to each PropBank sense. To parse a text into an AMR graph, most of the AMR parsers require alignment between the word (tokens) of the sentence and the nodes of the corresponding graph (see, for instance, (Flanigan et al., 2014; Wang et al., 2015; Zhou et al., 2016; Damonte et al., 2017). However, this anchoring # ::snt Não era surpresa para mim # ::alignments 2-3|0 (s / surpresa :polarity :domain (e / eu)) Figure 2: Alignment produced by JAMR aligner for the sentence “N˜ao era surpresa para mim” (It was no surprise to me). The JAMR aligned only the span 2-3, which is the token surpresa (surprise), with node 0, which is the root of the graph; - and eu nodes were not aligned. This wrong or bad alignments occur because of the JAMR aligner adopts a string-match strategy that is focused on the English language. Besides, these issues contribute to a decrease"
2020.emnlp-main.123,D14-1162,0,0.0944817,"Missing"
2020.lrec-1.176,W10-1001,1,0.747351,"Missing"
2020.lrec-1.176,W13-4829,1,0.814744,"Missing"
2020.lrec-1.176,W11-2308,0,0.076365,"Missing"
2020.lrec-1.176,E17-1090,0,0.0641901,"Missing"
2020.lrec-1.176,W11-4504,1,0.434882,"Missing"
2020.lrec-1.176,D15-1133,0,0.0708394,"Missing"
2020.lrec-1.176,P18-1022,0,0.0659801,"Missing"
2020.lrec-1.176,N10-2011,1,0.782921,"Missing"
2020.lrec-1.176,W16-4123,0,0.0222544,"Missing"
2020.lrec-1.176,P17-2102,0,0.0484565,"Missing"
2020.msr-1.6,N19-1423,0,0.0274231,"to the word order and word forms, functional words, morphological features and other kinds of information are removed or changed in the UD structure to make it more similar to a semantic representation (called deep syntax). Diverse approaches have been applied on this shared task in previous editions, being mainly divided in inflection generation and word ordering tasks (Mille et al., 2019). Besides, there is a trend to use neural models for the same tasks. Recently, pre-trained language models have become standard in a variety of Natural Language Processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), including sentencelevel classification, sequence tagging and question answering. These models can be pre-trained on large corpora of available unannotated text, and then fine-tuned for specific tasks on smaller amounts of supervised data, relying on the induced language model structure to facilitate generalisation beyond the annotations. These pre-trained models have been widely used in text-to-text generation tasks such as text simplification, automatic summarisation, and machine translation, obtaining good results and outperforming the current state of the art. Howev"
2020.msr-1.6,2020.acl-main.167,0,0.224004,"gging and question answering. These models can be pre-trained on large corpora of available unannotated text, and then fine-tuned for specific tasks on smaller amounts of supervised data, relying on the induced language model structure to facilitate generalisation beyond the annotations. These pre-trained models have been widely used in text-to-text generation tasks such as text simplification, automatic summarisation, and machine translation, obtaining good results and outperforming the current state of the art. However, there are few initiatives for the data-to-text generation task. Lately, Mager et al. (2020) used GPT-2 for fine-tuning AMR-to-text generation task, showing improvements and This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 1 Available at https://universaldependencies.org/ Licence details: http:// 50 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 50–56 Barcelona, Spain (Online), December 12, 2020. that current pre-trained models can handle these representations even if the knowledge is not explicitly structured. In this context, this paper presents the description of the s"
2020.msr-1.6,W18-3601,0,0.0192794,"or, but there are some interesting ideas to explore. Among the learned lessons we may note that it is necessary to study strategies to represent UD inputs and to introduce structural knowledge into these pre-trained models. 1 Introduction Universal Dependencies1 (UD) have gained relevance in the Natural Language Processing (NLP) community. UD treebanks have already proved useful in the development of multilingual applications, becoming an advantage for developers. One of the successful applications of UD is related to Data-to-text generation. This may be seen in the two shared-tasks proposed (Mille et al., 2018; Mille et al., 2019) in which there were several participants. In general, the Surface Realisation Shared Task aims to continue with the development of natural language generation methods focused on the surface realisation task. Specifically, models in this task must generate sentences from dependency trees (or similar structures) in CoNLL format. This task comprises two tracks: (1) the Shallow Track, in which word order and word forms are removed from the UD structure, and (2) the Deep Track, which, in addition to the word order and word forms, functional words, morphological features and ot"
2020.msr-1.6,D19-6301,0,0.0704928,"me interesting ideas to explore. Among the learned lessons we may note that it is necessary to study strategies to represent UD inputs and to introduce structural knowledge into these pre-trained models. 1 Introduction Universal Dependencies1 (UD) have gained relevance in the Natural Language Processing (NLP) community. UD treebanks have already proved useful in the development of multilingual applications, becoming an advantage for developers. One of the successful applications of UD is related to Data-to-text generation. This may be seen in the two shared-tasks proposed (Mille et al., 2018; Mille et al., 2019) in which there were several participants. In general, the Surface Realisation Shared Task aims to continue with the development of natural language generation methods focused on the surface realisation task. Specifically, models in this task must generate sentences from dependency trees (or similar structures) in CoNLL format. This task comprises two tracks: (1) the Shallow Track, in which word order and word forms are removed from the UD structure, and (2) the Deep Track, which, in addition to the word order and word forms, functional words, morphological features and other kinds of informat"
2020.msr-1.6,2020.msr-1.1,0,0.0162319,"Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 1 Available at https://universaldependencies.org/ Licence details: http:// 50 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 50–56 Barcelona, Spain (Online), December 12, 2020. that current pre-trained models can handle these representations even if the knowledge is not explicitly structured. In this context, this paper presents the description of the system submitted by the NILC team to the track 2 of the Surface Realisation Shared Task 2020 (Track 2 - SR’20) (Mille et al., 2020). Our proposal is an End-to-End approach inspired by the work of Mager et al. (2020). We explore some strategies to sequentially represent UD structures and to fine-tune GPT-2 (Radford et al., 2019) on the pre-processed dataset2 . 2 Track 2 Dataset - SR’20 The dataset for the Track 2 is composed by UD structures and their corresponding sentences. The UD structure is similar to a dependency tree, however, some information are modified: • word order is removed by randomised scrambling; • words are replaced by their lemmas; • some prepositions and conjunctions (that can be inferred from other lex"
2020.msr-1.6,P02-1040,0,0.107207,"olf et al., 2019)3 . The model is trained on the joint of all training subsets. The fine-tuning is executed in 7 epochs (as the model converges at this time), using a batch size of 8, the AdamW optimizer with a learning rate of 6.25e-5, a max length of 350 in the source and target, and freezing the embeddings. For the decoding, we use a beam size of 15. At test time, we get tokenised sentences. We then post-process them by using the Moses detokeniser4 . 4 Results and Discussion The automatic performance of the diverse proposals at the shared task is computed by the following measures: • BLEU (Papineni et al., 2002): precision metric that computes the geometric mean of the n-gram precisions between the generated and reference texts, adding a brevity penalty for shorter sentences (we use the smoothed version and report results for n = 1, 2, 3, and 4); • NIST (Doddington, 2002): related n-gram similarity metric weighted in favor of less frequent ngrams, which are taken to be more informative; • Normalized edit distance (DIST): inverse, normalized, character-based string-edit distance that starts by computing the minimum number of character insertions, deletions and substitutions (all at cost 1) required to"
2020.msr-1.6,N18-1202,0,0.0305338,"k, which, in addition to the word order and word forms, functional words, morphological features and other kinds of information are removed or changed in the UD structure to make it more similar to a semantic representation (called deep syntax). Diverse approaches have been applied on this shared task in previous editions, being mainly divided in inflection generation and word ordering tasks (Mille et al., 2019). Besides, there is a trend to use neural models for the same tasks. Recently, pre-trained language models have become standard in a variety of Natural Language Processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), including sentencelevel classification, sequence tagging and question answering. These models can be pre-trained on large corpora of available unannotated text, and then fine-tuned for specific tasks on smaller amounts of supervised data, relying on the induced language model structure to facilitate generalisation beyond the annotations. These pre-trained models have been widely used in text-to-text generation tasks such as text simplification, automatic summarisation, and machine translation, obtaining good results and outperforming the current st"
2020.msr-1.6,W18-3608,1,0.849762,". Figure 2: Representation of the example in graphic format. 2 The corresponding source pretrained-amr-to-text code is available 51 at https://github.com/msobrevillac/ Finally, the dataset contains subsets from different domains. For English (our target language in this task), there are 4 files in the training and development (dev) set each, and 7 files for the test set from the previous edition (Mille et al., 2019) and 1 file for the test set in this edition. 3 System Description In first edition of this shared task, we submit a system in which we use a data augmentation strategy (Sobrevilla Cabezudo and Pardo, 2018) to deal with the track 1. However, for this edition, we focus on the track 2 and use only resources allowed by the shared task (closed sub-track). Differently from most of the work found in the literature, we propose an End-to-End approach, jointly learning the inflection generation and word ordering tasks. 3.1 GPT-2 for UD structures Inspired by the work of Mager et al. (2020), we use GPT-2 and fine-tune on the joint distribution of UD structure and text. Given a tokenized sentence w1 wN and the sequential UD structure a1 ...aM , we maximize the joint probability. pGP T −2 (w, a) = N Y pGP T"
2020.webnlg-1.14,2020.webnlg-1.7,0,0.0308617,"Missing"
2020.webnlg-1.14,D19-1052,0,0.036188,"Missing"
2020.webnlg-1.14,N19-1423,0,0.176486,"luded in the training set (seen categories) and categories not included in the training set (unseen categories) to evaluate the ability to generalise of the different approaches. In general, several approaches have been explored in this task and pipeline approaches have shown a better performance than End-to-End approaches for unseen categories but not for seen ones (Castro Ferreira et al., 2019), leaving the ability to adequately deal with both categories as an open problem. Transfer learning has gained relevance in the Natural Language Processing area and pretrained architectures like BERT (Devlin et al., 2019) or GPT (Radford et al., 2018) have outperformed prior state-of-the-art in several tasks and shown a good generalisation ability. Even though pretrained models have been widely used in text-to-text generation (such as text simplification and automatic summarisation), this is not the case for data-to-text generation, as the input of this task is generally a graph instead of a text. Recently, Mager et al. (2020) fine-tuned GPT-2 for a data-to-text generation task, showing improvements and that current pretrained models can deal with these representations even if the knowledge is not explicitly s"
2020.webnlg-1.14,W17-3518,0,0.304531,"baseline and other systems in almost all automatic measures. However, the human evaluation shows better results for our system. Besides, results suggest that BART may generate paraphrases of reference texts. 1 Introduction In recent years, the RDF-to-text generation task has gained interest from many researchers (BouayadAgha et al., 2014) as the RDF language, in which DBpedia is encoded, is widely used within the Linked Data framework and large scale datasets are encoded in this language. In order to foster the use of the RDF language in this context, the first WebNLG challenge was proposed (Gardent et al., 2017). This challenge only focused on the text generation task for English and provided a test set that comprised categories included in the training set (seen categories) and categories not included in the training set (unseen categories) to evaluate the ability to generalise of the different approaches. In general, several approaches have been explored in this task and pipeline approaches have shown a better performance than End-to-End approaches for unseen categories but not for seen ones (Castro Ferreira et al., 2019), leaving the ability to adequately deal with both categories as an open probl"
2020.webnlg-1.14,W07-0734,0,0.0554238,"a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture (Vaswani et al., 2017) with a bidirectional encoder similar to BERT (Devlin et al., 2019) and a left-to-right decoder similar to GPT (Radford et al., 2018) (Figure 2). 4 System Description As mentioned, our approach is based on BART. Thus, in order to preprocess the input for BART, we linearise the triples by putting all triples (in 132 5 Results and Discussion The performance of the several proposals at the challenge is computed by using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), chrF++ (Popovi´c, 2017), Bertscore (Zhang* et al., 2020), and BLEURT (Sellam et al., 2020).4 2 Available at https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ recaser/train-recaser.perl 3 We use the perl code available at https: //github.com/moses-smt/mosesdecoder/ tree/master/scripts/tokenizer for the punctuation normaliser and the detokeniser. 4 The platform used to compute the measures is the one proposed by Moussalem et al. (2020). Table 1 shows the results of our approach and the baselines.5 In general, our approach obtained worse performances tha"
2020.webnlg-1.14,2020.acl-main.703,0,0.214851,"text generation (such as text simplification and automatic summarisation), this is not the case for data-to-text generation, as the input of this task is generally a graph instead of a text. Recently, Mager et al. (2020) fine-tuned GPT-2 for a data-to-text generation task, showing improvements and that current pretrained models can deal with these representations even if the knowledge is not explicitly structured. In this context, this paper presents the system description submitted by the NILC team to the WebNLG+ challenge 2020 (Castro-Ferreira et al., 2020). Specifically, we fine-tune BART (Lewis et al., 2020), a denoising autoencoder for pretraining sequence-to-sequence models, on the RDF-totext generation dataset provided by this task.1 2 WebNLG+ Challenge The first WebNLG challenge (Gardent et al., 2017) consisted in generating English text from a set of RDF triples extracted from DBpedia. Differently from the previous edition, this edition comprises two tasks: • RDF-to-text generation, similarly to WebNLG 2017 but with new data and for English and Russian; • Text-to-RDF semantic parsing: converting a text into the corresponding set of RDF triples. 1 The corresponding source code is available at"
2020.webnlg-1.14,2020.tacl-1.47,0,0.0208469,"4 -0.499 UPC-POE / id14 4/4 -0.531 ORANGE-NLG / id13 4/4 -0.71 Huawei / id17 3/4 -0.425 Correctness Ours 3/4 -0.589 UPC-POE / id14 4/4 -0.701 4/4 -0.668 ORANGE-NLG // id13 Huawei / id17 3/4 -0.389 Text structure Ours 3/4 -0.402 UPC-POE / id14 4/4 -0.456 ORANGE-NLG / id13 3/4 -0.338 Huawei / id17 3/4 -0.373 Fluency Ours 5/5 -0.408 UPC-POE / id14 5/5 -0.508 ORANGE-NLG / id13 5/5 -0.332 Huawei / id17 5/5 -0.369 Avg. Raw results than the ones that are more related to n-gram overlapping. As future work, we plan to evaluate other alternatives for the linearisation process and use multilingual BART (Liu et al., 2020) in order to deal with the same task for Russian. Acknowledgements 81.605 75.845 79.959 84.743 This work was financed in part by the Coordenac¸a˜ o de Aperfeic¸oamento de Pessoal de N´ıvel Superior – Brasil (CAPES) – Finance Code 88882.328822/2019-01. The authors are also grateful to USP Research Office (PRP 668) for supporting this work, and would like to thank NVIDIA for donating the GPU. This work is part of the OPINANDO project (https://sites. google.com/icmc.usp.br/opinando/) and the USP/FAPESP/IBM Center for Artificial Intelligence (C4AI - http://c4ai.inova.usp.br/). Finally, this resear"
2020.webnlg-1.14,2020.acl-main.167,0,0.0150651,"bility to adequately deal with both categories as an open problem. Transfer learning has gained relevance in the Natural Language Processing area and pretrained architectures like BERT (Devlin et al., 2019) or GPT (Radford et al., 2018) have outperformed prior state-of-the-art in several tasks and shown a good generalisation ability. Even though pretrained models have been widely used in text-to-text generation (such as text simplification and automatic summarisation), this is not the case for data-to-text generation, as the input of this task is generally a graph instead of a text. Recently, Mager et al. (2020) fine-tuned GPT-2 for a data-to-text generation task, showing improvements and that current pretrained models can deal with these representations even if the knowledge is not explicitly structured. In this context, this paper presents the system description submitted by the NILC team to the WebNLG+ challenge 2020 (Castro-Ferreira et al., 2020). Specifically, we fine-tune BART (Lewis et al., 2020), a denoising autoencoder for pretraining sequence-to-sequence models, on the RDF-totext generation dataset provided by this task.1 2 WebNLG+ Challenge The first WebNLG challenge (Gardent et al., 2017)"
2020.webnlg-1.14,P02-1040,0,0.106393,"sing function, and (2) learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture (Vaswani et al., 2017) with a bidirectional encoder similar to BERT (Devlin et al., 2019) and a left-to-right decoder similar to GPT (Radford et al., 2018) (Figure 2). 4 System Description As mentioned, our approach is based on BART. Thus, in order to preprocess the input for BART, we linearise the triples by putting all triples (in 132 5 Results and Discussion The performance of the several proposals at the challenge is computed by using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), TER (Snover et al., 2006), chrF++ (Popovi´c, 2017), Bertscore (Zhang* et al., 2020), and BLEURT (Sellam et al., 2020).4 2 Available at https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ recaser/train-recaser.perl 3 We use the perl code available at https: //github.com/moses-smt/mosesdecoder/ tree/master/scripts/tokenizer for the punctuation normaliser and the detokeniser. 4 The platform used to compute the measures is the one proposed by Moussalem et al. (2020). Table 1 shows the results of our approach and the baselines.5 In general, our approa"
2020.webnlg-1.14,W17-4770,0,0.031216,"Missing"
2020.webnlg-1.14,2020.acl-main.704,0,0.0375388,"Missing"
2020.webnlg-1.14,2006.amta-papers.25,0,0.165593,"Missing"
D19-6313,W13-2322,0,0.662131,"eiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get large corpora with this kind of annotation (due to the difficult and expensive annotation task that it represents) constrains the development of research in other"
D19-6313,W17-3501,0,0.133797,"t the performance decreases in each cut as the cut quality decreases as well. • training on cut 1 plus each cut included progressively. At the beginning, the training set was composed by the cut 1. Then, a lower quality cut was added to the training set at each training phase until all the cuts were included. The goal of this experiment was to evaluate how the method performance varied when lower quality data was inserted into the training set. Experiments Experiments were performed using the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) methods provided by Castro Ferreira et al. (2017) to compare how each method behaved in the evaluated context. The SMT method used the same parameters proposed by Castro Ferreira et al. (2017) and a 5-gram language model trained on the BP corpus provided by Hartmann et al. (2017). Also, the AMR graph pre-processing comprised a compression and a pre-ordering step without delexicalization (described as -Delex+Compress+Preorder in the original paper) as this configuration got one of the best results. The NMT method used similar parameters to Castro Ferreira et al. (2017). The encoder was bidirectional RNN with GRU, each with a 1024D hidden unit"
D19-6313,N18-1104,0,0.343797,"e fact that there are less datasets in the deep track is directly related to the higher complexity of the conversion compared to the shallow track, for which a superficial processing (basically order randomization) is sufficient. Among the efforts to build or adapt semantic representations for non-English languages, it is possible to cite Abstract Meaning Representation (AMR) as an example. Although AMR was not born as an interlingua, several works have tried to use it in that way to annotate sentences in other languages like Chinese and Czech (Xue et al., 2014), Italian, Spanish, and German (Damonte and Cohen, 2018) and Brazilian Portuguese (Anchiˆeta and Pardo, 2018). Other works have tried to adapt the English AMR guidelines to Spanish and Brazilian Portuguese with some success (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). However, most of these works report a small number of AMRannotated sentences (compared to the English corpus) and are restricted to some domains like tales (“The Little Prince”). To the best of our knowledge, the only AMR-annotated corpus comparable (in terms of size) to the English corpus2 is the This paper presents an exploratory study that aims to evaluate"
D19-6313,S15-1026,0,0.0228902,"Missing"
D19-6313,N13-1073,0,0.0260316,"Missing"
D19-6313,D18-1045,0,0.0604478,"Missing"
D19-6313,P17-1017,0,0.0225501,"is the research area that aims to give to the computers the ability to generate texts in human language from some underlying representation of information (Reiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get lar"
D19-6313,L18-1157,1,0.884612,"Missing"
D19-6313,N18-3017,0,0.0217459,"nglish languages on it. Our methodology for generating corpus and the experiments performed are presented in §4. Furthermore, §5 contains the results and a discussion about the results. Finally, the conclusions and future work are presented in §6. 2 Related Work Several works have proven the usefulness of translating corpora to increase the dataset size and improve the performance of their models. For example, Klinger and Cimiano (2015) used Phrasebased MT and some quality estimation measures to build a corpus with the best translations and use it in Sentiment Analysis. Misu et al. (2012) and Gaspers et al. (2018) explored back-translation in Natural Language Understanding systems using different measures. Misu et al. (2012) showed that BLEU is not a good quality measure and Gaspers et al. (2018) used measures from alignments, machine translation and language models to select the best sentences to be included in the corpus. Monsalve et al. (2019) also explored some quality measures (BLEU and METEOR) to select the best sentences and build a non-English corpus for Reading Comprehension and Word Sense Disambiguation. Among the results, they showed that despite the introduction of low-quality sentences, th"
D19-6313,J05-1004,0,0.050963,"malism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph (Banarescu et al., 2013). This representation includes information about semantic roles, named entities, spatial-temporal information, and co-references, among other information. AMR-annotated sentences may be represented using logic forms, PENMAN notation, and graphs (Figure 1). AMR has gained relevance in the research community due to its attempt to abstract away from syntactic idiosyncrasies5 and its wide use of other comprehensive linguistic resources, such as PropBank (Palmer et al., 2005).6 The current AMR-annotated corpus for English contains 39,260 sentences. Some efforts have been performed to build a corpus for Non-English languages leveraging the alignments and the parallel corpora that exist and trying to consider AMR an interlingua (Xue et al., 2014; Damonte and Cohen, 2018; Anchiˆeta and Pardo, 2018). Other works tried to adapt the AMR guidelines to other languages (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). For Brazilian Portuguese, there are two AMRannotated corpora, one automatically built from the alignments between the sentences of the “T"
D19-6313,W17-6615,0,0.156579,"Missing"
D19-6313,P02-1040,0,0.10381,"(2017). The encoder was bidirectional RNN with GRU, each with a 1024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópic"
D19-6313,K15-1016,0,0.0347778,"arget language. However, the quality of the translations depends on the language pair. Thus, it is important to filter out some translations according to their quality. This may be accomplished by applying backtranslation and performing a quality evaluation (using some quality measures like BLEU or METEOR) in English. In Machine Translation, Backtranslation consists of translating a target sentence (in our case, Portuguese) into a source language (in our case, English). This approach has shown good performance in some classification tasks like Sentiment Analysis and Word Sense Disambiguation (Klinger and Cimiano, 2015; Monsalve et al., 2019). Furthermore, Monsalve et al. (2019) show that despite the introduction of sentences with low quality (according to quality measures), the performance of the classifiers continues improving. Also, this approach has been successful in the context of neural machine translation (Sennrich et al., 2016). In the case of NLG from semantic representations, it would be expected that quality is critical since low-quality sentences may lead to models learning incorrect language. Additionally, other issues that may impact the performance of this task are the translation of the sem"
D19-6313,P18-1080,0,0.0285281,"involve language generation, it is noted that back-translation has been widely, and successfully, used in neural machine translation. The aim was to generate synthetic source sentences to increase the parallel training dataset (Sennrich et al., 2016; Edunov et al., edu/LDC2017T10. 3 Available at https://catalog.ldc.upenn. edu/LDC2019T07 4 A cut consists of a set of sentences of the corpus with a similar quality. 95 ∃ d, m, m1, d1: instance(d, describe-01) ∧ instance(m, man) ∧ instance(m1, mission) ∧ instance(d1, disaster) ∧ ARG0(d, m) ∧ ARG1(d, m1) ∧ ARG2(d, d1) (a) Logic format 2018). Also, Prabhumoye et al. (2018) applied back-translation to perform style transfer with good results. Concerning the described work, a question emerges: How can back-translation influence NLG from semantic representations? It is important to note that not only English sentences will be translated into BP ones, but its corresponding semantic representations will be translated to handle representations for Portuguese. Thus, several issues related to alignments may affect the performance (in addition to the quality translation). The following sections show the influence of backtranslation in NLG. 3 (d / describe-01 :ARG0 (m /"
D19-6313,W07-0734,0,0.0557334,"tional RNN with GRU, each with a 1024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópico~e.5 :mod (r / research-01~e.7,6"
D19-6313,P16-1009,0,0.200901,"ine Translation, Backtranslation consists of translating a target sentence (in our case, Portuguese) into a source language (in our case, English). This approach has shown good performance in some classification tasks like Sentiment Analysis and Word Sense Disambiguation (Klinger and Cimiano, 2015; Monsalve et al., 2019). Furthermore, Monsalve et al. (2019) show that despite the introduction of sentences with low quality (according to quality measures), the performance of the classifiers continues improving. Also, this approach has been successful in the context of neural machine translation (Sennrich et al., 2016). In the case of NLG from semantic representations, it would be expected that quality is critical since low-quality sentences may lead to models learning incorrect language. Additionally, other issues that may impact the performance of this task are the translation of the semantic representation and the alignments between language pairs. In this context, this paper presents an exploratory study that aims to evaluate the usefulness of back-translation in NLG from semantic representations for non-English languages. Specifically, AMR and Brazilian Portuguese (BP) are chosen as semantic representa"
D19-6313,L18-1486,0,0.0225367,"Missing"
D19-6313,2006.amta-papers.25,0,0.0144211,"024D hidden unit. Source and target word embeddings were 300D each and both were trained jointly with It is worth noting that each configuration was performed using the cuts generated by F and METEOR to evaluate the quality measure in the corpus selection task. The test was performed on the automatically generated test set described in §4.1.1. In order to compare the results in a real context, the methods were also evaluated on the AMR-annotated BP dataset described in §3. Similar to Castro Ferreira et al. (2017), we used BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as metrics to evaluate fluency, adequacy and postediting efforts of the models, respectively. 98 Eu0 posso1 trabalhar2 no3 meu 4 tópico 5 de6 pesquisa 7 atual 8 . 9 I 0 can 1 work 2 on 3 my 4 current 5 research 6 topic 7 . 8 (a) Alignments between English and Brazilian Portuguese sentences (p / possible-01~e.1 :ARG1 (w / work-01~e.2 :ARG0 (i / i~e.0,4) :ARG1~e.3 (t / topic~e.7 :mod (r / research-01~e.6 :ARG0 i) :time (c / current~e.5)))) (p / possible-01~e.1 :ARG1 (w / trabalhar-01~e.2 :ARG0 (i / eu-eu~e.4,0) :ARG1~e.3 (t / tópico~e.5 :mod (r / research-01~e.7,6 :ARG0 i) :time (c / atual~e.8)"
D19-6313,W18-3601,1,0.874759,"slation as Strategy to Tackle the Lack of Corpus in Natural Language Generation from Semantic Representations Marco Antonio Sobrevilla Cabezudo♣ Simon Mille♠ Thiago Alexandre Salgueiro Pardo♣ ♣ Interinstitutional Center for Computational Linguistics (NILC) Institute of Mathematical and Computer Sciences, University of S˜ao Paulo. S˜ao Carlos/SP, Brazil ♠ Universitat Pompeu Fabra. Barcelona, Spain msobrevillac@usp.br, simon.mille@upf.edu, taspardo@icmc.usp.br Abstract There are not as many works for languages other than English: in 2018, the first multilingual surface realization was proposed (Mille et al., 2018). This event proposed two tasks, one focused on reordering a dependency tree and generating inflected words (called shallow track), and the other one focused on generating sentences from a deepsyntax representation similar to a semantic representation (called deep track). It is important to note that while NLG methods were evaluated in corpora for ten different languages in the shallow track, the deep track was limited to evaluating NLG methods on three languages (English, Spanish, and French). The fact that there are less datasets in the deep track is directly related to the higher complexity"
D19-6313,W19-4028,1,0.875699,"the efforts to build or adapt semantic representations for non-English languages, it is possible to cite Abstract Meaning Representation (AMR) as an example. Although AMR was not born as an interlingua, several works have tried to use it in that way to annotate sentences in other languages like Chinese and Czech (Xue et al., 2014), Italian, Spanish, and German (Damonte and Cohen, 2018) and Brazilian Portuguese (Anchiˆeta and Pardo, 2018). Other works have tried to adapt the English AMR guidelines to Spanish and Brazilian Portuguese with some success (Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). However, most of these works report a small number of AMRannotated sentences (compared to the English corpus) and are restricted to some domains like tales (“The Little Prince”). To the best of our knowledge, the only AMR-annotated corpus comparable (in terms of size) to the English corpus2 is the This paper presents an exploratory study that aims to evaluate the usefulness of backtranslation in Natural Language Generation (NLG) from semantic representations for nonEnglish languages. Specifically, Abstract Meaning Representation and Brazilian Portuguese (BP) are chosen as semantic representa"
D19-6313,xue-etal-2014-interlingua,0,0.035807,"Missing"
D19-6313,W16-6644,0,0.020963,"anguage from some underlying representation of information (Reiter and Dale, 2000). This area has gained relevance in the Natural Language Processing community and in the industry in the last years. There are several works and efforts in NLG for English.1 Recently, shared-tasks focused on NLG from semantic representations have gained the attention of the NLG community. Thus, several representations have emerged for attending different contexts. For example, the RDF-based representation presented by Gardent et al. (2017) in its WebNLG challenge, the dialog-act-based representation presented by Novikova et al. (2016), and Abstract Meaning Representation (Banarescu et al., 2013). 1 Most of the work may be found at https://aclweb. org/anthology/sigs/siggen/. 2 Available at https://catalog.ldc.upenn. 94 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 94–103 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chinese corpus, containing 10,149 annotated sentences in its first version.3 This difficulty to get large corpora with this kind of annotation (due to the difficult and expensive annotation task that"
hartmann-etal-2014-large,C10-1030,0,\N,Missing
hartmann-etal-2014-large,C10-3015,0,\N,Missing
hartmann-etal-2014-large,W09-2112,0,\N,Missing
hartmann-etal-2014-large,P11-2013,0,\N,Missing
hartmann-etal-2014-large,J06-3001,0,\N,Missing
hartmann-etal-2014-large,P11-1038,0,\N,Missing
hartmann-etal-2014-large,J96-2004,0,\N,Missing
hartmann-etal-2014-large,W11-4523,0,\N,Missing
L18-1157,W13-2322,0,0.852736,"sentation is one of the most important components in semantic parsing. Its production is motivated by the hypothesis that semantics may be used to improve many natural language tasks, such as summarization, question answering, textual entailment, and machine translation, among others. In this context, there are several available meaning representations, as the traditional First-Order Logic (FOL), as detailed in Jurafsky and Martin (2009), semantic networks (Lehmann, 1992), Universal Networking Language (UNL) (Uchida et al., 1996), and, more recently, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In particular, AMR got the attention of the scientific community due to its relatively simpler structure, establishing the connections/relations among nodes/concepts, making them easy to read. Moreover, AMR structures are arguably easier to produce than traditional formal meaning representations (Bos, 2016). According to Banarescu et al. (2013), AMR-annotated corpora are motivated by the need of providing to the NLP community datasets with embedded annotations related to the traditional tasks of NLP, for instance, named entity recognition, semantic role labeling, word sense disambiguation, a"
L18-1157,P13-2131,0,0.143998,"“machine”, and the relations are :ARG0 and :ARG1, represented by labeled directed edges in the graph. In Figure 2, the symbols “a”, “g”, and “m” are variables, which may be re-used in the annotation, corresponding to reentrancies (multiple incoming edges) in the graph. Moreover, AMR represents negation in a different way. It uses the :polarity relation between the negated concept and the constant ‘−’ (minus signal). For instance, the sentence “I do not much like to take the tone of a moralist.”, extracted Figure 4: PENMAN notation representing negation Finally, to evaluate the AMR structures, Cai and Knight (2013) introduced the Smacth metric, which computes the degree of overlap between two AMR structures, computing precision, recall, and f-score over AMR annotation triples. 3. Our Corpus There are some available corpora in the Linguistic Data Consortium (LDC), which offer texts in different domains but are not freely available. For now, only two AMR corpora are publicly accessible2 : Bio AMR Corpus and the Little Prince Corpus. The first includes texts from the biomedical domain, extracted from PubMed3 , whereas the second contains the full text of the famous novel The Little Prince, written by Antoi"
L18-1157,E17-1051,0,0.0217133,". In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sentence in AMR representat"
L18-1157,duran-aluisio-2012-propbank,0,0.471341,"Missing"
L18-1157,P14-1134,0,0.106989,"instance, named entity recognition, semantic role labeling, word sense disambiguation, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For"
L18-1157,P16-1001,0,0.013052,"ord sense disambiguation, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes"
L18-1157,kingsbury-palmer-2002-treebank,0,0.344817,"ections/relations among nodes/concepts, making them easy to read. Moreover, AMR structures are arguably easier to produce than traditional formal meaning representations (Bos, 2016). According to Banarescu et al. (2013), AMR-annotated corpora are motivated by the need of providing to the NLP community datasets with embedded annotations related to the traditional tasks of NLP, for instance, named entity recognition, semantic role labeling, word sense disambiguation, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and tex"
L18-1157,N15-1114,0,0.0221401,"ingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sentence in AMR representation. However, in spite of the difficulties, it is important to put some effort on corpus creation for other lang"
L18-1157,L16-1630,0,0.0241591,"resentation with good acceptance and several applications in the Natural Language Processing area. Following what has been done for other languages, and using an alignment-based approach for annotation, we annotated the Little Prince book, which went into the public domain and explored some language-specific annotation issues. Keywords: Abstract Meaning Representation (AMR), corpus annotation, Portuguese language 1. Introduction Due to its wide applicability and potentialities, Natural Language Understanding (NLU) has gained interest and fostered research on themes of computational semantics (Oepen et al., 2016). According to Ovchinnikova (2012), NLU is the field of Natural Language Processing (NLP) that deals with machine reading comprehension. The objective of an NLU system is to specify a computational model to interpret one or more input text fragments. The interpretation is usually carried out by a semantic parsing technique, which maps natural language into a suitable meaning representation. A meaning representation is one of the most important components in semantic parsing. Its production is motivated by the hypothesis that semantics may be used to improve many natural language tasks, such as"
L18-1157,J05-1004,0,0.0390519,"s/concepts, making them easy to read. Moreover, AMR structures are arguably easier to produce than traditional formal meaning representations (Bos, 2016). According to Banarescu et al. (2013), AMR-annotated corpora are motivated by the need of providing to the NLP community datasets with embedded annotations related to the traditional tasks of NLP, for instance, named entity recognition, semantic role labeling, word sense disambiguation, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamg"
L18-1157,N15-1119,0,0.017639,"s that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sentence in AMR representation. However, in spite of the difficulties, it is important to put some effort on corpus creation for other languages. Annotated corpora are important resources, as they provide qualitative and reusable data for b"
L18-1157,K15-1004,0,0.0283101,"ic role labeling, word sense disambiguation, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) too"
L18-1157,W16-6603,0,0.0353948,"., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sentence in AMR representation. However, in spite of the difficulties, it is important to put some effort on corpus creation for other languages. Annotated corpora are important resource"
L18-1157,P17-2002,0,0.0270942,"istic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sentence in AMR representation. However, in spite of the difficulties, it is important to put some effort on corpus creation for other languages. Annotated corpora are important resources, as they provide q"
L18-1157,N15-1040,0,0.0994551,"Missing"
L18-1157,D16-1065,0,0.0834566,"on, and coreference. In this sense, the AMR annotation especially focuses on the predicate-argument structure as defined in PropBank (Kingsbury and Palmer, 2002; Palmer et al., 2005). Another characteristic of AMR annotation is that words that do not significantly contribute to the meaning of a sentence (which are referred as “syntactic sugar” in the original paper) are left out of the annotation, as articles and the infinitive particle “to”. From the currently available datasets, many semantic parsers emerged (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017). Furthermore, with the available parsers, some applications were developed for summarization (Liu et al., 2015) and text generation (Pourdamghani et al., 2016; Song et al., 2017), entity linking (Pan et al., 2015; Burns et al., 2016), and question answering (Mitra and Baral, 2016), among others. Although there are some available annotated corpora, most of them are for English, producing a gap between English and other languages. In addition, creating such corpora is a very expensive task. For instance, Banarescu et al. (2013) took from 7 to 10 minutes to annotate a sent"
N13-2003,estopa-etal-2000-use,0,0.133535,"Missing"
N13-2003,loukachevitch-2012-automatic,0,0.0191548,"em. Since the ATE approaches generate large lists of TCs, we have the third problem that is the time and human effort spent for validating the TCs, which usually is manually performed. The fourth problem is that the results are still not satisfactory and there is a natural ATE challenge since the difficulty in obtaining a consensus among the experts about which words are terms of a specific domain (Vivaldi and Rodr´ıguez, 2007). Our proposed ATE approach uses machine learning (ML), since it has been achieving high precision values (Zhang et al., 2008; Foo and Merkel, 2010; Zhang et al., 2010; Loukachevitch, 2012). Although ML may also generate noise and silence, it facilitates the use of a large number of TCs and their features, since ML techniques learn by themselves how to recognize a term and then they save time extracting them. Our approach differs from others because we adopt a rich feature set using varied knowledge levels. With this, it is possible to decrease the silence and noise and, consequently, to improve the ATE results. Our features range from simple statistical (e.g., term frequency) and linguistic (e.g., part of Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 16–23,"
N13-2003,W96-0213,0,0.0555424,"Missing"
N13-2003,vivaldi-etal-2012-using,0,0.0390532,"Missing"
N13-2003,zhang-etal-2008-comparative,0,0.0219076,"Missing"
P19-2011,L18-1157,1,0.892513,"Missing"
P19-2011,N16-1087,0,0.220405,"underlying non-linguistic representation of information (Reiter and Dale, 2000). Tools generated by this area are useful for other applications like Automatic Summarization, Question-Answering Systems, and others. There are several efforts in NLG for English1 . For example, one may see the works of Krahmer et al. (2003) and Li et al. (2018), which focused on referring expressions generation, and the work of (Gatt and Reiter, 2009), focused on developing a surface realisation tool called SimpleNLG. One may also easily find other works that tried to generate text from semantic representations (Flanigan et al., 2016; Ferreira et al., 2017; Puzikov and Gurevych, 2018b). For Brazilian Portuguese, there are few works, some of them focused on representations like Universal Networking Language (UNL) (Nunes et al., 2002) or Resource Description Framework (RDF) 2 1 Most of the works may be found in the main NLP publication portal at https://www.aclweb.org/anthology/ 3 Available at https://catalog.ldc.upenn.edu/LDC2017T10. Available at http://alt.qcri.org/semeval2017/task9/. 81 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 81–88 c Floren"
P19-2011,W09-0613,0,0.296255,"is a promising area in Natural Language Processing (NLP) community. NLG aims to build computer systems that may produce understandable texts in English or other human languages from some underlying non-linguistic representation of information (Reiter and Dale, 2000). Tools generated by this area are useful for other applications like Automatic Summarization, Question-Answering Systems, and others. There are several efforts in NLG for English1 . For example, one may see the works of Krahmer et al. (2003) and Li et al. (2018), which focused on referring expressions generation, and the work of (Gatt and Reiter, 2009), focused on developing a surface realisation tool called SimpleNLG. One may also easily find other works that tried to generate text from semantic representations (Flanigan et al., 2016; Ferreira et al., 2017; Puzikov and Gurevych, 2018b). For Brazilian Portuguese, there are few works, some of them focused on representations like Universal Networking Language (UNL) (Nunes et al., 2002) or Resource Description Framework (RDF) 2 1 Most of the works may be found in the main NLP publication portal at https://www.aclweb.org/anthology/ 3 Available at https://catalog.ldc.upenn.edu/LDC2017T10. Availa"
P19-2011,S17-2159,0,0.0126177,"which have shown performance increase in NLG methods (Song et al., 2018). Some limitations of these methods were the alignment dependency (similar to the previous approaches) and the linearisation of long sentences. NMT-based methods could not represent or capture information for long sentences, producing un6 Delexicalisation aims to decrease the data sparsity by replacing some common tokens by constants. 7 Compression tries to keep important concepts and relations in the text generation process. 8 Linearisation tries to transform the graph into a sequence of tokens. 5 Except for the work of Gruzitis et al. (2017), who incorporated the system proposed by Flanigan et al. (2016) into their pipeline. 84 essary to evaluate the influence of the quality of translations and how this affects the performance of the text generator. Additionally to the above issue, there are currently large corpora for Portuguese (for example, the corpus used by Hartmann et al. (2017)), which may allow to train robust language models. The main challenges for Portuguese are its morphologically marked nature and its high syntactic variation9 . These challenges contribute to data sparseness. Thus, two-stage strategies might not be u"
P19-2011,J16-3006,0,0.0245933,"It is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph. This representation includes information about semantic roles, named entities, wiki entities, spatial-temporal information, and co-references, among other information. AMR has gained attention mainly due to its simplicity to be read by humans and computers, its attempt to abstract away from syntactic idiosyncrasies (focusing only on semantic processing) and its wide use of other comprehensive linguistic resources, such as PropBank (Palmer et al., 2005) (Bos, 2016). For English, there is a large AMR-annotated corpus that contains 39,260 AMR-annotated sentences2 , which allows deeper studies in NLG and experiments with different approaches (mainly statistical approaches). This may be evidenced in the SemEval-2017 shared-task 9 (May and Priyadarshi, 2017)3 . For Brazilian Portuguese, Anchiˆeta and Pardo (2018) built the first corpus using sentences from the “The Little Prince” book. The authors took advantage of the alignment between the English and Brazilian Portuguese versions of the book to import the AMR structures from one language to This paper pres"
P19-2011,S15-1026,0,0.0342976,"Missing"
P19-2011,W17-6615,0,0.039666,"Missing"
P19-2011,W18-3601,0,0.130986,"udo and Thiago Alexandre Salgueiro Pardo Interinstitutional Center for Computational Linguistics (NILC) Institute of Mathematical and Computer Sciences, University of S˜ao Paulo S˜ao Carlos/SP, Brazil msobrevillac@usp.br, taspardo@icmc.usp.br Abstract (Moussallem et al., 2018), and other ones that are very specific to the Referring Expression Generation (Pereira and Paraboni, 2008; Lucena et al., 2010) and Surface Realisation tasks (Oliveira and Sripada, 2014; Silva et al., 2013). More recently, several representations have emerged in the NLP area (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018). In particular, Abstract Meaning Representation (AMR) has gained interest from the research community (Banarescu et al., 2013). It is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph. This representation includes information about semantic roles, named entities, wiki entities, spatial-temporal information, and co-references, among other information. AMR has gained attention mainly due to its simplicity to be read by humans and computers, its attempt to abstract away from syntactic idiosyncrasies (focusing on"
P19-2011,P17-1014,0,0.111541,"ortuguese were poor (compared to similar languages like Spanish). Two reasons to explain this issue are related to the amount of data for Portuguese in this task (less than English or Spanish) and the quality 2.3 Natural Language Generation from Abstract Meaning Representation In relation to generation methods from Abstract Meaning Representation, it was possible to highlight approaches based on machine translation 83 (Pourdamghani et al., 2016; Ferreira et al., 2017), on transformation to intermediate representations (Lampouras and Vlachos, 2017; Mille et al., 2017), on deep learning models (Konstas et al., 2017; Song et al., 2018), and on rule extraction (from graphs and trees) (Song et al., 2016; Flanigan et al., 2016). Methods based on transformation into intermediate representations focused on transforming AMR graphs into simpler representations (usually dependency trees) and then using an appropriate surface realization system. Authors usually took advantage of the similarity between dependency trees and AMR graphs to map some results. However, some problems in this approach were the need to manually build transformation rules (excepting for Lampouras and Vlachos (2017), who automatically perfor"
P19-2011,S17-2158,0,0.0218769,"e point to highlight is that the results for Portuguese were poor (compared to similar languages like Spanish). Two reasons to explain this issue are related to the amount of data for Portuguese in this task (less than English or Spanish) and the quality 2.3 Natural Language Generation from Abstract Meaning Representation In relation to generation methods from Abstract Meaning Representation, it was possible to highlight approaches based on machine translation 83 (Pourdamghani et al., 2016; Ferreira et al., 2017), on transformation to intermediate representations (Lampouras and Vlachos, 2017; Mille et al., 2017), on deep learning models (Konstas et al., 2017; Song et al., 2018), and on rule extraction (from graphs and trees) (Song et al., 2016; Flanigan et al., 2016). Methods based on transformation into intermediate representations focused on transforming AMR graphs into simpler representations (usually dependency trees) and then using an appropriate surface realization system. Authors usually took advantage of the similarity between dependency trees and AMR graphs to map some results. However, some problems in this approach were the need to manually build transformation rules (excepting for Lampour"
P19-2011,J03-1003,0,0.228351,"Missing"
P19-2011,L18-1481,0,0.0320082,"Missing"
P19-2011,J82-2005,0,0.655663,"Missing"
P19-2011,S17-2096,0,0.0341677,"Missing"
P19-2011,W18-6561,0,0.0220293,"Missing"
P19-2011,W10-1617,0,0.0814018,"Missing"
P19-2011,W18-3607,0,0.0235089,"Missing"
P19-2011,W17-5525,0,0.0362713,"Missing"
P19-2011,S17-2090,0,0.0762153,"ences, among other information. AMR has gained attention mainly due to its simplicity to be read by humans and computers, its attempt to abstract away from syntactic idiosyncrasies (focusing only on semantic processing) and its wide use of other comprehensive linguistic resources, such as PropBank (Palmer et al., 2005) (Bos, 2016). For English, there is a large AMR-annotated corpus that contains 39,260 AMR-annotated sentences2 , which allows deeper studies in NLG and experiments with different approaches (mainly statistical approaches). This may be evidenced in the SemEval-2017 shared-task 9 (May and Priyadarshi, 2017)3 . For Brazilian Portuguese, Anchiˆeta and Pardo (2018) built the first corpus using sentences from the “The Little Prince” book. The authors took advantage of the alignment between the English and Brazilian Portuguese versions of the book to import the AMR structures from one language to This paper presents a more recent literature review on Natural Language Generation. In particular, we highlight the efforts for Brazilian Portuguese in order to show the available resources and the existent approaches for this language. We also focus on the approaches for generation from semantic representat"
P19-2011,P17-2002,0,0.0144058,"ng for Lampouras and Vlachos (2017), who automatically perform this) and the need of alignments between the AMR graph and intermediate representations, which could bring noise into the generation process. Overall, this approach presented poor results (compared to other approaches) in automatic evaluations5 Methods based on rule extraction obtained better results than the approach mentioned previously. This approach tries to learn conversion rules from AMR graphs (or trees) to the final text. First methods of this approach tried to transform the AMR graph into a tree before learning rules. As (Song et al., 2017) mentioned, these methods suffer with the loss of information (by not using graphs and being restricted to trees), due to its projective nature. Likewise, (Song et al., 2016) and (Song et al., 2017) could suffer from the same problem (ability to deal with non-projective structures) due to their nature to extract and apply the learned rules. Furthermore, these methods used some manual rules to keep the text fluency. However, these rules did not produce a statistically significant increase in the performance, when compared to learned rules. Some problems of this approach are related to: (1) the"
P19-2011,W14-4412,0,0.0752879,"Missing"
P19-2011,D16-1224,0,0.15026,"his issue are related to the amount of data for Portuguese in this task (less than English or Spanish) and the quality 2.3 Natural Language Generation from Abstract Meaning Representation In relation to generation methods from Abstract Meaning Representation, it was possible to highlight approaches based on machine translation 83 (Pourdamghani et al., 2016; Ferreira et al., 2017), on transformation to intermediate representations (Lampouras and Vlachos, 2017; Mille et al., 2017), on deep learning models (Konstas et al., 2017; Song et al., 2018), and on rule extraction (from graphs and trees) (Song et al., 2016; Flanigan et al., 2016). Methods based on transformation into intermediate representations focused on transforming AMR graphs into simpler representations (usually dependency trees) and then using an appropriate surface realization system. Authors usually took advantage of the similarity between dependency trees and AMR graphs to map some results. However, some problems in this approach were the need to manually build transformation rules (excepting for Lampouras and Vlachos (2017), who automatically perform this) and the need of alignments between the AMR graph and intermediate representatio"
P19-2011,J05-1004,0,0.0698243,"arescu et al., 2013). It is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph. This representation includes information about semantic roles, named entities, wiki entities, spatial-temporal information, and co-references, among other information. AMR has gained attention mainly due to its simplicity to be read by humans and computers, its attempt to abstract away from syntactic idiosyncrasies (focusing only on semantic processing) and its wide use of other comprehensive linguistic resources, such as PropBank (Palmer et al., 2005) (Bos, 2016). For English, there is a large AMR-annotated corpus that contains 39,260 AMR-annotated sentences2 , which allows deeper studies in NLG and experiments with different approaches (mainly statistical approaches). This may be evidenced in the SemEval-2017 shared-task 9 (May and Priyadarshi, 2017)3 . For Brazilian Portuguese, Anchiˆeta and Pardo (2018) built the first corpus using sentences from the “The Little Prince” book. The authors took advantage of the alignment between the English and Brazilian Portuguese versions of the book to import the AMR structures from one language to Thi"
P19-2011,P18-1150,0,0.0652646,"ompared to similar languages like Spanish). Two reasons to explain this issue are related to the amount of data for Portuguese in this task (less than English or Spanish) and the quality 2.3 Natural Language Generation from Abstract Meaning Representation In relation to generation methods from Abstract Meaning Representation, it was possible to highlight approaches based on machine translation 83 (Pourdamghani et al., 2016; Ferreira et al., 2017), on transformation to intermediate representations (Lampouras and Vlachos, 2017; Mille et al., 2017), on deep learning models (Konstas et al., 2017; Song et al., 2018), and on rule extraction (from graphs and trees) (Song et al., 2016; Flanigan et al., 2016). Methods based on transformation into intermediate representations focused on transforming AMR graphs into simpler representations (usually dependency trees) and then using an appropriate surface realization system. Authors usually took advantage of the similarity between dependency trees and AMR graphs to map some results. However, some problems in this approach were the need to manually build transformation rules (excepting for Lampouras and Vlachos (2017), who automatically perform this) and the need"
P19-2011,W16-6603,0,0.0891631,"hods used data augmentation strategies to deal with data sparsity (Elder and Hokamp, 2018; Sobrevilla Cabezudo and Pardo, 2018). One point to highlight is that the results for Portuguese were poor (compared to similar languages like Spanish). Two reasons to explain this issue are related to the amount of data for Portuguese in this task (less than English or Spanish) and the quality 2.3 Natural Language Generation from Abstract Meaning Representation In relation to generation methods from Abstract Meaning Representation, it was possible to highlight approaches based on machine translation 83 (Pourdamghani et al., 2016; Ferreira et al., 2017), on transformation to intermediate representations (Lampouras and Vlachos, 2017; Mille et al., 2017), on deep learning models (Konstas et al., 2017; Song et al., 2018), and on rule extraction (from graphs and trees) (Song et al., 2016; Flanigan et al., 2016). Methods based on transformation into intermediate representations focused on transforming AMR graphs into simpler representations (usually dependency trees) and then using an appropriate surface realization system. Authors usually took advantage of the similarity between dependency trees and AMR graphs to map some"
P19-2011,W18-3602,0,0.102645,"nformation (Reiter and Dale, 2000). Tools generated by this area are useful for other applications like Automatic Summarization, Question-Answering Systems, and others. There are several efforts in NLG for English1 . For example, one may see the works of Krahmer et al. (2003) and Li et al. (2018), which focused on referring expressions generation, and the work of (Gatt and Reiter, 2009), focused on developing a surface realisation tool called SimpleNLG. One may also easily find other works that tried to generate text from semantic representations (Flanigan et al., 2016; Ferreira et al., 2017; Puzikov and Gurevych, 2018b). For Brazilian Portuguese, there are few works, some of them focused on representations like Universal Networking Language (UNL) (Nunes et al., 2002) or Resource Description Framework (RDF) 2 1 Most of the works may be found in the main NLP publication portal at https://www.aclweb.org/anthology/ 3 Available at https://catalog.ldc.upenn.edu/LDC2017T10. Available at http://alt.qcri.org/semeval2017/task9/. 81 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 81–88 c Florence, Italy, July 28 - August 2, 2019. 2019 Associati"
P19-2011,W18-6557,0,0.110109,"nformation (Reiter and Dale, 2000). Tools generated by this area are useful for other applications like Automatic Summarization, Question-Answering Systems, and others. There are several efforts in NLG for English1 . For example, one may see the works of Krahmer et al. (2003) and Li et al. (2018), which focused on referring expressions generation, and the work of (Gatt and Reiter, 2009), focused on developing a surface realisation tool called SimpleNLG. One may also easily find other works that tried to generate text from semantic representations (Flanigan et al., 2016; Ferreira et al., 2017; Puzikov and Gurevych, 2018b). For Brazilian Portuguese, there are few works, some of them focused on representations like Universal Networking Language (UNL) (Nunes et al., 2002) or Resource Description Framework (RDF) 2 1 Most of the works may be found in the main NLP publication portal at https://www.aclweb.org/anthology/ 3 Available at https://catalog.ldc.upenn.edu/LDC2017T10. Available at http://alt.qcri.org/semeval2017/task9/. 81 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 81–88 c Florence, Italy, July 28 - August 2, 2019. 2019 Associati"
P19-2011,W18-3608,1,0.913708,"issues are discussed in what follows. 2.1 Natural Language Generation for Portuguese In general, we could find few works for Portuguese (considering the existing works for English). These works focus mainly on the referring expression generation (Pereira and Paraboni, 2008; Lucena et al., 2010) and surface realization tasks (Silva et al., 2013; Oliveira and Sripada, 2014), usually restricted to specific domains and applications (like undergraduate test scoring). Nevertheless, there are some recent attempts focused on other tasks and in more general domains (Moussallem et al., 2018; Sobrevilla Cabezudo and Pardo, 2018). Among the NLG approaches, we may highlight the use of templates (Pereira and Paraboni, 2008; Novais et al., 2010b), rules (Novais and Paraboni, 2.2 Natural Language Generation from Semantic Representations Recently, the number of works on NLG from semantic representations has increased. This increase is reflected in the shared tasks WebNLG (Gardent et al., 2017), E2E Challenge (Novikova et al., 2017), Semeval Task-9 (May and Priyadarshi, 2017) and Surface Realization Shared-Task (Belz et al., 2011; Mille et al., 2018). In general, there is a trend to apply methods based on neural networks. H"
R15-1057,P02-1047,0,0.240813,"437 2.2 Corpus RST-DT-PT CSTNews Rhetalho Summ-it CorpusTCC RST-DT Semi-supervised Discourse Parsing All the above cited approaches to DP use annotated data to extract discursive knowledge and are limited to the availability of this resource, which is expensive to obtain. Specially, it is important to note that, to obtain good performance in the task more data is necessary. Semi-supervised learning (SSL) is employed in scenarios in which there is some labeled data and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar res"
R15-1057,W01-1605,0,0.147156,"Missing"
R15-1057,P12-2042,0,0.0164249,"btain good performance in the task more data is necessary. Semi-supervised learning (SSL) is employed in scenarios in which there is some labeled data and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar results for a different set of relations and a more sophisticated classifier. Building on this, there is an interesting idea, known as never-ending learning (NEL) by Carlson et al. (2010), in which they apply SSL with infinite unlabeled data. The needed data is widely and freely available on the web. Their arc"
R15-1057,P09-1075,0,0.0180718,"according to RST. From Soricut and Marcu (2003). the EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012). To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations b"
R15-1057,P12-1007,1,0.855186,"EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al., 2010; Feng and Hirst, 2012) and dynamic conditional random field (Joty et al., 2012). To obtain acceptable results, these approaches need plenty of labeled data. But even more than other levels of linguistic information, such as morphology or syntax, the annotation of discourse is an expensive task. Given this fact, what can we do when there is not enough data to perform effective learning of DP, as in languages with little annotated data? This paper describes a methodology to overcome the problem of insufficient labeled data in the task of identifying rhetorical relations between Introduction A text is composed of cohe"
R15-1057,W98-0306,0,0.178079,"and large availability of unlabeled data, and manual annotation is an expensive task (Zhu, 2008). Related to the use of SSL in DP, Marcu and Echihabi (2002) used naive Bayes to train binary classifiers to distinguish between some types of relations, as Elaboration vs. Cause-ExplanationEvidence. For example, for this binary classifier, applying SSL, the accuracy increased from approximately 0.6 to 0.95 after the use of millions of new instances. Chiarcos (2012) used SSL to develop a probabilistic model mapping the occurrence of discourse markers and verbs to rhetorical relations. For Italian, Soria and Ferrari (1998) conducted work in the same direction. Sporleder and Lascarides (2005) performed similar work to Marcu and Echihabi, with similar results for a different set of relations and a more sophisticated classifier. Building on this, there is an interesting idea, known as never-ending learning (NEL) by Carlson et al. (2010), in which they apply SSL with infinite unlabeled data. The needed data is widely and freely available on the web. Their architecture runs 24 hours per day, forever, obtaining new information and performing a learning task. With the aim of surpassing the limitation of labeled RST in"
R15-1057,N03-1030,0,0.509887,"enough labeled data to obtain good discourse parsing, specially in the relation identification step, and the additional use of unlabeled data is a plausible solution. A workflow is presented that uses a semi-supervised learning approach. Instead of only a predefined additional set of unlabeled data, texts obtained from the web are continuously added. This obtains near human perfomance (0.79) in intra sentential rhetorical relation identification. An experiment for English also shows improvement using a similar workflow. 1 Figure 1: An example of sentence-level structure according to RST. From Soricut and Marcu (2003). the EDUs, explicating the intentions of the author. For example, consider the sentence in Figure 1. It is segmented into three EDUs, numbered from 1 to 3. EDUs 2 and 3 are related by the relation Enablement, forming a new span of text, which is related to 1 by the relation Attribution. In each relation, EDUs can be Nucleus (more essential) or Satellite to the writer’s purpose. Many approaches have been used in DP, the majority of them using machine learning algorithms, such as probabilistic models (Soricut and Marcu, 2003), SVMs (Reitter, 2003; duVerle and Prendinger, 2009; Hernault et al.,"
R15-1057,D12-1083,0,0.0262114,"Missing"
R15-1057,D09-1036,0,0.0319408,"greement). The authors, then, use the probabilistic model with manual segmentation and syntactic trees to see the impact of this information in the parsing and the model achieves 0.75. Hernault et al. (2010) use support vector machine (SVM) classifiers to perform DP. This discourse parser is named HILDA (HIgh-Level Discourse Analyser). This work used a set of 41 rhetorical relations and achieves a F-measure of 0.48 in the step of relation identification, both intra-sentential and inter-sentential. Feng and Hirst (2012) improve HILDA by incorporating new proposed features and some adapted from Lin et al. (2009). Another important decision was the specification of features for intra-sentential and inter-sentential relationships and the use of contextual features in the building of the rhetorical tree. Considering the approach to intra-sentential relation identification, with 18 RST relations this work achieves a macro average F-measure of 0.49 and weighted average Fmeasure of 0.77 in relation identification. Joty et al. (2012) use a joint modelling approach to identify the structure and the relations at the sentence-level using DCRFs (dynamic conditional random fields) and a non-greedy bottom-up meth"
R15-1057,P95-1037,0,0.0236275,"n discourse and, to avoid overfitting of a learning model on the lessfrequent relations, no balancing was made. The relation Summary, for example, occurs only 2 times, and Elaboration occurs 1491 times, making very difficult the identification of the Summary relation. 4 Adapted Models Syntactic information is crucial in SPADE (Soricut and Marcu, 2003) and for Portuguese the parser most similar to that used by Soricut and Marcu is the LX-parser (Stanford parser trained to Portuguese (Silva et al., 2010)). After the parsing of the text by the syntactic parser, the same lexicalization procedure (Magerman, 1995) was applied and adapted according to the tagset used by LX-parser. In this adaptation, only pairs of adjacent segments at sentence-level were considered, and nuclearity was not considered, in order to avoid sparseness in the data. Training the adapted model (here called SPADE-PT) using the RST-DT-PT achieved F-measure of 0.30. The precision was 0.69, but the recall was only 0.19. The same features used by HILDA (Hernault et al., 2010) were extracted from the pairs of adjacent segments at sentence-level and many machine learning algorithms were tested, besides the SVM, which was used in the or"
S13-2095,W11-0705,0,0.065952,"ontained in Twitter messages. The task included two sub-tasks: a expression-level classification (Task A) and a message-level classification (Task B). Our system participated in Task B. In this task, for a given message, our system should classify it as positive, negative, or neutral. Our system was coded using Python and the CLiPS Pattern library (De Smedt and Daelemans, 2012). This last library provides the part-of-speech tagger and the SVM algorithm used in this work1 . 2 Related work Despite the significant number of works in sentiment analysis, few works have approached Twitter messages. Agarwal et al. (2011) explored new features for sentiment classification of twitter messages. Davidov et al. (2010) studied the use of hashtags and emoticons in sentiment classification. Diakopoulos and Shamma (2010) analyzed the people’s sentiment on Twitter for first U.S. presidential debate in 2008. The majority of works in sentiment analysis uses either machine learning techniques or lexicon-based 1 Our system code is freely available at http://github.com/pedrobalage/SemevalTwitterHybridClassifier 568 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Worksho"
S13-2095,C10-2028,0,0.0464791,"on (Task A) and a message-level classification (Task B). Our system participated in Task B. In this task, for a given message, our system should classify it as positive, negative, or neutral. Our system was coded using Python and the CLiPS Pattern library (De Smedt and Daelemans, 2012). This last library provides the part-of-speech tagger and the SVM algorithm used in this work1 . 2 Related work Despite the significant number of works in sentiment analysis, few works have approached Twitter messages. Agarwal et al. (2011) explored new features for sentiment classification of twitter messages. Davidov et al. (2010) studied the use of hashtags and emoticons in sentiment classification. Diakopoulos and Shamma (2010) analyzed the people’s sentiment on Twitter for first U.S. presidential debate in 2008. The majority of works in sentiment analysis uses either machine learning techniques or lexicon-based 1 Our system code is freely available at http://github.com/pedrobalage/SemevalTwitterHybridClassifier 568 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 568–572, Atlanta, Georgia, June 14-15, 2013."
S13-2095,W02-1011,0,0.0155085,"Missing"
S13-2095,J11-2001,0,0.0886735,"y to express sentiment and feelings about aspects of the world. In this scenario, understanding the sentiment contained in a message is of vital importance in order to understand users behavior and for market analysis (Java et al., 2007; Kwak et al., 2010). The research area that deals with the computational treatment of opinion, sentiment and subjectivity in texts is called sentiment analysis (Pang et al., 2002). Sentiment analysis is usually associated with a text classification task. Sentiment classifiers are commonly categorized in two basic approaches: lexicon-based and machine learning (Taboada et al., 2011). A lexicon-based classifier uses a lexicon to provide the polarity, or semantic orientation, of each word or phrase in the text. A machine learning classifier learns features (usually the vocabulary) from annotated corpus or labeled examples. In this paper, we present a hybrid system for sentiment classification in Twitter messages. Our system combines three different approaches: rule-based, lexicon-based and machine learning. The purpose of our system is to better understand the use of a hybrid system in Twitter text and to verify the performance of this approach in an open evaluation contes"
S13-2095,S13-2052,0,0.159671,"Missing"
S14-2074,S13-2095,1,0.866369,"Missing"
S14-2074,J11-2001,0,0.0309643,"Missing"
S14-2074,S13-2072,0,0.155587,"uld improve in relation to the previous one by including modifications on both lexicon-based and machine learning approaches. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 428 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 428–432, Dublin, Ireland, August 23-24, 2014. 2 Related work The aim of this pipeline architecture is to improve the classification process. In Balage Filho and Pardo (2013), we have shown that this hybrid classification approach may outperform the individual approaches. In the following subsections, we detail the components of our system. In the next section, we explain how the confidence level was determined. The analysis of Tweets has gained lots of interest recently. One evidence is the expressive number of participants in the SemEval-2013 Task 2: Sentiment Analysis in Twitter (Nakov et al., 2013). There were a total of 149 submissions from 44 teams. The best performing system on twitter dataset for task B was reported by Mohammad et al. (2013) with an F-mesa"
S14-2074,S13-2053,0,0.0215125,"is based on the idea that the polarity of a text can be given by the sum of the individual polarity values of each word or phrase present in the text. For this, a sentiment lexicon identifies polarity words and assigns polarity values to them (known as semantic orientations). In the 2013 system, we had used SentiStrength lexicon (Thelwall et al., 2010). In 2014, we improved our lexicon-based classifier by using a larger sentiment lexicon. We used the sentiment lexicon provided by Opinion-Lexicon (Hu and Liu, 2004) and a list of sentiment hashtags provided by the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). For dealing with negation, we used a handcrafted list of negative words. 429 In our algorithm, the semantic orientations of each individual word in the text are added up. In this approach, the algorithm searches for each word in the lexicon and only the words that were found are returned. We associate the value +1 to the positive words, and -1 to the negative words. If a polarity word is negated, its value is inverted. This lexicon-based classifier assumes the signal of the final score as the sentiment class (positive or negative) and the score zero as neutral. 3.3 with 8987 messages. The 20"
S14-2074,S13-2052,0,0.0950896,"Missing"
S14-2074,S14-2009,0,\N,Missing
S14-2074,N13-1039,0,\N,Missing
S14-2075,2020.rocling-1.35,0,0.0756292,"Missing"
S14-2075,D10-1101,0,0.0239136,"in the internet: facts and opinions. Facts are objective statements about entities and events in the world. Opinions are subjective statements that reflect people’s sentiments or perceptions about the entities and events. According to Liu, by that time, there was a lot of attention on the processing of facts but little work had been done on the processing of opinions. 2 Related work Jin and Hovy (2009) reported one the first works using sequential labeling for aspect extraction. In this work, the authors used a Lexicalized Hidden Markov Model to learn patterns to extract aspects and opinions. Jakob and Gurevych (2010) trained This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 433 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 433–436, Dublin, Ireland, August 23-24, 2014. a Conditional Random Field for aspect extraction. In this work, the authors report the results for a single domain and a cross domain experiment. They show that even in other domains the method could be good. Kim and Hovy (2006) explored"
S14-2075,W06-0301,0,0.0497716,"ns. Jakob and Gurevych (2010) trained This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 433 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 433–436, Dublin, Ireland, August 23-24, 2014. a Conditional Random Field for aspect extraction. In this work, the authors report the results for a single domain and a cross domain experiment. They show that even in other domains the method could be good. Kim and Hovy (2006) explored the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. Their method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. Houen (2011) presented a system for opinion mining with semantic analysis. The author explores the use of the semantic frame-based analyzer FrameNet (Baker et al., 1998) for modeling features in a machine learning approach. The author found that the FrameNet information was not helpful in this classifier. 3 semantic role labeling system, and the ARK SEMAFOR, a Semantic An"
S14-2075,P98-1013,0,\N,Missing
S14-2075,C98-1013,0,\N,Missing
S14-2075,S14-2004,0,\N,Missing
W07-0203,N03-1020,0,0.069115,"ency in a sentence. SuPor-2 provides a flexible way of combining linguistic and non-linguistic features for extraction. There are profound differences from TextRank. First, it is clearly language-dependent. Also, its graph-based methods do not assign weights to their vertices in order to select sentences for extraction. Instead, they traverse a graph in very specific and varied ways that mirror both linguistic interdependencies and important connections between the nodes. 6 Assessing the Four Systems To assess the degree of informativeness of the systems previously described, we adopt ROUGE2 (Lin and Hovy, 2003), whose recall rate mirrors the informativeness degree of automatically generated extracts by correlating automatic summaries with ideal ones. The two modified versions of TextRank require linguistic knowledge but at a low cost. This is certainly due to varying only preprocessing, while the main decision procedure is kept unchanged and language-independent. Those three systems do not need training, one of the main arguments in favor of TextRank (Mihalcea and Tarau, 2004). In contrast, SuPor-2 relies on training and this is certainly one of its main bottlenecks. It also employs linguistic knowl"
W07-0203,W01-0100,0,0.139099,"is its unique and distinguishing characteristic. In what follows we first review the different levels of processing in extractive AS (Section 2), then we describe TextRank and its implementation to summarize Brazilian Portuguese texts (Section 3). Our suggested modifications of TextRank are presented in Section 4, whilst SuPor-2 is described in Section 5. Finally, we compare the results of the four automatic summarizers when running on Brazilian Portuguese texts (Section 6), and make some remarks on linguistic independence for extractive AS in Section 7. 2 A Review of Automatic Summarization Mani (2001) classifies AS methods based upon three levels of linguistic processing to summarize a text, namely: · · Shallow level. At this level only features at the surface of the text are explored. For example, location (Edmunson, 1969), sentence length and presence of signaling phrases (e.g., Kupiec et al., 1995). Combined, such features may yield a salience function that drives selection of sentences of the source text to include in a summary. Entity level. The aim here is to build an internal representation of the source text that conveys its entities and corresponding 18 · relationships. These amou"
W07-0203,P05-3013,0,0.0572829,"ealized independently of the surface choices of the source text. This comprises, thus, a rewriting task. This article focuses solely on extracts of source texts written in Brazilian Portuguese. For extractive Automatic Summarization (AS), several methods have been suggested that are based upon statistics or data readily available in the source text. Word frequency (Luhn, 1958) and sentence position (Edmundson, 1969) methods are classic examples of that. Usually, extractive AS does not take into account linguistic and semantic knowledge in order to be portable to distinct domains or languages (Mihalcea, 2005). Graph-based methods aim at the same and have been gaining a lot of interest because they usually do not rely on any linguistic resource and run pretty fast. Exemplars of those are LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). In spite of their potentialities, we claim that there is a compromise in pursuing a language-free setting: however portable a system may be, it may also produce extracts that lack the degree of informativeness needed for use. Informativeness, in the current context, refers to the ability of an automatic summarizer to produce summaries that con"
W07-0203,W97-0703,0,0.781003,"ied versions of that, and SuPor-2 (Leite and Rino, 2006). TextRank works in a completely unsupervised way. Our two variations, although still unsupervised, include diverse linguistic knowledge in the preprocessing phase. SuPor-2 is the only machine learning-based system amongst the four ones, and it was built to summarize texts in Brazilian Portuguese, although it may be customized to other languages. Unlike the others, it embeds more sophisticated decision features that rely on varied linguistic resources. Some of them correspond to full summarization methods by themselves: Lexical Chaining (Barzilay and Elhadad, 1997), Relationship Mapping (Salton et al., 1997), and Importance of Topics (Larocca Neto et al., 2000). This is its unique and distinguishing characteristic. In what follows we first review the different levels of processing in extractive AS (Section 2), then we describe TextRank and its implementation to summarize Brazilian Portuguese texts (Section 3). Our suggested modifications of TextRank are presented in Section 4, whilst SuPor-2 is described in Section 5. Finally, we compare the results of the four automatic summarizers when running on Brazilian Portuguese texts (Section 6), and make some r"
W07-0203,W04-3252,0,\N,Missing
W09-2105,W03-1602,0,0.0579795,"potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to undo any undesirable simplification or to choose alternative simplifications. These alternative simplifications may be produced in two cases: i) to compose a new subject in simplifications involving relatives and appositions and ii) to choose among one of the coordinate or subordinate simplifications when there is ambiguity regarding to conjunctions. Inui et al. (2003) proposes a rule-based system for text simplification aimed at deaf people. The authors create readability assessments based on questionnaires answered by teachers about the deaf. With approximately one thousand manually created rules, the authors generate several paraphrases for each sentence and train a classifier to select the simpler ones. Promising results are obtained, although different types of errors on the paraphrase generation are encountered, such as problems with verb conjugation and regency. In our work we produce alternative simplifications only in the two cases explained above."
W09-2105,2005.jeptalnrecital-court.15,0,0.0397125,"on. Siddharthan's system deals with nonfinite clauses which are not handled by our system at this stage. Lal and Ruger’s (2002) created a bayesian summarizer with a built-in lexical simplification module, based on WordNet and MRC psycholinguistic database3. The system focuses on schoolchildren and provides background information about people and locations in the text, which are retrieved from databases. Our rule-based simplification system only replaces discourse markers for more common ones using lexical resources built in our project, instead of inserting additional information in the text. Max (2005, 2006) applies text simplification in the writing process by embedding an interactive text simplification system into a word processor. At the user’s request, an automatic parser analyzes an individual sentence and the system applies handcrafted rewriting rules. The resulting suggested simplifications are ranked by a score of syntactic complexity and potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to"
W10-1601,J07-3008,0,0.0429788,"Missing"
W10-2312,W04-1013,0,0.0607092,"Missing"
W10-2312,W01-0100,0,0.786103,"Missing"
W10-2312,W02-0404,0,0.0214442,"sed on the number of CST relations that a segment has. This criterion is based on the idea that relevant information is more repeated/elaborated and related to other segments across documents. This may be easily verified in practice. In this paper we follow such ideas. A methodology for enriching multidocument summaries produced by superficial summarizers was proposed by Zhang et al. (2002). The authors incorporated the information given by CST relations to MEAD (Radev et al., 2000) summarization process, showing that giving preference to segments with CST relations produces better summaries. Otterbacher et al. (2002) investigated how CST relations may improve cohesion in summaries, which was tested by ordering sentences in summaries according to CST relations. The idea used behind this ordering is that sentences related by CST relations should appear closer in the final summaries as well as should respect possible temporal constraints indicated by some relations. Afantenos et al. (2004) proposed another summarization methodology that extracts message templates from the texts (using information extraction tools) and, according to the type of CST relation between two templates, produces a unified message th"
W10-2312,J98-3005,0,0.144852,"Missing"
W10-2312,W00-1009,0,0.953114,"c knowledge to produce summaries. This approach usually has low cost and is more robust, but it produces poor results. On the other hand, deep approaches use more linguistic knowledge to produce summaries. In general terms, in this approach it is commonly used syntactical, semantic and discourse parsers to analyze the original documents. A very common way to analyze documents consists in establishing semantic relations among the documents parts, which helps identifying commonalities and differences in information. Within this context, discourse models as CST (Cross-document Structure Theory) (Radev, 2000) are useful (see, e.g., Afantenos et al., 2004; Afantenos, 2007; Jorge and Pardo, 2009, 2010; Radev and Mckeown, 1998; Radev et al., 2001; Zhang et al., 2002). It was proposed in Mani and Maybury (1999) a general architecture for multidocument summarization, with analysis, transformation, and synthesis stages. The first stage consists in analyzing and formally representing the content of the original documents. The second stage consists mainly in transforming the represented content into a condensed content that will be included in the final summary. One of the most important tasks in this sta"
W10-2312,W00-0403,0,0.372979,"ur system, called CSTSumm (CST-based SUMMarizer), produces multidocument summaries from input CST-analyzed news documents. We mainly investigate content selection methods for producing both generic and preference-based summaries. Particularly, we formalize and codify our content selection strategies as operators that perform the previously cited transformation stage. We run our experiments with Brazilian Portuguese news texts (previously analyzed according to CST by human experts) and show that we produce more informative summaries in comparison with some superficial summarizers (Pardo, 2005; Radev et al., 2000). We also use CST to enrich these superficial summarizers, showing that the results also improve. Our general hypothesis for this work is that the deep knowledge provided by CST helps to improve information and quality in summaries. This work is organized as follows. In Section 2, the main concepts of the CST model are introduced and the works that have already used CST for multidocument summarization are reviewed. In Section 3, we present CSTSumm, while its evaluation is reported in Section 4. Some final remarks are presented in Section 5. 2 Related Work 2.1 Cross-document Structure Theory Ra"
W11-4501,J96-2004,0,0.444695,"Missing"
W11-4501,I08-1019,0,0.0357121,"Missing"
W11-4501,W11-0123,0,0.021596,"Missing"
W11-4501,W00-1009,0,0.269639,"find contradictions, and understand the temporal evolution of a fact or event, which would allow them to approach the information in which they are interested in a more organized way. In another vein, this type of knowledge might also be useful for several computer applications, such as web browsers and automatic summarizers, which would have more information available to produce their results and meet the users’ needs more efficiently. Some theories or models on multi-document relationships have been proposed for this purpose. One of the most used is the Crossdocument Structure Theory (CST) (Radev, 2000). In this work, we propose to investigate the automatic identification of the relations among portions of several texts suggested by the CST, developing an automated multidocument parser. We explore, in particular, the use of traditional (flat) and hierarchical machine learning techniques in this task, using a corpus of news texts written in Brazilian Portuguese, already annotated according to CST, which allows applying machine learning techniques and testing them. The results obtained are that the performance of some 1 Proceedings of the 8th Brazilian Symposium in Information and Human Langua"
W11-4501,J98-3005,0,0.111758,"Missing"
W11-4514,W02-0302,0,0.0885354,"Missing"
W11-4531,W01-0100,0,0.117169,"c.usp.br Abstract. Multi-document summarization is the automatic production of a unique summary from a collection of texts. In this paper, we propose a statistical generative approach for multi-document summarization that combines simple information such as sentence position in the text and semantic-discursive information from CST (Cross-Document Structure Theory). In particular, we formulate the multi-document summarization task using a Noisy-Channel model. 1. Introduction Multi-Document Summarization (MDS) is the process of building a summary from a group of texts that have similar content (Mani, 2001). In this work we explore a Generative Approach for MDS by using a NoisyChannel framework (Shannon, 1948) for learning a MDS model. In this approach we integrate semantic-discursive knowledge to model different Multi-Document phenomena such as redundant, complementary and contradictory information. This semantic-discursive information across documents is given, for example, by CST model (Cross-Document Structure Theory) (Radev, 2000) and also RST (Rhetorical Structure Theory) (Mann and Thompson, 1987). This novel approach yields a theoretical generative learning model for MDS. 2. A Noisy-Chann"
W11-4531,W00-1009,0,0.0249828,"ing a Noisy-Channel model. 1. Introduction Multi-Document Summarization (MDS) is the process of building a summary from a group of texts that have similar content (Mani, 2001). In this work we explore a Generative Approach for MDS by using a NoisyChannel framework (Shannon, 1948) for learning a MDS model. In this approach we integrate semantic-discursive knowledge to model different Multi-Document phenomena such as redundant, complementary and contradictory information. This semantic-discursive information across documents is given, for example, by CST model (Cross-Document Structure Theory) (Radev, 2000) and also RST (Rhetorical Structure Theory) (Mann and Thompson, 1987). This novel approach yields a theoretical generative learning model for MDS. 2. A Noisy-Channel approach for Multi-document Sumarization The Noisy-Channel model is represented by a framework composed of three parts: a source, a noisy-channel and a decoder. This structure is showed in Figure 1. Source Noisy-Channel Message X P(x) P(y|x) Message Y P(y) Decoding P(x|y) Figure 1. Noisy-Channel Model 224 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 224–228, c Cuiab´ a, MT, Brazil,"
W13-4012,D08-1079,0,0.117309,"ns and future work. Abstract In this paper, we describe novel methods for topic segmentation based on patterns of discourse organization. Using a corpus of news texts, our results show that it is possible to use discourse features (based on Rhetorical Structure Theory) for topic segmentation and that we outperform some well-known methods. 1 Introduction Topic segmentation aims at finding the boundaries among topic blocks in a text (Chang and Lee, 2003). This task is useful for a number of important applications such as information retrieval (Prince and Labadié, 2007), automatic summarization (Wan, 2008) and questionanswering systems (Oh et al., 2007). In this paper, following Hearst (1997), we assume that a text or a set of texts develop a main topic, exposing several subtopics as well. We also assume that a topic is a particular subject that we write about or discuss (Hovy, 2009), and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). Therefore, the task of topic segmentation aims at dividing a text into topically coherent segments, or subtopics. The granularity of a subtopic is not defined, as a subtopic may contain one"
W13-4012,W13-4806,1,0.764403,"Missing"
W13-4012,Y03-1018,0,0.0184568,"r automatic strategies to find the subtopics. The corpus that we use is described in Section 4. Section 5 presents some results and Section 6 contains the conclusions and future work. Abstract In this paper, we describe novel methods for topic segmentation based on patterns of discourse organization. Using a corpus of news texts, our results show that it is possible to use discourse features (based on Rhetorical Structure Theory) for topic segmentation and that we outperform some well-known methods. 1 Introduction Topic segmentation aims at finding the boundaries among topic blocks in a text (Chang and Lee, 2003). This task is useful for a number of important applications such as information retrieval (Prince and Labadié, 2007), automatic summarization (Wan, 2008) and questionanswering systems (Oh et al., 2007). In this paper, following Hearst (1997), we assume that a text or a set of texts develop a main topic, exposing several subtopics as well. We also assume that a topic is a particular subject that we write about or discuss (Hovy, 2009), and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). Therefore, the task of topic segment"
W13-4012,J97-1003,0,0.662922,"ntation based on patterns of discourse organization. Using a corpus of news texts, our results show that it is possible to use discourse features (based on Rhetorical Structure Theory) for topic segmentation and that we outperform some well-known methods. 1 Introduction Topic segmentation aims at finding the boundaries among topic blocks in a text (Chang and Lee, 2003). This task is useful for a number of important applications such as information retrieval (Prince and Labadié, 2007), automatic summarization (Wan, 2008) and questionanswering systems (Oh et al., 2007). In this paper, following Hearst (1997), we assume that a text or a set of texts develop a main topic, exposing several subtopics as well. We also assume that a topic is a particular subject that we write about or discuss (Hovy, 2009), and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). Therefore, the task of topic segmentation aims at dividing a text into topically coherent segments, or subtopics. The granularity of a subtopic is not defined, as a subtopic may contain one or more sentences or paragraphs. Several methods have been tested for topic segmentation"
W13-4012,R09-1028,0,0.152816,"among topic blocks in a text (Chang and Lee, 2003). This task is useful for a number of important applications such as information retrieval (Prince and Labadié, 2007), automatic summarization (Wan, 2008) and questionanswering systems (Oh et al., 2007). In this paper, following Hearst (1997), we assume that a text or a set of texts develop a main topic, exposing several subtopics as well. We also assume that a topic is a particular subject that we write about or discuss (Hovy, 2009), and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). Therefore, the task of topic segmentation aims at dividing a text into topically coherent segments, or subtopics. The granularity of a subtopic is not defined, as a subtopic may contain one or more sentences or paragraphs. Several methods have been tested for topic segmentation. There are, however, no studies on how discourse structure directly mirrors topic boundaries in texts and how they may contribute to such task, although such possible correlation has been suggested (e.g., Hovy and Lin, 1998). 2 Related work Several approaches have tried to measure the similarity across sentences and t"
W13-4012,X98-1026,0,0.0737299,"pics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). Therefore, the task of topic segmentation aims at dividing a text into topically coherent segments, or subtopics. The granularity of a subtopic is not defined, as a subtopic may contain one or more sentences or paragraphs. Several methods have been tested for topic segmentation. There are, however, no studies on how discourse structure directly mirrors topic boundaries in texts and how they may contribute to such task, although such possible correlation has been suggested (e.g., Hovy and Lin, 1998). 2 Related work Several approaches have tried to measure the similarity across sentences and to estimate where topic boundaries occur. One well-known approach, that is heavily used for topic segmentation, is TextTiling (Hearst, 1997), which is based on lexical cohesion. For this strategy, it is assumed that a set of lexical items is used during the development of a subtopic in a text and, when that subtopic changes, a significant proportion of vocabulary also changes. Passoneau and Litman (1997), in turn, have combined multiple linguistic features for topic segmentation of spoken text, such a"
W13-4012,N12-1022,0,0.0424955,"Missing"
W13-4012,J97-1005,0,0.211088,"Missing"
W13-4806,J96-2004,0,0.268848,"termine possible subtopic boundaries, but the task is very subjective and levels of agreement among humans tend to be low (see, e.g., Hearst, 1997; Passonneau and Litman, 1997; Sitbon and Bellot, 2006; Kazantseva and Szpakowicz, 2012). Agreement also varies depending on the text genre/type that is segmented. For example, technical reports have headings and subheadings, while other genres, such as news texts, have little demarcation. The quality of annotation may refer to the agreement and consistency with which it is applied. As adopted by Hearst (1997), we used the traditional kappa measure (Carletta, 1996), which is better than simple percent agreement measures because it subtracts from the counts the expected chance agreement among judges. The kappa measure produces results up to 1, when agreement is perfect. It is assumed in the area that a 0.60 value is enough to the annotation to be reliable and conclusions may be drawn; however, it is known that such value highly depends on the subjectivity of the task at hand. In our case, we expect a lower value, since determining subtopics boundaries is a difficult task. From left to right, Table 1 shows the days of annotation, the groups of annotators"
W13-4806,Y03-1018,0,0.607875,"as a major shift by one annotator and as a minor shift by other, suggesting low agreement. Mohri et al. (2010) used 447 news of the TDT corpus of broadcast news speech and newspaper articles. The corpus has human-labeled story boundaries treated as topic boundaries. Other researchers automatically produce reference segmentation. Choi (2000) produced an artificial test corpus of 700 documents from the Brown corpus. For document generation, the procedure consists extracting, for instance, ten segments of 311 sentences each, taken from different documents and combining them to form one document. Chang and Lee (2003), in turn, collected 1285 writings to be segmented into subtopics using their method for topic segmentation. It is interesting to notice how varied the annotation procedures in the above works were. This is expected, since corpora are created for several different (linguistic and computational) purposes. However, corpus annotation practices have evolved with time and some basic steps are expected to be followed in the research. As already cited in the first section, Hovy and Lavid (2010) split the corpus annotation in seven steps. The authors claim that it is necessary to follow these steps in"
W13-4806,A00-2004,0,0.0373345,"from the ISL Meeting Corpus (Burger et al., 2002). The authors asked to two annotators to segment the texts at two levels: major and minor, corresponding to the more and less important topic shifts. The authors noticed many cases where topic boundaries were annotated as a major shift by one annotator and as a minor shift by other, suggesting low agreement. Mohri et al. (2010) used 447 news of the TDT corpus of broadcast news speech and newspaper articles. The corpus has human-labeled story boundaries treated as topic boundaries. Other researchers automatically produce reference segmentation. Choi (2000) produced an artificial test corpus of 700 documents from the Brown corpus. For document generation, the procedure consists extracting, for instance, ten segments of 311 sentences each, taken from different documents and combining them to form one document. Chang and Lee (2003), in turn, collected 1285 writings to be segmented into subtopics using their method for topic segmentation. It is interesting to notice how varied the annotation procedures in the above works were. This is expected, since corpora are created for several different (linguistic and computational) purposes. However, corpus"
W13-4806,P03-1071,0,0.0469618,"the annotator provided a brief onesentence description, effectively creating a chapter outline. In these two studies, texts were segmented based on paragraph boundaries. The agreement among annotators was 0.64 in Hearst’s annotation and 0.29 in Kazantseva and Szpakowicz’s study. Passonneau and Litman (1997) used a corpus composed of 20 transcribed narratives about a movie to be segmented by seven untrained annotators. The size of the narratives was roughly 13,500 words. The authors requested judges to mark boundaries using their notion of communicative intention as the segmentation criterion. Galley et al. (2003) worked on a sample of 25 meetings transcribed from the ICSI Meeting corpus (Janin et al., 2003). They had at least three human judges to mark each speaker change (which is a potential boundary) as either a boundary or non-boundary. The final segmentation was based on the opinion of the majority. Gruenstein et al. (2007) used 40 meetings from the same corpus and 16 additional ones from the ISL Meeting Corpus (Burger et al., 2002). The authors asked to two annotators to segment the texts at two levels: major and minor, corresponding to the more and less important topic shifts. The authors notic"
W13-4806,J97-1003,0,0.662156,"ions. Section 2 shows a brief discussion about relevant issues related to corpus annotation and some work on subtopic segmentation. Section 3 describes our annotation under the light of the 7 annotation questions, including the description of our dataset and the quality evaluation of the subtopic segmentation. Section 4 presents final remarks. 2. Related work There are several initiatives to create corpora that are linguistically annotated with varied phenomena from diverse perspectives, both for written and for spoken/transcribed data. We briefly overview some of these works in what follows. Hearst (1997) was one of the pioneer works in the area of subtopic segmentation with the proposal of the TextTiling algorithm. The author used a corpus of 12 magazine (expository) articles that had their subtopics segmented by technical researchers. The size of the texts varied from 1,800 to 2,500 words. In order to produce a reference 1 CSTNews corpus - http://www2.icmc.usp.br/~taspardo/sucinto/cstnews.html 50 segmentation, Hearst considered that a boundary was true if at least three out of the seven enrolled judges placed a boundary mark there. Kazantseva and Szpakowicz (2012), in turn, chose a fiction b"
W13-4806,R09-1028,0,0.232407,"is subtopic segmentation in news texts. Koch (2009) describes that a text can be considered coherent if it displays continuity, i.e., the topical progression must take place so that there are no breaks or interruptions overly long on topic in progress. In this work, following Hearst (1997), we assume that a text or a set of texts develop a main topic, exposing several subtopics as well. We also assume that a topic is a particular subject that we write about or discuss (Hovy, 2009), and subtopics 51 are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Hennig, 2009). News texts usually do not have explicit marking of subtopics; however, the topicality exists as an organizing principle of the text. As an example, Figure 1 shows a short text (translated from one of the texts in CSTNews corpus) with sentences identified by numbers between square brackets and a possible segmentation. We also show the identification of each subtopic in angle brackets after the corresponding text passages. The main topic is a plane crash. The first text block is about the victims and where the plane was; the second block describes the plane; and the last one describes the crew"
W13-4806,N12-1022,0,0.479401,"overview some of these works in what follows. Hearst (1997) was one of the pioneer works in the area of subtopic segmentation with the proposal of the TextTiling algorithm. The author used a corpus of 12 magazine (expository) articles that had their subtopics segmented by technical researchers. The size of the texts varied from 1,800 to 2,500 words. In order to produce a reference 1 CSTNews corpus - http://www2.icmc.usp.br/~taspardo/sucinto/cstnews.html 50 segmentation, Hearst considered that a boundary was true if at least three out of the seven enrolled judges placed a boundary mark there. Kazantseva and Szpakowicz (2012), in turn, chose a fiction book (with 20 chapters) to be segmented by at least six undergraduate students. For each topic boundary, the annotator provided a brief onesentence description, effectively creating a chapter outline. In these two studies, texts were segmented based on paragraph boundaries. The agreement among annotators was 0.64 in Hearst’s annotation and 0.29 in Kazantseva and Szpakowicz’s study. Passonneau and Litman (1997) used a corpus composed of 20 transcribed narratives about a movie to be segmented by seven untrained annotators. The size of the narratives was roughly 13,500"
W13-4806,J97-1005,0,0.546227,"o/cstnews.html 50 segmentation, Hearst considered that a boundary was true if at least three out of the seven enrolled judges placed a boundary mark there. Kazantseva and Szpakowicz (2012), in turn, chose a fiction book (with 20 chapters) to be segmented by at least six undergraduate students. For each topic boundary, the annotator provided a brief onesentence description, effectively creating a chapter outline. In these two studies, texts were segmented based on paragraph boundaries. The agreement among annotators was 0.64 in Hearst’s annotation and 0.29 in Kazantseva and Szpakowicz’s study. Passonneau and Litman (1997) used a corpus composed of 20 transcribed narratives about a movie to be segmented by seven untrained annotators. The size of the narratives was roughly 13,500 words. The authors requested judges to mark boundaries using their notion of communicative intention as the segmentation criterion. Galley et al. (2003) worked on a sample of 25 meetings transcribed from the ICSI Meeting corpus (Janin et al., 2003). They had at least three human judges to mark each speaker change (which is a potential boundary) as either a boundary or non-boundary. The final segmentation was based on the opinion of the"
W13-4806,W12-3307,0,0.0158179,"port sources, the crew members were Russian. <subtopic: details about the flight crew> Figure 1. Example of a text with identified subtopics It is usual to find subtopics that are repeated later on in the same text, and should therefore be connected, and marked with the same label. Another important matter is that the granularity of a subtopic is not defined, as a subtopic may contain one or more sentences or paragraphs. Some researchers use paragraphs as the basic information unit (e.g., Hearst, 1997; Kazantseva and Szpakowicz, 2012), while others employ sentences (e.g., Chang and Lee, 2003; Riedl and Biemann, 2012). We also do not distinguish among the notions of subtopic shift and subtopic drift, as proposed by Carlson and Marcu (2001). According to Carlson and Marcu, a subtopic shift is a sharp change in focus, while a drift is a smooth change from the information presented in the first span to the information presented in the second. In our annotation, they were both classified as a change in subtopic. 3.2. Selecting the corpus We used a corpus composed of 50 clusters of news texts written in Brazilian Portuguese, collected from several sections of mainstream news agencies in Brazil: Politics, Sports"
W13-4806,sitbon-bellot-2006-tools,0,0.0230548,"he encoding (since different editors and operating systems may use varied encoding standards, as Unicode and UTF-8), to make it uniform. 3.6. Choosing and applying evaluation measures The underlying premise of an annotation is that if people cannot agree enough, then either the theory is wrong (or badly stated or instantiated), or the process itself is flawed (Hovy and Lavid, 2010). At first, it may seem intuitive to determine possible subtopic boundaries, but the task is very subjective and levels of agreement among humans tend to be low (see, e.g., Hearst, 1997; Passonneau and Litman, 1997; Sitbon and Bellot, 2006; Kazantseva and Szpakowicz, 2012). Agreement also varies depending on the text genre/type that is segmented. For example, technical reports have headings and subheadings, while other genres, such as news texts, have little demarcation. The quality of annotation may refer to the agreement and consistency with which it is applied. As adopted by Hearst (1997), we used the traditional kappa measure (Carletta, 1996), which is better than simple percent agreement measures because it subtracts from the counts the expected chance agreement among judges. The kappa measure produces results up to 1, whe"
W13-4806,D08-1079,0,0.169697,"58, c Fortaleza, CE, Brazil, October 21–23, 2013. 2013 Sociedade Brasileira de Computa¸ca ˜o that information retrieval with the identification of subtopics in the retrieved texts may provide the user with text fragments that are semantically and topically related to a given query. This makes it easier for the user to quickly find the information of interest. Oh et al. (2007) suggest that a question answering system, which aims to answer a question/query submitted by the user, may link this query to the subtopics in a text in order to increase the accuracy of the identification of the answer. Wan (2008) says that, given some subtopic segmentation, automatic summarization may produce summaries that select different aspects from the collection of texts, producing better summaries. Given its usefulness, it is common to prepare a reference segmentation that supports not only the study and understanding of the phenomenon, but also the development and evaluation of systems for automatic subtopic segmentation. As the construction of corpora is a time consuming and very expensive task, it is necessary to follow procedures to systematize it and to ensure a reliable annotation, in order to produce a s"
W13-4815,E09-1005,0,0.014081,"el para o PT-BR por meio do método multilíngue com recursos de tradução automática proposto por [Nóbrega and Pardo 2012]. Normalmente, métodos de DLS empregam informações de contexto local, palavras que estão em torno da PA, para determinar o sentido mais adequado. Contudo, diversas aplicações do PLN inserem-se no cenário multidocumento, no qual a computação ocorre sobre uma coleção de textos, tais como Sumarização Multidocumento, Agrupamento de Textos, Recuperação de Informação, etc. No cenário supracitado, embora alguns trabalhos de DLS possibilitem empregar informação multidocumento, como [Agirre and Soroa 2009], que fazem uso de grafos para representar os contextos das palavras por meio de relações entre seus respectivos sentidos, não há relato de experimentos ou análises de DLS nesse cenário. Neste trabalho, a fim de investigar a DLS no cenário multidocumento e direcionada ao PT-BR, dada a carência de trabalhos de DLS nessas áreas, apresentamse dois métodos de DLS para substantivos comuns que empregam o método de recuperação de synsets da Wn-Pr para palavras de idiomas diferentes do inglês proposto por [Nóbrega and Pardo 2012]. O primeiro método é uma adaptação do algoritmo de [Mihalcea and Moldov"
W13-4815,J96-2004,0,0.116335,"Missing"
W13-4815,kilgarriff-rosenzweig-2000-english,0,0.435672,"lho de [Nóbrega and Pardo 2012]. Por fim, a conclusão deste trabalho é apresentada na Seção 6. 2. Trabalhos Relacionados O algoritmo de [Lesk 1986] atribui rótulos às palavras no contexto e adota que o sentido mais adequado para uma PA é aquele mais similar com esses rótulos. Um rótulo, dada uma palavra e suas possíveis definições em um dicionário, é um conjunto lexical extraído dessas definições. A métrica de similaridade utilizada é quantificada pelo número de palavras sobrepostas, ou seja, número de palavras presentes na definição dos sentidos e nos rótulos das palavras no contexto da PA. [Kilgarriff et al. 2000] descrevem uma simplificação desse algoritmo, a fim de diminuir seu tempo de execução computacional, e [Banerjee 2002] apresentam a adaptações, na proposta original de Lesk, para utilização da Wn-Pr. [Mihalcea and Moldovan 1999] fazem uso da Wn-Pr como RS e apresentam um algoritmo de desambiguação pautado em pares de palavras, sendo uma destas a PA e a 139 outra, uma palavra do contexto (palavra-contexto). Nesse algoritmo, a PA é desambiguada com o synset que ocorre mais vezes com a palavra-contexto. Para tanto, por meio de padrões de queries (chaves de consulta) entre as palavras do conjunto"
W13-4815,P99-1020,0,0.57837,"irre and Soroa 2009], que fazem uso de grafos para representar os contextos das palavras por meio de relações entre seus respectivos sentidos, não há relato de experimentos ou análises de DLS nesse cenário. Neste trabalho, a fim de investigar a DLS no cenário multidocumento e direcionada ao PT-BR, dada a carência de trabalhos de DLS nessas áreas, apresentamse dois métodos de DLS para substantivos comuns que empregam o método de recuperação de synsets da Wn-Pr para palavras de idiomas diferentes do inglês proposto por [Nóbrega and Pardo 2012]. O primeiro método é uma adaptação do algoritmo de [Mihalcea and Moldovan 1999], que, após experimentos, mostra-se eficaz na desambiguação de palavras consideradas mais ambíguas. O segundo é uma adaptação para aumentar a eficiência e desempenho do melhor método de [Nóbrega and Pardo 2012] no cenário multidocumento. Este artigo está dividido em mais 5 seções. Na Seção 2, disserta-se sobre trabalhos de DLS relacionados. A análise de córpus, que direcionou e proporcionou a avaliação dos métodos propostos, será apresentada na Seção 3. Os métodos desenvolvidos, bem como os artefatos técnicos e teóricos, serão descritos na Seção 4. Na Seção 5 são discutidas as avaliações dos"
W13-4829,P11-1032,0,0.0211172,"d in this evaluation are posemo (12,878 entries), which stands for positive emotion, and negemo (15,115 entries), which stands for negative emotion. Other categories would also be useful (e.g., affect, anger, sad, etc), however, we decided it would be a fair comparison against the other lexicons if we used only these two. The English version for LIWC dictionary has been used for a number of relevant works in sentiment analysis. For example, SentiStrength [Thelwall et al. 2010] uses LIWC dictionary for building its internal word list, which is the core of this sentiment classifier; Ott et al. [Ott et al. 2011] uses it to identify fictitious opinions that have been deliberately written to sound authentic; Kim et al. [Kim et al. 2012] uses it to classify anonymous texts. 2.2. OpinionLexicon The OpinionLexicon [Souza et al. 2011] is a dictionary built for sentiment analysis task. To construct this resource, the authors applied three methods from the literature: a corpusbased, a thesaurus-based and an automatic translation system. The lexicon in the version 2.1 is composed of 30,678 entries (30,236 words and 442 phrases). Opinion Lexicon was used by Souza and Vieira [Souza and Vieira 2012] for twitter"
W13-4829,W11-4507,0,0.0114689,"however, we decided it would be a fair comparison against the other lexicons if we used only these two. The English version for LIWC dictionary has been used for a number of relevant works in sentiment analysis. For example, SentiStrength [Thelwall et al. 2010] uses LIWC dictionary for building its internal word list, which is the core of this sentiment classifier; Ott et al. [Ott et al. 2011] uses it to identify fictitious opinions that have been deliberately written to sound authentic; Kim et al. [Kim et al. 2012] uses it to classify anonymous texts. 2.2. OpinionLexicon The OpinionLexicon [Souza et al. 2011] is a dictionary built for sentiment analysis task. To construct this resource, the authors applied three methods from the literature: a corpusbased, a thesaurus-based and an automatic translation system. The lexicon in the version 2.1 is composed of 30,678 entries (30,236 words and 442 phrases). Opinion Lexicon was used by Souza and Vieira [Souza and Vieira 2012] for twitter sentiment analysis; and by Ribeiro Junior et al. [Ribeiro Junior et al. 2012] to assess vehicle features in blogs. 2.3. SentiLex SentiLex [Silva et al. 2012] is a lexicon constructed for social judgments domain. The lexi"
W13-4829,J11-2001,0,0.0602472,". The core of this program is a lexicon resource, best known as LIWC dictionary, which recently has been made available for Portuguese Language 1 . Sentiment analysis, or opinion mining, is a relatively new topic of research in natural language processing that has gained lots of attention due to the growth of the social web. A common task in sentiment analysis is text classification. In this task, a text, sentence or piece of opinion may be classified as positive, negative or neutral. Sentiment classification is commonly categorized in two basic approaches: machine learning and lexicon-based [Taboada et al. 2011]. Machine learning approach uses a set of features, usually the vocabulary, which are learned from annotated corpora or labelled examples. The lexicon-based approach uses a lexicon to provide the polarity, or semantic orientation, for each word or phrase in the text. This last approach does not require an annotated corpora, and it is known for its domain independence, while the machine learning approach tends to adapt to the domain the classifier was trained [Aue and Gamon 2005]. The main component for the lexicon-based sentiment classifier is the lexicon resource, which needs to be precise a"
W14-0404,P06-2005,0,0.105193,"pose some procedures to minimize them. 1. Introduction Corpus normalization has become a common challenge for everyone interested in processing a web corpus. Some normalization tasks are language and genre independent, like boilerplate removal and deduplication of texts. Others, like orthographic errors correction and internet slang handling, are not. Two approaches to web corpus normalization have been discussed in Web as a Corpus (WAC) literature. One of them is to tackle the task as a translation problem, being the web texts the source language and the normalized texts the target language (Aw et al., 2006; Contractor et al., 2010; Schlippe et al., 2013). Such approach requires a parallel corpus of original and normalized texts of reasonable size for training a system with acceptable accuracy. The other approach is to tackle the problem as a number of sub problems to be solved in sequence 22 Felix Bildhauer & Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 22–28, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics keeping the corrected form as an additional annotation layer, may be the best solution. 2. Related Work"
W14-0404,cardoso-2012-rembrandt,0,0.0133997,"Entities from spelling corrections because when nonrecognized lowercase words are checked by spellers, there is the risk of wrong correction. Indeed, the more extensive is the speller lexicon, the greater is the risk of miscorrection. The genre under inspection presents a widespread misuse of case. By one side, lower case is used in place of uppercase in the initial letter of proper names. On the other side, upper case is used to emphasize any kind of word. Our first tentative to tackle the problem of capitalization was to submit the samples to a Named Entity Recognizer. We chose Rembrandt2 (Cardoso, 2012), a Portuguese NER that enhances both lexical knowledge extracted from Wikipedia and statistical knowledge. The procedure was: 1) to submit the sample to Rembrandt; 2) to capitalize the recognized entities written in lower case; 3) to change all the words capitalized, except the named entities, to lower case. Then we tagged the sample with MXPOST to evaluate the effect on POS tagging accuracy. The number of errors of POS tagging increased (149) when compared to the one of the sample without preprocessing (138). The Table 3. Percentage of case use in newspaper and products reviews corpus genres"
W14-0404,J96-2004,0,0.20529,"Missing"
W14-0404,hartmann-etal-2014-large,1,0.841051,"Missing"
W14-0404,P03-1020,0,0.0427581,"needed. Furthermore, the processing of a new genre is an opportunity not only to make genre-adaptation, but also to improve general purpose features of NLP tools. used as feature by Named Entities Recognizers (NER), POS taggers and parsers. To evaluate whether the case use distribution is different from that of a corpus of well written texts, we compared the statistics of case use in our corpus with those of a newspaper corpus (http://www.linguateca.pt/CETENFolha/), as shown in Table 3. 6.1 Case normalization: truecasing In NLP the problem of case normalization is usually called “truecasing” (Lita et al, 2003, Manning et al., 2008). The challenge is to decide when uppercase should be changed into lower case and when lower case should be changed into upper case. In brief, truecasing is the process of correcting case use in badly-cased or non-cased text. The problem is particularly relevant in two scenarios; speech recognition and informal web texts. We prioritized the case normalization for two reasons: first, badly-cased text seems to be a generalized problem in the genre of products reviews and, second, it is important to make case normalization before using a spell checker. This is crucial to “p"
W14-0404,J06-3001,0,0.617775,"Missing"
W14-0404,C10-2022,0,\N,Missing
W15-1607,E06-1039,0,0.47984,"Missing"
W15-1607,J96-2004,0,0.700186,"Missing"
W15-1607,C10-1039,0,0.135436,"Missing"
W15-1607,D14-1168,0,0.0465664,"Missing"
W15-1607,C00-1072,0,0.178681,"Missing"
W15-1607,W08-0112,0,0.0426043,"Missing"
W15-1607,J98-3005,0,0.352247,"Missing"
W15-1607,radev-etal-2004-mead,0,0.14731,"Missing"
W15-1607,J11-2001,0,0.0346847,"Missing"
W15-1607,Y10-1079,0,0.145908,"Missing"
W15-1607,W13-4829,1,\N,Missing
W15-4608,C14-1089,0,0.152061,"f the text for it to make sense. It also involves aspects that are out 60 Proceedings of the SIGDIAL 2015 Conference, pages 60–67, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics and establishes relations among different textual segments. In particular, this work is based on the following assumptions: (i) there are transition patterns of discursive relations (CST and RST) in locally coherent summaries; (ii) and coherent summaries show certain distinct intra- and interdiscursive relation organization (Lin et al., 2011), (Castro Jorge et al., 2014), (Feng et al., 2014). The model we propose aims at incorporating such issues, learning summary discourse organization preferences from corpus. This paper is organized as follows: in Section 2, it is presented an overview of the most relevant researches related to local coherence; Section 3 details the proposed approach in this paper; Section 4 shows the experimental setup and the obtained results; finally, Section 5 presents some final remarks. 1 (The Justice Department)S is conducting an (anti-trust trial)O against (Microsoft Corp.)X with (evidence)X that (the company)S is increasingly attempting to crush (compe"
W15-4608,J08-1001,0,0.518974,"erence, it is possible to recognize different terms that refer to the same entity in the texts (resulting, therefore, in only one column in the grid). Syntax provides the functions of the entities; if not used, the grid only indicates if an entity occurs or not in each sentence; if salience is used, different grids are produced for more frequent and less frequent entities. It is important to notice that any combination of these features may be used. Lin et al. (2011) assumed that local coherence implicitly favors certain types of discursive relation transitions. Based on the Entity Model from Barzilay and Lapata (2008), the authors used terms instead of entities and discursive information instead of syntactic information. The terms are the stemmed forms of open class words: nouns, verbs, adjectives and adverbs. The discursive relations used in this work came from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The authors developed the Discursive Grid, which is composed of sentences (rows) and terms (columns) with discursive relations used over their arguments. For example, part of the discursive grid (b) for a text (a) is shown in Figure 3. authors proposed to represent entities in a graph and th"
W15-4608,J95-2003,0,0.76727,"Missing"
W15-4608,P13-1010,0,0.0175255,"ure 9, the cells in gray do not characterize a valid transition (since only the superior diagonal of the grid is necessary in this model). The probabilities of relations present in the transitions are calculated as the ratio between the frequency of a specific relation in the grid and the total number of valid transitions between two sentences. For instance, the probability of the RST relation “elaboration” (i.e., the relation 2 64 www.icmc.usp.br/~taspardo/sucinto/cstnews.html ence: the LSA method of Foltz et al. (1998), the Entity Grid Model of Barzilay and Lapata (2008), the Graph Model of Guinaudeau and Strube (2013), the Shallow RST Model of Feng et al (2014), the RST Model of Dias et al. (2014b) and the Entity-based Model with CST bool of Castro Jorge et al. (2014). The LSA method, Entity Grid, Graph and Shallow RST Models were adapted to Brazilian Portuguese, using the appropriate available tools and resources for this language, as the PALAVRAS parser (Bick, 2000) that was used to identify the summary entities, which are all nouns and proper nouns. The implementation of these methods carefully followed each step of the original ones. Barzilay and Lapata‟s method has been implemented without coreference"
W15-4608,W10-2312,1,0.846135,"among others (Koch and Travaglia, 2002). Textual coherence occurs in local and global levels (Dijk and Kintsch, 1983). Local level coherence is presented by the local relationship among the parts of a text, for instance, sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Psycholinguistics consider that local coherence is essential in order to achieve global coherence (Mckoon, 1992). The main phenomena that affect coherence in multi-document summaries are redundant, complementary and contradictory information (Jorge and Pardo, 2010). These phenomena may occur because the information contained in the summaries possibly come from different sources that narrate the same topic. Thus, a good multidocument summary should a) not contain redundant information, b) properly link and order complementary information, and c) avoid or treat contradictory information. In this context, we present, in this paper, a discourse-based model for capturing the above properties and distinguishing coherent from incoherent (or less coherent) multi-document summaries. Cross-document Structure Theory (CST) (Radev, 2000) and Rhetorical Structure The"
W15-4608,J04-4001,0,0.105567,"Missing"
W15-4608,P11-1100,0,0.103242,"that there are relationships among the elements of the text for it to make sense. It also involves aspects that are out 60 Proceedings of the SIGDIAL 2015 Conference, pages 60–67, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics and establishes relations among different textual segments. In particular, this work is based on the following assumptions: (i) there are transition patterns of discursive relations (CST and RST) in locally coherent summaries; (ii) and coherent summaries show certain distinct intra- and interdiscursive relation organization (Lin et al., 2011), (Castro Jorge et al., 2014), (Feng et al., 2014). The model we propose aims at incorporating such issues, learning summary discourse organization preferences from corpus. This paper is organized as follows: in Section 2, it is presented an overview of the most relevant researches related to local coherence; Section 3 details the proposed approach in this paper; Section 4 shows the experimental setup and the obtained results; finally, Section 5 presents some final remarks. 1 (The Justice Department)S is conducting an (anti-trust trial)O against (Microsoft Corp.)X with (evidence)X that (the co"
W15-4608,W01-0100,0,0.554183,"Missing"
W15-4608,prasad-etal-2008-penn,0,0.0302134,"rids are produced for more frequent and less frequent entities. It is important to notice that any combination of these features may be used. Lin et al. (2011) assumed that local coherence implicitly favors certain types of discursive relation transitions. Based on the Entity Model from Barzilay and Lapata (2008), the authors used terms instead of entities and discursive information instead of syntactic information. The terms are the stemmed forms of open class words: nouns, verbs, adjectives and adverbs. The discursive relations used in this work came from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The authors developed the Discursive Grid, which is composed of sentences (rows) and terms (columns) with discursive relations used over their arguments. For example, part of the discursive grid (b) for a text (a) is shown in Figure 3. authors proposed to represent entities in a graph and then to model local coherence by applying centrality measures to the nodes in the graph. Their main assumption was that this bipartite graph contained the entity transition information needed for the computation of local coherence, thus feature vectors and a learning phase are unnecessary. Figure 4 shows pa"
W15-4608,W00-1009,0,0.1215,"ctory information (Jorge and Pardo, 2010). These phenomena may occur because the information contained in the summaries possibly come from different sources that narrate the same topic. Thus, a good multidocument summary should a) not contain redundant information, b) properly link and order complementary information, and c) avoid or treat contradictory information. In this context, we present, in this paper, a discourse-based model for capturing the above properties and distinguishing coherent from incoherent (or less coherent) multi-document summaries. Cross-document Structure Theory (CST) (Radev, 2000) and Rhetorical Structure Theory (RST) (Mann and Thompson, 1998) relations are used to create the discursive model. RST considers that each text presents an underlying rhetorical structure that allows the recovery of the writer‟s communicative intention. RST relations are structured in the form of a tree, where Elementary Discourse Units (EDUs) are located in the leaves of this tree. CST, in turn, organizes multiple texts on the same topic Abstract Multi-document summarization is a very important area of Natural Language Processing (NLP) nowadays because of the huge amount of data in the web."
W15-5612,W13-4012,1,0.87949,"s. For news texts in Brazilian Portuguese, the state of the art consists in two different summarization approaches of Castro Jorge and Pardo (2010) and Ribaldo (2013). Based on deep knowledge, Castro Jorge and Pardo developed the CSTSumm system that employs CST relations to produce preference-based summaries. Sentences are ranked according to the number of CST relationship they hold. Ribaldo, in turn, took advantage of superficial knowledge and developed a multi-document system, called RSumm, which segments texts into subtopics using TextTiling (an adapted version for Portuguese, described in Cardoso et al., 2013) and group the subtopics using measures of similarity. After clustering, a relationship map is created and the relevant content is selected by the segmented bushy path (Salton et al., 1997). In the segmented bushy path, at least one sentence of each subtopic is selected to compose the summary. As we can see, those works do not combine semantic discourse knowledge such as RST and CST for content selection. In this study, we argue that the semantic discourse knowledge improves the process of MDS. 4. The CSTNews corpus Our main resource is the CSTNews1 corpus (Cardoso et al., 2011), composed of 5"
W15-5612,N09-1041,0,0.0144997,"ecimento semântico-discursivo para selecionar conteúdo produz sumários mais informativos. 1. Introduction Automatic Multi-Document Summarization (MDS) aims at selecting the relevant information from multiple documents on the same topic to produce a summary (Mani, 2001). It has seen increasing attention because it can be useful in a variety of areas, mainly due to help coping with information overload. Two main approaches are generally considered in MDS. The superficial approach uses statistical or some limited linguistic information to build a summary, usually has low cost and is more robust (Haghighi and Vanderwende, 2009; Ribaldo, 2013; Castro Jorge, 2015). The deep approach uses linguistically motivated assumptions and demands high-cost resources, but it produces summaries of higher quality in terms of information, coherence and cohesion (Marcu, 1997; Afantenos et al., 2007; Uzêda et al., 2010; Castro Jorge and Pardo, 2010). However, studies based on superficial or deep knowledge do not deal jointly with relevance of different sentences in a source text, multi-document phenomena and subtopics. 81 Joint semantic discourse models for automatic multi-document summarization In a source text, some sentences are m"
W15-5612,J97-1003,0,0.03007,"oaches that achieve good results use multi-document semantic discourse models (Radev, 2000; Zhang et al., 2002; Castro Jorge and Pardo, 2010; Kumar et al., 2014). However, those works are not concerned about the relevance of sentences in each text together with multi-document phenomena as a human does when writing a summary. Another feature is that each text of a collection develops the main topic, exposing different subtopics as well. A topic is a particular subject that we write about or discuss, and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Salton et al., 1997; Hennig, 2009). For example, a set of news texts related to an earthquake typically contains information about the magnitude of the earthquake, its location, casualties and rescue efforts (Bollegala et al., 2010). There are some proposals that combine the subtopical structure and multi-document relationship (Salton et al., 1997; Wan, 2008; Harabagiu and Lacatusu, 2010) to find important information, but without treating the salience of a sentence in its text. We may say that current strategies for MDS have separately used each of the three criteria of relevance of informa"
W15-5612,R09-1028,0,0.0340246,"e multi-document semantic discourse models (Radev, 2000; Zhang et al., 2002; Castro Jorge and Pardo, 2010; Kumar et al., 2014). However, those works are not concerned about the relevance of sentences in each text together with multi-document phenomena as a human does when writing a summary. Another feature is that each text of a collection develops the main topic, exposing different subtopics as well. A topic is a particular subject that we write about or discuss, and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Salton et al., 1997; Hennig, 2009). For example, a set of news texts related to an earthquake typically contains information about the magnitude of the earthquake, its location, casualties and rescue efforts (Bollegala et al., 2010). There are some proposals that combine the subtopical structure and multi-document relationship (Salton et al., 1997; Wan, 2008; Harabagiu and Lacatusu, 2010) to find important information, but without treating the salience of a sentence in its text. We may say that current strategies for MDS have separately used each of the three criteria of relevance of information, multi-document phenomena and s"
W15-5612,W04-1013,0,0.00443367,"lected without satellite propositions. Using the same rank, we propose a variation called RCT-2, which selects full sentences. Two other variations are the RCT-3 and the RCT-4 methods. For these strategies, the total score for each sentence is similar to the first two, with the difference that the RST score is normalized by the size (height) of its discourse tree. RCT-1 and RCT-3 only select nuclear propositions of the best sentences, while RCT-2 and RCT-4 pick out full sentences. 6. Results and discussion This section presents comparisons of the results over the reference corpus using ROUGE (Lin, 2004), a standard evaluation metric used in text summarization, which produces scores that often correlate quite well with human judgments for ranking systems. This metric computes n-gram overlapping between a human reference and an automatic summary. The methods are compared to CSTSumm (Castro Jorge and Pardo, 2010) and RSumm systems (Ribaldo, 2013), that have used the same corpus as here. In Table 1, it is observed that, in the RST group (lines 9-11), RST-3 method, that selects full sentences, has the best ROUGE evaluation. Since RST-1 and RST-2 select only nuclei, they produce summaries with man"
W15-5612,W10-4327,0,0.0154497,"lying RST-1 method, the score (in bold) of sentences 1 and 2 is 4, and for sentence 3 is 6. Whereas sentence 3 has the higher score, its nuclei are selected to compose a summary. Since RST relations do not indicate if there is redundancy between nodes, we control it using cosine measure (Salton, 1989). A: the nodes are propositions B: the nodes are sentences ELABORATION ELABORATION S N SEQUENCE N 1 4 N N 2 ELABORATION 4 S 3 4 5 3 SEQUENCE S N 3 S N VOLITIONAL-RESULT N 2 1 N 3 2 2 3 1 Figure 2: Example of a discourse tree using RST Because all these scores depend on the length of the document (Louis et al., 2010) and on the number of propositions in a sentence, a rank based on the sum of propositions’ scores may insert discrepancies in the method and does not mirror the relevance of sentences in a multi-document scenario. More than this, as we work on news texts, it is expected that first sentences are more relevant, differently from Figure 2 (part A), where the last sentence was more important than the former. As a solution, we proposed to compute the score for sentences, not for propositions, and to normalize each score by the height of the tree, resulting in a number ranged from 0 to 1. In Figure 2"
W15-5612,W97-0713,0,0.0384396,"ry (Mani, 2001). It has seen increasing attention because it can be useful in a variety of areas, mainly due to help coping with information overload. Two main approaches are generally considered in MDS. The superficial approach uses statistical or some limited linguistic information to build a summary, usually has low cost and is more robust (Haghighi and Vanderwende, 2009; Ribaldo, 2013; Castro Jorge, 2015). The deep approach uses linguistically motivated assumptions and demands high-cost resources, but it produces summaries of higher quality in terms of information, coherence and cohesion (Marcu, 1997; Afantenos et al., 2007; Uzêda et al., 2010; Castro Jorge and Pardo, 2010). However, studies based on superficial or deep knowledge do not deal jointly with relevance of different sentences in a source text, multi-document phenomena and subtopics. 81 Joint semantic discourse models for automatic multi-document summarization In a source text, some sentences are more important than others because of their position in the text or in a rhetorical structure, thus, they cannot be treated uniformly (Wan, 2008). In the case of news texts, it is known that the first or leading paragraph usually expres"
W15-5612,W00-1009,0,0.47283,"st or leading paragraph usually expresses the main fact reported in the news. Therefore, selecting sentences from the beginning of the text could be a good summary (Saggion and Poibeau, 2013). More sophisticated techniques use analysis of the discourse structure of texts for determining the most important sentences (Marcu, 1997; O’Donnell, 1997; Uzêda et al., 2010). In order to deal with multi-document phenomena such as redundant, contradictory and complementary information, that occur in a collection of texts, approaches that achieve good results use multi-document semantic discourse models (Radev, 2000; Zhang et al., 2002; Castro Jorge and Pardo, 2010; Kumar et al., 2014). However, those works are not concerned about the relevance of sentences in each text together with multi-document phenomena as a human does when writing a summary. Another feature is that each text of a collection develops the main topic, exposing different subtopics as well. A topic is a particular subject that we write about or discuss, and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Salton et al., 1997; Hennig, 2009). For example, a set of news texts related"
W15-5612,D08-1079,0,0.0874002,"it produces summaries of higher quality in terms of information, coherence and cohesion (Marcu, 1997; Afantenos et al., 2007; Uzêda et al., 2010; Castro Jorge and Pardo, 2010). However, studies based on superficial or deep knowledge do not deal jointly with relevance of different sentences in a source text, multi-document phenomena and subtopics. 81 Joint semantic discourse models for automatic multi-document summarization In a source text, some sentences are more important than others because of their position in the text or in a rhetorical structure, thus, they cannot be treated uniformly (Wan, 2008). In the case of news texts, it is known that the first or leading paragraph usually expresses the main fact reported in the news. Therefore, selecting sentences from the beginning of the text could be a good summary (Saggion and Poibeau, 2013). More sophisticated techniques use analysis of the discourse structure of texts for determining the most important sentences (Marcu, 1997; O’Donnell, 1997; Uzêda et al., 2010). In order to deal with multi-document phenomena such as redundant, contradictory and complementary information, that occur in a collection of texts, approaches that achieve good r"
W15-5618,W01-0100,0,0.832411,"específicas, revelando estratégias humanas de sumarização, e que essas estratégias produzem resultados iniciais que são competitivos com um sistema do estado da arte para o português. 1. Introduction The increasing of new technologies has had an impact on the amount of available textual information on the web. Consequently, Multi-document Summarization (MDS) appears to be a useful Natural Language Processing (NLP) application to promote quick access to large quantities of information, since it produces a unique summary from a collection or cluster of texts on the same topic or related topics [Mani 2001]. Within a generic perspective, the multi-document summary should ideally contain the most relevant information of the topic that is being discussed in the source texts. Moreover, MDS should not only focus on the extraction of relevant information, but also deal with the multi-document challenges, such as redundant, complementary and contradictory information, different writing styles and varied referential expressions. There are two ways of approaching MDS [Mani 2001]. The superficial/shallow approach uses little linguistic information (or statistics) to build summaries. The deep approach is"
W15-5618,W00-1009,0,0.0404478,"h where each sentence becomes a node and the weighted connections between nodes codify the similarity between the corresponding sentences. A redundant sentence is the one that is strongly connected to other sentences. In deep approaches, semantic-based MDS methods commonly map nouns of the input sentences onto concepts of a hierarchy or ontology, and then select the sentences with the most frequent concepts of the collection to produce the summary (e.g. Lin et al. (2010)). Discourse-based methods take into account discourse relations such as those of the Cross-document Structure Theory (CST) [Radev 2000]. These works represent the input texts in a graph, where each node codifies one sentence and the connections represent the CST relations established among those sentences. For content selection, one method consists in extracting sentences that have more CST connections with other sentences, assuming that they are redundant and, then, more relevant. In this paper, we test features from the above approaches to look for a good summarization strategy. We describe the method used in this work in the next section. 3. Corpus-based Investigation of HMDS strategies The experiments in this work were c"
W15-5619,P05-1018,0,0.0361706,"ionality and acceptability, among others [Kock and Travagila 2002]. Textual coherence occurs in local and global levels [Dijk and Kintsch 1983]. Local level coherence is presented by the local relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently,"
W15-5619,N10-1099,0,0.0573078,"gila 2002]. Textual coherence occurs in local and global levels [Dijk and Kintsch 1983]. Local level coherence is presented by the local relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graph"
W15-5619,P11-2022,0,0.0168214,"ijk and Kintsch 1983]. Local level coherence is presented by the local relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation S"
W15-5619,C14-1089,0,0.0207116,"esented by the local relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation Summary A (coherent summary) Summary B (inc"
W15-5619,W07-2321,0,0.0261559,"relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation Summary A (coherent summary) Summary B (incoherent summary) (S1) In the"
W15-5619,J95-2003,0,0.280336,"umptions hold and that we improve the original results in the area. Section 2 presents an overview of the most relevant researches related to local coherence. In Section 3, the coherence models proposed in this work are described. Section 4 shows the experimental setup and the obtained results. Finally, Section 5 presents some final remarks. 2. Related Work One of the most used local coherence models is the one of Barzilay and Lapata (2008), which proposed an Entity Grid Model to evaluate local coherence, i.e., to classify coherent or incoherent texts. This model is based on Centering Theory [Grosz et al. 1995]; the authors’ hypothesis is that locally coherent texts present certain regularities concerning entity distribution. These regularities are calculated over a matrix (entity grid) in which the rows represent the sentences of the text, and the columns the text entities. Barzilay and Lapata's approach used (+) or not (-) syntactical, coreference and salience information. The syntactical information uses the grammatical function of the entities. For example, in the “Department” column in the entity grid in Figure 2b, it is 152 Enriching entity grids and graphs with discourse relations: the impac"
W15-5619,P13-1010,0,0.0770648,"entences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation Summary A (coherent summary) Summary B (incoherent summary) (S1) In the last five years, astronomers have identified a few dozen objects th"
W15-5619,P11-1100,0,0.0139982,"the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151 Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation Summary A (coherent summary) Summary B (incoherent summary) (S1) In the last five years, astronomers have identified a few dozen objects that are even smaller th"
W15-5619,W00-1009,0,0.103063,"ey do not revolve around a star, they revolve around each other. (S3) The biggest celestial body, whose size is seven times greater than Jupiter, was detected about 400 light years from our solar system. (S4) The extraordinary fact is that it does not revolve around a star, but around another cold body that is twice its size. Figure 1. Examples of coherent (A) and incoherent (B) summaries The discursive information used in this work is related to intra or inter text organization, i.e., the Rhetorical Structure Theory (RST) [Mann and Thompson 1987] and the Crossdocument Structure Theory (CST) [Radev 2000], respectively. RST considers that each text presents an underlying rhetorical structure that allows the recovery of the writer’s communicative intention. RST relations are structured in the form of a tree, where Elementary Discourse Units (EDUs) are located in the leaves of this tree, whereas CST organizes multiple texts on the same topic and establishes relations among different textual segments, forming a graph. Considering that all well-formed and coherent texts have a well-defined discursive organization, this paper shows how discursive information (RST and CST) may improve the accuracy"
W15-5619,N07-1055,0,\N,Missing
W17-6605,W04-1013,0,0.0399237,"n method, incorporating knowledge about the importance of the sentences in the on-line source opinions (in reviews) in order to better select the content to compose the summary. The summarization method is the one proposed by [Condori and Pardo 2017], which has already outperformed other well-known aspect-based methods in the area. The sentence importance assessment is carried out by the TOP(X) method [de Sousa et al. 2015]. We evaluate the enriched summarization method on part of a corpus related to electronic products, and measure summary informativeness using the traditional ROUGE measure [Lin 2004]. Our results show that our enriched summarization method significantly outperforms the original method. The remaining of this paper is organized as follows. Section 2 briefly introduces some related work and the original summarization method of [Condori and Pardo 2017]. In Section 3, we introduce the TOP(X) method, which was used to enrich the summarization method. In Section 4, we present the corpus used in our evaluation. Section 5 reports the achieved results. Finally, some conclusions are presented in Section 6. 32 Improving Opinion Summarization by Assessing Sentence Importance in On-li"
W17-6605,W15-1607,1,0.890833,"Missing"
W18-3608,W17-3531,0,0.0370166,"Missing"
W18-3608,P17-4012,0,0.037573,"derstanding of the sentence (saving relations in two directions). In the case of the Decoder, we used two layers and the attention mechanism proposed by Bahdanau et al. (2014) in order to consider all words in the contexts (due to the unordered words). This proposal was similar to the recurrent neural network language model proposed in (Hasler et al., 2017). Finally, we used a Adam Optimizer with a initial learning rate of 0.001, a dropout value of 0.3, 500 hidden units, 15 epochs, and, for the generation of the sequence, we applied beam search of size 10. Let us mention that we used OpenNMT (Klein et al., 2017) to train our model. These parameters were effective during the training, excepting the number of epochs because we did not try other settings. Figure 2: Sub-tree of the sentence that includes “term”, “for”, “judge”, “year”, and “a” One problem related to the training dataset generation was the possibility of the sub-tree’s elements to appear in different ordering in the CoNLL format. This would produce different instances, as we build the samples by breadth search in a sub-tree. Thus, we could get the sample “term|NOUN|root for|ADP|case judge|NOUN|nmod year|NOUN|compound a|DET|det” or “term|N"
W18-3608,J15-3005,0,0.156662,"ta to this track is shown in Figure 1. In this example, we may see information about lemmas, grammatical categories, inflection information and dependency relations. Track 1 can be seen as word ordering and inflection generation tasks. Word ordering is a fundamental problem in Natural Language Generation (Reiter and Dale, 2000). This problem have been widely studied, e.g., we may see the works proposed for the Shared Task in Surface Realization (Belz et al., 2011). In relation to this problem, this has been addressed using language modeling (Schmaltz et al., 2016) and syntax-based approaches (Zhang and Clark, 2015). Recently, sequence-to-sequence models have also been used to tackle this problem, obtaining good results (Hasler et al., 2017). In this paper, we present a neural-based method that works at the syntactic level to order the words (which we refer by NILC-SWORNEMO, standing for “Syntax-based Word ORdering using NEural MOdels”, developed by the NILC research group on Computational Linguistics). Additionally, we apply a bottom-up approach to build the sentence and, using language-specific lexicons, we produce the word forms of each lemma in the sentence. Our system is described in Section 2. In S"
W18-3608,2005.mtsummit-papers.11,0,0.125841,"in CoNLL format - “Bush nominated Jennifer M. Anderson for a 15-year term as associate judge of the Superior Court of the District of Columbia, replacing Steffen W. Graae.” 2.1 some conclusions and future work are discussed in Section 4. 2 Data Preparation As we mentioned, we used a neural model to order the words in the syntactic level, and this kind of model requires several instances to learn. Therefore, the first step was to generate and prepare our dataset. The dataset used to train our models was composed by the training dataset provided by the task and a portion of the Europarl corpus (Koehn, 2005), comprising approximately 70,000 sentences for each language (English, Portuguese, and Spanish). As our neural model works on words of a sentence according to their syntactic levels, we had to preprocess the dataset to get the words of each sentence by syntactic level. Thus, we run the UDPipe tool (Straka and Strakov´a, 2017) on the dataset and obtained all the information about lemmas, grammatical categories, and dependency relations. Then, we got all the sub-trees (sub-root and children, only via breadth search) and generated a sequence for each sub-tree. Each sequence was composed by token"
W18-3608,W18-3601,0,0.113226,"described in Section 2. In Section 3, the results of our proposal are presented. Finally, Introduction In recent years, Universal Dependencies1 (UD) have gained interest from many researchers across different areas of Natural Language Processing (NLP). Currently, there are treebanks for about 50 languages that are freely available2 . UD treebanks have already proved useful in the development of multilingual applications, becoming an advantage for developers. Thus, the creation of an application for a specific language may be replicable to other languages. The Surface Realization Shared Task (Mille et al., 2018) aims at continuing with the development of natural language generation methods focused on the surface realization task. In this edition of the task, two tracks were proposed: (1) 1 Available at http://universaldependencies.org/#en Available at https://lindat.mff.cuni.cz/repository/ xmlui/handle/11234/1-1983 2 58 Proceedings of the First Workshop on Multilingual Surface Realisation, pages 58–64 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Figure 1: Unordered sentence in CoNLL format - “Bush nominated Jennifer M. Anderson for a 15-year term as associate"
W18-3608,P02-1040,0,0.104353,"3-gram language model for English (Chelba et al., 2013), Portuguese (Cunha, 2016) and Spanish (Cardellino, 2016) in order to find the correct position of the words and the proper nouns. That motivated us to follow a bottom-up approach to build a sentence. Thus, the joining between two neighbor syntactic levels makes more sense (as analyzing from the lowest levels brings correct expressions like “15 - year” or “as associate judge”, instead of “for a year term judge”). 2.4 3 Results and Analysis The performance of the methods in the Task 1 was computed using the following four metrics: • BLEU (Papineni et al., 2002): precision metric that computes the geometric mean of the n-gram precisions between the generated text and reference texts, adding a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 1, 2, 3, and 4; Inflection Generation • NIST (Doddington, 2002): related n-gram similarity metric weighted in favor of less frequent n-grams, which are taken to be more informative; In order to recover the correct words included in a sentence (and not lemmas), we created a lexicon for each language (English, Portuguese and Spanish). To do this, we ran the UDPipe tool5 o"
W18-3608,D14-1162,0,0.0884544,"al model that we used was a sequenceto-sequence model (Encoder-Decoder) (Sutskever et al., 2014) in which the input was composed by a sequence of tokens in a sub-tree extracted by the syntactic dependency relations (described in Subsection 2.1) and the output was composed by the lemmas of the same sequence in the correct order. In general, each token in the encoder was represented by embeddings composed by the concatenation of the word embedding, the embedding of the grammatical category and the embedding of the dependency relation. We used word embeddings of 300 dimensions provided by GloVe (Pennington et al., 2014) for English3 , Portuguese4 (Hartmann et al., 2017), and Spanish (built over the corpus provided by Cardellino (2016)). In the case of the other features, we used the number of values that they may assume to generate the size of the embedding. The type of cells in the Recurrent Neural Network (RNN) that we used was the Long ShortTerm Memory (LSTM). We used a Bidirectional LSTM (Bi-LSTM) in the Encoder because it could give us a general understanding of the sentence (saving relations in two directions). In the case of the Decoder, we used two layers and the attention mechanism proposed by Bahda"
W18-3608,D16-1255,0,0.0398979,"Missing"
W18-3608,K17-3009,0,0.0383225,"Missing"
W19-4028,W13-0101,0,0.123261,"coverage of the corresponding lexical resource that supports the annotation. 1 Introduction In recent years, there has been renewed interest in the Natural Language Processing (NLP) community in language understanding and dialogue. Thus, the issue of how the semantic content of language should be represented has reentered into the NLP discussion. In this context, several semantic representations, like Universal Networking Language (UNL) (Uchida et al., 1996), the semantic representation used in the Groningen Meaning Bank (Basile et al., 2012), Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), and, more recently, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), have emerged. Abstract Meaning Representation is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph (Banarescu et al., 2013). This representation includes information about se236 Proceedings of the 13th Linguistic Annotation Workshop, pages 236–244 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ∃ w, b, b1: instance(w, want-01) ∧ instance(b, boy) ∧ instance(b1, believe-01) ∧ instance(g, gi"
W19-4028,L18-1157,1,0.709278,"Missing"
W19-4028,W13-2322,0,0.886341,"In recent years, there has been renewed interest in the Natural Language Processing (NLP) community in language understanding and dialogue. Thus, the issue of how the semantic content of language should be represented has reentered into the NLP discussion. In this context, several semantic representations, like Universal Networking Language (UNL) (Uchida et al., 1996), the semantic representation used in the Groningen Meaning Bank (Basile et al., 2012), Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), and, more recently, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), have emerged. Abstract Meaning Representation is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph (Banarescu et al., 2013). This representation includes information about se236 Proceedings of the 13th Linguistic Annotation Workshop, pages 236–244 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ∃ w, b, b1: instance(w, want-01) ∧ instance(b, boy) ∧ instance(b1, believe-01) ∧ instance(g, girl) ∧ ARG0(w, b) ∧ ARG1(w, b1) ∧ ARG0(b1, g) ∧ ARG1(b1, b) (a) Logic 2 (w / want-01 :ARG"
W19-4028,basile-etal-2012-developing,0,0.03036,"enomena to solve. More than this, efforts are necessary to increase the coverage of the corresponding lexical resource that supports the annotation. 1 Introduction In recent years, there has been renewed interest in the Natural Language Processing (NLP) community in language understanding and dialogue. Thus, the issue of how the semantic content of language should be represented has reentered into the NLP discussion. In this context, several semantic representations, like Universal Networking Language (UNL) (Uchida et al., 1996), the semantic representation used in the Groningen Meaning Bank (Basile et al., 2012), Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), and, more recently, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), have emerged. Abstract Meaning Representation is a semantic formalism that aims to encode the meaning of a sentence with a simple representation in the form of a directed rooted graph (Banarescu et al., 2013). This representation includes information about se236 Proceedings of the 13th Linguistic Annotation Workshop, pages 236–244 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ∃ w, b, b1: insta"
W19-4028,J16-3006,0,0.0219548,"ormation. AMR may be represented using logic forms (see (a) in Figure 1), PENMAN notation (see (b) in Figure 1), and graphs (see (c) in Figure 1). AMR has gained relevance in the research community due to its easiness to be read by computers and humans (as it could be represented using graphs or first-order logic, which are representations that are more familiar to computers and humans, respectively), its attempt to abstract away from syntactic idiosyncrasies (making the tasks to focus only on semantic processing) and its wide use of other comprehensive linguistic resources, such as PropBank (Bos, 2016). In relation to its attempt to abstract away from syntactic idiosyncrasies, it may be seen that AMR annotation in Figure 1 could be generated from the sentences “The boy wants the girl to believe him.” and “The boy wants to be believed by the girl.”, which are semantically similar, but with different syntactic realizations. Regarding the use of linguistic resources, AMR annotation in Figure 1 shows information provided by PropBank, as the framesets “want-01” and “believe-01”, and some semantic roles that they require. The available AMR-annotated corpora for English are large, containing appro"
W19-4028,P13-2131,0,0.0734321,"sis for the construction of the previously cited Verbo-Brasil. 238 it is the main verb of the main sentence “Ieltsin adotou outras medidas simb´olicas”). Finally, the relations among all concepts were identified. Similar to the work of Migueles-Abraira et al. (2018), our proposal tried to adapt the AMR guidelines to Brazilian Portuguese, making some modifications on it in order to deal with the specific linguistic phenomena. The general guideline used to annotate a sentence is described as follows: the new verbs were put in a list in order to enrich the repository in the future. Smatch score (Cai and Knight, 2013) was used to calculate the inter-annotator agreement. Unlike the work of Banarescu et al. (2013), which built a gold standard (using the total agreement between the annotators), the way to calculate the inter-annotator agreement consisted in comparing all annotations in an all-against-all configuration, obtaining the average of all inter-annotator agreements. Finally, the annotated versions of the sentences belonging to the agreement sample that were included in the final corpus were chosen by an adjudicator (since that more than one possible annotation exists). • To use the framesets of Verbo"
W19-4028,N18-1104,0,0.409633,"elieved by the girl.”, which are semantically similar, but with different syntactic realizations. Regarding the use of linguistic resources, AMR annotation in Figure 1 shows information provided by PropBank, as the framesets “want-01” and “believe-01”, and some semantic roles that they require. The available AMR-annotated corpora for English are large, containing approximately 39,000 sentences. Some efforts have been performed for using AMR as an interlingua and building corpus for Non-English languages, taking advantage of the alignments and the parallel corpora that exist (Xue et al., 2014; Damonte and Cohen, 2018). Other works tried to adapt the AMR guidelines to other languages (Migueles-Abraira et al., 2018), considering its cross-linguistic potential. It is unnecessary to stress the importance of corpus creation for other languages. Annotated corpora provide qualitative and reusable data for building or improving existing methods and apAbstract Meaning Representation (AMR) is a recent and prominent semantic representation with good acceptance and several applications in the Natural Language Processing area. For English, there is a large annotated corpus (with approximately 39K sentences) that suppor"
W19-4028,duran-aluisio-2012-propbank,0,0.0688997,"Missing"
W19-4028,S15-1026,0,0.44084,"Missing"
W19-4028,L18-1486,0,0.268045,"Missing"
W19-4028,N15-3006,0,0.0757781,"RG0 (g / girl) :ARG1 b)) (b) One of the first works that tried to build an AMRannotated corpus for a Non-English language was proposed by Xue et al. (2014). The main goal of this work was to evaluate the potentiality of AMR to work as an interlingua. In order to achieve this goal, the authors annotated 100 English sentences of the Penn Treebank using AMR and then translated them to Czech and Chinese, which were annotated with AMR as well. Their main finding was that the level of compatibility of AMR between English and Chinese was higher than between English and Czech. In other research line, Vanderwende et al. (2015) proposed an AMR parser to convert Logic Form representations into AMR for English. The authors also built an AMR-annotated corpus for French, German, Spanish, and Japanese. Damonte and Cohen (2018) developed an AMR parser for English and used parallel corpora to learn AMR parsers for Italian, Spanish, German, and Chinese. The main results showed that the new parsers overcame structural differences between the languages. The authors also proposed a method to evaluate the parsers that does not need gold standard data in the target languages. In the case of Spanish, Migueles-Abraira et al. (2018"
W19-4028,xue-etal-2014-interlingua,0,0.498303,"boy wants to be believed by the girl.”, which are semantically similar, but with different syntactic realizations. Regarding the use of linguistic resources, AMR annotation in Figure 1 shows information provided by PropBank, as the framesets “want-01” and “believe-01”, and some semantic roles that they require. The available AMR-annotated corpora for English are large, containing approximately 39,000 sentences. Some efforts have been performed for using AMR as an interlingua and building corpus for Non-English languages, taking advantage of the alignments and the parallel corpora that exist (Xue et al., 2014; Damonte and Cohen, 2018). Other works tried to adapt the AMR guidelines to other languages (Migueles-Abraira et al., 2018), considering its cross-linguistic potential. It is unnecessary to stress the importance of corpus creation for other languages. Annotated corpora provide qualitative and reusable data for building or improving existing methods and apAbstract Meaning Representation (AMR) is a recent and prominent semantic representation with good acceptance and several applications in the Natural Language Processing area. For English, there is a large annotated corpus (with approximately"
