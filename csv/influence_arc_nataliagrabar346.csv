2001.jeptalnrecital-poster.13,P98-1082,0,\N,Missing
2001.jeptalnrecital-poster.13,C98-1079,0,\N,Missing
2001.jeptalnrecital-poster.13,W99-0904,0,\N,Missing
2002.jeptalnrecital-long.3,J95-4004,0,0.0382364,"Missing"
2002.jeptalnrecital-long.3,W98-1504,0,0.0709291,"Missing"
2002.jeptalnrecital-long.3,A97-1016,0,0.0206001,"Missing"
2002.jeptalnrecital-long.3,J93-2006,0,0.0490421,"Missing"
2002.jeptalnrecital-long.3,W02-0304,1,0.778864,"Missing"
2003.jeptalnrecital-long.26,P02-1053,0,0.00601299,"Missing"
2003.jeptalnrecital-long.26,W02-1011,0,\N,Missing
2003.jeptalnrecital-long.27,W99-0904,0,0.265217,"Missing"
2003.jeptalnrecital-long.27,2002.jeptalnrecital-long.22,0,0.0998581,"Missing"
2004.jeptalnrecital-poster.11,W03-1802,0,0.0564785,"Missing"
2004.jeptalnrecital-poster.11,P98-1082,0,0.0642843,"Missing"
2004.jeptalnrecital-poster.11,C92-2082,0,0.0921647,"Missing"
2005.jeptalnrecital-long.9,P98-1082,0,0.078689,"Missing"
2005.jeptalnrecital-long.9,P99-1044,0,0.0496055,"Missing"
2005.jeptalnrecital-long.9,2005.jeptalnrecital-long.30,0,0.0564338,"Missing"
2005.jeptalnrecital-long.9,2003.jeptalnrecital-long.27,1,0.725279,"Missing"
2006.jeptalnrecital-poster.16,P97-1005,0,0.13253,"Missing"
2006.jeptalnrecital-poster.16,matsumoto-tanaka-2002-automatic,0,0.0488467,"Missing"
2007.jeptalnrecital-poster.9,2006.jeptalnrecital-poster.16,1,0.666092,"Missing"
2009.jeptalnrecital-court.31,P98-1082,1,0.804799,"Missing"
2009.jeptalnrecital-court.31,W08-0507,0,0.0226735,"Missing"
2009.jeptalnrecital-court.31,C04-1054,0,0.07182,"Missing"
2014.lilt-11.7,W11-0104,0,0.0262622,"Missing"
2014.lilt-11.7,cartoni-meyer-2012-extracting,0,0.122341,"quantity dimension along a maximum/minimum axis and the semantic values big and small, and (2) a quality dimension along a positive/negative axis and the values good (excess; higher degree) and bad (lack; lower degree). In order to provide corpus-based insights into this semantic categorization, we analyze French evaluative prefixes alongside their English translation equivalents in a parallel corpus. To do so, we focus on periphrastic translations, as they are likely to ‘spell out’ the meaning of the French prefixes. The data used were extracted from the Europarl parallel corpus (Koehn 2005; Cartoni and Meyer 2012). Using a tailormade program, we first aligned the French prefixed words with the corresponding word(s) in English target sentences, before proceeding to the evaluation of the aligned sequences and the manual analysis of the bilingual data. Results confirm that translation data can be used as evidence for semantics in morphological research and help refine existing semantic descriptions of evaluative prefixes. Keywords: contrastive morphology, evaluative prefixation, seman1 Institut libre Marie Haps (Translation - Interpreting), Brussels, Belgium for English Corpus Linguistics, Universit´ e ca"
2014.lilt-11.7,2003.mtsummit-papers.9,0,0.0200314,"by a space, did not function as a prefix but rather as another part of speech (e.g. demi in deux ans et demi (‘two years and half’), micro in intervention hors micro (‘off-microphone intervention’)). The relevant and corrected alignments were then evaluated by means of the BLEU precision measure (Papinemi et al. 2002). This measure is widely used for the evaluation of automatic machine translation results. Quality is considered to be the correspondence between the automatic output and human translation. The measure has often been reported to correlate quite significantly with human judgment (Coughlin 2003). It consists in counting the number of words in the automatically aligned target sequence and in the adjusted target sequence: the percentage of common words between them corresponds to the BLEU measure. The values of the BLEU score range between 1 (perfect alignment) and 0. For example, consider the following cases: . In {mini-trait´e, mini-treaty}, {microclimat, microclimate}, {micro. . . entreprises, micro-companies}, {ultratechnique, ultra-technical}, all the extracted and aligned words are correct, which gives a precision rate of 1; In {ultrasimple, a wholly simple system}, two words (vi"
2014.lilt-11.7,P91-1017,0,0.0729185,"Missing"
2014.lilt-11.7,P02-1033,0,0.156541,"Missing"
2014.lilt-11.7,W02-0808,0,0.127653,"Missing"
2014.lilt-11.7,2005.mtsummit-papers.11,0,0.0687592,"09)): (1) a quantity dimension along a maximum/minimum axis and the semantic values big and small, and (2) a quality dimension along a positive/negative axis and the values good (excess; higher degree) and bad (lack; lower degree). In order to provide corpus-based insights into this semantic categorization, we analyze French evaluative prefixes alongside their English translation equivalents in a parallel corpus. To do so, we focus on periphrastic translations, as they are likely to ‘spell out’ the meaning of the French prefixes. The data used were extracted from the Europarl parallel corpus (Koehn 2005; Cartoni and Meyer 2012). Using a tailormade program, we first aligned the French prefixed words with the corresponding word(s) in English target sentences, before proceeding to the evaluation of the aligned sequences and the manual analysis of the bilingual data. Results confirm that translation data can be used as evidence for semantics in morphological research and help refine existing semantic descriptions of evaluative prefixes. Keywords: contrastive morphology, evaluative prefixation, seman1 Institut libre Marie Haps (Translation - Interpreting), Brussels, Belgium for English Corpus Lin"
2014.lilt-11.7,N03-2016,0,0.0231154,"he alignment rate of our tailor-made program. In the present study, we experimentally relied on a small set of known translation equivalents for some prefixes (a total of six French-English prefix pairs were used). This can be enriched in future work using more prefix pairs or other types of recurring translation equivalents (e.g. adverbs found in frequent periphrastic patterns). We believe that this will improve the coverage of the results generated here. In addition, the study demonstrates that prefixes are useful anchor points for automatic alignment at word level (cf. (Simard et al. 1992; Kondrak et al. 2003)). However, it is important to note that if, unlike French and English, the languages investigated are morphologically distant (e.g. French and Japanese, English and Hungarian), anchor points are less useful as no or little common morphological and semantic regularities can be found. For such pairs of languages, statistical approaches, such as those implemented in GIZA++, would probably perform better. As regards the filtering of words that are not morphologically prefixed, although they formally contain a prefix-like initial string, the manual filtering step will be sped up in future work by"
2014.lilt-11.7,W03-0301,0,0.0504603,"ing segments could not be extracted automatically. The alignments were divided in two subsets and validated by two evaluators working independently and applying the same validation criteria. 2,938 alignments were kept after the validation phase (several words were discarded during this manual filtering, cf. above). The validation (a, b, c, and d types) reveals that the mean BLEU precision rate on the target sequences is 0.76, which is satisfactory in view of comparable automatic alignment tasks. For instance, a similar task was undertaken during the Word Alignment challenge held in 2003 (see (Mihalcea and Pedersen 2003)). On English-French data with null alignments allowed, the precision rates varied between 0.37 and 0.72 when evaluated against the set of sure translations from the reference set. When probable translations from the reference set were also considered, the precision rate reached 0.77. As expected, the performance of the tailor-made program in this study decreases when the prefix is separated by a blank from the corresponding base word (in source or target segments), as well as when the translation of the prefix is non-morphological. After a final deduplication phase, we were left with 1,985 va"
2014.lilt-11.7,P03-1058,0,0.0749788,"Missing"
2014.lilt-11.7,P00-1056,0,0.22279,"lib´eral). The manual processing phase of step 1 consists in filtering the list of the automatically extracted words in order to discard the ones that are clearly not morphologically prefixed, even though formally they contain a prefix-like initial string (e.g. extracteur, maximal, miette). 176 / LiLT volume 11, issue 6 December 2014 During the second step, alignment is performed at word level. The objective here is to detect, in the target sentence, the word (or the segment) that corresponds to the source prefixed word. This task was performed separately with the word-alignment tool GIZA++ (Och and Ney 2000) and with a tailor-made alignment program. We wanted to compare these two tools because, as will be clear below, they rely on two different approaches for the detection of words or segments to be aligned. In this study we used the aligner that offered the best coverage (see section 5.1). GIZA++ applies several statistical alignment models, such as IBM-4, IBM-5 and HMM. The main clues for aligning at word level are provided by the contexts of use of the words. As for the tailor-made program, it relies on lexical information and the presence of cognates. It also applies several heuristics. For i"
2014.lilt-11.7,P02-1040,0,0.0969912,"valuative prefixes in translation / 177 when examining the French prefixed word in context or the English translations (cf. {extra europ´eens, non-European}, {ultrap´eriph´eriques, outermost}). We also weeded out numerous cases where the prefix-like string, which was separated from the following word by a space, did not function as a prefix but rather as another part of speech (e.g. demi in deux ans et demi (‘two years and half’), micro in intervention hors micro (‘off-microphone intervention’)). The relevant and corrected alignments were then evaluated by means of the BLEU precision measure (Papinemi et al. 2002). This measure is widely used for the evaluation of automatic machine translation results. Quality is considered to be the correspondence between the automatic output and human translation. The measure has often been reported to correlate quite significantly with human judgment (Coughlin 2003). It consists in counting the number of words in the automatically aligned target sequence and in the adjusted target sequence: the percentage of common words between them corresponds to the BLEU measure. The values of the BLEU score range between 1 (perfect alignment) and 0. For example, consider the fol"
2014.lilt-11.7,1992.tmi-1.7,0,0.439055,"ly help us improve the alignment rate of our tailor-made program. In the present study, we experimentally relied on a small set of known translation equivalents for some prefixes (a total of six French-English prefix pairs were used). This can be enriched in future work using more prefix pairs or other types of recurring translation equivalents (e.g. adverbs found in frequent periphrastic patterns). We believe that this will improve the coverage of the results generated here. In addition, the study demonstrates that prefixes are useful anchor points for automatic alignment at word level (cf. (Simard et al. 1992; Kondrak et al. 2003)). However, it is important to note that if, unlike French and English, the languages investigated are morphologically distant (e.g. French and Japanese, English and Hungarian), anchor points are less useful as no or little common morphological and semantic regularities can be found. For such pairs of languages, statistical approaches, such as those implemented in GIZA++, would probably perform better. As regards the filtering of words that are not morphologically prefixed, although they formally contain a prefix-like initial string, the manual filtering step will be sped"
2014.lilt-11.7,C04-1192,0,0.0911276,"Missing"
2014.lilt-11.7,I13-1104,0,0.0249308,"Missing"
2014.lilt-11.7,W09-2413,0,\N,Missing
2015.jeptalnrecital-long.16,W03-1802,0,0.0927467,"Missing"
2015.jeptalnrecital-long.16,W07-1007,0,0.374198,"Missing"
2015.jeptalnrecital-long.16,W14-4812,1,0.265192,"Missing"
2015.jeptalnrecital-long.16,S12-1066,0,0.0470717,"Missing"
2015.jeptalnrecital-long.16,S12-1054,0,0.0232111,"Missing"
2015.jeptalnrecital-long.16,2009.jeptalnrecital-court.7,0,0.121054,"Missing"
2015.jeptalnrecital-long.16,S12-1068,0,0.0283365,"Missing"
2015.jeptalnrecital-long.16,D12-1066,0,0.0563253,"Missing"
2015.jeptalnrecital-long.16,S12-1069,0,0.0492536,"Missing"
2015.jeptalnrecital-long.26,P05-1074,0,0.164661,"Missing"
2015.jeptalnrecital-long.26,P01-1008,0,0.224801,"Missing"
2015.jeptalnrecital-long.26,C08-1013,0,0.0470686,"Missing"
2015.jeptalnrecital-long.26,W09-2105,0,0.0188513,"Missing"
2015.jeptalnrecital-long.26,P13-2080,0,0.0613598,"Missing"
2015.jeptalnrecital-long.26,W03-1608,0,0.127638,"Missing"
2015.jeptalnrecital-long.26,N10-1017,0,0.029706,"Missing"
2015.jeptalnrecital-long.26,P10-1052,0,0.0225705,"Missing"
2015.jeptalnrecital-long.26,J10-3003,0,0.0602214,"Missing"
2015.jeptalnrecital-long.26,P00-1056,0,0.379459,"Missing"
2015.jeptalnrecital-long.26,I05-1011,0,0.0732799,"Missing"
2015.jeptalnrecital-long.26,W04-3219,0,0.126457,"Missing"
2015.jeptalnrecital-long.26,I05-5011,0,0.0594898,"Missing"
2015.jeptalnrecital-long.26,P06-2096,0,0.0254875,"Missing"
2016.jeptalnrecital-long.12,2015.jeptalnrecital-long.16,1,0.68239,"Missing"
2016.jeptalnrecital-long.12,2000.eamt-1.1,0,0.231685,"Missing"
2016.jeptalnrecital-long.20,P05-1074,0,0.104517,"Missing"
2016.jeptalnrecital-long.20,P01-1008,0,0.309056,"Missing"
2016.jeptalnrecital-long.20,2004.jeptalnrecital-recital.6,0,0.233922,"Missing"
2016.jeptalnrecital-long.20,J92-4003,0,0.62845,"Missing"
2016.jeptalnrecital-long.20,P13-2080,0,0.0613264,"Missing"
2016.jeptalnrecital-long.20,J10-3003,0,0.0837705,"Missing"
2016.jeptalnrecital-long.20,2009.jeptalnrecital-court.5,0,0.13809,"Missing"
2016.jeptalnrecital-poster.31,2005.mtsummit-papers.11,0,0.00865447,"Missing"
2017.jeptalnrecital-court.5,P10-1133,0,0.0779385,"Missing"
2017.jeptalnrecital-court.5,P10-1052,0,0.0863653,"Missing"
2018.jeptalnrecital-court.16,L16-1320,0,0.0619348,"Missing"
2018.jeptalnrecital-court.24,P16-1047,0,0.0360226,"Missing"
2018.jeptalnrecital-court.24,P14-1007,0,0.0469611,"Missing"
2018.jeptalnrecital-court.24,S12-1041,0,0.060735,"Missing"
2018.jeptalnrecital-long.1,P13-3015,0,0.0677759,"Missing"
2018.jeptalnrecital-long.1,W16-4123,0,0.0633468,"Missing"
2019.jeptalnrecital-court.11,S13-1004,0,0.0660689,"Missing"
2019.jeptalnrecital-court.11,N13-1092,0,0.0750583,"Missing"
2019.jeptalnrecital-court.11,W18-7002,1,0.857696,"Missing"
2019.jeptalnrecital-court.11,P12-1091,0,0.0605889,"Missing"
2019.jeptalnrecital-court.11,D15-1181,0,0.0355065,"Missing"
2019.jeptalnrecital-court.11,2005.mtsummit-papers.11,0,0.285186,"Missing"
2019.jeptalnrecital-court.11,2007.tmi-papers.12,0,0.184495,"Missing"
2019.jeptalnrecital-court.11,S14-2055,0,0.0521663,"Missing"
2019.jeptalnrecital-court.11,N12-1019,0,0.0508666,"Missing"
2019.jeptalnrecital-court.11,W02-1037,0,0.286522,"Missing"
2019.jeptalnrecital-court.11,J05-4003,0,0.256361,"Missing"
2019.jeptalnrecital-court.11,P06-1011,0,0.0987231,"Missing"
2019.jeptalnrecital-court.11,E06-1021,0,0.138738,"Missing"
2019.jeptalnrecital-court.11,L16-1491,0,0.0391638,"Missing"
2019.jeptalnrecital-court.11,W06-1603,0,0.141988,"Missing"
2019.jeptalnrecital-court.11,P13-2125,0,0.0452561,"Missing"
2019.jeptalnrecital-court.11,P15-1150,0,0.0681752,"Missing"
2019.jeptalnrecital-court.11,P03-1010,0,0.236638,"Missing"
2019.jeptalnrecital-court.11,U06-1019,0,0.060352,"Missing"
2019.jeptalnrecital-court.11,U05-1023,0,0.162379,"Missing"
2019.jeptalnrecital-court.11,S14-2044,0,0.0439856,"Missing"
2019.jeptalnrecital-court.11,C10-1152,0,0.0972945,"Missing"
2019.jeptalnrecital-long.5,W18-5614,1,0.847588,"Missing"
2019.jeptalnrecital-long.5,W15-2604,1,0.853082,"Missing"
2019.jeptalnrecital-long.5,X96-1019,0,0.802383,"Missing"
2019.jeptalnrecital-long.5,P10-1052,0,0.116292,"Missing"
2019.jeptalnrecital-long.5,W08-0606,0,0.0536337,"Missing"
2020.bucc-1.7,R19-1020,1,0.893002,"Missing"
2020.bucc-1.7,W09-3102,0,0.0477576,"udiences, so that the delivered information and the phrasing are not identical. More importantly, the order in which the information is delivered is not the same, which means that the document structure cannot be used for assuming where to look for parallel sentences. For our experiments, we took 39 randomly selected document pairs from that corpus and manually annotated them for two types of sentence pairs : Monolingual parallel corpora are useful for a variety of sequence-to-sequence tasks in natural language processing, such as text simplification (Xu et al., 2015), paraphrase acquisition (Deléger and Zweigenbaum, 2009) or style transfer (Jhamtani et al., 2017). In order to build such parallel corpora, the typical approach is to start from comparable corpora and extract sentence pairs that share the same meaning. For instance, the participants of the BUCC 2017 shared task had to address this problem using bilingual corpora (Zweigenbaum et al., 2017). One major obstacle is that, when considering two documents A and B, every single sentence from A has to be evaluated against every single sentence of B, when document metadata cannot be used to make assumptions as to where to look for corresponding sentences. Th"
2020.bucc-1.7,W18-7002,1,0.832648,"ach of the 39 document pairs, and ended up with 1,164,407 sentence pairs. Thus, given that, out of more than one million possible pairs, only 266 sentence pairs are considered as useful for the parallel corpus creation, we observe a high degree of imbalance: little less than 4,400:1. Our purpose is to reduce this imbalance for facilitating the search of parallel sentences and improving the overall quality of the results. Data collection and pre-processing To perform our experiments, we work with a French comparable corpus containing biomedical documents with technical and simplified contents (Grabar and Cardon, 2018). The corpus is composed of three subcorpora: drug information for medical practitioners and patients released by the French Ministry of Health1 , medical literature reviews and 2 https://france.cochrane.org/ revues-cochrane 3 https://fr.wikipedia.org/ 4 https://fr.vikidia.org/ 1 http://base-donnees-publique. medicaments.gouv.fr/ 44 3. Method Data: A pair of syntactic trees (T1 and T2 ), a list of stop words (SW) Result: Boolean Boolean ← False; if one verb is found in both sentences then foreach leaf in T1 (L1 ) not found in SW do foreach leaf in T2 (L2 ) not found in SW do if L1 is identical"
2020.bucc-1.7,W17-4902,0,0.0122729,"e phrasing are not identical. More importantly, the order in which the information is delivered is not the same, which means that the document structure cannot be used for assuming where to look for parallel sentences. For our experiments, we took 39 randomly selected document pairs from that corpus and manually annotated them for two types of sentence pairs : Monolingual parallel corpora are useful for a variety of sequence-to-sequence tasks in natural language processing, such as text simplification (Xu et al., 2015), paraphrase acquisition (Deléger and Zweigenbaum, 2009) or style transfer (Jhamtani et al., 2017). In order to build such parallel corpora, the typical approach is to start from comparable corpora and extract sentence pairs that share the same meaning. For instance, the participants of the BUCC 2017 shared task had to address this problem using bilingual corpora (Zweigenbaum et al., 2017). One major obstacle is that, when considering two documents A and B, every single sentence from A has to be evaluated against every single sentence of B, when document metadata cannot be used to make assumptions as to where to look for corresponding sentences. This produces a large amount of noise, and e"
2020.bucc-1.7,P18-1249,0,0.0381704,"rs that have no chance of being of interest for the building of a parallel corpus. Hence, the purpose is to reduce the amount of manual check that needs to be performed on the output of a classifier. 2. • Equivalence : the sentences mean the same, but they are not identical; • Inclusion : the meaning of one sentence is included in the other one, where additional information can also be found. This retains information about sentence splitting or merging and about information deletion or addition. The documents are pre-processed for syntactic POStagging and syntactic analysis into constituents (Kitaev and Klein, 2018). In the manually annotated set, only sentences that have a verb are kept. This yields 266 sentence pairs: 136 equivalent pairs, and 130 inclusion pairs (56 in one direction, 74 in the other one). For the automatic processing, we produced the whole possible combinations of sentences within each of the 39 document pairs, and ended up with 1,164,407 sentence pairs. Thus, given that, out of more than one million possible pairs, only 266 sentence pairs are considered as useful for the parallel corpus creation, we observe a high degree of imbalance: little less than 4,400:1. Our purpose is to reduc"
2020.bucc-1.7,N10-1063,0,0.0501953,"ted against every single sentence of B, when document metadata cannot be used to make assumptions as to where to look for corresponding sentences. This produces a large amount of noise, and even with highly performing algorithms, the result of the extraction has to be manually checked for quality. With large volumes of data, this can be extremely costly. This is a known issue when working with comparable corpora (Zhang and Zweigenbaum, 2017). Yet, the issue is either not mentioned in works on parallel corpora creation from comparable corpora, or external information is used, such as metadata (Smith et al., 2010), which helps a lot the task. In our work, we propose and evaluate methods for filtering out sentences and sentence pairs that have no chance of being of interest for the building of a parallel corpus. Hence, the purpose is to reduce the amount of manual check that needs to be performed on the output of a classifier. 2. • Equivalence : the sentences mean the same, but they are not identical; • Inclusion : the meaning of one sentence is included in the other one, where additional information can also be found. This retains information about sentence splitting or merging and about information de"
2020.bucc-1.7,Q15-1021,0,0.013641,"xts address the same topic for different audiences, so that the delivered information and the phrasing are not identical. More importantly, the order in which the information is delivered is not the same, which means that the document structure cannot be used for assuming where to look for parallel sentences. For our experiments, we took 39 randomly selected document pairs from that corpus and manually annotated them for two types of sentence pairs : Monolingual parallel corpora are useful for a variety of sequence-to-sequence tasks in natural language processing, such as text simplification (Xu et al., 2015), paraphrase acquisition (Deléger and Zweigenbaum, 2009) or style transfer (Jhamtani et al., 2017). In order to build such parallel corpora, the typical approach is to start from comparable corpora and extract sentence pairs that share the same meaning. For instance, the participants of the BUCC 2017 shared task had to address this problem using bilingual corpora (Zweigenbaum et al., 2017). One major obstacle is that, when considering two documents A and B, every single sentence from A has to be evaluated against every single sentence of B, when document metadata cannot be used to make assumpt"
2020.bucc-1.7,W17-2510,0,0.0195947,"ress this problem using bilingual corpora (Zweigenbaum et al., 2017). One major obstacle is that, when considering two documents A and B, every single sentence from A has to be evaluated against every single sentence of B, when document metadata cannot be used to make assumptions as to where to look for corresponding sentences. This produces a large amount of noise, and even with highly performing algorithms, the result of the extraction has to be manually checked for quality. With large volumes of data, this can be extremely costly. This is a known issue when working with comparable corpora (Zhang and Zweigenbaum, 2017). Yet, the issue is either not mentioned in works on parallel corpora creation from comparable corpora, or external information is used, such as metadata (Smith et al., 2010), which helps a lot the task. In our work, we propose and evaluate methods for filtering out sentences and sentence pairs that have no chance of being of interest for the building of a parallel corpus. Hence, the purpose is to reduce the amount of manual check that needs to be performed on the output of a classifier. 2. • Equivalence : the sentences mean the same, but they are not identical; • Inclusion : the meaning of on"
2020.bucc-1.7,W17-2512,0,0.0173215,"at corpus and manually annotated them for two types of sentence pairs : Monolingual parallel corpora are useful for a variety of sequence-to-sequence tasks in natural language processing, such as text simplification (Xu et al., 2015), paraphrase acquisition (Deléger and Zweigenbaum, 2009) or style transfer (Jhamtani et al., 2017). In order to build such parallel corpora, the typical approach is to start from comparable corpora and extract sentence pairs that share the same meaning. For instance, the participants of the BUCC 2017 shared task had to address this problem using bilingual corpora (Zweigenbaum et al., 2017). One major obstacle is that, when considering two documents A and B, every single sentence from A has to be evaluated against every single sentence of B, when document metadata cannot be used to make assumptions as to where to look for corresponding sentences. This produces a large amount of noise, and even with highly performing algorithms, the result of the extraction has to be manually checked for quality. With large volumes of data, this can be extremely costly. This is a known issue when working with comparable corpora (Zhang and Zweigenbaum, 2017). Yet, the issue is either not mentioned"
2020.coling-main.62,D19-3009,0,0.011021,"oles, 1958) is a readability metric. It does not compare the output with the reference or the input, and only provides formal indicators such as sentence length and number of syllables per word. It is an adaptation of the Flesch (Flesch, 1948) readability measure – that was designed for English – to the French language. The absolute indexes are not informative by themselves: the measure is described to be more relevant for comparisons. Higher scores mean that the text should be easier to read. We computed the first two metrics with the EASSE evaluation suite for automatic text simplification (Alva-Manchego et al., 2019). 5 Quantitative and Qualitative Results Models Identity baseline WikiLarge FR CLEAR NPT 1:1 NPT 1:5 NPT 1:10 NPT 1:25 NPT 1:50 NPT 1:75 PTS 1:5 PTS 1:10 PTS 1:25 PTS 1:50 PTS 1:75 PTT 1:5 PTT 1:10 PTT 1:25 PTT 1:50 PTT 1:75 WikiLarge FR BLEU SARI Kandel 60.02 25.05 81.15 39.08 37.61 89.71 0.15 20.52 94.32 5.83 25.60 98.20 14.82 30.38 96.23 33.74 35.01 92.97 25.88 34.44 92.26 44.48 38.93 90.52 49.67 38.02 89.71 15.06 30.28 103.29 33.70 35.12 102.17 26.16 34.44 99.84 44.49 39.05 100.63 49.70 38.26 97.76 23.98 33.68 95.56 30.94 34.05 94.61 37.29 34.74 91.40 32.68 36.73 98.81 34.20 36.47 89.05 CL"
2020.coling-main.62,2020.cl-1.4,0,0.0109491,"ich can be seen as a monolingual translation task. It compares the system output with the reference data. This metric gives a rough indication of the performance of a system, especially regarding grammaticality and meaning preservation, but it is not a strong indicator for simplification (Sulem et al., 2018); (2) SARI (Xu et al., 2016) is currently considered as the most common metric for text simplification. SARI is computed by comparing the system output against the reference, and against the input as well. It should be noted that SARI is more reliable when several references are available (Alva-Manchego et al., 2020; Zhang and Lapata, 2017), which is not the case in our experiments; (3) Kandel (Kandel and Moles, 1958) is a readability metric. It does not compare the output with the reference or the input, and only provides formal indicators such as sentence length and number of syllables per word. It is an adaptation of the Flesch (Flesch, 1948) readability measure – that was designed for English – to the French language. The absolute indexes are not informative by themselves: the measure is described to be more relevant for comparisons. Higher scores mean that the text should be easier to read. We compu"
2020.coling-main.62,W14-1206,0,0.0212852,"h BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification. 1 Introduction The goal of text simplification is to make complex texts accessible to a given target audience (children, foreigners, diseased people...) or to make complex texts to be more easily processed by NLP applications. There is little existing work on biomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpo"
2020.coling-main.62,2020.lrec-1.686,0,0.0170182,"be more easily processed by NLP applications. There is little existing work on biomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first descr"
2020.coling-main.62,2020.lrec-1.169,0,0.0181089,"d sentences. We evaluate the results with BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification. 1 Introduction The goal of text simplification is to make complex texts accessible to a given target audience (children, foreigners, diseased people...) or to make complex texts to be more easily processed by NLP applications. There is little existing work on biomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of t"
2020.coling-main.62,W18-7002,1,0.749869,"resent and discuss the results (Section 5). We also provide the WikiLarge FR corpus and a set of native parallel sentences in French, from general and biomedical languages. 2 Linguistic Data Corpus WikiLarge FR CLEAR Total Pairs Tokens 297,494 12,753,567 4,596 226,149 Train Pairs Tokens 296,402 12,695,192 4,196 206,500 Validation Pairs Tokens 992 42,676 300 7,381 Test Pairs Tokens 100 4,302 100 4,965 Table 1: Size of the two parallel corpora We use two sets of parallel corpora dedicated to the simplification. One set is obtained from the freely available simplification corpus CLEAR in French (Grabar and Cardon, 2018). This is a comparable This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 710 Proceedings of the 28th International Conference on Computational Linguistics, pages 710–716 Barcelona, Spain (Online), December 8-13, 2020 corpus, which contains three types of texts (medical literature reviews, drug information, and medical articles from Wikipedia and Vikidia), and from which parallel sentences were extracted. This parallel corpus contains 4,596 sentence pairs, which is not sufficient when using the cu"
2020.coling-main.62,P13-1151,0,0.0234379,"Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first describe the data used (Section 2) and various configurations of the experiments (Section 3). We then indicate the evaluation principles (Section 4), and present and discuss the results (Section 5). We also provide the WikiLarge FR corpus and a set of native parallel sentences in French, from general and biomedical languages. 2 Linguistic Data Corpus WikiLarge FR CLEAR Total Pairs Tokens"
2020.coling-main.62,P17-4012,0,0.0418138,"28th International Conference on Computational Linguistics, pages 710–716 Barcelona, Spain (Online), December 8-13, 2020 corpus, which contains three types of texts (medical literature reviews, drug information, and medical articles from Wikipedia and Vikidia), and from which parallel sentences were extracted. This parallel corpus contains 4,596 sentence pairs, which is not sufficient when using the current state-of-the-art methods, that rely on deep learning. Another corpus is obtained thanks to the automatic translation of WikiLarge in French. The translation has been done using OpenNMT-py (Klein et al., 2017) with the default parameters, and the En-Fr model provided. This WikiLarge FR parallel corpus contains almost 300,000 sentence pairs. Table 1 indicates the volume of data in both corpora. We segmented the CLEAR corpus into train, validation and test sets: 100 examples for testing, three times as many for validation, and the rest for training. WikiLarge FR is already segmented in these three sets, yet we decided to reduce the WikiLarge FR test set from 359 pairs to 100 to make it comparable with the CLEAR test set. As these two corpora contain data from Wikipedia, we checked for duplicates to a"
2020.coling-main.62,P17-2014,0,0.0141215,"make complex texts to be more easily processed by NLP applications. There is little existing work on biomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplificati"
2020.coling-main.62,P02-1040,0,0.110017,"idual training took about five hours on a Geforce RTX 2070 GPU. During the simplification phase, we use the --replace_unk flag which tells the program to copy unknown words from the input to the output, except for the PTS set of experiments where it is replaced by the --phrase_table flag that uses the lexicon. We report three baselines: (1) one model trained on CLEAR only, (2) another model trained on WikiLarge FR only, and (3) the identity baseline which corresponds to the case where the output is the copy of the input. 711 4 Evaluation We evaluate the models using several metrics: (1) BLEU (Papineni et al., 2002), initially designed for the evaluation in machine translation, is also used in text simplification which can be seen as a monolingual translation task. It compares the system output with the reference data. This metric gives a rough indication of the performance of a system, especially regarding grammaticality and meaning preservation, but it is not a strong indicator for simplification (Sulem et al., 2018); (2) SARI (Xu et al., 2016) is currently considered as the most common metric for text simplification. SARI is computed by comparing the system output against the reference, and against th"
2020.coling-main.62,P19-1037,0,0.0767324,"araphrases. Then, we train neural models on these parallel corpora using different ratios of general and specialized sentences. We evaluate the results with BLEU, SARI and Kandel scores. The results point out that little specialized data helps significantly the simplification. 1 Introduction The goal of text simplification is to make complex texts accessible to a given target audience (children, foreigners, diseased people...) or to make complex texts to be more easily processed by NLP applications. There is little existing work on biomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according t"
2020.coling-main.62,D18-1081,0,0.0121401,"FR only, and (3) the identity baseline which corresponds to the case where the output is the copy of the input. 711 4 Evaluation We evaluate the models using several metrics: (1) BLEU (Papineni et al., 2002), initially designed for the evaluation in machine translation, is also used in text simplification which can be seen as a monolingual translation task. It compares the system output with the reference data. This metric gives a rough indication of the performance of a system, especially regarding grammaticality and meaning preservation, but it is not a strong indicator for simplification (Sulem et al., 2018); (2) SARI (Xu et al., 2016) is currently considered as the most common metric for text simplification. SARI is computed by comparing the system output against the reference, and against the input as well. It should be noted that SARI is more reliable when several references are available (Alva-Manchego et al., 2020; Zhang and Lapata, 2017), which is not the case in our experiments; (3) Kandel (Kandel and Moles, 1958) is a readability metric. It does not compare the output with the reference or the input, and only provides formal indicators such as sentence length and number of syllables per w"
2020.coling-main.62,D11-1038,0,0.0374764,"med on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first describe the data used (Section 2) and various configurations of the experiments (Section 3). We then indicate the evaluation principles (Section 4), and present and discuss the results (Section 5). We also provide the WikiLarge FR corpus and a set of native parallel sentences in French, from general and biomedical languages. 2 Linguistic Data Corpus WikiLarge FR CLEAR To"
2020.coling-main.62,Q15-1021,0,0.0232097,"iomedical text simplification in English (Peng et al., 2012; Shardlow and Nawaz, 2019), and on text simplification for the general language in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first describe the data used (Section 2) and various configurations of the experim"
2020.coling-main.62,Q16-1029,0,0.069437,"baseline which corresponds to the case where the output is the copy of the input. 711 4 Evaluation We evaluate the models using several metrics: (1) BLEU (Papineni et al., 2002), initially designed for the evaluation in machine translation, is also used in text simplification which can be seen as a monolingual translation task. It compares the system output with the reference data. This metric gives a rough indication of the performance of a system, especially regarding grammaticality and meaning preservation, but it is not a strong indicator for simplification (Sulem et al., 2018); (2) SARI (Xu et al., 2016) is currently considered as the most common metric for text simplification. SARI is computed by comparing the system output against the reference, and against the input as well. It should be noted that SARI is more reliable when several references are available (Alva-Manchego et al., 2020; Zhang and Lapata, 2017), which is not the case in our experiments; (3) Kandel (Kandel and Moles, 1958) is a readability metric. It does not compare the output with the reference or the input, and only provides formal indicators such as sentence length and number of syllables per word. It is an adaptation of"
2020.coling-main.62,D17-1062,0,0.302325,"e in French (Abdul Rauf et al., 2020; Gala et al., 2020; Sauvan et al., 2020; Brouwers et al., 2014). Research on text simplification is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first describe the data used (Section 2) and various configurations of the experiments (Section 3). We then indicate the evaluation principles (Section 4), and present and discuss the results (Section 5). We also provide the Wi"
2020.coling-main.62,C10-1152,0,0.0482401,"is usually performed on open domain English. Currently used methods rely on deep learning approaches and require large parallel monolingual corpora in which one complex sentence is paired with one or more simplified versions (Nisioi et al., 2017; Cooper and Shardlow, 2020). Two English datasets are commonly used: (1) Newsela (Xu et al., 2015), a corpus with news articles that are manually re-written according to four levels of simplification, and (2) WikiLarge (Zhang and Lapata, 2017), issued from a compilation of three previously released simplification corpora all extracted from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). The availability of corpora and resources plays indeed a very important role and condition the feasibility of the NLP research. We build and test models for biomedical text simplification in French. We first describe the data used (Section 2) and various configurations of the experiments (Section 3). We then indicate the evaluation principles (Section 4), and present and discuss the results (Section 5). We also provide the WikiLarge FR corpus and a set of native parallel sentences in French, from general and biomedical languages. 2 Linguistic Data C"
2020.jeptalnrecital-deft.1,2020.lrec-1.851,1,0.759904,"Missing"
2020.jeptalnrecital-deft.1,2020.jeptalnrecital-deft.4,0,0.0657627,"Missing"
2020.jeptalnrecital-deft.1,W18-7002,1,0.875702,"Missing"
2020.jeptalnrecital-deft.1,W18-5614,1,0.866685,"Missing"
2020.jeptalnrecital-deft.1,W19-5029,1,0.892526,"Missing"
2020.jeptalnrecital-deft.1,E12-2021,0,0.0586855,"Missing"
2020.jeptalnrecital-taln.21,jiang-etal-2014-native,0,0.0579351,"Missing"
2020.jeptalnrecital-taln.21,N15-1160,0,0.0184865,"Missing"
2020.jeptalnrecital-taln.21,C16-1198,0,0.0668921,"Missing"
2020.jeptalnrecital-taln.21,P16-1176,0,0.0460771,"Missing"
2020.lrec-1.851,R19-1020,1,0.922048,"corpus (Grabar and Cardon, 2018), which includes information about drugs, medical literature reviews, and medicine-related articles from Wikipedia and Vikidia. The purpose of this corpus is to propose comparable contents which are distinguished by their technicality: technical and difficult to understand texts are paired with the corresponding simple or simplified texts. This is another factor that distinguishes our dataset from the existing datasets in other languages mentioned in section 1.. The candidate pairs of sentences were generated automatically while building a classification method(Cardon and Grabar, 2019) and then validated and selected manually. That method is similar to the one described in section 4.1.. The main difference is that it is based on the Random Forest classifier algorithm, whereas below we use it as a Regressor. In the work presented in this paper, the goal is to retain sentence pairs pertaining to various degrees of similarity in order to be able to train a model to assign values on a continuous scale instead of binary values (aligned or not aligned). Hence, the semantic similarity between sentences within a given pair is due to their technicality and to the complexity of their"
2020.lrec-1.851,S17-2001,0,0.0246588,"ost of the time performed on a continuous scale where scores range from 0 (the sentences express completely unrelated meanings) to 5 (the meaning is exactly the same in both sentences). Several challenges dedicated to semantic textual similarity (STS) have been held within the SemEval evaluation campaign between 2012 and 2017. STS provides the research community with bilingual and monolingual data. In our work, we are interested in monolingual semantic similarity. In relation with the monolingual semantic similarity, data from a few languages (English, Spanish and Arabic) have been exploited (Cer et al., 2017) and made available for the research community. The overall STS benchmark data for English1 , with data taken from editions held from 2012 to 2017, contains 8,628 sentence pairs, while only 250 sentence pairs were proposed for Spanish and for Arabic. Besides, similar data are also proposed for Portuguese through the ASSIN workshop (Feitosa and Pinheiro, 2017) dataset, which is composed of 10,000 pairs – 5,000 for Brazilian Portuguese and 5,000 for European Portuguese. All those datasets are taken from general language and various sources : news articles, forum posts and video subtitles. Yet, t"
2020.lrec-1.851,W17-6619,0,0.0123906,"rch community with bilingual and monolingual data. In our work, we are interested in monolingual semantic similarity. In relation with the monolingual semantic similarity, data from a few languages (English, Spanish and Arabic) have been exploited (Cer et al., 2017) and made available for the research community. The overall STS benchmark data for English1 , with data taken from editions held from 2012 to 2017, contains 8,628 sentence pairs, while only 250 sentence pairs were proposed for Spanish and for Arabic. Besides, similar data are also proposed for Portuguese through the ASSIN workshop (Feitosa and Pinheiro, 2017) dataset, which is composed of 10,000 pairs – 5,000 for Brazilian Portuguese and 5,000 for European Portuguese. All those datasets are taken from general language and various sources : news articles, forum posts and video subtitles. Yet, there is no similar data in French. In our work, we introduce a semantic textual similarity corpus for French. We first describe the data that have been used and the annotation process, then we present the resulting resource. We also describe an experiment that shows an attempt at reproducing the annotation automatically. 2. Corpus and Annotation Process In th"
2020.lrec-1.851,W18-7002,1,0.691654,"tempt at reproducing the annotation automatically. 2. Corpus and Annotation Process In this section, we first present the data provided to the annotators. We then describe the annotation process and analyse the annotation criteria defined by the annotators. 2.1. Data Processed The same batch with 1,010 sentence pairs was provided to five annotators. The sentence pairs are issued from a general language corpus containing sentences extracted from Wikipedia 2 and Vikidia 3 articles, and from texts related to the medical field. In this last case, the sentences are extracted from the CLEAR corpus (Grabar and Cardon, 2018), which includes information about drugs, medical literature reviews, and medicine-related articles from Wikipedia and Vikidia. The purpose of this corpus is to propose comparable contents which are distinguished by their technicality: technical and difficult to understand texts are paired with the corresponding simple or simplified texts. This is another factor that distinguishes our dataset from the existing datasets in other languages mentioned in section 1.. The candidate pairs of sentences were generated automatically while building a classification method(Cardon and Grabar, 2019) and the"
2020.lrec-1.851,C16-1109,0,0.0285702,", and perform a few experiments for the automatic grading of semantic similarity. Keywords: semantic similarity, manual annotation, French language, regression 1. Introduction Semantic textual similarity is a subtask of Natural Language Processing. At the level of sentences, the task consists in evaluating to what extent two sentences express the same meaning. This task is useful for several applications, such as machine translation, text summarization, information retrieval, natural language generation, or text simplification (Wieting et al., 2019; Vadapalli et al., 2017; Yasui et al., 2019; Kajiwara and Komachi, 2016). The computing of the semantic textual similarity requires corpora with annotated pairs of sentences. The annotation is most of the time performed on a continuous scale where scores range from 0 (the sentences express completely unrelated meanings) to 5 (the meaning is exactly the same in both sentences). Several challenges dedicated to semantic textual similarity (STS) have been held within the SemEval evaluation campaign between 2012 and 2017. STS provides the research community with bilingual and monolingual data. In our work, we are interested in monolingual semantic similarity. In relati"
2020.lrec-1.851,L18-1615,0,0.0601703,"Missing"
2020.lrec-1.851,I17-2034,0,0.0252003,"e the annotation process, analyse these data, and perform a few experiments for the automatic grading of semantic similarity. Keywords: semantic similarity, manual annotation, French language, regression 1. Introduction Semantic textual similarity is a subtask of Natural Language Processing. At the level of sentences, the task consists in evaluating to what extent two sentences express the same meaning. This task is useful for several applications, such as machine translation, text summarization, information retrieval, natural language generation, or text simplification (Wieting et al., 2019; Vadapalli et al., 2017; Yasui et al., 2019; Kajiwara and Komachi, 2016). The computing of the semantic textual similarity requires corpora with annotated pairs of sentences. The annotation is most of the time performed on a continuous scale where scores range from 0 (the sentences express completely unrelated meanings) to 5 (the meaning is exactly the same in both sentences). Several challenges dedicated to semantic textual similarity (STS) have been held within the SemEval evaluation campaign between 2012 and 2017. STS provides the research community with bilingual and monolingual data. In our work, we are interes"
2020.lrec-1.851,P19-1427,0,0.0230131,"annotators. We describe the annotation process, analyse these data, and perform a few experiments for the automatic grading of semantic similarity. Keywords: semantic similarity, manual annotation, French language, regression 1. Introduction Semantic textual similarity is a subtask of Natural Language Processing. At the level of sentences, the task consists in evaluating to what extent two sentences express the same meaning. This task is useful for several applications, such as machine translation, text summarization, information retrieval, natural language generation, or text simplification (Wieting et al., 2019; Vadapalli et al., 2017; Yasui et al., 2019; Kajiwara and Komachi, 2016). The computing of the semantic textual similarity requires corpora with annotated pairs of sentences. The annotation is most of the time performed on a continuous scale where scores range from 0 (the sentences express completely unrelated meanings) to 5 (the meaning is exactly the same in both sentences). Several challenges dedicated to semantic textual similarity (STS) have been held within the SemEval evaluation campaign between 2012 and 2017. STS provides the research community with bilingual and monolingual data. In"
2020.lrec-1.851,P19-2056,0,0.027962,", analyse these data, and perform a few experiments for the automatic grading of semantic similarity. Keywords: semantic similarity, manual annotation, French language, regression 1. Introduction Semantic textual similarity is a subtask of Natural Language Processing. At the level of sentences, the task consists in evaluating to what extent two sentences express the same meaning. This task is useful for several applications, such as machine translation, text summarization, information retrieval, natural language generation, or text simplification (Wieting et al., 2019; Vadapalli et al., 2017; Yasui et al., 2019; Kajiwara and Komachi, 2016). The computing of the semantic textual similarity requires corpora with annotated pairs of sentences. The annotation is most of the time performed on a continuous scale where scores range from 0 (the sentences express completely unrelated meanings) to 5 (the meaning is exactly the same in both sentences). Several challenges dedicated to semantic textual similarity (STS) have been held within the SemEval evaluation campaign between 2012 and 2017. STS provides the research community with bilingual and monolingual data. In our work, we are interested in monolingual s"
2021.jeptalnrecital-deft.1,2021.jeptalnrecital-deft.2,0,0.0632114,"Missing"
2021.jeptalnrecital-deft.1,Q13-1032,0,0.0263255,"Missing"
2021.jeptalnrecital-deft.1,S13-2045,0,0.0237811,"Missing"
2021.jeptalnrecital-deft.1,2021.jeptalnrecital-deft.3,0,0.0903058,"Missing"
2021.jeptalnrecital-deft.1,W18-5614,1,0.84626,"Missing"
2021.jeptalnrecital-deft.1,W19-5029,1,0.900129,"Missing"
2021.jeptalnrecital-deft.1,horbach-etal-2014-finding,0,0.0465509,"Missing"
2021.jeptalnrecital-deft.1,W19-4433,0,0.0440774,"Missing"
2021.jeptalnrecital-deft.1,E09-1065,0,0.145203,"Missing"
2021.jeptalnrecital-deft.1,E12-2021,0,0.147089,"Missing"
2021.jeptalnrecital-taln.30,2020.coling-main.62,1,0.854997,"Missing"
2021.jeptalnrecital-taln.30,P17-4012,0,0.039981,"Missing"
2021.jeptalnrecital-taln.30,P02-1040,0,0.116379,"Missing"
2021.jeptalnrecital-taln.30,Q16-1029,0,0.0724308,"Missing"
2021.jeptalnrecital-taln.30,D17-1062,0,0.0526124,"Missing"
F13-1005,W98-0709,0,0.031896,"Missing"
F13-1005,W02-1106,0,0.0965263,"Missing"
F13-1005,W07-0908,0,0.0600387,"Missing"
F14-1027,C08-1013,0,0.0541128,"Missing"
F14-1027,W07-1007,0,0.054015,"Missing"
F14-1027,P98-1082,0,0.239763,"Missing"
F14-1027,W03-1608,0,0.0768176,"Missing"
F14-1027,N10-1017,0,0.0295242,"Missing"
F14-1027,J10-3003,0,0.0577459,"Missing"
F14-1027,P00-1056,0,0.195856,"Missing"
F14-1027,I05-1011,0,0.094946,"Missing"
F14-1027,W04-3219,0,0.117368,"Missing"
F14-1027,I05-5011,0,0.0413668,"Missing"
F14-1027,P06-2096,0,0.0586517,"Missing"
L16-1420,W14-1202,1,\N,Missing
L16-1596,P05-1074,0,0.0285358,"; Sekine, 2005; Shen et al., 2006). More particularly, named entities and numbers are part of the clues used for the extraction of paraphrases, such as in {PERS1 killed PERS2, PERS1 let PERS2 die from loss of blood} or {PERS1 shadowed PERS2, PERS1 kept his eyes on PERS2} (Shinyama et al., 2002). Bilingual parallel corpora. Bilingual parallel corpora typically contain translations of a given text in another language. Once they are aligned at the level of sentences, they can also be used for the detection of paraphrases. Different translations of a given linguistic unit can provide paraphrases (Bannard and Callison-Burch, 2005; CallisonBurch et al., 2008; Kok and Brockett, 2010). For instance, the paraphrases {under control, in check} can be extracted because they are translations of unter kontrolle (Bannard and Callison-Burch, 2005). 3. Objectives We have multi-fold objectives: • computing the similarity of strings permits to detect linguistic units (words, expressions, etc.) that show common surface features such as with {When did Charle de Gaulle die?, Charles de Gaulle died in 1970} (Malakasiotis and Androutsopoulos, 2007), • distributional methods allow to detect units that occur in similar contexts. Such unit"
L16-1596,P01-1008,0,0.138817,"be seen as the result of reformulation. Usually, the existing approaches exploit paradigmatic properties of words and their capacity to replace each other in a given context. These approaches depend on the corpora exploited. Four types of corpora are usually distinguished: monolingual, monolingual parallel, monolingual comparable, and bilingual parallel. Monolingual corpora. Two kinds of approaches may be used with monolingual corpora: McKeown, 2001). They allow to extract paraphrases such as {countless, lots of}, {undertone, low voice}, {shrubs, bushes}, {refuse, say no}, {dull tone, gloom} (Barzilay and McKeown, 2001). Monolingual comparable corpora. Monolingual comparable corpora typically contain texts on the same event but created independently, such as news articles. The thematic coherence of these texts and the distributional methods or alignment of comparable sentences may lead to the detection of paraphrases (Shinyama et al., 2002; Sekine, 2005; Shen et al., 2006). More particularly, named entities and numbers are part of the clues used for the extraction of paraphrases, such as in {PERS1 killed PERS2, PERS1 let PERS2 die from loss of blood} or {PERS1 shadowed PERS2, PERS1 kept his eyes on PERS2} (S"
L16-1596,C08-1013,0,0.0477992,"Missing"
L16-1596,P13-2080,0,0.0183174,"ndroutsopoulos and Malakasiotis, 2010; Bouamor et al., 2012). The objective is to detect linguistic expressions that differ by their form but convey the same or similar meaning: • In information retrieval and extraction, paraphrases permit to increase the coverage of the found or extracted results. For instance, pairs like {myocardial infarction, heart attack} and {Alzheimer’s disease, neurodegenerative disease} contain different expressions that convey identical or close semantics; • In machine translation, paraphrases permit to avoid lexical repetitions (Scarpa, 2010); • Textual entailment (Dagan et al., 2013) consists of creating relation between two textual segments, called Text and Hypothesis. Entailment is a directional relation, in which the truth of the Hypothesis must be inferred through the analysis of the Text. For instance, the Text The drugs that slow down or halt Alzheimer’s disease work best the earlier you administer them allows inferring that the Hypothesis Alzheimer’s disease is treated by drugs is true; while the Hypothesis Alzheimer’s disease is cured by drugs cannot be inferred from this Text. In this example, the paraphrases {administer drugs, treated by drugs} permit to establi"
L16-1596,W03-1608,0,0.0609713,"be good candidates for the paraphrase (e.g. {Y is solved by X, Y is resolved in X}) (Lin and Pantel, 2001; Pasc¸a and Dienes, 2005). Monolingual parallel corpora. When a given text is translated more than once in another language, these translations allow to build monolingual parallel corpora. One of the most used corpora is Jules Verne’s 20 000 lieux sous la mer that has been translated twice in English. Once these corpora are aligned at the sentence level, it is possible to exploit them with word alignment tools (Och and Ney, 2000). Various methods have been proposed for such exploitation (Ibrahim et al., 2003; Quirk et al., 2004; Barzilay and 3761 1. propose annotation guidelines for reformulations and to test them when annotating enunciations from French spoken corpora. These guidelines are presented in a previous work (Eshkol-Taravella and Grabar, 2014) and outlined at the end of Section 1. They allow creating the reference data; 2. study three specific reformulation markers: c’est-`adire (in other words), je veux dire (that is to say / I mean), and disons (let’s say). Several other reformulation markers exist (notamment, en d’autres mots, en d’autres termes...), but we propose to concentrate he"
L16-1596,N10-1017,0,0.0193557,"ntities and numbers are part of the clues used for the extraction of paraphrases, such as in {PERS1 killed PERS2, PERS1 let PERS2 die from loss of blood} or {PERS1 shadowed PERS2, PERS1 kept his eyes on PERS2} (Shinyama et al., 2002). Bilingual parallel corpora. Bilingual parallel corpora typically contain translations of a given text in another language. Once they are aligned at the level of sentences, they can also be used for the detection of paraphrases. Different translations of a given linguistic unit can provide paraphrases (Bannard and Callison-Burch, 2005; CallisonBurch et al., 2008; Kok and Brockett, 2010). For instance, the paraphrases {under control, in check} can be extracted because they are translations of unter kontrolle (Bannard and Callison-Burch, 2005). 3. Objectives We have multi-fold objectives: • computing the similarity of strings permits to detect linguistic units (words, expressions, etc.) that show common surface features such as with {When did Charle de Gaulle die?, Charles de Gaulle died in 1970} (Malakasiotis and Androutsopoulos, 2007), • distributional methods allow to detect units that occur in similar contexts. Such units have similar contextual or syntactic vectors, and m"
L16-1596,P10-1052,0,0.0564012,"Missing"
L16-1596,J10-3003,0,0.0174995,"one of their interlocutor, with or without specific markers. The objective is then to modify some aspects (lexical, syntactic, semantic, pragmatic) but to keep the semantic content constant (G¨ulich and Kotschi, 1987; Kanaan, 2011). Specific reformulation markers may provide the formal mark-up of reformulations. Reformulation is closely related to paraphrases, in that way that reformulated sequences can produce the paraphrases (Neveu, 2004). Reformulation and paraphrase play an important role in languages: Reformulation and paraphrase also play an important role in different NLP applications (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010; Bouamor et al., 2012). The objective is to detect linguistic expressions that differ by their form but convey the same or similar meaning: • In information retrieval and extraction, paraphrases permit to increase the coverage of the found or extracted results. For instance, pairs like {myocardial infarction, heart attack} and {Alzheimer’s disease, neurodegenerative disease} contain different expressions that convey identical or close semantics; • In machine translation, paraphrases permit to avoid lexical repetitions (Scarpa, 2010); • Textual entailmen"
L16-1596,W07-1407,0,0.0290088,"ction of paraphrases. Different translations of a given linguistic unit can provide paraphrases (Bannard and Callison-Burch, 2005; CallisonBurch et al., 2008; Kok and Brockett, 2010). For instance, the paraphrases {under control, in check} can be extracted because they are translations of unter kontrolle (Bannard and Callison-Burch, 2005). 3. Objectives We have multi-fold objectives: • computing the similarity of strings permits to detect linguistic units (words, expressions, etc.) that show common surface features such as with {When did Charle de Gaulle die?, Charles de Gaulle died in 1970} (Malakasiotis and Androutsopoulos, 2007), • distributional methods allow to detect units that occur in similar contexts. Such units have similar contextual or syntactic vectors, and may be good candidates for the paraphrase (e.g. {Y is solved by X, Y is resolved in X}) (Lin and Pantel, 2001; Pasc¸a and Dienes, 2005). Monolingual parallel corpora. When a given text is translated more than once in another language, these translations allow to build monolingual parallel corpora. One of the most used corpora is Jules Verne’s 20 000 lieux sous la mer that has been translated twice in English. Once these corpora are aligned at the sentenc"
L16-1596,P00-1056,0,0.311085,"lar contexts. Such units have similar contextual or syntactic vectors, and may be good candidates for the paraphrase (e.g. {Y is solved by X, Y is resolved in X}) (Lin and Pantel, 2001; Pasc¸a and Dienes, 2005). Monolingual parallel corpora. When a given text is translated more than once in another language, these translations allow to build monolingual parallel corpora. One of the most used corpora is Jules Verne’s 20 000 lieux sous la mer that has been translated twice in English. Once these corpora are aligned at the sentence level, it is possible to exploit them with word alignment tools (Och and Ney, 2000). Various methods have been proposed for such exploitation (Ibrahim et al., 2003; Quirk et al., 2004; Barzilay and 3761 1. propose annotation guidelines for reformulations and to test them when annotating enunciations from French spoken corpora. These guidelines are presented in a previous work (Eshkol-Taravella and Grabar, 2014) and outlined at the end of Section 1. They allow creating the reference data; 2. study three specific reformulation markers: c’est-`adire (in other words), je veux dire (that is to say / I mean), and disons (let’s say). Several other reformulation markers exist (notam"
L16-1596,I05-1011,0,0.0270724,"Missing"
L16-1596,W04-3219,0,0.0349468,"r the paraphrase (e.g. {Y is solved by X, Y is resolved in X}) (Lin and Pantel, 2001; Pasc¸a and Dienes, 2005). Monolingual parallel corpora. When a given text is translated more than once in another language, these translations allow to build monolingual parallel corpora. One of the most used corpora is Jules Verne’s 20 000 lieux sous la mer that has been translated twice in English. Once these corpora are aligned at the sentence level, it is possible to exploit them with word alignment tools (Och and Ney, 2000). Various methods have been proposed for such exploitation (Ibrahim et al., 2003; Quirk et al., 2004; Barzilay and 3761 1. propose annotation guidelines for reformulations and to test them when annotating enunciations from French spoken corpora. These guidelines are presented in a previous work (Eshkol-Taravella and Grabar, 2014) and outlined at the end of Section 1. They allow creating the reference data; 2. study three specific reformulation markers: c’est-`adire (in other words), je veux dire (that is to say / I mean), and disons (let’s say). Several other reformulation markers exist (notamment, en d’autres mots, en d’autres termes...), but we propose to concentrate here on these three ma"
L16-1596,I05-5011,0,0.0213698,". Monolingual corpora. Two kinds of approaches may be used with monolingual corpora: McKeown, 2001). They allow to extract paraphrases such as {countless, lots of}, {undertone, low voice}, {shrubs, bushes}, {refuse, say no}, {dull tone, gloom} (Barzilay and McKeown, 2001). Monolingual comparable corpora. Monolingual comparable corpora typically contain texts on the same event but created independently, such as news articles. The thematic coherence of these texts and the distributional methods or alignment of comparable sentences may lead to the detection of paraphrases (Shinyama et al., 2002; Sekine, 2005; Shen et al., 2006). More particularly, named entities and numbers are part of the clues used for the extraction of paraphrases, such as in {PERS1 killed PERS2, PERS1 let PERS2 die from loss of blood} or {PERS1 shadowed PERS2, PERS1 kept his eyes on PERS2} (Shinyama et al., 2002). Bilingual parallel corpora. Bilingual parallel corpora typically contain translations of a given text in another language. Once they are aligned at the level of sentences, they can also be used for the detection of paraphrases. Different translations of a given linguistic unit can provide paraphrases (Bannard and Ca"
L16-1596,P06-2096,0,0.0241361,"corpora. Two kinds of approaches may be used with monolingual corpora: McKeown, 2001). They allow to extract paraphrases such as {countless, lots of}, {undertone, low voice}, {shrubs, bushes}, {refuse, say no}, {dull tone, gloom} (Barzilay and McKeown, 2001). Monolingual comparable corpora. Monolingual comparable corpora typically contain texts on the same event but created independently, such as news articles. The thematic coherence of these texts and the distributional methods or alignment of comparable sentences may lead to the detection of paraphrases (Shinyama et al., 2002; Sekine, 2005; Shen et al., 2006). More particularly, named entities and numbers are part of the clues used for the extraction of paraphrases, such as in {PERS1 killed PERS2, PERS1 let PERS2 die from loss of blood} or {PERS1 shadowed PERS2, PERS1 kept his eyes on PERS2} (Shinyama et al., 2002). Bilingual parallel corpora. Bilingual parallel corpora typically contain translations of a given text in another language. Once they are aligned at the level of sentences, they can also be used for the detection of paraphrases. Different translations of a given linguistic unit can provide paraphrases (Bannard and Callison-Burch, 2005;"
L16-1596,J13-3001,0,\N,Missing
R19-1020,P02-1020,0,0.140629,"ons of future work. 2 Method We use the CLEAR comparable medical corpus (Grabar and Cardon, 2018) available online1 which contains three comparable sub-corpora in French. Documents within these sub-corpora are contrasted by the degree of technicality of the information they contain with typically specialized • lexicon-based methods which rely on similarity of subwords or words from the processed texts or on machine translation (Madnani et al., 2012). The features exploited can be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 1 http://natalia.grabar.free.fr/ resources.php#clear 169 and simplified versions of a given text. These corpora cover three genres: drug information, summaries of scientific articles, and encyclopedia articles. We also exploit a reference dataset with sentences manually aligned by two annotators. 2.1 Those three corpora have different degrees of parallelism: Wikipedia and Vikidia articles are written independently from each other, drug information documents are related to the same drugs but the types of information presented for experts and laypeople vary"
R19-1020,S16-1081,0,0.0437679,"Missing"
R19-1020,N13-1092,0,0.0718052,"Missing"
R19-1020,S13-1004,0,0.0135027,"lel sentences from this comparable corpus. We also propose to test what is the impact of imbalance on categorization results: imbalance of categories is indeed the natural characteristics in textual data. The existing work on searching parallel sentences in monolingual comparable corpora indicates that the main difficulty is that such sentences may show low lexical overlap but be nevertheless parallel. Recently, this task gained in popularity in general-language domain thanks to the semantic text similarity (STS) initiative. Dedicated SemEval competitions have been proposed for several years (Agirre et al., 2013, 2015, 2016). The objective, for a given pair of sentences, is to predict whether they are semantically similar and to assign a similarity score going from 0 (independent semantics) to 5 (semantic equivalence). This task is usually explored in generallanguage corpora (Coster and Kauchak, 2011; Hwang et al., 2015; Kajiwara and Komachi, 2016; Brunato et al., 2016). Among the exploited methods, we can notice: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external r"
R19-1020,W18-7002,1,0.648772,"text simplification. Indeed, such pairs of sentences contain cues on transformations which are suitable for the simplification, such as lexical substitutes and syntactic modifications. Yet, this kind of resources is seldom available, especially in languages other than English. As a matter of fact, it is easier to access comparable corpora: they cover the same topics but are differentiated by their registers (documents created for medical professionals and documents created for patients). More precisely, we can exploit an existing monolingual comparable corpus with medical documents in French (Grabar and Cardon, 2018). The purpose of our work is to detect and align parallel sentences from this comparable corpus. We also propose to test what is the impact of imbalance on categorization results: imbalance of categories is indeed the natural characteristics in textual data. The existing work on searching parallel sentences in monolingual comparable corpora indicates that the main difficulty is that such sentences may show low lexical overlap but be nevertheless parallel. Recently, this task gained in popularity in general-language domain thanks to the semantic text similarity (STS) initiative. Dedicated SemEv"
R19-1020,P12-1091,0,0.0622552,"Missing"
R19-1020,D15-1181,0,0.0149449,", 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). There has been work for detection of paraphrases in French comparable biomedical corpora (Del´eger and Zweigenbaum, 2009), but there is no work on building a corpus for text simplification in the biomedical domain. Our work is positioned in this area. In what follows, we first present the linguistic material used, and the methods proposed. We then present and discuss the results obtained, and conclude with directions of future work. 2 Method We use the CLEAR comparable medical corpus (Grabar and Cardon, 2018) available online1 which contains three comparable s"
R19-1020,W03-1004,0,0.440371,"ence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). There has been work for detection of paraphrases in French comparable biomedical corpora (Del´eger and Zweigenbaum, 2009), but there is no work on building a corpus for text simplification in the biomedical domain. Our work is positioned in this area. In what follows, we first present the linguistic material used, and the methods proposed. We then present and discuss the results obtained, and conclude with directions of future work. 2 Method We use the CLEAR comparable medical corpus ("
R19-1020,N15-1022,0,0.013923,"fficulty is that such sentences may show low lexical overlap but be nevertheless parallel. Recently, this task gained in popularity in general-language domain thanks to the semantic text similarity (STS) initiative. Dedicated SemEval competitions have been proposed for several years (Agirre et al., 2013, 2015, 2016). The objective, for a given pair of sentences, is to predict whether they are semantically similar and to assign a similarity score going from 0 (independent semantics) to 5 (semantic equivalence). This task is usually explored in generallanguage corpora (Coster and Kauchak, 2011; Hwang et al., 2015; Kajiwara and Komachi, 2016; Brunato et al., 2016). Among the exploited methods, we can notice: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The fea"
R19-1020,C16-1109,0,0.220393,"Missing"
R19-1020,E06-1021,0,0.0642389,"ntences are collected in two languages and correspond to mutual translations. In the general language, the Europarl (Koehn, 2005) corpus provides such sentences in several pairs of languages. Yet, the dimension on which the parallelism is positioned can come from other levels, such as expert and non-expert register of language. The following pair of sentences (first in expert and second in non-expert languages) illustrates this: 168 Proceedings of Recent Advances in Natural Language Processing, pages 168–177, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_020 2006; Nelken and Shieber, 2006; Zhu et al., 2010); derstand medical and health information is an important issue, which motivates our work. In order to perform biomedical text simplification, we propose to collect parallel sentences, which align difficult and simple information, as they provide crucial and necessary indicators for automatic systems for text simplification. Indeed, such pairs of sentences contain cues on transformations which are suitable for the simplification, such as lexical substitutes and syntactic modifications. Yet, this kind of resources is seldom available, especially in languages other than Englis"
R19-1020,L16-1491,0,0.0137354,"sed for the text simplification. The purpose of text simplification is to provide simplified versions of texts, in order to remove or replace difficult words or information. Simplification can be concerned with different linguistic aspects, such as lexicon, syntax, semantics, pragmatics and even document structure. Automatic text simplification can be used as a preprocessing step for NLP applications or for producing suitable versions of texts for humans. In this second case, simplified documents are typically created for children (Vu et al., 2014), for people with low literacy or foreigners (Paetzold and Specia, 2016), for people with mental or neurodegenerative disorders (Chen et al., 2016), or for laypeople who face specialized documents (Leroy et al., 2013). Our work is related to the creation of simplified medical documents for laypeople, such as patients and their relatives. It has indeed been noticed that medical and health documents contain information that is difficult to understand by patients and their relatives, mainly because of the presence of technical and specialized terms and notions. This situation has a negative effect on the healthcare process (AMA, 1999; Mcgray, 2005; Rudd, 2013). Hence"
R19-1020,2005.mtsummit-papers.11,0,0.0972812,"d been noticed that medical and health documents contain information that is difficult to understand by patients and their relatives, mainly because of the presence of technical and specialized terms and notions. This situation has a negative effect on the healthcare process (AMA, 1999; Mcgray, 2005; Rudd, 2013). Hence, helping patients to better unIntroduction Parallel sentences provide semantically similar information which can vary on a given dimension. Typically, parallel sentences are collected in two languages and correspond to mutual translations. In the general language, the Europarl (Koehn, 2005) corpus provides such sentences in several pairs of languages. Yet, the dimension on which the parallelism is positioned can come from other levels, such as expert and non-expert register of language. The following pair of sentences (first in expert and second in non-expert languages) illustrates this: 168 Proceedings of Recent Advances in Natural Language Processing, pages 168–177, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_020 2006; Nelken and Shieber, 2006; Zhu et al., 2010); derstand medical and health information is an important issue, which motivates our w"
R19-1020,W06-1603,0,0.105532,"Missing"
R19-1020,S14-2055,0,0.0233829,"lence). This task is usually explored in generallanguage corpora (Coster and Kauchak, 2011; Hwang et al., 2015; Kajiwara and Komachi, 2016; Brunato et al., 2016). Among the exploited methods, we can notice: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyag"
R19-1020,P13-2125,0,0.0131315,"PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). There has been work for detection of paraphrases in French comparable biomedical corpora (Del´eger and Zweigenbaum, 2009), but there is no work on building a corpus for text simplification in the biomedical domain. Our work is positioned in this area. In what follows, we first present t"
R19-1020,N12-1019,0,0.0173243,"a. In what follows, we first present the linguistic material used, and the methods proposed. We then present and discuss the results obtained, and conclude with directions of future work. 2 Method We use the CLEAR comparable medical corpus (Grabar and Cardon, 2018) available online1 which contains three comparable sub-corpora in French. Documents within these sub-corpora are contrasted by the degree of technicality of the information they contain with typically specialized • lexicon-based methods which rely on similarity of subwords or words from the processed texts or on machine translation (Madnani et al., 2012). The features exploited can be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 1 http://natalia.grabar.free.fr/ resources.php#clear 169 and simplified versions of a given text. These corpora cover three genres: drug information, summaries of scientific articles, and encyclopedia articles. We also exploit a reference dataset with sentences manually aligned by two annotators. 2.1 Those three corpora have different degrees of parallelism: Wikipedia and Vikidia articles are"
R19-1020,L18-1615,0,0.0468281,"it operations (insertion, deletion and substitution) at the level of characters. The cost of each operation is set to 1; 8. Word-based minimal edit distance (Levenshtein, 1966). This feature is computed with words as units within sentence. It takes into account the same three edit operations with the same cost set to 1. This feature permits to compute the cost of lexical transformation of one sentence into another; 9. WAVG. This features uses word embeddings. The word vectors of each sentence are averaged, and the similarity score is calculated by comparing the two resulting sentence vectors (Stajner et al., 2018); 1. Number of common non-stopwords. This feature permits to compute the basic lexical overlap between specialized and simplified versions of sentences (Barzilay and Elhadad, 2003). It concentrates on non-lexical content of sentences; 10. CWASA. This feature is the continuous word alignment-based similarity analysis, as described in (Franco-Salvador et al., 2016). 2. Percentage of words from one sentence included in the other sentence, computed in both directions. This features represents possible lexical and semantic inclusion relations between the sentences; For the last two features, we tra"
R19-1020,P15-1150,0,0.0596778,"Missing"
R19-1020,U06-1019,0,0.0179223,"et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). There has been work for detection of paraphrases in French comparable biomedical corpora (Del´eger and Zweigenbaum, 2009), but there is no work on building a corpus for text simplification in the biomedical domain. Our work is positioned in this area. In what follo"
R19-1020,U05-1023,0,0.0803561,"Method We use the CLEAR comparable medical corpus (Grabar and Cardon, 2018) available online1 which contains three comparable sub-corpora in French. Documents within these sub-corpora are contrasted by the degree of technicality of the information they contain with typically specialized • lexicon-based methods which rely on similarity of subwords or words from the processed texts or on machine translation (Madnani et al., 2012). The features exploited can be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 1 http://natalia.grabar.free.fr/ resources.php#clear 169 and simplified versions of a given text. These corpora cover three genres: drug information, summaries of scientific articles, and encyclopedia articles. We also exploit a reference dataset with sentences manually aligned by two annotators. 2.1 Those three corpora have different degrees of parallelism: Wikipedia and Vikidia articles are written independently from each other, drug information documents are related to the same drugs but the types of information presented for experts and laypeople vary, while simplified summar"
R19-1020,S14-2044,0,0.0309862,"Missing"
R19-1020,C10-1152,0,0.0386448,"wo languages and correspond to mutual translations. In the general language, the Europarl (Koehn, 2005) corpus provides such sentences in several pairs of languages. Yet, the dimension on which the parallelism is positioned can come from other levels, such as expert and non-expert register of language. The following pair of sentences (first in expert and second in non-expert languages) illustrates this: 168 Proceedings of Recent Advances in Natural Language Processing, pages 168–177, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_020 2006; Nelken and Shieber, 2006; Zhu et al., 2010); derstand medical and health information is an important issue, which motivates our work. In order to perform biomedical text simplification, we propose to collect parallel sentences, which align difficult and simple information, as they provide crucial and necessary indicators for automatic systems for text simplification. Indeed, such pairs of sentences contain cues on transformations which are suitable for the simplification, such as lexical substitutes and syntactic modifications. Yet, this kind of resources is seldom available, especially in languages other than English. As a matter of f"
R19-1026,P82-1020,0,0.819473,"Missing"
R19-1026,P11-2049,0,0.0216251,"apted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic p"
R19-1026,W10-3010,0,0.0270406,"2001) pioneered the area. It uses regular expressions to detect the cues and to identify medical terms in their scope. It was later adapted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addre"
R19-1026,Q17-1010,0,0.0432003,", several models have been introduced to generate vector representations of words helping machine learning approaches to better capture their semantics. The models used in the negation/speculation detection task are the following ones. word2vec (Mikolov et al., 2013) is a predictive model to learn word embeddings from plain text. The embeddings can be calculated using two model architectures: the continuous bag-of-words (CBOW) and Skip-Gram (SG) models. In this work, we use use the SG model; it treats each context-target pair as new observation, which is suitable for large datasets. fastText (Bojanowski et al., 2017) addresses the Word2vec’s main issue: the words, which do not occur in the vocabulary, cannot be represented. Hence, this algorithm uses subword information: each word is represented as a bag of all possible character n-grams it contains. The word is padded using a set of unique symbols which helps singling out prefixes and suffixes. The full sequence is added to the bag of n-grams as well. The vector now denotes every char n-gram and the word vector is the sum of its char n-gram vectors. Since the char n-gram representations across words are ANNOTATION LAYERS These two corpora are Part-of-Spe"
R19-1026,W04-3103,0,0.0325595,"hile the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural network. Given the results from the latter approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We manually annotated two corpora from the biomedical field. Table 1 presents some statistics on these corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking"
R19-1026,S12-1035,0,0.121998,"nd negation, which has resulted in models for their automatic detection. These corpora can be divided into two categories: (1) corpora annotated with cues and scopes, such as Bioscope (Vincze et al., 2008) or *SEM-2012, and (2) corpora focusing on concepts and named entities, such as I2B2 and Mipacq. We briefly describe these corpora. The Bioscope corpus (Vincze et al., 2008) contains reports of radiological examinations, scientific articles, and abstracts from biomedical articles. Each sentence and each negation and speculation cue/scope pair receives unique identifier. The *SEM-2012 corpus (Morante and Blanco, 2012) consists of a Sherlock Holmes novel and three other short stories written by Sir Arthur Conan Doyle. It contains 5,520 sentences, among which 1,227 sentences are negated. Each occurrence of the negation, the cue and its scope are annotated, as well as the focus of the negation if relevant. In this corpus, cues and scopes can be discontinuous. The I2B2/VA-2010 challenge (Uzuner et al., 2011) featured several tasks using US clinical records. One task aimed the detection of statements and of their SPECULATION The expression of speculation can be even more complex than negation. Indeed, speculati"
R19-1026,C10-1155,0,0.0774336,"Missing"
R19-1026,D09-1145,0,0.0284986,"dicated to the negation detection, NegEx (Chapman et al., 2001) pioneered the area. It uses regular expressions to detect the cues and to identify medical terms in their scope. It was later adapted to various languages including French (Del´eger and Grouin, 2012). ConText (Harkema et al., 2009), derived from NegEx, covers more objectives: negation, temporality, and the subject concerned by this information in the clinical texts. It has been adapted to French (Abdaoui et al., 2017). In another work, medical concepts may receive additional labels (positive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion sem"
R19-1026,J12-2005,0,0.0161413,"se corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS with CLINICAL TRIALS One corpus contains clinical trial protocols in French. They were mainly obtained from the National Cancer Institute registry1 . The typica"
R19-1026,W08-0606,0,0.251077,"Missing"
R19-1026,P14-1007,0,0.392957,"ive, negative or uncertain) Elkin et al. ¨ ur and Radev (2009); Øvrelid et al. (2005a). Ozg¨ (2010); Kilicoglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic paths between the cues and the candidate tokens in constituency and dependency parsed trees. Fancellu et al. (2016) uses neural networks to solve the problem of negation scope detection. One approach uses Feed-forward neural network, while the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural net"
R19-1026,N18-1202,0,0.023428,"types of word vector representations and recurrent neural networks for the detection of negation and speculation. There has not been much work of this type on French corpora, especially for the biomedical domain which contains specific negation and speculation phenomena. We showed that a CRF layer yields better performance than softmax on exact scope match. Finally, the models have been applied in a cross-corpus context. Besides, we plan to improve our neural network performance by providing richer feature. In particular, recent embedding techniques, such as BERT or ELMO (Devlin et al., 2018; Peters et al., 2018) may provide more accurate representation of the sentences. Moreover, in order to provide more accurate features, we plan to move from TreeTagger, which makes a substantial number of mistakes on our datasets, to a POS tagger/lemmatizer dedicated to French biomedical texts. Syntactic parsing of sentences may also provide useful features for the detection of scope. but causes the disappearance of normal B lymphocytes for several months, which could increase the occurrence of serious infections because these lymphocytes participate in the immune defense.) However, most of our errors impact recall"
R19-1026,D16-1078,0,0.0171152,"coglu and Bergler (2010) exploit lexical, grammatical and syntactic information to detect speculation and its scope. ScopeFinder (Apostolova et al., 2011) detects the scope of negation and speculation with rules built automatically from BioScope (lexico-syntactic patterns extraction). NegBio (Peng et al., 2018) detects both negation and speculation in radiology reports with rules based on universal dependency graphs. 3.3 French clin. trials – 6,547 150,084 7,880 1,025 630 as a fall-back by Packard et al. (2014) when the main MRS (minimal recursion semantics) Crawler cannot parse the sentence. Qian et al. (2016) addresses the scope detection with an approach based on a convolutional neural network which extracts features from various syntactic paths between the cues and the candidate tokens in constituency and dependency parsed trees. Fancellu et al. (2016) uses neural networks to solve the problem of negation scope detection. One approach uses Feed-forward neural network, while the other, which appears to be more efficient for the task, uses a bidirectional Long Short-Term Memory (BiLSTM) neural network. Given the results from the latter approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We ma"
R19-1026,S12-1041,0,0.448698,"tive sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS with CLINICAL TRIALS One corpus contains clinical trial protocols in French. They were mainly obtained from the National Cancer Institute registry1 . The typical protocol consists of two parts: the summary of the trial, which indicates the purpose of the trial and the methods applied; and a detailed description of the trial with the inclusion and exclusion criteria. 1 225 https://www.e-cancer.fr Form Pas de dyspn´ee . Lemma pas de dyspn´ee . POS ADV PRP NOM SENT Cue pas scope de dyspn´ee Table 2: Excerpt from the CAS corpus. The columns contain linguistic"
R19-1026,W10-3002,0,0.0298461,"approach, it inspired our work. 4 FRENCH MEDICAL CORPORA We manually annotated two corpora from the biomedical field. Table 1 presents some statistics on these corpora: the number of words, the variety of the vocabulary, the number of sentences, the number of negative sentences with one or more negations. The Inter Annotator Agreement (IAA) on negation annotation is high (Cohen’s κ=0.8461). SUPERVISED LEARNING To our knowledge, Light et al. (2004) is the first work to include supervised learning for speculation detection. It relies on SVM to select speculative sentences in MEDLINE abstracts. Tang et al. (2010) proposes a cascade method based on CRF and SVM classifiers to detect speculation cues and another CRF classifier to identify their scopes. Velldal et al. (2012) proposes a SVM-based cue detection system, trained on simple n-grams features computed on the local lexical context (words and lemmas). This system offers a hybrid detection of the scope, which combines expert rules, operating on syntactic dependency trees, with a ranking SVM that learns a discriminative ranking function over nodes in constituent trees. It was further improved by Read et al. (2012) and is used 4.1 ESSAI: FRENCH CORPUS"
W02-0304,1997.iwpt-1.3,0,0.01589,"Missing"
W02-0304,W98-1504,0,0.271191,"Missing"
W02-0304,A97-1016,0,\N,Missing
W02-0304,J95-4004,0,\N,Missing
W02-1403,P98-1082,0,0.0303293,". Various methods have been proposed to discover relations between terms (see Jacquemin and Bourigault (2002) for a review). We divide them into internal and external methods, in the same way as McDonald (1993) for proper names. Internal methods look at the constituency of terms, and compare terms based on the words they contain. Term matching can rely directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefenstette, 1994). The detection of appropriate syntactic patterns of cooccurrence is another method to uncover relations between terms in corpora (Hearst, 1992; Séguéla and Aussenac, 1999). In previous work we applied lexical methods to identify relations between terms on the basis on their content words, taking morphological variants into account. Our goal was then to assess the feasibility of such structuri"
W02-1403,C92-2082,0,0.020099,"directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefenstette, 1994). The detection of appropriate syntactic patterns of cooccurrence is another method to uncover relations between terms in corpora (Hearst, 1992; Séguéla and Aussenac, 1999). In previous work we applied lexical methods to identify relations between terms on the basis on their content words, taking morphological variants into account. Our goal was then to assess the feasibility of such structuring by studying it on an existing, hierarchically structured terminology. Ignoring this existing structure and starting from the set of its terms, we attempt to discover hierarchical term-toterm links and compare them with the preexisting relations. Our goal in the present paper is to analyze ‘new’ relations. ‘New’ means that these induced relati"
W02-1403,W93-0104,0,0.010883,"ckground Terminology structuring, i.e., organizing a set of terms through semantic relations, is one of the difficult issues that have to be addressed when building terminological resources. These relations include subsumption or hyperonymy (the is-a relation), meronymy (part-of and its variants), as well as other, diverse relations, sometimes called ‘transversal’ (e.g., cause, or the general see also). Various methods have been proposed to discover relations between terms (see Jacquemin and Bourigault (2002) for a review). We divide them into internal and external methods, in the same way as McDonald (1993) for proper names. Internal methods look at the constituency of terms, and compare terms based on the words they contain. Term matching can rely directly on raw word forms (Bodenreider et al., 2001), on morphological variants (Jacquemin and Tzoukermann, 1999), on syntactic structure (Bourigault, 1994; Jacquemin and Tzoukermann, 1999) or on semantic variants (synonyms, hyperonyms, etc.) (Hamon et al., 1998). External methods take advantage of the context in which terms occur: they examine the behavior of terms in corpora. Distributional methods group terms that occur in similar contexts (Grefen"
W02-1403,C98-1079,0,\N,Missing
W09-1311,W08-0507,0,0.0145304,"thesis, replication of mitochondrial DNA and mtDNA replication) convey the same or different meaning. This is particularly important for deciphering and computing semantic similarity between words and terms. 89 In our previous work, we proposed to use the existing biomedical terminologies (i.e., Gene Ontology (Gene Ontology Consortium, 2001), Snomed (Cˆot´e et al., 1997), UMLS (NLM, 2007)), wich provide complex terms, and to acquire from them lexical resources of synonyms. Indeed, the use of complex biomedical terms seems to be less suitable and generalizable as compared to lexical resources (Poprat et al., 2008). Within the biological area, we proposed to exploit the Gene Ontology (GO), and more specifically to exploit compositional structure of its terms (Hamon and Grabar, 2008). However, with the acquisition of synonymy we faced two problems: (1) contextual character of these relations (Cruse, 1986), i.e., two terms or words are considered as synonyms if they can occur within the Proceedings of the Workshop on BioNLP, pages 89–96, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics same context, which makes this relation more or less broad depending on the usage; (2) abil"
W09-1311,C04-1054,0,0.0744704,"depending on languages and domains, such resources are not equally well described. Morphological description is the most complete for both general (Burnage, 1990; Hathout et al., 2001) and biomedical (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003) languages. But the situation is not as successful at the semantic level: little synonym resources can be found. If WordNet (Fellbaum, 1998) proposes general language synonym relations for English, the corresponding resources for other languages are not freely available. Moreover, the initiative for fitting WordNet to the biomedical area (Smith and Fellbaum, 2004) seems to have been abandoned, although there is a huge need for this kind of resources. Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositi"
W09-1311,W08-0500,0,\N,Missing
W12-2413,E09-1005,0,0.024365,"e pagerank-derived algorithm (Brin and Page, 1998). When 109 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109–117, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics expansion component lipid head component storage expansion component head component retention (of) lipids Figure 1: Parsing tree of the terms lipid storage and retention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is done on resources provided by a specialized domain;"
W12-2413,P98-1082,1,0.639561,"answers of the system will be more or less exhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on"
W12-2413,D07-1061,0,0.0349937,"etention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is done on resources provided by a specialized domain; (2) the pagerank algorithm is exploited for the filtering of semantic relations generated with linguistically-based approaches; (3) the pagerank algorithm is adapted to the small size of the processed data. In the following of this paper, we present first the material (section 2), then the method we propose (section 3). We then describe the experiments performed and the results (section 4), as well as their"
W12-2413,P97-1004,0,0.103427,"m will be more or less exhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on the usage; (2) ability o"
W12-2413,C04-1162,0,0.0249817,"eight and to filter the synonym relations with the pagerank-derived algorithm (Brin and Page, 1998). When 109 Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109–117, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics expansion component lipid head component storage expansion component head component retention (of) lipids Figure 1: Parsing tree of the terms lipid storage and retention of lipids processing textual data, this algorithm has been previously applied in different contexts such as semantic disambiguation (Mihalcea et al., 2004; Sinha and Mihalcea, 2007; Agirre and Soroa, 2009), summarization (Fernandez et al., 2009) and, more recently, for the identification of synonyms (Sinha and Mihalcea, 2011). This last work takes into account the usage of a given word in corpora and its known synonyms from lexical resources. Other related works propose also the exploitation of the random walk algorithm for the detection of semantic relatedness of words (Gaume, 2006; Hughes and Ramage, 2007) and of documents (Hassan et al., 2007). Our work is different from the previous work in several ways: (1) the acquisition of synonymy is d"
W12-2413,C08-1100,0,0.0219499,"xhaustive. Several solutions may be exploited when deciphering the synonymy relations: 1. Exploitation of the existing resources in which the synonyms are already encoded. However, in the biomedical domain, such resources are not well described. If the morphological description is the most complete (NLM, 2007; Schulz et al., 1999; Zweigenbaum et al., 2003), little or no freely available synonym resources can be found, while the existing terminologies often lack the synonyms. 2. Exploitation and adaptation of the existing methods (Grefenstette, 1994; Hamon et al., 1998; Jacquemin et al., 1997; Shimizu et al., 2008; Wang and Hirst, 2011). 3. Proposition of new methods specifically adapted to the processed data. Due to the lack of resources, we propose to exploit the solutions 2 and 3. In either of these situations, the question arises about the robustness and the validity of the acquired relations. For instance, (Hamon and Grabar, 2008) face two problems: (1) contextual character of synonymy relations (Cruse, 1986), i.e., two words are considered as synonyms if they can occur within the same context, which makes this relation more or less broad depending on the usage; (2) ability of automatic tools to d"
W12-2413,C98-1079,1,\N,Missing
W12-2413,W06-3809,0,\N,Missing
W14-1116,pustejovsky-etal-2010-iso,0,0.209033,"lenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for identification of temporal relations and found that HeidelTime has the best combination of performance and adaptability. We propose to exploit this automatic systems,"
W14-1116,chang-manning-2012-sutime,0,0.0989398,"the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for iden"
W14-1116,strotgen-gertz-2012-temporal,0,0.392422,"Missing"
W14-1116,S13-2001,0,0.0470794,"xtraction tasks related to different kinds of contextual information. 1 https://www.i2b2.org/NLP 101 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101–105, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics documents from the test set are annotated to provide the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of te"
W14-1116,S07-1014,0,0.0144859,"ous I2B2 contests1 addressed the information extraction tasks related to different kinds of contextual information. 1 https://www.i2b2.org/NLP 101 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101–105, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics documents from the test set are annotated to provide the reference data for evaluation. Temporality has become an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013;"
W14-1116,P12-1077,0,0.0147441,"ecome an important research field in the NLP topics and several challenges addressed this taks: ACE (ACE challenge, 2004), SemEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), I2B2 2012 (Sun et al., 2013). We propose to continue working on the extraction of temporal information related to medical events. This kind of study relies on several important tasks when processing the narrative documents : identification and normalization of linguistic expressions that are indicative of the temporality (Verhagen et al., 2007; Chang and Manning, 2012; Str¨otgen and Gertz, 2012; Kessler et al., 2012), and their modelization and chaining (Batal et al., 2009; Moskovitch and Shahar, 2009; Pustejovsky et al., 2010; Sun et al., 2013; Grouin et al., 2013). The identification of temporal expressions provides basic knowledge for other tasks processing the temporality information. The existing available automatic systems such as HeidelTime (Str¨otgen and Gertz, 2012) or SUTIME (Chang and Manning, 2012) exploit rule-based approaches, which makes them adaptable to new data and areas. During a preliminary study, we tested several such systems for identification of temporal relations and found that He"
W14-1116,W04-3113,0,0.0286453,"Missing"
W14-1116,moriceau-tannier-2014-french,0,0.0127311,"2 3 Method HeidelTime is a cross-domain temporal tagger that extracts temporal expressions from documents and normalizes them according to the Timex3 annotation standard, which is part of the markup language TimeML (Pustejovsky et al., 2010). This is a rule-based system. Because the source code and the resources (patterns, normalization information, and rules) are strictly separated, it is possible to develop and implement resources for additional languages and areas using HeidelTime’s rule syntax. HeidelTime is provided with modules for processing documents in several languages, e.g. French (Moriceau and Tannier, 2014). In English, several versions of the system exist, such as general-language English and scientific English. HeidelTime uses different normalization strategies depending on the domain of the documents that are to be processed: news, narratives (e.g. Wikipedia articles), colloquial (e.g. SMS, tweets), and scientific (e.g. biomedical studies). The news strategy allows to fix the document creation date. This date is important for computing and normalizing the relative dates, such as two weeks ago or 5 days later, for which the reference point in time is necessary: if the document creation date is"
W14-1116,S10-1010,0,\N,Missing
W14-1202,S12-1067,0,0.0138193,"pecia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene (hydrogen sulfide)) and living organisms (Sapromyces, Acholeplasma laidlawii) that group very specific terms hardly known by laymen. The 104,649 selected terms are tokenized and segmented into words (or tokens) to obtain 29,641 unique words: trisulfure d’hydrog`ene gives three words"
W14-1202,S12-1066,0,0.0126059,"to the lexical simplification within the SemEval challenge in 20121 . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene ("
W14-1202,S12-1054,0,0.0251193,"Missing"
W14-1202,W07-1007,0,0.0353449,"t. Such studies can be used for filtering the terms extracted from specialized corpora (Korkontzelos et al., 2008). The features exploited include for instance the presence and the specificity of pivot words (Drouin and Langlais, 2006), the neighborhood of the term in corpus or the diversity of its components computed with statistical measures such as C-Value or PageRank (Daille, 1995; Frantzi et al., 1997; Maynard and Ananiadou, 2000). Another possibility is to check whether lexical units occur within reference terminologies and, if they do, they are considered to convey specialized meaning (Elhadad and Sutaria, 2007). 2.3 NLP studies The application of the readability measures is another way to evaluate the complexity of words and terms. Among these measures, it is possible to distinguish classical readability measures and computational readability measures (Franc¸ois, 2011). Classical measures usually rely on number of letters and/or of syllables a word contains and on linear regression models (Flesch, 1948; Gunning, 1973), while computational readability measures may involve vector models and a great variability of features, among which the following have been used to process the biomedical documents an"
W14-1202,S12-1069,0,0.0133027,"different features (Wang, 2006; ZengTreiler et al., 2007; Leroy et al., 2008). Specific task has been dedicated to the lexical simplification within the SemEval challenge in 20121 . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities"
W14-1202,S12-1068,0,0.0133474,"e substitutes according to how ”simple” they are (Specia et al., 2012). The participants applied rule-based and/or machine learning systems. Combinations of various features have been used: lexicon from spoken corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, latent semantic analysis, mutual information and word frequency (Jauhar and Specia, 2012); Wikipedia frequency, word length, n-grams of characters and of words, random indexing and syntactic complexity of documents (Johannsen et al., 2012); n-grams and frequency from Wikipedia, Google n-grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). 3 orders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. We keep here five axes related to the main medical notions (disorders, abnormalities, procedures, functions, anatomy). The objective is not to consider axes such as chemical products (trisulfure d’hydrog`ene (hydrogen sulfide)) and living organisms (Sapromyces, Acholeplasma laidlawii) that group very specific terms hardly known by laymen. The 104,649 selected terms are tokenized and segmented into words (or tokens) to obtain 29,641"
W14-1202,C00-1077,0,0.0836542,"l in terminology (W¨uster, 1981; Cabr´e and Estop`a, 2002; Cabr´e, 2000). The specificity of terms to a given field is usually studied. The notion of understandability can be derived from it. Such studies can be used for filtering the terms extracted from specialized corpora (Korkontzelos et al., 2008). The features exploited include for instance the presence and the specificity of pivot words (Drouin and Langlais, 2006), the neighborhood of the term in corpus or the diversity of its components computed with statistical measures such as C-Value or PageRank (Daille, 1995; Frantzi et al., 1997; Maynard and Ananiadou, 2000). Another possibility is to check whether lexical units occur within reference terminologies and, if they do, they are considered to convey specialized meaning (Elhadad and Sutaria, 2007). 2.3 NLP studies The application of the readability measures is another way to evaluate the complexity of words and terms. Among these measures, it is possible to distinguish classical readability measures and computational readability measures (Franc¸ois, 2011). Classical measures usually rely on number of letters and/or of syllables a word contains and on linear regression models (Flesch, 1948; Gunning, 197"
W14-1202,S12-1046,0,\N,Missing
W14-4812,W07-1007,0,0.121199,"ocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small corpus and several statistical association measures for building aligned lexicon with technical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarcti"
W14-4812,S12-1066,0,0.0433777,"on. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The fi"
W14-4812,S12-1054,0,0.174723,"Missing"
W14-4812,S12-1068,0,0.0563235,"word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed by their non-technical equivalents). The first initiative of the kind appeared with the collaborative effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the methods was applied to the most frequently occurring medical"
W14-4812,D12-1066,0,0.137719,"echnical terms from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other languages followed. In French, researchers proposed methods for the acquisition of syntactic variation (Del´eger and Zweigenbaum, 2008; Cartoni and Del´eger, 2011) from comparable specialized and non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of terminological variation (Hahn et al., 2001), synonymy (Fern´andez-Silva et al., 2011) and paraphrasing (Max et al., 2012) is also relevant to outline the topics. (4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink disease} (5) {consommation r´eguli`ere, consommer de fac¸on r´eguli`ere} (regular use), {gˆene a` la lecture, empˆeche de lire} (reading difficulty), {´evolution de l’affection, la maladie e´ volue} (evolution of the condition) (6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport en calories} (calorie supply), {calculer les doses, doses sont calcul´ees} (calculate the dose), {efficacit´e est renforc´ee, r"
W14-4812,S12-1069,0,0.091097,"eiler et al., 2007; Leroy et al., 2008; Franc¸ois and Fairon, 2013). • Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical simplification of texts in English has been addressed during the SemEval 2012 challengea . Given a short input text and a target word in English, and given several English substitutes for the target word that fit the context, the goal was to rank these substitutes according to how simple they are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual information and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length, n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012); n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word frequency (Amoia and Romanelli, 2012). • Dedicated resources. The building of resources suitable for performing the simplification is another related research topics. Such resources are mainly two-fold lexica in which specialized and non-specialized vocabularies are aligne"
W14-4814,W13-1617,0,0.0139078,"g edit distance (Levenshtein, 1966). This measure considers three operations: deletion, addition and substitution of characters. Each operations cost is set to 1. For instance, the Levenshtein distance between ambolie and embolie is 1, that corresponds to the substitution of a by e. The minimal length of the processed words should not be lesser than six characters, because with shorter words the propositions contain too much of errors. The motivation for this kind of processing is that it is possible and frequent to find misspelled words in real documents, especially in the forum discussions (Balahur, 2013). In both cases, the computed forms inherit the semantic type of the terms from the terminology. For instance, ambolie inherits the D D ISORDER semantic type of embolie. Besides, we also added the medication names from the Th´eriaque resource4 . These are assigned to the C C HEMICAL PRODUCTS semantic type. The whole resource contains 158,298 entries. 2.3 Resource with verbal forms We have built a resource with inflected forms of verbs: 177,468 forms for 1,964 verbs. The resource is built from the information available online5 . The resource contains simple (consulte, consultes, consultons (con"
W14-4814,C10-2013,0,0.0126126,"rovides 263 entries. 3 Method We first perform the description of verbs in a way similar to FS and then compare the observations made in the two corpora processed. The proposed method comprises three steps: corpora pre-processing (section 3.1), semantic annotation (section 3.2), and contrastive analysis of verbs (section 3.3). The method relies on some existing tools and on specifically designed Perl scripts. 3.1 Corpora pre-processing The corpora are collected online from the websites indicated above and properly formatted. The corpora are then analyzed syntactically using the Bonsai parser (Candito et al., 2010). Its output contains sentences segmented into syntactic chunks (e.g., NP, PP, VP) in which words are assigned parts of speech, as shown in the example that follows: Le traitement repose sur les d´eriv´es thiazidiques, plus accessibles, disponibles sous forme de m´edicaments g´en´eriques. (The treatment is based on thiazidic derivates, more easily accessible, and available as generic drugs.) ((SENT (NP (DET Le) (NC traitement)) (VN (V repose)) (PP (P sur) (NP (DET les) (NC 4 5 http://www.theriaque.org/ http://leconjugueur.lefigaro.fr/frlistedeverbe.php 117 d´eriv´es) (AP (ADJ thiazidiques) (CO"
W14-4814,koeva-2010-lexicon,0,0.0216755,"f (L’Homme, 1998; Lerat, 2002). Rather, being a predicative unit that involves participants called arguments, the verb can be specialized or not, depending on its argumental structure and the nature of these arguments. In our study, the description of verbs is similar to the one performed in Frame Semantics (FS) (Fillmore, 1982), since we provide semantic information about the verbs co-occurents. The Frame Semantics framework is increasingly used for the description of lexical units in different languages (Atkins et al., 2003; Pad´o and Pitel, 2007; Burchardt et al., 2009; Borin et al., 2010; Koeva, 2010) and specialized fields (Dolbey et al., 2006; Schmidt, 2009; Pimentel, 2011). Among other things, Frame Semantics provides for a full description of the semantic and syntactic properties of lexical units. FS puts forward the notion of ”frames”, which are defined as conceptual scenarios that underlie lexical realizations in language. A frame comprises a frame evoking lexical units (ULs) and the Frame Elements (FEs), which represent the participants to the verbal process. For instance, in FrameNet (Ruppenhofer et al., 2006), the frame CURE is described as a situation that involves some specific"
W14-4814,2007.jeptalnrecital-long.25,0,0.133159,"Missing"
W18-5610,E09-1003,0,0.0328945,"of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translat"
W18-5610,S13-1004,0,0.0212142,"atement) and not to compute their similarity score, and (3) we work with data in French which were not exploited for this kind of task yet. To our knowledge, the only work which exploited articles from French encyclopedia performed manual alignment of sentences (Brouwers et al., 2014). In relation with monolingual comparable corpora, the main difficulty is that sentences may show low lexical overlap but be nevertheless parallel. Recently, this task gained in popularity thanks to the semantic text similarity (STS) initiative. Dedicated SemEval competitions have been proposed for several years (Agirre et al., 2013, 2015, 2016). The objective, for a given pair of sentences, is to predict if they are semantically similar and to assign similarity score going from 0 (independent semantics) to 5 (semantic equivalence). This task is usually explored in general-language corpora. Among the exploited methods, we can notice: • lexicon-based methods which rely on similarity of subwords or words from the processed texts or on machine translation (Madnani et al., 2012). The features exploited can 84 In what follows, we first present the linguistic material used, and the methods proposed. We then present and discuss"
W18-5610,W03-1004,0,0.418126,"Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). These methods and types of features can of course be combined for optimizing the results (Bjerva et al., 2014; Lai and Hockenmaier, 2014; Zhao et al., 2014; Rychalska et al., 2016; Severyn et al., 2013; Kiros et al., 2015; He et al., 2015; Tsubaki et al., 2016; Mueller and Thyagarajan, 2016). Our objective is close to the second type of works: we want to detect and align parallel sentences from monologual comparable corpora. Yet, there are some differences: (1) we work with corpora rel"
W18-5610,S14-2114,0,0.0584966,"Missing"
W18-5610,W04-3208,0,0.0192424,"of simplification is to transform documents in order to make them easier 2 Existing Work In parallel corpora, sentence alignment can rely on empirical information, such as relative length of the sentences in each language (Gale and Church, 1993), or lexical information (Chen, 1993). In comparable corpora, both monolingual and bilingual, sentences present relatively loose common semantics and do not necessarily occur in the same order. It should also be noted that (1) the degree of parallelism can vary from nearly parallel corpora, with a lot of parallel sentences, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2"
W18-5610,J93-1004,0,0.584816,"simplified documents are typically created for children (Son et al., 1008; De Belder and Moens, 2010; Vu et al., 2014), people with low literacy or foreigners (Paetzold and Specia, 2016), people with mental or neurodegenerative disorders (Chen et al., 2016), or laypeople who face specialized documents (Arya et al., 2011; Leroy et al., 2013). In the second case, the purpose of simplification is to transform documents in order to make them easier 2 Existing Work In parallel corpora, sentence alignment can rely on empirical information, such as relative length of the sentences in each language (Gale and Church, 1993), or lexical information (Chen, 1993). In comparable corpora, both monolingual and bilingual, sentences present relatively loose common semantics and do not necessarily occur in the same order. It should also be noted that (1) the degree of parallelism can vary from nearly parallel corpora, with a lot of parallel sentences, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th I"
W18-5610,N13-1092,0,0.0852407,"Missing"
W18-5610,W14-1206,0,0.0166865,"ajan, 2016). Our objective is close to the second type of works: we want to detect and align parallel sentences from monologual comparable corpora. Yet, there are some differences: (1) we work with corpora related to the biomedical area and not to the general language, (2) we have to state if two sentences have to be aligned (binary statement) and not to compute their similarity score, and (3) we work with data in French which were not exploited for this kind of task yet. To our knowledge, the only work which exploited articles from French encyclopedia performed manual alignment of sentences (Brouwers et al., 2014). In relation with monolingual comparable corpora, the main difficulty is that sentences may show low lexical overlap but be nevertheless parallel. Recently, this task gained in popularity thanks to the semantic text similarity (STS) initiative. Dedicated SemEval competitions have been proposed for several years (Agirre et al., 2013, 2015, 2016). The objective, for a given pair of sentences, is to predict if they are semantically similar and to assign similarity score going from 0 (independent semantics) to 5 (semantic equivalence). This task is usually explored in general-language corpora. Am"
W18-5610,P12-1091,0,0.0840366,"Missing"
W18-5610,D15-1181,0,0.0166393,"ication (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). These methods and types of features can of course be combined for optimizing the results (Bjerva et al., 2014; Lai and Hockenmaier, 2014; Zhao et al., 2014; Rychalska et al., 2016; Severyn et al., 2013; Kiros et al., 2015; He et al., 2015; Tsubaki et al., 2016; Mueller and Thyagarajan, 2016). Our objective is close to the second type of works: we want to detect and align parallel sentences from monologual comparable corpora. Yet, there are some differences: (1) we work with corpora related to the biomedical area and not to the general language, (2) we have to"
W18-5610,W11-1209,0,0.112549,"elative length of the sentences in each language (Gale and Church, 1993), or lexical information (Chen, 1993). In comparable corpora, both monolingual and bilingual, sentences present relatively loose common semantics and do not necessarily occur in the same order. It should also be noted that (1) the degree of parallelism can vary from nearly parallel corpora, with a lot of parallel sentences, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are"
W18-5610,N09-2045,0,0.0603209,"Missing"
W18-5610,P93-1002,0,0.290191,"children (Son et al., 1008; De Belder and Moens, 2010; Vu et al., 2014), people with low literacy or foreigners (Paetzold and Specia, 2016), people with mental or neurodegenerative disorders (Chen et al., 2016), or laypeople who face specialized documents (Arya et al., 2011; Leroy et al., 2013). In the second case, the purpose of simplification is to transform documents in order to make them easier 2 Existing Work In parallel corpora, sentence alignment can rely on empirical information, such as relative length of the sentences in each language (Gale and Church, 1993), or lexical information (Chen, 1993). In comparable corpora, both monolingual and bilingual, sentences present relatively loose common semantics and do not necessarily occur in the same order. It should also be noted that (1) the degree of parallelism can vary from nearly parallel corpora, with a lot of parallel sentences, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text"
W18-5610,P02-1020,0,0.134433,"lot of parallel sentences, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resourc"
W18-5610,2007.tmi-papers.12,0,0.0699848,"urce graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012),"
W18-5610,S14-2055,0,0.101772,"cific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the s"
W18-5610,S16-1091,0,0.0141436,"), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). These methods and types of features can of course be combined for optimizing the results (Bjerva et al., 2014; Lai and Hockenmaier, 2014; Zhao et al., 2014; Rychalska et al., 2016; Severyn et al., 2013; Kiros et al., 2015; He et al., 2015; Tsubaki et al., 2016; Mueller and Thyagarajan, 2016). Our objective is close to the second type of works: we want to detect and align parallel sentences from monologual comparable corpora. Yet, there are some differences: (1) we work with corpora related to the biomedical area and not to the general language, (2) we have to state if two sentences have to be aligned (binary statement) and not to compute their similarity score, and (3) we work with data in French which were not exploited for this kind of task yet. To our knowledge, the"
W18-5610,N12-1019,0,0.0210527,"ask gained in popularity thanks to the semantic text similarity (STS) initiative. Dedicated SemEval competitions have been proposed for several years (Agirre et al., 2013, 2015, 2016). The objective, for a given pair of sentences, is to predict if they are semantically similar and to assign similarity score going from 0 (independent semantics) to 5 (semantic equivalence). This task is usually explored in general-language corpora. Among the exploited methods, we can notice: • lexicon-based methods which rely on similarity of subwords or words from the processed texts or on machine translation (Madnani et al., 2012). The features exploited can 84 In what follows, we first present the linguistic material used, and the methods proposed. We then present and discuss the results obtained, and conclude with directions of future work. 3 Wikipedia articles are considered as technical texts while Vikidia articles are considered as their simplified versions (they are created for children 8 to 13 year old). Similarly to the works done in English, we associate Vikidia with Simple Wikipedia5 . Only articles related to the medical portal are exploited in this work. These encyclopedia articles have been downloaded in A"
W18-5610,P13-2125,0,0.0318162,"the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word"
W18-5610,W02-1037,0,0.100899,"between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and X"
W18-5610,J05-4003,0,0.0931846,"emantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘"
W18-5610,W13-5634,0,0.0347663,"Missing"
W18-5610,P06-1011,0,0.0603378,"ted can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using cla"
W18-5610,P15-1150,0,0.0755577,"Missing"
W18-5610,E06-1021,0,0.221838,"that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resour"
W18-5610,L16-1491,0,0.0202155,"and 0.73 F-measure, respectively. 1 Introduction The purpose of text simplification is to provide simplified versions of texts, in order to remove or replace difficult words or information. Simplification can be concerned with different linguistic aspects, such as lexicon, syntax, semantics, pragmatics and even document structure. Simplification can address needs of people or NLP applications (Brunato et al., 2014). In the first case, simplified documents are typically created for children (Son et al., 1008; De Belder and Moens, 2010; Vu et al., 2014), people with low literacy or foreigners (Paetzold and Specia, 2016), people with mental or neurodegenerative disorders (Chen et al., 2016), or laypeople who face specialized documents (Arya et al., 2011; Leroy et al., 2013). In the second case, the purpose of simplification is to transform documents in order to make them easier 2 Existing Work In parallel corpora, sentence alignment can rely on empirical information, such as relative length of the sentences in each language (Gale and Church, 1993), or lexical information (Chen, 1993). In comparable corpora, both monolingual and bilingual, sentences present relatively loose common semantics and do not necessar"
W18-5610,N09-2024,0,0.0209502,"d Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). These m"
W18-5610,W06-1603,0,0.0463929,"corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersect"
W18-5610,2007.mtsummit-papers.50,0,0.023435,"ited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA), topics modelling, word embeddings, etc. (Barzilay and Elhadad, 2003; Guo and Diab, 2012; Zhao et al., 2014; Kiros et al., 2015; He et al., 2015; Mueller and Thyagarajan, 2016). These methods and types of features can of course be combined for optimizing the results (Bjerva et al., 2014; Lai and Hockenmaier, 2014; Zhao et al., 2014; Rychalska et al., 2016; Severyn et al., 2013; Kiros et al., 2015; He et al., 2015; Tsubaki et al., 2016; M"
W18-5610,P03-1010,0,0.118293,"ligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence of synonyms, hyperonyms or antonyms (Mihalcea et al., 2006; Fernando and Stevenson, 2008; Lai and Hockenmaier, 2014); 1. detection of comparable documents using for instance generative models (Zhao and Vogel, 2002) or similarity scores (Utiyama and Isahara, 2003; Fung and Cheung, 2004); 2. detection of candidate sentences, or subphrastic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituent"
W18-5610,P08-1040,0,0.069527,"Missing"
W18-5610,W16-3411,0,0.0564752,"Missing"
W18-5610,U06-1019,0,0.0222984,"tic segments, for the alignment using for instance cross-lingual information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2006), sequence alignment trees (Munteanu and Marcu, 2002), mutual translations (Munteanu and Marcu, 2005; Kumano et al., 2007; Abdul-Rauf and Schwenk, 2009), or dynamic programming (Yang and Li, 2003); • syntax-based methods which exploit the syntactic modelling of sentences. The features often exploited are: syntactic categories, syntactic overlap, syntactic dependencies and constituents, predicat-argument relations, edition distance between syntactic trees (Wan et al., 2006; Severyn et al., 2013; Tai et al., 2015; Tsubaki et al., 2016); 3. filtering and selection of correct extractions using classification (Munteanu and Marcu, 2005; Tillmann and Xu, 2009; Hewavitharana and Vogel, 2011; S, tef˘anescu et al., 2012), similarity measure of translations (Fung and Cheung, 2004; Hewavitharana and Vogel, 2011), error rate (Abdul-Rauf and Schwenk, 2009), generative models (Zhao and Vogel, 2002; Quirk et al., 2007), or specific rules (Munteanu and Marcu, 2002; Yang and Li, 2003). • corpus-based methods which exploit distributional methods, latent semantic analysis (LSA),"
W18-5610,U05-1023,0,0.0622713,"nces, to verynon-parallel corpora (Fung and Cheung, 2004); and that (2) such corpora can contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the"
W18-5610,S14-2044,0,0.0665485,"Missing"
W18-5610,C10-1152,0,0.156623,"contain parallel information at various degrees of granularity, such as documents, sentences or sub-phrastic segments (Hewavitharana and Vogel, 2011). Detection of 83 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 83–93 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics be: lexical overlap, sentence length, string edition distance, numbers, named entities, the longest common substring (Clough et al., 2002; Zhang and Patrick, 2005; Qiu et al., 2006; Zhao et al., 2014; Nelken and Shieber, 2006; Zhu et al., 2010); parallel sentences in comparable corpora is thus a substantial challenge and requires specific methods. Several existing works are related to machine translation: bilingual comparable corpora are exploited for creation of parallel and aligned corpora. Usually, these methods rely on three steps: • knowledge-based methods which exploit external resources, such as WordNet (Miller et al., 1993) or PPDB (Ganitkevitch et al., 2013). The features exploited can be: overlap with external resources, distance between the synsets, intersection of synsets, semantic similarity of resource graphs, presence"
W18-5610,2012.eamt-1.37,0,0.048163,"Missing"
W18-5614,P16-1047,0,0.0249628,"cal cases may be very similar to clinical documents: it describes patients, and proposes their diagnosis based on examination, imaging, and biological and genetic information. Besides, numerical values and abbreviations are also present. Misspellings, which are quite frequent in clinical documents, may be missing in publications on clinical cases. 3.2 • Negation. Negation indicates whether a given disorder, procedure or treatment are present or not in the medical history and care of a given patient. For this reason, its annotation and detection are important. We adopt the approach proposed by Fancellu et al. (2016) and adapted for French by Dalloux et al. (2018) based on Machine Learning techniques trained on annotated data. This follows a two-step process: (1) the negation markers are detected with a specifically trained CRF; (2) the scope of each detected marker is found with a neural network (BiLSTM with a CRF layer). On the French and English data tested, the detection of negation gives up to 0.98 for the cues and 0.86 for their scope; Annotation of the corpus Currently, the corpus contains linguistic and semantic annotations. At the linguistic level, the corpus is PoS-tagged and lemmatized with a t"
W18-5614,W15-2604,0,0.230688,"Missing"
W18-5614,W08-0606,0,0.0420645,"uestions. Other portals may also provide access to scientific literature following specific purposes, like indexing of reliable literature, such as proposed by HON (Boyer et al., 1997), CISMEF (Darmoni et al., 1999), and other similar initiatives (Risk and Dzenowagis, 2001). Thanks to some research works, there are also scientific corpora which provide precise annotations and categorizations. These are mainly built for the purposes of challenges (Kelly et al., 2013; Goeuriot et al., 2014) but may also be provided from works of researchers, such as POS-tag (Tsuruoka et al., 2005) and negation (Szarvas et al., 2008) annotated corpora. As for clinical corpora, they are related to hospital and clinical events of patients. Such corpora typically describe medical history of patients and the medical care they are undergoing. It is complicated to obtain free access to this kind of medical data and, for this reason, there are very few clinical corpora freely available for the research. In our work, we are mainly interested in clinical corpora: the proposed literature review of the existing work is aimed at clinical corpora which are freely available for the research. We present here the main existing clinical c"
W18-7002,P11-2087,0,0.02434,"be used for research purposes. Finally, these sources provide comparable corpora, distinguished by their technicality, on different topics: medical topics in encyclopedia, various drugs in drug leaflets, and questions related to treatment and diagnosis of disorders in Cochrane summaries. A part of these data have been aligned manually at the level of sentences (Section 4). Comparable corpora of this kind are also available, among which the most frequently used is the pair built with English Wikipedia1 and English Simple Wikipedia2 . This corpus is widely used by researchers (Zhu et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011). A similar comparable corpus also exists in French and can be built fromq French Wikipedia3 and Vikidia4 , which has been created for children. This source in French has been used for the detection of rules for syntactic transformations (Brouwers et al., 2012). Besides, researchers working on English also exploit history of revisions of articles from Simple Wikipedia (Yatskar et al., 2010), simplified versions of scientific articles5 (Elhadad and Sutaria, 2007), simplified versions of novels 6 (Vajjala and Meurers, 2015), as well as simplified versions of educationa"
W18-7002,klerke-sogaard-2012-dsim,0,0.0191182,"-processing step of documents undergoing other NLP treatments: syntactic analysis (Chandrasekar and Srinivas, 1997; Jonnalagadda et al., 3 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 3–9, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics 3 they can be exploited for simplification work. Several parallel corpora for several languages have been created, mainly thanks to the manual simplification of their contents: Spanish (Bott et al., 2014), Italian (Brunato et al., 2014), Brazilian Portuguese (Caseli et al., 2009), Danish (Klerke and Sgaard, 2012), and of course English (Chandrasekar and Srinivas, 1997; Daelemans et al., 2004; Petersen and Ostendorf, 2007; Specia et al., 2012). Yet, these parallel corpora are seldom freely available. Some of these corpora also explicitly indicate what has been simplified and how (removal, segmentation...). Hence, a multi-axial annotation schema has been proposed for this purpose with several simplification classes: split, merge, reorder, insert (verbs, subjects and other components), delete (verbs, subjects and other components), transform (lexical substitution, replacement of anaphora, nounverb, verb-"
W18-7002,C96-2183,0,0.513734,"pecialized and plain languages. A subset of this corpus has been processed manually in order to find and align parallel sentences. This subset currently contains 663 pairs with parallel sentences. Alignment has been done by two annotators and shows 0.76 inter-annotator agreement. The corpus with comparable data is available for research (http://natalia. grabar.free.fr/resources.php). 1 2 Introduction Corpora for Simplification If the first works in development of simplification tools have mainly relied on manually crafted simplification rules following the linguistic intuition of researchers (Chandrasekar et al., 1996; Siddharthan, 2006; Max, 2008), recent works are mostly guided by linguistic data and rely on dedicated corpora. Most often, parallel corpora are exploited in this task. They provide original texts together with their simplified versions. Sometimes, aligned corpora are also available, in which the correspondence is done at the level of sentences. This kind of corpora provide direct correspondence between complex and simple (or simplified) sentences. Notice that comparable corpora, containing complex and simple documents addressing the same topics, are more easily available but require specifi"
W18-7002,L16-1491,0,0.0569235,"e (or simplified) sentences. Notice that comparable corpora, containing complex and simple documents addressing the same topics, are more easily available but require specific methods or pre-processings before Research performed in text simplification provides tools and resources for the creation of simplified versions of texts. Simplification can be positioned at different levels (ie. lexical, syntactic, semantic, pragmatic and structural). It can be useful for different kinds of human users: children (Son et al., 2008; De Belder and Moens, 2010; Vu et al., 2014), foreigners or poor-readers (Paetzold and Specia, 2016), people with neurodegenerative disorders (Chen et al., 2016), lay people reading specialized documents (Arya et al., 2011; Leroy et al., 2013). In these cases, simplification may guarantee a better access to the contents of documents. Simplification may also be exploited as a pre-processing step of documents undergoing other NLP treatments: syntactic analysis (Chandrasekar and Srinivas, 1997; Jonnalagadda et al., 3 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 3–9, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics 3 they ca"
W18-7002,P11-2117,0,0.0421027,"purposes. Finally, these sources provide comparable corpora, distinguished by their technicality, on different topics: medical topics in encyclopedia, various drugs in drug leaflets, and questions related to treatment and diagnosis of disorders in Cochrane summaries. A part of these data have been aligned manually at the level of sentences (Section 4). Comparable corpora of this kind are also available, among which the most frequently used is the pair built with English Wikipedia1 and English Simple Wikipedia2 . This corpus is widely used by researchers (Zhu et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011). A similar comparable corpus also exists in French and can be built fromq French Wikipedia3 and Vikidia4 , which has been created for children. This source in French has been used for the detection of rules for syntactic transformations (Brouwers et al., 2012). Besides, researchers working on English also exploit history of revisions of articles from Simple Wikipedia (Yatskar et al., 2010), simplified versions of scientific articles5 (Elhadad and Sutaria, 2007), simplified versions of novels 6 (Vajjala and Meurers, 2015), as well as simplified versions of educational and news articles7 . 3.1"
W18-7002,W07-1007,0,0.0511809,"th English Wikipedia1 and English Simple Wikipedia2 . This corpus is widely used by researchers (Zhu et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011). A similar comparable corpus also exists in French and can be built fromq French Wikipedia3 and Vikidia4 , which has been created for children. This source in French has been used for the detection of rules for syntactic transformations (Brouwers et al., 2012). Besides, researchers working on English also exploit history of revisions of articles from Simple Wikipedia (Yatskar et al., 2010), simplified versions of scientific articles5 (Elhadad and Sutaria, 2007), simplified versions of novels 6 (Vajjala and Meurers, 2015), as well as simplified versions of educational and news articles7 . 3.1 Encyclopedia Articles This source provides articles from two collaborative encyclopedia in French available online: Wikipedia and Vikidia. French Wikipedia is intended for French-speaking people, while Vikidia has been created for providing similar information for 8 to 13 year old children. These two encyclopedia provide articles on a great variety of topics: politics, economics, medecine, culture, geography, etc. Wikipedia shows a better coverage than Vikidia:"
W18-7002,W04-3208,0,0.0203405,"ter-rater agreement is 0.76 (Cohen, 1960). It is computed within the set of the aligned sentences from the two annotators. Such inter-annotator agreement is qualified as substantial according to the usual interpretation scale (Landis and Koch, 1977) and may indicate a good reliability of the obtained data. Another interesting point is related to the parallelism between the technical and simple versions of documents. It has been indeed observed that the degree of parallelism in comparable corpora may vary from almost parallel corpora, with many parallel sentences, to very-non-parallel corpora (Fung and Cheung, 2004). In the CLEAR corpus, we can observe that aligned sentences are rarer in the Drugs and Encylopedia corpora than in the Cochrane corpus. Indeed, these three sources have different principles involved during the creation of their contents: 3. The meaning of one sentence can be fully included in another sentence. This is the case of semantic inclusion. In the following example, the content of the simplified sentence is included in the technical sentence: • We found no studies that reported the effect of whole grain diets on total cardiovascular mortality or cardiovascular events (total myocardia"
W18-7002,N09-2045,0,0.0805825,"Missing"
W18-7002,W13-5634,0,0.0583486,"Missing"
W18-7002,P08-1040,0,0.0922669,"Missing"
W18-7002,W16-3411,0,0.0569974,"Missing"
W18-7002,N10-1056,0,0.0270358,"available, among which the most frequently used is the pair built with English Wikipedia1 and English Simple Wikipedia2 . This corpus is widely used by researchers (Zhu et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011). A similar comparable corpus also exists in French and can be built fromq French Wikipedia3 and Vikidia4 , which has been created for children. This source in French has been used for the detection of rules for syntactic transformations (Brouwers et al., 2012). Besides, researchers working on English also exploit history of revisions of articles from Simple Wikipedia (Yatskar et al., 2010), simplified versions of scientific articles5 (Elhadad and Sutaria, 2007), simplified versions of novels 6 (Vajjala and Meurers, 2015), as well as simplified versions of educational and news articles7 . 3.1 Encyclopedia Articles This source provides articles from two collaborative encyclopedia in French available online: Wikipedia and Vikidia. French Wikipedia is intended for French-speaking people, while Vikidia has been created for providing similar information for 8 to 13 year old children. These two encyclopedia provide articles on a great variety of topics: politics, economics, medecine,"
W18-7002,C10-1152,0,0.265846,"reviews), and can be used for research purposes. Finally, these sources provide comparable corpora, distinguished by their technicality, on different topics: medical topics in encyclopedia, various drugs in drug leaflets, and questions related to treatment and diagnosis of disorders in Cochrane summaries. A part of these data have been aligned manually at the level of sentences (Section 4). Comparable corpora of this kind are also available, among which the most frequently used is the pair built with English Wikipedia1 and English Simple Wikipedia2 . This corpus is widely used by researchers (Zhu et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011). A similar comparable corpus also exists in French and can be built fromq French Wikipedia3 and Vikidia4 , which has been created for children. This source in French has been used for the detection of rules for syntactic transformations (Brouwers et al., 2012). Besides, researchers working on English also exploit history of revisions of articles from Simple Wikipedia (Yatskar et al., 2010), simplified versions of scientific articles5 (Elhadad and Sutaria, 2007), simplified versions of novels 6 (Vajjala and Meurers, 2015), as well as simplified ve"
W18-7003,2016.jeptalnrecital-long.12,1,0.758541,"een, in relation with the history of surgery of hallux valgus. Absence of arthrosynovitis at the level of metatarsophalangeal articulations. 2 Material Two short excerpts of deidentified clinical documents are used: summary discharge in cardiology and radiology report of feet and hands. These texts are used in two versions: original (technical) and manually simplified (see Figure 2). Due to the experimental setting of eye-tracking experiments, the texts used are short: 48 words in text1 and 112 words text2 . For the simplification, we use automatically built resources (Grabar and Hamon, 2014; Antoine and Grabar, 2016), which provide pairs of equivalent terms such as {myocard; heart muscle}, {desmorrhexy; rupture of ligaments}, and pairs of hyperonyms such as {metatarsophalangeal→foot}. Synonyms and paraphrases are used in priority, and hyperonyms are used when synonyms and paraphrases are not available. The simplification is typically done for words and terms which have been judged as nonunderstandable in previous research, for which almost 30,000 medical words from the UMLS (Lindberg et al., 1993) and Snomed International (Cˆot´e, 1996) terms have been manually categorized as understandable or non-underst"
W18-7003,P11-2087,0,0.0239375,"relation with the history of surgery of foot deformation. Absence of inflammation of the membrane at the level of foot articulations. Figure 2: Translated examples with original (upper) and simplified (lower) texts. 2. On the other side, health documents show a given readability level and can be more or less difficult to read and to understand. Here, the purpose is to diagnose the difficulty of information and to make this information more easily accessible for laypeople. Typically, this process is addressed by researchers in NLP for the readability diagnosis and for the text simplification (Biran et al., 2011; Brouwers et al., 2012; Glavas and Stajner, 2015). • testset1 : original text1 and simplified text2 , • testset2 : simplified text1 and original text2 . Figure 2 presents the English translation of the text1 in the original and simplified versions. Our work is related to the second aspect: diagnosis of text readability. 3 In what follows, we first present the material used (Section 2) and the protocol of the approach (Section 3) to reach the objectives. Section 4 is dedicated to the description and discussion of the results obtained, and Section 5 draws the conclusion and proposes some issues"
W18-7003,P15-2011,0,0.0151751,"ot deformation. Absence of inflammation of the membrane at the level of foot articulations. Figure 2: Translated examples with original (upper) and simplified (lower) texts. 2. On the other side, health documents show a given readability level and can be more or less difficult to read and to understand. Here, the purpose is to diagnose the difficulty of information and to make this information more easily accessible for laypeople. Typically, this process is addressed by researchers in NLP for the readability diagnosis and for the text simplification (Biran et al., 2011; Brouwers et al., 2012; Glavas and Stajner, 2015). • testset1 : original text1 and simplified text2 , • testset2 : simplified text1 and original text2 . Figure 2 presents the English translation of the text1 in the original and simplified versions. Our work is related to the second aspect: diagnosis of text readability. 3 In what follows, we first present the material used (Section 2) and the protocol of the approach (Section 3) to reach the objectives. Section 4 is dedicated to the description and discussion of the results obtained, and Section 5 draws the conclusion and proposes some issues for the future work. All experiments are performe"
W18-7003,L16-1420,1,0.928571,"rovide pairs of equivalent terms such as {myocard; heart muscle}, {desmorrhexy; rupture of ligaments}, and pairs of hyperonyms such as {metatarsophalangeal→foot}. Synonyms and paraphrases are used in priority, and hyperonyms are used when synonyms and paraphrases are not available. The simplification is typically done for words and terms which have been judged as nonunderstandable in previous research, for which almost 30,000 medical words from the UMLS (Lindberg et al., 1993) and Snomed International (Cˆot´e, 1996) terms have been manually categorized as understandable or non-understandable (Grabar and Hamon, 2016). Overall, the text1 has undergone seven modifications, and the text2 ten modifications. After the simplification, text1 contains 65 words and text2 82 words. As a matter of fact, text1 has become longer because its original version contains several compoundings which simplification requires paraphrasing with several words. These texts are used to build two testsets, in which the order of technical and simplified texts varies: EXAM: SONOGRAPHY OF HANDS AND FEET REASON: Pain in articulations Hands: Inflammation of tendons or of articulation membrane cannot be observed. Forefoot: Interesting reo"
W18-7003,W16-4123,0,0.0167766,"ovements and the reading, such as: relation between speech and eye movements, when participants are looking at picture segments which correspond to the sentences they are hearing (Cooper, 1974; Tanenhaus et al., 1995; Wendt et al., 2014); reading of texts in first and second languages (Altarriba et al., 1996; Bisson et al., 2014); reading of texts by dyslexic people (Rubino and Minden, 1973; Elterman et al., 1980; Nilsson Benfatto et al., 2016) and autists (Yaneva et al., 2015); processing of syntactic structures (Frenck-Mestre and Pynte, 1997; Clifton and Staub, 2011; Trueswell et al., 1994; Singh et al., 2016); detection and processing of errors (Keating, 2009); evaluation of text complexity during the translation (Sharmin et al., 2008) and language acquisition (Balakrishna, 2015). Usually, in relation with understanding of texts, two closely related aspects are distinguished (Figure 1): text to another. When the text is easy to read and understand, saccades are longer, and they become shorter when the text is complicated because readers need more time for reading; • Fixations are periods during which the eyes are stable. Fixations correspond to moments when visual information is analyzed. Duration"
W19-5011,S16-1160,0,0.0139093,"g which are important factors in readability, such as cohesion, syntactic ambiguity, rhetorical organization, and propositional density (Collins-Thompson, 2014). Moreover, traditional readability measures were demonstrated to be unreliable for non-traditional documents (Si and P. Callan, 2001). As a result of such limitations and due to the recent growth of computational and data resources, the focus of NLP researchers moved to computational readability measurements, which rely on the use of machine learning algorithms on richer linguistic features (Malmasi et al., 2016; Ronzano et al., 2016; Bingel et al., 2016). 3 Materials For the experiments, we used the publicly available set of words with annotations1 . The process of words collection and annotation is briefly described below. 3.1 Linguistic data description The set of required biomedical terms was obtained from the French part of Snomed International2 (Cˆot´e et al., 1993). Snomed Int contains 151,104 medical terms organized into eleven semantic axes such as disorders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. For the word understandability study, five axes related to the main medical notion"
W19-5011,S16-1157,0,0.0706934,"Missing"
W19-5011,goeuriot-etal-2008-characterization,1,0.489302,"s (acn´e (acne), fragment (fragment)). Not so much effort has been devoted to the exploitation of NLP potential in the measurement of readability of medical texts. In the biomedical domain, as well as in general language, the readability assessment is currently approached as a classification task. The difference is that in the former a much smaller variety of features has been tested: a combination of classical readability formulas with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters (Poprat et al., 2006); stylistic (Grabar et al., 2007) or discursive (Goeuriot et al., 2008) features; morphological features (Chmielik and Grabar, 2011); combinations of different features from those listed above (Zeng-Treiler et al., 2007). Among the recent experiments dedicated to readability study in the medical domain are, for example, manual rating of medical words (Zheng et al., 2002), automatic rating of medical words on the basis of their presence in different vocabularies (Borst et al., 2008), exploitation of machine learning approach with various features (Grabar et al., 2014). The last experiment achieved up to 85.0 F-score on individual annotations. 1 http://natalia.grab"
W19-5011,W14-1202,1,0.896377,"Missing"
W19-5011,S16-1154,0,0.0256188,"xt, ignoring deeper levels of text processing which are important factors in readability, such as cohesion, syntactic ambiguity, rhetorical organization, and propositional density (Collins-Thompson, 2014). Moreover, traditional readability measures were demonstrated to be unreliable for non-traditional documents (Si and P. Callan, 2001). As a result of such limitations and due to the recent growth of computational and data resources, the focus of NLP researchers moved to computational readability measurements, which rely on the use of machine learning algorithms on richer linguistic features (Malmasi et al., 2016; Ronzano et al., 2016; Bingel et al., 2016). 3 Materials For the experiments, we used the publicly available set of words with annotations1 . The process of words collection and annotation is briefly described below. 3.1 Linguistic data description The set of required biomedical terms was obtained from the French part of Snomed International2 (Cˆot´e et al., 1993). Snomed Int contains 151,104 medical terms organized into eleven semantic axes such as disorders and abnormalities, procedures, chemical products, living organisms, anatomy, social status, etc. For the word understandability study,"
W19-5013,S13-2056,0,0.0882014,"Missing"
W19-5013,hamon-etal-2017-pomelo,1,0.891174,"Missing"
W19-5029,J92-4003,0,0.506649,"Missing"
W19-5029,P10-1052,0,0.120129,"Missing"
W19-5029,L18-1201,0,0.0614524,"Missing"
W19-5029,W18-5614,1,0.510736,"ized domains (e.g., clinical notes or justice decisions) are not easily accessible unless authorization (Chapman et al., 2011). 273 Proceedings of the BioNLP 2019 workshop, pages 273–282 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 2.1 Corpus and annotation guidelines work, we only focus on the clinical case description. This set has been manually annotated with general and fine-grained information, which is described in the two following sections. This corpus is part of a larger and yet growing corpus, which currently contains over 4,100 clinical cases (Grabar et al., 2018). Corpus In the clinical domain, in order to overcome the privacy and ethical issues when working on electronic health records, one solution consists in using clinical case reports. Indeed, it is quite common to find freely available publications from scientific journals which report in clinical cases of real de-identified or fake patients. Such clinical cases are usually published and discussed to improve medical knowledge (Atkinson, 1992) of colleagues and medical students. One may find scientific journals specifically dedicated to case reports, such as the Journal of Medical Case Reports la"
W19-5029,W11-0411,1,0.539433,"Recall and Fmeasure values (Sebastiani, 2002). General information We computed interannotator agreement scores on the normalized values for general information: Age, Gender and Outcome, and on the annotated text spans for Origin. We achieved excellent agreements for Age and Gender (κ=0.939), differences being due to omissions; poor agreement for Outcome (κ=0.369) due to differences of interpretation between close values (e.g., recovery vs. improvement for long-term diseases); and very low agreement for Origin (κ=-0.762) since spans of text were often distinct between annotators. As stated by Grouin et al. (2011), the κ metric is not well suited for annotations of text since it relies on a random baseline for which the number of units that may be annotated is hard to define. As a consequence, the classical F-measure is often used as an approximation of inter-annotator agreement. In the following experiments, we present the inter-annotator agreements through Precision, Recall, and F-measure. P 0.5660 0.5714 0.7042 0.3151 0.3744 0.7500 0.4260 0.5135 0.5597 0.4328 0.5563 0.2596 0.5567 0.3077 0.5950 0.5378 0.4426 R 0.8511 0.2857 0.2747 0.8519 0.8913 0.5816 0.8267 0.2879 0.8824 0.8056 0.8778 0.6116 0.6888"
W19-5033,W14-1206,0,0.306476,"Missing"
W19-5033,P16-1009,0,0.0373141,"k of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Almost all"
W19-5033,W13-5634,0,0.0323944,"Simplification can be done at lexical, syntactic, semantic but also pragmatic and stylistic levels. Simplification can be useful in two main contexts: as help provided to human readers, which guarantees better access and understanding of the content of documents (Son et al., 2008; Paetzold and Specia, 2016; Chen et al., 2016; Arya et al., 2011; Leroy et al., 2013), and as a pre-processing step for other NLP tasks and applications, which makes easier the work of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned"
W19-5033,P08-1040,0,0.0325031,"rovide a simplified version for a given text. Simplification can be done at lexical, syntactic, semantic but also pragmatic and stylistic levels. Simplification can be useful in two main contexts: as help provided to human readers, which guarantees better access and understanding of the content of documents (Son et al., 2008; Paetzold and Specia, 2016; Chen et al., 2016; Arya et al., 2011; Leroy et al., 2013), and as a pre-processing step for other NLP tasks and applications, which makes easier the work of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They expl"
W19-5033,P08-4006,0,0.0143486,"e contains more than one clause, like one main and one secondary, it can be split into two sentences by transforming the secondary clause into the main clause of another sentence. Sometimes, the splitting should be blocked because it can make the understanding of the main clause more difficult (Brunato et al., 2014). In our corpus, merged and split sentences are detected using their proximity in the corpus and multiple alignments, like in these examples: 3.2 Semantic Annotation The simplification-induced transformations are annotated semantically using YAWAT (Yet Another Word Alignment Tool) (Germann, 2008). YAWAT permits to visualize and manipulate parallel texts. The tool was designed for working with parallel bilingual texts related to mutual translations (Yu et al., 2012). We propose to exploit it with monolingual parallel texts related to simplification. YAWAT displays the two parallel and aligned sentences side by side. The annotator can then align the words using the matrix (Figure 1), and to assign the type of transformation to each pair of text segments considered. The number of squares displayed vertically correspond to the number of words that are counted in the sentence on the left ("
W19-5033,P15-2011,0,0.0142684,"also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain. 1 • Methods based on distribution probabilities, like word embeddings (Mikolov et al., 2013; Pennington et al., 2014), are used to acquire a lexicon and substitution rules for simplification. When trained on relevant data (Wikipedia, Simple Wikipedia, PubMed Central...), word embeddings can contain simpler equivalents, that can be exploited to perform the simplification (Glavas and Stajner, 2015; Kim et al., 2016). Nonetheless, such methods require consequent filtering to keep only the best candidates. Those methods generally provide good coverage and, when the filtering is efficient, good precision; Introduction The purpose of automatic text simplification is to provide a simplified version for a given text. Simplification can be done at lexical, syntactic, semantic but also pragmatic and stylistic levels. Simplification can be useful in two main contexts: as help provided to human readers, which guarantees better access and understanding of the content of documents (Son et al., 200"
W19-5033,D16-1114,0,0.0131404,"f transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain. 1 • Methods based on distribution probabilities, like word embeddings (Mikolov et al., 2013; Pennington et al., 2014), are used to acquire a lexicon and substitution rules for simplification. When trained on relevant data (Wikipedia, Simple Wikipedia, PubMed Central...), word embeddings can contain simpler equivalents, that can be exploited to perform the simplification (Glavas and Stajner, 2015; Kim et al., 2016). Nonetheless, such methods require consequent filtering to keep only the best candidates. Those methods generally provide good coverage and, when the filtering is efficient, good precision; Introduction The purpose of automatic text simplification is to provide a simplified version for a given text. Simplification can be done at lexical, syntactic, semantic but also pragmatic and stylistic levels. Simplification can be useful in two main contexts: as help provided to human readers, which guarantees better access and understanding of the content of documents (Son et al., 2008; Paetzold and Spe"
W19-5033,2009.jeptalnrecital-court.22,0,0.0293968,"Missing"
W19-5033,P12-1107,0,0.0625441,"Missing"
W19-5033,Q16-1029,0,0.0155451,"and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Almost all the existing work"
W19-5033,P17-2014,0,0.0131354,"1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Almost all the existing works address text simplification in English, while other languages are"
W19-5033,L16-1491,0,0.0134348,"im et al., 2016). Nonetheless, such methods require consequent filtering to keep only the best candidates. Those methods generally provide good coverage and, when the filtering is efficient, good precision; Introduction The purpose of automatic text simplification is to provide a simplified version for a given text. Simplification can be done at lexical, syntactic, semantic but also pragmatic and stylistic levels. Simplification can be useful in two main contexts: as help provided to human readers, which guarantees better access and understanding of the content of documents (Son et al., 2008; Paetzold and Specia, 2016; Chen et al., 2016; Arya et al., 2011; Leroy et al., 2013), and as a pre-processing step for other NLP tasks and applications, which makes easier the work of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to"
W19-5033,D17-1062,0,0.0131916,"andrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Almost all the existing works address text simplification in English, whil"
W19-5033,D14-1162,0,0.0903816,", syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain. 1 • Methods based on distribution probabilities, like word embeddings (Mikolov et al., 2013; Pennington et al., 2014), are used to acquire a lexicon and substitution rules for simplification. When trained on relevant data (Wikipedia, Simple Wikipedia, PubMed Central...), word embeddings can contain simpler equivalents, that can be exploited to perform the simplification (Glavas and Stajner, 2015; Kim et al., 2016). Nonetheless, such methods require consequent filtering to keep only the best candidates. Those methods generally provide good coverage and, when the filtering is efficient, good precision; Introduction The purpose of automatic text simplification is to provide a simplified version for a given text"
W19-5033,C10-1149,0,0.0113617,"her NLP tasks and applications, which makes easier the work of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 20"
W19-5033,C10-1152,0,0.0276968,"pplications, which makes easier the work of other NLP modules and may improve the overall results (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008; Blake et al., 2007; Stymne et al., 2013; Wei et al., 2014; Beigman Klebanov et al., 2004). We can see that potentially this task may play an important role. Three main types of methods are currently exploited in text simplification: • Methods issued from machine translation tackle the problem as translation from technical to simple text. A growing number of works propose to exploit this type of method to English texts (Zhao et al., 2010; Zhu et al., 2010; Wubben et al., 2012; Sennrich et al., 2016; Xu et al., 2016; Wang et al., 2016a,b; Zhang and Lapata, 2017; Nisioi et al., 2017). They exploit corpora made of parallel and aligned sentences, that mainly derive from the Simple English Wikipedia English Wikipedia corpus (SEW-EW). Globally, those methods seem to maintain a balance between the quality of the simplification, good coverage and precision. • Methods based on knowledge and rules. For instance, the use of WordNet (Miller et al., 309 Proceedings of the BioNLP 2019 workshop, pages 309–318 c Florence, Italy, August 1, 2019. 2019 Associati"
