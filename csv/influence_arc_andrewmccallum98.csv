2007.mtsummit-papers.32,J84-3009,0,0.701915,"Missing"
2010.amta-papers.31,W09-1114,0,0.102244,"Missing"
2010.amta-papers.31,D08-1023,0,0.0317471,"ied mode not including a language model, or in a full mode including one, in which case a heuristic beam-search is used. While we also employ overlapping biphrases here, we perform decoding and training using a sampling approach on a set of features including a language model and a distortion model. Sampling has also been used in the synchronous grammar paradigm (hierarchical models), for the training of synchronous grammars when Bayesian priors are given (Blunsom et al., 2009) and for estimating the partition function for a model using a synchronous grammar intersected with a language model (Blunsom and Osborne, 2008). 3 3.1 Sampling For Training and Decoding Overview Algorithm 1 gives an overview of the procedure used for training and decoding. The flow for training and decoding differs only in two places, which concern loading and updating the model parameters; otherwise exactly the same code can be used. The algorithm starts by initializing each source sentence with a gloss (choosing the most likely translation per source word). If the task is decoding, a previously trained model is loaded, while for training the model parameters are initialized. Starting from the glosses, an iterative Markov-Chain Mont"
2010.amta-papers.31,P09-1088,0,0.0123533,"slation model that does not incorporate a language model or distortion. Decoding is done through dynamic programming, either in the simplified mode not including a language model, or in a full mode including one, in which case a heuristic beam-search is used. While we also employ overlapping biphrases here, we perform decoding and training using a sampling approach on a set of features including a language model and a distortion model. Sampling has also been used in the synchronous grammar paradigm (hierarchical models), for the training of synchronous grammars when Bayesian priors are given (Blunsom et al., 2009) and for estimating the partition function for a model using a synchronous grammar intersected with a language model (Blunsom and Osborne, 2008). 3 3.1 Sampling For Training and Decoding Overview Algorithm 1 gives an overview of the procedure used for training and decoding. The flow for training and decoding differs only in two places, which concern loading and updating the model parameters; otherwise exactly the same code can be used. The algorithm starts by initializing each source sentence with a gloss (choosing the most likely translation per source word). If the task is decoding, a previo"
2010.amta-papers.31,W02-1001,0,0.00506116,"candidate preferred over y 0 by the objective function, and where η is a learning rate that is set differently in different variants of the method. The translation for the next iteration is sampled according to a transition distribution Q(y 0 |y). In our case, Q(y 0 |y) is zero for translations not in the neighborhood of y, and proportional to the current learnt model probability P (y 0 |x, Θ) otherwise (normalized by the sum of all neighbor probabilities). In preliminary experiments we tried several of the possibilities provided by FACTORIE for setting the learning rate (averaged perceptron (Collins, 2002), 1 Note that, while the neighborhood is local (per-sentence), the objective score is global (corpus-wide). This corresponds to a factor model of the corpus with shared parameters for sentence factors. Figure 1: Consistent (left) and inconsistent (right) pairs of biphrases. The x-axis (y-axis) denotes positions in the source (target) sentence. The squares indicate the spans of words covered on either side by the biphrases. MIRA (Crammer and Singer, 2003) and confidence weighted updates (Dredze et al., 2008)) and found that the averaged perceptron, which amounts to setting the learning rate to"
2010.amta-papers.31,D09-1107,0,0.13334,"hm 1: Training and decoding Related Work Arun et. al (2009) use operations in a nonoverlapping phrase-based setting and traditional features to obtain random samples of alignments, where translation samples are obtained by a form of marginalization that corresponds to “forgetting” the alignment and only outputting the target string. In contrast to our work, their approach is strongly connected to the traditional phrase-based setting, and is concerned with the question of whether sampling and marginalization can reach equally good translations as dynamic programming for the same type of model. Kääriäinen (2009) has developed a system for phrase-based translations using overlapping biphrases, which allows decoding to use a representation which is consistent with that used when heuristically extracting the biphrases from bilingual data, contrary to what is the case with standard phrase-based systems. Each biphrase is a feature, and biphrase parameters are estimated on the basis of maximizing the likelihood of a large bilingual corpus relative to a simplified translation model that does not incorporate a language model or distortion. Decoding is done through dynamic programming, either in the simplifie"
2010.amta-papers.31,P07-2045,0,0.00594016,"position on target side. biphrases: Two (alignment) biphrases are consistent if the matching criterion is the same for source and target side, otherwise they are inconsistent. Existence of (partial) overlap is used as the matching criterion. A biphrase is consistent with an alignment set, if it is consistent with all biphrases in this set, see Figure 1 for an example. For weighting the biphrases we resort to a heuristic similar to the conditional link probability scoring in (Moore, 2005). However, we use weights that can be read off directly from the phrase-table as used by the popular Moses (Koehn et al., 2007) pipeline: the phrasal and lexicalized biphrase probabilities P (f |e) and P (e|f ) are multiplied instead of using P (e, f ). Additionally we normalize the biphrase weights for length (lf and le ), using the geometric mean, in order not to penalize longer biphrases. This has also the effect of increased contiguity of alignments. The resulting biphrase weight is: q lf 3.4 q Plex (e|f )Pphr (e|f ) le Plex (f |e)Pphr (f |e) Neighborhood Operators At present, four operators are used to generate a neighborhood of the current translation: 1. Remove: For each target position the word at this positio"
2010.amta-papers.31,J00-2004,0,0.0144949,"many sparse features (e.g. one binary feature per biphrase) we would expect a confidence weighting scheme to perform better. 3.3 Phrase Alignment We call a phrase alignment a set of biphrases that express correspondences between spans of words in source and target sentences. In our case a phrase alignment is used for two purposes: first, to compute some features of a translation and secondly, for constructing part of the proposal neighborhood (proposed changes to current translation). We employ a greedy phrase alignment algorithm of source and target sentences similar to Competitive Linking (Melamed, 2000): first, biphrases2 that match on both source and target side are ordered by a heuristic weight (which does not depend on the model Θ). Then, biphrases are added to the alignment set, in descending order of their weight, if they are consistent with the current alignment set. Our notion of consistency allows for overlapping 2 We use a biphrase table extracted by the heuristics in a standard run of the Moses pipeline. 2. Insert: For each target position, if the trigram at this position is not present in the language model, a random word is inserted according to the trigram language model. Figure"
2010.amta-papers.31,H05-1011,0,0.00841497,"urce span is mapped using the internal word-alignment, the leftmost by choosing the point at the corresponding relative position on target side. biphrases: Two (alignment) biphrases are consistent if the matching criterion is the same for source and target side, otherwise they are inconsistent. Existence of (partial) overlap is used as the matching criterion. A biphrase is consistent with an alignment set, if it is consistent with all biphrases in this set, see Figure 1 for an example. For weighting the biphrases we resort to a heuristic similar to the conditional link probability scoring in (Moore, 2005). However, we use weights that can be read off directly from the phrase-table as used by the popular Moses (Koehn et al., 2007) pipeline: the phrasal and lexicalized biphrase probabilities P (f |e) and P (e|f ) are multiplied instead of using P (e, f ). Additionally we normalize the biphrase weights for length (lf and le ), using the geometric mean, in order not to penalize longer biphrases. This has also the effect of increased contiguity of alignments. The resulting biphrase weight is: q lf 3.4 q Plex (e|f )Pphr (e|f ) le Plex (f |e)Pphr (f |e) Neighborhood Operators At present, four operato"
2010.amta-papers.31,P02-1040,0,\N,Missing
2010.amta-papers.31,W05-0836,0,\N,Missing
2010.amta-papers.31,P08-1024,0,\N,Missing
2010.amta-papers.31,2009.eamt-smart.4,0,\N,Missing
2010.amta-papers.31,P91-1023,0,\N,Missing
2010.amta-papers.31,W11-2130,0,\N,Missing
2010.amta-papers.31,P06-1096,0,\N,Missing
2020.coling-main.448,D19-1329,0,0.0198558,"using a sampled k-shot training data for fine-tuning. The models parameters are trained on the set of training tasks and are then fine-tuned with k training examples per label for a target test task. The fine-tuned models are then evaluated on the entire test-set for the task. We evaluate on k ∈ {4, 8, 16}. For each task, for every k, we sample 10 training datasets and report the mean and standard deviation, since model performance can be sensitive to the k examples chosen for training. In the few-shot setting it can be unreasonable to assume access to a large validation set (Yu et al., 2018; Kann et al., 2019), thus for the fine-tuning step we tuned the hyper-parameters for all baselines on a held out validation task. We used SciTail, a scientific NLI task, and electronics domain of Amazon sentiment classification task as the validation tasks. We took the hyper-parameters that gave best average performance on validation data of these tasks, for each value of k. For LEOPARD, we only tune the number of epochs for fine-tuning, use the learned per-layer learning rates and reuse remaining hyperparameters (see Appendix C). We evaluate multiple transfer learning baselines as well as a meta-learning baseli"
2020.coling-main.448,P19-1441,0,0.100789,"sks and don’t trivially apply to diverse tasks considered here. We evaluate the following methods: BERTbase : We use the cased BERT-base model (Devlin et al., 2018) which is a state-of-the-art transformer (Vaswani et al., 2017) model for NLP. BERT uses language model pre-training followed by supervised fine-tuning on a downstream task. For fine-tuning, we tune all parameters as it performed better on the validation task. Multi-task BERT (MT-BERT): This is the BERT-base model trained in a multi-task learning setting on the set of training tasks. Our MT-BERT is comparable to the MT-DNN model of Liu et al. (2019) that is trained on the tasks considered here and uses the cased BERT-base as the initialization. We did not use the specialized stochastic answer network for NLI used by MT-DNN. For this model, we tune all the parameters during fine-tuning. MT-BERTsoftmax : This is the multi-task BERT model above, where we only tune the softmax layer during fine-tuning. Prototypical BERT (Proto-BERT): This is the prototypical network method (Snell et al., 2017) that uses BERT-base as the underlying neural model. Following Snell et al. (2017), we used euclidean distance as the distance metric. All methods are"
2020.coling-main.448,W03-0419,0,0.324468,"Missing"
2020.coling-main.448,D13-1170,0,0.0123718,"Missing"
2020.coling-main.448,W18-5446,0,0.0772798,"Missing"
2020.coling-main.448,Q19-1040,0,0.0733015,"Missing"
2020.emnlp-main.392,D14-1139,0,0.0400462,"Missing"
2020.emnlp-main.392,P05-1022,0,0.23399,"aches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when"
2020.emnlp-main.392,P06-1111,0,0.152222,"Missing"
2020.emnlp-main.392,P19-1551,0,0.0198779,"to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when training with more data or when the ground truth parse trees are very different than the ones in S-DIORA’s output frontier after initialization. Other work has explored methods for differentiable structured inference (Niculae et al., 2018; Mensch and Blondel, 2018; Corro and Titov, 2019a,b), which may also be suitable. It’s worth noting that PCFGs are not graphical models (Liang et al., 2009), and marginal inference is often not tractable,11 which is why these approximate methods may be helpful. Grammar induction. There is a rich research history in grammar induction and unsupervised parsing (Fu and Booth, 1975; Angluin, 1980; Carroll and Charniak, 1992). We cover notable work not already mentioned in Appendix A.2. 7 Conclusion We introduce S-DIORA, an extension to DIORA that enables for easy recovery from local errors and is not subject to wash out from vector averaging. Ou"
2020.emnlp-main.392,D16-1001,0,0.0209041,"trategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recov"
2020.emnlp-main.392,N19-1423,0,0.0210802,"notation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amounts of unlabeled data. DIORA is a strong unsupervised parser in spite of its locally greedy nature. DIORA works by encoding all subtrees covering a particular span as separate vectors, and then computing a weighted average of these vectors — DIORA uses this averaged vector later in the dynamic program to represent the entire forest of trees covering a span. DIORA computes a score for each subtree; intuitively, a subtree’s score affects how strongly it is 4832 Proceedings of the 2020 Conference on Empirical Meth"
2020.emnlp-main.392,P18-2058,0,0.0187415,"data used for fine-tuning. 1 DIORA DIORA … … S-DIORA S-DIORA 1 1 2 2 Figure 1: DIORA (top row) is sensitive to locally nonoptimal decisions. By assigning a low weight to a potentially important subtree when recursively computing the vector for a target tree, it is difficult or impossible to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on"
2020.emnlp-main.392,W05-1506,0,0.107565,"fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring for unsupervised parsing when training with more data"
2020.emnlp-main.392,P19-1228,0,0.175854,"termediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amo"
2020.emnlp-main.392,P18-1249,0,0.276204,"train DIORA for supervised parsing using a binarized version of the ‘ground truth’ parse trees from the English Penn Treebank (Marcus et al., 1993). The training procedure is done by optimizing the structured SVM loss: sup Jtree = max(0, S(ˆ y) S(y) + 1), where S(ˆ y ) is the score of the maximal tree and S(y) is the score of the ‘ground truth’. 1 Since the outside vector is used for word prediction, tricks associated with the inside-outside algorithm using only backpropagation of the inside-pass (Eisner, 2016) are not obviously applicable, if at all. 4834 We use the off-the-shelf parser from Kitaev and Klein (2018) as a baseline and the results are shown in Table 1. Although DIORA is strong in unsupervised parsing, the supervised parsing results are not as competitive with the baseline as we had expected, and lead us to consider deeply why this might be the case. We posit the low performance in supervised parsing is due to DIORA’s inability to effectively recover from local errors. Predicting trees in DIORA is exact — you are guaranteed to find the highest scoring tree given the scalar values associated with each span, but there is a weakness when assigning the scalar values. Specifically, the scalar va"
2020.emnlp-main.392,P14-5010,0,0.00331042,"iminaries: Constituency Parsing We measure the performance of our changes via unsupervised and supervised parsing on the test set of the WSJ Penn Treebank (Marcus et al., 1993).4 All models (S-DIORA and baselines) output unlabeled binary trees5 and are evaluated via sentence level F1 (S-F1). For supervised constituency parsing we use the off-the-shelf parser from Kitaev and Klein (2018) as a baseline to compare against DIORA and SDIORA. For training we use the parse trees from training split of PTB and evaluate using the validation data. We binarize the ground truth using the Stanford parser (Manning et al., 2014) and train for 10 epochs. Results against the binary trees and original n-ary (ingoring labels in both cases) is shown in Table 1. Both DIORA and S-DIORA are trained from random initialization using the structured SVM loss from Kitaev and Klein (2018). We see that DIORA is not competitive with the Kitaev and Klein (2018) parser, and attribute this to wash out and its inability to recover from errors. For S-DIORA we train and evaluate with 2 {1, 2, 3, 4} and results are shown in Table 2. We see, unsurprisingly, that regardless of the beamsize at training, when = 1 at test time the performance i"
2020.emnlp-main.392,D19-1587,0,0.0247266,"nd VP segment recall, but also has the fewest VP-A errors. This suggests that errors related to segment recall are likely folded into a different category such as PP attachment. The right-skewed model XLNet =1.5 substantially improves over XLNet =0 in SBAR recall and is comparable in this category with S-DIORA. Interestingly, although increasing the size of 8 To compute phrase embeddings, we follow the procedure from (Kitaev and Klein, 2018) which concatenates the forward and backward LSTM vectors at the beginning and end of each phrase. To compute vector similarity we follow the procedure in Kobayashi et al. (2019) which uses ELMo sentence embeddings for RST parsing — rather than document level parsing, our work pertains to sentence level parsing. in S-DIORA results in a near monotonic improvement in all categories (with some minor exceptions), S-DIORA shows a very different error profile when compared to pre-trained LMs, despite having a better S-F1. For instance, the pre-trained LMs make fewer coordinations errors, and perform better with adverbial phrases (ADVP), than any version of S-DIORA. In future work, it may be useful to understand why parser performance does not increase monotonically. Perhaps"
2020.emnlp-main.392,P03-1040,0,0.11365,"Missing"
2020.emnlp-main.392,D12-1096,0,0.10557,"or unsupervised parsing. In addition, we present a new baseline demonstrating that pretrained language models are better at unsupervised parsing than previously known. 5.1 Linguistic Error Analysis Parsing F1 is useful to quickly compare performance between parsers, and previous work in unsupervised parsing often also report segment recall to give a sense of which phrases are most often captured in the output. To provide an even more thorough treatment of linguistic errors we add labels to the parse trees using the parser from Kitaev and Klein (2018) and then run the Berkeley parser analyzer (Kummerfeld et al., 2012). This latter tool classifies mistakes for each predicted tree by the type of phrases (or patterns like coordination) involved in the error, allowing analysis of the types of errors being made by a model. In Table 4 we show the parsing F1, segment recall, and error counts as determined by the analyzer. By segment recall, we see that C-PCFG outperforms DIORA in segment recall for NP and PP, explaining its high S-F1. The linguistic analysis tells a slightly different story — C-PCFG makes less errors associated with NP internal structure and clause attachment, but substantially more errors associ"
2020.emnlp-main.392,2020.tacl-1.50,0,0.0181253,"e to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achiev"
2020.emnlp-main.392,N15-1067,0,0.0192985,"what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Jo"
2020.emnlp-main.392,J93-2004,0,0.0873489,"h it is still possible to make an error by ignoring a potentially important subtree. Fortunately, this can be alleviated by adding a beam to each cell of the chart, allowing multiple subtrees over any span to be considered. The key benefit of our modification is that error recovery is easily possible, where previously the vector serves as a bottleneck that makes error recovery difficult or impossible. We initialize an instance of S-DIORA using the previously released DIORA model, then finetune before evaluating on the target domain, constituency parse trees from the English WSJ Treebank (PTB, Marcus et al. 1993). In one experimental setting, we assume no access to the evaluation domain and use a subset of DIORA’s training data, a concatenation of the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018b) corpora (hereinafter NLI). In the other setting, we assume access to raw text in the target domain, parse tree labels excluded. In both cases, we see S-DIORA improves on the original DIORA performance by at least 4 F1, and training on the PTB raw text leads to more than 3 F1 over the previous state of the art in constituency parsing. In summary, the main contributions in this paper are: (a"
2020.emnlp-main.392,2020.iwpt-1.11,0,0.0731426,"Missing"
2020.emnlp-main.392,D10-1120,0,0.10064,"Missing"
2020.emnlp-main.392,P92-1017,0,0.723093,"Missing"
2020.emnlp-main.392,N18-1202,1,0.752702,"arge amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised blank-filling objective on large amounts of unlabeled data. DIORA is a strong unsupervised parser in spite of its locally greedy nature. DIORA works by encoding all subtrees covering a particular span as separate vectors, and then computing a weighted average of these vectors — DIORA uses this averaged vector later in the dynamic program to represent the entire forest of trees covering a span. DIORA computes a score for each subtree; intuitively, a subtree’s score affects how strongly it is 4832 Proceedings of the 20"
2020.emnlp-main.392,P11-1108,0,0.12071,"ion Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to induce syntactic tree structures for input sentences without access to labeled training data, and achieves near stateof-the-art results on unsupervised constituency parsing. DIORA resembles pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), in that it is trained with a self-supervised"
2020.emnlp-main.392,C92-1032,0,0.0618314,"put (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options w"
2020.emnlp-main.392,P99-1054,0,0.393799,"idema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudillo, 2016) is a deterministic sparse alternative to the softmax, and Gumbel-Softmax (Jang et al., 2017) uses the categorical reparameterization trick to sample a discrete value during training. Both have attractive properties but alone would not be sufficient for overcoming local errors in S-DIORA. Nonetheless, these options would be worth exploring f"
2020.emnlp-main.392,D18-1412,0,0.0193458,"ially important subtree when recursively computing the vector for a target tree, it is difficult or impossible to recover and the important subtree is washed out (represented in light gray). Our method, S-DIORA (bottom row) can recover from errors, and the desired tree ends up at the top of the beam in the right-most column. Introduction Syntactic parse trees are valuable intermediate features for many NLP pipelines (He et al., 2018; Strubell et al., 2018), as a soft constraint (Rush and Collins, 2012), a hard constraint (Lee et al., 2019b), or in multi-task learning with syntactic scaffolds (Swayamdipta et al., 2018). Syntactic inductive bias can also improve generalization of deep learning models (Kuncoro et al., 2020). These results have motivated researchers to pursue unsupervised parsing, with the hope of training syntax-dependent models on large amounts of data without annotation (Klein and Manning, 2002; Bod, 2006; Ponvert et al., 2011; Shen et al., 2019; Kim et al., 2019, inter alia). Of these models, we focus on the deep insideoutside recursive autoencoder (DIORA; Drozdov et al. 2019a). DIORA encodes sentences in a procedure resembling the inside-outside algorithm (Baker, 1979), which allows it to"
2020.emnlp-main.392,J11-4006,0,0.0318519,"and PTB with no improvements in unsupervised parsing). Most benefit is achieved using = 3, although in some cases it helps to increase it further (see Figure 5). Increasing the beam also helps with different classes of errors. In Figure 4 we see the benefit in sentences with tricky coordination. 5.4 Labeled Parsing We evaluate the labeled trees from §5.1, and the best performing S-DIORA model achieves 80.7 9 The NP-I category covers missed gold phrases within large noun phrases. In general, much of NP structure in PTB is not annotated, and in future work it is worth using the data provided by Vadas and Curran (2011) to investigate NP structure, as determined by unsupervised parsers, more thoroughly. He was punched and kicked by one player and the other broke his jaw He was punched and kicked by one player and the other broke his jaw Figure 4: Two sentences where beam-search helps with ambiguous coordination structures, correctly nesting noun phrases (top) and getting better coordination of verb phrases (bottom). The displayed parse tree output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3 respectively. labeled parsing F1 on the validation data (72.3 recall, 91.2 precision, and 11.7 complete"
2020.emnlp-main.392,Q17-1019,0,0.0246175,"rror recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors by maintaining a beam of size at each cell in the chart. S-DIORA is often faster and discovers better trees than DIORA, but there are other methods for extracting lists of best or plausible parses (Resnik, 1992; Roark and Johnson, 1999; Charniak and Johnson, 2005; Huang and Chiang, 2005; Bouchard-cˆot´e et al., 2009) that might further improve performance. Sparse structured inference. Various work has explored sparse alternatives to soft-weighting. Sparsemax (Martins and Astudill"
2020.emnlp-main.392,Q18-1019,1,0.887078,"Missing"
2020.emnlp-main.392,N18-1101,0,0.0578444,"Missing"
2020.emnlp-main.392,P19-1180,0,0.0753977,"Missing"
2020.emnlp-main.392,P05-1044,0,0.245577,"Missing"
2020.emnlp-main.392,P09-1009,0,0.11279,"Missing"
2020.emnlp-main.392,P17-1076,0,0.0229894,"the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow From the outset the tobacco industry has been uncertain as to what strategy to follow Figure 5: As the beam-size increases, S-DIORA’s output tends to match the ground truth more closely. The displayed output, top to bottom, are from PTB, then S-DIORAP T B with = 1, 3, 5 respectively. vised parsers do not need error recovery because they use models with rich features and model each span score independently (Cross and Huang, 2016; Stern et al., 2017; Kitaev and Klein, 2018; Mrini et al., 2019). Previous research has attempted to achieve the “best of both worlds” by distilling a strong model for supervised parsing via an unsupervised model’s output (Le and Zuidema, 2015). These approaches are closely related to fast and accurate parsing. More accurate models tend to use richer features that are more expensive to compute, influencing researchers to find efficient techniques to offset the loss in speed (Vieira and Eisner, 2017). In this paper, we use the most simple approach to learn to parse with the capability to recover from local errors"
2020.emnlp-main.85,W10-1703,0,0.0150525,"so measuring the model’s ability to provide a diverse set of answers in order to match all the answer clusters. While such an approach can penalize a correct model prediction when it does not match an existing reference answer, we counter this issue by (a) gathering and clustering a large number of reference answers, and (b) utilizing methods of matching non-exact matches, such as WordNet (Miller, 1995) and contextual language models such as RoBERTa (Liu et al., 2019). Generative evaluation approaches are also used in other NLP tasks such as summarization (Radev et al., 2003) and translation (Callison-Burch et al., 2010). 2 We evaluate on a set of competitive baseline models — from QA models powered by large masked LMs such as B ERT, to the direct prediction of answers in a language-modeling paradigm using a large G PT-2 LM (Radford et al., 2018), as well as G PT-2 fine-tuned upon the training data. While most models perform quite poorly at this challenging task, when G PT-2 was fine-tuned using the FAMILY-F EUD training set its performance did improved drastically, although remaining significantly 1. We introduce a large-scale QA dataset of 9.7k questions regarding common sense knowledge of prototypical situ"
2020.emnlp-main.85,P09-1068,0,0.0431438,"ouse. 6 Related Work A wide variety of common sense reasoning datasets address related topics. Many datasets cover physical and spatial reasoning (Bisk et al., 2019), social common sense (Sap et al., 2019b), and common sense understanding of plausible sequences of events (Zellers et al., 2018, 2019; Huang et al., 2019; Bhagavatula et al., 2019; Sap et al., 2019a) or understanding of the entailments of a sentence (Zhang et al., 2017; Bowman et al., 2015; Roemmele et al., 2011; Levesque et al., 2012). There is also a long history of work in modeling scripts and frames (Schank and Abelson, 1977; Chambers and Jurafsky, 2009; Fillmore et al., 1976; Ferraro and Van Durme, 2016; Weber et al., 2020), which is related to the current focus on prototypical situations. Recent works have also sought to characterize the ability of pre-trained language models to understand common sense reasoning, showing such models perform well at common sense reasoning tasks even without fine-tuning, allowing one to explore the common sense reasoning inherent in those models (Tamborrino et al., 2020; Weir et al., 2020). Of particular relevance to the current work, Weir et al. (2020) explored the ability of pre-trained models to predict s"
2020.emnlp-main.85,2021.ccl-1.108,0,0.134017,"Missing"
2020.emnlp-main.85,P11-2057,0,0.350449,"coherent amount of agreement regarding the clustering of answers. 2.4 Analysis of the Dataset The data presented here involves a range of different types of common sense knowledge. To explore the distribution of different kinds of reasoning, and to test whether that distribution of reasoning varied between the publicly available data and the crowdsourced development and test set, we propose a small inventory of six types of common sense reasoning. We are not aware of an agreed-upon typology of all commonsense reasoning types. Categorizations of different types of commonsense reasoning exist (LoBue and Yates, 2011; Boratko et al., 2018), but since each provided categorizations needed for specific tasks (RTE and the ARC dataset, respectively), neither fully covered the range of commonsense types seen in the current work. After consulting both those prior works and a separate part of the training data, we characterize the data into the following six types. These types consist of (1) M ENTAL OR S O CIAL R EASONING , (2) K NOWLEDGE OF P RO TOTYPICAL S ITUATIONS which one is familiar with, (3) R EASONING ABOUT NOVEL , COMPLEX 6 The four total expert annotators annotated a random set of 10 questions together"
2020.emnlp-main.85,P14-2005,0,0.0251555,"lustering was agreed on.6 During this clustering phase answers could be marked as invalid as well — most commonly, either due to low-quality annotations or a clear misunderstanding of a question. In order to keep these clusters roughly similar to the granularity of answers used in the training data and to avoid low-quality evaluation we eliminated questions for which the 8 most popular clusters did not contain at least 85 of the 100 responses. Since each set of answers was clustered twice and adjudicated, we measure the agreement with a cluster agreement metric BLANC (Recasens and Hovy, 2011; Luo et al., 2014), an extension of the Rand index used to score coreference clustering. Using this, the similarity between the clusters produced by any two annotators averaged out to a BLANC score of 83.17, suggesting a coherent amount of agreement regarding the clustering of answers. 2.4 Analysis of the Dataset The data presented here involves a range of different types of common sense knowledge. To explore the distribution of different kinds of reasoning, and to test whether that distribution of reasoning varied between the publicly available data and the crowdsourced development and test set, we propose a s"
2020.emnlp-main.85,D19-1454,0,0.0653514,"Missing"
2020.emnlp-main.85,N19-1421,0,0.0777479,"are evaluated by providing a ranked list of answers in response to a question. These answers are then compared to the set of reference answers for that question and scored based upon how similar they are to the known answers. While one might instead convert questionanswer pairs into a multiple-choice paradigm by generating negatives, it is difficult to generate good negative examples, and the quality of a dataset can be compromised if such examples are either too easy or easily identified using biases in the negative example generation process (Mostafazadeh et al., 2016; Zellers et al., 2018; Talmor et al., 2019; Schwartz et al., 2017; Gururangan et al., 2018; Poliak et al., 2018). We outline here our proposed method for scoring these ranked lists of predicted answers. The dataset ground truth is a ranked list of clusters of answers, including weights(cluster sizes) associated with each cluster. A first component in such an evaluation is to match each answer to an existing cluster of answers, if any cluster is acceptable. We try both simple methods such as exact match as well as more flexible ways of matching to clusters, such as using synonyms from WordNet (Miller, 1995) or a vector-based similarity"
2020.emnlp-main.85,2020.acl-main.357,0,0.0457939,"h for each question constrained to Reddit. This resulted in a set of 85,781 Reddit posts total. Searches were constrained to Reddit in order to focus upon advice and personal narratives which 7 Note that since our scores are always calculated as a percentage of the max score one could receive, M AX A NSWERS is slightly different than hits@k in this setting. Language Model Baseline We also report a language model generation baseline, due to the improved representation power of modern language models and recent evidence of their power in modeling common sense reasoning tasks (Weir et al., 2020; Tamborrino et al., 2020). The baseline is performed using the AI2 G PT-2 large model (Radford et al., 2019) (specifically, the Hugging Face PyTorch implementation (Wolf et al., 2019)). We perform both a zero-shot evaluation and an evaluation after fine-tuning with using our training data. Because the original FAMILY-F EUD prompts are not structured as completion tasks, we transform the original question by hand-designed transformation rules in order for it to be compatible with the G PT-2 training data. E.g “Name something people do when they wake up.” → “One thing people do when they wake up is ...”. The hand-design"
2020.emnlp-main.85,2020.emnlp-main.612,0,0.0259265,"Missing"
2020.emnlp-main.85,2020.acl-main.515,0,0.0362727,"Missing"
2020.emnlp-main.85,D18-1009,0,0.325223,"wers. In each, models are evaluated by providing a ranked list of answers in response to a question. These answers are then compared to the set of reference answers for that question and scored based upon how similar they are to the known answers. While one might instead convert questionanswer pairs into a multiple-choice paradigm by generating negatives, it is difficult to generate good negative examples, and the quality of a dataset can be compromised if such examples are either too easy or easily identified using biases in the negative example generation process (Mostafazadeh et al., 2016; Zellers et al., 2018; Talmor et al., 2019; Schwartz et al., 2017; Gururangan et al., 2018; Poliak et al., 2018). We outline here our proposed method for scoring these ranked lists of predicted answers. The dataset ground truth is a ranked list of clusters of answers, including weights(cluster sizes) associated with each cluster. A first component in such an evaluation is to match each answer to an existing cluster of answers, if any cluster is acceptable. We try both simple methods such as exact match as well as more flexible ways of matching to clusters, such as using synonyms from WordNet (Miller, 1995) or a ve"
2020.emnlp-main.85,P19-1472,0,0.107105,"Missing"
2020.emnlp-main.85,2020.acl-main.184,0,0.0892005,"Missing"
2020.emnlp-main.85,Q17-1027,0,0.0683286,"Missing"
2020.emnlp-main.85,W05-0909,0,\N,Missing
2020.emnlp-main.85,P03-1048,0,\N,Missing
2020.emnlp-main.85,P18-2124,0,\N,Missing
2020.emnlp-main.85,W18-2607,1,\N,Missing
2020.emnlp-main.85,N19-1423,0,\N,Missing
2020.findings-emnlp.270,N18-2016,0,0.0228938,", a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus. 1 Figure 1: Example sentences from the WLP corpus, and their nearest neighbours based on sentence representations obtained from S CI BERT. Introduction Being able to represent natural language descriptions of scientific experiments in a structured form promises to allow tackling a range of challenges from automating biomedical experimental protocols (Kulkarni et al., 2018) to gaining materials science insight by large scale mining of the literature (Mysore et al., 2019). To facilitate these applications, recent work has created datasets annotated with sentence level semantic structure for procedural scientific text from experimental biology (Kulkarni et al., 2018) and materials science (Mysore et al., 2019). However, these corpora, the Wet Lab Protocols corpus (WLP) and the Materials Science Procedural Text (MSPT) corpus remain small. This motivates approaches to parsing that are likely to generalize given limited labelled data. We propose an instance-based edg"
2020.findings-emnlp.270,W19-4007,1,0.825851,"line and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus. 1 Figure 1: Example sentences from the WLP corpus, and their nearest neighbours based on sentence representations obtained from S CI BERT. Introduction Being able to represent natural language descriptions of scientific experiments in a structured form promises to allow tackling a range of challenges from automating biomedical experimental protocols (Kulkarni et al., 2018) to gaining materials science insight by large scale mining of the literature (Mysore et al., 2019). To facilitate these applications, recent work has created datasets annotated with sentence level semantic structure for procedural scientific text from experimental biology (Kulkarni et al., 2018) and materials science (Mysore et al., 2019). However, these corpora, the Wet Lab Protocols corpus (WLP) and the Materials Science Procedural Text (MSPT) corpus remain small. This motivates approaches to parsing that are likely to generalize given limited labelled data. We propose an instance-based edge-factored approach for the relation prediction sub-problem of shallow semantic parsing. To predict"
2020.findings-emnlp.427,N12-1007,0,0.0315881,"proportion of times, a path leads to the correct answer entity starting from each entity in the cluster. To perform clustering, we use hierarchical agglomerative clustering with average linkage with the entity-entity similarity defined in §2.2.1. We extract a non-parameteric number of clusters from the hierarchy using a threshold on the linkage function. Agglomerative clustering has been shown to be effective in many knowledge-base related tasks such as entity resolution (Lee et al., 2012; Vashishth et al., 2018) and in general has shown to outperform flat clustering methods such as K-means (Green et al., 2012; Kobren et al., 2017). A flat clustering is extracted from the hierarchical clustering by using a threshold on the linkage function score. We perform a breadth first search from the root of the tree stopping at nodes for which the linkage is above the given threshold. The nodes where the search stops give a flat clustering (refer to §A.2 for more detail on this). 2.2.3 Parameter Estimation Next we discuss how to estimate path prior and precision terms. There exists abundant modeling choices to estimate them. For example, following Chen et al. (2018), we could train a neural network model to e"
2020.findings-emnlp.427,D16-1019,0,0.0220113,"nchmarks (§3.3). To evaluate the nonparametric nature of our approach, we also evaluate on an ‘open-world’ setting (§2.3) in which new entities are added to the KG. We demonstrate our proposed approach is competitive to several stateof-the-art methods on benchmarks in the standard setting, but it greatly outperforms other methods in the online setting (§3.4). The best hyper-parameters for all experiments including the range of hyperparameter tried and results on validation set are noted in §A.6. 3.1 Data and Evaluation Protocol Data. We evaluate on the following KBC datasets: NELL-995, FB122 (Guo et al., 2016), WN18RR (Dettmers et al., 2018). FB122 is a subset of the dataset derived from Freebase, FB15K (Bordes et al., 2013), containing 122 relations regarding people, locations, and sports. NELL-995 (Xiong et al., 2017) a subset of the NELL derived from the 995th iteration of the system. WN18RR was created by Dettmers et al. (2018) from WN18 by removing inverse relation test-leakage. Experimental Setting Knowledge Base Completion. Given an entity e1 and a relation r, our task is retrieve all entities e2 such that (e1 , r, e2 ) belongs in the edges E in a KG G . This task is known as tail prediction"
2020.findings-emnlp.427,N18-1165,0,0.0195553,"rm flat clustering methods such as K-means (Green et al., 2012; Kobren et al., 2017). A flat clustering is extracted from the hierarchical clustering by using a threshold on the linkage function score. We perform a breadth first search from the root of the tree stopping at nodes for which the linkage is above the given threshold. The nodes where the search stops give a flat clustering (refer to §A.2 for more detail on this). 2.2.3 Parameter Estimation Next we discuss how to estimate path prior and precision terms. There exists abundant modeling choices to estimate them. For example, following Chen et al. (2018), we could train a neural network model to estimate P(p |ce1q , rq ). However, with our original goal of designing a simple and efficient non-parametric model, we estimate these parameters by simple count statistics from the KG. E.g., the path prior P(p |c, rq ) is estimated as ∑ec ∈c ∑ p0 ∈Pn (ec ,rq ) 1 [type(p0 ) = p] ∑ec ∈c ∑ p0 ∈Pn (ec ,rq ) 1 (2) For each entity in cluster c, we consider the paths that connect ec to entities it is directly connected to via edge type rq (Pn (ec , rq ) in §2.1). The path prior for a path type p is computed as the proportion of times the type of paths in Pn"
2020.findings-emnlp.427,D12-1045,0,0.0351756,"significantly better performance (§3.3). A similar argument applies for the path-precision term in which we calculate the proportion of times, a path leads to the correct answer entity starting from each entity in the cluster. To perform clustering, we use hierarchical agglomerative clustering with average linkage with the entity-entity similarity defined in §2.2.1. We extract a non-parameteric number of clusters from the hierarchy using a threshold on the linkage function. Agglomerative clustering has been shown to be effective in many knowledge-base related tasks such as entity resolution (Lee et al., 2012; Vashishth et al., 2018) and in general has shown to outperform flat clustering methods such as K-means (Green et al., 2012; Kobren et al., 2017). A flat clustering is extracted from the hierarchical clustering by using a threshold on the linkage function score. We perform a breadth first search from the root of the tree stopping at nodes for which the linkage is above the given threshold. The nodes where the search stops give a flat clustering (refer to §A.2 for more detail on this). 2.2.3 Parameter Estimation Next we discuss how to estimate path prior and precision terms. There exists abund"
2020.findings-emnlp.427,D10-1106,0,0.0376325,"ine method’s results. Athlete Cluster (athlete-led-sports-team, team-plays-in-league) (athlete-home-stadium, league-stadiums−1 ) Politician Cluster (politician-us-member-of-political-group, person-belongs-to-organization−1 , agent-belongs-to-organization) (agent-collaborates-with-agent, agent-belongs-to-organization) Table 6: High scoring paths in different clusters for the query agent-belongs-to-organization in NELL-995 this deficiency by guessing counter examples from rules and making it more scalable. Statistical relational learning methods (Getoor and Taskar, 2007; Kok and Domingos, 2007; Schoenmackers et al., 2010) and probabilistic logic approaches (Richardson and Domingos, 2006; Broecheler et al., 2010; Wang et al., 2013) combine machine learning and logic to learn rules. However, none of these work derive reasoning rules dynamically from similar entities in the knowledge graph. Bayesian non-parametric approaches for linkprediction. There is a rich body of work in bayesian non-parametrics to automatically learn the latent dimension of entities (Kemp et al., 2006; Xu et al., 2006). Our method does not learn latent dimension of entities, instead our work is nonparametric because it gathers reasoning pat"
2020.findings-emnlp.427,D19-1265,0,0.0224724,"rformance (iv) Our approach almost matches our performance in oracle setting indicating the effectiveness of the online clustering and fast parameter approximation. (v) Lastly, we perform closest to the offline best results outperforming all variants of RotatE. 4 Related Work Open-world KG completion. Shi and Weninger (2018) consider the task of open-world KG completion. However, they use text descriptions to learn entity representations using convolutional neural networks. Our model does not use additional text data and we use very simple entity representations that helps us to perform well. Tang et al. (2019) learns to update a KG with new links by reading news. Even though they handle adding or deleting new edges, they do not observe new entities. Lastly, none of them learn from similar entities using a CBR approach. Inductive representation learning on KGs. Recent works (Teru et al., 2020; Wang et al., 2020) learn entity independent relation representations and hence allow them to handle unseen entities. However, they do not perform contextual reasoning by gathering reasoning paths from similar entities. Moreoever, in our open-world setting, we consider the more challenging setting, where new fa"
2020.findings-emnlp.427,D17-1060,0,0.0270265,"tive to several stateof-the-art methods on benchmarks in the standard setting, but it greatly outperforms other methods in the online setting (§3.4). The best hyper-parameters for all experiments including the range of hyperparameter tried and results on validation set are noted in §A.6. 3.1 Data and Evaluation Protocol Data. We evaluate on the following KBC datasets: NELL-995, FB122 (Guo et al., 2016), WN18RR (Dettmers et al., 2018). FB122 is a subset of the dataset derived from Freebase, FB15K (Bordes et al., 2013), containing 122 relations regarding people, locations, and sports. NELL-995 (Xiong et al., 2017) a subset of the NELL derived from the 995th iteration of the system. WN18RR was created by Dettmers et al. (2018) from WN18 by removing inverse relation test-leakage. Experimental Setting Knowledge Base Completion. Given an entity e1 and a relation r, our task is retrieve all entities e2 such that (e1 , r, e2 ) belongs in the edges E in a KG G . This task is known as tail prediction. If the relation is instead the inverse relation r−1 , we assume that we are given an e02 and asked to predict entities e01 such that (e01 , r−1 , e02 ) belongs in the edges E (head prediction). To be exactly comp"
2021.acl-long.160,D19-1040,0,0.0904865,"their logits depending on how long they are trained, we post-hoc calibrate each of our models using temperature scaling (Guo et al., 2017) and a shift parameter. We report the total error (e.g., the sum of the errors between the mean confidence and the empirical accuracy) on the UFET dev set and the OntoNotes dev set. Entity Representations We are interested in the usefulness of the trained entity typing models in a downstream task. Following Onoe and Durrett (2020b), we evaluate entity representation given by the box-based and vector-based models on the Coreference Arc Prediction (CAP) task (Chen et al., 2019) derived from PreCo (Chen et al., 2018). This task is a binary classification problem, requiring to judge if two mention spans (either in one sentence or two sentences) are the same entity or not. As in Onoe and Durrett (2020b), we obtain type predictions (a vector of probabilities associated with types) for each span and use it as an entity representation. The final prediction of coreference for a pair of mentions is given by the cosine similarity between the entity type probability vectors with a threshold 0.5. The original data split provides 8k examples for each of the training, dev, and t"
2021.acl-long.160,2020.acl-main.749,0,0.680801,"Missing"
2021.acl-long.160,P18-1009,0,0.107586,"ve typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach require"
2021.acl-long.160,Q14-1037,1,0.790002,"types as boxes. Entity typing Entity typing and named entity recognition (Tjong Kim Sang and De Meulder, 2003) are old problems in NLP. Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hiera"
2021.acl-long.160,W09-1109,0,0.0535765,"mates the effective region within the actor box. Now the edges of actor are contained in the edges of person in the most of dimensions, indicating that the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs t"
2021.acl-long.160,W09-3711,0,0.0522225,"mates the effective region within the actor box. Now the edges of actor are contained in the edges of person in the most of dimensions, indicating that the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs t"
2021.acl-long.160,D17-1284,0,0.0156317,"003) are old problems in NLP. Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hierarchy of and correlations between entity types. Our experiments showed several benefits of box embedd"
2021.acl-long.160,N06-2015,0,0.0143871,"observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hype"
2021.acl-long.160,2020.emnlp-main.21,1,0.845791,"Table 10 in Appendix C lists the 30 pairs. Robustness Entity typing datasets with very large ontologies like UFET are noisy; does our box-based model’s notion of hierarchy do a better job of handling intrinsic noise in a dataset? To test this in a controlled fashion, we synthetically create noisy labels by randomly dropping the gold labels with probability 13 .8 We derive two noisy training sets from the UFET training set: 1) adding noise to the coarse types and 2) adding noise to fine & ultra-fine types. We train on these noised datasets and evaluate on the standard UFET dev set. Calibration Desai and Durrett (2020) study calibration of pre-trained Transformers such as BERT and RoBERTa (Liu et al., 2019) on natural language inference, paraphrase detection, and commonsense reasoning. In a similar manner, we investigate if our box-based entity typing model is calibrated: do the probabilities assigned to types by the model match the empirical likelihoods of those types? Since models may naturally have different scales 8 If this causes the gold type set to be empty, we retain the original gold type(s); however, this case is rare. Model P R F1 Box Vector 52.8 38.8 44.8 53.0 36.3 43.1 Choi et al. (2018) Label"
2021.acl-long.160,N19-1423,0,0.0456209,"et D: ∑ L=− ) , − xM β +e k yM − β ) + (1 − tkgold ) · log(1 − pθ (tk |m, s)), . Following Dasgupta et al. (2020), we approximate the expected volume of a Gumbel box using a softplus function: ) ( ∏ xM,i − xm,i − 2γ , Vol(x) ≈ softplus β i where i is an index of each coordinate and γ ≈ 0.5772 is the Euler–Mascheroni constant,4 and softplus(x) = 1t log(1 + exp(xt)), with t as an inverse temperature value. Mention and Context Encoder We format the context words s and the mention span m as x = [CLS] m [SEP] s [SEP] and chunk into WordPiece tokens (Wu et al., 2016). Using pre-trained BERT5 (Devlin et al., 2019), we encode the whole sequence into a single vector by taking the hidden vector at the [CLS] token. A highway layer (Srivastava et al., 2015) projects down the hidden vector h[CLS] ∈ Rℓ to the R2d space, where ℓ is the hidden dimension of the encoder (BERT), and d is the dimension of the box space. This highway layer transforms representations in a vector space to the box space without impeding the gradient flow. We further split the hidden vector ¯ ∈ R2d into two vectors: the center point of the h box cx ∈ Rd and the offset from the maximum and minimum corners ox ∈ Rd . The minimum and maximu"
2021.acl-long.160,2020.codi-1.3,0,0.0312954,"t al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b). Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models. 7 Conclusion In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hierarchy of and correlations between entity types. Our experiments showed several benefits of box embeddings over the equivalent vector-based model, including typing performance, calibration, and robustness to noise. Acknowledgments Thanks"
2021.acl-long.160,E17-1068,0,0.127328,"t the person box almost contains this “effective” actor box. 6 Related Work Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela, 2058 (a) (b) Figure 3: Edges of (a) the person box vs the actor box and (b) the person box vs the minimum bounding box of the intersections between mention & context boxes and the actor box. 2017; L´opez and Strube, 2020) can also model hierarchical relationship"
2021.acl-long.160,D19-1641,0,0.56804,"-based model, the boxbased model improves primarily in macro-recall compared to macro-precision. Choi et al. (2018) is a LSTM-based model using GloVe (Pennington et al., 2014). On top of this model, Xiong et al. (2019) add a graph convolution layer to model type dependencies. Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data. Note that past work on this dataset has used BERT-base (Onoe and Durrett, 2019). Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019). Some of the gain from our vector-based model can be attributed to our use of BERT-Large; however, our box model still achieves stronger performance than the corresponding vector-based version which uses the same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes. Our box-based model consistently outperforms the vector-based model in macro-recall and F1 across the three classes. The largest gap in macro-recall is in the fine class, leading to the largest gap in macro-F1 within the three classes. We also list the numbers from prior work in Table"
2021.acl-long.160,2021.ccl-1.108,0,0.0482288,"Missing"
2021.acl-long.160,2020.findings-emnlp.42,0,0.039003,"Missing"
2021.acl-long.160,P18-1010,1,0.879896,"res the latent type hierarchies better than the vector-based model does.1 1 Introduction The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging th"
2021.acl-long.160,D15-1182,0,0.0665569,"Missing"
2021.acl-long.160,2020.findings-emnlp.54,1,0.735776,"2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018). We build an entity typing model that jointly embeds each entity mention and entity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking int"
2021.acl-long.160,D14-1162,0,0.10189,"Missing"
2021.acl-long.160,N18-1202,0,0.0651462,".2 67.1 38.4 40.7 46.2 46.6 39.4 42.2 43.8 40.4 Table 2: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) comparing various systems. Our box-based model outperforms models from past work as well as our vector-based baseline. F1.9 Compared to the vector-based model, the boxbased model improves primarily in macro-recall compared to macro-precision. Choi et al. (2018) is a LSTM-based model using GloVe (Pennington et al., 2014). On top of this model, Xiong et al. (2019) add a graph convolution layer to model type dependencies. Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data. Note that past work on this dataset has used BERT-base (Onoe and Durrett, 2019). Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019). Some of the gain from our vector-based model can be attributed to our use of BERT-Large; however, our box model still achieves stronger performance than the corresponding vector-based version which uses the same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes."
2021.acl-long.160,D16-1144,0,0.591369,"tity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking intersections of boxes corresponds to computing joint distributions, which allows us to model mentiontype relations (what types does this mention exhibit?) and type-type relations (what is the type hierarchy?). Concretely, we can compute the conditional probability of a type given the entity mention with straightforward volume calculations, allowing us to construct a probabilistic type classification model. Compared to embedding types as points in Euclidean space (Ren et al., 2016a), the box space is expressive and suitable for representing entity types due to its geometric properties. Boxes can nest, overlap, or be completely disjoint to capture 2051 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2051–2064 August 1–6, 2021. ©2021 Association for Computational Linguistics … The Hunger Games, the first of 3 best selling books by Suzanne Collins. Figure 1: A mention (Suzanne Collins) and three entity types are embedded into a vector space (left) and"
2021.acl-long.160,E17-1119,0,0.0217716,"erforms the vector-based model on two benchmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance. In our other experiments, the box-based model also performs better at predicting supertypes and subtypes consistently and being robust against label noise, indicating that our approach is capable of capturing the latent hierarchical structure in entity types. 2 Motivation When predicting class labels like entity types that exhibit a hierarchical structure, we naturally want our model’s output layer to be sensitive to this structure. Previous work (Ren et al., 2016a; Shimaoka et al., 2017; Choi et al., 2018; Onoe and Durrett, 2019, inter alia) has fundamentally treated types as vectors, as shown in the left half of Figure 1. As is standard in multiclass or multi-label classification, the output layer of these models typically involves taking a dot product between a mention embedding and each possible type. A type could be more general and predicted on more examples by having higher norm,2 but it is hard for these representations to capture that a coarse type like Person will have many mutually orthogonal subtypes. By contrast, box embeddings naturally represent these kinds of"
2021.acl-long.160,N19-1250,1,0.893025,"hmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance. In our other experiments, the box-based model also performs better at predicting supertypes and subtypes consistently and being robust against label noise, indicating that our approach is capable of capturing the latent hierarchical structure in entity types. 2 Motivation When predicting class labels like entity types that exhibit a hierarchical structure, we naturally want our model’s output layer to be sensitive to this structure. Previous work (Ren et al., 2016a; Shimaoka et al., 2017; Choi et al., 2018; Onoe and Durrett, 2019, inter alia) has fundamentally treated types as vectors, as shown in the left half of Figure 1. As is standard in multiclass or multi-label classification, the output layer of these models typically involves taking a dot product between a mention embedding and each possible type. A type could be more general and predicted on more examples by having higher norm,2 but it is hard for these representations to capture that a coarse type like Person will have many mutually orthogonal subtypes. By contrast, box embeddings naturally represent these kinds of hierarchies as shown in the right half of F"
2021.acl-long.160,W03-0419,0,0.528764,"Missing"
2021.acl-long.160,P18-1025,1,0.921986,"z and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018). We build an entity typing model that jointly embeds each entity mention and entity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking intersections of boxes corresponds to computing joint distributions, which allows us to model mentiontype relations (what types does this mention exhibit?) and type-type relations (what is the type hierarchy?). Concretely, we can compute the conditional probability of a type given the entity mention with straightforward volume calculations, allowing us to construct a probabilistic type"
2021.acl-long.160,N19-1084,0,0.117704,"he same pretrained model. Table 2 breaks down the performance into the coarse, fine, and ultra-fine classes. Our box-based model consistently outperforms the vector-based model in macro-recall and F1 across the three classes. The largest gap in macro-recall is in the fine class, leading to the largest gap in macro-F1 within the three classes. We also list the numbers from prior work in Table 2. HY XLarge (L´opez and Strube, 2020), a hyperbolic model designed to learn hierarchical structure in entity types, exceeds the performance of the models with similar sizes such as Choi et al. (2018) and Xiong et al. (2019) especially in macrorecall. In the ultra-fine class, both our box-based model and HY XLarge achieve higher macro-F1 compared to their vector-based counterparts. One possible reason for the higher recall of our 9 We omit the test number of L´opez and Strube (2020), since they report results broken down into coarse, fine, and ultra-fine types instead of an aggregated F1 value. However, based on the development results, their approach substantially underperforms the past work of Onoe and Durrett (2019) regardless. model is a stronger ability to model dependencies between types. Instead of failing"
2021.acl-long.160,N18-1002,0,0.0166108,"size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict con"
2021.acl-long.160,P15-2048,0,0.0625899,"Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind 1 The code is available at https://github.com/ yasumasaonoe/Box4Types. of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; L´opez and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment. In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional"
2021.acl-long.160,S18-2022,0,0.600077,"Missing"
2021.acl-long.349,D19-5611,0,0.0182042,"ejad et al., 2019) for running test-time inference. Guo et al. (2020) report a significant improvement over prior non-autoregressive models and superior performance comparing to autoregressive methods on IWSLT’14 German-English task. Their finding is consistent with our improvement using the pretained BERT model. However, our Joint-EBM model is a different way of using BERT for translation, which does not require separate BERT models for source and target language. Please see Section 4.9 for a detailed comparison. Finally, other works also discuss using BERT to improve the performance of NMT. Clinchant et al. (2019) describe initializing the embedding or the whole encoder with BERT’s parameters. Zhu et al. (2020) use an attention model to incorporate the output of BERT into encoder and decoder of NMT. In our approach, we use BERT as an external energy-based ranker. 4 4.1 Experiments Datasets We use German-English (De→En), RomanianEnglish (Ro→En) and Italian-English (It→En) from IWSLT’14 datasets and French-English (Fr→En) from IWSLT’17 translation tasks. We also use IWSLT’14 English-German (En→De) to show that the proposed method can be expanded to translation tasks with a different target language. All"
2021.acl-long.349,P19-1285,0,0.0532535,"Missing"
2021.acl-long.349,D19-1633,0,0.0950372,"to our work, Guo et al. (2020) proposes using two different BERT models as an encoder of the source language (X-BERT) and a decoder of the target language (Y-BERT). Guo et al. (2020) add an extra trainable encoder-decoder adaption module followed by a feed-forward module to each layer of the decoder and a feed-forward module to each layer of the encoder. (Please see Guo et al. (2020) for more detail on the architecture.) For fine-tuning XY-BERT for translation tasks, Guo et al. (2020) keep all XY-BERT’s parameters fixed except the parameters of the new modules, and use mask-predict decoding (Ghazvininejad et al., 2019) for running test-time inference. Guo et al. (2020) report a significant improvement over prior non-autoregressive models and superior performance comparing to autoregressive methods on IWSLT’14 German-English task. Their finding is consistent with our improvement using the pretained BERT model. However, our Joint-EBM model is a different way of using BERT for translation, which does not require separate BERT models for source and target language. Please see Section 4.9 for a detailed comparison. Finally, other works also discuss using BERT to improve the performance of NMT. Clinchant et al. ("
2021.acl-long.349,D19-1632,0,0.0409334,"Missing"
2021.acl-long.349,P17-4012,0,0.0354536,"wani et al., 2017) as our BaseNMT. Our Transformer architecture includes six encoder and six decoder layers, and the number of attention heads, embedding dimension and inner-layer dimension are 8, 512 and 4096, respectively. We use dropout, weight decay, label smoothing to regularize our models. We use layer normalization and early stopping. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and  = 1e−8 and we use the same learning rate scheduler as Ott et al. (2019). We trained our models on 1 Nvidia TITANX GPU. 2 We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al., 2019) toolkits. 4531 Table 1: BLEU score comparison for IWSLT, FLoRes, and WMT (indicated using *) tasks. De− →En Fr− →En It− →En Ro− →En Si− →En Ne− →En En− →De De→En* En→De* BaseNMT + Beam BaseNMT + Sample BaseNMT + LM BaseNMT + MLM NCE-EBR Marginal-EBR Shared-EBR Conditional-EBM 33.87 33.98 34.25 34.42 34.47 35.68 35.75 37.58 31.50 31.59 31.56 32.13 32.00 33.77 33.80 35.02 32.08 32.22 32.52 33.68 32.89 34.00 34.14 36.05 33.21 33.64 33.01 33.85 32.23 34.48 34.65 37.19 7.10 7.19 7.11 7.70 7.98 8.62 10.29 10.47 6.07 6.44 6.02 7.21 7.36 7.26 9.25 9.82 28.83 28.85 28.91"
2021.acl-long.349,N16-1133,0,0.0163489,"r models: ωθ (y) = exp(−Eθ (y)). They train the EBM using noise contrastive estimation (Gutmann and Hyv¨arinen, 2010). We find this less suitable for re-ranking in the translation tasks (see Section 4). Discriminative re-ranking was first introduced by Shen et al. (2004) for improving the performance of machine translation (MT). They have trained a linear separator using the perceptron learning algorithm to distinguish the top r translations from the rest of the translations in the n-best possible outputs. The features for the discriminator are extracted from both source and target sentences. Mizumoto and Matsumoto (2016) combine the score of MT and the linear model using more complex syntactical features to re-rank the target sentences. Here, we rely on the features learned by BERT, and given the high capacity of the energy model, we train the energy model to respect the ranking of every pair of samples. Gulcehre et al. (2017) describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two"
2021.acl-long.349,W19-5333,0,0.0129941,"n NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of using the direct model plus LM, nevertheless, backtranslation can also be added into our model for further improvement. Recently, Salazar et al. (2020) use masked language"
2021.acl-long.349,N19-4009,0,0.019104,"l. (2019) we use the devtest dataset for all our evaluations. 4.2 Base Model We use the Transformer2 (Vaswani et al., 2017) as our BaseNMT. Our Transformer architecture includes six encoder and six decoder layers, and the number of attention heads, embedding dimension and inner-layer dimension are 8, 512 and 4096, respectively. We use dropout, weight decay, label smoothing to regularize our models. We use layer normalization and early stopping. Models are optimized using Adam (Kingma and Ba, 2015) with parameters β1 = 0.9, β2 = 0.98, and  = 1e−8 and we use the same learning rate scheduler as Ott et al. (2019). We trained our models on 1 Nvidia TITANX GPU. 2 We use the implementation in Opennmt (Klein et al., 2017) and Fairseq (Ott et al., 2019) toolkits. 4531 Table 1: BLEU score comparison for IWSLT, FLoRes, and WMT (indicated using *) tasks. De− →En Fr− →En It− →En Ro− →En Si− →En Ne− →En En− →De De→En* En→De* BaseNMT + Beam BaseNMT + Sample BaseNMT + LM BaseNMT + MLM NCE-EBR Marginal-EBR Shared-EBR Conditional-EBM 33.87 33.98 34.25 34.42 34.47 35.68 35.75 37.58 31.50 31.59 31.56 32.13 32.00 33.77 33.80 35.02 32.08 32.22 32.52 33.68 32.89 34.00 34.14 36.05 33.21 33.64 33.01 33.85 32.23 34.48 34.6"
2021.acl-long.349,K19-1084,0,0.0629332,"Missing"
2021.acl-long.349,W18-6319,0,0.0199208,"t, 4532 we have Shared-EBR that trains single MarginalEBM for the tasks with the same target language. Shared-EBR is only trained on IWSLT and FLoRes tasks with English target. For this method, we first sample a translation task and then sample a batch from that task and follow Algorithm 1 for the training of the Marginal-EBM. Finally, as an upper bound for the best achievable result, we also extract the translations from the sample that are closest to the gold data (based on BLEU score). 4.6 Table 3: The effect of using gold data in the ranking objective for Marginal-EBR. 3 We use SacreBLEU (Post, 2018) as a consistent BLEU implementation for all of our experiments. 0.0 0.25 0.75 1.0 De→En Fr→En 35.68 33.77 35.00 33.15 34.20 31.65 33.75 30.82 Table 4: Effect of Entropy Regularization on IWSLT’14 DE-EN Results Table 1 shows the performance of the described methods for IWSLT, FLoRes, and WMT translation tasks.3 BaseNMT+Sample achieves a better score than beam decoding suggesting that our multinomial sampling supports the modes of the distribution defined by the BaseNMT. Similarly, oracle values are high, indicating that the samples also support the desired distribution. This satisfies the nece"
2021.acl-long.349,2020.acl-main.240,0,0.0222475,"as also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of using the direct model plus LM, nevertheless, backtranslation can also be added into our model for further improvement. Recently, Salazar et al. (2020) use masked language models (MLM) such as BERT to score hypotheses from NMT. Salazar et al. (2020) describe the score of a MLM as pseudo-log-likelihood score (PLL). To calculate PLL score of a sentence, each token wi in the sentence is sequentially masked, which allows the calculation of log p(wi |wi ) from the output of the MLM. The normalized pseudolog-probability of the sentence is the average of logprobability of the masked words P given the rest of the words in the sentence: N1 N i=1 log p(wi |wi ), where N is the length of the sentence. We use this approach as one of our baselines. In"
2021.acl-long.349,P16-1009,0,0.217941,"describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of usi"
2021.acl-long.349,P16-1162,0,0.316259,"describe using language model (LM) to improve the performance of NMT using shallow and deep fusion. Shallow models combine the marginal probability of predicting each word in NMT and LM: log PNMT (yi |y<i ) + λ log PLM (yi |y<i ), while deep fusion concatenates the hidden states of two models before predicting each word and uses parallel data to fine-tune the weights. Similar to deep fusion, Domhan and Hieber (2017) feed the unnormalized output of LM to the decoder of NMT. Domhan and Hieber (2017) jointly train the LM and NMT using monolingual target-side data and parallel data, respectively. Sennrich et al. (2016a) augment the parallel training data with monolingual data with the target language and back-translation. Re-ranking with LM has also been explored by Ng et al. (2019), where they decode the output 4530 based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y), where p(y|x) is the direct model provided by NMT, p(x|y) is computed via back-translation and p(y) is an LM. Our approach differs from the previous methods that use LMs for re-ranking as we train our energy-based model to be consistent with the task measure instead of using pre-trained LMs. In our experiments, we only explore the effect of usi"
2021.acl-long.349,N04-1023,0,0.210541,"valuated at y. The log-linear model simplifies training the parameters θ: ∇θ pθ (y) = P φ(y)−E y). The expectation term ˆ ∼pθ (.) φ(ˆ y y∈D can be estimated using rejecting sampling or importance sampling given the proposal distribution q. Deng et al. (2020) extend this approach for text generation by using unrestricted EBMs instead of log-linear models: ωθ (y) = exp(−Eθ (y)). They train the EBM using noise contrastive estimation (Gutmann and Hyv¨arinen, 2010). We find this less suitable for re-ranking in the translation tasks (see Section 4). Discriminative re-ranking was first introduced by Shen et al. (2004) for improving the performance of machine translation (MT). They have trained a linear separator using the perceptron learning algorithm to distinguish the top r translations from the rest of the translations in the n-best possible outputs. The features for the discriminator are extracted from both source and target sentences. Mizumoto and Matsumoto (2016) combine the score of MT and the linear model using more complex syntactical features to re-rank the target sentences. Here, we rely on the features learned by BERT, and given the high capacity of the energy model, we train the energy model t"
2021.acl-long.349,P16-1159,0,0.171551,"partment of Computer Science, College of Computing and Informatics University of North Carolina Charlotte {sbhatta9,rooshenas}@uncc.edu Subhajit Naskar, Simeng Sun, Mohit Iyyer, and Andrew McCallum College of Information and Computer Science, University of Massachusetts Amherst {snaskar,simeng,miyyer,mccallum}@cs.umass.edu Abstract The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energybased model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), whi"
2021.acl-long.349,D18-1397,0,0.0699061,"r Science, College of Computing and Informatics University of North Carolina Charlotte {sbhatta9,rooshenas}@uncc.edu Subhajit Naskar, Simeng Sun, Mohit Iyyer, and Andrew McCallum College of Information and Computer Science, University of Massachusetts Amherst {snaskar,simeng,miyyer,mccallum}@cs.umass.edu Abstract The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energybased model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in"
2021.acl-long.349,N18-2021,1,0.368379,", thus each local region is trained to have similar ranking as that BLEU score for the samples in the region. 2 Energy-Based Reranking Using EBM Eθ to reweight the samples from an NMT defines a new probability distribution over the output sentences (see Grover et al. (2019)): Pθ (y|x) ∝ PNMT (y|x) exp( −EθT(y,x) ), where T is temperature. The ideal re-ranker requires an EBM with the energy function Eθ (y, x) such that Pθ (y|x) and BLEU(y, yi ) have similar modes for all (xi , yi ) ∈ D, where D is an empirical data distribution. To train θ we use rank-based training (Rohanimanesh et al., 2011; Rooshenas et al., 2018, 2019). Rank-based training enforces that the samples from Pθ (.) have similar ranking with respect to both the energy score and task measure (see Figure 2). To sample from Pθ (y|x), we sample k sentences from PNMT (y|x) using multinomial sampling from locally normalized distributions over the output and reweight the samples based on the energy network exp( −EθT(y,x) ). Then we resample two sentences, y1 and y2 , from the renormalized set, which defines a conditional distribution: θ (y,x)/T ) P i (y|x) = P exp(−E (a similar samk exp(−Eθ (yk ,x)/T ) pling approach has been used in Deng et al."
2021.acl-long.364,P18-2109,0,0.0453295,"Missing"
2021.acl-long.364,2021.emnlp-main.382,0,0.0364458,"luate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each new mention is either placed into one of a number of candidate clusters, or a new cluster if similarity does not exceed a given threshold (Allaway et al. (2021) use a similar approach for joint entity and event coreference). Shrimpton et al. (2015) note that the this incremental clustering does not process mentions in constant time/memory, and thus is not “truly streaming”. They present the only truly streaming approach for CDC by introducing a number of memory managef which we ment policies that limit the size of M, describe in more detail in Section 3.3. One of the key problems inhibiting further research on streaming CDC is a lack of suitable evaluation datasets for measuring system performance. The datasets used in Rao et al. (2010) are either sm"
2021.acl-long.364,P98-1012,0,0.8564,"ms that can be trained end-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenar"
2021.acl-long.364,P19-1409,0,0.0254451,"in, 1998; Mann and Yarowsky, 2003; Gooi and Allan, 2004). In the past decade, research on CDC has mainly focused in improving scalability (Singh et al., 2011), and jointly learning to perform CDC with other tasks such as entity linking (Dutta and Weikum, 2015) and event coreference (discussed in the next paragraph). This work similarly investigates whether entity linking is beneficial for CDC, however we use entity linkers that are pretrained separately and kept fixed during inference. Recently, there has been a renewed interest in performing CDC jointly with cross-document event coreference (Barhom et al., 2019; Meged et al., 2020; Cattan et al., 2020; Caciularu et al., 2021) on the ECB+ dataset (Cybulska and Vossen, 2014). Although we do not evaluate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each n"
2021.acl-long.364,2021.findings-emnlp.225,0,0.0149028,"he past decade, research on CDC has mainly focused in improving scalability (Singh et al., 2011), and jointly learning to perform CDC with other tasks such as entity linking (Dutta and Weikum, 2015) and event coreference (discussed in the next paragraph). This work similarly investigates whether entity linking is beneficial for CDC, however we use entity linkers that are pretrained separately and kept fixed during inference. Recently, there has been a renewed interest in performing CDC jointly with cross-document event coreference (Barhom et al., 2019; Meged et al., 2020; Cattan et al., 2020; Caciularu et al., 2021) on the ECB+ dataset (Cybulska and Vossen, 2014). Although we do not evaluate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each new mention is either placed into one of a number of candidate clus"
2021.acl-long.364,N04-1002,0,0.337552,"d-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of rese"
2021.acl-long.364,D11-1072,0,0.105173,"Missing"
2021.acl-long.364,K18-1050,0,0.0156204,"Introduction The ability to disambiguate mentions of named entities in text is a central task in the field of information extraction, and is crucial to topic tracking, knowledge base induction and question answering. Recent work on this problem has focused almost solely on entity linking–based ap∗ Work done during an internship at Google Research. Code and data available at: https://github.com/ rloganiv/streaming-cdc 1 proaches, i.e., models that link mentions to a fixed set of known entities. While significant strides have been made on this front—with systems that can be trained end-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et a"
2021.acl-long.364,D13-1027,0,0.0322108,"ering, the best performance on MedMentions is achieved using GRINCH. These results highlight the importance of benchmarking CDC systems on a number of different datasets; patterns observed on a single dataset do not extrapolate well to other settings. It is also interesting to observe that a much simpler approach often works better than the more complex one. Error Analysis We characterize the errors of these models by investigating: a) the entities whose mentions are conflated (e.g., are wrongly clustered together) and split (e.g., wrongly grouped into separate clusters) using the approach of Kummerfeld and Klein (2013), and b) differences in performance on entities that are seen vs. unseen during training for models that use in-domain data. A sub4723 AIDA MUC B Exact Match Oracle Within-Doc 90.2 15.2 Greedy NN Feature-Based MLM BLINK (Wiki) RELIC (Wiki) RELIC (In-Domain) Hybrid GRINCH MLM BLINK (Wiki) RELIC (Wiki) RELIC (In-Domain) 3 MedMentions CEAF Avg. MUC B 84.1 46.8 81.0 47.1 85.1 36.4 78.8 16.5 94.2 75.9 58.2 92.4 93.2 94.7 89.0 71.1 56.3 89.4 80.7 90.1 87.3 58.1 56.6 83.6 84.5 88.5 90.2 68.4 57.0 88.5 86.1 91.1 37.8 64.3 91.6 82.8 59.2 26.9 88.3 84.0 41.5 23.2 82.5 69.5 46.2 38.1 87.5 78.8 3 Zeshel C"
2021.acl-long.364,N15-1158,0,0.317319,"st CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of research has received little attention from the NLP community. To our knowledge there are only two existing works on the task (Rao et al., 2010; Shrimpton et al., 2015), and only the latter evaluates truly streaming systems, i.e., systems that process new mentions in constant time with constant memory. One crucial factor limiting research on this topic is a lack of free, publicly accessible benchmark datasets; datasets used in existing works are either small and impossible to reproduce (e.g., the dataset collected by Shrimpton et al. (2015) only contains a few hundred unique entities, and many of the 4717 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro"
2021.acl-long.364,P11-1080,1,0.759294,"al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of research has received li"
2021.acl-short.37,P19-1409,0,0.0279343,"nference. This experiment shows that although the multilingual index contributes to MOLEMAN’s overall performance, the pairwise training data is sufficient for high performance in a monolingual setting. 5 Discussion and Future Work We have recast the entity linking problem as an application of a more generic mention encoding task. This approach is related to methods which perform clustering on test mentions in order to improve inference (Le and Titov, 2018; Angell et al., 2020), and can also be viewed as a form of crossdocument coreference resolution (Rao et al., 2010; Shrimpton et al., 2015; Barhom et al., 2019). We also take inspiration from recent instance-based language modelling approaches (Khandelwal et al., 2020; Lewis et al., 2020b). Our experiments demonstrate that taking an instance-based approach to entity-linking leads to better retrieval performance, particularly on rare entities, for which adding a small number of mentions leads to superior performance than a single description. For future work, we would like to explore the application of this instance-based approach to entity knowledge related tasks (Seo et al., 2018; Petroni et al., 2020), and to entity discovery (Ji et al., 2017). 281"
2021.acl-srw.7,D18-1443,0,0.266591,"(Cheng and Lapata, 2016), pointer-generator methods (See et al., 2017) and reinforcement learning (Wu and Hu, 2018). These methods perform well in high resource summarization datasets with small documents such as CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), etc. However, summarization over long documents with thousands of tokens is a more practically relevant problem. Existing solutions focus on leveraging document structure (Cohan et al., 2018) or do mixed model summarization involving compression or selection followed by abstractive summarization (Liu et al., 2018; Gehrmann et al., 2018). However, these methods require large amounts of training data. Low resource settings are common in real world applications as curating domain specific datasets especially over long documents and on a large scale, is both expensive and time consuming. A human summarizing a long document would first understand the text, then highlight the important information, and finally paraphrase it to generate a summary. Building on this intuition, we present a low-resource long document summarization algorithm (Section 2) operating in 3 steps: Abstractive summarization is the task of compressing a long d"
2021.acl-srw.7,P16-1154,0,0.028732,"guage Models Ahsaas Bajaj∗ University of Massachusetts Amherst abajaj@umass.edu Pavitra Dangati* University of Massachusetts Amherst sdangati@umass.edu Kalpesh Krishna University of Massachusetts Amherst Pradhiksha Ashok Kumar University of Massachusetts Amherst Rheeya Uppaal Goldman Sachs Bradford Windsor Goldman Sachs Eliot Brenner Goldman Sachs Rajarshi Das University of Massachusetts Amherst Andrew McCallum University of Massachusetts Amherst Abstract ing key information. Typical abstractive summarization algorithms use seq2seq models with attention (Chopra et al., 2016), copy mechanisms (Gu et al., 2016), content selection (Cheng and Lapata, 2016), pointer-generator methods (See et al., 2017) and reinforcement learning (Wu and Hu, 2018). These methods perform well in high resource summarization datasets with small documents such as CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), etc. However, summarization over long documents with thousands of tokens is a more practically relevant problem. Existing solutions focus on leveraging document structure (Cohan et al., 2018) or do mixed model summarization involving compression or selection followed by abstractive summarization"
2021.acl-srw.7,P17-1099,0,0.044943,"angati* University of Massachusetts Amherst sdangati@umass.edu Kalpesh Krishna University of Massachusetts Amherst Pradhiksha Ashok Kumar University of Massachusetts Amherst Rheeya Uppaal Goldman Sachs Bradford Windsor Goldman Sachs Eliot Brenner Goldman Sachs Rajarshi Das University of Massachusetts Amherst Andrew McCallum University of Massachusetts Amherst Abstract ing key information. Typical abstractive summarization algorithms use seq2seq models with attention (Chopra et al., 2016), copy mechanisms (Gu et al., 2016), content selection (Cheng and Lapata, 2016), pointer-generator methods (See et al., 2017) and reinforcement learning (Wu and Hu, 2018). These methods perform well in high resource summarization datasets with small documents such as CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), etc. However, summarization over long documents with thousands of tokens is a more practically relevant problem. Existing solutions focus on leveraging document structure (Cohan et al., 2018) or do mixed model summarization involving compression or selection followed by abstractive summarization (Liu et al., 2018; Gehrmann et al., 2018). However, these methods require large amounts of"
2021.acl-srw.7,2021.ccl-1.108,0,0.0575876,"Missing"
2021.acl-srw.7,N18-1101,0,0.0272143,"et al., 2018): a strong extract-then-abstract baseline where content selection is posed as a wordlevel sequence tagging problem. Similar to our setting, their content selector also uses large pretrained models (ELMo, Peters et al., 2018), which we finetune on our training set. 3.3 Validating the choice of f (s, t) In Section 2 we used GPT-2 perplexity scores to measure the extent to which a source sentence grounds a target sentence. To motivate this choice, we measure its correlation with existing entailment datasets. We randomly sample 5000 sentences from each class of the MultiNLI dataset (Williams et al., 2018) and compute the perplexity of the hy3 The training and validation splits together comprise of 96 documents. The test split was not used. 74 References pothesis with the premise as context. As seen in Figure 2, entailment pairs tend to have the lowest perplexity. This motivates our choice of f (s, t), since hypothesis sentences are best grounded in premise sentences for entailment pairs. We hypothesize contradiction sentences have slightly lower perplexity than neutral due to more word overlap. To further validate the merit of GPT-2 perplexity, we conduct ablations using alternatives for f (s,"
2021.acl-srw.7,W04-3252,0,0.0334931,"1/L boost in performance and outperform the best baseline (per metric) by 2.0 / 1.0 R-1/L points. Our model’s improvements are statistically significant (p-value< 0.06) except for when comparing our extractor + f.t BART with Bottom-up + f.t BART, the p-value is 0.16 due to the small test set. Refer Appendix A.3 for qualitative analysis of our proposed model’s generations. We present results on the Amicus test set. We compare our model against several competitive baselines: 1. NE: no extraction 2. Random: a random selection of the same number of sentences as our extractive summary 3. TextRank (Mihalcea and Tarau, 2004; Liu et al., 2018): unsupervised graph based approach to rank text chunks within a document 4. Bottom-up summarizer (Gehrmann et al., 2018): a strong extract-then-abstract baseline where content selection is posed as a wordlevel sequence tagging problem. Similar to our setting, their content selector also uses large pretrained models (ELMo, Peters et al., 2018), which we finetune on our training set. 3.3 Validating the choice of f (s, t) In Section 2 we used GPT-2 perplexity scores to measure the extent to which a source sentence grounds a target sentence. To motivate this choice, we measure"
2021.acl-srw.7,K16-1028,0,0.150794,"heeya Uppaal Goldman Sachs Bradford Windsor Goldman Sachs Eliot Brenner Goldman Sachs Rajarshi Das University of Massachusetts Amherst Andrew McCallum University of Massachusetts Amherst Abstract ing key information. Typical abstractive summarization algorithms use seq2seq models with attention (Chopra et al., 2016), copy mechanisms (Gu et al., 2016), content selection (Cheng and Lapata, 2016), pointer-generator methods (See et al., 2017) and reinforcement learning (Wu and Hu, 2018). These methods perform well in high resource summarization datasets with small documents such as CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), etc. However, summarization over long documents with thousands of tokens is a more practically relevant problem. Existing solutions focus on leveraging document structure (Cohan et al., 2018) or do mixed model summarization involving compression or selection followed by abstractive summarization (Liu et al., 2018; Gehrmann et al., 2018). However, these methods require large amounts of training data. Low resource settings are common in real world applications as curating domain specific datasets especially over long documents and on a large scale, is both expensi"
2021.acl-srw.7,P02-1040,0,0.11361,"lin, Germany. Association for Computational Linguistics. 1. Entailment score from a RoBERTa based MNLI classifier (Liu et al., 2019) Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with attentive recurrent neural networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 93–98, San Diego, California. Association for Computational Linguistics. 2. Cosine similarity of averaged embeddings from final layer of BERT (Devlin et al., 2019) 3. BLEU scores (Papineni et al., 2002) We present ROUGE scores using our whole extractthen-abstract pipeline with different choices of f (s, t) in Table 4. We note that perplexity performs the best, 2.4 ROUGE-1 better than the best alternative and also performs 3.41 ROUGE-1 better than entailment. We hypothesize that RoBERTa overfits on the MNLI dataset that also has known biases (Gururangan et al., 2018). The code can be found on Github here.4 4 Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long doc"
2021.acl-srw.7,D19-1616,0,0.0167991,"cantly smaller than the popular CNN/Daily Mail benchmark (Nallapati et al., 2016) and has significantly longer documents and summaries. 3. Feed the compressed document to a state-ofthe-art abstractive summarizer pretrained on a related domain to generate a coherent and fluent summary To tackle data scarcity, we use pretrained language models in all three steps, which show strong generalization (Devlin et al., 2019) and are sample efficient (Yogatama et al., 2019). Notably, our step (1) uses a novel method based on GPT-2 perplexity (Radford et al., 2019) to ground sentences. Unlike prior work (Parida and Motlicek, 2019; Magooda and Litman, 2020) tackling data scarcity in summarization, our method needs no synthetic data augmentation. Moreover, we study a significantly more resource constrained setting — a complex legal briefs dataset (Section 2) with only 120 available (document, summary) pairs and an average of 4.3K tokens per document; Parida and Motlicek (2019) assume access to 90,000 pairs with a maximum of 0.4K source document tokens, Magooda and Litman (2020) use 370 pairs with 0.2K source document tokens. Despite this challenging setup, our method beats an abstractor-only approach by 6 ROUGE-L points"
2021.acl-srw.7,N18-1202,0,0.00857019,"tions. We present results on the Amicus test set. We compare our model against several competitive baselines: 1. NE: no extraction 2. Random: a random selection of the same number of sentences as our extractive summary 3. TextRank (Mihalcea and Tarau, 2004; Liu et al., 2018): unsupervised graph based approach to rank text chunks within a document 4. Bottom-up summarizer (Gehrmann et al., 2018): a strong extract-then-abstract baseline where content selection is posed as a wordlevel sequence tagging problem. Similar to our setting, their content selector also uses large pretrained models (ELMo, Peters et al., 2018), which we finetune on our training set. 3.3 Validating the choice of f (s, t) In Section 2 we used GPT-2 perplexity scores to measure the extent to which a source sentence grounds a target sentence. To motivate this choice, we measure its correlation with existing entailment datasets. We randomly sample 5000 sentences from each class of the MultiNLI dataset (Williams et al., 2018) and compute the perplexity of the hy3 The training and validation splits together comprise of 96 documents. The test split was not used. 74 References pothesis with the premise as context. As seen in Figure 2, entai"
2021.crac-1.11,P10-1143,0,0.0458992,"n-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A cluster tree inherently represents event or entity coreference attempts to solve the multiple alternative flat clusterings, any of which tasks separately, some prior work does exploit the can be selected as the predicted coreference"
2021.crac-1.11,W05-0620,0,0.141208,"y, functions of their lowest common ancestor in the tree structure, such as the linkage score, can be informative of the likelihood of two mentions being in the same predicted flat cluster. 3.2 Relational Dependency Edges Consider the decision of whether two events are coreferent. If the entities involved in the two events are coreferent, we might be more inclined to believe that the two events are coreferent. Similarly, we might be more inclined to believe two entity mentions are coreferent if they participate in the same real-world event. We formalize this intuition by using semantic roles (Carreras and Màrquez, 2005) for modeling dependencies between event mentions and entity mentions. For each event, we identify entity mentions occurring in specific semantic roles. In this work, we use four semantic role labels – ARG0, ARG1, ARG-LOC and ARG-TMP, and collect a set of relational dependency edges between entity mentions and event mentions. In particular, we use argr (mi ) to denote the entity mention in the semantic role r for event mention mi , and arg−1 r (mi ) to denote the event mention for which the entity mention mi occurs in role r. In case an event mention mi does not have an argument in a semantic"
2021.crac-1.11,W09-3208,0,0.0299979,"cially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A cluster tree inherently represents event or entity coreference attempts to solve the multiple alternative flat clusterings, any of which tasks separately, some prior work does exploit the can be selected"
2021.crac-1.11,D17-1226,0,0.032592,"Missing"
2021.crac-1.11,P16-1061,0,0.020852,"r entity X (e) (e) frel (mi ,mj ) = φ(e) mentions given the updated tree over event menr gr (mi , mj ) r tions and alternate until the cost function in Eq 2 X (e) (v) −1 −1 has converged. = φr fsim (argr (mi ), argr (mj )) We use hierarchical agglomerative clustering r (HAC) to build cluster trees as it allows use of an arWe also experiment with relational similarity bitrary pairwise scoring function and is widely used (v) features gr for a pair of event mentions computed for coreference resolution in prior work (Lee et al., (e) (e) (e) using fsim = find instead of defining fsim using a 2012; Clark and Manning, 2016; Barhom et al., (hierarchical) clustering of entity mentions. Addi- 2019; Cattan et al., 2020, inter-alia). Moreover, tionally, we experiment with relational similarity average-linkage HAC provides a 2-approximation (e) features computed using fsim defined using cluster- to a dissimilarity-based version of Dasgupta’s cost (e) (e) ing of entity mentions as well as using fsim = find . function (Cohen-Addad et al., 2019). Our proposed We define relational features analogously for entity inference alternates between optimizing the first mentions as well. and the second term in Eq 2. Clustering ev"
2021.crac-1.11,cybulska-vossen-2014-using,0,0.0208502,"ns given event mentions using the joint similarity function (e) fjoint optimizes the second term in Eq. 2. Finally, on convergences, we use a threshold to pick a flat clustering over event and entity mentions given their cluster trees. At test time, we use threshold values picked using dev data. 4 Experiments # Topics # Documents # Event Mentions # Event Clusters # Entity Mentions # Entity Clusters Train 25 574 3808 1527 4758 1286 Dev 8 196 1245 409 1476 330 Test 10 206 1780 805 2055 608 Total 43 976 6833 2741 8289 2224 Table 1: Statistics on the standard train/dev/test split of ECB+ dataset (Cybulska and Vossen, 2014). Evaluation Metrics and Setup We use the official CoNLL scorer (Pradhan et al., 2014) and report coreference resolution metrics like MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the average of these three metric, the CONLL-F1 metric. We follow the evaluation setup of (Cybulska and Vossen, 2014), also used in other recent work (Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Following the precedent set by recent work, we remove singletons from both gold as well as predicted clusters during evaluation. We use gold event and entity mentions,"
2021.crac-1.11,H05-1013,0,0.25859,"Missing"
2021.crac-1.11,D13-1203,0,0.0339401,"). We formalize a cost function that uses an independently trained pairwise similarity (§ 3.4) along with relational similarity (§ 3.5) defined using these dependencies in a way that captures uncertainty over coreference decisions. Finally, we describe our joint inference procedure to optimize the proposed cost function in § 3.7. Entity coreference resolution, especially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean"
2021.crac-1.11,I17-1081,0,0.0652444,"Missing"
2021.crac-1.11,N12-1007,0,0.0320572,"r tree inherently represents event or entity coreference attempts to solve the multiple alternative flat clusterings, any of which tasks separately, some prior work does exploit the can be selected as the predicted coreference of menargument-predicate structure to derive additional features for enriching entity/event representations. tions. These tree structures are a popular choice among cross-document coreference models, and In this work, we go a step further and perform joint are typically built using hierarchical agglomerative clustering of event and entity mentions instead of clustering (Green et al., 2012; Kenyon-Dean et al., merely using event or entity mentions to derive 2018; Cattan et al., 2020, inter alia). additional lexical features. A cluster tree has mentions as its leaves. For Prior work that jointly predicts event and entity instance, a cluster tree over event mentions would coreference decisions does so at a flat clustering have Mv as its set of leaves. Each internal node level (Lee et al., 2012; Barhom et al., 2019). The joint clustering method incrementally merges en- represents the cluster of its descendant leaves. An internal node with children x and y is associated tity or eve"
2021.crac-1.11,2021.eacl-main.265,0,0.034299,"resent jointness tal NLP task that has several downstream appli- only at a flat clustering level. cations such as question answering (Chen et al., In this work, we present a cost function that cap2019; Bhattacharjee et al., 2020), textual entail- tures both the dependency between coreference ment (Mitkov et al., 2012), building and maintain- decisions of event and entity mentions as well as ing KBs (Angeli et al., 2015; Angell et al., 2021), the uncertainty of these decisions. Our proposed and multi-document summarization (Falke et al., cost extends Dasgupta’s cost function for hierarchi2017; Huang and Kurohashi, 2021). Often these cal (tree-structured) clustering (Dasgupta, 2016), downstream applications consume a set of docu- generalizing it to model the dependencies between ments, and thus require detection of coreference two separate clustering problems of event and enrelations between event and entity mentions spread tity coreference. To optimize this cost, we describe across documents such as multiple news articles. an efficient inference procedure, which is based Event and entity coreference decisions often on iterated conditional modes. Our inference alhave rich dependencies on each other. Consider"
2021.crac-1.11,2020.tacl-1.5,0,0.014563,"reference decisions. Finally, we describe our joint inference procedure to optimize the proposed cost function in § 3.7. Entity coreference resolution, especially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A cluster tree inherently represents event or entity"
2021.crac-1.11,S18-2001,0,0.0581888,"Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A cluster tree inherently represents event or entity coreference attempts to solve the multiple alternative flat clusterings, any of which tasks separately, some prior work does exploit the can be selected as the predicted coreference of menargument-predicate structure to derive additional features for enriching entity/event representations. tions. These tree structures are a popular choice among cross-document coreference models, and In this work, we go"
2021.crac-1.11,D12-1045,0,0.204449,"n help avoid such errors. For instance, coreference relationships between arguments of v1, v2, and v3 could provide crucial evidence in support of v1 and v2 being coreferent, and v2 and v3 being not coreferent. Previous work that exploits the argumentpredicate structure either rely on lexical similarity between arguments while resolving related event mentions (Yang et al., 2015; Choubey and Huang, 2017; Yu et al., 2020, inter alia), or is limited to al1 Introduction gorithmic approaches without an explicitly defined Coreference resolution refers to the task of detect- cost function (He, 2007; Lee et al., 2012; Barhom et al., 2019). Moreover, these approaches do not ing mentions of various entities and events and capture the uncertainty in coreference decisions identifying groups of mentions referring to the same real-world entity or event. It is a fundamen- during the inference as they represent jointness tal NLP task that has several downstream appli- only at a flat clustering level. cations such as question answering (Chen et al., In this work, we present a cost function that cap2019; Bhattacharjee et al., 2020), textual entail- tures both the dependency between coreference ment (Mitkov et al.,"
2021.crac-1.11,D17-1018,0,0.0175743,"s in a way that captures uncertainty over coreference decisions. Finally, we describe our joint inference procedure to optimize the proposed cost function in § 3.7. Entity coreference resolution, especially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A clu"
2021.crac-1.11,N18-2108,0,0.0364678,"Missing"
2021.crac-1.11,2021.ccl-1.108,0,0.017861,"Missing"
2021.crac-1.11,H05-1004,0,0.210054,"ster trees. At test time, we use threshold values picked using dev data. 4 Experiments # Topics # Documents # Event Mentions # Event Clusters # Entity Mentions # Entity Clusters Train 25 574 3808 1527 4758 1286 Dev 8 196 1245 409 1476 330 Test 10 206 1780 805 2055 608 Total 43 976 6833 2741 8289 2224 Table 1: Statistics on the standard train/dev/test split of ECB+ dataset (Cybulska and Vossen, 2014). Evaluation Metrics and Setup We use the official CoNLL scorer (Pradhan et al., 2014) and report coreference resolution metrics like MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the average of these three metric, the CONLL-F1 metric. We follow the evaluation setup of (Cybulska and Vossen, 2014), also used in other recent work (Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Following the precedent set by recent work, we remove singletons from both gold as well as predicted clusters during evaluation. We use gold event and entity mentions, evaluate coreference performance at topic level with gold topics, and finally compute a micro average of the metrics across all topics. Dataset We run experiments on the ECB+ corpus which consists of both wi"
2021.crac-1.11,2020.findings-emnlp.440,0,0.0112804,"uncertainty over coreference decisions. Finally, we describe our joint inference procedure to optimize the proposed cost function in § 3.7. Entity coreference resolution, especially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a hierarchical clustering or cluster tree, Kenyon-Dean et al., 2018; Cattan et al., 2020; Yu denoted T , to represent the uncertainty of coreferet al., 2020). While the majority of the work on ence decisions. A cluster tree inherently repre"
2021.crac-1.11,M95-1005,0,0.803589,"t clustering over event and entity mentions given their cluster trees. At test time, we use threshold values picked using dev data. 4 Experiments # Topics # Documents # Event Mentions # Event Clusters # Entity Mentions # Entity Clusters Train 25 574 3808 1527 4758 1286 Dev 8 196 1245 409 1476 330 Test 10 206 1780 805 2055 608 Total 43 976 6833 2741 8289 2224 Table 1: Statistics on the standard train/dev/test split of ECB+ dataset (Cybulska and Vossen, 2014). Evaluation Metrics and Setup We use the official CoNLL scorer (Pradhan et al., 2014) and report coreference resolution metrics like MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the average of these three metric, the CONLL-F1 metric. We follow the evaluation setup of (Cybulska and Vossen, 2014), also used in other recent work (Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Following the precedent set by recent work, we remove singletons from both gold as well as predicted clusters during evaluation. We use gold event and entity mentions, evaluate coreference performance at topic level with gold topics, and finally compute a micro average of the metrics across all topics. Dataset We run experim"
2021.crac-1.11,C02-1139,0,0.20387,"dencies between event and entity mentions (§ 3.2). We formalize a cost function that uses an independently trained pairwise similarity (§ 3.4) along with relational similarity (§ 3.5) defined using these dependencies in a way that captures uncertainty over coreference decisions. Finally, we describe our joint inference procedure to optimize the proposed cost function in § 3.7. Entity coreference resolution, especially in the within-document setting has seen a tremendous amount of improvement over recent years. Early work on entity coreference used hand-crafted syntactic and semantic features (Ng and Cardie, 2002; Daumé III and Marcu, 2005; Durrett and Klein, 2013) while recent top-performing models are neural models that perform mention detection, followed by mention clustering in an end-to-end fashion (Lee et al., 2017, 2018; Meged et al., 2020; Joshi et al., 2020). Event coreference has also seen similar trends with early work using lexical features such as Wordnet synsets, head lemma of the verb (Chen and Ji, 2009; Bejan and Harabagiu, 2010) while more recent work uses event embeddings from pre-trained word embeddings or 3.1 Cluster Trees pretrained language models (Lu and Ng, 2018; We will use a"
2021.crac-1.11,P14-2006,0,0.0181614,"term in Eq. 2. Finally, on convergences, we use a threshold to pick a flat clustering over event and entity mentions given their cluster trees. At test time, we use threshold values picked using dev data. 4 Experiments # Topics # Documents # Event Mentions # Event Clusters # Entity Mentions # Entity Clusters Train 25 574 3808 1527 4758 1286 Dev 8 196 1245 409 1476 330 Test 10 206 1780 805 2055 608 Total 43 976 6833 2741 8289 2224 Table 1: Statistics on the standard train/dev/test split of ECB+ dataset (Cybulska and Vossen, 2014). Evaluation Metrics and Setup We use the official CoNLL scorer (Pradhan et al., 2014) and report coreference resolution metrics like MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the average of these three metric, the CONLL-F1 metric. We follow the evaluation setup of (Cybulska and Vossen, 2014), also used in other recent work (Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Following the precedent set by recent work, we remove singletons from both gold as well as predicted clusters during evaluation. We use gold event and entity mentions, evaluate coreference performance at topic level with gold topics, and finally compute"
2021.eacl-main.223,2020.emnlp-main.525,1,0.804005,"chieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grained control and/or require substantial human labor. In some prior work (Keskar et al., 2019; Tu et al., 2019), users choose among a static set of predefined attributes (e.g., sentiment) that only provide coarse-grained control. Other work (Roemmele and Gordon, 2015; Clark et al., 2018) presents users with multiple generated continuations, which requires substantial reading effort and might not contain topics that users want to see. Finally, options could be nod"
2021.eacl-main.223,P19-1285,0,0.0671411,"Missing"
2021.eacl-main.223,N19-1423,0,0.0116174,"on generator uses GPT2 to encode the input prompt x1 , ..., xI and passes the output embedding to K different linear layers L1 , ..., LK . To model the dependency of clusters, a Transformer (Vaswani et al., 2017) takes the K embeddings as input and predicts the cluster centers c1 , ...cK in GloVe (Pennington et al., 2014) space. During testing, each predicted cluster center is normalized by its L2 norm, and we use the M closest words in the normalized GloVe space to represent the topic Ti , which users can choose. We choose to learn the cluster centers in GloVe space rather than GPT2 or BERT (Devlin et al., 2019) space because the non-contextualized word embeddings are easier to visualize. Users can easily understand the meaning of a cluster center by seeing nearby words. We normalize GloVe space in this work to make the squared L2 distance equal to twice the cosine distance between two embeddings. Our architecture is similar to the one in Chang et al. (2021), but we use a pretrained GPT2 encoder rather than train a BERT-like Transformer from scratch. Another difference is that we ignore the connection between the second Transformer and the output of GPT2 to save GPU memory for handling a longer input"
2021.eacl-main.223,P18-1082,0,0.150572,"loVe embeddings are first passed through a linear layer to make their dimension become the same as the hidden state size of GPT2. Then, the transformed embeddings are added with special positional embeddings pfI , which are different from those for the prompt pw . i The special positional embedding tells GPT2 that the inserted embeddings have a different meaning and where the conditional generation starts. The GPT2 encoder’s output goes through a softmax layer, which computes the probability of each token being observed as the first word piece in the continuation y1 . We adopt top-k sampling (Fan et al., 2018), which reduces the chance of sampling words with low probability, to pick the next word, and autoregressively sample one token ybo at a time to generate the continuation yb1 , ..., ybO . 2.2.2 Model Training We train the generator using the continuation of a prompt and some randomly selected nonstop words in the continuation as its generation conditions. Since the continuation contains the randomly-selected words, the generator would be heavily penalized if it ignores the conditions by assigning low probabilities to the selected words in all the continuation positions. An example is illustrat"
2021.eacl-main.223,P19-1254,0,0.0478183,"Missing"
2021.eacl-main.223,W19-2405,0,0.0167825,"n et al., 2019), a sequence of event structure (subject, verb, object, preposition, modifier) (Ammanabrolu et al., 2020), a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphras"
2021.eacl-main.223,N16-1014,0,0.0319295,"46 ± 3.47 56.41 ± 4.41 4.07 ± 0.10 24.49 ± 2.77 48.69 ± 4.61 4.15 ± 0.11 Table 5: Comparison of conditional text generators. The numbers in Dist-1, Dist-2, Recall, and Precision are percentages. Lower perplexity (PPL) and inference time are better. The better performances between PPLM and our method are highlighted. In human evaluation, we report the mean ± standard error of each method. words related to the topics. The fluency of the generated text is measured using the perplexity (Serban et al., 2016) of the original GPT2 (with 345M parameters) without being fine-tuned on Wikipedia. Dist-n (Li et al., 2016) is the ratio between the number of unique n-grams and the number of all n-grams in the continuations, where n=1 or 2. Higher Dist-n implies more diverse generations. The average inference time per input prompt is also presented. 3.3.2 Human Evaluation We present the prompt and the generated continuation and ask the worker to score the generation’s fluency from 1 (not fluent at all) to 5 (very fluent). Next, we show K topics and ask which topics are mentioned in the generation. Treating the worker’s choices as prediction and the topics our model conditions on as ground truth, we report the ave"
2021.eacl-main.223,2020.findings-emnlp.165,0,0.0377742,"s in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics. 5 Conclusion We propose an interactive writing assistant that generates topic options given an input prompt and generates the continuatio"
2021.eacl-main.223,P17-4008,0,0.0257131,"n We present the prompt and the generated continuation and ask the worker to score the generation’s fluency from 1 (not fluent at all) to 5 (very fluent). Next, we show K topics and ask which topics are mentioned in the generation. Treating the worker’s choices as prediction and the topics our model conditions on as ground truth, we report the average precision and recall of the prediction. 3.3.3 Conditional Text Generator Baselines We compare our method with PPLM (Plug and Play Language Models) (Dathathri et al., 2020) due to its strong performance against the weighted decoding approach from Ghazvininejad et al. (2017) when the condition is a bag of words. The condition for PPLM is the union of the top M words in the chosen topics and each word’s weight is neglected. We use our generation model without conditioning on any word (i.e., n = 0) during testing3 as the base model of PPLM. We also present the performance of the base model itself as a reference to know the significance of our improvement (denoted as GPT2). 3.3.4 Results Table 5 indicates that our model outperforms PPLM in all metrics except in Dist-1 and Dist-2. We suspect that our model generates slightly less 3 We find the model performs similarl"
2021.eacl-main.223,N19-4016,0,0.0212796,"a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the"
2021.eacl-main.223,Q18-1027,0,0.0183595,"let users express their preferences. The options could be manually defined classes (e.g., sentiment) (Keskar et al., 2019; Dathathri et al., 2020), semantic frames (Tu et al., 2019), or event structures such as (subject, verb, object, modifier) (Martin et al., 2018; Tambwekar et al., 2019; Ammanabrolu et al., 2020). The forms of options allow users to control the attributes of the generated text but require labels or classifiers that map the text to the attributes/options. The options could also be a single query word at the beginning (Austin, 2019), the article title (Yan, 2016), politeness (Niu and Bansal, 2018) or specificity (See et al., 2019b) of the text, or the length of the generated sentence (Tu et al., 2019). However, the options cannot provide fine-grained control on topical directions of the generated contents. A related research direction is the multi-stage story generation. To make a long story more coherent, recent work proposes to generate a skeleton and then generate the full text guided by 2608 the skeleton. The skeleton could be a sequence of SRL frames (Fan et al., 2019), a sequence of event structure (subject, verb, object, preposition, modifier) (Ammanabrolu et al., 2020), a story"
2021.eacl-main.223,W18-1505,0,0.0217192,"olu et al., 2020), a story premise (Fan et al., 2018), or a story summary (Chen et al., 2019). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrase"
2021.eacl-main.223,D14-1162,0,0.0873552,"al., 2019) if the corresponding label data are available in the training corpus. Model Prediction The goal of our option generator is to predict the K cluster centers of words in the possible continuations and use the cluster centers as the topics user could choose from. As in Figure 3 (b), the option generator uses GPT2 to encode the input prompt x1 , ..., xI and passes the output embedding to K different linear layers L1 , ..., LK . To model the dependency of clusters, a Transformer (Vaswani et al., 2017) takes the K embeddings as input and predicts the cluster centers c1 , ...cK in GloVe (Pennington et al., 2014) space. During testing, each predicted cluster center is normalized by its L2 norm, and we use the M closest words in the normalized GloVe space to represent the topic Ti , which users can choose. We choose to learn the cluster centers in GloVe space rather than GPT2 or BERT (Devlin et al., 2019) space because the non-contextualized word embeddings are easier to visualize. Users can easily understand the meaning of a cluster center by seeing nearby words. We normalize GloVe space in this work to make the squared L2 distance equal to twice the cosine distance between two embeddings. Our archite"
2021.eacl-main.223,2020.emnlp-main.349,0,0.0234431,"Missing"
2021.eacl-main.223,K19-1079,0,0.20838,"tates United 10 Step 1: Let’s see what language models would say Input Prompt: “Barack Obama writes a new book” Step 3: Please talk more about these topics User Figure 1: Given an input prompt, the Transformerbased LM provides K = 10 topics that might be mentioned next and each topic is represented by M = 3 words. The user could guide the generation process by choosing a subset of topics. Introduction Recently, Transformer-based language models (LMs) have achieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grai"
2021.eacl-main.223,N19-1170,0,0.226503,"tates United 10 Step 1: Let’s see what language models would say Input Prompt: “Barack Obama writes a new book” Step 3: Please talk more about these topics User Figure 1: Given an input prompt, the Transformerbased LM provides K = 10 topics that might be mentioned next and each topic is represented by M = 3 words. The user could guide the generation process by choosing a subset of topics. Introduction Recently, Transformer-based language models (LMs) have achieved impressive performance in language generation tasks (Radford et al., 2019; Dai et al., 2019) such as open-domain story generation (See et al., 2019a). When writing with the LM, users often desire an intuitive and effective way to control what a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grai"
2021.eacl-main.223,D19-5605,0,0.253381,"at a LM is going to generate (Keskar et al., 2019). To address this need, interactive writing assistants provide options to reveal possible developments of the story and generate continuations guided by the user-selected options. Interactive writing assistants have wide applications in creative writing (Roemmele and Gordon, 2015; Clark et al., 2018; Akoury et al., 2020), education (Luo et al., 2015), and gaming (Walton, 2020). Nevertheless, the existing systems’ options usually do not provide fine-grained control and/or require substantial human labor. In some prior work (Keskar et al., 2019; Tu et al., 2019), users choose among a static set of predefined attributes (e.g., sentiment) that only provide coarse-grained control. Other work (Roemmele and Gordon, 2015; Clark et al., 2018) presents users with multiple generated continuations, which requires substantial reading effort and might not contain topics that users want to see. Finally, options could be nodes in a plot graph that are handcrafted (Luo et al., 2015) or derived from a collaboration between humans and machine (Li et al., 2013), but such choices are usually limited due to the high cost of preparing the options. To address these limita"
2021.eacl-main.223,D18-1462,0,0.0244819,"ches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics. 5 Conclusion We propose an interactive writing assistant that generates topi"
2021.eacl-main.223,2020.emnlp-main.698,0,0.0197423,"19). Users can revise the skeleton to control the generated text, but the approaches assume the existence of the skeleton extractor or labels in the training corpus. Besides, the systems cannot suggest options given the partial text, which is one of the main focuses of our interactive writing assistant. The skeleton could also be multiple keyphrases. The keyphrases are extracted based on word frequency (Ippolito et al., 2019; Tan et al., 2020; Wu et al., 2020), an off-the-shelf keyword extraction method (Peng et al., 2018; Goldfarb-Tarrant et al., 2019; Yao et al., 2019; Rashkin et al., 2020; Zhang et al., 2020), a sentence compression dataset and reinforcement learning (Xu et al., 2018), or image caption datasets and ConceptNet (Lin et al., 2020). Most of the studies focus on modeling the longterm dependency among the keyphrases and/or forcing the generation to contain the keyphrases. Instead, we focus on allowing users to determine the topical directions of the generation. Compared with conditioning on keyphrases, our interactive writing assistant is especially helpful when users do not know the exact phrases they want to see or when the given keyphrase extractor does not detect the desired topics."
2021.eacl-main.77,D19-1645,0,0.0159302,"As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we squeeze the facets of a sentence pattern into a single embedding, the embedding is more likely to be affected by the irrelevant facets f"
2021.eacl-main.77,P12-2011,0,0.0365768,"es is the cost of collecting the labels. Distant supervision (Mintz et al., 2009) and its follow-up work enable us to collect a large amount of training data at a low cost, but the violation of its assumptions often introduces substantial noise into the supervision signal. Our goal is to alleviate the noise issue by representing every sentence pattern using multiple embeddings. Other noise reduction methods have also been proposed (Roth et al., 2013). For instance, we can adopt multi-instance learning techniques (Yao et al., 2010; Surdeanu et al., 2012; Amin et al., 2020), global topic model (Alfonseca et al., 2012), or both (Roth and Klakow, 2013). We can also reduce the noise by counting the number of shared entity pairs between a sentence pattern and a KB relation (Takamatsu et al., 2012; Su et al., 2018). Nevertheless, the studies focus on mitigating the noise caused by assuming similarity between the sentence patterns and KB relations that co-occur with the same entity pairs, while our method can also reduce the noise from two sentence patterns sharing the same entity pair. Besides, our method is complementary to popular noise reduction methods because our improvement is shown in the training data t"
2021.eacl-main.77,2020.bionlp-1.20,0,0.0232893,"and point out one of the major challenges is the cost of collecting the labels. Distant supervision (Mintz et al., 2009) and its follow-up work enable us to collect a large amount of training data at a low cost, but the violation of its assumptions often introduces substantial noise into the supervision signal. Our goal is to alleviate the noise issue by representing every sentence pattern using multiple embeddings. Other noise reduction methods have also been proposed (Roth et al., 2013). For instance, we can adopt multi-instance learning techniques (Yao et al., 2010; Surdeanu et al., 2012; Amin et al., 2020), global topic model (Alfonseca et al., 2012), or both (Roth and Klakow, 2013). We can also reduce the noise by counting the number of shared entity pairs between a sentence pattern and a KB relation (Takamatsu et al., 2012; Su et al., 2018). Nevertheless, the studies focus on mitigating the noise caused by assuming similarity between the sentence patterns and KB relations that co-occur with the same entity pairs, while our method can also reduce the noise from two sentence patterns sharing the same entity pair. Besides, our method is complementary to popular noise reduction methods because ou"
2021.eacl-main.77,P17-1151,0,0.0175977,"g equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Technology Collaborative, in part by the National Science Foundation (NSF) grant numbers DMR-1534431 and IIS-1514053. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. Our method is conceptually related to some studies for lexical semantics. For example, word sense induction or unsupervised hypernymy detection can be addressed by clustering the co-occurring words (Neelakantan et al., 2014; Athiwaratkun and Wilson, 2017; Chang et al., 2018). However, the clustering-based methods do not apply to RE because the co-occurring matrix for RE is much sparser (see Section 2.4 for more details). Finally, our work is inspired by Chang et al. (2021), but they focus on improving the sentence representation rather than RE. We encourage the facet embeddings to become the centers in Kmeans clustering instead of NNSC (non-negative sparse coding) clustering used in Chang et al. (2021), due to its simplicity, efficiency, and better RE performance. Moreover, we discover that an additional regularization described in the append"
2021.eacl-main.77,P19-1279,0,0.110159,"erence of the European Chapter of the Association for Computational Linguistics, pages 909–919 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Bell and Dax Shepard. Compositional universal schema (Verga et al., 2016) realizes the idea by using a LSTM (Hochreiter and Schmidhuber, 1997) to encode each sentence pattern into an embedding and encouraging the embedding to be similar to the embedding of the co-occurred entity pair. As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern"
2021.eacl-main.77,P17-2057,1,0.853261,"dding to be similar to the embedding of the co-occurred entity pair. As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we squeeze the facets of a sentence pattern into a single embedding"
2021.eacl-main.77,2020.aacl-main.75,0,0.0606232,"ural Encoder Pattern Embedding Table Lookup Close Entity Pair Embedding Neural Encoder Close Pattern Embedding Figure 1: Comparison between the multi-facet and compositional universal schema. In our training loss, we encourage one of the facet embeddings from a sentence pattern to be similar to its co-occurred entity pair. Relation extraction (RE) is a crucial step in automatic knowledge base construction (AKBC). A major challenge of RE is that the frequency of relations in the real world is a long-tail distribution but collecting sufficient human annotations for every relation is infeasible (Han et al., 2020). Distant supervision is proposed to alleviate the issue (Mintz et al., 2009). Distant supervision assumes that a sentence pattern expresses a relation if the sentence pattern co-occurs with an entity pair and the entity pair has the relation. For example, we assume the sentence pattern “$ARG1, the partner of fellow $ARG2” is likely to express the spouse * indicates equal contribution Pattern Embedding 2 Entity Pair Embedding Pattern Embedding 4 Introduction ∗ Pattern Embedding 1 Close relation if we observe a text clip “... Angelina Jolie, the partner of fellow Brad Pitt ...” in our training"
2021.eacl-main.77,D17-1109,0,0.0278526,"with “$ARG1 , the wife of $ARG2” and “$ARG1 and her fellow $ARG2”. Another possible reason is that the articles in our corpus tend to use more specific patterns to express the relation between a pair of entities (Shwartz et al., 2017). When performing RE, we compute the symmetric similarity between ith sentence pattern and jth (4) Experiments We primarily compare our method with compositional universal schema (CUSchema) (Verga et al., 2016) because CUSchema is one of the state-of-theart RE methods in the small model regime (without using large pre-trained language models) (Chang et al., 2016; Chaganty et al., 2017).1 In Section 3.1, we visualize and analyze the facet embeddings. Next, we use distant-supervised RE tasks to evaluate our symmetric similarity measurement in Section 3.2, and detect entailment between sentence patterns to evaluate our asymmetric similarity measurement in Section 3.3. 3.1 Embedding Visualization We visualize the embeddings of sentence patterns and a KB relation from the single embedding model and multi-facet embedding model that perform the best in the RE tasks (i.e., Ours (Single-Trans) and Ours (Trans) in Table 1). We project the facet embeddings to a 2-dimensional space usi"
2021.eacl-main.77,N18-1045,1,0.764036,"ant from the Collaborative R&D Fund managed by the Massachusetts Technology Collaborative, in part by the National Science Foundation (NSF) grant numbers DMR-1534431 and IIS-1514053. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. Our method is conceptually related to some studies for lexical semantics. For example, word sense induction or unsupervised hypernymy detection can be addressed by clustering the co-occurring words (Neelakantan et al., 2014; Athiwaratkun and Wilson, 2017; Chang et al., 2018). However, the clustering-based methods do not apply to RE because the co-occurring matrix for RE is much sparser (see Section 2.4 for more details). Finally, our work is inspired by Chang et al. (2021), but they focus on improving the sentence representation rather than RE. We encourage the facet embeddings to become the centers in Kmeans clustering instead of NNSC (non-negative sparse coding) clustering used in Chang et al. (2021), due to its simplicity, efficiency, and better RE performance. Moreover, we discover that an additional regularization described in the appendixis crucial to overc"
2021.eacl-main.77,D14-1113,1,0.897164,"d facet embedding ηk si,k is the same as minimizing a Kmeans loss, so the loss term induced by positive sample pairs encourage each si,k to become the cluster center of its nearby co-occurring entity pair embeddings. The details of our training algorithm could be found in the appendix. The co-occurrence matrices in RE tasks are usually extremely sparse, and most of the sentence patterns only co-occur with a few entity pairs, which makes it difficult to derive multiple high-quality embeddings by clustering the co-occurring entity pair embeddings as in multi-sense word embedding methods such as Neelakantan et al. (2014). The proposed method solves this sparsity challenge by predicting the cluster centers using a neural model. For instance, even if “$ARG1 ’s partner $ARG2” does not co-occur with many entity pairs, its embeddings are encouraged to be close to the embeddings of entity pairs that co-occur with other similar patterns (e.g., “$ARG1 and her partner $ARG2”). 912 Asym({˜si,k}, {˜sj,m}) Asym({˜sj,m}, {˜si,k}) s˜i,k s˜j,m KB relation Sim({˜ si,k }, {˜ sj,m }) by s˜i,k s˜j,m Asym({˜ si,k }, {˜ sj,m }) + Asym({˜ sj,m }, {˜ si,k }) . 2 3 Figure 3: Comparison of the asymmetric similarities. Asym({˜ si,k },"
2021.eacl-main.77,N13-1008,1,0.894543,"ttern “$ARG1, the partner of fellow $ARG2” is likely to express the spouse * indicates equal contribution Pattern Embedding 2 Entity Pair Embedding Pattern Embedding 4 Introduction ∗ Pattern Embedding 1 Close relation if we observe a text clip “... Angelina Jolie, the partner of fellow Brad Pitt ...” in our training corpus and a knowledge base tells us that Angelina Jolie and Brad Pitt has the spouse relation. Accordingly, we can infer that another entity pair is likely to have the spouse relation if we observe the text “, the partner of fellow” between them in a new corpus. Universal schema (Riedel et al., 2013) extends this assumption by treating every sentence pattern as a relation, which means we assume that sentence patterns or relations in a knowledge base are similar if they co-occur with the same entity pair. For example, we assume “$ARG1, the partner of fellow $ARG2” and “$ARG1, the wife of fellow $ARG2” are similar if they both co-occur with (Kristen Bell, Dax Shepard). Consequently, we can infer that “$ARG1, the wife of fellow $ARG2” also implies spouse relation as “$ARG1, the partner of fellow $ARG2” even if the knowledge base does not record the spouse relation between Kristen 909 Proceed"
2021.eacl-main.77,E17-1007,0,0.0384759,"atterns have an entailment relation. We suspect the reason is that more specific patterns could contain more words that are similar to the words of other patterns expressing different relations. For example, “$ARG1 , the wife of fellow $ARG2” have a facet embedding for spouse relation and another facet embedding for the co-worker relation because the pattern has high word overlapping with “$ARG1 , the wife of $ARG2” and “$ARG1 and her fellow $ARG2”. Another possible reason is that the articles in our corpus tend to use more specific patterns to express the relation between a pair of entities (Shwartz et al., 2017). When performing RE, we compute the symmetric similarity between ith sentence pattern and jth (4) Experiments We primarily compare our method with compositional universal schema (CUSchema) (Verga et al., 2016) because CUSchema is one of the state-of-theart RE methods in the small model regime (without using large pre-trained language models) (Chang et al., 2016; Chaganty et al., 2017).1 In Section 3.1, we visualize and analyze the facet embeddings. Next, we use distant-supervised RE tasks to evaluate our symmetric similarity measurement in Section 3.2, and detect entailment between sentence p"
2021.eacl-main.77,N18-1075,0,0.0391431,"Missing"
2021.eacl-main.77,D12-1042,0,0.0386061,"trend of recent studies and point out one of the major challenges is the cost of collecting the labels. Distant supervision (Mintz et al., 2009) and its follow-up work enable us to collect a large amount of training data at a low cost, but the violation of its assumptions often introduces substantial noise into the supervision signal. Our goal is to alleviate the noise issue by representing every sentence pattern using multiple embeddings. Other noise reduction methods have also been proposed (Roth et al., 2013). For instance, we can adopt multi-instance learning techniques (Yao et al., 2010; Surdeanu et al., 2012; Amin et al., 2020), global topic model (Alfonseca et al., 2012), or both (Roth and Klakow, 2013). We can also reduce the noise by counting the number of shared entity pairs between a sentence pattern and a KB relation (Takamatsu et al., 2012; Su et al., 2018). Nevertheless, the studies focus on mitigating the noise caused by assuming similarity between the sentence patterns and KB relations that co-occur with the same entity pairs, while our method can also reduce the noise from two sentence patterns sharing the same entity pair. Besides, our method is complementary to popular noise reductio"
2021.eacl-main.77,P12-1076,0,0.0390518,"e violation of its assumptions often introduces substantial noise into the supervision signal. Our goal is to alleviate the noise issue by representing every sentence pattern using multiple embeddings. Other noise reduction methods have also been proposed (Roth et al., 2013). For instance, we can adopt multi-instance learning techniques (Yao et al., 2010; Surdeanu et al., 2012; Amin et al., 2020), global topic model (Alfonseca et al., 2012), or both (Roth and Klakow, 2013). We can also reduce the noise by counting the number of shared entity pairs between a sentence pattern and a KB relation (Takamatsu et al., 2012; Su et al., 2018). Nevertheless, the studies focus on mitigating the noise caused by assuming similarity between the sentence patterns and KB relations that co-occur with the same entity pairs, while our method can also reduce the noise from two sentence patterns sharing the same entity pair. Besides, our method is complementary to popular noise reduction methods because our improvement is shown in the training data that have been cleaned (Verga et al., 2016). Conclusion In this work, we address the limitation of representing each sentence pattern using only a single embedding, and our approa"
2021.eacl-main.77,D15-1174,0,0.0313386,"each sentence pattern into an embedding and encouraging the embedding to be similar to the embedding of the co-occurred entity pair. As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we sque"
2021.eacl-main.77,N16-1103,1,0.309338,"not every facet is similar to all the facets of another sentence pattern cooccurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they cooccur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their singlefacet embedding counterpart, compositional universal schema (CUSchema) (Verga et al., 2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available. 1 Pattern Embedding 3 Close Pattern Embedding 3 Pattern Embedding 4 Pattern Embedding 5 Pattern Embedding 5 Neural Encoder and Decoder Neural Encoder and Decoder Table Lookup Training Data Sentence Pattern co-occur Entity Pair co-occur Sentence Pattern $ARG1 moved in $ARG1 = Angelina Jolie $ARG1, the partner with $ARG2 $ARG2 = Brad Pitt of fellow $ARG2 Baseline: Compositional Universal Schema Neural"
2021.eacl-main.77,E17-1058,1,0.838821,"nto an embedding and encouraging the embedding to be similar to the embedding of the co-occurred entity pair. As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we squeeze the facets of a s"
2021.eacl-main.77,N18-1080,1,0.843465,"of the co-occurred entity pair. As in the lower part of Figure 1, the model makes the embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we squeeze the facets of a sentence pattern into a single embedding, the embedding is more likely to be af"
2021.eacl-main.77,D10-1099,1,0.648996,"20) summarize the trend of recent studies and point out one of the major challenges is the cost of collecting the labels. Distant supervision (Mintz et al., 2009) and its follow-up work enable us to collect a large amount of training data at a low cost, but the violation of its assumptions often introduces substantial noise into the supervision signal. Our goal is to alleviate the noise issue by representing every sentence pattern using multiple embeddings. Other noise reduction methods have also been proposed (Roth et al., 2013). For instance, we can adopt multi-instance learning techniques (Yao et al., 2010; Surdeanu et al., 2012; Amin et al., 2020), global topic model (Alfonseca et al., 2012), or both (Roth and Klakow, 2013). We can also reduce the noise by counting the number of shared entity pairs between a sentence pattern and a KB relation (Takamatsu et al., 2012; Su et al., 2018). Nevertheless, the studies focus on mitigating the noise caused by assuming similarity between the sentence patterns and KB relations that co-occur with the same entity pairs, while our method can also reduce the noise from two sentence patterns sharing the same entity pair. Besides, our method is complementary to"
2021.eacl-main.77,N19-1083,1,0.93011,"he embeddings of two sentence patterns similar if they co-occur with the same entity pair. Baldini Soares et al. (2019) rely on a similar assumption and achieve state-of-theart results on supervised RE tasks by replacing the LSTM with a large pre-trained language model. The variants of universal schema have many different applications, including multilingual RE (Verga et al., 2016), knowledge base construction (Toutanova et al., 2015; Verga et al., 2017), question answering (Das et al., 2017), documentlevel RE (Verga et al., 2018), N-ary RE (Akimoto et al., 2019), open information extraction (Zhang et al., 2019), and unsupervised relation discovery (Percha and Altman, 2018). Nevertheless, one sentence pattern could contain multiple facets, and each facet could imply a different relation. In Figure 1, “$ARG1, the partner of fellow $ARG2” could imply the entity pair has the spouse relation, the co-worker relation, or both. “$ARG1 moved in with $ARG2” could imply the spouse relation, the parent relation, ..., etc. If we squeeze the facets of a sentence pattern into a single embedding, the embedding is more likely to be affected by the irrelevant facets from other patterns co-occurred with the same entit"
2021.emnlp-demo.24,E17-1068,0,0.0634813,"Missing"
2021.emnlp-demo.24,2021.naacl-main.104,1,0.7031,"es sampled from the rest of the transitive closure (not seen during training) and a fixed set of random negatives with the same positive to negative ratio as training. As seen in Table 5, we are able to replicate the result from Patel et al. (2020). sentences, premise and hypothesis, the model is required to pick whether the premise entails the hypothesis, contradicts the hypothesis, or whether neither relationship holds. The task of NLI is setup as multi-class classification, and in the two-class version, the model is only required to decide whether the premise entails the hypothesis or not (Mishra et al., 2021). Although NLI deals with a pair of sentences at a time, in the space of all possible sentences the transitive relation of entailment establishes a partial order. If the sentences are encoded as boxes then we can train box containment to capture the transitive entailment relation. To demonstrate this, we choose the MNLI corpus (Williams et al., 2018) from the GLUE benchmark (Wang et al., 2018). Since the MNLI dataset presents the NLI task as a three-class problem, we collapse contradiction and neutral labels into a single label called not-entails to obtain a two-class problem with class labels"
2021.emnlp-demo.24,2021.acl-long.160,1,0.845166,"where the corners of boxes are modeled using Gumbel random variables. These latter models lacked valid probabilistic semantics, however, a fact rectified in Boratko et al. (2021). While each methodological improvement demonstrated better performance on various modeling tasks, the implementations grew more complex, bringing with it various challenges related to performance and numerical stability. Various applications of probabilistic box embeddings (eg. modeling joint-hierarchies (Patel et al., 2020), uncertain knowledge graph representation (Chen et al., 2021), or fine-grained entity typing (Onoe et al., 2021)) have relied on bespoke implementations, adding unnecessary difficulty and differences in implementation when applying box embeddings to new tasks. To mitigate this issue and make applying and extending box embeddings easier, we saw the need to introduce a reusable, unified, stable library that provides the basic functionalities needed in studying box embeddings. To this end, we introduce “Box Embeddings”, a fully open-source Python library hosted on PyPI. The contributions of this work are as follows: • Provide a modular and reusable library that aids the researchers in studying probabilisti"
2021.emnlp-demo.24,D19-1631,0,0.0484524,"Missing"
2021.emnlp-demo.24,P18-1025,1,0.910474,"negative exponential measure, allowing the calcufor training and inference (Hinton, 2007; LeCun lation of arbitrary marginal, joint, and conditional et al., 2015). Vectors are the most common choice probabilities. Cone representations are not particuof representation, as linear transformations are well understood and element-wise non-linearities of- larly flexible, however - for instance, the resulting fer increased representational capacity while be- probability model cannot represent negative correlation - motivating the development of probabilistic ∗ * Equal Contributions. box embeddings (Vilnis et al., 2018), where entities 1 The source code and the usage and API documentation for are represented by n-dimensional rectangles (i.e. the library is available at https://github.com/iesl/ box-embeddings and https://www.iesl.cs. Cartesian products of intervals) in Euclidean space. umass.edu/box-embeddings/main/index.html, Probabilistic box embeddings have undergone respectively. A quick video tutorial is available at https://youtu.be/MEPDw8sIwUY. several rounds of methodological improvements. 203 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
2021.emnlp-demo.24,W18-5446,0,0.0570682,"Missing"
2021.emnlp-demo.24,N18-1101,0,0.0119898,"othesis, contradicts the hypothesis, or whether neither relationship holds. The task of NLI is setup as multi-class classification, and in the two-class version, the model is only required to decide whether the premise entails the hypothesis or not (Mishra et al., 2021). Although NLI deals with a pair of sentences at a time, in the space of all possible sentences the transitive relation of entailment establishes a partial order. If the sentences are encoded as boxes then we can train box containment to capture the transitive entailment relation. To demonstrate this, we choose the MNLI corpus (Williams et al., 2018) from the GLUE benchmark (Wang et al., 2018). Since the MNLI dataset presents the NLI task as a three-class problem, we collapse contradiction and neutral labels into a single label called not-entails to obtain a two-class problem with class labels entails and not-entails. In order to obtain box representation for the premise and hypothesis sentences, we use a neural network E to first get vector representations vp and vh for the premise and the hypothesis, respectively. Both these vectors are then interpreted as the parameters θp := vp and θh := vh of a box tensor. Finally, the probability of"
2021.emnlp-main.101,D17-1205,0,0.0175112,"allows us to minimize inter-annotator inconbroad characterization of the field, and a BATTERY sistencies between annotators, and means that if a subset, in which 257 papers were sampled using particular aspect of the annotation requires specialkeywords such as “Li battery” or “Li10GeP2S12”. ized knowledge, an appropriate expert can focus By providing this split, one can measure both the on it, in a manner similar to allocating different ability of models to learn a single focused domain, tasks to different expertise levels during crowdand whether models generalize across the broader sourcing (Wang et al., 2017). This approach of didomain. viding the mention annotation tasks also increases Given the papers, synthesis procedures were the speed and efficiency of annotation, by reducing extracted from each publication using a parathe number of different phenomena each annotator † BRAT: https://brat.nlplab.org/ must pay attention to. Because minimizing inter1340 The present dataset was annotated by 3 domain experts using the BRAT annotation tool† , using non-nested mentions. In building the dataset, papers for annotation were picked to contain a mix of randomly selected papers and those from a more focus"
2021.emnlp-main.101,2020.lrec-1.834,0,0.237194,"a material or material quantities (Property-Unit, Amount-Unit) and numbers to be linked to these units (Number). Appendix C.3 expands on these entity types. 2.2 Dataset Statistics Table 1 outlines the resultant size of our corpus, MS-M ENTIONS. We also list the corpus statistics These distinctions allow us to understand the role for the Materials Science Procedural Text (MSPT) of these materials within the synthesis procedure. corpus described in Mysore et al. (2019), the SOFCWhile some of these distinctions represent informa- Exp Corpus of Friedrich et al. (2020), the SC1339 CoMIcs corpus of Yamaguchi et al. (2020) and the Wet Lab Protocols (WLP) corpus of Kulkarni et al. (2018), which has also been augmented for a recent WNUT shared task (Tabassum et al., 2020). Of these, MSPT is the most comparable annotation in structures annotated and domain of text. WLP bears resemblance in most of its annotated structures, but is in a different domain, while both SOFC-Exp and SC-CoMIcs are in the same Material Science domain, but do not focus upon procedural text. The current corpus is therefore the largest corpus we are aware of for materials science procedural text. Furthermore, our corpus also spans a range of"
2021.emnlp-main.395,2020.emnlp-main.389,0,0.0853548,"t supervision ⇤ Equal contribution. through span constraints. These span constraints in4818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4818–4831 c November 7–11, 2021. 2021 Association for Computational Linguistics dicate that a certain sequence of words in a sentence form a constituent span in its parse tree, and we obtain these partial ground-truths without explicit user annotation. We take inspiration from previous work incorporating distant supervision into parsing (Haghighi and Klein, 2006; Finkel and Manning, 2009; Ganchev et al., 2010; Cao et al., 2020), and design a novel fully neural system that improves a competitive neural unsupervised parser (DIORA; Drozdov et al. 2019) using span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We"
2021.emnlp-main.395,N19-1116,1,0.927948,"rence on Empirical Methods in Natural Language Processing, pages 4818–4831 c November 7–11, 2021. 2021 Association for Computational Linguistics dicate that a certain sequence of words in a sentence form a constituent span in its parse tree, and we obtain these partial ground-truths without explicit user annotation. We take inspiration from previous work incorporating distant supervision into parsing (Haghighi and Klein, 2006; Finkel and Manning, 2009; Ganchev et al., 2010; Cao et al., 2020), and design a novel fully neural system that improves a competitive neural unsupervised parser (DIORA; Drozdov et al. 2019) using span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsi"
2021.emnlp-main.395,D13-1057,0,0.0285768,"traint injection methods were usually used as an added loss to the supervised loss function. In this work, we show that the distant supervision through constraint injection is beneficial for unsupervised setting as well. Structural SVM with Latent Variables The PS-SVM loss we introduce in this work can be loosely thought of as an application-specific instantiation of Structural SVM with Latent Variables (Yu and Joachims, 2009). Various works have extended Structural SVM with Latent Variables to incorporate constraints for tasks such as sequence labeling (Yu, 2012) and co-reference resolution (Chang et al., 2013), although none we have seen focus on unsupervised constituency parsing. Perhaps a more clear distinction is that Yu and Joachims (2009) focuses on latent variables within supervised tasks, and PS-SVM is meant to improve convergence of an unsupervised learning algorithm (i.e., DIORA). Additional Related Work In Appendix A.3 we list additional work in unsupervised parsing not already mentioned. 7 Conclusion In this work, we present a method for enhancing DIORA with distant supervision from span constraints. We call this approach Partially Structured SVM (PS-SVM). We find that span constraints b"
2021.emnlp-main.395,W01-0713,0,0.419018,"Missing"
2021.emnlp-main.395,N04-4028,1,0.276541,"us properties of the above sentence make it difficult to parse. For instance, the sentence construction lacks syntactic cues and there is no verb in the sentence. There is also substantial ambiguity with respect to hyphenation, and the second hyphen is acting as a colon. These properties make it difficult to capture the spans (skeletal - muscle) or the second (HIF - 1↵) despite being constraints. 6 Related Work Learning from Partially Labeled Corpora Pereira and Schabes (1992) modify the insideoutside algorithm to respect span constraints. Similar methods have been explored for training CRFs (Culotta and McCallum, 2004; Bellare and McCallum, 2007). Rather than modify the weight assignment in DIORA, which is inspired by the inside-outside algorithm, we supervise the tree predicted from the inside-pass. Concurrent work to ours in distant supervision trains RoBERTa for constituency parsing using answer spans from question-answering datasets and wikipedia hyperlinks (Shi et al., 2021). Although effective, their approach depends entirely on the set of constraints. In contrast, PS-SVM enhances DIORA, which is a model that outputs a parse tree without any supervision. The span constraints in this work are derived"
2021.emnlp-main.395,N19-1114,0,0.0193451,"xt, since it is a shift in domain from the DIORA pre-training, we first train for 20 epochs using a concatenation of MedMentions and CRAFT data with only the reconstruction loss5 (called DIORAf t for “fine-tune”). Then, we train for 40 epochs like previously mentioned, using performance on a subset of 3k random sentences from the CRAFT training data for early stopping. Hyperparameters are in Appendix A.2. 5 The training jointly with MedMentions and CRAFT is a special case of “intermediate fine-tuning” (Phang et al., 2018). F1 General Purpose Ordered Neuron† (Shen et al., 2019) Compound PCFG† (Kim et al., 2019a) DIORA‡ (Drozdov et al., 2019) S-DIORA† (Drozdov et al., 2020) 48.1 ±1.0 55.2 ±2.5 56.8 57.6 ±3.2 Constituency Tests RoBERTa† (Cao et al., 2020) 62.8 ±1.6 DIORA Span Constraints +CCKY +PS-SVM NCBL +PS-SVM M INDIFF +PS-SVM R ESCALE +PS-SVM S TRUCTURE R AMP 57.5 60.4 ±0.1 59.0 ±0.8 61.2 ±0.6 59.9 ±1.0 Table 2: Parsing F1 on PTB. The average F1 across random seeds is measured on the test set, and the standard deviation is shown as subscript when applicable. †: Indicates that standard deviation is the approximate lower bound derived from the mean, max, and number of random seeds. ‡: Indicates no"
2021.emnlp-main.395,P18-1249,0,0.144117,"defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alternative to ground truth a"
2021.emnlp-main.395,P04-1061,0,0.226258,"Missing"
2021.emnlp-main.395,P92-1017,0,0.729626,"ructure. Here is a typical sentence and ground truth parse for that case: ((HIF - 1↵) KO) - ((skeletal - muscle) (HIF - 1↵) knockout mouse) Various properties of the above sentence make it difficult to parse. For instance, the sentence construction lacks syntactic cues and there is no verb in the sentence. There is also substantial ambiguity with respect to hyphenation, and the second hyphen is acting as a colon. These properties make it difficult to capture the spans (skeletal - muscle) or the second (HIF - 1↵) despite being constraints. 6 Related Work Learning from Partially Labeled Corpora Pereira and Schabes (1992) modify the insideoutside algorithm to respect span constraints. Similar methods have been explored for training CRFs (Culotta and McCallum, 2004; Bellare and McCallum, 2007). Rather than modify the weight assignment in DIORA, which is inspired by the inside-outside algorithm, we supervise the tree predicted from the inside-pass. Concurrent work to ours in distant supervision trains RoBERTa for constituency parsing using answer spans from question-answering datasets and wikipedia hyperlinks (Shi et al., 2021). Although effective, their approach depends entirely on the set of constraints. In co"
2021.emnlp-main.395,P11-1108,0,0.0782594,"Missing"
2021.emnlp-main.395,W12-4501,0,0.265538,"as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alternative to ground truth annotation and gives 2 F1 improvement over DIORA. Importantly, training DIORA with PS-SVM is more effective than simply injecting available constraints into parse tree decoding at test time. We also conduct experiments with different types of span constraints. Our detailed analysis shows that entity-based constraints are similarly useful"
2021.emnlp-main.395,W09-1119,0,0.0117968,"is not available, but partial span constraints are readily available. PMI Constraints We use the phrases defined in the vocab from Mikolov et al. (2013) as a lexicon, treating exact matches found in Ontonotes as constraints. The phrases are learned through word statistics by applying pointwise mutual information (PMI) to find relevant bi-grams, then replacing these bi-grams with a new special token representing the phrase — applied multiple times this technique is used to find arbitrarily long phrases. Gazetteer We use a list of 1.5 million entity names automatically extracted from Wikipedia (Ratinov and Roth, 2009), which has been effective for supervised entity-centric tasks with both log-linear and neural models (Liu et al., 2019a). We derive constraints by finding exact matches in the Ontonotes corpus that are in the gazetteer. A lexicon containing entity names is often called a gazetteer. 4.2 Training Details In all cases, we initialize our model’s parameters from pre-trained DIORA (Drozdov et al., 2019). We then continue training using a combination of the reconstruction and PS-SVM loss. Given sentence x and constraints z, the instance loss is: J(x, z) = Jrec (x) + JP S (x, z) For the newswire doma"
2021.emnlp-main.395,2020.acl-main.722,0,0.0197238,"ts. We hope our findings will help “bridge the gap” between supervised and unsupervised parsing. Broader Impact be acquired at reduced cost or even automatically extracted. The gazetteer used in our experiments is automatically extracted from Wikipedia, and our experiments are only for English, which is the language with by far the most Wikipedia entries. Although, similarly sized gazetteers may be difficult to attain in other languages, Mikheev et al. (1999) point out larger gazetteers do not necessarily boost performance, and gazetteers have already proven effective in low-resource domains (Rijhwani et al., 2020). In any case, we use gazetteers in the most naive way by finding exact text matches. When extending our approach to other languages, an entity recognition model may be a suitable replacement for the gazetteer. Acknowledgements We are grateful to our colleagues at UMass NLP and the anonymous reviewers for feedback on drafts of this work. This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Chan Zuckerberg Initiative, in part by the IBM Research AI through the AI Horizons Network, and in part by the National Science Foundation (NSF) grant numbers D"
2021.emnlp-main.395,P05-1044,0,0.324887,"Missing"
2021.emnlp-main.395,P09-1009,0,0.0732978,"Missing"
2021.emnlp-main.395,P17-1076,0,0.0235161,"ng span constraints defined on a portion of the training data. In the large majority of cases, the number of spans constraints per sentence is much lower than that specified by a full parse tree. We find that entity spans are effective as constraints, and can readily be acquired from existing data or derived from a gazetteer. In our experiments, we use DIORA as our baseline and improve upon it by injecting these span constraints as a source of distant supervision. We introduce a new method for training DIORA that leverages the structured SVM loss often used in supervised constituency parsing (Stern et al., 2017; Kitaev and Klein, 2018), but only depends on partial structure. We refer to this method as partially structured SVM (PS-SVM). Our experiments indicate PS-SVM improves upon unsupervised parsing performance as the model adjusts its prediction to incorporate span constraints (depicted in Figure 1). Using ground-truth entities from Ontonotes (Pradhan et al., 2012) as constraints, we achieve more than 5 F1 improvement over DIORA when parsing English WSJ Penn Treebank (Marcus et al., 1993). Using automatically extracted span constraints from an entity-based lexicon (i.e. gazetteer) is an easy alte"
2021.emnlp-main.395,D18-1412,0,0.0219915,"an 100 span recall in Table 4. This approach to model training is often called “distant supervision” (Mintz et al., 2009; Shi et al., 2021). In contrast, “partial supervision” implies gold partial labels are available, which we explore as synthetic data (§5.4), but in general do not make this assumption. Joint Supervision An implicit way to incorporate constraints is through multi-task learning (MTL; Caruana, 1997). Even when relations between the tasks are not modeled explicitly, MTL has shown promise throughout a range of text processing tasks with neural models (Collobert and Weston, 2008; Swayamdipta et al., 2018; Kuncoro et al., 2020). Preliminary experiments with joint NER did not improving parsing results. This is in-line with DIORA’s relative weakness in representing fine-grained entity types. Modifications of DIORA to improve its semantic representation may prove to make joint NER more viable. 5.5.3 Parsing of PTB vs. CRAFT As mentioned in §5.5.1, there is considerable dif- Constraint Injection Methods There exists a ference in the text between PTB and CRAFT. It rich literature in constraint injection (Ganchev follows that there would be a difference in diffi- et al., 2010; Chang et al., 2012) ."
2021.emnlp-main.395,N12-1087,0,0.0252074,"nt Injection Methods There exists a ference in the text between PTB and CRAFT. It rich literature in constraint injection (Ganchev follows that there would be a difference in diffi- et al., 2010; Chang et al., 2012) . Both methods culty when parsing these two types of data. After are based on Expectation Maximization (EM) algorunning the parser from Kitaev and Klein (2018) on rithm (Dempster et al., 1977) where the constraint each dataset, it appears CRAFT is more difficult to is injected in the E-step of calculating the posterior parse than PTB. For CRAFT, the unlabeled parsing distribution (Samdani et al., 2012). Another line F1 is 81.3 and the span recall for entities is 37.6. of work focuses injecting constraint in the M-step For PTB, the unlabeled parsing F1 is 95. (Lee et al., 2019; Mehta et al., 2018) by reflecting 4825 the degree of constraint satisfaction of prediction as the weight of the gradient. Our approach is similar to Chang et al. (2012) as we select the highest scoring output that satisfies constraints and learn from it. PS-SVMR ESCALE is based on Lee et al. (2019). The aforementioned constraint injection methods were usually used as an added loss to the supervised loss function. In t"
2021.emnlp-main.395,Q18-1019,1,0.8471,"Missing"
2021.emnlp-main.395,2020.emnlp-main.196,0,0.0235256,"ically more effective. a large endeavor. For example, the 20k sentences of biomedical treebanking in the CRAFT corpus 1 Introduction required 80 annotator hours per week for 2.5 years, Syntactic parse trees are helpful for various down- include 6 months for annotator training (Verspoor stream tasks such as speech recognition (Moore et al., 2011). However, although many domains et al., 1995), machine translation (Akoury et al., and many languages lack full treebanks, they do 2019), paraphrase generation (Iyyer et al., 2018), often have access to other annotated resources such semantic parsing (Xu et al., 2020), and informa- as NER, whose spans might provide some partial tion extraction (Naradowsky, 2014). While super- syntactic supervision. We explore whether unsuvised syntactic parsers are state-of-the-art models pervised parsing methods can be enhanced with for creating these parse trees, their performance distant supervision from such spans to enable the does not transfer well across domains. Moreover, types of benefits afforded by supervised syntactic new syntactic annotations are prohibitively expen- parsers without the need for expensive syntactic sive; the original Penn Treebank required eig"
2021.emnlp-main.395,P19-1180,0,0.0240996,"Missing"
2021.emnlp-main.395,2021.naacl-main.234,0,0.0588775,"Missing"
2021.emnlp-main.469,2020.coling-main.448,1,0.436755,"ic framework (Vinyals et al., 2016), where in each episode a minibatch of tasks are sampled along with their support and validation sets, and the model parameters are optimized as above. 2.2 Task Distribution for Meta-Learning Meta-learning assumes access to a distribution P(T ) over tasks. The goal is to utilize tasks Ti ∼ P(T ) sampled from this distribution to train a learning procedure that generalizes to unseen tasks T 0 ∼ P(T ) from the distribution. Supervised meta-learning often utilizes a fixed task dataset to create P(T ) by sub-sampling from all class labels (Vinyals et al., 2016). Bansal et al. (2020b) sought to provide an unsupervised approach that proposes tasks from unlabelled data. The resulting Subset Masked Language Modeling Tasks (SMLMT) approach proposes self-supervised tasks to enable meta-learning and improves few-shot learning across a diverse set of classification tasks. Sampling an N -way task from SMLMT requires first sampling a size-N subset of the vocabulary, which are subsequently mapped to consecutive integer ids and serve as labels for the task. Then to sample examples for each label, sentences containing that word are sampled and the occurrences of the word are masked"
2021.emnlp-main.469,2020.emnlp-main.38,1,0.376746,"ic framework (Vinyals et al., 2016), where in each episode a minibatch of tasks are sampled along with their support and validation sets, and the model parameters are optimized as above. 2.2 Task Distribution for Meta-Learning Meta-learning assumes access to a distribution P(T ) over tasks. The goal is to utilize tasks Ti ∼ P(T ) sampled from this distribution to train a learning procedure that generalizes to unseen tasks T 0 ∼ P(T ) from the distribution. Supervised meta-learning often utilizes a fixed task dataset to create P(T ) by sub-sampling from all class labels (Vinyals et al., 2016). Bansal et al. (2020b) sought to provide an unsupervised approach that proposes tasks from unlabelled data. The resulting Subset Masked Language Modeling Tasks (SMLMT) approach proposes self-supervised tasks to enable meta-learning and improves few-shot learning across a diverse set of classification tasks. Sampling an N -way task from SMLMT requires first sampling a size-N subset of the vocabulary, which are subsequently mapped to consecutive integer ids and serve as labels for the task. Then to sample examples for each label, sentences containing that word are sampled and the occurrences of the word are masked"
2021.emnlp-main.469,P07-1056,0,0.31424,"Missing"
2021.emnlp-main.469,D19-1649,0,0.0184069,"d value of λt throughout training. 1-shot N = 5 N = 10 5-shot N = 5 N = 10 Unsupervised Cluster Cluster-ccnet 60.1 61.3 44.1 46.1 78.8 80.4 65.2 67.7 Supervised Proto-Adv (CNN) Proto-Adv (BERT) BERT-Pair Cluster-ccnet 42.2 41.9 67.4 67.7 28.9 27.4 54.9 52.9 58.7 54.7 78.6 84.3 44.4 37.4 66.9 74.1 Table 5: Results on Fewrel 2.0 test set. We then compare their test set performance with previously published results: the BERT-Pair, ProtoAdversarial (CNN), and Proto-Adversarial (BERT) are supervised meta-learning models trained on FewRel training data and using BERT or CNN as the text encoder. See Gao et al. (2019) for details. Interestingly, our unsupervised meta-learned models that do not use any FewRel training data outperform the supervised baselines in the 5-shot setting. Performance is lower than BERT-Pair on 1-shot tasks, potentially because our models have not been trained for 1-shot tasks like BERT-Pair. Finally, fine-tuning our best model on the FewRel training data leads to the best overall performance. 5 Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al"
2021.emnlp-main.469,2020.acl-main.465,0,0.0249796,"Missing"
2021.emnlp-main.469,D18-1398,0,0.028543,"s trained on FewRel training data and using BERT or CNN as the text encoder. See Gao et al. (2019) for details. Interestingly, our unsupervised meta-learned models that do not use any FewRel training data outperform the supervised baselines in the 5-shot setting. Performance is lower than BERT-Pair on 1-shot tasks, potentially because our models have not been trained for 1-shot tasks like BERT-Pair. Finally, fine-tuning our best model on the FewRel training data leads to the best overall performance. 5 Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al., 2019). Unsupervised meta-learning has been explored in computer vision (Hsu et al., 2019; Khodadadeh et al., 2019) and reinforcement learning (Gupta et al., 2018). Hsu et al. (2019) cluster images using pre-trained embeddings to create tasks. Metz et al. (2019) 4.4 Evaluation on FewRel 2.0 benchmark meta-learn an unsupervised update rule in a semiFewRel (Han et al., 2018; Gao et al., 2019) is a supervised framework. Bansal et al. (2020b) decommon benchmark for few-shot learning in NLP, veloped the SMLMT appro"
2021.emnlp-main.469,D18-1498,0,0.0469642,"Missing"
2021.emnlp-main.469,D18-1514,0,0.127133,"g method, tential to enable numerous important applications and perform comparably to supervised meth(Hospedales et al., 2020) such as neural architecods on the FewRel 2.0 benchmark. ture search, continual learning, hyper-parameter optimization, learning in low-resource settings, 1 Introduction etc. Existing work in meta-learning for NLP, howHumans show a remarkable capability to accu- ever, defaults to task distributions that tend to be rately solve a wide range of problems efficiently overly simplistic, e.g. using existing supervised – utilizing a limited amount of computation and datasets (Han et al., 2018; Dou et al., 2019; Bansal experience. Deep learning models, by stark con- et al., 2020a) or unsupervised cloze-style tasks with trast, can be trained to be highly accurate on a nar- uniform selection of words from the vocabulary row task while being highly inefficient in terms of (Bansal et al., 2020b). Given the lack of explothe amount of compute and data required to reach ration on this critical component, we propose to that accuracy. Within natural language processing devise and evaluate various task distributions in the (NLP), recent breakthroughs in unsupervised pre- context of unsupervi"
2021.emnlp-main.469,P18-1031,0,0.0168339,"ng NLP meta- NLP, prior to this work. Jabri et al. (2019) found learning methods. unsupervised curriculum to be beneficial for metareinforcement learning. We refer to Hospedales Before submitting to the competition site for test et al. (2020) for a comprehensive review of metaset results, we first use the validation set to select the best model(s), where we observed that the Clus- learning. ter approaches performs better than the other task Self-supervised learning has emerged as an effiproposals (see validation results in Supplementary). cient approach to representation learning in NLP 5819 (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Multi-task learning of pre-trained models has shown improved results on many tasks (Phang et al., 2018; Liu et al., 2019a), including few-shot setting. Yin et al. (2020) leveraged entailment tasks for few-shot learning. Du et al. (2020) developed self-training methods for semi-supervised few-shot learning. Recently, extremely large language models have been shown to have few-shot capacities (Brown et al., 2020), while Schick and Schütze (2020) demonstrated few-shot capacities for small models in the semi-supe"
2021.emnlp-main.469,E17-2068,0,0.0219487,"ilar contexts, leading to harder to classify sentences as we see in our analysis (Sec 4.2). Moreover, choosing different clusters to sample words across tasks provides a natural diversity over topics in the training tasks. On the other hand, picking words from different clusters (inter-cluster sampling) can still lead to tasks where the sentences are easy to classify due to easily distinguishable contexts. Specifically, clustering of pre-trained word embeddings using k-means has been proven effective in generating topical clusters rivaling topic models (Sia et al., 2020). We use the FastText (Joulin et al., 2017) embeddings as word representations. We choose FastText as it is fast, incorporates subword information, can generate embeddings for out-of-vocabulary words, and has been found to yield topical clusters (Sia et al., 2020). Since cluster sizes can be imbalanced, we pick clusters proportional to the number of words in the cluster. Thus, assuming {C1 , . . . , Cm } to be the m clusters of the word vocabulary, we replace the 5814 T ∼ λt Dt + (1 − λt )S To construct Dt , we consider the following word (i.e. label) representation for clustering, obtained by the average representation under the model"
2021.emnlp-main.469,P19-1441,0,0.108213,"or downstream few-shot performance of models trained on SMLMT (see Sec. 4.3). Since models trained on SMLMT have never seen pairs of sentences as input, it leads to a train-test mismatch for sentence-pair classification tasks. To remedy this, we introduce a simple but effective contrastive learning task over sentence-pairs that bridges this gap. Contrastive learning has been used to learn effective sentence representations (Logeswaran and Lee, 2018). Next sentence prediction, a sentencepair task, was used in the training of BERT (Devlin et al., 2018) which was later found to be not effective (Liu et al., 2019b). BERT considered segments instead of full sentences, however the downstream tasks often require reasoning over complete sentences. Thus, we consider classifying whether two sentences come from the same document as opposed to different documents, as a sentence-pair task to enable cross-sentence reasoning. This simple objective was found to be quite effective in our experiments. Note that during meta-training, this can be treated as an additional task in the task distribution. Since the SMLMT task distribution consists of an exponential number of tasks, we sample the sentence-pair task in an"
2021.emnlp-main.469,2021.ccl-1.108,0,0.0651472,"Missing"
2021.emnlp-main.469,2021.naacl-main.88,0,0.0620866,"Missing"
2021.emnlp-main.469,N18-1202,0,0.0237021,"to this work. Jabri et al. (2019) found learning methods. unsupervised curriculum to be beneficial for metareinforcement learning. We refer to Hospedales Before submitting to the competition site for test et al. (2020) for a comprehensive review of metaset results, we first use the validation set to select the best model(s), where we observed that the Clus- learning. ter approaches performs better than the other task Self-supervised learning has emerged as an effiproposals (see validation results in Supplementary). cient approach to representation learning in NLP 5819 (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Multi-task learning of pre-trained models has shown improved results on many tasks (Phang et al., 2018; Liu et al., 2019a), including few-shot setting. Yin et al. (2020) leveraged entailment tasks for few-shot learning. Du et al. (2020) developed self-training methods for semi-supervised few-shot learning. Recently, extremely large language models have been shown to have few-shot capacities (Brown et al., 2020), while Schick and Schütze (2020) demonstrated few-shot capacities for small models in the semi-supervised setting. Meanw"
2021.emnlp-main.469,W03-0419,0,0.706226,"Missing"
2021.emnlp-main.469,2020.emnlp-main.135,0,0.014803,"topically related and hence occur in similar contexts, leading to harder to classify sentences as we see in our analysis (Sec 4.2). Moreover, choosing different clusters to sample words across tasks provides a natural diversity over topics in the training tasks. On the other hand, picking words from different clusters (inter-cluster sampling) can still lead to tasks where the sentences are easy to classify due to easily distinguishable contexts. Specifically, clustering of pre-trained word embeddings using k-means has been proven effective in generating topical clusters rivaling topic models (Sia et al., 2020). We use the FastText (Joulin et al., 2017) embeddings as word representations. We choose FastText as it is fast, incorporates subword information, can generate embeddings for out-of-vocabulary words, and has been found to yield topical clusters (Sia et al., 2020). Since cluster sizes can be imbalanced, we pick clusters proportional to the number of words in the cluster. Thus, assuming {C1 , . . . , Cm } to be the m clusters of the word vocabulary, we replace the 5814 T ∼ λt Dt + (1 − λt )S To construct Dt , we consider the following word (i.e. label) representation for clustering, obtained by"
2021.emnlp-main.469,W18-5446,0,0.0787529,"Missing"
2021.emnlp-main.469,2020.lrec-1.494,0,0.0125422,"sk from the quency: SMLMT with a sampling proportional same cluster; Sentence Cluster (S-C, c5): this is to log-frequency (see 3.1); (3) Cluster: SMLMT the sentence clustering approach to task proposal where labels are picked from same word cluster presented in sec 3.3. For evaluation, we consider (see 3.1); (4) Dynamic: curriculum-based task sam- 4-way tasks sampled from the above methods and pling with Cluster as the static distribution (see evaluate average accuracy over 5000 tasks. We 3.2); (5) Cluster-ccnet: same as Cluster but using consider a BERT model (r1) which is not trained ccnet (Wenzek et al., 2020) as the corpora, which on the SMLMT distribution but is trained on the consists of web crawled data; (6) SentCluster: alter- related masked language modeling (MLM) task. native to SMLMT which proposes tasks from sub- To enable evaluation of this model, we use it as sets of sentence clustering (see 3.3); (7) SentPair: a prototypical network model (Snell et al., 2017). the sentence-pair tasks (see 3.4). All methods, ex- We also consider meta-trained models trained on cept SentCluster and Cluster-ccnet, have Wikipedia the SMLMT distribution with uniform sampling as the text corpora. The sentence"
2021.emnlp-main.469,2020.emnlp-main.660,0,0.0613467,"Missing"
2021.emnlp-main.469,N18-1109,0,0.0156804,"N as the text encoder. See Gao et al. (2019) for details. Interestingly, our unsupervised meta-learned models that do not use any FewRel training data outperform the supervised baselines in the 5-shot setting. Performance is lower than BERT-Pair on 1-shot tasks, potentially because our models have not been trained for 1-shot tasks like BERT-Pair. Finally, fine-tuning our best model on the FewRel training data leads to the best overall performance. 5 Related Work Meta-learning applications in NLP have yielded improvements on specific tasks (Gu et al., 2018; Chen et al., 2018; Guo et al., 2018; Yu et al., 2018; Han et al., 2018; Dou et al., 2019). Unsupervised meta-learning has been explored in computer vision (Hsu et al., 2019; Khodadadeh et al., 2019) and reinforcement learning (Gupta et al., 2018). Hsu et al. (2019) cluster images using pre-trained embeddings to create tasks. Metz et al. (2019) 4.4 Evaluation on FewRel 2.0 benchmark meta-learn an unsupervised update rule in a semiFewRel (Han et al., 2018; Gao et al., 2019) is a supervised framework. Bansal et al. (2020b) decommon benchmark for few-shot learning in NLP, veloped the SMLMT approach to unsupervised which consists of many few-shot re"
2021.emnlp-main.755,2020.findings-emnlp.427,1,0.536675,". number of columns in a table is 5.8 and in Spider dataset (Yu et al., 2018), the average number of columns is 28.1. On the other hand, our model has to consider all possible Freebase relations (in thousands). Previous work perform schema-aware encoding which is not possible in our case because of the large number of relations. The retrieve step of C BR - KBQA can be seen as a pruning step which narrows the number of candidate relations by retrieving relevant questions and their logical forms. Case-based Reasoning for KB completion: Recently, a CBR based KB reasoning approach was proposed by Das et al. (2020a,b). They retrieve similar entities and then find KB reasoning paths from them. However, their approach does not handle complex natural language queries and only operate on structured triple queries. Additionally, the logical forms handled by our model have much more expressive power than knowledge base paths. Program Synthesis and Repair: Repairing / revising generated programs has been studied in the field of program synthesis. For example, prior work repairs a program based on syntax of the underlying language (Le et al., 2017), by generating sketches (Hua et al., 2018). More recently, Gup"
2021.emnlp-main.755,D13-1160,0,0.0281857,"ble for questions which needs copying relations from multiple nearest neighbors. Please refer to (§E) for further related work. 5 Limitations and Future Work To the best of our knowledge, we are the first to propose a neuralized CBR approach for KBQA. We showed that our model is effective in handling complex questions over KBs, but our work also has several limitations. First, our model relies on the availability of supervised logical forms such as S PARQL queries, which can be expensive to annotate at scale. In the future, we plan to explore ways to directly learn from question-answer pairs (Berant et al., 2013; Liang et al., 2016). Even though, C BR - KBQA is modular and has several advantages, the retrieve and reuse components of our model are trained separately. In future, we plan to explore avenues for end to end learning for CBR. 9602 Acknowledgments We thank Prof. Yu Su and Yu Gu (Ohio State University) for their help in setting up the Freebase server, Andrew Drozdov, Kalpesh Krishna, Subendhu Rongali and other members of the UMass IESL and NLP groups for helpful discussion and feedback. RD and DT are funded in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base"
2021.emnlp-main.755,P18-1015,0,0.0357632,"Missing"
2021.emnlp-main.755,2020.emnlp-main.550,0,0.0389815,"Missing"
2021.emnlp-main.755,2021.acl-short.47,0,0.0147704,"16; Balog et al., 2016; Zhong et al., 2017a), SQL queries over relational databases (Zhong et al., 2017b), program-structured neural network layouts (Andreas et al., 2016), or even proofs for mathematical theorems (Polu and Sutskever, 2020). Our work differs in our use of the programs of multiple retrieved similar queries to generate the target program. K-NN approach in other NLP applications: Khandelwal et al. (2020) demonstrate improvements in language modeling by utilizing explicit examples from training data. There has been work in machine translation (Zhang et al., 2018; Gu et al., 2018; Khandelwal et al., 2021) that uses nearest neighbor translation pair to guide the decoding process. Recently, Hossain et al. (2020) proposed a retrieve-edit-rerank approach for text generation in which each retrieved candidate from the training set is edited independently and then re-ranked. In contrast, C BR - KBQA generates the program jointly from all the retrieved cases and is more suitable for questions which needs copying relations from multiple nearest neighbors. Please refer to (§E) for further related work. 5 Limitations and Future Work To the best of our knowledge, we are the first to propose a neuralized C"
2021.emnlp-main.755,2020.acl-main.703,0,0.08955,"odule. The retrieval module computes dense representation of the given query and uses it to retrieve other similar query representation from a training set. 2.2 Reuse Inspired by the recent advances in neural dense passage retrieval (Das et al., 2019; Karpukhin et al., The reuse step generates an intermediate logical form from the k cases that are fed to it as input 2020), we use a ROBERTA-base encoder to encode from the retriever module. Pre-trained encodereach question independently. Also, we want to retrieve questions that have high relational simi- decoder transformer models such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2020) have enlarity instead of questions which share the same joyed dramatic success on semantic parsing (Lin entities (e.g. we prefer to score the query pair 1 (Who is Justin Bieber’s brother?, Who is Rihanna’s https://cloud.google.com/ brother?), higher than (Who is Justin Bieber’s natural-language 9596 NL: What do jamaican people speak? SPARQL: select distinct ?x where { m.03_r3 location.country.languages_spoken ?x } Graph-query: jamaica location.country. languages_spoken type:lang Figure 2: An example of a SPARQL logical form for a simple query and its equivalent"
2021.emnlp-main.755,2020.emnlp-main.522,0,0.0321778,"trained BART-base weights. We use k=20 cases and decode with a beam size of 5. Initial learning rate is set to 5 × 10−5 and is decayed linearly through training. Further details for the EMNLP reproducibility checklist is given in §A.2. 3.1 Entity Linking The first step required to generate an executable LF for a NL query is to identify and link the entities present in the query. For our experiments, we use a combination of an off-the-shelf entity linker and a large mapping of mentions to surface forms. For the off-the-shelf linker, we use a recently proposed high precision entity linker E LQ (Li et al., 2020). To further improve recall of our system, we first identify mention spans of entities in the question by tagging it with a NER2 system. Next, we link entities not linked by E LQ by exact matching with surface form annotated in FACC1 project (Gabrilovich et al., 2013). Our entity linking results are shown in Table 2. 3.2 KBQA Results This section reports the performance of C BR - KBQA on various benchmarks. We report the strict exact match accuracy where we compare the list of predicted answers by executing the generated S PARQL program to the list of gold answers3 . A question is answered cor"
2021.emnlp-main.755,2020.acl-main.677,0,0.0271099,"ence is able to generalize even if the query paraphrase is not present in the train set. Retrieve and edit: C BR - KBQA shares similarities with the RETRIEVE - AND - EDIT framework (Hashimoto et al., 2018) which utilizes retrieved nearest neighbor for structured prediction. However, unlike our method they only retrieve a single nearest neighbor and will unlikely be able to generate programs for questions requiring relations from multiple nearest neighbors. Generalizing to unseen database schemas: There has been work in program synthesis that generates SQL programs for unseen database schemas (Wang et al., 2020; Lin et al., 2020). However, these work operate on web or Wikipedia tables with small schemas. For example, in WikiTableQuestions (Pasupat and Liang, 2015) the average. number of columns in a table is 5.8 and in Spider dataset (Yu et al., 2018), the average number of columns is 28.1. On the other hand, our model has to consider all possible Freebase relations (in thousands). Previous work perform schema-aware encoding which is not possible in our case because of the large number of relations. The retrieve step of C BR - KBQA can be seen as a pruning step which narrows the number of candidate"
2021.emnlp-main.755,N18-1120,0,0.0914934,"semantic parsing (Dong and Lapata, 2016; Balog et al., 2016; Zhong et al., 2017a), SQL queries over relational databases (Zhong et al., 2017b), program-structured neural network layouts (Andreas et al., 2016), or even proofs for mathematical theorems (Polu and Sutskever, 2020). Our work differs in our use of the programs of multiple retrieved similar queries to generate the target program. K-NN approach in other NLP applications: Khandelwal et al. (2020) demonstrate improvements in language modeling by utilizing explicit examples from training data. There has been work in machine translation (Zhang et al., 2018; Gu et al., 2018; Khandelwal et al., 2021) that uses nearest neighbor translation pair to guide the decoding process. Recently, Hossain et al. (2020) proposed a retrieve-edit-rerank approach for text generation in which each retrieved candidate from the training set is edited independently and then re-ranked. In contrast, C BR - KBQA generates the program jointly from all the retrieved cases and is more suitable for questions which needs copying relations from multiple nearest neighbors. Please refer to (§E) for further related work. 5 Limitations and Future Work To the best of our knowledge,"
2021.findings-acl.343,W12-4501,0,0.536909,"r a typical user on large document collections in domains such as blogs, stories, books, etc. Moreover, a reduction in energy use of these models can be of benefit to cloud service providers’ costs and there can also be environmental benefits (Strubell et al., 2019; Schwartz et al., 2020). There are two main computational bottlenecks in using end-to-end coreference models on long documents: (i) span and span-pair representations for all spans in the document are simultaneously considered, and (ii) the coreference decision for We evaluate our approach on three coreference datasets: CoNLL-2012 (Pradhan et al., 2012), Litbank (Bamman et al., 2019), and MedMentions (Mohan and Li, 2019) and observe competitive accuracy to state of the art coreference models based on end-to-end training while achieving both faster training and inference running times. Our approach is also more memory efficient and up to 10x faster than the recently proposed memory-based incremental coreference resolution model on Litbank (Toshniwal et al., 2020b). Finally, we demonstrate the scalability of our approach by running it on a novel of two million tokens in 14 minutes while requiring just 12GB of GPU RAM, while previous work can o"
2021.findings-acl.343,2020.tacl-1.5,0,0.232327,"ention is a token span s = xi , . . . , xj 1 . We use xi to refer to the contextualized embedding of the token i (see Section 4.1 for more details on the encoder). The model comprises of two parts which are jointly trained: (a) a mention-proposer, and (b) an antecedent-predictor. The mention-proposer model evaluates all spans S in the dataset and proposes a small set of potential mentions M ⊂ S. The antecedent prediction model evaluates the mentions suggested by the mention proposer and produces coreference clusters (chains) C ⊂ P(M), where P(·) is the powerset. Recent work (Lee et al., 2018; Joshi et al., 2020; Xu and Choi, 2020, inter alia) has built upon the first neural, end-to-end coreference model (Lee et al., 2017). Each of these models introduce two scoring functions sm (s) and sa (m1 , m2 ). sm (s) represents the scores that a span s is a mention, and sa (m1 , m2 ) is the score for mention m1 being an antecedent of mention m2 . These scoring functions are used to define the joint mention proposal and antecedent prediction model for coreference. Mention proposer: The previous works use a neural network for sm : S → R. The architecture takes in a mention span and outputs a score. For each men"
2021.findings-acl.343,P19-1355,1,0.88821,"Missing"
2021.findings-acl.343,D19-1588,0,0.0585244,"of mention spans in a document and addressing bottleneck (i). We propose token level scoring functions for the bilinear inference model originally proposed by Lee et al. (2018). To address bottleneck (ii), we use token windows, token-level k-nearest neighbor relationships along with low-rank matrix approximations of the token similarity matrix thereby improving time/memory efficiency. We also propose an approach to drop token representations from memory to reduce memory requirements while still maintaining the accuracy. Introduction Recent advances in coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020; Wu et al., 2020) have been largely based on the end-to-end model proposed by Lee et al. (2017). However, these models are costly both in terms of training and inference time, as well as memory requirements, especially for long documents. The large computational cost makes the models infeasible to run for a typical user on large document collections in domains such as blogs, stories, books, etc. Moreover, a reduction in energy use of these models can be of benefit to cloud service providers’ costs and there can also be environmental benefits (Strubell et al., 2019; Schwartz et al., 2020"
2021.findings-acl.343,2021.acl-short.3,0,0.3534,"art coreference models based on end-to-end training while achieving both faster training and inference running times. Our approach is also more memory efficient and up to 10x faster than the recently proposed memory-based incremental coreference resolution model on Litbank (Toshniwal et al., 2020b). Finally, we demonstrate the scalability of our approach by running it on a novel of two million tokens in 14 minutes while requiring just 12GB of GPU RAM, while previous work can only scale to documents of just around eleven thousand tokens even with up to 48GB of GPU RAM. Concurrent to our work, Kirstain et al. (2021) also proposes a bilinear token level scoring function for coreference. The focus of our work however 3921 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3921–3931 August 1–6, 2021. ©2021 Association for Computational Linguistics is on long documents and we further introduce a token k-nn graph approximation, a low-rank matrix factorization and an approach to drop non essential candidate antecedents to improve mem/time scalablity. 2 Background: End-to-end Within-Document Coreference End-to-end within-document coreference resolution models jointly discover a se"
2021.findings-acl.343,D17-1018,0,0.114236,"e, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time/memory complexity via token windows and nearest neighbor sparsification methods for more efficient antecedent prediction. We show our approach’s resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy. In this paper, we propose an approximation to the end-to-end coreference model (Lee et al., 2017) that scales to long documents by addressing both these bottlenecks. Our proposed approach operates at the token level instead of the span level, removing the quadratic dependence on the number of mention spans in a document and addressing bottleneck (i). We propose token level scoring functions for the bilinear inference model originally proposed by Lee et al. (2018). To address bottleneck (ii), we use token windows, token-level k-nearest neighbor relationships along with low-rank matrix approximations of the token similarity matrix thereby improving time/memory efficiency. We also propose an"
2021.findings-acl.343,N18-2108,0,0.0443311,"Missing"
2021.findings-acl.343,2020.emnlp-main.536,0,0.0999307,"Missing"
2021.findings-acl.343,2020.acl-main.481,0,0.0972965,"r representations for all spans in the document are simultaneously considered, and (ii) the coreference decision for We evaluate our approach on three coreference datasets: CoNLL-2012 (Pradhan et al., 2012), Litbank (Bamman et al., 2019), and MedMentions (Mohan and Li, 2019) and observe competitive accuracy to state of the art coreference models based on end-to-end training while achieving both faster training and inference running times. Our approach is also more memory efficient and up to 10x faster than the recently proposed memory-based incremental coreference resolution model on Litbank (Toshniwal et al., 2020b). Finally, we demonstrate the scalability of our approach by running it on a novel of two million tokens in 14 minutes while requiring just 12GB of GPU RAM, while previous work can only scale to documents of just around eleven thousand tokens even with up to 48GB of GPU RAM. Concurrent to our work, Kirstain et al. (2021) also proposes a bilinear token level scoring function for coreference. The focus of our work however 3921 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3921–3931 August 1–6, 2021. ©2021 Association for Computational Linguistics is on long"
2021.findings-acl.343,2020.emnlp-main.685,0,0.189576,"r representations for all spans in the document are simultaneously considered, and (ii) the coreference decision for We evaluate our approach on three coreference datasets: CoNLL-2012 (Pradhan et al., 2012), Litbank (Bamman et al., 2019), and MedMentions (Mohan and Li, 2019) and observe competitive accuracy to state of the art coreference models based on end-to-end training while achieving both faster training and inference running times. Our approach is also more memory efficient and up to 10x faster than the recently proposed memory-based incremental coreference resolution model on Litbank (Toshniwal et al., 2020b). Finally, we demonstrate the scalability of our approach by running it on a novel of two million tokens in 14 minutes while requiring just 12GB of GPU RAM, while previous work can only scale to documents of just around eleven thousand tokens even with up to 48GB of GPU RAM. Concurrent to our work, Kirstain et al. (2021) also proposes a bilinear token level scoring function for coreference. The focus of our work however 3921 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3921–3931 August 1–6, 2021. ©2021 Association for Computational Linguistics is on long"
2021.findings-acl.343,2020.acl-main.622,0,0.329637,"ment and addressing bottleneck (i). We propose token level scoring functions for the bilinear inference model originally proposed by Lee et al. (2018). To address bottleneck (ii), we use token windows, token-level k-nearest neighbor relationships along with low-rank matrix approximations of the token similarity matrix thereby improving time/memory efficiency. We also propose an approach to drop token representations from memory to reduce memory requirements while still maintaining the accuracy. Introduction Recent advances in coreference resolution (Lee et al., 2018; Joshi et al., 2019, 2020; Wu et al., 2020) have been largely based on the end-to-end model proposed by Lee et al. (2017). However, these models are costly both in terms of training and inference time, as well as memory requirements, especially for long documents. The large computational cost makes the models infeasible to run for a typical user on large document collections in domains such as blogs, stories, books, etc. Moreover, a reduction in energy use of these models can be of benefit to cloud service providers’ costs and there can also be environmental benefits (Strubell et al., 2019; Schwartz et al., 2020). There are two main co"
2021.findings-acl.343,2020.emnlp-main.695,0,0.0349553,"Missing"
2021.findings-acl.343,2020.emnlp-main.686,0,0.180619,"an s = xi , . . . , xj 1 . We use xi to refer to the contextualized embedding of the token i (see Section 4.1 for more details on the encoder). The model comprises of two parts which are jointly trained: (a) a mention-proposer, and (b) an antecedent-predictor. The mention-proposer model evaluates all spans S in the dataset and proposes a small set of potential mentions M ⊂ S. The antecedent prediction model evaluates the mentions suggested by the mention proposer and produces coreference clusters (chains) C ⊂ P(M), where P(·) is the powerset. Recent work (Lee et al., 2018; Joshi et al., 2020; Xu and Choi, 2020, inter alia) has built upon the first neural, end-to-end coreference model (Lee et al., 2017). Each of these models introduce two scoring functions sm (s) and sa (m1 , m2 ). sm (s) represents the scores that a span s is a mention, and sa (m1 , m2 ) is the score for mention m1 being an antecedent of mention m2 . These scoring functions are used to define the joint mention proposal and antecedent prediction model for coreference. Mention proposer: The previous works use a neural network for sm : S → R. The architecture takes in a mention span and outputs a score. For each mention span s, the mo"
2021.naacl-main.205,D13-1184,0,0.0284567,"like virus, disease, etc, the clustering-based inference should offer improvements. Table 4 shows that our approach is able to correctly link more ambiguous mentions compared to independent model7 . Figure 3 shows two examples from this subset where C LUSTERING - BASED inference is able to make the correct linking decision and I NDEPENDENT is not. 6 Related Work Entity linking is widely studied and often focused on linking mentions to Wikipedia entities (also known as Wikification) (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Entity linking is often done independently for each mention in the document (Ratinov et al., 2011; Raiman and Raiman, 2018) or by modeling dependencies between predictions of entities in a document (Cheng and Roth, 2013; Ganea and Hofmann, 2017; Le and Titov, 2018). In the biomedical domain, Unified Medical Language System (UMLS) is often used as a knowledge-base for entities (Mohan and Li, 2019; Leaman and Lu, 2016). While UMLS is a rich ontology of concepts and relationships between them, this domain is low resource compared to Wikipedia with respect to number of labeled training data for"
2021.naacl-main.205,D07-1074,0,0.21653,"re highly ambiguous and could refer to many different entities, such as common nouns like virus, disease, etc, the clustering-based inference should offer improvements. Table 4 shows that our approach is able to correctly link more ambiguous mentions compared to independent model7 . Figure 3 shows two examples from this subset where C LUSTERING - BASED inference is able to make the correct linking decision and I NDEPENDENT is not. 6 Related Work Entity linking is widely studied and often focused on linking mentions to Wikipedia entities (also known as Wikification) (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Entity linking is often done independently for each mention in the document (Ratinov et al., 2011; Raiman and Raiman, 2018) or by modeling dependencies between predictions of entities in a document (Cheng and Roth, 2013; Ganea and Hofmann, 2017; Le and Titov, 2018). In the biomedical domain, Unified Medical Language System (UMLS) is often used as a knowledge-base for entities (Mohan and Li, 2019; Leaman and Lu, 2016). While UMLS is a rich ontology of concepts and relationships between them, this domain"
2021.naacl-main.205,N19-1423,0,0.0232431,"contain an entity. At the end, every mention is clustered together with exactly one entity, and there could be entities present as singleton clusters such as {e4 } and {e5 }. Note that m3 correctly links to its gold entity e1 as a result of being clustered with mentions m1 , m2 even though it has higher affinity with entity e3 : w(m3 , e3 ) > w(m3 , e1 ). 3.2 Affinity Models We parameterize ψ(·, ·) and φ(·, ·) using two separate deep transformer encoders (Vaswani et al., 2017) for our mention-mention affinity model and mention-entity affinity model — specifically we use the BERT architecture (Devlin et al., 2019) 3 This process is equivalent to single-linkage hierarchical initialized using the weights from BioBERT (Lee agglomerative clustering with the constraint that two entities cannot be in the same cluster. et al., 2019). 2600 3.2.1 Mention-Mention Model The mention-mention model is also a crossencoder, taking as input a pair of mention in context and producing a single scalar affinity for every pair. The input tokens take the form: [CLS] < mi > [SEP] < mj > [SEP] where < mi > := cl [START]mi [END]cr where mi is the mention tokens and cl and cr are the left and right context of the mention in the"
2021.naacl-main.205,Q14-1037,0,0.020616,"hips are also explored in (Le and Titov, 2018) which extends the pairwise CRF model (Ganea and Hofmann, 2017) to use mention-level relationships in addition to entity relationships. These works use attention in a way to build the context representation of the mentions. However, as mentioned by Logeswaran et al. (2019) is not well suited for zero-shot linking. Coreference (both within and across documents) has also been explored by past work (Dutta and Weikum, 2015). This work uses an iterative procedure that performs hard clustering for the sake of aggregating the contexts of entity mentions. Durrett and Klein (2014) presents a CRF-based model for joint NER, within-document coreference, and linking. They show that jointly modeling these three tasks improves performance over the independent baselines. This differs from our work since we do not require coreference decisions to be correct in order to make correct linking decisions. Other work performs joint entity and event coreference (Barhom et al., 2019) without linking. 7 Conclusion In this work, we presented a novel clustering-based inference procedure which enables joint entity linking predictions. We evaluate the effectiveness of our approach on the t"
2021.naacl-main.205,D15-1101,0,0.0193847,"ndidate is within the candidate set and/or entities are limited to those in the dataset rather than those in the knowledge-base. Mention-mention relationships are also explored in (Le and Titov, 2018) which extends the pairwise CRF model (Ganea and Hofmann, 2017) to use mention-level relationships in addition to entity relationships. These works use attention in a way to build the context representation of the mentions. However, as mentioned by Logeswaran et al. (2019) is not well suited for zero-shot linking. Coreference (both within and across documents) has also been explored by past work (Dutta and Weikum, 2015). This work uses an iterative procedure that performs hard clustering for the sake of aggregating the contexts of entity mentions. Durrett and Klein (2014) presents a CRF-based model for joint NER, within-document coreference, and linking. They show that jointly modeling these three tasks improves performance over the independent baselines. This differs from our work since we do not require coreference decisions to be correct in order to make correct linking decisions. Other work performs joint entity and event coreference (Barhom et al., 2019) without linking. 7 Conclusion In this work, we pr"
2021.naacl-main.205,D17-1277,0,0.0359634,"Missing"
2021.naacl-main.205,D11-1072,0,0.0792282,"any different entities, such as common nouns like virus, disease, etc, the clustering-based inference should offer improvements. Table 4 shows that our approach is able to correctly link more ambiguous mentions compared to independent model7 . Figure 3 shows two examples from this subset where C LUSTERING - BASED inference is able to make the correct linking decision and I NDEPENDENT is not. 6 Related Work Entity linking is widely studied and often focused on linking mentions to Wikipedia entities (also known as Wikification) (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Hoffart et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). Entity linking is often done independently for each mention in the document (Ratinov et al., 2011; Raiman and Raiman, 2018) or by modeling dependencies between predictions of entities in a document (Cheng and Roth, 2013; Ganea and Hofmann, 2017; Le and Titov, 2018). In the biomedical domain, Unified Medical Language System (UMLS) is often used as a knowledge-base for entities (Mohan and Li, 2019; Leaman and Lu, 2016). While UMLS is a rich ontology of concepts and relationships between them, this domain is low resource compared to Wikipedia with re"
2021.naacl-main.205,P18-1148,0,0.078404,"ectly use Language System (UMLS) (Bodenreider, 2004). information from one mention (or its linking deciThe mention expression is highly ambiguous sion) to inform the prediction of another mention. and easily confused with the more prevalent en- On the other hand, entity linking methods that do 2598 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2598–2608 June 6–11, 2021. ©2021 Association for Computational Linguistics jointly consider entity linking decisions (Ganea and Hofmann, 2017; Le and Titov, 2018) are designed for cases in which all of the entities in the knowledge-base to have example mentions or metadata at training time (Logeswaran et al., 2019). In this paper, we propose an entity linking model in which entity mentions are either (1) linked directly to an entity in the knowledge-base or (2) join a cluster of other mentions and link as a cluster to an entity in the knowledge-base. Some mentions may be difficult to link directly to their referent ground truth entity, but may have very clear coreference relationships to other mentions. So long as one mention among the group of mention"
2021.naacl-main.205,W03-0428,0,0.287594,"Missing"
2021.naacl-main.205,P19-1335,0,0.0875733,"ion expression is clearly related to the other two highlighted mentions which are much less ambiguous. If considered independently expression is more closely related to an incorrect entity. tity, Gene expression. This linking decision may become easier with sufficient training exam1 Introduction ples (or sufficiently rich structured information in Ambiguity is inherent in the way entities are men- the knowledge-base) . However, in biomedical (Motioned in natural language text. Grounding such am- han and Li, 2019) and other specialized domains biguous mentions to their corresponding entities, (Logeswaran et al., 2019), it is often the case that the task of entity linking, is critical to many applica- the knowledge-base information is largely incomplete. Furthermore, the scarcity of training data tions: automated knowledge base construction and completion (Riedel et al., 2013; Surdeanu et al., leads to a setting in which most entities have not 2012), information retrieval (Meij et al., 2014), been observed at training. smart assistants (Balog and Kenter, 2019), quesState-of-the-art entity linking methods which are tion answering (Dhingra et al., 2020), text mining able to link entities unseen at training ti"
2021.naacl-main.205,P14-5010,0,0.00381639,"eural Inhibition C1150854 tryptophanase activity C0243102 enzyme activity Response Inhibition C0027790 activity activity Entity KB enzyme activity activity. C1150854 C0243102 Figure 3: Example predictions on Ambiguous Mentions. Here we show two example outputs for highly ambigous mention surface forms (inhibition and activity). The independent model incorrectly makes predictions on these surface forms. The clustering-based model is able to have each ambiguous mention link to a less ambiguous mention in the same abstract and thereby make correct predictions. split into sentences using CoreNLP (Manning et al., 2014). (iii) Overlapping mentions are resolved by preferring longer mentions that begin earlier in each sentence, and mentions are truncated at sentence boundaries. This results in 379 mentions to be dropped from the total of 203,282. (iv) Finally, the corpus is saved into the IOB2 tag format. The same preprocessing steps are used for BC5CDR, except overlapping mentions are not dropped. 5.4 Candidate Generation For both datasets, we use a character n-gram TFIDF model to produce candidates for all of the mentions in all splits. The candidate generator utilizes the 200k most frequent character n-gram"
2021.naacl-main.205,P18-1010,1,0.895381,"e task of entity linking, is critical to many applica- the knowledge-base information is largely incomplete. Furthermore, the scarcity of training data tions: automated knowledge base construction and completion (Riedel et al., 2013; Surdeanu et al., leads to a setting in which most entities have not 2012), information retrieval (Meij et al., 2014), been observed at training. smart assistants (Balog and Kenter, 2019), quesState-of-the-art entity linking methods which are tion answering (Dhingra et al., 2020), text mining able to link entities unseen at training time make (Leaman and Lu, 2016; Murty et al., 2018). predictions for each mention independently (LoConsider the excerpt of text from a biomed- geswaran et al., 2019; Wu et al., 2019). In this way, ical research paper in Figure 1, the three high- the methods may have difficulty linking mentions lighted mentions (expression, facial expressions, which, as in the example above, have little lexical and facially expressive) all link to the same entity, similarity with the entities in the knowledge-base, namely C0517243 - Facial Expresson as well as mentions for which the context is highly in the leading biomedical KB, Unified Medical ambiguous. Thes"
2021.naacl-main.205,P11-1138,0,0.124447,"Missing"
2021.naacl-main.205,N13-1008,1,0.740456,"t training exam1 Introduction ples (or sufficiently rich structured information in Ambiguity is inherent in the way entities are men- the knowledge-base) . However, in biomedical (Motioned in natural language text. Grounding such am- han and Li, 2019) and other specialized domains biguous mentions to their corresponding entities, (Logeswaran et al., 2019), it is often the case that the task of entity linking, is critical to many applica- the knowledge-base information is largely incomplete. Furthermore, the scarcity of training data tions: automated knowledge base construction and completion (Riedel et al., 2013; Surdeanu et al., leads to a setting in which most entities have not 2012), information retrieval (Meij et al., 2014), been observed at training. smart assistants (Balog and Kenter, 2019), quesState-of-the-art entity linking methods which are tion answering (Dhingra et al., 2020), text mining able to link entities unseen at training time make (Leaman and Lu, 2016; Murty et al., 2018). predictions for each mention independently (LoConsider the excerpt of text from a biomed- geswaran et al., 2019; Wu et al., 2019). In this way, ical research paper in Figure 1, the three high- the methods may ha"
2021.naacl-main.205,2020.acl-main.335,0,0.0188065,"or MedMentions and one hour for BC5CDR. Code and data to reproduce experiments will be made available. 5.6 Results We compare our clustering-based inference procedure, which we refer to our approach as C LUSTERING - BASED, to a state-of-the-art independent inference procedure, I NDEPENDENT, which is the zero-shot architecture proposed by Logeswaran et al.. This same model is used as the mentionentity affinity model used in our approach. We also compare to to an n-gram tf-idf model (our candidate generation model), TAGGERO NE (Leaman 5.5 Training and Inference Details and Lu, 2016), B IO S YN (Sung et al., 2020), and Our model contains 220M parameters, the major- S AP BERT (Liu et al., 2020) on both MedMentions and BC5CDR. ity of which are contained within the two separate BERT-based models. We optimize both the models Table 2 shows performance of the baseline modwith mini-batch stochastic gradient descent using els, I NDEPENDENT, and C LUSTERING - BASED inthe Adam optimizer (Kingma and Ba, 2014) with ference procedure on MedMentions and BC5CDR. recommended learning rate of 5e-5 (Devlin et al., We report results using the gold mention segmen2603 MedMentions Overall Acc. on Acc. Seen Unseen Overall Ac"
2021.naacl-main.205,D12-1042,0,0.118269,"Missing"
2021.naacl-main.68,E17-1068,0,0.157787,"random variables is captured by vector similarities. Without an explicit dependency structure it is difficult to enforce logical reasoning rules to maintain global consistency. In order to go beyond triple-level uncertainty modeling, we consider each entity as a binary random variable. However, representing such a probability distribution in an embedding space and reasoning over it is non-trivial. It is difficult to model marginal and joint probabilities for entities using simple geometric objects like vectors. In order to encode probability distributions in the embedding space, recent works (Lai and Hockenmaier, 2017; Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020) represent random variables as more complex geometric objects, such as cones and axis-aligned hyperrectangles (boxes), and use volume as the probability measure. Inspired by such advances of probability measures in embeddings, we present BEUrRE (Box Embedding for Uncertain RElational Data)2 . BEUrRE represents entities as boxes. Relations are modeled as two separate affine transforms on the head and tail entity boxes. Confidence of a triple is modeled by the intersection between the two transformed boxes. Fig. 1 shows how a fact abo"
2021.naacl-main.68,2020.emnlp-main.460,1,0.61838,"and tail entity boxes. We also incorporate logic constraints that capture the high-order dependency of facts and enhance global reasoning consistency. Extensive experiments show the promising capability of BEUrRE on confidence prediction and fact ranking for UKGs. The results are encouraging and suggest various extensions, including deeper transformation architectures as well as alternative geometries to allow for additional rules to be imposed. In this context, we are also interested in extending the use of the proposed technologies into more downstream tasks, such as knowledge association (Sun et al., 2020) and event hierarchy induction (Wang et al., 2020). Another direction is to use BEUrRE for ontology construction and population, since box embeddings are naturally capable of capturing granularities of concepts. Ethical Considerations posed in the paper aims to model uncertainty in knowledge graphs more accurately, and the effectiveness of the proposed model is supported by the empirical experiment results. Acknowledgment We appreciate the anonymous reviewers for their insightful comments and suggestions. This material is based upon work sponsored by the DARPA MCS program under Contract No. N6"
2021.naacl-main.68,P18-1025,1,0.889755,"d by vector similarities. Without an explicit dependency structure it is difficult to enforce logical reasoning rules to maintain global consistency. In order to go beyond triple-level uncertainty modeling, we consider each entity as a binary random variable. However, representing such a probability distribution in an embedding space and reasoning over it is non-trivial. It is difficult to model marginal and joint probabilities for entities using simple geometric objects like vectors. In order to encode probability distributions in the embedding space, recent works (Lai and Hockenmaier, 2017; Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020) represent random variables as more complex geometric objects, such as cones and axis-aligned hyperrectangles (boxes), and use volume as the probability measure. Inspired by such advances of probability measures in embeddings, we present BEUrRE (Box Embedding for Uncertain RElational Data)2 . BEUrRE represents entities as boxes. Relations are modeled as two separate affine transforms on the head and tail entity boxes. Confidence of a triple is modeled by the intersection between the two transformed boxes. Fig. 1 shows how a fact about the genre of the B"
2021.naacl-main.68,2020.emnlp-main.51,1,0.714844,"constraints that capture the high-order dependency of facts and enhance global reasoning consistency. Extensive experiments show the promising capability of BEUrRE on confidence prediction and fact ranking for UKGs. The results are encouraging and suggest various extensions, including deeper transformation architectures as well as alternative geometries to allow for additional rules to be imposed. In this context, we are also interested in extending the use of the proposed technologies into more downstream tasks, such as knowledge association (Sun et al., 2020) and event hierarchy induction (Wang et al., 2020). Another direction is to use BEUrRE for ontology construction and population, since box embeddings are naturally capable of capturing granularities of concepts. Ethical Considerations posed in the paper aims to model uncertainty in knowledge graphs more accurately, and the effectiveness of the proposed model is supported by the empirical experiment results. Acknowledgment We appreciate the anonymous reviewers for their insightful comments and suggestions. This material is based upon work sponsored by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval"
2021.repl4nlp-1.24,P19-1082,0,0.0240093,"eams won their championships. Then, argmax and filter operators are applied on the returned dates, yielding answers, i.e., “1999” for “David Beckham”. Semantic parsing provides a weak supervision framework to learn to perform all these reasoning steps from just the question answer pairs. Semantic parsers define a set of rules (or grammar) for generating logical forms from natural language questions. Candidate logical forms are executable queries on the knowledge bases that yield the corresponding answers. Neural semantic parsers (NSPs) (Liang et al., 2016; Guo et al., 2018; Shen et al., 2019; Guo et al., 2019) employ a neural network to translate natural language questions into logical forms. NSPs have shown good performance on KBQA tasks (Liang et al., 231 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 231–240 Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics 2016; Plepi et al., 2021) and further improved with reinforcement learning (Guo et al., 2018), multitask learning (Shen et al., 2019), and most recently meta-learning (Hua et al., 2020). Most previous works place more emphasis on modeling the reasoning behavi"
2021.repl4nlp-1.24,P16-2033,0,0.0414261,"Missing"
2021.repl4nlp-1.25,Q15-1023,0,0.0704549,"Missing"
2021.repl4nlp-1.25,2021.ccl-1.108,0,0.0844155,"Missing"
2021.repl4nlp-1.25,P19-1598,0,0.137884,"ts on the downstream tasks over existing methods. 1 Introduction Self-supervised representation learning on large text corpora using language modeling objectives has been shown to yield generalizable representations that improve performance for many downstream tasks. Examples of such approaches include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XLNET (Yang et al., 2019), GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2019) etc. However, whether such models retain structured knowledge in their representation is still an open question (Petroni et al., 2019; Poerner et al., 2019; Logan et al., 2019; Roberts et al., 2020) which has led to active research on knowledge-informed rep∗ Equal Contribution resentations (Zhang et al., 2019; Sun et al., 2019; Peters et al., 2019; Soares et al., 2019). Models that learn knowledge-informed representations can be broadly classified into two categories. The first approach augments language model pretraining with the aim of storing structured knowledge in the model parameters. This is typically done by augmenting the pre-training task, for example by masking entity mentions (Sun et al., 2019) or enforcing representational similarity in sentences conta"
2021.repl4nlp-1.25,D19-1005,0,0.0615947,"own to yield generalizable representations that improve performance for many downstream tasks. Examples of such approaches include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XLNET (Yang et al., 2019), GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2019) etc. However, whether such models retain structured knowledge in their representation is still an open question (Petroni et al., 2019; Poerner et al., 2019; Logan et al., 2019; Roberts et al., 2020) which has led to active research on knowledge-informed rep∗ Equal Contribution resentations (Zhang et al., 2019; Sun et al., 2019; Peters et al., 2019; Soares et al., 2019). Models that learn knowledge-informed representations can be broadly classified into two categories. The first approach augments language model pretraining with the aim of storing structured knowledge in the model parameters. This is typically done by augmenting the pre-training task, for example by masking entity mentions (Sun et al., 2019) or enforcing representational similarity in sentences containing the same entities (Soares et al., 2019). While this makes minimal assumptions, it requires memorizing all facts encountered during training in the model parameters, nec"
2021.repl4nlp-1.25,N18-1002,0,0.0281824,"l to select one entity from the mention’s candidates, we find the highest scoring entity, eˆi = arg maxj∈Ci sij , and use that as a target in a cross-entropy loss: Lcr = cross entropy(softmax(sij ), Ieˆi ) (1) where the softmax is over all entities (not just for mention i) in the sentence and Ieˆi is a one-hot vector with 1 for the entity eˆi and 0 everywhere else. This objective enforces the model to rank one candidate higher than others candidates for the same mention as well as candidates of other entities. Similar objective has been explored for dealing with noise in entity typing models (Xu and Barbosa, 2018; Abhishek et al., 2017). The overall objective is a combination of bert-style MLM, mention-masking (MM) and candidate ranking: Candidate Ranking To further enable the model to use the correct entities for a mention, we use a weak entity linking objective that forces the model to rank one of the entities, from the candidate set Lmlm + αLmm + βLcr 4 Experiments Implementation details are in Supplementary. Code of our models is available here1 . 1 243 (2) Source Code: https://github.com/dungtn/KNIT Models evaluated: (1) RoBERTa (Liu et al., 2019b): the model uses the MLM objective for pre-traini"
2021.repl4nlp-1.25,D19-1250,0,0.06374,"Missing"
2021.repl4nlp-1.25,P19-1139,0,0.267682,"nguage modeling objectives has been shown to yield generalizable representations that improve performance for many downstream tasks. Examples of such approaches include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XLNET (Yang et al., 2019), GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2019) etc. However, whether such models retain structured knowledge in their representation is still an open question (Petroni et al., 2019; Poerner et al., 2019; Logan et al., 2019; Roberts et al., 2020) which has led to active research on knowledge-informed rep∗ Equal Contribution resentations (Zhang et al., 2019; Sun et al., 2019; Peters et al., 2019; Soares et al., 2019). Models that learn knowledge-informed representations can be broadly classified into two categories. The first approach augments language model pretraining with the aim of storing structured knowledge in the model parameters. This is typically done by augmenting the pre-training task, for example by masking entity mentions (Sun et al., 2019) or enforcing representational similarity in sentences containing the same entities (Soares et al., 2019). While this makes minimal assumptions, it requires memorizing all facts encountered durin"
2021.repl4nlp-1.25,N18-1167,0,0.0301919,"Missing"
2021.repl4nlp-1.28,D19-1522,0,0.0620279,"Missing"
2021.repl4nlp-1.28,2020.acl-main.617,0,0.0632545,"Missing"
2021.repl4nlp-1.28,W09-1109,0,0.05137,"tional edges and transitive closure edges, both from an overfitting and generalization standpoint, identifying subsets where further improvement is needed. The source code for our model and the dataset can be found in https://github.com/ iesl/box-to-box-transform.git. 2 Related Work Recent advances in representing one single hierarchy mainly fall in two categories: 1) representing hierarchies in non-Euclidian space (eg. hyperbolic space, due to the curvature’s inductive bias to model tree-like structures) 2) using region-based representations instead of vectors for each node in the hierarchy (Erk, 2009). Hyperbolic space has been shown to be efficient in representing hierarchical relations, but also encounters difficulties in training (Nickel and Kiela, 2017; Ganea et al., 2018b; Chamberlain et al., 2017). Categorization models in psychology often represent a concept as a region (Nosofsky, 1986; Smith et al., 1988; Hampton, 1991). Vilnis and McCallum (2015) and Athiwaratkun and Wilson (2018) use Gaussian distributions to embed each word in the corpus, the latter of which uses thresholded divergences which amount to region representations. Vendrov et al. (2016) and Lai and Hockenmaier (2017)"
2021.repl4nlp-1.28,P18-1025,1,0.389962,"making generalization between hierarchies infeasible. In this work, we introduce a learned box-to-box transformation that respects the structure of each hierarchy. We demonstrate that this not only improves the capability of modeling cross-hierarchy compositional edges but is also capable of generalizing from a subset of the transitive reduction. 1 Introduction Representation learning for hierarchical relations is crucial in natural language processing because of the hierarchical nature of common knowledge, for example, <Bird I S A Animal> (Athiwaratkun and Wilson, 2018; Vendrov et al., 2016; Vilnis et al., 2018; Nickel and Kiela, 2017). The I S A relation represents meaningful hierarchical relationships between concepts and plays an essential role in generalization for other relations, such as the generalization of <organ PART O F person> based on <eye PART O F of person>, and <organ I S A eye>. The fundamental nature of the I S A relation means that it is inherently involved in a large amount of compositional reasoning involving other relations. Modeling hierarchies is essentially the problem of modeling a poset, or partially ordered set. The task of inferring missing edges that requires learning a"
2021.repl4nlp-1.28,E17-1068,0,0.0202635,"n the hierarchy (Erk, 2009). Hyperbolic space has been shown to be efficient in representing hierarchical relations, but also encounters difficulties in training (Nickel and Kiela, 2017; Ganea et al., 2018b; Chamberlain et al., 2017). Categorization models in psychology often represent a concept as a region (Nosofsky, 1986; Smith et al., 1988; Hampton, 1991). Vilnis and McCallum (2015) and Athiwaratkun and Wilson (2018) use Gaussian distributions to embed each word in the corpus, the latter of which uses thresholded divergences which amount to region representations. Vendrov et al. (2016) and Lai and Hockenmaier (2017) make use of the reverse product order on Rn+ , which effectively results in cone representations. Vilnis et al. (2018) further extend this cone representation to axis-aligned hyperRegion representations are also used for tasks which do not require modeling hierarchy. In Vilnis et al. (2018), the authors also model conditional probability distributions using box embeddings. Abboud et al. (2020) and Ren et al. (2020) take a different approach, using boxes for their capacity to contain many vectors to provide slack in the loss function when modeling knowledge base triples or representing logical"
C04-1081,O98-3002,0,0.159471,"e roughly understood as discriminatively-trained hidden Markov models with next-state transition functions represented by exponential models (as in maximum entropy classifiers), and with great flexibility to view the observation sequence in terms of arbitrary, overlapping features, with long-range dependencies, and at multiple levels of granularity. These beneficial properties suggests that CRFs are a promising approach for Chinese word segmentation. New word detection is one of the most important problems in Chinese information processing. Many machine learning approaches have been proposed (Chen and Bai, 1998; Wu and Jiang, 2000; Nie et al., 1995). New word detection is normally considered as a separate process from segmentation. However, integrating them would benefit both segmentation and new word detection. CRFs provide a convenient framework for doing this. They can produce not only a segmentation, but also confidence in local segmentation decisions, which can be used to find new, unfamiliar character sequences surrounded by high-confidence segmentations. Thus, our new word detection is not a stand-alone process, but an integral part of segmentation. Newly detected words are re-incorporated in"
C04-1081,N04-4028,1,0.409574,"n order to improve segmentation; improved segmentation can potentially further improve new word detection. We measure the performance of new word detection by its improvements on segmentation. Given a word segmentation proposed by the CRF, we can compute a confidence in each segment. We detect as new words those that are not in the existing word list, yet are either highly confident segments, or low confident segments that are surrounded by high confident words. A confidence threshold of 0.9 is determined by cross-validation. Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004). The standard forward-backward algorithm (Rabiner, 1990) calculates Zx , the total likelihood of all label sequences y given a sequence x. 5 Experiments and Analysis To make a comprehensive evaluation, we use all four of the datasets from a recent Chinese word segmentation bake-off competition (Sproat and Emerson, 2003). These datasets represent four different segmentation standards. A summary of the datasets is shown in Table 1. The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall. 5.1 Experimental design Since CTB and PK are provided in the G"
C04-1081,P03-1035,0,0.0113523,"Missing"
C04-1081,W02-2018,0,0.00496449,"Chow, 1990). The parameters can be estimated by maximum likelihood—maximizing the conditional probability of a set of label sequences, each given their corresponding input sequences. The log-likelihood of training set {(xi , yi ) : i = 1, ...M } is written X LΛ = log PΛ (yi |xi ) i = Ã T X X X i t=1 ! λk fk (yt−1 , yt , x, t) − log Zxi k Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al., 1995), can be used to train CRFs. However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). The gradient of the likelihood is ∂PΛ (y|x)/∂λk = X (i) fk (yt−1 , yt , x(i) , t) i,t − X i,y,t PΛ (y|x(i) )fk (yt−1 , yt , x(i) , t) . 4. Third-order: Here inputs are examined in the context of the current, and two previous states. Feature function are represented as f (yt−2 , yt−1 , yt , x). CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. 3 2.1 Regularization in CRFs To avoid over-fitting, log-likelihood is"
C04-1081,N04-1042,1,0.426596,"as follows. LΛ = X log PΛ (yi |xi ) − i X λ2 k 2σk2 (2) k where σk2 is the variance for feature dimension k. The variance can be feature dependent. However for simplicity, constant variance is often used for all features. We experiment an alternate version of Gaussian prior in which the variance is feature dependent. We bin features by frequency in the training set, and let the features in the same bin share the same variance. The discounted value is set to be λk where ck is the count of features, M is dck /M e×σ 2 the bin size set by held out validation, and dae is the ceiling function. See Peng and McCallum (2004) for more details and further experiments. 2.2 State transition features Varying state-transition structures with different Markov order can be specified by different CRF feature functions, as determined by the number of output labels y examined together in a feature function. We define four different state transition feature functions corresponding to different Markov orders. Higher-order features capture more long-range dependencies, but also cause more data sparseness problems and require more memory for training. The best Markov order for a particular application can be selected by held-ou"
C04-1081,N03-1028,0,0.0289698,"The parameters can be estimated by maximum likelihood—maximizing the conditional probability of a set of label sequences, each given their corresponding input sequences. The log-likelihood of training set {(xi , yi ) : i = 1, ...M } is written X LΛ = log PΛ (yi |xi ) i = Ã T X X X i t=1 ! λk fk (yt−1 , yt , x, t) − log Zxi k Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al., 1995), can be used to train CRFs. However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). The gradient of the likelihood is ∂PΛ (y|x)/∂λk = X (i) fk (yt−1 , yt , x(i) , t) i,t − X i,y,t PΛ (y|x(i) )fk (yt−1 , yt , x(i) , t) . 4. Third-order: Here inputs are examined in the context of the current, and two previous states. Feature function are represented as f (yt−2 , yt−1 , yt , x). CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. 3 2.1 Regularization in CRFs To avoid over-fitting, log-likelihood is usually penalized by so"
C04-1081,W03-1719,0,0.0374979,"ot in the existing word list, yet are either highly confident segments, or low confident segments that are surrounded by high confident words. A confidence threshold of 0.9 is determined by cross-validation. Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004). The standard forward-backward algorithm (Rabiner, 1990) calculates Zx , the total likelihood of all label sequences y given a sequence x. 5 Experiments and Analysis To make a comprehensive evaluation, we use all four of the datasets from a recent Chinese word segmentation bake-off competition (Sproat and Emerson, 2003). These datasets represent four different segmentation standards. A summary of the datasets is shown in Table 1. The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall. 5.1 Experimental design Since CTB and PK are provided in the GB encoding while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible. Note that this conversion could potentially worsen performance slightly due to a few conversion errors. We use cross-validation to choose Markov-order and perform feature selection."
C04-1081,J00-3004,0,0.024271,"mentation fall roughly into two categories: heuristic dictionary-based methods and statistical machine learning methods. In dictionary-based methods, a predefined dictionary is used along with hand-generated rules for segmenting input sequence (Wu, 1999). However these approaches have been limited by the impossibility of creating a lexicon that includes all possible Chinese words and by the lack of robust statistical inference in the rules. Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuurmans, 2001) and supervised learning (Teahan et al., 2000). Many current approaches suffer from either lack of exact inference over sequences or difficulty in incorporating domain knowledge effectively into segmentation. Domain knowledge is either not used, used in a limited way, or used in a complicated way spread across different components. For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge. Gao et al (2003) uses class-based language for word segmentation where some word category information can be incorporated. Zhang et al (2003) use a hierarchical hidden Markov Model to incorp"
C04-1081,W00-1207,0,0.00910744,"Missing"
C04-1081,W03-1727,0,0.00917881,"AS dataset. 5.4 AS 0.894 0.871 0.889 CTB 0.792 0.849 Error analysis and discussion Several typical errors are observed in error analysis. One typical error is caused by inconsistent segmentation labeling in the test set. This is most notorious in CTB dataset. The second most typical error is in new, out-of-vocabulary words, especially proper names. Although our new word detection fixes many of these problems, it is not effective enough to recognize proper names well. One solution to this problem could use a named entity extractor to recognize proper names; this was found to be very helpful in Wu (2003). One of the most attractive advantages of CRFs (and maximum entropy models in general) is its the flexibility to easily incorporate arbitrary features, here in the form domain-knowledge-providing lexicons. However, obtaining these lexicons is not a trivial matter. The quality of lexicons can affect the performance of CRFs significantly. In addition, compared to simple models like n-gram language models (Teahan et al., 2000), another shortcoming of CRF-based segmenters is that it requires significantly longer training time. However, training is a one-time process, and testing time is still lin"
C04-1081,O03-4002,0,0.754422,"Missing"
C04-1081,W03-1709,0,0.209267,"Missing"
C04-1081,P04-1054,0,\N,Missing
C04-1081,W03-1726,0,\N,Missing
D09-1009,D08-1112,1,0.689141,"s such that each iteration simulates one minute of labeling by setting cmax = 60, cskip = 2 and clabel = 4. For instance active learning, we use Algorithm 1 but without the skip option, and set clabel = 0.7. We use N = 10 iterations, so the entire experiment simulates 10 minutes of annotation time. For efficiency, we consider the 500 most frequent unlabeled features in each iteration. To start, ten randomly selected seed labeled features are provided. We use random (rand) selection, uncertainty sampling (US) (using sequence entropy, normalized by sequence length) and information density (ID) (Settles and Craven, 2008) to select instance queries. We use Entropy Regularization (ER) (Jiao et al., 2006) to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {10−3 , 10−2 , 10−1 , 1, 10} multi#labeled plied by #unlabeled for each data set and query selection method. Seed instances are provided such that the simulated labeling time is equivalent to labeling 10 features. We evaluate on two sequence labeling tasks. The apartments task involves segmenting 300 apartment classified ads into 11 fields including features, rent, neighborhood, and contact. We use the same feature processin"
D09-1009,P07-1036,0,0.0296531,"tion 3 If we use squared error for S, the partial derivative is the ˆ k − E[Gk (x, y)]). covariance multiplied by 2(G 4 We this notation for an indicator function that returns 1 if the condition in braces is satisfied, and 0 otherwise. 83 ple a label. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. In experiments in this paper, using this optimization does not significantly affect final accuracy. We use numerical optimization to estimate model parameters. In general GE objective functions are not convex. Consequently, we init"
D09-1009,P05-1046,0,0.04412,"Missing"
D09-1009,N06-1041,0,0.29052,"objective functions are not convex. Consequently, we initialize 0th-order CRF parameters using a sliding window logistic regression model trained with GE. We also include a Gaussian prior on parameters with σ 2 = 10 in the objective function. 3.2 Learning with labeled features The training procedure described above requires a set of observational tests or input features with target distributions over labels. Estimating a distribution could be a difficult task for an annotator. Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). For example, we say that the word feature call has label contact. A label for a feature simply indicates that the feature is a good indicator of the label. Note that features can have multiple labels, as does included in the active learning session shown in Table 1. We convert an input feature with a set of labels L into a distribution by assigning probability 1/|L |for each l ∈ L and probability 0 for each l ∈ / L. By assigning 0 probability to labels l ∈ / L, we can use the speed-up described in the previous section. 3.3 4 Active Learning by Labeling Features Feature"
D09-1009,P06-1027,0,0.0602515,"es. The most probable output sequence and transition marginal distributions can be computed using variants of Viterbi and forward-backward. Provided a training data distribution p˜, we estimate CRF parameters by maximizing the conditional log likelihood of the training data. We train models using LM M L (θ) with expected gradient (Salakhutdinov et al., 2003). To additionally leverage unlabeled data, we compare with entropy regularization (ER). ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 3 Generalized Expectation Criteria In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. GE criteria are terms in a parameter estimation objective function that express preferences on the L(θ) = Ep˜(x,y) [log p(y|x; θ)] We use numerical optimization to maximize L(θ), which requires the gradient of L(θ) with respect to the parameters. It can be shown that the partial derivative with respect to parameter j is"
D09-1009,P08-1099,1,0.57616,"settles@cs.wisc.edu Abstract In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. This paper proposes an active learning approach in which the user provides “labels” for input features, rather than instances. A labeled input feature denotes that a particular input feature, for example the word call, is highly indicative of a particular label, such as contact. Table 1 provides an excerpt of a feature active learning session. In this paper, we advocate using generalized expectation (GE) criteria (Mann and McCallum, 2008) for learning with labeled features. We provide an alternate treatment of the GE objective function used by Mann and McCallum (2008) and a novel speedup to the gradient computation. We then provide a pool-based feature active learning algorithm that includes an option to skip queries, for cases in which a feature has no clear label. We propose and evaluate feature query selection algorithms that aim to reduce model uncertainty, and compare to several baselines. We evaluate our method using both real and simulated user experiments on two sequence labeling tasks. Compared to previous approaches"
D09-1014,P06-1009,0,0.0428469,"Missing"
D09-1014,J03-1002,0,0.00273005,"The top N alignment features that have maximum MI with this correct labeling are selected as alignment criteria. We bin target expectations of these criteria into 11 bins as [0.05, 0.1, 0.2, 0.3, . . . , 0.9, 0.95].7 In our experiments, we set N = 10 and use a fixed weight w = 10.0 for all expectation criteria (no tuning of parameters was performed). Table 4 shows a sample of GE criteria used in our experiments.8 4.1 Alternate approaches We compare our method against alternate approaches that either learn alignment or extraction models from training data. Alignment approaches. We use GIZA++ (Och and Ney, 2003) to train generative directed alignment models: HMM and IBM Model4 (Brown et al., 1993) from training record-text pairs. These models are currently being used in state-of-the-art machine translation systems. Alignments between matching DB records and text sequences are then used for labeling at test time. 7 Mann and McCallum (2008) note that GE criteria are robust to deviation of specified targets from actual expectations. 8 A complete list of expectation criteria is available at http://www.cs.umass.edu/∼kedarb/dbie expts.txt. 136 Extraction approaches. The first alternative (DB-CRF) trains a"
D09-1014,J93-2003,0,0.0495704,"several advantages over previous methods. First, we are able to model field ordering and context around fields by learning an extractor from annotations of the text itself. Second, a probabilistic model for word alignment can exploit dependencies among alignments, and is also robust to errors, formatting differences, and missing fields in text and the record. Our word alignment model is a conditional random field (CRF) (Lafferty et al., 2001) that generates alignments between tokens of a text sequence and a matching database record. The structure of the graphical model resembles IBM Model 1 (Brown et al., 1993) in which each target (record) word is assigned one or more source (text) words. The alignment is generated conditioned on both the record and text sequence, and therefore supports large sets of rich and nonindependent features of the sequence pairs. Our model is trained without the need for labeled word alignments by using generalized expectation (GE) criteria (Mann and McCallum, 2008) that penalize the divergence of specific model expectations from target expectations. Model parameters are estimated by minimizing this divergence. To limit over-fitting we include a L2 -regularization term in"
D09-1014,N04-1042,1,0.685502,"Missing"
D09-1014,D07-1087,0,0.315736,"that uses heuristic alignments. 1 In many cases, however, there already exists a database with schema related to the desired output, and records that are imperfectly rendered in the available unlabeled text. This database can serve as a source of significant supervised guidance to machine learning methods. Previous work on using databases to train information extractors has taken one of three simpler approaches. In the first, a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence (Agichtein and Ganti, 2004; Canisius and Sporleder, 2007). However, this approach does not model context, errors or different formats of fields in text, and requires large number of database entries to learn an accurate language model. The second approach (Sarawagi and Cohen, 2004; Michelson and Knoblock, 2005; Mansuri and Sarawagi, 2006) uses database or dictionary lookups in combination with similarity measures to add features to the text sequence. Although these features are very informative, learning algorithms still require annotated data to make use of them. The final approach heuristically labels texts using matching records and learns extrac"
D09-1014,P07-1036,0,0.280919,"Missing"
D09-1014,D08-1112,0,0.0423423,"Missing"
D09-1014,P05-1046,0,0.0424161,"Missing"
D09-1014,N06-1041,0,0.0146215,"criteria (Section 3.2). We evaluate our alignment model indirectly in terms of token labeling accuracy (i.e., percentage of correctly labeled tokens in test citation data) since we do not have annotated alignments. The alignment model is then used to train a ExtrCRF model as described in Section 3.3. Again, we use token labeling accuracy for evaluation. We also measure F1 performance as the harmonic mean of precision and recall for each label. Thus, a conditional model lets us use these arbitrary helpful features that cannot be exploited tractably in a generative model. As is common practice (Haghighi and Klein, 2006; Mann and McCallum, 2008), we simulate user-specified expectation criteria through statistics on manually labeled citation texts. For extraction criteria, we select for each label, the top N extraction features ordered by mutual information (MI) with that label. Also, we aggregate the alignment features of record tokens whose alignment with a target text token results in a correct label assignment. The top N alignment features that have maximum MI with this correct labeling are selected as alignment criteria. We bin target expectations of these criteria into 11 bins as [0.05, 0.1, 0.2, 0.3, ."
D09-1014,N06-1012,1,0.881701,"Missing"
D09-1014,P09-1011,0,0.0475797,"Missing"
D09-1014,H05-1010,0,0.0608245,"Missing"
D09-1014,P08-1099,1,0.948767,"del is a conditional random field (CRF) (Lafferty et al., 2001) that generates alignments between tokens of a text sequence and a matching database record. The structure of the graphical model resembles IBM Model 1 (Brown et al., 1993) in which each target (record) word is assigned one or more source (text) words. The alignment is generated conditioned on both the record and text sequence, and therefore supports large sets of rich and nonindependent features of the sequence pairs. Our model is trained without the need for labeled word alignments by using generalized expectation (GE) criteria (Mann and McCallum, 2008) that penalize the divergence of specific model expectations from target expectations. Model parameters are estimated by minimizing this divergence. To limit over-fitting we include a L2 -regularization term in the objective. The model expectations in GE criteria are taken with respect to a set of alignment latent variables that are either specific to each sequence pair (local) or summarizing the entire data set (global). This set is constructed by including all alignment variables a that satisfy a certain binary feature (e.g., f (a, x1 , y1 , x2 ) = 1, for labeled record (x1 , y1 ), and text"
D09-1092,J93-2003,0,0.0347959,"Missing"
D09-1092,P08-1088,0,0.452554,"Missing"
D09-1092,D08-1038,0,0.102748,"ble, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. 1 Introduction Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006). Much of this work, however, has occurred in monolingual contexts. In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience. In this paper, we present the polylingual topic model (PLTM). We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedin"
D09-1092,W02-0902,0,0.175315,"Missing"
D09-1092,2007.mtsummit-tutorials.1,0,0.0100598,"anguages). There are many potential applications for polylingual topic models. Although research literature is typically written in English, bibliographic 880 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the contents of collections in unfamiliar languages and identify trends in topic prevalence. 2 z α Related Work ... z Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007). Tam, Lane and Schultz (Tam et al., 2007) also show improvements in machine translation using bilingual topic models. Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added. We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages. A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented"
D10-1099,P07-1073,0,0.741937,"uctured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1 Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. Th"
D10-1099,W02-1001,0,0.00454491,"eatures fi,r,t i gument of c, has the entity type t and the candidate tuple c is labelled as instance of relation r. For exPair ample, f1,founded,person fires if Ye1 (argument i = 1) is in state person, and Ye1 ,e2 in state founded, regardless of the state of Ye2 . 1017 domly picks a variable Yc and samples its relation value conditioned on its Markov Blanket. At test time we decrease the temperature of our sampler in order to find an approximation of the MAP solution. 3.3 Training Most learning methods need to calculate the model expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per"
D10-1099,P04-1054,0,0.045712,"R a relation instance.3 It denotes the membership of the tuple c in the relation R. For example, founded (B ILL G ATES , M ICROSOFT) is a relation instance denoting that B ILL G ATES and M ICROSOFT are related in the founded relation. In the following we will always consider some set of candidate tuples C that may or may not be related. We define Cn ⊂ C to be set of all n-ary tuples in C. Note that while our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2 . Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, B ILL G ATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 2 The pyramid algorithm of Kate and Mooney (2010) may scale well, but it is not clear how to apply their scheme to crossdocument extraction. 1014 3 Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. We care ab"
D10-1099,P05-1045,0,0.00774075,"rformance, the score is normalized by the number of relation mentions. For manual evaluation we pick the top ranked 50 relation instances for the most frequent relations. We ask three annotators to inspect the mentions of these relation instances to decide whether they are correct. Upon disagreement, we use majority vote. To summarize precisions across relations, we take their average, and their average weighted by the proportion of predicted instances for the given relation. 5.1.1 Data preprocessing We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE. We treat consecutive tokens which share the same category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in (Mintz et"
D10-1099,P10-1030,0,0.0372026,"hich inference is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate an"
D10-1099,W10-2924,0,0.0649842,"ile our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2 . Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, B ILL G ATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 2 The pyramid algorithm of Kate and Mooney (2010) may scale well, but it is not clear how to apply their scheme to crossdocument extraction. 1014 3 Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. We care about entity types for two reasons. First, they can be important for downstream applications: if consumers of our extracted facts know the type of entities, they can find them more easily, visualize them more adequately, and perform operations specific to these types (write emails to persons, book a hotel in a city, etc.). Second, they are useful for extracting binary relations due to selectional p"
D10-1099,P09-1113,0,0.0581474,"ies expressed in structured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1 Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves muc"
D10-1099,W04-2407,0,0.0180525,"e category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features Xic similar to those used in (Mintz et al., 2009): lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. We use the openNLP POS tagger4 to obtain POS tags and employ the MaltParser (Nivre et al., 2004) for dependency parsing. For candidate tuples with arity 1 (entity types) we use the following features: the entity’s word form, the POS sequence, the head of the entity in the dependency parse tree, the Stanford named entity tag, and the left and right words to the current entity mention phrase. 5.1.2 Configurations We apply the following configurations of our factor graphs. As our baseline, and roughly equivalent to previous work (Mintz et al., 2009), we pick the templates TBias and TMen . These describe a fully disconnected graph, and we will refer to this configuration as isolated. Next, w"
D10-1099,W09-1406,1,0.324897,"diction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping process. Here in each iteration of bootstrapping, extracted facts that violate compatibility constraints will not be used to generate additional patterns in the next iteration. 5 Experiments W"
D10-1099,D08-1009,0,0.0360096,"Missing"
D11-1001,W09-1402,0,0.192323,"teractions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods"
D11-1001,P05-1022,0,0.0491878,"nally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t ∈ T . It  creates one active feature for each element in t, t ∈ TReg × feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic ch"
D11-1001,P01-1030,0,0.00930482,"al search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constrain"
D11-1001,W09-1401,0,0.0854254,"ed to automatically extract structured representations from biomedical text—a process often referred to as biomedical text mining. One major focus of biomedical text mining has been the extraction of named entities, such genes or gene products, and of flat binary relations between such entities, such as protein-protein interactions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare"
D11-1001,D10-1125,0,0.0226592,"l variables λ that will appear as local penalties in the subproblems to be solved. The algorithm will try to tune these variables such that at convergence the coupling constraints will be fulfilled. This is done by first op¯) over I. Now, timizing s2 (e, a) over O and s2 (¯ e, a whenever there is disagreement between two variables to be coupled, the corresponding dual parameter is shifted, increasing the chance that next time both models will agree. For example, if in the first iteration we predict e6,Bind = 1 but e¯6,Bind = 0, we set λ6,Bind = −α where α is some stepsize (chosen according to Koo et al. (2010)). This will decrease the coefficient for e6,Bind , and increase the coefficient for e¯6,Bind . Hence, we have a higher chance of agreement for this variable in the next iteration. The algorithm repeats the process described above until all variables agree, or some predefined number R of iterations is reached. In the former case we in fact have the exact solution to the original ILP. 1 The ILP representation could be taken from the MLNs of Riedel et al. (2009) and the mapping to ILPs of Riedel (2008). 6 2 2,3 3 3 2,3 2,3 3 3 3 (e, a)← bestOut (λ) ¯)← bestIn (−λ) (¯ e, a (e, a)← bestOut (cout ("
D11-1001,P08-2026,0,0.0132043,"semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t ∈ T . It  creates one active feature for each element in t, t ∈ TReg × feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009)."
D11-1001,P11-1163,0,0.771461,"Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition"
D11-1001,W11-1806,0,0.0295477,"Missing"
D11-1001,E06-1011,0,0.0169088,"ugh ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q . Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforcing consistency between them and the ei,t and ai,j,r variables is not. We address this in section 3.3.1. Reconstruction of events from solutions (e, a, b) can be done almost exactly as described by Björne et al. (2009). However, while they group binding arguments according to ad-hoc rules based on dependency paths from trigger to argument, we simply query the variables bp,q . To simplify our exposition we introduce additional notation. We denote the set of protein head tokens with Prot (x); the set of a possible targets def for"
D11-1001,N09-1018,1,0.791779,"e framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Ea"
D11-1001,W10-1905,0,0.0559999,"in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods in Natural Language"
D11-1001,N10-1123,0,0.0861519,"del jointly predicts triggers and arguments. Notably, the highest scoring event structure under this model can be found efficiently in O (mn) time where m is the number of trigger candidates, and n the number of argument candidates. This is only slightly slower than the O (m0 n) runtime of a pipeline, where m0 is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to 2 train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference he"
D11-1001,C04-1197,0,0.0201207,", 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic ar8 guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments We evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We als"
D11-1001,W06-1616,1,0.727428,"icates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyaka"
D11-1001,W11-1807,1,0.583242,"d version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 Theme 5 6 7 8 Theme Theme Figure 2: Two binding events with identical trigger. The projection grap"
D11-1001,W09-1406,1,0.586125,"me; this theme can a be protein or, as in our case, another event. Regulations may also have zero or one cause arguments that denote events or proteins which trigger the regulation. In the BioNLP shared task, we are also asked to find a trigger (or clue) token for each event. This token grounds the event in text and allows users to 3 2.1 Event Projection To formulate the search for event structures of the form shown in figure 1a) as an optimization problem, it will be convenient to represent them through a set of binary variables. We introduce such a representation, inspired by previous work (Riedel et al., 2009; Björne et al., 2009) and based on a projection of events to a graph structure over tokens, as seen figure 1b). Consider sentence x and a set of candidate trigger tokens, denoted by Trig (x). We label each candidate i with the event type it is a trigger for, or None if it is not a trigger. This decision is represented through a set of binary variables ei,t , one for each possible event type t. In our example we have e6,Binding = 1. The set of possible event types will be denoted as T , the regulation event types as def {PosReg, NegReg, Reg} and its complement TReg = def T  TReg . as T¬reg ="
D11-1001,W11-1808,1,0.924223,"LP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 Theme 5 6 7 8 Theme Theme Figure 2: Two binding events with identical trigger. The projection graph does not change even if both events are merged. 2 quickly validate extracted events. For examp"
D11-1001,W04-2401,0,0.0933504,"ovide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique a"
D11-1001,D10-1001,0,0.498723,"here m0 is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to 2 train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the"
D11-1001,P05-1073,0,0.0609705,"Missing"
D11-1001,C10-1088,0,\N,Missing
D11-1001,W11-1801,0,\N,Missing
D11-1024,J90-1003,0,0.200263,"an external reference corpus. Ideally, all such co-occurrence information would already be accounted for in the model. We believe that one of the main contributions of our work is demonstrating that standard topic models do not fully utilize available co-occurrence information, and that a held-out reference corpus is therefore not required for purposes of topic evaluation. Equation 1 is very similar to pointwise mutual information (PMI), but is more closely associated with our expert annotations than PMI (which achieves AUC 0.64 and AIC 170.51). PMI has a long history in language technology (Church and Hanks, 1990), and was recently used by Newman et al. (2010) to evaluate topic models. When expressed in terms of count variables as in equation 1, PMI includes an (t) additional term for D(vm ). The improved performance of our metric over PMI implies that what matters is not the difference between the joint probability of words m and l and the product of marginals, but the conditional probability of each word given the each of the higher-ranked words in the topic. In order to provide intuition for the behavior of our topic coherence metric, table 1 shows three example topics and their topic coherence scor"
D11-1024,N10-1012,0,0.30875,"nnot be shown without reducing users’ confidence. The evaluation of statistical topic models has traditionally been dominated by either extrinsic methods (i.e., using the inferred topics to perform some external task such as information retrieval (Wei and Croft, 2006)) or quantitative intrinsic methods, such as computing the probability of held-out documents (Wallach et al., 2009). Recent work has focused on evaluation of topics as semanticallycoherent concepts. For example, Chang et al. (2009) found that the probability of held-out documents is not always a good predictor of human judgments. Newman et al. (2010) showed that an automated evaluation metric based on word co-occurrence statistics gathered from Wikipedia could predict human evaluations of topic quality. AlSumait et al. (2009) used differences between topic-specific distributions over words and the corpus-wide distribution over words to identify overly-general “vacuous” topics. Finally, Andrzejewski et al. (2009) developed semi-supervised methods that avoid specific user-labeled semantic coherence problems. The contributions of this paper are threefold: (1) To identify distinct classes of low-quality topics, some of which are not flagged b"
D11-1135,P07-1073,0,0.135341,"Missing"
D11-1135,E09-1018,0,0.0118929,"g features ‘PER-LOC’ and ‘ORG-LOC’ can push the model to split the clusters into two and put the third case into a new cluster. Hence we propose Rel-LDA1. It is similar to Rel-LDA, except that each tuple is represented with more features. Besides p, s, and d, we introduce trigger words, lexical pattern, POS tag pattern, the named entity pair and the syntactic category pair features for each tuple. Lexical pattern is the word sequence between the two arguments of a tuple and POS tag pattern is the POS tag sequence of the lexical pattern. See Table 1 as an example. Following typical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). H"
D11-1135,P11-1054,0,0.0752508,"Missing"
D11-1135,P04-1054,0,0.340081,"Missing"
D11-1135,P05-1045,0,0.0328784,". Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less fr"
D11-1135,N10-1061,1,0.765118,"Missing"
D11-1135,P04-1053,0,0.672056,"Missing"
D11-1135,P10-1150,0,0.0486091,"(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity mention itself and its named"
D11-1135,P09-1113,0,0.0858364,"n different relations. The last cluster shown in the table is a mixture of news companies and government agencies. This may be because this entity cluster is affected by many relations. 4.2 Distant Supervision based Relation Extraction Our generative models detect clusters of dependency paths and their arguments. Such clusters are interesting in their own right, but we claim that they can also be used to help a supervised relation extractor. We validate this hypothesis in the context of relation extraction with distant supervision using predicted clusters as features. Following previous work (Mintz et al., 2009), we use Freebase as our distant supervision source, and align related entity pairs to the New York Times articles discussed earlier. Our training and test instances are pairs of entities for which both arguments appear in at least one sentence together. Features of each instance are extracted from all sentences in which both entities appear together. The gold label for each instance comes from Freebase. If a pair of entities is not related according to Freebase, we consider it a negative example. Note that this tends to create some amount of noise: some pairs may be related, but their relatio"
D11-1135,W04-2407,0,0.0404193,"our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path needs to be shorter than 10 edges, since longer paths occur less frequently. Second, the dependency relations in the path should connect two content words, i.e. nouns, verbs, adjectives and adverbs. For example, in phrase ‘solve a problem’, ‘obj(solve, problem)’ is kept, while ‘det(problem, a)’ is discarded. Finally, the dependency labels on the path must not be: ‘conj’, ‘ccomp’, ‘paratax"
D11-1135,P02-1006,0,0.0905542,"Missing"
D11-1135,P10-1044,0,0.0284402,"xample. Following typical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features includ"
D11-1135,C02-1151,0,0.229914,"Missing"
D11-1135,P10-1045,0,0.0267957,"ical EM learning(Charniak and Elsner, 2009), we start with a much simpler generative model, expose the model to fewer features first, and iteratively add more features. First, we train a Rel-LDA model, i.e. the model only generates the dependency path, source and destination arguments. After each interval of 10 iterations, we introduce one additional feature. We add the features in the order of trigger, lexical pattern, POS, NER pair, and syntactic pair. 3.3 Type-LDA model We know that relations can only hold between certain entity types, known as selectional preferences (Ritter et al., 2010; Seaghdha, 2010; Kozareva and Hovy, 2010). Hence we propose Type-LDA model. This model can capture the selectional preferences of relations to their arguments. In the mean time, it clusters tuples into relational clusters, and arguments into different entity clusters. The entity clusters could be interesting in many ways, for example, defining fine-grained entity types and finding new concepts. We split the features of a tuple into relation level features and entity level features. Relation level features include the dependency path, trigger, lex and POS features; entity level features include the entity men"
D11-1135,N03-1033,0,0.00961188,"t some noisy documents, for example, obituary content, lists and so on. Obituary articles often contain syntax that diverges from standard newswire text. This leads to parse errors with WSJ-trained parsers and in turn, makes extraction harder. We also filter out documents that contain lists or tables of items (such as books, movies) because this semi-structured information is not the focus of our current work. After filtering we are left with approximately 428K documents. They are preprocessed in several steps. First we employ Stanford tools to tokenize, sentence-split and Part-Of-Speech tag (Toutanova et al., 2003) a document. Next we recognize named entities (Finkel et al., 2005) by labelling tokens with PERSON, ORGANIZATION, LOCATION, MISC and NONE tags. Consecutive tokens which share the same category are assembled into entity mentions. They serve as source and destination arguments of the tuples we seek to model. Finally we parse each sentence of a document using MaltParser (Nivre et al., 2004) and extract dependency paths for each pair of named entity mentions in one sentence. Following DIRT (Lin and Pantel, 2001), we filter out tuples that do not satisfy the following constraints. First, the path"
D11-1135,D10-1099,1,0.399453,"Missing"
D11-1135,P08-1004,0,\N,Missing
D11-1135,D09-1001,0,\N,Missing
D11-1135,P08-1000,0,\N,Missing
D12-1067,W06-2920,0,0.0660709,"And what kind of reduction in runtime does this reduction in edges lead to? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, price and cut (PPC) on the same model. This model contains all constraints presented in 3.2. The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach. We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions. This n"
D12-1067,P05-1022,0,0.0635364,"ss, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation"
D12-1067,D07-1001,0,0.0237009,"Missing"
D12-1067,N07-1011,1,0.832712,"maller integrality gap and higher speed. 1 Introduction Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008)"
D12-1067,N03-1016,0,0.0334835,"ed prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Running Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood as a method to lazily construct a consistent heuristic. In every step this method finds edge"
D12-1067,P10-1001,0,0.313816,"2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 201"
D12-1067,D10-1125,0,0.44312,"dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal"
D12-1067,P09-1039,0,0.244399,"of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo a"
D12-1067,D11-1022,0,0.0338713,"with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in whi"
D12-1067,E06-1011,0,0.641719,"such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prun"
D12-1067,P05-1012,0,0.0682936,"ected edges. Then a directed graph y ⊆ C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Graph-based models define parametrized scoring functions that are trained to discriminate between correct and incorrect parse trees. So called arcfactored or first order models are the most basic variant of such functions: they assess the quality of a tree by scoring each edge in isolation (McDonald et al., 2005b; McDonald et al., 2005a). Formally, arcfactored models are scoring functions of the form X shh,mi (x, w) (1) s (y; x, w) = hh,mi∈y where w is a weight vector and shh,mi (x, w) scores the edge hh, mi with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: X maximize shh,mi y hh,mi∈y (2) subject to y ∈ Y. 2.2 Dependency trees are representations of the syntactic structure of a sentence (Nivre et al., 2007). They determine, for each token of a sentence, the s"
D12-1067,H05-1066,0,0.482353,"Missing"
D12-1067,W06-1616,1,0.90248,"o evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistic"
D12-1067,N10-1117,1,0.820419,"rams. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By us"
D12-1067,W04-2401,0,0.0270605,"Missing"
D12-1067,N12-1054,0,0.0281506,"sed on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of"
D12-1067,D10-1001,0,0.248793,"g can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal incidence vectors, k"
D12-1067,D08-1016,1,0.959552,"(Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all c"
D12-1067,N06-1054,0,0.0303788,"i on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clark"
D12-1067,D07-1096,1,\N,Missing
D12-1101,D07-1101,0,0.0770565,"Missing"
D12-1101,N07-1011,1,0.883627,"Missing"
D12-1101,P11-1055,0,0.0667494,"Missing"
D12-1101,N10-1117,0,0.0611354,"Missing"
D12-1101,P11-1080,1,0.826634,"Missing"
D12-1101,D10-1099,1,0.881719,"Missing"
D14-1113,P12-1092,0,0.164654,"d, d(a, c) ≤ d(a, b) + d(b, c). 1059 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tance not more than the sum of the distances plant– pollen and plant–refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities. Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type’s tokens into discriminated senses, use the clusters to re-label the corpus’ tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and the computational expense"
D14-1113,E14-1048,1,0.59384,"larger context windows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in 1060 downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA). Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks. There is considerably less prior work on learning multiple"
D14-1113,I11-1079,0,\N,Missing
D14-1113,N04-1043,0,\N,Missing
D14-1113,N12-1052,0,\N,Missing
D14-1113,W09-1119,0,\N,Missing
D14-1113,D13-1141,0,\N,Missing
D14-1113,J92-4003,0,\N,Missing
D14-1113,P08-1068,0,\N,Missing
D14-1113,D10-1114,0,\N,Missing
D14-1113,P10-1040,0,\N,Missing
D14-1113,N10-1013,0,\N,Missing
D14-1113,P14-2131,0,\N,Missing
D17-1283,P04-1056,0,0.0644416,"and 2674 not bytes. Additionally, we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by Kalchbrenner et al. (2016). We are the first to use dilated convolutions for sequence labeling. The broad effective input width of the ID-CNN helps aggregate document-level context. Ratinov and Roth (2009) incorporate document context in their greedy model by adding features based on tagged entities within a large, fixed window of tokens. Prior work has also posed a structured model that couples predictions across the whole document (Bunescu and Mooney, 2004; Sutton and McCallum, 2004; Finkel et al., 2005). 6 Experimental Results We describe experiments on two benchmark English named entity recognition datasets. On CoNLL-2003 English NER, our ID-CNN performs on par with a Bi-LSTM not only when used to produce per-token logits for structured inference, but the ID-CNN with greedy decoding also performs on-par with the Bi-LSTM-CRF while running at more than 14 times the speed. We also observe a performance boost in almost all models when broadening the context to incorporate entire documents, achieving an average F1 of 90.65 on CoNLL-2003, out-perfo"
D17-1283,Q16-1026,0,0.434242,"fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x testtime speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds. 1 The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling (Ling et al., 2015; Ma and Hovy, 2016; Chiu and Nichols, 2016; Lample et al., 2016). While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Convol"
D17-1283,Q14-1037,0,0.0450566,"uences, it also tags at more than 4.5 times the speed of the greedy Bi-LSTM, demonstrative of the benefit of our IDCNNs context-aggregating computation that does not depend on the length of the sequence. 6.4 OntoNotes 5.0 English NER We observe similar patterns on OntoNotes as we do on CoNLL. Table 7 lists overall F1 scores of our models compared to those in the existing literature. The greedy Bi-LSTM out-performs the lexModel Bi-LSTM-CRF Bi-LSTM ID-CNN Speed 1× 4.60× 7.96× Table 6: Relative test-time speed of document models (fastest batch size for each model). Model Ratinov and Roth (2009)6 Durrett and Klein (2014) Chiu and Nichols (2016) Bi-LSTM-CRF Bi-LSTM-CRF-Doc Bi-LSTM ID-CNN-CRF (1 block) ID-CNN-Doc (3 blocks) ID-CNN (3 blocks) ID-CNN (1 block) F1 83.45 84.04 86.19 ± 0.25 86.99 ± 0.22 86.81 ± 0.18 83.76 ± 0.10 86.84 ± 0.19 85.76 ± 0.13 85.27 ± 0.24 84.28 ± 0.10 Speed 1× 1.32× 24.44× 1.83× 21.19× 13.21× 26.01× Table 7: F1 score of sentence and document models on OntoNotes. icalized greedy model of Ratinov and Roth (2009), and our ID-CNN out-performs the Bi-LSTM as well as the more complex model of Durrett and Klein (2014) which leverages the parallel coreference annotation available in the OntoNote"
D17-1283,P05-1045,0,0.00712656,"loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by Kalchbrenner et al. (2016). We are the first to use dilated convolutions for sequence labeling. The broad effective input width of the ID-CNN helps aggregate document-level context. Ratinov and Roth (2009) incorporate document context in their greedy model by adding features based on tagged entities within a large, fixed window of tokens. Prior work has also posed a structured model that couples predictions across the whole document (Bunescu and Mooney, 2004; Sutton and McCallum, 2004; Finkel et al., 2005). 6 Experimental Results We describe experiments on two benchmark English named entity recognition datasets. On CoNLL-2003 English NER, our ID-CNN performs on par with a Bi-LSTM not only when used to produce per-token logits for structured inference, but the ID-CNN with greedy decoding also performs on-par with the Bi-LSTM-CRF while running at more than 14 times the speed. We also observe a performance boost in almost all models when broadening the context to incorporate entire documents, achieving an average F1 of 90.65 on CoNLL-2003, out-performing the sentence-level model while still decodi"
D17-1283,W03-0426,0,0.263649,"n-structured graphical model, or approximates this search with a beam (Collobert et al., 2011; Weiss et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). These outperform similar systems that use the same features, but independent local predictions. On the other hand, the greedy sequential prediction (Daum´e III et al., 2009) approach of Ratinov and Roth (2009), which employs lexicalized features, gazetteers, and word clusters, outperforms CRFs with similar features. LSTMs (Hochreiter and Schmidhuber, 1997) were used for NER as early as the CoNLL shared task in 2003 (Hammerton, 2003; Tjong Kim Sang and De Meulder, 2003). More recently, a wide variety of neural network architectures for NER have been proposed. Collobert et al. (2011) employ a one-layer CNN with pre-trained word embeddings, capitalization and lexicon features, and CRF-based prediction. Huang et al. (2015) achieved state-of-the-art accuracy on partof-speech, chunking and NER using a Bi-LSTMCRF. Lample et al. (2016) proposed two models which incorporated Bi-LSTM-composed character embeddings alongside words: a Bi-LSTMCRF, and a greedy stack LSTM which uses a simple shift-reduce grammar to compose words into"
D17-1283,P82-1020,0,0.727408,"Missing"
D17-1283,N06-2015,0,0.0184881,"models require more computation time 2) more complicated models tend to over-fit on this relatively small dataset and 3) most accurate deep CNN architectures repeatedly up-sample and down-sample the inputs. We do not compare to stacked LSTMs for similar reasons — a single LSTM is already slower than a 4-layer CNN. Since our task is sequence labeling, we desire a model that maintains the token-level resolution of the input, making dilated convolutions an elegant solution. 6.3 We evaluate using labeled data from the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003) and OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2006). Following previous work, we use the same OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task (Pradhan et al., 2012). For both datasets, we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance (Ratinov and Roth, 2009). As in previous work we evaluate the performance of our models using segment-level micro-averaged F1 score. Hyperparameters that resulted in the best performance on the validation set were selected via grid search. A more detailed description of the data, evaluatio"
D17-1283,P16-1101,0,0.265882,"ism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x testtime speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds. 1 The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling (Ling et al., 2015; Ma and Hovy, 2016; Chiu and Nichols, 2016; Lample et al., 2016). While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and"
D17-1283,P14-1062,0,0.131932,"hey fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Convolutional neural networks (CNNs) provide exactly this property (Kim, 2014; Kalchbrenner et al., 2014). Rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. Their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. This provides, for example, audio generation models that can be trained in parallel (van den Oord et al., 2016). Introduction In order to democratize large-scale NLP and information extraction while minimizing our environmental footprint, we require fast, resourceDespite the clear computational advantages"
D17-1283,D14-1181,0,0.0660297,"accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Convolutional neural networks (CNNs) provide exactly this property (Kim, 2014; Kalchbrenner et al., 2014). Rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once. Their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware. This provides, for example, audio generation models that can be trained in parallel (van den Oord et al., 2016). Introduction In order to democratize large-scale NLP and information extraction while minimizing our environmental footprint, we require fast, resourceDespite the cl"
D17-1283,N16-1030,0,0.751492,"s to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x testtime speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds. 1 The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling (Ling et al., 2015; Ma and Hovy, 2016; Chiu and Nichols, 2016; Lample et al., 2016). While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited. Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input. Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models. Convolutional neural network"
D17-1283,D15-1180,0,0.0132472,". Though our models would also likely benefit from additional features such as character representations and lexicons, we focus on simpler models which use word-embeddings alone, leaving more elaborate input representations to future work. In these NER approaches, CNNs were used for low-level feature extraction that feeds into alternative architectures. Overall, end-to-end CNNs have mainly been used in NLP for sentence classification, where the output representation is lower resolution than that of the input Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words. While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration. Our work draws on the use of dilated convolutions for image segmentation in the computer vision community (Yu and Koltun, 2016; Chen et al., 2015). Similar to our block, Yu and Koltun (2016) employ a context-module of stacked dilated convolutions of exponentially increasing dilation width. Dilated convolutions were recently applied to the task of speech generation (van den Oord et al., 2016), and concurrent w"
D17-1283,W14-1609,1,0.28147,"ormance on part-of-speech tagging and CoNLL NER without lexicons. Chiu and Nichols (2016) evaluate a similar network but propose a novel method for encoding lexicon matches, presenting results on CoNLL and OntoNotes NER. Yang et al. (2016) use GRU-CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages. In general, distributed representations for text can provide useful generalization capabilities for NER systems, since they can leverage unsupervised pre-training of distributed word representations (Turian et al., 2010; Collobert et al., 2011; Passos et al., 2014). Though our models would also likely benefit from additional features such as character representations and lexicons, we focus on simpler models which use word-embeddings alone, leaving more elaborate input representations to future work. In these NER approaches, CNNs were used for low-level feature extraction that feeds into alternative architectures. Overall, end-to-end CNNs have mainly been used in NLP for sentence classification, where the output representation is lower resolution than that of the input Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015)."
D17-1283,W12-4501,0,0.0185449,"y up-sample and down-sample the inputs. We do not compare to stacked LSTMs for similar reasons — a single LSTM is already slower than a 4-layer CNN. Since our task is sequence labeling, we desire a model that maintains the token-level resolution of the input, making dilated convolutions an elegant solution. 6.3 We evaluate using labeled data from the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003) and OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2006). Following previous work, we use the same OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task (Pradhan et al., 2012). For both datasets, we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance (Ratinov and Roth, 2009). As in previous work we evaluate the performance of our models using segment-level micro-averaged F1 score. Hyperparameters that resulted in the best performance on the validation set were selected via grid search. A more detailed description of the data, evaluation, optimization and data pre-processing can be found in the Appendix. Baselines 6.3.1 CoNLL-2003 English NER Sentence-level prediction Table 1 lists F1 scores of models pre"
D17-1283,W09-1119,0,0.935325,"effectiveness for NLP, including for RNNs. We encourage its further application. 5 Related work The state-of-the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain-structured graphical model, or approximates this search with a beam (Collobert et al., 2011; Weiss et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). These outperform similar systems that use the same features, but independent local predictions. On the other hand, the greedy sequential prediction (Daum´e III et al., 2009) approach of Ratinov and Roth (2009), which employs lexicalized features, gazetteers, and word clusters, outperforms CRFs with similar features. LSTMs (Hochreiter and Schmidhuber, 1997) were used for NER as early as the CoNLL shared task in 2003 (Hammerton, 2003; Tjong Kim Sang and De Meulder, 2003). More recently, a wide variety of neural network architectures for NER have been proposed. Collobert et al. (2011) employ a one-layer CNN with pre-trained word embeddings, capitalization and lexicon features, and CRF-based prediction. Huang et al. (2015) achieved state-of-the-art accuracy on partof-speech, chunking and NER using a Bi"
D17-1283,W03-0419,0,0.12514,"Missing"
D17-1283,D15-1174,0,0.0249583,"011; Passos et al., 2014). Though our models would also likely benefit from additional features such as character representations and lexicons, we focus on simpler models which use word-embeddings alone, leaving more elaborate input representations to future work. In these NER approaches, CNNs were used for low-level feature extraction that feeds into alternative architectures. Overall, end-to-end CNNs have mainly been used in NLP for sentence classification, where the output representation is lower resolution than that of the input Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words. While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration. Our work draws on the use of dilated convolutions for image segmentation in the computer vision community (Yu and Koltun, 2016; Chen et al., 2015). Similar to our block, Yu and Koltun (2016) employ a context-module of stacked dilated convolutions of exponentially increasing dilation width. Dilated convolutions were recently applied to the task of speech generation (van den Oord et al., 2016"
D17-1283,P10-1040,0,0.00414153,"a Bi-LSTM-CRF, achieving state-ofthe-art performance on part-of-speech tagging and CoNLL NER without lexicons. Chiu and Nichols (2016) evaluate a similar network but propose a novel method for encoding lexicon matches, presenting results on CoNLL and OntoNotes NER. Yang et al. (2016) use GRU-CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages. In general, distributed representations for text can provide useful generalization capabilities for NER systems, since they can leverage unsupervised pre-training of distributed word representations (Turian et al., 2010; Collobert et al., 2011; Passos et al., 2014). Though our models would also likely benefit from additional features such as character representations and lexicons, we focus on simpler models which use word-embeddings alone, leaving more elaborate input representations to future work. In these NER approaches, CNNs were used for low-level feature extraction that feeds into alternative architectures. Overall, end-to-end CNNs have mainly been used in NLP for sentence classification, where the output representation is lower resolution than that of the input Kim (2014); Kalchbrenner et al. (2014);"
D17-1283,P15-1032,0,0.0143272,"at test time. Ma et al. (2017) present dropout with expectationlinear regularization, which explicitly regularizes these two predictors to behave similarly. All of our best reported results include such regularization. This is the first investigation of the technique’s effectiveness for NLP, including for RNNs. We encourage its further application. 5 Related work The state-of-the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain-structured graphical model, or approximates this search with a beam (Collobert et al., 2011; Weiss et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). These outperform similar systems that use the same features, but independent local predictions. On the other hand, the greedy sequential prediction (Daum´e III et al., 2009) approach of Ratinov and Roth (2009), which employs lexicalized features, gazetteers, and word clusters, outperforms CRFs with similar features. LSTMs (Hochreiter and Schmidhuber, 1997) were used for NER as early as the CoNLL shared task in 2003 (Hammerton, 2003; Tjong Kim Sang and De Meulder, 2003). More recently, a wide variety of neural network architectu"
D17-1283,D15-1161,0,\N,Missing
D18-1306,N06-2015,0,0.0334138,"eins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results. 1 Introduction Identifying entities in text is a vital component in language understanding, facilitating knowledge base construction (Riedel et al., 2013), question answering (Bordes et al., 2015), and search. Identifying these entities are particularly important in biomedical data. While large scale Named Entity Recognition (NER) datasets exist in news and web data (Tjong Kim Sang and De Meulder, 2003; Hovy et al., 2006), biomedical NER datasets are typically smaller and contain only one or two types per dataset. Ultimately, we would like to identify all entity types present across the union of the label sets during inference while leveraging all the available annotations to train our models. While one may train a single model across the union of all the datasets available, this training 2 Model Our models build on state-of-the-art NER systems (Lample et al., 2016) based on bi-directional Long Short Term Memory (BiLSTM) feature extractors fed into a conditional random field (CRF). The data consists of input s"
D18-1306,N16-1030,0,0.655185,"rtant in biomedical data. While large scale Named Entity Recognition (NER) datasets exist in news and web data (Tjong Kim Sang and De Meulder, 2003; Hovy et al., 2006), biomedical NER datasets are typically smaller and contain only one or two types per dataset. Ultimately, we would like to identify all entity types present across the union of the label sets during inference while leveraging all the available annotations to train our models. While one may train a single model across the union of all the datasets available, this training 2 Model Our models build on state-of-the-art NER systems (Lample et al., 2016) based on bi-directional Long Short Term Memory (BiLSTM) feature extractors fed into a conditional random field (CRF). The data consists of input sequence of tokens x = {x1 , . . . , xT } where each token is a sequence of characters xt = {c1 , . . . , cKt }. The output consists of labels for each token in the sequence y = {y1 , . . . , yT }. Labeling is done using the BILOU tagging scheme, following previous observations that it outperforms the BIO tagging scheme (Ratinov and Roth, 2009). We have D such datasets of input tokens and output labels. 2824 Proceedings of the 2018 Conference on Empi"
D18-1306,C08-1113,0,0.373165,"hat since the normalization term is the same here as for a standard CRF, we can still use the same dynamic programming algorithm as for a regular CRF to compute this log Z. Now, in order to compute the first term, we note that it is similar to the computation required to compute log Z – whereas log Z is obtained by summing over all possible output sequences, this term is obtained by summing over all possible output sequences which have indices in E fixed to the correct label and indices in N taking values from ∪j6=i Sj . Thus, this can be computed using the same dynamic programming algorithm (Tsuboi et al., 2008), and the implementation of training this model is compatible with modern automatic differentiation libraries. 3 Experimental Results We perform experiments on two benchmark Biocreative datasets as well as the recently introduced MedMentions data (Murty et al., 2018). Our experiments consider three types of models. The single CRF model naively concatenates all training datasets together and assumes complete labeling, multi CRF has a single Bi-LSTM feature encoder with a separate CRF for each dataset (Section 2.3.1), and EM CRF has a single feature encoder and a single CRF trained with EM margi"
D18-1306,P18-1010,1,0.897568,"to compute log Z – whereas log Z is obtained by summing over all possible output sequences, this term is obtained by summing over all possible output sequences which have indices in E fixed to the correct label and indices in N taking values from ∪j6=i Sj . Thus, this can be computed using the same dynamic programming algorithm (Tsuboi et al., 2008), and the implementation of training this model is compatible with modern automatic differentiation libraries. 3 Experimental Results We perform experiments on two benchmark Biocreative datasets as well as the recently introduced MedMentions data (Murty et al., 2018). Our experiments consider three types of models. The single CRF model naively concatenates all training datasets together and assumes complete labeling, multi CRF has a single Bi-LSTM feature encoder with a separate CRF for each dataset (Section 2.3.1), and EM CRF has a single feature encoder and a single CRF trained with EM marginalization (Section 2.3.2). For full dataset statistics and specific implementation details see supplementary material. 3.1 Biocreative V / VI Biocreative V Chemical Disease Relation (CDR): consists of 1,500 titles and abstracts from PubMed, human annotated with chem"
D18-1306,N18-1080,1,0.875248,"x) = s(x, y) − logsumexp s(x, y0 ) (1) y0 The log normalization term here is: P logsumexp s(x, y0 ) = log y0 exp s(x, y0 ) y0 where the sum goes over all possible labelings y 0 of the sequence and is computed efficiently using dynamic programming (Lafferty et al., 2001). Feature Encoder BiLSTM Our model takes a sequence of tokens from a single abstract as input. Tokens are generated using byte-pair encodings (BPE) (Gage, 1994; Sennrich et al., 2016), which have recently been shown to be effective for tokenization of biological texts by addressing the issue of rare or out-of-vocabulary tokens (Verga et al., 2018). BPE starts from white space tokenization and breaks down the tokens further. Because all of the evaluations are on the span level rather than the token level, the use of BPE does not impact any numerical performance. Each token t produced from BPE is mapped to a d dimensional word embedding w. Character level features have been shown to improve NER accuracy (Lafferty et al., 2001; Lample et al., 2016; Passos et al., 2014). We encode characters in a word using another BiLSTM, similar to Lample et al. (2016), and obtain a character based embedding for every word by concatenating the last hidde"
D18-1306,W14-1609,1,0.898726,"994; Sennrich et al., 2016), which have recently been shown to be effective for tokenization of biological texts by addressing the issue of rare or out-of-vocabulary tokens (Verga et al., 2018). BPE starts from white space tokenization and breaks down the tokens further. Because all of the evaluations are on the span level rather than the token level, the use of BPE does not impact any numerical performance. Each token t produced from BPE is mapped to a d dimensional word embedding w. Character level features have been shown to improve NER accuracy (Lafferty et al., 2001; Lample et al., 2016; Passos et al., 2014). We encode characters in a word using another BiLSTM, similar to Lample et al. (2016), and obtain a character based embedding for every word by concatenating the last hidden state of the forward and backward character LSTM. We concatenate this character based embedding with the d-dimensional word embedding and input it to the word-level BiLSTM. This feature representation is then projected to the label dimension L using a linear layer, giving a matrix of scores [fil ] where fil is the score for predicting label l ∈ [L] for token i ∈ [T ]. 2.2 on the output representations from the BiLSTM mode"
D18-1306,P17-1161,0,0.0222123,". In Table 3, we see that the single CRF model performs very poorly in this extreme setting due to the large amount of missing annotations. The multi CRF and EM CRF both perform well and come close to the performance of a single CRF trained on the full data, which is approximately twice as much annotated data. 4 Related Work Until recently, feature engineered machine learning models were the highest performing approaches to NER (Ratinov and Roth, 2009; Passos et al., 2014). More recently, neural network based approaches have become state-of-the-art (Lample et al., 2016; Strubell et al., 2017; Peters et al., 2017). In BioNLP, many highest performing systems still use engineered features fed into a CRF (Wei et al., 2015; Leaman et al., 2015; Leaman and Lu, 2016). In addition to the two datasets we explored in this work, there are several other popular bio NER datasets for chemicals (Krallinger et al., 2015), species (Wang et al., 2010), diseases 2827 (Do˘gan et al., 2014), and genes (Tanabe et al., 2005). In concurrent work, Wang et al. (2018) train a model very similar to our multi-CRF model on multiple biological NER datasets with non-fully overlapping labels. Additionally, they experiment with differ"
D18-1306,W09-1119,0,0.815874,"e union of all the datasets available, this training 2 Model Our models build on state-of-the-art NER systems (Lample et al., 2016) based on bi-directional Long Short Term Memory (BiLSTM) feature extractors fed into a conditional random field (CRF). The data consists of input sequence of tokens x = {x1 , . . . , xT } where each token is a sequence of characters xt = {c1 , . . . , cKt }. The output consists of labels for each token in the sequence y = {y1 , . . . , yT }. Labeling is done using the BILOU tagging scheme, following previous observations that it outperforms the BIO tagging scheme (Ratinov and Roth, 2009). We have D such datasets of input tokens and output labels. 2824 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2824–2829 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Training example where one label set contains Chemical/Protein and the other contains Chemical/Disease. Here Chemical and Disease annotations are given and Outside is ambiguous. Tokens labeled as Outside could potentially be either Outside or Protein (top). The shaded labels are the gold labels. The EM CRF marginalizes"
D18-1306,N13-1008,1,0.625099,"ning to insist on labels that are present in the data, while filling in “missing labels”. This allows us to leverage all the available data within a single model. In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results. 1 Introduction Identifying entities in text is a vital component in language understanding, facilitating knowledge base construction (Riedel et al., 2013), question answering (Bordes et al., 2015), and search. Identifying these entities are particularly important in biomedical data. While large scale Named Entity Recognition (NER) datasets exist in news and web data (Tjong Kim Sang and De Meulder, 2003; Hovy et al., 2006), biomedical NER datasets are typically smaller and contain only one or two types per dataset. Ultimately, we would like to identify all entity types present across the union of the label sets during inference while leveraging all the available annotations to train our models. While one may train a single model across the union"
D18-1306,P16-1162,0,0.0102549,"de could potentially be either Outside or Protein (top). The shaded labels are the gold labels. The EM CRF marginalizes over all potential sequences (bot). 2.1 log P (y|x) = s(x, y) − logsumexp s(x, y0 ) (1) y0 The log normalization term here is: P logsumexp s(x, y0 ) = log y0 exp s(x, y0 ) y0 where the sum goes over all possible labelings y 0 of the sequence and is computed efficiently using dynamic programming (Lafferty et al., 2001). Feature Encoder BiLSTM Our model takes a sequence of tokens from a single abstract as input. Tokens are generated using byte-pair encodings (BPE) (Gage, 1994; Sennrich et al., 2016), which have recently been shown to be effective for tokenization of biological texts by addressing the issue of rare or out-of-vocabulary tokens (Verga et al., 2018). BPE starts from white space tokenization and breaks down the tokens further. Because all of the evaluations are on the span level rather than the token level, the use of BPE does not impact any numerical performance. Each token t produced from BPE is mapped to a d dimensional word embedding w. Character level features have been shown to improve NER accuracy (Lafferty et al., 2001; Lample et al., 2016; Passos et al., 2014). We en"
D18-1306,D17-1283,1,0.862208,"tails in supplementary). In Table 3, we see that the single CRF model performs very poorly in this extreme setting due to the large amount of missing annotations. The multi CRF and EM CRF both perform well and come close to the performance of a single CRF trained on the full data, which is approximately twice as much annotated data. 4 Related Work Until recently, feature engineered machine learning models were the highest performing approaches to NER (Ratinov and Roth, 2009; Passos et al., 2014). More recently, neural network based approaches have become state-of-the-art (Lample et al., 2016; Strubell et al., 2017; Peters et al., 2017). In BioNLP, many highest performing systems still use engineered features fed into a CRF (Wei et al., 2015; Leaman et al., 2015; Leaman and Lu, 2016). In addition to the two datasets we explored in this work, there are several other popular bio NER datasets for chemicals (Krallinger et al., 2015), species (Wang et al., 2010), diseases 2827 (Do˘gan et al., 2014), and genes (Tanabe et al., 2005). In concurrent work, Wang et al. (2018) train a model very similar to our multi-CRF model on multiple biological NER datasets with non-fully overlapping labels. Additionally, they"
D18-1306,W03-0419,0,0.507186,"Missing"
D18-1548,E17-1005,0,0.0143286,"orates the entire parse. Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and N"
D18-1548,D16-1211,0,0.0203332,"ic method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL. We leave exploration of more advanced scheduled sampling techniques to future work. 4 Experimental results We present results on the CoNLL-2005 shared task (Carreras and M`arquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fi"
D18-1548,P13-2074,0,0.0208766,"2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text. 1 Introduction Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipelin"
D18-1548,D15-1112,0,0.403405,"Missing"
D18-1548,C12-1059,0,0.0237121,"d Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL. We leave exploration of more advanced scheduled sampling techniques to future work. 4 Experimental results We present results on the CoNLL-2005 shared task (Carreras and M`arquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained E"
D18-1548,D14-1159,0,0.0246151,"n in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text. 1 Introduction Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input,"
D18-1548,D17-1206,0,0.03627,"itov (2017) encode syntax using a graph CNN over a predicted syntax tree, out-performing models without syntax on CoNLL-2009. These works are limited to incorporating partial dependency paths between tokens whereas our technique incorporates the entire parse. Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learnin"
D18-1548,E17-2026,0,0.018335,"ereas our technique incorporates the entire parse. Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Pal"
D18-1548,P18-2058,0,0.150037,"o whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al. (2017) indicate that many of the errors made by a syn"
D18-1548,W05-0620,0,0.125057,"Missing"
D18-1548,P17-1044,0,0.14727,"tence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al. (2017) indicate t"
D18-1548,D08-1008,0,0.25022,"Missing"
D18-1548,P11-2121,0,0.026917,"øgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL. We leave exploration of more advanced scheduled sampling techniques to future work. 4 Experimental results We present results on the CoNLL-2005 shared task (Carreras and M`arquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al"
D18-1548,Q16-1023,0,0.0353317,"nd Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al. (2017) indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained inference using a relatively low-accuracy predicted parse can provide small improvements in SRL accuracy, providing a gold-quality parse leads to substantial gains. Marcheggiani and Titov (2017) incorporate syntax from a high-quality parser (Kiperwasser and Goldberg, 2016) using graph convolutional neural networks (Kipf and Welling, 2017), but like He et al. (2017) they attain only small increases over a model with no syntactic parse, and even perform worse than a syntax-free model on out-of-domain data. These works suggest that though syntax has the potential to improve neural network SRL models, we have not 5027 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5027–5038 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Our implementation in TensorFlow (Abadi et al.,"
D18-1548,D17-1018,0,0.0286747,"ully employed neural networks by embedding lexicalized features and providing them as factors in the model of T¨ackstr¨om et al. (2015). More recent neural models are syntax-free. Zhou and Xu (2015), Marcheggiani et al. (2017) and He et al. (2017) all use variants of deep LSTMs with constrained decoding, while Tan et al. (2018) apply self-attention to obtain state-ofthe-art SRL with gold predicates. Like this work, He et al. (2017) present end-to-end experiments, predicting predicates using an LSTM, and He et al. 5031 (2018) jointly predict SRL spans and predicates in a model based on that of Lee et al. (2017), obtaining state-of-the-art predicted predicate SRL. Concurrent to this work, Peters et al. (2018) and He et al. (2018) report significant gains on PropBank SRL by training a wide LSTM language model and using a task-specific transformation of its hidden representations (ELMo) as a deep, and computationally expensive, alternative to typical word embeddings. We find that LISA obtains further accuracy increases when provided with ELMo word representations, especially on out-of-domain data. Some work has incorporated syntax into neural models for SRL. Roth and Lapata (2016) incorporate syntax by"
D18-1548,C10-1081,0,0.0444971,"rovement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text. 1 Introduction Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extr"
D18-1548,Q18-1005,0,0.0404249,"out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989)"
D18-1548,P17-1186,0,0.0272308,"x using a graph CNN over a predicted syntax tree, out-performing models without syntax on CoNLL-2009. These works are limited to incorporating partial dependency paths between tokens whereas our technique incorporates the entire parse. Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e"
D18-1548,D14-1162,0,0.0820792,"and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016). Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL. We leave exploration of more advanced scheduled sampling techniques to future work. 4 Experimental results We present results on the CoNLL-2005 shared task (Carreras and M`arquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fine-tuned task-specific parameters (Peters et al., 2018) in order to best compare to prior work. Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search, and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set. We convert constituencies to dependencies using the Stanford head rules v3.5 (de Marneffe and Manning, 2008). A detailed description of hyperparameter settings and data pre-processing can be found in Appendix A. We compare our LISA mo"
D18-1548,N18-1202,0,0.780226,"coder, recently shown to achieve state-of-the-art performance on SRL (Tan et al., 2018), and which provides a natural mechanism for incorporating syntax, as described in §2.2. Our implementation replicates Vaswani et al. (2017). The input to the network is a sequence X of T token representations xt . In the standard setting these token representations are initialized to pretrained word embeddings, but we also experiment with supplying pre-trained ELMo representations combined with task-specific learned parameters, which have been shown to substantially improve performance of other SRL models (Peters et al., 2018). For experiments with gold predicates, we concatenate a predicate indicator embedding pt following previous work (He et al., 2017). We project2 these input embeddings to a representation that is the same size as the output of the self-attention layers. We then add a positional encoding vector computed as a deterministic sinusoidal function of t, since the self-attention has no innate notion of token position. We feed this token representation as input to a series of J residual multi-head self-attention layers with feed-forward connections. Denoting the jth self-attention layer as T (j) (·), t"
D18-1548,P05-1072,0,0.361466,"P (yfrole t |PG , V G , X ) T t=1 f =1 + log P (ytprp |X ) |X) + 1 log P (head(t) + dep |PG , X ) 2 log P (yt i (7) where 1 and 2 are penalties on the syntactic attention loss. We train the model using Nadam (Dozat, 2016) SGD combined with the learning rate schedule in Vaswani et al. (2017). In addition to MTL, we regularize our model using dropout (Srivastava et al., 2014). We use gradient clipping to avoid exploding gradients (Bengio et al., 1994; Pascanu et al., 2013). Additional details on optimization and hyperparameters are included in Appendix A. 3 Related work Early approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008) focused on developing rich sets of linguistic features as input to a linear model, often combined with complex constrained inference e.g. with an ILP (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) showed that constraints could be enforced more efficiently using a clever dynamic program for exact inference. Sutton and McCallum (2005) modeled syntactic parsing and SRL jointly, and Lewis et al. (2015) jointly modeled SRL and CCG parsing. Collobert et al. (2011) were among the first to use a neural network model for"
D18-1548,J08-2005,0,0.689377,".0 F1 on news and more than 2.0 F1 on out-of-domain text. 1 Introduction Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates t"
D18-1548,K17-1041,0,0.358006,"ation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al."
D18-1548,D17-1159,0,0.374663,"ms (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al. (2017) indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained inference using a relatively low-accuracy predicted parse can provide small improvements in SRL accuracy, providing a gold-quality parse leads to substantial gains. Marcheggiani and Titov (2017) incorporate syntax from a high-quality parser (Kiperwasser and"
D18-1548,P16-1113,0,0.475011,"ed an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it. He et al. (2017) indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained inference using a relatively low-accuracy predicted parse can provide small improvements in SRL accuracy, providing a gold-quality parse leads to substantial gains. Marcheggiani and Titov (2017) incorporate sy"
D18-1548,P16-2038,0,0.0505579,"ency paths between tokens whereas our technique incorporates the entire parse. Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017). Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search (Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-b"
D18-1548,W08-1301,0,0.0165384,"Missing"
D18-1548,J05-1004,0,0.546859,"Missing"
D18-1548,W05-0636,1,0.730999,"exploding gradients (Bengio et al., 1994; Pascanu et al., 2013). Additional details on optimization and hyperparameters are included in Appendix A. 3 Related work Early approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008) focused on developing rich sets of linguistic features as input to a linear model, often combined with complex constrained inference e.g. with an ILP (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) showed that constraints could be enforced more efficiently using a clever dynamic program for exact inference. Sutton and McCallum (2005) modeled syntactic parsing and SRL jointly, and Lewis et al. (2015) jointly modeled SRL and CCG parsing. Collobert et al. (2011) were among the first to use a neural network model for SRL, a CNN over word embeddings which failed to out-perform non-neural models. FitzGerald et al. (2015) successfully employed neural networks by embedding lexicalized features and providing them as factors in the model of T¨ackstr¨om et al. (2015). More recent neural models are syntax-free. Zhou and Xu (2015), Marcheggiani et al. (2017) and He et al. (2017) all use variants of deep LSTMs with constrained decoding"
D18-1548,Q15-1003,0,0.0630819,"Missing"
D18-1548,J08-2002,0,0.0774049,"odel with predicted predicates on both corpora. We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fine-tuned task-specific parameters (Peters et al., 2018) in order to best compare to prior work. Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search, and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set. We convert constituencies to dependencies using the Stanford head rules v3.5 (de Marneffe and Manning, 2008). A detailed description of hyperparameter settings and data pre-processing can be found in Appendix A. We compare our LISA models to four strong baselines: For experiments using predicted predicates, we compare to He et al. (2018) and the ensemble model (PoE) from He et al. (2017), as well as a version of our own self-attention model which does not incorporate syntactic information (SA). To compare to more prior work, we present additional results on CoNLL-2005 with models given gold predicates at test time. In these experiments we also compare to Tan et al. (2018), the previous state-of-the"
D18-1548,N03-1033,0,0.249652,"Missing"
D18-1548,P15-2115,0,0.0395339,"2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text. 1 Introduction Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer ling"
D18-1548,K17-3001,0,0.0134618,"as well as a version of our own self-attention model which does not incorporate syntactic information (SA). To compare to more prior work, we present additional results on CoNLL-2005 with models given gold predicates at test time. In these experiments we also compare to Tan et al. (2018), the previous state-of-the art SRL model using gold predicates and standard embeddings. We demonstrate that our models benefit from injecting state-of-the-art predicted parses at test time (+D&M) by fixing the attention to parses predicted by Dozat and Manning (2017), the winner of the 2017 CoNLL shared task (Zeman et al., 2017) which we re-train using ELMo embeddings. In all cases, using these parses at test time improves performance. We also evaluate our model using the gold syntactic parse at test time (+Gold), to provide an upper bound for the benefit that syntax could have for SRL using LISA. These experiments show that despite LISA’s strong performance, there remains substantial room for improvement. In §4.3 we perform further analysis comparing SRL models using gold and predicted parses. 5032 GloVe He et al. (2017) PoE He et al. (2018) SA LISA +D&M +Gold P 81.8 81.3 83.52 83.1 84.59 87.91 Dev R 81.2 81.9 81.28"
D18-1548,P16-1147,1,0.908909,"Missing"
D18-1548,P15-1109,0,0.499234,"igh-level representation of meaning from a sentence, labeling e.g. who did what to whom. Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013). Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax. An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models. Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather t"
D18-1548,J93-2004,0,\N,Missing
D18-2018,L18-1433,0,0.0319321,", 2016, inter alia). However, most work in open domain settings (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018) only uses simple retrievers (such as TF-IDF based ones). As a result, there is a notable decrease in the performance of the QA system. One roadblock for training a sophisticated retriever is the lack of available training data which annotates the relevance of a retrieved context with respect to the question. We believe our annotated retrieval data can be used to train a better ranker/retriever without obliging annotators to explicitly connect the supporting passages (Jansen et al., 2018). The underlying retriever in our interface is a simple Elasticsearch, similar to the one used by Clark et al. (2018). The interface is populated by default with the top ranked sentences that are retrieved with the given question as the input query. However, we noticed that results thus retrieved were often irrelevant to answering the question. To address this, our labeling interface also allows annotators to input their own custom queries. We found that reformulating the initial query significantly improved the quality of the retrieved context (results). We encouraged the annotators to mark t"
D18-2018,W18-2607,1,0.850362,"McCallum College of Information and Computer Sciences University of Massachusetts, Amherst MA Maria Chang, Achille Fokoue, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, Michael Witbrock IBM Research, Yorktown Heights NY Abstract very core of the paper, since the main contribution is a dataset that contains complex questions. In this work, in order to overcome some of the limitations of Clark et al. (2018) described above, we present a detailed annotation interface for the ARC dataset that allows a distributed set of annotators to label the knowledge and reasoning types (Boratko et al., 2018). Following an annotation round involving over ten people at two institutions, we measure and report statistics such as inter-rater agreement, and the distribution of knowledge and reasoning type labels in the dataset. Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it"
D19-1161,N19-1116,1,0.848341,"lin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refer the reader to Drozdov et al. (2019). For this work, it is key to understand two capabilities that DIORA provides: each span in a sentence is represented as a vector and DIORA induces a maximally likely binary tree for the sentence. We can directly label the constituents of a sentence by clustering the learned span vectors from DIORA and assigning a label to each cluster. DIORA’s autoencoder objective incentivizes the model to learn representations that compress the sentence well in order to reconstruct the input leading to the discovery of syntactic structure. To encourage phrase representations to be easily clusterable into a"
D19-1161,D15-1075,0,0.101709,"Missing"
D19-1161,N19-1423,0,0.162955,"he previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refer the reader to Drozdov et al. (2"
D19-1161,P06-1111,0,0.0477938,"eled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tokens because previous latent parsers do not scale to longer sentences (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Unlike previous work, we achieve strong results in unlabeled constituency parsing using a single model for both bracketing and labeling. Our approach relies on clustering span representations, which are fixed-length continuous vectors learned end-to-end using DIORA and do not require external resources such as part-of-speech tags. Furthermore, we enhance the DIORA architecture with latent codes: the model learns a distribution over these codes that loosely aligns with the groundtruth assignment of phrase types and, more importantl"
D19-1161,N19-1419,0,0.0712323,"Missing"
D19-1161,W18-5452,0,0.026974,"Missing"
D19-1161,N19-1114,0,0.0756686,"Missing"
D19-1161,P18-1249,0,0.0998597,"Missing"
D19-1161,P02-1017,0,0.496704,"ibed in (Drozdov et al., 2019). We are interested in clustering the learned vectors a(i, j) such that each span may be mapped to a phrase type. To enhance this clustering based approach, we augment the DIORA architecture with latent codes, shown in the right half of the figure. Introduction The deep inside-outside recursive autoencoder (Drozdov et al., 2019, DIORA) is part of a recent trend in fully unsupervised neural constituency parsers (Shen et al., 2018; Williams et al., 2018a; Htut et al., 2018; Shen et al., 2019; Kim et al., 2019). However, these works and nearly all previous research (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013) have focused on unlabeled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences long"
D19-1161,N18-1202,1,0.843099,"relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refe"
D19-1161,D18-1179,0,0.123893,"relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On 1507 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1507–1512, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people). 2 DIORA: Deep Inside-Outside Recursive Autoencoders DIORA is a recursive autoencoder that learns to reconstruct an input sentence. A fundamental step in the reconstruction is to build a chart using the inside-outside algorithm (Baker, 1979), which represents a soft weighting over all possible binary trees of the input sentence. For all the model details, we refe"
D19-1161,P11-1108,0,0.0567434,"Missing"
D19-1161,D13-1204,0,0.0508066,"Missing"
D19-1161,P19-1452,0,0.0161425,"and BERT do not induce structure whatsoever and depend on DIORA for the Induced evaluation. ELMoCI uses only the context-insensitive character embeddings produced by ELMo. pervised labeled parsing. When a reference parse is provided, it is only necessary to derive ad-hoc phrase vectors using the contextualized token vectors from these models. Peters et al. (2018b) describe an effective way to do so for ELMo, which involves concatenating the token vectors at the beginning and end of the phrase.4 For BERT, it is critical to look at all layers as lower layers tend to be more syntactic in nature (Tenney et al., 2019). For both models, we report the max F1 and mean F1, and for the Induced evaluation we use the parses extracted from DIORA. 4.3 WSJ Unsupervised constituency parsing has often been evaluated on different splits of the WSJ. For labeled constituency parsing, models that produce 4 We tried many combinations (4 variants for ELMo and nearly 200 for BERT). They are described in Appendix A.2. 1509 4.4 Model Ablations As an alternative to clustering the constituent vectors with K-means, one can treat the codebook affinity scores, (C > W x), as a soft assignment over the clusters represented by each co"
D19-1161,Q18-1019,1,0.893278,"Missing"
D19-1161,N18-1101,0,0.0273822,"Missing"
D19-1161,C08-1091,0,0.0760276,"s on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tokens because previous latent parsers do not scale to longer sentences (Haghighi and Klein, 2006; Borensztajn and Zuidema, 2007; Reichart and Rappoport, 2008). Unlike previous work, we achieve strong results in unlabeled constituency parsing using a single model for both bracketing and labeling. Our approach relies on clustering span representations, which are fixed-length continuous vectors learned end-to-end using DIORA and do not require external resources such as part-of-speech tags. Furthermore, we enhance the DIORA architecture with latent codes: the model learns a distribution over these codes that loosely aligns with the groundtruth assignment of phrase types and, more importantly, improves the quality of the clusters. Our code-enhanced DIO"
D19-1161,D10-1001,0,0.0405301,"ntence where the leaves of the tree are the words in the sentence. The tree is not labeled. This may be derived from the ground truth parse or induced using DIORA. When induced, we extract a binary tree by running the CKY algorithm2 over DIORA’s learned compatibility scores. 1 More details about the equation C > (CW x) are discussed in Appendix A.3. It’s worth noting that can be an arbitrary function, in this work we use the identity function. 2 The CKY algorithm is an efficient dynamic programming approach for recognizing constituency trees using exact inference (Kasami, 1966; Younger, 1967; Rush et al., 2010). 1508 – WSJ (Test) – Gold Induced Model F1µ F1max F1µ F1max – WSJ-10 – Model Upper Bound Majority (NP) 76.3 30.6 76.3 30.6 59.7 24.5 59.7 24.5 Upper Bound 86.0 Majority (NP) 32.0 86.0 32.0 64.6 25.2 64.6 25.2 ELMo ELMoCI BERT 58.5 53.4 41.8 59.4 56.3 42.2 43.5 38.5 38.1 48.2 40.2 38.3 ELMo ELMoCI BERT 67.8 65.9 54.6 68.9 67.3 57.8 50.1 46.0 44.5 53.0 47.6 45.2 DIORA DIORACB DIORA⇤CB 62.5 ±0.5 64.5 ±0.6 66.4 ±0.7 63.4 65.5 67.8 50.2 ±0.5 49.8 ±0.7 50.4 ±0.7 51.4 50.6 51.5 DIORA DIORACB DIORA⇤CB 72.7 ±1.5 73.2 ±1.7 74.9 ±1.1 76.2 75.7 76.7 55.2 ±0.7 54.5 ±1.2 53.9 ±0.8 56.3 56.6 55.1 PCFG† - 51"
D19-1161,P07-1049,0,0.588301,"2019). We are interested in clustering the learned vectors a(i, j) such that each span may be mapped to a phrase type. To enhance this clustering based approach, we augment the DIORA architecture with latent codes, shown in the right half of the figure. Introduction The deep inside-outside recursive autoencoder (Drozdov et al., 2019, DIORA) is part of a recent trend in fully unsupervised neural constituency parsers (Shen et al., 2018; Williams et al., 2018a; Htut et al., 2018; Shen et al., 2019; Kim et al., 2019). However, these works and nearly all previous research (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013) have focused on unlabeled constituency parsing. In this paper, we instead focus on labeled constituency parsing for English. The small number of previous works that exist in this area suffer from substantial weaknesses: 1) the models depend on ground-truth part-of-speech tags, which are not always available and known to boost constituency parsing scores (Kitaev and Klein, 2018), 2) none can simultaneously identify and label constituents (instead they typically depend on an external latent parser), and 3) they ignore sentences longer than ten tok"
D19-5313,P17-1097,0,0.0429269,"Missing"
D19-5816,D07-1074,0,0.0439162,"linker that finds an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25 retriever on the dev and test queries are also ignored while building the alias table. At test time, our reranker considers all the candidate entity paragraphs that a mention is linked to via the alias table. Although simple, we find this strategy to work well for our task and we plan to use a learned entity linker for future work. 2 Co"
D19-5816,D17-1277,0,0.0264935,"and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model. We assume that all spans of entity mentions have been identified in the paragraph text by a one-time preprocessing, with an entity tagger.3 Entity Linking The next component of our model is an entity linker that finds an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25"
D19-5816,P11-1115,0,0.0340076,"s an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25 retriever on the dev and test queries are also ignored while building the alias table. At test time, our reranker considers all the candidate entity paragraphs that a mention is linked to via the alias table. Although simple, we find this strategy to work well for our task and we plan to use a learned entity linker for future work. 2 Code, pre-trained models a"
D19-5816,W17-2609,0,0.0690815,"Missing"
D19-5816,P18-1223,0,0.0539173,"Missing"
D19-5816,D17-1061,0,0.167894,"oth the initial paragraph and the representation of all the entities within it to determine which evidence to gather next. Essentially, our method introduces a new way of multi-step retrieval that uses information about intermediate entities. A standard way of doing multistep retrieval is via pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001) in which relevant terms from initial retrieved documents are used to reformulate the initial question. A few recent works learn to reformulate the query using task specific reward such as document recall or performance on a QA task (Nogueira and Cho, 2017; Buck et al., 2018; Das et al., 2019). However, these methods do not necessarily use the information about entities present in the evidence as they might not be the more frequent/salient terms in it. 113 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 113–118 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Which US city did Ron Teachworth complete his BA in? Rochester Hills (formerly Avon Township) is a city in northeast Oakland County in the U.S. state of Michigan…... Ronald S. Teachworth is an American artist, actor, w"
D19-5816,N18-1202,0,0.0190123,"rward. The training set had on an avg. 6.35 positive chains per example suggesting a multi-instance multi-label learning training setup (Surdeanu et al., 2012). However, for this work, we treat each chain independently. We use a simple binary cross-entropy loss to train the network. 3 Experiments For all our experiment, unless specified otherwise, we use the open domain corpus4 released by Yang et al. (2018) which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018).5 For the B ERT encoder, we use the B ERT- BASE - UNCASED model.6 We use the implementation of widely-used BM25 retrieval 4 https://hotpotqa.github.io/wiki-readme.html 5 https://allennlp.org/models 6 https://github.com/google-research/bert Model @2 ACCURACY @5 @10 @20 MAP BM25 P RF - TFIDF P RF - RM P RF - TASK 0.093 0.088 0.083 0.097 0.191 0.157 0.175 0.198 0.259 0.204 0.242 0.267 0.324 0.258 0.296 0.330 0.412 0.317 0.406 0.420 B ERT-re-ranker Q UERY +E- DOC 0.146 0.101 0.271 0.223 0.347 0.301 0.409 0.367 0.470 0.568 Our Model 0.230 0.482 0.612 0.674 0.654 Table 1: Retrieval performance of m"
D19-5816,D12-1042,0,0.0424696,"D and E and as we show empirically, not considering both leads to decrease in performance. During training, we mark a chain of paragraphs as a positive example, if the last paragraph of the chain is present in the supporting facts, since that is a chain of reasoning that led to a relevant paragraph. All other paragraph chains are treated as negative examples. In our experiments, we consider chains of length 2, although extending to longer chains is straightforward. The training set had on an avg. 6.35 positive chains per example suggesting a multi-instance multi-label learning training setup (Surdeanu et al., 2012). However, for this work, we treat each chain independently. We use a simple binary cross-entropy loss to train the network. 3 Experiments For all our experiment, unless specified otherwise, we use the open domain corpus4 released by Yang et al. (2018) which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018).5 For the B ERT encoder, we use the B ERT- BASE - UNCASED model.6 We use the implementation of widely-used BM25 retrieval 4 https://hotpot"
D19-5816,N18-1059,0,0.0645134,"Missing"
D19-5816,Q18-1021,0,0.0340131,"o the reader model. We achieve a 10.59 absolute increase in F1 score than the baseline. It should be noted that we use the simple baseline reader model and we are confident that we can achieve better scores by using more sophisticated reader architectures, e.g. using B ERT based architectures. Our results show that retrieval is an important component of an opendomain system and equal importance should be given to both the retriever and reader component. 3.3 Zero-shot experiment on Wikihop We experiment if our model trained on H OTPOT QA can generalize to another multi-hop dataset – W IK IHOP (Welbl et al., 2018), without any training. In the W IKIHOP dataset, a set of candidate introductory Wikipedia paragraphs are given per question. Hence, we do not need to use our initial BM25 retriever. We assign the first entity mention occurring in a 116 Model BM25 B ERT-re-ranker (zs) Our Model (zs) acc@2 acc@5 0.06 0.08 0.10 0.30 0.27 0.41 et al., 2017; Zamani et al., 2018, inter-alia). Bagof-words and contextual embedding models, such as word2vec and BERT, have also been explored extensively for various IR tasks, from document to sentence-level retrieval (Padigela et al., 2019; Zamani and Croft, 2016, 2017)."
D19-5816,D18-1259,0,0.335373,"eira and Cho (2019). However, unlike us, they do not model the chains of evidence paragraphs required for a multi-hop question. Secondly, they also do not have a entity linking component to identify the relevant paragraphs. Our model out-performs them for multi-hop QA. To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in Yang et al. (2018), it improved the QA performance on the hidden test set by 10.59 F1 points.2 2 Methodology Our approach is summarized in Figure 2. The first component of our model is a standard IR system that takes in a natural language query ‘Q’ and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model. We assume that all spans of entity mentions have been identified in the paragraph text by a one-time preprocessing, with an entity tagger.3 Entity Linking The next component of our model is an entity linker that finds an"
D19-5816,N19-1423,0,\N,Missing
E17-1013,P15-1016,1,0.916712,"e or more entities in the KB’s entity-relation graph. For example, we may have no evidence directly linking Melinda Gates and Seattle. However, we may infer with some likelihood that Melinda–lives-in– Seattle, by observing that the KB contains the path Melinda–spouse–Bill–chairman–Microsoft– HQ-in–Seattle (Fig. 1a). Symbolic rules of this form are learned by the Path Ranking Algorithm (PRA) (Lao et al., 2011). Dramatic improvement in generalization can be obtained by reasoning about paths, not in terms of relation-symbols, but Universal Schema style relation-vector-embeddings. This is done by Neelakantan et al. (2015), where RNNs compose the per-edge relation embeddings along an arbitrarylength path, and output a vector embedding representing the inferred relation between the two entities at the end-points of the path. This approach thus represents a key example of complex reasoning over Horn clause chains using neural networks. However, for multiple reasons detailed below it is inaccurate and impractical. This paper presents multiple modeling advances that significantly increase the accuracy and practicality of RNN-based reasoning on Horn clause chains in large-scale KBs. (1) Previous work, including (Lao"
E17-1013,P11-1062,0,0.0254858,"our setting where we have more than 3M entity pairs. They also model entities as just a scalar weight whereas we learn both entity and type representations. Lastly it has been shown by Neelakantan et al. (2015) that non-linear composition function out-performs linear functions (as used by them) for relation extraction tasks. The performance of relation extraction methods have been improved by incorporating entity types for their candidate entities, both in sentence level (Roth and Yih, 2007; Singh et al., 2013) and KB relation extraction (Chang et al., 2014), and in learning entailment rules (Berant et al., 2011). Serban et al. (2016) use RNNs to generate factoid question from Freebase. ht “ f pWhh ht´1 ` Wih yrt ` Weh yet q (6) Weh P Rmˆh is the new parameter matrix for projecting the entity representation. Figure 2 shows our model with an example path between entities (Microsoft, USA) with countryOfHQ (country of head-quarters) as the query relation. 4 # 27,791 23,599 46 3.22M 2218 4.7 7 191M Related Work Two early works on extracting clauses and reasoning over paths are SHERLOCK (Schoenmackers et al., 2010) and the Path Ranking Algorithm (PRA) (Lao et al., 2011). SHERLOCK extracts purely symbolic c"
E17-1013,D14-1165,0,0.00860027,"all entity pairs, making it prohibitive to be used in our setting where we have more than 3M entity pairs. They also model entities as just a scalar weight whereas we learn both entity and type representations. Lastly it has been shown by Neelakantan et al. (2015) that non-linear composition function out-performs linear functions (as used by them) for relation extraction tasks. The performance of relation extraction methods have been improved by incorporating entity types for their candidate entities, both in sentence level (Roth and Yih, 2007; Singh et al., 2013) and KB relation extraction (Chang et al., 2014), and in learning entailment rules (Berant et al., 2011). Serban et al. (2016) use RNNs to generate factoid question from Freebase. ht “ f pWhh ht´1 ` Wih yrt ` Weh yet q (6) Weh P Rmˆh is the new parameter matrix for projecting the entity representation. Figure 2 shows our model with an example path between entities (Microsoft, USA) with countryOfHQ (country of head-quarters) as the query relation. 4 # 27,791 23,599 46 3.22M 2218 4.7 7 191M Related Work Two early works on extracting clauses and reasoning over paths are SHERLOCK (Schoenmackers et al., 2010) and the Path Ranking Algorithm (PRA)"
E17-1013,D13-1080,0,0.0998141,"graph starting from es till et . Let π “ tes , r1 , e1 , r2 , . . . , rk , et u P S denote a path between pes , et q. The length of a path is the number of relations in it, hence, plenpπq “ kq. Let yrt P Rd denote the vector representation of rt . The Path-RNN model combines all the relations in π sequentially using a RNN with an intermediate representation ht P Rh at step t given by r r r yrt q. ht “ f pWhh ht´1 ` Wih Ppr|es , et q “ (3) where σ is the sigmoid function. Path-RNN and other models such as the Path Ranking Algorithm (PRA) and its extensions (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014) makes it impractical to be used in downstream applications, since it requires training and maintaining a model for each relation type. Moreover, parameters are not shared across multiple target relation types leading to large number of parameters to be learned from the training data. In (3), the Path-RNN model selects the maximum scoring path between an entity pair to make a prediction, possibly ignoring evidence from other important paths. Not only is this a waste of computation (since we have to compute a forward pass for all the paths anyway), but also the relations"
E17-1013,N13-1008,1,0.886626,"Missing"
E17-1013,D14-1044,0,0.375025,"s till et . Let π “ tes , r1 , e1 , r2 , . . . , rk , et u P S denote a path between pes , et q. The length of a path is the number of relations in it, hence, plenpπq “ kq. Let yrt P Rd denote the vector representation of rt . The Path-RNN model combines all the relations in π sequentially using a RNN with an intermediate representation ht P Rh at step t given by r r r yrt q. ht “ f pWhh ht´1 ` Wih Ppr|es , et q “ (3) where σ is the sigmoid function. Path-RNN and other models such as the Path Ranking Algorithm (PRA) and its extensions (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014) makes it impractical to be used in downstream applications, since it requires training and maintaining a model for each relation type. Moreover, parameters are not shared across multiple target relation types leading to large number of parameters to be learned from the training data. In (3), the Path-RNN model selects the maximum scoring path between an entity pair to make a prediction, possibly ignoring evidence from other important paths. Not only is this a waste of computation (since we have to compute a forward pass for all the paths anyway), but also the relations in all other paths do n"
E17-1013,W16-1309,0,0.0259326,"Missing"
E17-1013,S13-1001,0,0.0314207,"e seen as a simple type of reasoning, as can other work in tensor factorization (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). However these methods can be understood as operating on single pieces of evidence: for example, inferring that Microsoft–located-in–Seattle implies Microsoft– HQ-in–Seattle. A highly desirable, richer style of reasoning Introduction There is a rising interest in extending neural networks to perform more complex reasoning, formerly addressed only by symbolic and logical reasoning systems. So far this work has mostly focused on small or synthetic data (Grefenstette, 2013; Bowman et al., 2014; Rockt¨aschel and Riedel, 2016). Our interest is primarily in reasoning about large knowledge bases (KBs) with diverse semantics, populated from text. One method 1 The code and data are available https://rajarshd.github.io/ChainsofReasoning/ at 132 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 132–141, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Feb 6, 1999 William H. Gates,chairman of Microsoft Corp. and his wife Melinda gave $3.3B to their t"
E17-1013,D15-1038,0,0.646542,"path between an entity pair to make a prediction, possibly ignoring evidence from other important paths. Not only is this a waste of computation (since we have to compute a forward pass for all the paths anyway), but also the relations in all other paths do not get any gradients updates during training as the max operation returns zero gradient for all other paths except the maximum scoring one. This is especially ineffective during the initial stages of the training since the maximum probable path will be random. The Path-RNN model and other multi-hop relation extraction approaches (such as Guu et al. (2015)) ignore the entities in the path. Consider the following paths JFK–locatedIn– (1) r r P Rdˆh are the paramWhh P Rhˆh and Wih eters of the RNN. Here r denotes the query relation. Path-RNN has a specialized model for predicting each query relation r, with separate paramr , Wr q for each r. f is the sigeters pyrrt , Whh ih moid function. The vector representation of path π pyπ q is the last hidden state hk . The similarity of yπ with the query relation vector yr is computed as the dot product between them: scorepπ, rq “ yπ ¨ yr σpmaxps1 , s2 , . . . , sN qq (2) Pairs of entities may have several"
E17-1013,D10-1106,0,0.0124827,"Missing"
E17-1013,D11-1049,0,0.920453,"doing random walks in the knowledge graph starting from es till et . Let π “ tes , r1 , e1 , r2 , . . . , rk , et u P S denote a path between pes , et q. The length of a path is the number of relations in it, hence, plenpπq “ kq. Let yrt P Rd denote the vector representation of rt . The Path-RNN model combines all the relations in π sequentially using a RNN with an intermediate representation ht P Rh at step t given by r r r yrt q. ht “ f pWhh ht´1 ` Wih Ppr|es , et q “ (3) where σ is the sigmoid function. Path-RNN and other models such as the Path Ranking Algorithm (PRA) and its extensions (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014) makes it impractical to be used in downstream applications, since it requires training and maintaining a model for each relation type. Moreover, parameters are not shared across multiple target relation types leading to large number of parameters to be learned from the training data. In (3), the Path-RNN model selects the maximum scoring path between an entity pair to make a prediction, possibly ignoring evidence from other important paths. Not only is this a waste of computation (since we have to compute a forward pass for all th"
E17-1013,D12-1093,0,0.0427635,"s in the knowledge graph starting from es till et . Let π “ tes , r1 , e1 , r2 , . . . , rk , et u P S denote a path between pes , et q. The length of a path is the number of relations in it, hence, plenpπq “ kq. Let yrt P Rd denote the vector representation of rt . The Path-RNN model combines all the relations in π sequentially using a RNN with an intermediate representation ht P Rh at step t given by r r r yrt q. ht “ f pWhh ht´1 ` Wih Ppr|es , et q “ (3) where σ is the sigmoid function. Path-RNN and other models such as the Path Ranking Algorithm (PRA) and its extensions (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014) makes it impractical to be used in downstream applications, since it requires training and maintaining a model for each relation type. Moreover, parameters are not shared across multiple target relation types leading to large number of parameters to be learned from the training data. In (3), the Path-RNN model selects the maximum scoring path between an entity pair to make a prediction, possibly ignoring evidence from other important paths. Not only is this a waste of computation (since we have to compute a forward pass for all the paths anyway), b"
E17-1013,D15-1174,0,0.0865462,"r our model predicts the true relations with high confidence. This is a first step towards the capturing existential quantification for logical inference in vector space. Length of Clauses: Figure 4 shows the length distribution of top scoring paths in the test set. The distribution peaks at lengths“ t3, 4, 5u, suggesting that previous approaches (Yang et al., 2015) which restrict the length to 3 might limit performance and generalizability. Limitation: A major limitation of our model is inability to handle long textual patterns because of sparsity. Compositional approaches for modeling text (Toutanova et al., 2015; Verga et al., 2016) are a right step in this direction and we leave this as future work. Qualitative Analysis Entities as Existential Quantifiers: Table 5 shows the body of two horn clauses. Both the clauses are predictive of the fact location.containspx, bq. The first clause is universally true irrespective of the entities present in the chain (transitive property). However the value of the second clause is only true conditional on the instantiations of the entities. The score of the Path-RNN model is independent of the entity values, whereas our model outputs a different score based on the"
E17-1013,N13-1095,0,0.0220493,". rpa, bq ô r´1 pb, aq. for populating a KB from text (and for representing diverse semantics in the KB) is Universal Schema (Riedel et al., 2013; Verga et al., 2016), which learns vector embeddings of relation types - the union of all input relation types, both from the schemas of multiple structured KBs, as well as expressions of relations in natural language text. An important reason to populate a KB is to support not only look-up-style question answering, but reasoning on its entities and relations in order to make inferences not directly stored in the KB. KBs are often highly incomplete (Min et al., 2013), and reasoning can fill in these missing facts. The “matrix completion” mechanism that underlies the common implementation of Universal Schema can thus be seen as a simple type of reasoning, as can other work in tensor factorization (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). However these methods can be understood as operating on single pieces of evidence: for example, inferring that Microsoft–located-in–Seattle implies Microsoft– HQ-in–Seattle. A highly desirable, richer style of reasoning Introduction There is a rising interest in extending neural networks to perform m"
E17-1013,P16-1136,0,0.272857,"e addition of the entity type representations. The entity type representations are learned during training. We limit the number of entity types for an entity to 7 most frequently occurring types in the KB. Let yet P Rm denote the representation of entity et , then 4 now becomes Table 2: Statistics of the dataset. contain one entity. Guu et al. (2015) introduce new compositional techniques by modeling additive and multiplicative interactions between relation matrices in the path. However they model only a single path between an entity pair in-contrast to our ability to consider multiple paths. Toutanova et al. (2016) improves upon them by additionally modeling the intermediate entities in the path and modeling multiple paths. However, in their approach they have to store scores for intermediate path length for all entity pairs, making it prohibitive to be used in our setting where we have more than 3M entity pairs. They also model entities as just a scalar weight whereas we learn both entity and type representations. Lastly it has been shown by Neelakantan et al. (2015) that non-linear composition function out-performs linear functions (as used by them) for relation extraction tasks. The performance of re"
E17-1013,N16-1103,1,0.831251,"true relations with high confidence. This is a first step towards the capturing existential quantification for logical inference in vector space. Length of Clauses: Figure 4 shows the length distribution of top scoring paths in the test set. The distribution peaks at lengths“ t3, 4, 5u, suggesting that previous approaches (Yang et al., 2015) which restrict the length to 3 might limit performance and generalizability. Limitation: A major limitation of our model is inability to handle long textual patterns because of sparsity. Compositional approaches for modeling text (Toutanova et al., 2015; Verga et al., 2016) are a right step in this direction and we leave this as future work. Qualitative Analysis Entities as Existential Quantifiers: Table 5 shows the body of two horn clauses. Both the clauses are predictive of the fact location.containspx, bq. The first clause is universally true irrespective of the entities present in the chain (transitive property). However the value of the second clause is only true conditional on the instantiations of the entities. The score of the Path-RNN model is independent of the entity values, whereas our model outputs a different score based on the entities in the chai"
E17-1058,D13-1160,0,0.0106084,"Missing"
E17-1058,D14-1067,0,0.00919077,"Missing"
E17-1058,D13-1161,0,0.0100082,"Missing"
E17-1058,P16-1200,0,0.0759131,"Missing"
E17-1058,P12-1059,0,0.024684,"Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016). These models can generalize to column entries unseen at training by learning compositional pattern encoders to parameterize the column matrix in place of embeddings. Most of these models do not generalize to unseen entity pairs and none of them generalize to unseen entities. Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is ‘row-less’ having no explicit parameters for entity pairs and entities. Entity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling Model Parameters Entity Embeddings 3.7 e6 Attention 3.1 e5 Mean Pool/Max Pool/Max Relation 1.5 e5 Table 1: Number of parameters for the different models on the entity type dataset. 3.3 Related Work 1 Many past papers restrict negative samples to be of the same type as the positive example. We simply sample uniformly from the entire set of row entries 2 data and code available at https://github.com/ patverga/torch-relation-extraction/tree/ rowless-updates Training The vector representation of the rows and the columns are the parameters of the model. Riedel et al. (2013) 616 Query Encoder"
E17-1058,P09-1113,0,0.0264452,"ust, we add ‘pattern dropout’ for entity pairs with many mentions. We set V ¯(r) to be m randomly sampled mentions for entity pairs with greater than m total mentions. In our experiments we set m = 10 and at test time we use all mentions. We then use V ¯(r) to obtain the aggregated row representation as discussed above. We randomly sample 200 columns unobserved with row r to act as the negative samples. All models are implemented in Torch2 and are trained using Adam (Kingma and Ba, 2015) with default momentum related hyperparameters. 4 Relation extraction for KB completion has a long history. Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned. Most of the embeddingbased methods learn representations for entities (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al. (2013) use entity pair representations. ‘Column-less’ versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016). These models can generalize to column entries unseen at training by learning compositional pattern encoders to parameterize the column matrix in place of embeddings. Most of th"
E17-1058,N15-1054,1,0.154502,"xtual), enabling easier downstream processing closer to natural language interaction with the KB. Finally, our model gains additional training signal from multitask learning of textual and KB types. Since universal schema leverages large amounts of unlabeled text we desire the benefits of entity pair modeling, and row-less universal schema facilitates learning entity pair representations without the drawbacks of the traditional oneembedding-per-pair approach. The majority of current embedding methods for KB entity type prediction operate with explicit entity representations (Yao et al., 2013; Neelakantan and Chang, 2015) and hence, cannot generalize to unseen entities. In relation extraction, entity-level models (Nickel et al., 2011; Garc´ıa-Dur´an et al., 2016; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al. (2013) observe that the entity pair model outperforms e"
E17-1058,D14-1113,1,0.0280589,"orced to to act as a centroid to all possible columns observed with that particular row. For example, the entity pair Bill and Melinda Gates could hold the relation ‘per:spouse’ or ‘per:co-worker’. A queryspecific aggregation mechanism can produce separate representations for this entity pair dependent on the query. The Max Relation aggregation function represents the row as its most similar column to the query vector of interest. Given a query relation c, cmax = argmaxc¯∈V ¯(r) v(¯ c).v(c) v(r) = v(cmax ) A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks. This model has the advantage of creating a query-specific entity pair representation, but is more susceptible to noisy training data as a single incorrect piece of evidence could be used to form a prediction. Finally, we look at an Attention aggregation function over columns (Figure 3) which is similar to a single-layer memory network (Sukhbaatar et al., 2015). The soft attention mechanism has been used to selectively focus on relevant parts in many different models (Bahdanau et al., 2015; Graves et al., 2014; Neelakantan et al., 2016). In this"
E17-1058,N13-1008,1,0.529643,"downstream tasks such as question answering (Bordes et al., 2014) and semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013). An effective approach to AKBC is universal schema, which predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types—not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem. In the standard formulation for relation extraction (Riedel et al., 2013), entity pairs and relations occupy the rows and columns of the matrix respectively (Figure 1a). Analogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 1b). The row and column entries are represented as learned vectors with compatibility determined by a scoring function. In its original form, universal schema can reason only about row entries and column entries explicitly seen during training. Unseen rows and columns observed at test time do not have a learned embedding. This problem is referred to as the cold"
E17-1058,P15-1016,1,0.299142,"kel et al., 2011; Garc´ıa-Dur´an et al., 2016; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al. (2013) observe that the entity pair model outperforms entity models in cases where the entity pair was seen at training time. Most similar to this work, Neelakantan et al. (2015) classify KB relations by finding the maximum scoring path between two entities. This model is also ‘rowless’ and does not directly model entities or entity pairs. There are several important differences in this work. Neelakantan et al. (2015) learn per-relation classifiers to predict only a small set of KB relations, while we instead predict all relations, including textual relations. We also explore aggregation functions that pool evidence from multiple paths while Neelakantan et al. (2015) only chose the maximum scoring path. Additionally, we demonstrate that our models can perform on par w"
E17-1058,W16-1313,0,0.019238,"xample attention model in a row-less universal schema relation extractor. In the attention model, we compute the dot product between the representation of the query relation and the representation of an entity pair’s observed relation type followed by a softmax, giving a weighting over the observed relation types. This output is then used to get a weighted sum over the set of representations of the observed relation types. The result is a query-specific vector representation of the entity pair. The Max Relation model takes the most similar observed relation’s representation. 5 and Weld, 2012; Shimaoka et al., 2016). More recently, embedding-based methods for knowledge base entity type prediction have been proposed (Yao et al., 2013; Neelakantan and Chang, 2015). These methods have explicit entity representations, hence cannot generalize to unseen entities. Experimental Results In this section, we compare our models that have aggregate row representations with models that have explicit row representations on entity type prediction and relation extraction tasks. Finally, we perform experiments on a column-less universal schema model. Table 1 shows that the row-less models require far fewer parameters sinc"
E17-1058,D12-1042,0,0.08034,"Missing"
E17-1058,D15-1174,0,0.567354,"spectively (Figure 1a). Analogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 1b). The row and column entries are represented as learned vectors with compatibility determined by a scoring function. In its original form, universal schema can reason only about row entries and column entries explicitly seen during training. Unseen rows and columns observed at test time do not have a learned embedding. This problem is referred to as the cold-start problem in recommendation systems (Schein et al., 2002). Recently Toutanova et al. (2015) and Verga et al. (2016) proposed ‘column-less’ versions of universal schema models that generalize to unseen column entries. They learn compositional pattern encoders to parameterize the column matrix in place of individual column embeddings. However, these models still do not generalize to unseen row entries. In this work, we present a ‘row-less’ extension of universal schema that generalizes to unseen entities and entity pairs. Rather than representing each row entry with an explicit dense vector, we encode each entity or entity pair as aggregate functions over their observed column entries"
E17-1058,N16-1103,1,0.811679,"ogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 1b). The row and column entries are represented as learned vectors with compatibility determined by a scoring function. In its original form, universal schema can reason only about row entries and column entries explicitly seen during training. Unseen rows and columns observed at test time do not have a learned embedding. This problem is referred to as the cold-start problem in recommendation systems (Schein et al., 2002). Recently Toutanova et al. (2015) and Verga et al. (2016) proposed ‘column-less’ versions of universal schema models that generalize to unseen column entries. They learn compositional pattern encoders to parameterize the column matrix in place of individual column embeddings. However, these models still do not generalize to unseen row entries. In this work, we present a ‘row-less’ extension of universal schema that generalizes to unseen entities and entity pairs. Rather than representing each row entry with an explicit dense vector, we encode each entity or entity pair as aggregate functions over their observed column entries. This is beneficial bec"
H05-1094,A00-2030,0,0.0271885,"over the cascaded approach. 7 Related Work Researchers have begun to accumulate experimental evidence that joint training and decoding yields better performance than the cascaded approach. As mentioned earlier, the original work on dynamic CRFs (Sutton et al., 2004) demonstrated improvement due to joint training in the domains of part-of-speech tagging and noun-phrase 753 chunking. Also, Carreras and Marquez (Carreras & M`arquez, 2004) have obtained increased performance in clause finding by training a cascade of perceptrons to minimize a single global error function. Finally, Miller et al. (Miller et al., 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable. For example, Miller et al. report that they originally attempted to annotate newswire articles for all of parsing, relations, and named entities, but they stopped because the annotation was simply too expensive. Instead they hand-labeled"
H05-1094,N03-1028,0,0.0488391,"only if st−1 has the label S PEAK ER NAME , st has the label OTHER , and the word xt begins with a capital letter. The chief practical advantage 749 X log Z(xj ) − j X λ2 k 2σ 2 (4) k where the final term is a zero-mean Gaussian prior placed on parameters to avoid overfitting. Although this maximization cannot be done in closed form, it can be optimized numerically. Particularly effective are gradientbased methods that use approximate second-order information, such as conjugate gradient and limited-memory BFGS (Byrd et al., 1994). For more information on current training methods for CRFs, see Sha and Pereira (2003). 3 Dynamic CRFs Dynamic conditional random fields (Sutton et al., 2004) extend linear-chain CRFs in the same way that dynamic Bayes nets (Dean & Kanazawa, 1989) extend HMMs. Rather than having a single monolithic state variable, DCRFs factorize the state at each time step by an undirected model. Formally, DCRFs are the class of conditionally-trained undirected models that repeat structure and parameters over a sequence. If we denote by Φc (yc,t , xt ) the repetition of clique c at time step t, then a DCRF defines the probability of a label sequence s given the input x as: Q Φc (yc,t , xt ) p("
H05-1094,W04-2412,0,0.0119153,"mes from newswire text; whereas the available training data for new applications, such as extracting appointment information from email, tends to be much smaller. Thus, we need to transfer regularities learned from a well-studied subtask, such as finding person names in newswire text, to a new, related task, such as finding names of speakers in email seminar announcements. In previous NLP systems, transfer is often accomplished by training a model for the subtask, and using its prediction as a feature for the new task. For example, recent CoNLL shared tasks (Tjong Kim Sang & De Meulder, 2003; Carreras & Marquez, 2004), which are standard data sets for such common NLP tasks as clause idenSecond, information from the main task can inform the subtask. This is especially important for learning transfer, because the new domain often has different characteristics than the old domain, which is often a standard benchmark data set. For example, named-entity recognizers are usually trained on newswire text, which is more structured and grammatical than email, so we expect an off-the-shelf named-entity recognizer to perform somewhat worse on email. An email task, however, often has domain-specific features, such as P"
H05-1094,N04-1001,0,0.0176341,"Missing"
H05-1094,W03-0419,0,0.0322451,"Missing"
H05-1094,W05-0620,0,\N,Missing
K18-1001,D15-1161,0,0.0320739,"zer techniques to enhance SGD such as Adam (Kingma and Ba, 2015), Averaged SGD (Polyak and Juditsky, 1992) and YellowFin (Zhang et al., 2017), none of them performed as well as mini-batch SGD with a batch-size of 1. We also employed gradient clipping to a norm of 5.0, a learning rate of 0.01, learning rate decay of 0.05, dropout with p = 0.5, and early stopping, tuned on the citation development data. We initialized our word level embeddings using pre-trained 100 dimensional Glove embeddings (Pennington et al., 2014), which gave better performance on our tasks than the skip-n-gram embeddings (Ling et al., 2015) used in the original work of Lample et al. (2016). The datasets were pre-processed to zeroreplace all occurrences of numbers. Finally, we experimented with both IOBES and IOB tagging schemes, with IOB demonstrating higher performance on our tasks. Embedding size We tune the embedding size (rank constraint) for the hidden state matrix A, varying from 10 to 40, alongside the neural network parameters, and report results when fixing the other hyperparameters and varying embedding size, similar to ablation analysis. Table 4 shows the impact of different embedding sizes on the performance of the m"
K18-1001,P14-1056,1,0.825781,"edical domain, where the small training set gives our parsimonious approach to output representation learning an extra advantage. Figure 1: An example result from the CLEF eHealth dataset. The soft output constraint suggests tagging patient status as Myshift/Others if there already is a Myshift Status tag. Note that we have the same phrase tagged as Myshift Status in the training dataset. constraints on the overall prediction, without sacrificing efficient exact inference. While soft and hard global constraints have a rich history in sequence tagging (Koo et al., 2010; Rush and Collins, 2012; Anzaroot et al., 2014), they have been underexplored in the context of neural-network based feature extraction models. In response, we present a latent-variable CRF model with a novel mechanism for learning latent constraints without overfitting, using low-rank embeddings of large-cardinality latent variables. For example, these non-local constraints appear in fine-grained nested field extraction, which requires hierarchical consistency between the subtags of an entity. Further, information extraction and slot filling tasks often require domain specific constraints — for example, we must avoid extracting the same f"
K18-1001,D14-1162,0,0.099056,"er settings from the LSTM+CRF model of Lample et al. (2016). Although, we did explore different optimizer techniques to enhance SGD such as Adam (Kingma and Ba, 2015), Averaged SGD (Polyak and Juditsky, 1992) and YellowFin (Zhang et al., 2017), none of them performed as well as mini-batch SGD with a batch-size of 1. We also employed gradient clipping to a norm of 5.0, a learning rate of 0.01, learning rate decay of 0.05, dropout with p = 0.5, and early stopping, tuned on the citation development data. We initialized our word level embeddings using pre-trained 100 dimensional Glove embeddings (Pennington et al., 2014), which gave better performance on our tasks than the skip-n-gram embeddings (Ling et al., 2015) used in the original work of Lample et al. (2016). The datasets were pre-processed to zeroreplace all occurrences of numbers. Finally, we experimented with both IOBES and IOB tagging schemes, with IOB demonstrating higher performance on our tasks. Embedding size We tune the embedding size (rank constraint) for the hidden state matrix A, varying from 10 to 40, alongside the neural network parameters, and report results when fixing the other hyperparameters and varying embedding size, similar to abla"
K18-1001,W16-5901,0,0.0219317,"ded-state latent CRF, is shown in Figure 2c. We introduce a sequence of hidden states z = {z1 , z2 , ..., zT } where zt is one of M possible discrete hidden states and M &gt;&gt; N . Similarly, the corresponding energy for a particular joint configuration over y and z is E(y, z |x) = T X ψxz (xt , zt ) + ψyz (yt , zt ) t=1 + ψzz (zt , zt+1 ) (4) where ψxz (xt , zt ), ψyz (yt , zt ) are the local interaction log-potentials between the input features 3 (Paszke et al., 2017), we only need to implement the forward pass, as automatic differentiation (back-propagation) is equivalent to the backward pass (Eisner, 2016). MAP inference. At test time, we run the Viterbi algorithm to search for the best configuration over z rather than over y. Mapping from the hidden state zt to the output label yt is deterministic given the output state embedding. and hidden states, and the hidden states and output states, respectively. The hidden state dynamics come from the log-scores ψzz (zt , zt+1 ) for transitioning between hidden state zt to zt+1 . The posterior distribution over output labels can be computed by summing over all possible configurations of z P(y |x) = 1 X exp (E(y, z |x)) Z z (5) 3 P P 0 0 where Z = y0 z0"
K18-1001,D10-1125,0,0.238326,"on. Our biggest improvement comes in the medical domain, where the small training set gives our parsimonious approach to output representation learning an extra advantage. Figure 1: An example result from the CLEF eHealth dataset. The soft output constraint suggests tagging patient status as Myshift/Others if there already is a Myshift Status tag. Note that we have the same phrase tagged as Myshift Status in the training dataset. constraints on the overall prediction, without sacrificing efficient exact inference. While soft and hard global constraints have a rich history in sequence tagging (Koo et al., 2010; Rush and Collins, 2012; Anzaroot et al., 2014), they have been underexplored in the context of neural-network based feature extraction models. In response, we present a latent-variable CRF model with a novel mechanism for learning latent constraints without overfitting, using low-rank embeddings of large-cardinality latent variables. For example, these non-local constraints appear in fine-grained nested field extraction, which requires hierarchical consistency between the subtags of an entity. Further, information extraction and slot filling tasks often require domain specific constraints —"
K18-1001,N16-1030,0,0.594492,"the output prediction model. However, output variables obey a variety of hard and soft constraints — for example, in sequence tagging tasks such as named entity recognition, I-PER cannot follow B-ORG. Interestingly, even with such powerful local featurization, the DNN model does not automatically capture a mode of the output distribution through local decisions alone, and can violate these constraints. Successful applications of DNNs to sequence tagging gain from incorporating a simple linear chain probabilistic graphical model to enforce consistent output predictions (Collobert et al., 2011; Lample et al., 2016), and more generally the addition of a graphical model to enforce output label consistency is common practice for other tasks such as image segmentation (Chen et al., 2018). Previous work in DNN-featurized sequence tagging with graphical models for information extraction has limited its output structure modeling to these simple local Markovian dependencies. In this work, we explore the addition of latent variables to the prediction model, and through a parsimonious factorized parameter structure, perform representation learning of hidden state embeddings in the graphical model, complementary t"
K19-1053,N19-1423,0,0.0383268,"Missing"
K19-1053,P18-2081,0,0.27479,"gs, and can predict how a politician will vote on a bill. As demonstrated in this paper, this model works well when a politician’s voting track record has already been established. However, it fails for politicians not in the training data, such as those who have never been elected to office or voted on bills relevant to the issues in the target bill. Although we did not implement any Ideal Point Models, they obviously share this weakness. There have also been approaches that incorporate additional knowledge about politicians and bills to yield extra insight into politicians’ voting patterns. Kornilova et al. (2018) enhanced the embeddings learned by their model by providing bill sponsorship information along with a CNN architecture for learning bill embeddings, while Nguyen et al. (2015) supplemented their model by taking into consideration the type of language legislators use. However, these models share the inability of earlier models to predict voting behavior for new politicians or political candidates. In fact, Kornilova et al. (2018) explicitly note that their model cannot handle unobserved politicians: they say, ”During testing, we only include legislators present in the training data”. The contr"
K19-1053,D16-1221,0,0.279744,"tance with respect to specific issues. A KB such as Freebase is likely to contain rich information such as events a congressperson attends, people they are related to, and personal details such as schools they were educated at; this information may be correlated with a politician’s stance on specific issues (Sunshine Hillygus, 2005; Duckitt and Sibley, 2010; Kraut and Lewis, 1975). Information in a KB is likely to be more restricted, but more reliable, than information extracted from news articles. We integrate these sources of information into the embedding based prediction model proposed in Kraft et al. (2016). We experiment with two representations for news articles: as the mean of the embeddings of the words in the article, and as a bag of words. To represent information in KBs, we first capture the KB relations using Universal Schema (US) (Riedel et al., 2013), and then construct relation embeddings using a neural network. We evaluate the proposed approaches on multiple sessions of Congress under two settings: (1) with only politicians that are observed at trainThe official voting records of United States congresspeople are preserved as roll call votes. Prediction of voting behavior of politicia"
K19-1053,P15-1139,0,0.147067,". However, it fails for politicians not in the training data, such as those who have never been elected to office or voted on bills relevant to the issues in the target bill. Although we did not implement any Ideal Point Models, they obviously share this weakness. There have also been approaches that incorporate additional knowledge about politicians and bills to yield extra insight into politicians’ voting patterns. Kornilova et al. (2018) enhanced the embeddings learned by their model by providing bill sponsorship information along with a CNN architecture for learning bill embeddings, while Nguyen et al. (2015) supplemented their model by taking into consideration the type of language legislators use. However, these models share the inability of earlier models to predict voting behavior for new politicians or political candidates. In fact, Kornilova et al. (2018) explicitly note that their model cannot handle unobserved politicians: they say, ”During testing, we only include legislators present in the training data”. The contributions of Kornilova et al. (2018), among them the CNNbased bill representation and the incorporation of bill metadata, are orthogonal to those in this paper. We leave the exp"
K19-1053,N13-1008,1,0.886181,"lated with a politician’s stance on specific issues (Sunshine Hillygus, 2005; Duckitt and Sibley, 2010; Kraut and Lewis, 1975). Information in a KB is likely to be more restricted, but more reliable, than information extracted from news articles. We integrate these sources of information into the embedding based prediction model proposed in Kraft et al. (2016). We experiment with two representations for news articles: as the mean of the embeddings of the words in the article, and as a bag of words. To represent information in KBs, we first capture the KB relations using Universal Schema (US) (Riedel et al., 2013), and then construct relation embeddings using a neural network. We evaluate the proposed approaches on multiple sessions of Congress under two settings: (1) with only politicians that are observed at trainThe official voting records of United States congresspeople are preserved as roll call votes. Prediction of voting behavior of politicians for whom no voting record exists, such as individuals running for office, is important for forecasting key political decisions. Prior work has relied on past votes cast to predict future votes, and thus fails to predict voting patterns for politicians wit"
N04-1042,W02-2018,0,0.143852,"uality, wherein the the empirical count of each feature matches its expected count according to the model PΛ (y|x). X i fk (yt−1 , yt , xi , t) = X PΛ (y|x)fk (yt−1 , yt , xi , t) i CRFs share many of the advantageous properties of standard maximum entropy models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995), can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003). We use BFGS for optimization. In our experiments, we shall focus instead on two other aspects of CRF deployment, namely regularization and selection of different model structure and feature types. 2.1 Regularization in CRFs To avoid over-fitting, log-likelihood is often penalized by some prior distribution over the parameters. Figure 1 shows an empirical distribution of parameters, Λ, learned from an unpenalized likelihood, including only features with non-zero count in the training set. Three prior distributions that have shape similar to this empirical distribution"
N04-1042,W03-0430,1,0.705659,"m in two steps looses the tight interaction between state transitions and observations. In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001), and explore several practical issues in applying CRFs to information extraction in general. The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences. CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003). The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase. Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000), exponential (Goodman, 2003), and hyperbolic-L1 (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, 12 and layout, as well as proposing a method for the beneficial use of zero-count fe"
N04-1042,N03-1028,0,0.226094,"ons. In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001), and explore several practical issues in applying CRFs to information extraction in general. The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences. CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003). The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase. Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000), exponential (Goodman, 2003), and hyperbolic-L1 (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, 12 and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties. We describe a large collection of"
N04-4028,W03-0413,0,\N,Missing
N04-4028,N03-1028,0,\N,Missing
N06-1012,P05-1001,0,0.0377375,"orithm FeatureBoost is analogous to AdaBoost, except that the meta-learning algorithm maintains weights on features instead of on instances. Feature subsets are automatically sampled based on which features, if corrupted, would most affect the ensemble’s prediction. They show that FeatureBoost is more robust than AdaBoost on synthetically corrupted UCI data sets. Their method does not easily extend to sequence models, especially natural-language models with hundreds of thousands of features. Model Single CRF(Base Feat.) Single CRF(All Feat.) (Sha and Pereira, 2003) (Kudo and Matsumoto, 2001) (Ando and Zhang, 2005) Combined CRF F1 89.60 94.34 94.38 94.39 94.70 94.77 Table 3: Results for the CoNLL 2000 Chunking Task. The bagged CRF performs significantly better than a single CRF (McNemar’s test; p < 0.01), and equals the results of (Ando and Zhang, 2005), who use a large amount of unlabeled data. wt = w wt begins with a capital letter wt contains only capital letters wt is a single capital letter wt contains some capital letters and some lowercase wt contains a numeric character wt contains only numeric characters wt appears to be a number wt is a string of at least two periods wt ends with a period wt c"
N06-1012,N03-1028,0,0.604741,"trained on subsets of the original features, and combined using a mixture model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models. The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modeled, The network had no problem lear"
N06-1012,W06-2918,0,0.102733,"Missing"
N06-1012,W03-0423,0,0.0285762,"re model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models. The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modeled, The network had no problem learning and then driving autonomously in one direction, but when drivin"
N06-1012,W03-0425,0,0.0437134,"combined using a mixture model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models. The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modeled, The network had no problem learning and then driving autonomously in one direc"
N06-1012,N01-1025,0,0.0162951,"Missing"
N06-1012,P05-1003,0,0.502188,"ng involves complex trade-offs among weights, which can be dangerous: a few highlyindicative features can swamp the contribution of many individually weaker features, causing their weights to be undertrained. Such a model is less robust, for the highly-indicative features may be noisy or missing in the test data. To ameliorate this weight undertraining, we introduce several new feature bagging methods, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is tra"
N06-1012,W04-3201,0,0.0141115,"include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models. The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modeled, The network had no problem learning and then driving autonomously in one direction, but when driving the other way, the network was erratic, swervin"
N06-1012,N03-1033,0,0.0224587,"hods, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts. These methods include the logarithmic opinion pools used by Smith et al. (2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features. 1 Introduction Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004). A discriminative probabilistic model is trained to maximize the conditional probability p(y|x) of output labels y given input variables x, as opposed to modeling the joint probability p(y, x), as in generative models such as the Naive Bayes classifier and hidden Markov models. The popularity of discriminative models stems from the great flexibility they allow in defining features: because the distribution over input features p(x) is not modele"
N06-1038,N04-4028,1,0.752258,"und in this automatically generated database. In this manner, we create a closed-loop system that alternates between bottomup extraction and top-down pattern discovery. This approach can be viewed as a type of alternating optimization, with analogies to formal methods such as expectation-maximization. The uncertainty in the bottom-up extraction step is handled by estimating the confidence of each extraction and pruning the database to remove entries with low confidence. One of the benefits of 299 a probabilistic extraction model is that confidence estimates can be straight-forwardly obtained. Culotta and McCallum (2004) describe the constrained forward-backward algorithm to efficiently estimate the conditional probability that a segment of text is correctly extracted by a CRF. Using this algorithm, we associate a confidence value with each relation extracted by the CRF. This confidence value is then used to limit the noise introduced by incorrect extractions. This differs from Nahm and Mooney (2000) and Mooney and Bunescu (2005), in which standard decision tree rule learners are applied to the unfiltered output of extraction. 4.3 Extracting Implicit Relations An implicit relation is one that does not have di"
N06-1038,P04-1054,1,0.23265,"l pass to label each entity as a person or company. Instead, an entity’s label is its relation to the principal entity. Below is an example of a labeled article: George W. Bush George is the son of George H. W. Bush Relation extraction is the task of discovering semantic connections between entities. In text, this usually amounts to examining pairs of entities in a document and determining (from local language cues) whether a relation exists between them. Common approaches to this problem include pattern matching (Brin, 1998; Agichtein and Gravano, 2000), kernel methods (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006), logistic regression (Kambhatla, 2004), and augmented parsing (Miller et al., 2000). The pairwise classification approach of kernel methods and logistic regression is commonly a two297 | and Barbara | {z Bush}. mother {z father } Additionally, by using a sequence model we can capture the dependence between adjacent labels. For example, in our data it is common to see phrases such as “son of the Republican president George H. W. Bush” for which the labels politicalParty, jobTitle, and father occur consecutively. Sequence models are specifically designed to handle the"
N06-1038,P99-1001,0,0.0496623,"be included probabilistically into extraction to improve its accuracy; also, our work focuses on mining from relational graphs, rather than single-table databases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling phase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumeratin"
N06-1038,P05-1060,0,0.0403393,"mplied by the text will dramatically increase the amount of information a user can uncover, effectively providing access to the implications of a corpus. We argue that integrating top-down and bottomup knowledge discovery algorithms discussed in Section 4.2 can enable this technology. By performing pattern discovery in conjunction with information extraction, we can collate facts from multiple sources to infer new relations. This is an example of cross-document fusion or cross-document information extraction, a growing area of research transforming raw extractions into usable knowledge bases (Mann and Yarowsky, 2005; Masterson and Kushmerik, 2003). 5 Experiments 5.1 Data We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia1 and labeled a to1 http://www.wikipedia.org Prescott Bush Hillary Clinton son husband George H. W. Bush party executive education education son Harken Energy education underling jobTitle education Bill Clinton rival George W. Bush party Democrat underling jobTitle education Dick Cheney education Yale executive rival jobTitle party President party party award Halliburton Republican Bob Dole wife Elizabeth Dole party participant party award WWII Nelson Rock"
N06-1038,A00-2030,0,0.142968,"atabases. McCallum and Jensen (2003) argue the theoretical benefits of an integrated probabilistic model for extraction and mining, but do not construct such a system. Our work is a step in the direction of their proposal, using an inference procedure based on a closed-loop iteration between extraction and relational pattern discovery. Most other work in this area mines raw text, rather than a database automatically populated via extraction (Hearst, 1999; Craven et al., 1998). This work can also be viewed as part of a trend to perform joint inference across multiple language processing tasks (Miller et al., 2000; Roth and tau Yih, 2002; Sutton and McCallum, 2004). Finally, using relational paths between entities is also examined in (Richards and Mooney, 1992) to escape local maxima in a first-order learning system. 3 Relation Extraction as Sequence Labeling phase method: first the entities in a document are identified, then a relation type is predicted for each pair of entities. This approach presents at least two difficulties: (1) enumerating all pairs of entities, even when restricted to pairs within a sentence, results in a low density of positive relation examples; and (2) errors in the entity re"
N06-1038,C02-1151,0,0.00930997,"Missing"
N07-1011,P04-1015,0,0.0152544,"prediction and training. However, whereas we explicitly use a ranking-based loss function, LASO uses a binary classification loss function that labels each candidate structure as correct or incorrect. Thus, each LASO training example contains all candidate predictions, whereas our training examples contain only the highest scoring incorrect prediction and the highest scoring correct prediction. Our experiments show the advantages of this ranking-based loss function. Additionally, we provide an empirical study to quantify the effects of different example generation and loss function decisions. Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses “early update” to update the parameters when an error is encountered. Our method uses a similar “early update” in that training examples are only generated for the first mistake made during prediction. However, they do not investigate rank-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are"
N07-1011,H05-1013,0,0.429184,"Missing"
N07-1011,P05-1045,0,0.0584295,"es of this ranking-based loss function. Additionally, we provide an empirical study to quantify the effects of different example generation and loss function decisions. Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses “early update” to update the parameters when an error is encountered. Our method uses a similar “early update” in that training examples are only generated for the first mistake made during prediction. However, they do not investigate rank-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions. There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002). Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms. Luo et al. (2004) do enable features"
N07-1011,P04-1018,0,0.557288,"Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions. There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002). Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms. Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work. Denis and Baldridge (2007) use a ranking loss function for pronoun coreference; however the examples are still pairs of pronouns, and the example generation is not error driven. Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of first87 order logic. Also, their method is neither"
N07-1011,P02-1014,0,0.917452,"y where Zx is the input-dependent normalizer and factor fc parameterizes the pairwisePnoun phrase compatibility as fc (yij , xij ) = exp( k λk fk (yij , xij )). Factor ft enforces the transitivity constraints by ft (·) = −∞ if transitivity is not satisfied, 1 otherwise. This is similar to the model presented in McCallum and Wellner (2005). A factor graph for the Pairwise Model is presented in Figure 2 for three noun phrases. For the First-Order model, an undirected graphical model can be defined as P (y|x) = 1 Y fc (yj , xj ) Zx y ∈y j Y ft (yj , xj ) Features We follow Soon et al. (2001) and Ng and Cardie (2002) to generate most of our features for the Pairwise Model. These include: • Match features - Check whether gender, number, head text, or entire phrase matches • Aliases - Heuristically decide if one noun is the acronym of the other where Zx is the input-dependent normalizer and factor fc parameterizes the cluster-wise noun P phrase compatibility as Again, fc (yj , xj ) = exp( k λk fk (yj , xj )). factor ft enforces the transitivity constraints by ft (·) = −∞ if transitivity is not satisfied, 1 otherwise. Here, transitivity is a bit more complicated, since it also requires that if yj = 1, then f"
N07-1011,P05-1020,0,0.543324,"en a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002). Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms. Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work. Denis and Baldridge (2007) use a ranking loss function for pronoun coreference; however the examples are still pairs of pronouns, and the example generation is not error driven. Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of first87 order logic. Also, their method is neither errordriven nor rank-based. McCallum and Wellner (2003) use a conditional random field that factors into a product of pairwise decisions about pairs of nouns. These pairwise decisions are made collectively using relational inference; however, as pointed out in Milch et al. (2004), this model has limited representationa"
N07-1011,W06-1633,0,0.141639,"nk-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions. There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002). Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms. Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work. Denis and Baldridge (2007) use a ranking loss function for pronoun coreference; however the examples are still pairs of pronouns, and the example generation is not error driven. Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. While in theory a metaclassifier can flexibly represent features, they do not explore features usi"
N07-1011,W04-2401,0,0.0181965,"eneration and loss function decisions. Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses “early update” to update the parameters when an error is encountered. Our method uses a similar “early update” in that training examples are only generated for the first mistake made during prediction. However, they do not investigate rank-based loss functions. Others have attempted to train global scoring functions using Gibbs sampling (Finkel et al., 2005), message propagation, (Bunescu and Mooney, 2004; Sutton and McCallum, 2004), and integer linear programming (Roth and Yih, 2004). The main distinctions of our approach are that it is simple to implement, not computationally intensive, and adaptable to arbitrary loss functions. There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002). Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms. Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work. Denis and Baldr"
N07-1011,J01-4004,0,0.949603,"log-likelihood of the labeled data. Two critical decisions for this method are (1) how to sample the training data, and (2) how to combine the pairwise predictions at test time. Systems often perform better when these decisions complement each other. Given a data set in which noun phrases have been manually clustered, the training data can be created by simply enumerating over each pair of noun phrases xij , where yij is true if xi and xj are in the same cluster. However, this approach generates a highly unbalanced training set, with negative examples outnumbering positive examples. Instead, Soon et al. (2001) propose the following sampling method: Scan the document from left to right. Compare each noun phrase xi to each preceding noun phrase xj , scanning from right to left. For each pair xi , xj , create a training instance hxij , yij i, where yij is 1 if xi and xj are coreferent. The scan for xj terminates when a positive example is constructed, or the beginning of the document is reached. This results in a training set that has been pruned of distant noun phrase pairs. At testing time, we can construct an undirected, weighted graph in which vertices correspond to noun phrases and edge weights a"
N07-1011,P04-1056,0,\N,Missing
N07-2028,P06-1027,0,0.212037,". It has previously been demonstrated to provide positive results in linear-chain CRFs, but the published method for calculating the entropy gradient requires significantly more computation than supervised CRF training. This paper presents a new derivation and dynamic program for calculating the entropy gradient that is significantly more efficient—having the same asymptotic time complexity as supervised CRF training. We also present efficient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications. 1 Jiao et al. (2006) apply this method to linearchain CRFs and demonstrate encouraging accuracy improvements on a gene-name-tagging task. However, the method they present for calculating the gradient of the entropy takes substantially greater time than the traditional supervised-only gradient. Whereas supervised training requires only classic forward/backward, taking time O(ns2 ) (sequence length times the square of the number of labels), their training method takes O(n2 s3 )—a factor of O(ns) more. This greatly reduces the practicality of using large amounts of unlabeled data, which is exactly the desired use-ca"
N07-2028,N06-2018,0,0.0834201,"ming experiments show that this method takes approximately 1.5 times as long as traditional supervised training—less than the constant factors would suggest.1 In practice, since the three extra dynamic programs do not require recalculation of the dot-product between parameters and input features (typically the most expensive part of inference), they are significantly faster than calculating the original forward/backward lattice. 5 Confidence Estimation In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confidence estimation. Kim et al. (2006) propose using entropy as a confidence estimator in active learning in CRFs, where examples with the most uncertainty are selected for presentation to humans labelers. In practice, they approximate the entropy of the labels given the N-best labels. Not only could our method quickly and exactly compute the true entropy, but it could also be used to find the subsequence that has the highest uncertainty, which could further reduce the additional human tagging effort. 6 Related Work Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similari"
N10-1111,P07-1036,0,0.584164,"the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data, and the preference of the domain expert when"
N10-1111,W02-1001,0,0.0567544,"putational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 1 Introduction Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before"
N10-1111,N07-1011,1,0.891642,"Missing"
N10-1111,D09-1134,0,0.0115376,"ser-defined constraints. By directly incorporating the model score and the constraints (as in Fmc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each 731 instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the graph, which slow down inference. Our approach directly incorporates these constraints into the ranking function, with no impact on inference time. 5 Experiments We carried out experiments on the Cora citation dataset. The task is to segment each citation into different fields, such as “author” and “title”. We use 300 instances as training data, 100 ins"
N10-1111,P08-1099,1,0.957678,"generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data,"
N13-1008,P07-1073,0,0.832835,"r employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditi"
N13-1008,P04-1054,0,0.032567,"e importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof-the-art distant supervision. 1 Introduction Most previous work in relation extraction uses a predefined, finite and fixed schema of relation types (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by usi"
N13-1008,P04-1053,0,0.00631927,"s us |O |fact pairs hf + , f − i, and for each pair we do an SGD update using the corresponding gradients of Objf + ,f − . For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approa"
N13-1008,P11-1055,0,0.874455,"model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. Never-Ending Learning and Bootstrapp"
N13-1008,P09-1113,0,0.979877,"s (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual relations is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H"
N13-1008,C12-1118,0,0.046746,"or us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent work on non-negative sparse embeddings (Murphy et al., 2012). In our problem columns correspond to relations, and rows correspond to entity tuples. By contrast, there columns are words, and rows are contextual features such as “words in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall fo"
N13-1008,N07-1071,0,0.0225058,"ve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at 74 Proceedings of NAACL-HLT 2013, pages 74–84, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguist"
N13-1008,D08-1009,0,0.00990576,"Missing"
N13-1008,D10-1106,0,0.0119867,"in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall for not-so-frequently mentioned facts. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed approach (Schoenmackers et al., 2008; Schoenmackers et al., 2010). However, their approach is substantially more complex, requires a categorization of entities into fine grained entity types, and needs inference in high tree-width Markov Networks. By contrast, our approach is based on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expressi"
N13-1008,N06-1039,0,0.0107607,"+ , f − i, and for each pair we do an SGD update using the corresponding gradients of Objf + ,f − . For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption"
N13-1008,D12-1042,0,0.926802,"ity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. Never-Ending Learning and Bootstrapping Our latent feature m"
N13-1008,C08-1107,0,0.00962304,"ly different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent work on non-negative sparse"
N13-1008,W04-3206,0,0.00865004,"Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics communit"
N13-1008,D11-1135,1,0.657866,"ervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find F ERGUSON–historian-at–H ARVARD but does not know F ERGUSON–is-a-professor-at– H ARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at 74 Proceedings of NAACL-HLT 2013, pages 74–84, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics certainly does not imply scientist-at. I"
N13-1008,W12-3022,1,0.842101,"that the neighborhood model amounts to a collection of local log-linear classifiers, one for each relation r with feature functions fr,r0 (t) = I [r0 6= r ∧ (r0 , t) ∈ O] and weights wr . This means that in contrast to model F, this model cannot harness any synergies between textual and pre-existing DB relations. 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity e we introduce a latent feature vector te of dimension K E . In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the"
N13-1008,P12-1075,1,0.691635,"that the neighborhood model amounts to a collection of local log-linear classifiers, one for each relation r with feature functions fr,r0 (t) = I [r0 6= r ∧ (r0 , t) ∈ O] and weights wr . This means that in contrast to model F, this model cannot harness any synergies between textual and pre-existing DB relations. 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity e we introduce a latent feature vector te of dimension K E . In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the"
N13-1008,P06-1107,0,0.00843022,"roach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in 78 the lexical semantics community, from LSA to recent w"
N16-1103,D14-1067,0,0.0820163,"Missing"
N16-1103,P07-1073,0,0.052685,"Missing"
N16-1103,P11-1055,0,0.277215,"Missing"
N16-1103,P14-1062,0,0.0080551,"Missing"
N16-1103,D14-1181,0,0.012117,"Missing"
N16-1103,2005.mtsummit-papers.11,0,0.0412202,"Missing"
N16-1103,D11-1049,0,0.0928618,"Missing"
N16-1103,D12-1093,0,0.153824,"Missing"
N16-1103,D15-1278,0,0.0243643,"Missing"
N16-1103,W15-1521,0,0.0209553,"Missing"
N16-1103,N13-1095,0,0.0616474,"Missing"
N16-1103,P09-1113,0,0.390475,"Missing"
N16-1103,P15-1016,1,0.625617,"Missing"
N16-1103,N13-1008,1,0.50869,"Missing"
N16-1103,N15-1118,0,0.0467431,"Missing"
N16-1103,E14-2023,1,0.894067,"Missing"
N16-1103,D12-1042,0,0.423396,"Missing"
N16-1103,D15-1174,0,0.521772,"Missing"
N16-1103,D15-1206,0,0.0359304,"Missing"
N16-1103,D10-1099,1,0.641519,"Missing"
N16-1103,N07-1016,0,0.0372461,"Missing"
N16-1103,D15-1203,0,0.185782,"Missing"
N16-1103,N15-1184,0,\N,Missing
N16-1103,D14-1044,0,\N,Missing
N16-1103,N15-1151,0,\N,Missing
N16-1103,P15-1061,0,\N,Missing
N18-1045,E12-1004,0,0.186881,"Gaussian embedding are determined by the training set of HypeNet. Other experimental details are described in our supplementary materials. The embeddings are tested on 11 datasets. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vuli´c et al., 2016) are also evaluated. The F1 and accuracy measurements are sometimes very similar even though the quality of prediction varies, so we adopted average precision, AP@all (Zhu, 2004) (equivalent to the area under the precision-recall curve when the constant interpolation is used), as the mai"
N18-1045,W11-2501,0,0.184725,"al., 2009) because DIH holds more often in this subset (i.e. SBOW works better) compared with that in the whole WaCkypedia corpus. The window size |W |of DIVE and Gaussian embedding are set as 20 (left 10 words and right 10 words). The number of embedding dimensions in DIVE L is set to be 100. The other hyper-parameters of DIVE and Gaussian embedding are determined by the training set of HypeNet. Other experimental details are described in our supplementary materials. The embeddings are tested on 11 datasets. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and"
N18-1045,W14-1610,0,0.0231701,"ensions in DIVE L is set to be 100. The other hyper-parameters of DIVE and Gaussian embedding are determined by the training set of HypeNet. Other experimental details are described in our supplementary materials. The embeddings are tested on 11 datasets. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vuli´c et al., 2016) are also evaluated. The F1 and accuracy measurements are sometimes very similar even though the quality of prediction varies, so we adopted average precision, AP@all (Zhu, 2004) (equivalent to the area under the"
N18-1045,Q15-1016,0,0.241834,"co-occurred context words while discarding their counts. In this study, we define the inclusion property based on counts of context words in (1) because the counts are an effective and noise-robust feature for the hypernymy detection using only the context distribution of words (Clarke, 2009; Vuli´c et al., 2016; Shwartz et al., 2017). Our goal is to produce lower-dimensional embeddings preserving the inclusion property that the embedding of hypernym y is larger than or equal to the embedding of its hyponym x in every dimension. Formally, the desired property can be written as Recent studies (Levy et al., 2015b; Shwartz et al., 2017) have underscored the difficulty of generalizing supervised hypernymy annotations to unseen pairs — classifiers often effectively memorize prototypical hypernyms (‘general’ words) and ignore relations between words. These findings motivate us to develop more accurate and scalable unsupervised embeddings to detect hypernymy and propose several scoring functions to analyze the embeddings from different perspectives. 1.1 Method Contributions • A novel unsupervised low-dimensional embedding method via performing non-negative matrix factorization (NMF) on a weighted PMI matr"
N18-1045,W09-0215,0,0.533938,"ulary, and #(x, c) indicates the number of times that word x and its context word c co-occur in a small window with size |W |in the corpus of interest D. Notice that the concept of DIH could be applied to different context word representations. For example, Geffet and Dagan (2005) represent each word by the set of its co-occurred context words while discarding their counts. In this study, we define the inclusion property based on counts of context words in (1) because the counts are an effective and noise-robust feature for the hypernymy detection using only the context distribution of words (Clarke, 2009; Vuli´c et al., 2016; Shwartz et al., 2017). Our goal is to produce lower-dimensional embeddings preserving the inclusion property that the embedding of hypernym y is larger than or equal to the embedding of its hyponym x in every dimension. Formally, the desired property can be written as Recent studies (Levy et al., 2015b; Shwartz et al., 2017) have underscored the difficulty of generalizing supervised hypernymy annotations to unseen pairs — classifiers often effectively memorize prototypical hypernyms (‘general’ words) and ignore relations between words. These findings motivate us to devel"
N18-1045,N15-1098,0,0.2426,"co-occurred context words while discarding their counts. In this study, we define the inclusion property based on counts of context words in (1) because the counts are an effective and noise-robust feature for the hypernymy detection using only the context distribution of words (Clarke, 2009; Vuli´c et al., 2016; Shwartz et al., 2017). Our goal is to produce lower-dimensional embeddings preserving the inclusion property that the embedding of hypernym y is larger than or equal to the embedding of its hyponym x in every dimension. Formally, the desired property can be written as Recent studies (Levy et al., 2015b; Shwartz et al., 2017) have underscored the difficulty of generalizing supervised hypernymy annotations to unseen pairs — classifiers often effectively memorize prototypical hypernyms (‘general’ words) and ignore relations between words. These findings motivate us to develop more accurate and scalable unsupervised embeddings to detect hypernymy and propose several scoring functions to analyze the embeddings from different perspectives. 1.1 Method Contributions • A novel unsupervised low-dimensional embedding method via performing non-negative matrix factorization (NMF) on a weighted PMI matr"
N18-1045,D16-1146,0,0.0500679,"Missing"
N18-1045,P15-1144,0,0.0514779,"Missing"
N18-1045,C12-1118,0,0.0155741,"would get higher or equal positive gradients from the first term than x in every dimension because #(x, c) ≤ #(y, c). This means Equation (1) tends to imply Equation (2) because the hypernym has larger gradients everywhere in the embedding space. Combining the analysis from the matrix factorization viewpoint, DIH in Equation (1) is approximately equivalent to the inclusion property in DIVE (i.e. Equation (2)). 2.3 Interpretability After applying the non-negativity constraint, we observe that each latent factor in the embedding is interpretable as previous findings suggest (Pauca et al., 2004; Murphy et al., 2012) (i.e. each dimension roughly corresponds to a topic). Furthermore, DIH suggests that a general word appears in more diverse contexts/topics. By preserving DIH using inclusion shift, the embedding of a general word (i.e. hypernym of many other words) tends to have larger values in these dimensions (topics). This gives rise to a natural and intuitive interpretation of our word embeddings: the word embeddings can be seen as unnormalized probability distributions over topics. In Figure 1, we visualize the unnormalized topical distribution of two words, rodent and mammal, as an example. Since rode"
N18-1045,P05-1014,0,0.822172,"s.edu Abstract Word2Vec (Mikolov et al., 2013), among other approaches based on matrix factorization (Levy et al., 2015a), successfully compress the SBOW into a much lower dimensional embedding space, increasing the scalability and applicability of the embeddings while preserving (or even improving) the correlation of geometric embedding similarities with human word similarity judgments. While embedding models have achieved impressive results, context distributions capture more semantic information than just word similarity. The distributional inclusion hypothesis (DIH) (Weeds and Weir, 2003; Geffet and Dagan, 2005; Cimiano et al., 2005) posits that the context set of a word tends to be a subset of the contexts of its hypernyms. For a concrete example, most adjectives that can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle (e.g. both can be obedient). However, the converse is not necessarily true — a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection (Weeds and Weir, 2003; Geffet and Dagan, 20"
N18-1045,D17-1185,0,0.191408,"Missing"
N18-1045,D08-1097,0,0.0481205,"both can be obedient). However, the converse is not necessarily true — a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection (Weeds and Weir, 2003; Geffet and Dagan, 2005; Shwartz et al., 2017). Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entailment (Sammons et al., 2011), coreference (Ponzetto and Strube, 2006), relation extraction (Demeester et al., 2016) and question answering (Huang et al., 2008). Leveraging the variety of contexts and inclusion properties in context distributions can greatly increase the ability to discover taxonomic structure among words (Shwartz et al., 2017). The inability to preserve these features limits the semantic representation power and downstream applicability of some popular unsupervised learning approaches such as Word2Vec. Several recently proposed methods aim to enModeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, coreference, relation extraction, and question answering. Supervised lea"
N18-1045,P15-2020,0,0.388543,"o indicate that unsupervised scoring functions which combine similarity and generality measurements work the best in general, but no one scoring function dominates across all datasets. A combination of unsupervised DIVE with the proposed scoring functions produces new state-of-the-art performances on many datasets in the unsupervised regime. ple baseline only relying on word frequency can achieve good results. Follow-up work models contexts by a mixture of Gaussians (Athiwaratkun and Wilson, 2017) relaxing the unimodality assumption but achieves little improvement on hypernym detection tasks. Kiela et al. (2015) show that images retrieved by a search engine can be a useful source of information to determine the generality of lexicons, but the resources (e.g. pre-trained image classifier for the words of interest) might not be available in many domains. Order embedding (Vendrov et al., 2016) is a supervised approach to encode many annotated hypernym pairs (e.g. all of the whole WordNet (Miller, 1995)) into a compact embedding space, where the embedding of a hypernym should be smaller than the embedding of its hyponym in every dimension. Our method learns embedding from raw text, where a hypernym embed"
N18-1045,N06-1025,0,0.0676959,"at can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle (e.g. both can be obedient). However, the converse is not necessarily true — a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection (Weeds and Weir, 2003; Geffet and Dagan, 2005; Shwartz et al., 2017). Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entailment (Sammons et al., 2011), coreference (Ponzetto and Strube, 2006), relation extraction (Demeester et al., 2016) and question answering (Huang et al., 2008). Leveraging the variety of contexts and inclusion properties in context distributions can greatly increase the ability to discover taxonomic structure among words (Shwartz et al., 2017). The inability to preserve these features limits the semantic representation power and downstream applicability of some popular unsupervised learning approaches such as Word2Vec. Several recently proposed methods aim to enModeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, su"
N18-1045,D16-1234,0,0.247715,"tributional sense is often taken to be the set of textual contexts (nearby tokens) in which that word appears, represented as a large sparse bag of words (SBOW). Without any supervision, 485 Proceedings of NAACL-HLT 2018, pages 485–495 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 code hypernym relations between words in dense embeddings, such as Gaussian embedding (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017), Boolean Distributional Semantic Model (Kruszewski et al., 2015), order embedding (Vendrov et al., 2016), H-feature detector (Roller and Erk, 2016), HyperVec (Nguyen et al., 2017), dual tensor (Glavaˇs and Ponzetto, 2017), Poincar´e embedding (Nickel and Kiela, 2017), and LEAR (Vuli´c and Mrkˇsi´c, 2017). However, the methods focus on supervised or semisupervised settings where a massive amount of hypernym annotations are available (Vendrov et al., 2016; Roller and Erk, 2016; Nguyen et al., 2017; Glavaˇs and Ponzetto, 2017; Vuli´c and Mrkˇsi´c, 2017), do not learn from raw text (Nickel and Kiela, 2017) or lack comprehensive experiments on the hypernym detection task (Vilnis and McCallum, 2015; Athiwaratkun and Wilson, 2017). The distribu"
N18-1045,W15-4208,0,0.117787,"ften in this subset (i.e. SBOW works better) compared with that in the whole WaCkypedia corpus. The window size |W |of DIVE and Gaussian embedding are set as 20 (left 10 words and right 10 words). The number of embedding dimensions in DIVE L is set to be 100. The other hyper-parameters of DIVE and Gaussian embedding are determined by the training set of HypeNet. Other experimental details are described in our supplementary materials. The embeddings are tested on 11 datasets. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vuli´c et"
N18-1045,P16-1226,0,0.155894,"ts. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vuli´c et al., 2016) are also evaluated. The F1 and accuracy measurements are sometimes very similar even though the quality of prediction varies, so we adopted average precision, AP@all (Zhu, 2004) (equivalent to the area under the precision-recall curve when the constant interpolation is used), as the main evaluation metric. The HyperLex dataset has a continuous score on each candidate word pair, so we adopt Spearman rank coefficient ρ (Fieller et al., 1957) as suggested by the re"
N18-1045,E17-1007,0,0.357482,"miano et al., 2005) posits that the context set of a word tends to be a subset of the contexts of its hypernyms. For a concrete example, most adjectives that can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle (e.g. both can be obedient). However, the converse is not necessarily true — a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection (Weeds and Weir, 2003; Geffet and Dagan, 2005; Shwartz et al., 2017). Hypernymy detection plays a key role in many challenging NLP tasks, such as textual entailment (Sammons et al., 2011), coreference (Ponzetto and Strube, 2006), relation extraction (Demeester et al., 2016) and question answering (Huang et al., 2008). Leveraging the variety of contexts and inclusion properties in context distributions can greatly increase the ability to discover taxonomic structure among words (Shwartz et al., 2017). The inability to preserve these features limits the semantic representation power and downstream applicability of some popular unsupervised learning approaches su"
N18-1045,N16-1142,0,0.0356801,"Missing"
N18-1045,C14-1212,0,0.258651,"in the whole WaCkypedia corpus. The window size |W |of DIVE and Gaussian embedding are set as 20 (left 10 words and right 10 words). The number of embedding dimensions in DIVE L is set to be 100. The other hyper-parameters of DIVE and Gaussian embedding are determined by the training set of HypeNet. Other experimental details are described in our supplementary materials. The embeddings are tested on 11 datasets. The first 4 datasets come from the recent review of Shwartz et al. (2017)1 : BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), Lenci/Benotto (Benotto, 2015), and Weeds (Weeds et al., 2014). The next 4 datasets are downloaded from the code repository of the H-feature detector (Roller and Erk, 2016)2 : Medical (i.e., Levy 2014) (Levy et al., 2014), LEDS (also referred to as ENTAILMENT or Baroni 2012) (Baroni et al., 2012), TM14 (i.e., Turney 2014) (Turney and Mohammad, 2015), and Kotlerman 2010 (Kotlerman et al., 2010). In addition, the performance on the test set of HypeNet (Shwartz et al., 2016) (using the random train/test split), the test set of WordNet (Vendrov et al., 2016), and all pairs in HyperLex (Vuli´c et al., 2016) are also evaluated. The F1 and accuracy measurements"
N18-1045,W03-1011,0,0.109659,"uke, mccallum}@cs.umass.edu Abstract Word2Vec (Mikolov et al., 2013), among other approaches based on matrix factorization (Levy et al., 2015a), successfully compress the SBOW into a much lower dimensional embedding space, increasing the scalability and applicability of the embeddings while preserving (or even improving) the correlation of geometric embedding similarities with human word similarity judgments. While embedding models have achieved impressive results, context distributions capture more semantic information than just word similarity. The distributional inclusion hypothesis (DIH) (Weeds and Weir, 2003; Geffet and Dagan, 2005; Cimiano et al., 2005) posits that the context set of a word tends to be a subset of the contexts of its hypernyms. For a concrete example, most adjectives that can be applied to poodle can also be applied to dog, because dog is a hypernym of poodle (e.g. both can be obedient). However, the converse is not necessarily true — a dog can be straight-haired but a poodle cannot. Therefore, dog tends to have a broader context set than poodle. Many asymmetric scoring functions comparing SBOW features based on DIH have been developed for hypernymy detection (Weeds and Weir, 20"
N18-1080,S17-2097,0,0.0377199,"ctivity affects_response decreases_activity affects_transport increases_reaction decreases_reaction decreases_MP P F1 46.7 ± 0.39 47.3 ± 0.19 90.9 ± 0.13 92.6 ± 0.12 Table 8: Precision, recall, and F1 results for CTD named entity recognition and relation extraction, comparing BPE to word-level tokenization. work approaches to relation extraction have focused on CNNs (dos Santos et al., 2015; Zeng et al., 2015) or LSTMs (Miwa and Bansal, 2016; Verga et al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, and more complex events (Kim et al., 2008; Riedel et al., 2011). Our work focuses on modeling relations between chemicals, diseases, genes and proteins, where available annotation is often at the document-"
N18-1080,E17-1013,1,0.854389,"(2) and that of ti is in Rd . 2.3 (0) ti and (1) ti are in R 4d Bi-affine Pairwise Scores (B) We project each contextually encoded token bi through two separate MLPs to generate two new versions of each token corresponding to whether it will serve as the first (head) or second (tail) argument of a relation: (1) (0) (B) ehead = Whead (ReLU(Whead bi i etail = i The LogSumExp scoring function is a smooth approximation to the max function and has the benefits of aggregating information from multiple predictions and propagating dense gradients as opposed to the sparse gradient updates of the max (Das et al., 2017). 2.5 Named Entity Recognition In addition to pairwise relation predictions, we (B) use the Transformer output bi to make entity (B) type predictions. We feed bi as input to a linear classifier which predicts the entity label for each token with per-class scores ci : )) (1) (0) (B) Wtail (ReLU(Wtail bi )) (B) ci = W (3) bi We use a bi-affine operator to calculate an N ×L×N tensor A of pairwise affinity scores, scoring each (head, relation, tail) triple: We augment the entity type labels with the BIO encoding to denote entity spans. We apply tags to the byte-pair tokenization by treating each s"
N18-1080,doddington-etal-2004-automatic,0,0.0554881,"11 tokens apart. We consider removing entity pair candidates with distances of 11, 25, 50, 100 and 500 (the maximum document length). The average sentence length is 22 tokens. We see that the model is not simply relying on short range relationships, but is leveraging information about distant entity pairs, with accuracy increasing as the maximum distance considered increases. Note that all results are taken from the same model trained on the full unfiltered training set. 4 Related work Relation extraction is a heavily studied area in the NLP community. Most work focuses on news and web data (Doddington et al., 2004; Riedel et al., 2010; Hendrickx et al., 2009).9 Recent neural net9 878 And TAC KBP: https://tac.nist.gov R F1 44.8 34.0 50.2 29.8 47.3 31.7 46.2 55.7 57.9 67.1 51.3 60.8 42.2 52.6 44.4 10.1 43.0 15.8 39.7 26.3 34.4 24.5 40.9 30.8 28.7 12.8 12.3 28.9 48.0 35.5 32.9 24.7 35.5 19.4 23.8 5.6 5.7 7.0 43.3 29.9 33.4 24.4 37.4 23.5 25.8 7.4 7.4 11.0 0.6 0.4 0.3 0.2 0.1 0.0 chem_gene chem_disease gene_disease Dataset all Figure 2: Performance on the CTD dataset when restricting candidate entity pairs by distance. The x-axis shows the coarse-grained relation type. The y-axis shows F1 score. Different"
N18-1080,P15-1061,0,0.123827,"3.6 11 25 50 100 500 0.5 F1 Score Total Micro F1 Macro F1 Chemical/Disease marker/mechanism therapeutic Gene/Disease marker/mechanism therapeutic Chemical/Gene increases_expression increases_MP decreases_expression increases_activity affects_response decreases_activity affects_transport increases_reaction decreases_reaction decreases_MP P F1 46.7 ± 0.39 47.3 ± 0.19 90.9 ± 0.13 92.6 ± 0.12 Table 8: Precision, recall, and F1 results for CTD named entity recognition and relation extraction, comparing BPE to word-level tokenization. work approaches to relation extraction have focused on CNNs (dos Santos et al., 2015; Zeng et al., 2015) or LSTMs (Miwa and Bansal, 2016; Verga et al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 20"
N18-1080,W09-2415,0,0.318656,"pair candidates with distances of 11, 25, 50, 100 and 500 (the maximum document length). The average sentence length is 22 tokens. We see that the model is not simply relying on short range relationships, but is leveraging information about distant entity pairs, with accuracy increasing as the maximum distance considered increases. Note that all results are taken from the same model trained on the full unfiltered training set. 4 Related work Relation extraction is a heavily studied area in the NLP community. Most work focuses on news and web data (Doddington et al., 2004; Riedel et al., 2010; Hendrickx et al., 2009).9 Recent neural net9 878 And TAC KBP: https://tac.nist.gov R F1 44.8 34.0 50.2 29.8 47.3 31.7 46.2 55.7 57.9 67.1 51.3 60.8 42.2 52.6 44.4 10.1 43.0 15.8 39.7 26.3 34.4 24.5 40.9 30.8 28.7 12.8 12.3 28.9 48.0 35.5 32.9 24.7 35.5 19.4 23.8 5.6 5.7 7.0 43.3 29.9 33.4 24.4 37.4 23.5 25.8 7.4 7.4 11.0 0.6 0.4 0.3 0.2 0.1 0.0 chem_gene chem_disease gene_disease Dataset all Figure 2: Performance on the CTD dataset when restricting candidate entity pairs by distance. The x-axis shows the coarse-grained relation type. The y-axis shows F1 score. Different colors denote maximum distance cutoffs. Table"
N18-1080,Q16-1023,0,0.0751209,"17). Finally, our model considers all mention pairs simultaneously rather than a single mention pair at a time. We employ a bi-affine function to form pairwise predictions between mentions. Such models have also been used for knowledge graph link prediction (Nickel et al., 2011; Li et al., 2016b), with variations such as restricting the bilinear relation matrix to be diagonal (Yang et al., 2015) or diagonal and complex (Trouillon et al., 2016). Our model is similar to recent approaches to graph-based dependency parsing, where bilinear parameters are used to score head-dependent compatibility (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). 5 Conclusion We present a bi-affine relation attention network that simultaneously scores all mention pairs within a document. Our model performs well on three datasets, including two standard benchmark biological relation extraction datasets and a new, large and high-quality dataset introduced in this work. Our model out-performs the previous state of the art on the Biocreative V CDR dataset despite us879 ing no additional linguistic resources or mention pair-specific features. Our current model predicts only into a fixed schema of relations given by the data. Howe"
N18-1080,P16-1200,0,0.303735,"te an N ×L×N tensor A of pairwise affinity scores, scoring each (head, relation, tail) triple: We augment the entity type labels with the BIO encoding to denote entity spans. We apply tags to the byte-pair tokenization by treating each subword within a mention span as an additional token with a corresponding B- or I- label. Ailj = (ehead L)etail i j where L is a d × L × d tensor, a learned embedding matrix for each of the L relations. In subsequent sections we will assume we have transposed the dimensions of A as d × d × L for ease of indexing. 2.4 network attention (Verga and McCallum, 2016; Lin et al., 2016; Yaghoobzadeh et al., 2017). We aggregate over all representations for each mention pair in order to produce per-relation scores for each entity pair. For each entity pair (phead , ptail ), let P head denote the set of indices of mentions of the entity phead , and let P tail denote the indices of mentions of the entity ptail . Then we use the LogSumExp function to aggregate the relation scores from A across all pairs of mentions of phead and ptail : X scores(phead , ptail ) = log exp(Aij ) 2.6 Entity Level Prediction Our data is weakly labeled in that there are labels at the entity level but"
N18-1080,P09-1113,0,0.383868,"tion pair in order to produce per-relation scores for each entity pair. For each entity pair (phead , ptail ), let P head denote the set of indices of mentions of the entity phead , and let P tail denote the indices of mentions of the entity ptail . Then we use the LogSumExp function to aggregate the relation scores from A across all pairs of mentions of phead and ptail : X scores(phead , ptail ) = log exp(Aij ) 2.6 Entity Level Prediction Our data is weakly labeled in that there are labels at the entity level but not the mention level, making the problem a form of strong-distant supervision (Mintz et al., 2009). In distant supervision, edges in a knowledge graph are heuristically applied to sentences in an auxiliary unstructured text corpus — often applying the edge label to all sentences containing the subject and object of the relation. Because this process is imprecise and introduces noise into the training data, methods like multiinstance learning were introduced (Riedel et al., 2010; Surdeanu et al., 2012). In multi-instance learning, rather than looking at each distantly labeled mention pair in isolation, the model is trained over the aggregate of these mentions and a single update is made. Mo"
N18-1080,P16-1105,0,0.140875,"acro F1 Chemical/Disease marker/mechanism therapeutic Gene/Disease marker/mechanism therapeutic Chemical/Gene increases_expression increases_MP decreases_expression increases_activity affects_response decreases_activity affects_transport increases_reaction decreases_reaction decreases_MP P F1 46.7 ± 0.39 47.3 ± 0.19 90.9 ± 0.13 92.6 ± 0.12 Table 8: Precision, recall, and F1 results for CTD named entity recognition and relation extraction, comparing BPE to word-level tokenization. work approaches to relation extraction have focused on CNNs (dos Santos et al., 2015; Zeng et al., 2015) or LSTMs (Miwa and Bansal, 2016; Verga et al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, and more complex events (Kim et a"
N18-1080,Q17-1008,0,0.418953,"ll as additional information from 3.3 New CTD Dataset 3.3.1 Data Existing biological relation extraction datasets including both CDR (§3.1) and CPR (§3.2) are relatively small, typically consisting of hundreds or a few thousand annotated examples. Distant supervision datasets apply document-independent, entitylevel annotations to all sentences leading to a large proportion of incorrect labels. Evaluations on this data involve either very small (a few hundred) gold annotated examples or cross validation to predict the noisy, distantly applied labels (Mallory et al., 2015; Quirk and Poon, 2017; Peng et al., 2017). We address these issues by constructing a new dataset using strong-distant supervision containing document-level annotations. The Comparative Toxicogenomics Database (CTD) curates interactions between genes, chemicals, and diseases. Each relation in the CTD is associated with a disambiguated entity pair and a PubMed article where the relation was observed. To construct this dataset, we collect the abstracts for each of the PubMed articles with at least one curated relation in the CTD database. As in §3.1, we use PubTator to automatically tag and disambiguate the entities in each of these abs"
N18-1080,E17-1110,0,0.331949,"odel parameters, as well as additional information from 3.3 New CTD Dataset 3.3.1 Data Existing biological relation extraction datasets including both CDR (§3.1) and CPR (§3.2) are relatively small, typically consisting of hundreds or a few thousand annotated examples. Distant supervision datasets apply document-independent, entitylevel annotations to all sentences leading to a large proportion of incorrect labels. Evaluations on this data involve either very small (a few hundred) gold annotated examples or cross validation to predict the noisy, distantly applied labels (Mallory et al., 2015; Quirk and Poon, 2017; Peng et al., 2017). We address these issues by constructing a new dataset using strong-distant supervision containing document-level annotations. The Comparative Toxicogenomics Database (CTD) curates interactions between genes, chemicals, and diseases. Each relation in the CTD is associated with a disambiguated entity pair and a PubMed article where the relation was observed. To construct this dataset, we collect the abstracts for each of the PubMed articles with at least one curated relation in the CTD database. As in §3.1, we use PubTator to automatically tag and disambiguate the entities"
N18-1080,W11-1808,1,0.742567,"al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, and more complex events (Kim et al., 2008; Riedel et al., 2011). Our work focuses on modeling relations between chemicals, diseases, genes and proteins, where available annotation is often at the document- or abstract-level, rather than the sentence level. Some previous work exists on cross-sentence relation extraction. Swampillai and Stevenson (2011) and Quirk and Poon (2017) consider featurized classifiers over cross-sentence syntactic parses. Most similar to our work is that of Peng et al. (2017), which uses a variant of an LSTM to encode document-level syntactic parse trees. Our work differs in three key ways. First, we operate over raw tokens negatin"
N18-1080,N13-1008,1,0.91395,"Missing"
N18-1080,S13-2056,0,0.0425824,"tion. work approaches to relation extraction have focused on CNNs (dos Santos et al., 2015; Zeng et al., 2015) or LSTMs (Miwa and Bansal, 2016; Verga et al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, and more complex events (Kim et al., 2008; Riedel et al., 2011). Our work focuses on modeling relations between chemicals, diseases, genes and proteins, where available annotation is often at the document- or abstract-level, rather than the sentence level. Some previous work exists on cross-sentence relation extraction. Swampillai and Stevenson (2011) and Quirk and Poon (2017) consider featurized classifiers over cross-sentence syntactic parses. Most similar to our work is that of Peng et al. (2017), which uses a"
N18-1080,D12-1042,0,0.700077,"tions never occur in the same sentence, the above example expresses that the chemical entity azathioprine can cause the side effect fibrosis. Relation extraction models which consider only within-sentence relation pairs cannot extract this fact without knowledge of the complicated coreference relationship between eight and azathioprine treatment, which, without features from a complicated pre-processing pipeline, cannot be learned by a model which considers entity pairs in isolation. Making separate predictions for each mention pair also obstructs multi-instance learning (Riedel et al., 2010; Surdeanu et al., 2012), a technique which aggregates entity representations from mentions in order to improve robustness to noise in the data. Like the majority of relation extraction data, most annotation for biological relations is distantly supervised, and so we could benefit from a model which is amenable to multi-instance learning. In addition to this loss of cross-sentence and cross-mention reasoning capability, traditional mention pair relation extraction models typically introduce computational inefficiencies by independently extracting features for and scoring every pair of mentions, even when those mentio"
N18-1080,R11-1004,0,0.171703,"ecifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, and more complex events (Kim et al., 2008; Riedel et al., 2011). Our work focuses on modeling relations between chemicals, diseases, genes and proteins, where available annotation is often at the document- or abstract-level, rather than the sentence level. Some previous work exists on cross-sentence relation extraction. Swampillai and Stevenson (2011) and Quirk and Poon (2017) consider featurized classifiers over cross-sentence syntactic parses. Most similar to our work is that of Peng et al. (2017), which uses a variant of an LSTM to encode document-level syntactic parse trees. Our work differs in three key ways. First, we operate over raw tokens negating the need for part-of-speech or syntactic parse features which can lead to cascading errors. We also use a feed-forward neural architecture which encodes long sequences far more efficiently compared to the graph LSTM network of Peng et al. (2017). Finally, our model considers all mention"
N18-1080,N16-1103,1,0.873458,"Missing"
N18-1080,W16-1312,1,0.875074,"affine operator to calculate an N ×L×N tensor A of pairwise affinity scores, scoring each (head, relation, tail) triple: We augment the entity type labels with the BIO encoding to denote entity spans. We apply tags to the byte-pair tokenization by treating each subword within a mention span as an additional token with a corresponding B- or I- label. Ailj = (ehead L)etail i j where L is a d × L × d tensor, a learned embedding matrix for each of the L relations. In subsequent sections we will assume we have transposed the dimensions of A as d × d × L for ease of indexing. 2.4 network attention (Verga and McCallum, 2016; Lin et al., 2016; Yaghoobzadeh et al., 2017). We aggregate over all representations for each mention pair in order to produce per-relation scores for each entity pair. For each entity pair (phead , ptail ), let P head denote the set of indices of mentions of the entity phead , and let P tail denote the indices of mentions of the entity ptail . Then we use the LogSumExp function to aggregate the relation scores from A across all pairs of mentions of phead and ptail : X scores(phead , ptail ) = log exp(Aij ) 2.6 Entity Level Prediction Our data is weakly labeled in that there are labels at the"
N18-1080,E17-1111,0,0.0226332,"r A of pairwise affinity scores, scoring each (head, relation, tail) triple: We augment the entity type labels with the BIO encoding to denote entity spans. We apply tags to the byte-pair tokenization by treating each subword within a mention span as an additional token with a corresponding B- or I- label. Ailj = (ehead L)etail i j where L is a d × L × d tensor, a learned embedding matrix for each of the L relations. In subsequent sections we will assume we have transposed the dimensions of A as d × d × L for ease of indexing. 2.4 network attention (Verga and McCallum, 2016; Lin et al., 2016; Yaghoobzadeh et al., 2017). We aggregate over all representations for each mention pair in order to produce per-relation scores for each entity pair. For each entity pair (phead , ptail ), let P head denote the set of indices of mentions of the entity phead , and let P tail denote the indices of mentions of the entity ptail . Then we use the LogSumExp function to aggregate the relation scores from A across all pairs of mentions of phead and ptail : X scores(phead , ptail ) = log exp(Aij ) 2.6 Entity Level Prediction Our data is weakly labeled in that there are labels at the entity level but not the mention level, makin"
N18-1080,D15-1203,0,0.247282,"0.5 F1 Score Total Micro F1 Macro F1 Chemical/Disease marker/mechanism therapeutic Gene/Disease marker/mechanism therapeutic Chemical/Gene increases_expression increases_MP decreases_expression increases_activity affects_response decreases_activity affects_transport increases_reaction decreases_reaction decreases_MP P F1 46.7 ± 0.39 47.3 ± 0.19 90.9 ± 0.13 92.6 ± 0.12 Table 8: Precision, recall, and F1 results for CTD named entity recognition and relation extraction, comparing BPE to word-level tokenization. work approaches to relation extraction have focused on CNNs (dos Santos et al., 2015; Zeng et al., 2015) or LSTMs (Miwa and Bansal, 2016; Verga et al., 2016a; Zhou et al., 2016b) and replacing stage-wise information extraction pipelines with a single endto-end model (Miwa and Bansal, 2016; Ammar et al., 2017; Li et al., 2017). These models all consider mention pairs separately. There is also a considerable body of work specifically geared towards supervised biological relation extraction including protein-protein (Pyysalo et al., 2007; Poon et al., 2014; Mallory et al., 2015), drugdrug (Segura-Bedmar et al., 2013), and chemicaldisease (Gurulingappa et al., 2012; Li et al., 2016a) interactions, a"
N18-1080,P16-2034,0,0.569703,"st CTD Pos 1,038 1,012 1,066 26,657 Neg 4,280 4,136 4,270 146,057 Table 1: Data statistics for the CDR Dataset and additional data from CTD. Shows the total number of abstracts, positive examples, and negative examples for each of the data set splits. 3.1.2 Baselines We compare against the previous best reported results on this dataset not using knowledge base features.6 Each of the baselines are ensemble methods for within- and cross-sentence relations that make use of additional linguistic features (syntactic parse and part-of-speech). Gu et al. (2017) encode mention pairs using a CNN while Zhou et al. (2016a) use an LSTM. Both make cross-sentence predictions with featurized classifiers. 3.1.1 Data Preprocessing The CDR dataset is concerned with extracting only chemically-induced disease relationships (drugrelated side effects and adverse reactions) concerning the most specific entity in the document. For example tobacco causes cancer could be marked as false if the document contained the more specific lung cancer. This can cause true relations to be labeled as false, harming evaluation performance. To address this we follow (Gu et al., 2016, 2017) 4 Docs 500 500 500 15,448 http://www.biocreative"
N18-2021,P07-1036,0,0.0544826,"iguration is used in the scores reported. For the baseline that implements beam search, each citation is labeled by employing a beam search on the space of all tags for each token and their subsequent configurations, while keeping track of the best k configurations from one token to the next. This search is further augmented by restarting the search from the best k found after one complete search, for a total of 10 times and 10 random restarts. 5 Related Work Generalized Expectation (GE) (Mann and McCallum, 2010), Posterior Regularization (Ganchev et al., 2010) and Constraint Driven Learning (Chang et al., 2007) are among well-known approaches to learn from domain knowledge decomposed over a set of constraints or labeled features. However, these methods cannot learn from black box domain knowledge based score functions. Score functions of this type are abundant in 133 gold answers for the natural language query as in Iyyer et al (2017). Table 1: Comparison of R-SPEN with GE and different search algorithms in terms of token-level accuracy, test set average score, and time taken for inference during test time. Method GE Greedy Search 10 restarts 100 restarts 1000 restarts Beam Search k=2 k=5 k=10 R-SPE"
N18-2021,P17-1167,0,0.0321528,"Missing"
N18-2021,D17-1252,0,0.0411722,"Missing"
N18-2021,N10-1111,1,0.785399,"t al., 2017). output variable dependencies is replaced with a deep neural network that takes y and x as input and outputs a scalar energy score, but is able to learn much richer correlations than are typically captured in factor graphs. Inference in SPENs is performed by gradient descent in the energy, back-propagated to cause steps in a relaxed y space. Whereas previous training procedures for SPENs used labeled data, here we train SPENs from only unlabeled data plus human-coded domain knowledge in the form of a scoring function. We do so by building on SampleRank (Rohanimanesh et al., 2011; Singh et al., 2010), which enforces that the rank of two sampled ys according to the trained factor graph is consistent with their rank according to distance to the labeled, true y. In our training method, pairs of y’s are obtained from successive steps of training-time gradient-descent inference on y; when their rank is not consistent with that of the domain knowledge function, we accordingly update the energy network parameters. We demonstrate our method on a citation field extraction task, for which we learn a neural network (1) that generalizes beyond the original domain knowledge function, and (2) that prov"
N19-1083,P15-1034,0,0.0930166,"only a few predicates are observed between an entity pair. Finally, the results also show consistent improvement of OpenKI model over only-Rowless and only-ENE models. This indicates that the models are complementary to each other. We further observe significant improvements by applying different attention mechanisms over the OpenKI MaxR model – thus establishing the effectiveness of our attention mechanism. Unseen entity: Table 4 shows the data statistics Figure 2: Structures of P (p|p0 ), P (p|s, o), P (p|s, p0 , o) are listed from top to bottom. 4.3 Results Baselines and Experimental Setup Angeli et al. (2015) employ point-wise mutual information (PMI) between target relations and observed predicates to map OpenIE predicates to KB relations. This is similar to our Bayes conditional probability P (p|p0 ). This baseline operates at predicate-level. To indicate the usefulness of entity neighborhood information, we also compare with P (p|s, p0 , o) as mentioned in Section 4.2. For the advanced embedding-based baselines, we compare with the E-model and the Rowless model (with MaxR and query attention) introduced in Section 2.1. Hyper-parameters: In our experiments, we use 25 dimensional embedding vector"
N19-1083,W12-3016,0,0.0614889,"Missing"
N19-1083,P16-1200,0,0.0826165,"el already captures soft entity type information while modeling the neighborhood information of an entity in contrast to the other methods that require explicit type constraint. Table 4: Statistics for unseen entities in test data. “Both seen” indicates both entities exist in training data; “One unseen” indicates only one of the entities in the pair exist in training data; “Both unseen” indicates both entities were unobserved during training. Models Rowless Model OpenKI with Dual Att. All data At least one seen 0.278 0.365 0.282 0.419 4.5 Prior works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Qin et al., 2018) on distantly supervised relation extraction performed evaluations on the New York Times (NYT) + Freebase benchmark data developed by Riedel et al. (2010)6 . The dataset contains sentences whose entity mentions are annotated with Freebase entities as well as relations. The training data consists of sentences from articles in 2005-2006 whereas the test data consists of sentences from articles in 2007. There are 1950 relational facts in our test data7 . In contrast to our prior experiments in the semi-structured setting with text predicates, in this experiment we consider the"
N19-1083,D12-1048,0,0.28922,"data or WebTables (Cafarella et al., 2008) by schema mapping (Rahm and Bernstein, 2001), but the quality is far below adequate for assuring correct data integration. We propose advancing progress in this direction by applying knowledge integration from OpenIE extractions. OpenIE extracts SPO (subject, predicate, object) triples, where each element is a text phrase, such as E1: (“Robin Hood”, “Full Cast and Crew”, “Leonardo Decaprio”) and E2: (“Ang Lee”, “was named best director for”, “Brokeback”). OpenIE has been studied for text extraction extensively (Yates et al., 2007; Fader et al., 2011; Mausam et al., 2012), and also for semi-structured sources (Bronzi et al., 2013), thus serves an effective tool for web-scale knowledge extraction. The remaining problem is to align text-phrase predicates1 from OpenIE to knowledge bases (KB). Knowledge integration answers the following question: given an OpenIE extraction (s, p, o), how can one populate an existing KB using relations in the pre-defined ontology? The problem of knowledge integration is not completely new. The DB community has been solving the problem using schema mapping techniques, identifying mappings from a source schema (OpenIE extractions in"
N19-1083,C10-1032,0,0.039388,", where new entity pairs are abundant and data are fairly sparse. 1 Introduction Web-scale knowledge extraction and alignment has been a vision held by different communities for decades. The Natural Language Processing (NLP) community has been focusing on knowledge extraction from texts. They apply either closed information extraction according to an ontology (Mintz et al., 2009; Zhou et al., 2005), restricting to a subset of relations pre-defined in the ontology, or open information extraction (OpenIE) ∗ 1 We also need to align text-phrase entities, which falls in the area of entity linking (Dredze et al., 2010; Ji et al., 2014); it is out of scope of this paper and we refer readers to relevant references. This work was performed while at Amazon. 762 Proceedings of NAACL-HLT 2019, pages 762–772 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics address”; “first name” and “last name” together mapped to “full name”. However, for our example “Full Cast and Crew”, which is a union of multiple KB relations such as “directed by”, “written by”, and “actor”, it is very hard to determine a mapping at the predicate level. On the other hand, the NLP community has pr"
N19-1083,P09-1113,0,0.652895,"entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional OpenIE extractions like ReVerb, but also boosts the performance on OpenIE from semi-structured data, where new entity pairs are abundant and data are fairly sparse. 1 Introduction Web-scale knowledge extraction and alignment has been a vision held by different communities for decades. The Natural Language Processing (NLP) community has been focusing on knowledge extraction from texts. They apply either closed information extraction according to an ontology (Mintz et al., 2009; Zhou et al., 2005), restricting to a subset of relations pre-defined in the ontology, or open information extraction (OpenIE) ∗ 1 We also need to align text-phrase entities, which falls in the area of entity linking (Dredze et al., 2010; Ji et al., 2014); it is out of scope of this paper and we refer readers to relevant references. This work was performed while at Amazon. 762 Proceedings of NAACL-HLT 2019, pages 762–772 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics address”; “first name” and “last name” together mapped to “full name”. However"
N19-1083,D11-1142,0,0.795649,"aligning relational data or WebTables (Cafarella et al., 2008) by schema mapping (Rahm and Bernstein, 2001), but the quality is far below adequate for assuring correct data integration. We propose advancing progress in this direction by applying knowledge integration from OpenIE extractions. OpenIE extracts SPO (subject, predicate, object) triples, where each element is a text phrase, such as E1: (“Robin Hood”, “Full Cast and Crew”, “Leonardo Decaprio”) and E2: (“Ang Lee”, “was named best director for”, “Brokeback”). OpenIE has been studied for text extraction extensively (Yates et al., 2007; Fader et al., 2011; Mausam et al., 2012), and also for semi-structured sources (Bronzi et al., 2013), thus serves an effective tool for web-scale knowledge extraction. The remaining problem is to align text-phrase predicates1 from OpenIE to knowledge bases (KB). Knowledge integration answers the following question: given an OpenIE extraction (s, p, o), how can one populate an existing KB using relations in the pre-defined ontology? The problem of knowledge integration is not completely new. The DB community has been solving the problem using schema mapping techniques, identifying mappings from a source schema ("
N19-1083,P11-1055,0,0.0271225,"is possible to further improve the performance of our model by incorporating text encoders as an additional signal. Some prior works (Verga et al., 2016; Toutanova et al., 2015) also leverage text encoders for relation inference. 5 Related Work Relation Extraction: Mintz et al. (2009) utilize the entity pair overlap between knowledge bases and text corpus to generate signals for automatic supervision. To avoid false positives during training, many works follow the at-least-one assumption, where at least one of the text patterns between the entity pair indicate an aligned predicate in the KB (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). These works do not leverage graph information. In addition, Universal Schema (Riedel et al., 2013; Verga et al., 2017) tackled this task by low-rank matrix factorization. Toutanova et al. (2015) exploit graph information for knowledge base completion. However, their work cannot deal with unseen entities since entities’ parameters are explicitly learned during training. Schema Mapping: Traditional schema mapping methods (Rahm and Bernstein, 2001) involve three kinds of features, namely, language (name or description), type constrain"
N19-1083,P18-1199,0,0.0245658,"Missing"
N19-1083,D12-1042,0,0.458546,"pe constraint. This shows that the ENE model already captures soft entity type information while modeling the neighborhood information of an entity in contrast to the other methods that require explicit type constraint. Table 4: Statistics for unseen entities in test data. “Both seen” indicates both entities exist in training data; “One unseen” indicates only one of the entities in the pair exist in training data; “Both unseen” indicates both entities were unobserved during training. Models Rowless Model OpenKI with Dual Att. All data At least one seen 0.278 0.365 0.282 0.419 4.5 Prior works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Qin et al., 2018) on distantly supervised relation extraction performed evaluations on the New York Times (NYT) + Freebase benchmark data developed by Riedel et al. (2010)6 . The dataset contains sentences whose entity mentions are annotated with Freebase entities as well as relations. The training data consists of sentences from articles in 2005-2006 whereas the test data consists of sentences from articles in 2007. There are 1950 relational facts in our test data7 . In contrast to our prior experiments in the semi-structured setting with text predicates"
N19-1083,D15-1174,0,0.484912,"th Query Attn. (Verga et al., 2017) 0.318 0.326 0.285 0.278 0.481 0.512 0.659 0.695 OpenKI with MaxR OpenKI with Query Att. OpenKI with Neighbor Att. OpenKI with Dual Att. 0.500 0.497 0.495 0.505 0.378 0.372 0.372 0.365 0.649 0.663 0.650 0.658 0.802 0.800 0.813 0.814 Table 3: Mean average precision (MAP) of different models over four data settings. icates are observed. Our training data consists of the rest of GKB and all the OpenIE text extractions. In addition, we exclude direct KB triples from training where corresponding entity pairs appear in the test data (following the data setting of (Toutanova et al., 2015)). Table 2 shows the data statistics 5 . We adopt a similar training strategy as Universal Schema for the Ceres dataset – that not only learns direct mapping from text predicates to KB relations, but also clusters OpenIE predicates and KB relations by their co-occurrence. However, for the ReVerb data containing a large number of text predicates compared to Ceres, we only learn the direct mapping from text predicates to KB relations that empirically works well for this dataset. In order to show the generalizability of our approach to traditional (non OpenIE) corpora, we also perform experiments"
N19-1083,E17-1058,1,0.870136,"and knowledge in existing knowledge bases: given a set of extractions regarding an entity pair (s, o) and also information of each entity, infer new relations for this pair. One drawback of this method is that it cannot handle unseen entities and entity pairs. Also, the technique tends to overfit when the data is sparse due to large number of parameters for entities and entity pairs. Unfortunately, in the majority of the real extractions we examined in our experiments, we can find only 1.4 textual triples on average between the subject and object. The latest proposal Rowless Universal Schema (Verga et al., 2017) removes the entityspecific parameters and makes the inference directly between predicates and relations, thereby allowing us to reason about unseen entity pairs. However, it completely ignores the entities themselves, so in a sense falls back to predicate-level decisions, especially when only one text predicate is observed. In this paper we propose a solution that leverages information about the individual entities whenever possible, and falls back to predicatelevel decisions only when both involved entities are new. Continuing with our example E1 – if we know from existing knowledge that “Le"
N19-1083,N07-4013,0,0.231086,"as been focusing on aligning relational data or WebTables (Cafarella et al., 2008) by schema mapping (Rahm and Bernstein, 2001), but the quality is far below adequate for assuring correct data integration. We propose advancing progress in this direction by applying knowledge integration from OpenIE extractions. OpenIE extracts SPO (subject, predicate, object) triples, where each element is a text phrase, such as E1: (“Robin Hood”, “Full Cast and Crew”, “Leonardo Decaprio”) and E2: (“Ang Lee”, “was named best director for”, “Brokeback”). OpenIE has been studied for text extraction extensively (Yates et al., 2007; Fader et al., 2011; Mausam et al., 2012), and also for semi-structured sources (Bronzi et al., 2013), thus serves an effective tool for web-scale knowledge extraction. The remaining problem is to align text-phrase predicates1 from OpenIE to knowledge bases (KB). Knowledge integration answers the following question: given an OpenIE extraction (s, p, o), how can one populate an existing KB using relations in the pre-defined ontology? The problem of knowledge integration is not completely new. The DB community has been solving the problem using schema mapping techniques, identifying mappings fr"
N19-1083,I17-1086,0,0.389642,"the validation set. an aggregation function. We compute the similarity wp|N b between an entity’s neighborhood information given by the above embeddings and a text predicate p to enforce a soft and fine-grained argument type constraint over the text predicate: T wp|N b T agg agg exp(vsubj · vpsubj + vobj · vpobj ) =P T agg subj T agg + vobj · vpobj ) 0 p0 exp(vsubj · vp0 3.5 Subject and object argument types of relations help in filtering out a large number of candidate relations that do not meet the argument type, and therefore serve as useful constraints for relation inference. Similar to (Yu et al., 2017), we identify the subject and object argument type of each relation by calculating its probability of co-occurrence with subject / object entity types. During inference, we select candidate relations by performing a post-processing filtering step using the subject and object’s type information when available. Finally, we combine both the query-dependent and neighborhood-based attention into a Dual Attention mechanism: wp|q+N b = wp|q · wp|N b wp|q+N b wp = P p0 wp0 |q+N b And the score function is given by: S Att (s, q, o) = Aggp∈R(s,o) (vp ) · vqT X  wp vp · vqT = (4) p 3.3 4 Joint Model: Op"
N19-1083,D15-1203,0,0.169436,"ws that the ENE model already captures soft entity type information while modeling the neighborhood information of an entity in contrast to the other methods that require explicit type constraint. Table 4: Statistics for unseen entities in test data. “Both seen” indicates both entities exist in training data; “One unseen” indicates only one of the entities in the pair exist in training data; “Both unseen” indicates both entities were unobserved during training. Models Rowless Model OpenKI with Dual Att. All data At least one seen 0.278 0.365 0.282 0.419 4.5 Prior works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Qin et al., 2018) on distantly supervised relation extraction performed evaluations on the New York Times (NYT) + Freebase benchmark data developed by Riedel et al. (2010)6 . The dataset contains sentences whose entity mentions are annotated with Freebase entities as well as relations. The training data consists of sentences from articles in 2005-2006 whereas the test data consists of sentences from articles in 2007. There are 1950 relational facts in our test data7 . In contrast to our prior experiments in the semi-structured setting with text predicates, in this experimen"
N19-1116,P18-2058,0,0.0217324,"der all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with synt"
N19-1116,W18-5452,0,0.386886,"ments 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: i,j∈{k} πki , πkj ← arg max[xi + xj + e(i, j)] i,j∈{k} Backtrack to get the maximal tree. procedure BACKTRACK(k) if SIZE(k) = 1 then return k i ← BACKTRACK(πki ) j ← BACKTRACK(πkj ) return (i, j) return BACKTRACK(k ← root) vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel ("
N19-1116,N18-1202,1,0.808351,"rained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (Peters et al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et a"
N19-1116,D18-1179,0,0.0665688,"rained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT (Peters et al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et a"
N19-1116,P02-1017,0,0.884927,"ndom Balanced Table 2: NLI unsupervised unlabeled binary constituency parsing comparing to CoreNLP predicted parses. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). F1 median and max are calculated over five random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work. details on WSJ split differences). Not binarizing the target trees sets an upper-bound on the performance of our models, denoted as UB in Table 3. We compare against previous notable models for this task: CCM (Klein and Manning, 2002) uses the EM algorithm to learn probable nested 1133 bracketings over a sentence using gold or induced part-of-speech tags, and PRLG (Ponvert et al., 2011) performs constituent parsing through consecutive rounds of sentence chunking. In Table 3, we see that DIORA outperforms the previous state of the art for WSJ-40, PRLG, in max F1. The WSJ-10 split has been difficult for latent tree parsers such as DIORA, PRPN, and ONLSTM, none of which (including our model) are able to improve upon previous non-neural methods. However, when we compare trends between WSJ-10 and WSJ-40, we see that DIORA does"
N19-1116,P04-1061,0,0.60752,"any pair of constituents and does not use structural supervision. Learning from Raw Text Unsupervised learning of syntactic structure has been an active research area (Brill et al., 1990), including for unsupervised segmentation (Ando and Lee, 2000; Goldwater et al., 2009; Ponvert et al., 2011) and unsupervised dependency parsing (Spitkovsky et al., 2013). Some models exploit the availability of parallel corpora in multiple languages (Das and Petrov, 2011; Cohen et al., 2011). Others have shown that dependency parsing can be used for unsupervised constituency parsing (Spitkovsky et al., 2013; Klein and Manning, 2004), or that it’s effective to prune a random subset of possible trees (Bod, 2006). These approaches aren’t necessarily orthogonal to DIORA. For instance, our model may benefit when combined with an unsupervised dependency parser. 5 Conclusion In this work we presented DIORA, an unsupervised method for inducing syntactic trees and representations of constituent spans. We showed 1136 inside-outside representations constructed with a latent tree chart parser and trained with an autoencoder language modeling objective learns syntactic structure of language effectively. In experiments on unsupervised"
N19-1116,P11-1108,0,0.719209,"ees and results provided by Htut et al. (2018). F1 median and max are calculated over five random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work. details on WSJ split differences). Not binarizing the target trees sets an upper-bound on the performance of our models, denoted as UB in Table 3. We compare against previous notable models for this task: CCM (Klein and Manning, 2002) uses the EM algorithm to learn probable nested 1133 bracketings over a sentence using gold or induced part-of-speech tags, and PRLG (Ponvert et al., 2011) performs constituent parsing through consecutive rounds of sentence chunking. In Table 3, we see that DIORA outperforms the previous state of the art for WSJ-40, PRLG, in max F1. The WSJ-10 split has been difficult for latent tree parsers such as DIORA, PRPN, and ONLSTM, none of which (including our model) are able to improve upon previous non-neural methods. However, when we compare trends between WSJ-10 and WSJ-40, we see that DIORA does a better job at extending to longer sequences. 3.2 Unsupervised Phrase Segmentation In many scenarios, one is only concerned with extracting particular con"
N19-1116,W12-4501,0,0.0169464,"meaningful representations for spans of text. Most language modeling methods focus only on explicitly modeling token representations and rely on ad-hoc postprocessing to generate representations for longer spans, typically relying on simple arithmetic functions of the individual tokens. To evaluate our model’s learned phrase representations, we look at the similarity between spans of the same type within labeled phrase datasets. We look at two datasets. CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000) is a shallow parsing dataset containing spans of noun phrases, verb phrases, etc. CoNLL 2012 (Pradhan et al., 2012) Model WSJ-10 WSJ-40 F1µ F1max F1µ F1max UB LB RB 87.8 28.7 61.7 87.8 28.7 61.7 85.7 12.0 40.7 85.7 12.0 40.7 CCM† CCMgold † PRLG † - 63.2 71.9 72.1 - 33.7 54.6 PRPNN LI PRPN‡ ON-LSTM‡ DIORA 66.3 ±0.8 70.5 ±0.4 65.1 ±1.7 67.7 ±0.7 68.5 71.3 66.8 68.5 60.6 ±0.2 52.4 60.9 Table 3: WSJ-10 and WSJ-40 unsupervised non-binary unlabeled constituency parsing with punctuation removed. † indicates that the model predicts a full, nonbinary parse with additional resources. ‡ indicates model was trained on WSJ data and PRPNN LI was trained on MultiNLI data. CCM uses predicted POS tags while CCMgold uses go"
N19-1116,D14-1081,0,0.0460728,"lculate marginal probabilities in order to align spans between sentences in entailment. Composition Loss TreeLSTM TreeLSTM MLP MLP MLPKernel MLPShared Margin Softmax Margin Softmax Softmax Softmax F1µ ∅ +PP 49.9 52.0 49.7 52.6 51.8 50.8 53.1 52.9 54.4 55.5 54.8 56.7 Table 6: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modified according to the post-processing heuristic (+P P ). The mean F1 is shown across three random seeds. Neural Inside-Outside Parsers The InsideOutside Recursive Neural Network (IORNN) (Le and Zuidema, 2014) is closest to ours. It is a graph-based dependency parser that uses beam search and can reliably find accurate parses when retaining a k-best list. In contrast, our model produces the most likely parse given the learned compatibility of the constituents. The Neural CRF Parser (Durrett and Klein, 2015), similar to DIORA, performs exact inference on the structure of a sentence, although requires a set of grammar rules and labeled parse trees during training. DIORA, like Liu et al. (2018), has a single grammar rule that applies to any pair of constituents and does not use structural supervision."
N19-1116,D15-1137,0,0.129869,"∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et al., 2011b; Le and Zuidema, 2015; Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting over all possible sub-trees (§2). Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference. To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without requiring any supervised t"
N19-1116,D11-1014,0,0.0974649,"t al., 2018a; Devlin et al., ∗ Equal contribution, randomly ordered. 2019), we propose a new deep learning method of unsupervised parser training that can extract both shallow parses (i.e., noun phrases or entities) and full syntactic trees from any domain or language automatically without requiring any labeled training data. In addition to producing parses, our model simultaneously builds representations for internal constituents that reflect syntactic and semantic regularities which can be leveraged by downstream tasks. Our model builds on existing work developing latent tree chart parsers (Socher et al., 2011b; Le and Zuidema, 2015; Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). These methods produce representations for all internal nodes in the tree (cells in the chart), each generated as a soft weighting over all possible sub-trees (§2). Unfortunately, they still require sentence-level annotations during training, as they are all trained to optimize a downstream task, typically natural language inference. To address these limitations, we present deep inside-outside recursive autoencoders (DIORA) which enable unsupervised discovery and representation of constituents without req"
N19-1116,D18-1184,0,0.0423187,"ur model’s output, we see that some trees are an exact replication of the binarized ground truth (Fig. 3), or very close (Fig. 4). For future work we intend to explore common patterns in DIORA’s learned structure, although some patterns are already recognizable, such as the affinity to group particles and verbs (Fig. 5). 4 Related Work Latent Tree Learning A brief survey of neural latent tree learning models was covered in (Williams et al., 2018a). The first positive result for neural latent tree parsing was shown in (Htut et al., 2018), which used a language modeling objective. The model in (Liu et al., 2018) uses an inside chart and an outside procedure to calculate marginal probabilities in order to align spans between sentences in entailment. Composition Loss TreeLSTM TreeLSTM MLP MLP MLPKernel MLPShared Margin Softmax Margin Softmax Softmax Softmax F1µ ∅ +PP 49.9 52.0 49.7 52.6 51.8 50.8 53.1 52.9 54.4 55.5 54.8 56.7 Table 6: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modified according to the post-processing heuristic (+P P ). The mean F1 is shown across three random seeds. Neural Inside-Outside Parsers The Ins"
N19-1116,P14-5010,0,0.0130058,"vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel (Choi et al., 2018), an unsupervised chart parser. The latter two of these models are trained to predict the downstream task of natural language inference (NLI). 3 To evaluate the effectiveness of DIORA, we run experiments on unsupervised parsing, unsuperprocedure CKY(chart)"
N19-1116,D13-1204,0,0.150575,"RF Parser (Durrett and Klein, 2015), similar to DIORA, performs exact inference on the structure of a sentence, although requires a set of grammar rules and labeled parse trees during training. DIORA, like Liu et al. (2018), has a single grammar rule that applies to any pair of constituents and does not use structural supervision. Learning from Raw Text Unsupervised learning of syntactic structure has been an active research area (Brill et al., 1990), including for unsupervised segmentation (Ando and Lee, 2000; Goldwater et al., 2009; Ponvert et al., 2011) and unsupervised dependency parsing (Spitkovsky et al., 2013). Some models exploit the availability of parallel corpora in multiple languages (Das and Petrov, 2011; Cohen et al., 2011). Others have shown that dependency parsing can be used for unsupervised constituency parsing (Spitkovsky et al., 2013; Klein and Manning, 2004), or that it’s effective to prune a random subset of possible trees (Bod, 2006). These approaches aren’t necessarily orthogonal to DIORA. For instance, our model may benefit when combined with an unsupervised dependency parser. 5 Conclusion In this work we presented DIORA, an unsupervised method for inducing syntactic trees and rep"
N19-1116,W05-0636,1,0.65145,"ynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific t"
N19-1116,N18-1101,0,0.298174,"procedure BACKTRACK(k) if SIZE(k) = 1 then return k i ← BACKTRACK(πki ) j ← BACKTRACK(πkj ) return (i, j) return BACKTRACK(k ← root) vised segment recall, and phrase similarity. The model has been implemented in PyTorch (Team, 2018) and the code is published online.3 For training details, see Appendix A.1. 3.1 Unsupervised Parsing We first evaluate how well our model predicts a full unlabeled constituency parse. We look at two data sets used in prior work (Htut et al., 2018), The Wall Street Journal (WSJ) section of Penn Treebank (Marcus et al., 1993), and the automatic parses from MultiNLI (Williams et al., 2018b). WSJ has gold human-annotated parses and MultiNLI contains automatic parses derived from a supervised parser (Manning et al., 2014). In addition to PRPN (Shen et al., 2018),4 we compare our model to deterministically constructed left branching, right branching, balanced, and random trees. We also compare to ON-LSTM (Shen et al., 2019), an extension of the PRPN model, RL-SPINN (Yogatama et al., 2017), an unsupervised shift-reduce parser, and ST-Gumbel (Choi et al., 2018), an unsupervised chart parser. The latter two of these models are trained to predict the downstream task of natural langua"
N19-1116,C18-1120,0,0.0259508,"he highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. 1 Under the current circumstances he says their scenario no longer seems unrealistic Figure 1: An unlabeled binary constituency parse from DIORA matching the ground truth. Introduction Syntactic parse trees are useful for downstream tasks such as relation extraction (Gamallo et al., 2012), semantic role labeling (Sutton and McCallum, 2005; He et al., 2018), machine translation (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), and text classification (Li and Roth, 2006; Tai et al., 2015). Traditionally, supervised parsers trained on datasets such as the Penn Treebank (Marcus et al., 1993) are used to obtain syntactic trees. However, the treebanks used to train these supervised parsers are typically small and restricted to the newswire domain. Unfortunately, models trained on newswire treebanks tend to perform considerably worse when applied to new types of data, and creating new domain specific treebanks with syntactic annotations is expensive and timeconsuming. Motivated by the desire to address the limitations o"
N19-1116,J93-2004,0,\N,Missing
N19-1116,D15-1075,0,\N,Missing
N19-1116,H94-1020,0,\N,Missing
N19-1116,W00-0726,0,\N,Missing
N19-1116,P11-1061,0,\N,Missing
N19-1116,P06-1109,0,\N,Missing
N19-1116,A00-2032,0,\N,Missing
N19-1116,P17-2012,0,\N,Missing
N19-1116,Q18-1019,1,\N,Missing
N19-1116,D18-1544,0,\N,Missing
N19-1116,N19-1423,0,\N,Missing
N19-1116,W12-0702,0,\N,Missing
P08-1099,W04-3234,0,0.0150615,"model of likelihood and the constraints. This model can be applied to the combination of labeled and unlabeled instances, but cannot be applied in situations where only labeled features are available. Additionally, our model can be easily combined with other semi-supervised criteria, such as entropy regularization. Finally, their model is a generative HMM which cannot handle the rich, nonindependent feature sets that are available to a CRF. There have been relatively few different approaches to CRF semi-supervised training. One approach has been that proposed in both Miller et al. (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. Since this is an orthogonal method for improving accuracy it can be combined with many of the other methods discussed above, and indeed we have obtained positive preliminary experimental results with GE criteria (not reported on here). Another method for semi-supervised CRF training is entropy regularization, initially proposed by Grandvalet and Bengio (2004) and extended to linear-chain CRFs by Jiao et al. (2006). In this formulation, the trad"
P08-1099,P05-1046,0,0.0720359,"ackward. Here we present a more efficient method that requires only one run of forward/backward.5 First we Pdecompose the probability into two parts: = j ? pθ (yi , yi+1 , yj ? Pi ? l|x) = j=1 pθ (yi , yi+1 , yj = l|x)I(j ∈ j ) + PJ ? j=i+1 pθ (yi , yi+1 , yj = l|x)I(j ∈ j ). Next, we show how to compute these terms efficiently. Similar to forward/backward, we build a lattice of intermediate results that then can be used to calculate the 5 (Kakade et al., 2002) propose a related method that computes p(y1..i = l1..i |yi+1 = l). 874 5 Experimental Results We use the CLASSIFIEDS data provided by Grenager et al. (2005) and compare with results reported by HK06 (Haghighi and Klein, 2006) and CRR07 (Chang et al., 2007). HK06 introduced a set of 33 features along with their majority labels, these are the primary set of additional constraints (Table 1). As HK06 notes, these features are selected using statistics of the labeled data, and here we used similar features here in order to compare with previous results. Though in practice we have found that feature selection is often intuitive, recent work has experimented with automatic feature selection using LDA (Druck et al., 2008). For some of the experiments we"
P08-1099,N06-1041,0,0.163899,"ious work has shown how to apply GE criteria to maximum entropy classifiers. In section 4, we extend GE criteria to semi-supervised learning of linear-chain conditional random fields, using conditional probability distributions of labels given features. To empirically evaluate this method we compare it with several competing methods for CRF training, including entropy regularization and expected gradient, showing that GE provides significant improvements. We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al., 2007). Finally, in Section 5.3 we show that feature-labeling can lead to dramatic reductions in the annotation time that is required in order to achieve the same level of accuracy as traditional instance-labeling. 2 Related Work There has been a significant amount of work on semi-supervised learning with small amounts of fully labeled data (see Zhu (2005)). However there has been comparatively less work on learning from alternative forms of labeled resources. One example is Schapire et al. (2002) who present a method in which features are"
P08-1099,P06-1027,0,0.167716,"d in both Miller et al. (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. Since this is an orthogonal method for improving accuracy it can be combined with many of the other methods discussed above, and indeed we have obtained positive preliminary experimental results with GE criteria (not reported on here). Another method for semi-supervised CRF training is entropy regularization, initially proposed by Grandvalet and Bengio (2004) and extended to linear-chain CRFs by Jiao et al. (2006). In this formulation, the traditional label likelihood (on supervised data) is augmented with an additional term that encourages the model to predict low-entropy label distributions on the unlabeled data: X O(θ; D, U ) = log pθ (y(d) |x(d) ) − λH(y|x). d This method can be quite brittle, since the minimal entropy solution assigns all of the tokens the same label.1 In general, entropy regularization is fragile, and accuracy gains can come only with precise settings of λ. High values of λ fall into the minimal entropy trap, while low values of λ have no effect on the model (see (Jiao et al., 20"
P08-1099,N04-1043,0,0.0494665,"ly sequences from a joint model of likelihood and the constraints. This model can be applied to the combination of labeled and unlabeled instances, but cannot be applied in situations where only labeled features are available. Additionally, our model can be easily combined with other semi-supervised criteria, such as entropy regularization. Finally, their model is a generative HMM which cannot handle the rich, nonindependent feature sets that are available to a CRF. There have been relatively few different approaches to CRF semi-supervised training. One approach has been that proposed in both Miller et al. (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. Since this is an orthogonal method for improving accuracy it can be combined with many of the other methods discussed above, and indeed we have obtained positive preliminary experimental results with GE criteria (not reported on here). Another method for semi-supervised CRF training is entropy regularization, initially proposed by Grandvalet and Bengio (2004) and extended to linear-chain CRFs by Jiao et al. (2006). In this fo"
P08-1099,P05-1044,0,0.173464,"ere |x |= |y |= n, and each label yi has s different possible discrete values. This model is analogous to maximum entropy models for structured outputs, where expectations can be efficiently calculated by dynamic programming. For a linear-chain CRF of Markov order one: ! X 1 pθ (y|x) = exp θk Fk (x, y) , Z(x) k ∂ X ∇L (θ) = log p(x, y, z; θ) ∂θ z X = p(z|y, x)fk (x, y, z) − Finally, there are some methods that use auxiliary tasks for training sequence models, though they do not train linear-chain CRFs per se. Ando and Zhang (2005) include a cluster discovery step into the supervised training. Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances “good”. Although these methods can often find good solutions, both are quite sensitive to the selection of auxiliary information, and making good selections requires significant insight.2 p(z, y 0 |x; θ)fk (x, y, z). z,y 0 In essence, this resembles the standard gradient for the CRF, except that there is an additional marginalization in the first term over the hidden variable z. This type of training has been applied by Quattoni et al. (2007) for hidden-state conditional random fields, and can be equally applied t"
P08-1099,P07-1036,0,\N,Missing
P09-1041,P05-1044,0,0.0275981,"the fact that short attachments are more common to improve unsupervised parsing accuracy. 4.1 In this section we compare GE training with methods for unsupervised parsing. We use the WSJ10 corpus (as processed by Smith (2006)), which is comprised of English sentences of ten words or fewer (after stripping punctuation) from the WSJ portion of the Penn Treebank. As in previous work sentences contain only part-of-speech tags. We compare GE and supervised training of an edge-factored CRF with unsupervised learning of a DMV model (Klein and Manning, 2004) using EM and contrastive estimation (CE) (Smith and Eisner, 2005). We also report the accuracy of an attach-right baseline6 . Finally, we report the accuracy of a constraint baseline that assigns a score to each possible edge that is the sum of the target expectations for all constraints on that edge. Possible edges without constraints receive a score of 0. These scores are used as input to the maximum spanning tree algorithm, which returns the best tree. Note that this is a strong baseline because it can handle uncertain constraints, and the tree constraint imposed by the MST algorithm helps information propagate across edges. We note that there are consid"
P09-1041,P92-1024,0,0.226393,"ictions and linguistic expectation constraints. In a comparison with two prominent “unsupervised” learning methods that require indirect biasing toward the correct syntactic structure, we show that GE can attain better accuracy with as few as 20 intuitive constraints. We also present positive experimental results on longer sentences in multiple languages. 1 Introduction Early approaches to parsing assumed a grammar provided by human experts (Quirk et al., 1985). Later approaches avoided grammar writing by learning the grammar from sentences explicitly annotated with their syntactic structure (Black et al., 1992). While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007;"
P09-1041,P06-1072,0,0.21503,"Missing"
P09-1041,P06-1109,0,0.0154581,". While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006). These indirect methods for 2 Related Work This work is closely related to the prototypedriven grammar induction method of Haghighi and Klein (2006), which uses prototype phrases to guide the EM algorithm in learning a PCFG. Direct comparison with this method is not possible because we are interested in dependency syntax rather than phrase structure syntax. However, the approach we advocate has several significant 360 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360–368, c Suntec, Singapore, 2-7 August 2"
P09-1041,D07-1070,0,0.038277,"Missing"
P09-1041,P01-1017,0,0.0605763,"nsupervised” learning methods that require indirect biasing toward the correct syntactic structure, we show that GE can attain better accuracy with as few as 20 intuitive constraints. We also present positive experimental results on longer sentences in multiple languages. 1 Introduction Early approaches to parsing assumed a grammar provided by human experts (Quirk et al., 1985). Later approaches avoided grammar writing by learning the grammar from sentences explicitly annotated with their syntactic structure (Black et al., 1992). While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006). These indirect methods"
P09-1041,D08-1016,0,0.013484,"Missing"
P09-1041,C04-1026,0,0.0235405,"te the marginals (Smith and Eisner, 2008). In this paper g is binary and normalized by its total count in the corpus. The expectation of g is then the probability that it indicates a true edge. 363 4 Linguistic Prior Knowledge 5 Training parsers using GE with the aid of linguists is an exciting direction for future work. In this paper, we use constraints derived from several basic types of linguistic knowledge. One simple form of linguistic knowledge is the set of possible parent tags for a given child tag. This type of constraint was used in the development of a rule-based dependency parser (Debusmann et al., 2004). Additional information can be obtained from small grammar fragments. Haghighi and Klein (2006) provide a list of prototype phrase structure rules that can be augmented with dependencies and used to define constraints involving parent and child tags, surrounding or interposing tags, direction, and distance. Finally there are well known hypotheses about the direction and distance of attachments that can be used to define constraints. Eisner and Smith (2005) use the fact that short attachments are more common to improve unsupervised parsing accuracy. 4.1 In this section we compare GE training w"
P09-1041,D07-1014,0,0.257426,"is a recently proposed framework for incorporating prior knowledge into the learning of conditional random fields (CRFs) (Lafferty et al., 2001). GE criteria express a preference on the value of a model expectation. For example, we know that “in English, when a determiner is directly to the left of a noun, the noun is usually the parent of the determiner”. With GE we may add a term to the objective function that encourages a feature-rich CRF to match this expectation on unlabeled data, and in the process learn about related features. In this paper we use a non-projective dependency tree CRF (Smith and Smith, 2007). While a complete exploration of linguistic prior knowledge for dependency parsing is beyond the scope of this paper, we provide several promising demonstrations of the proposed method. On the English WSJ10 data set, GE training outperforms two prominent unsupervised methods using only 20 constraints either elicited from a human or provided by an “oracle” simulating a human. We also present experiments on longer sentences in Dutch, Spanish, and Turkish in which we obtain accuracy comparable to supervised learning with tens to hundreds of complete parsed sentences. In this paper, we propose a"
P09-1041,W05-1504,0,0.0165495,"set of possible parent tags for a given child tag. This type of constraint was used in the development of a rule-based dependency parser (Debusmann et al., 2004). Additional information can be obtained from small grammar fragments. Haghighi and Klein (2006) provide a list of prototype phrase structure rules that can be augmented with dependencies and used to define constraints involving parent and child tags, surrounding or interposing tags, direction, and distance. Finally there are well known hypotheses about the direction and distance of attachments that can be used to define constraints. Eisner and Smith (2005) use the fact that short attachments are more common to improve unsupervised parsing accuracy. 4.1 In this section we compare GE training with methods for unsupervised parsing. We use the WSJ10 corpus (as processed by Smith (2006)), which is comprised of English sentences of ten words or fewer (after stripping punctuation) from the WSJ portion of the Penn Treebank. As in previous work sentences contain only part-of-speech tags. We compare GE and supervised training of an edge-factored CRF with unsupervised learning of a DMV model (Klein and Manning, 2004) using EM and contrastive estimation (C"
P09-1041,P08-1061,0,0.0272576,"Missing"
P09-1041,P09-1042,0,0.07408,"Missing"
P09-1041,P06-1111,0,0.641059,"ly there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006). These indirect methods for 2 Related Work This work is closely related to the prototypedriven grammar induction method of Haghighi and Klein (2006), which uses prototype phrases to guide the EM algorithm in learning a PCFG. Direct comparison with this method is not possible because we are interested in dependency syntax rather than phrase structure syntax. However, the approach we advocate has several significant 360 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360–368, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP advantages. GE is more general than prototypedriven learning because GE constraints can be uncertain. Additionally prototype-driven grammar induction needs to be used in c"
P09-1041,N07-2021,0,0.0589175,"Missing"
P09-1041,P04-1061,0,0.652032,"the grammar from sentences explicitly annotated with their syntactic structure (Black et al., 1992). While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006). These indirect methods for 2 Related Work This work is closely related to the prototypedriven grammar induction method of Haghighi and Klein (2006), which uses prototype phrases to guide the EM algorithm in learning a PCFG. Direct comparison with this method is not possible because we are interested in dependency syntax rather than phrase structure syntax. However, the approach we advocate has several significant 360 Proceedings of the 47th Annual Meeting of the"
P09-1041,P08-1068,0,0.0200007,"Missing"
P09-1041,P08-1099,1,0.523641,"rs using Generalized Expectation Criteria Gregory Druck Dept. of Computer Science University of Massachusetts Amherst, MA 01003 Gideon Mann Google, Inc. 76 9th Ave. New York, NY 10011 Andrew McCallum Dept. of Computer Science University of Massachusetts Amherst, MA 01003 gdruck@cs.umass.edu gideon.mann@gmail.com mccallum@cs.umass.edu Abstract leveraging prior knowledge can be cumbersome and unintuitive for a non-machine-learning expert. This paper proposes a method for directly guiding the learning of dependency parsers with naturally encoded linguistic insights. Generalized expectation (GE) (Mann and McCallum, 2008; Druck et al., 2008) is a recently proposed framework for incorporating prior knowledge into the learning of conditional random fields (CRFs) (Lafferty et al., 2001). GE criteria express a preference on the value of a model expectation. For example, we know that “in English, when a determiner is directly to the left of a noun, the noun is usually the parent of the determiner”. With GE we may add a term to the objective function that encourages a feature-rich CRF to match this expectation on unlabeled data, and in the process learn about related features. In this paper we use a non-projective"
P09-1041,N06-1020,0,0.0926177,"Missing"
P09-1041,W07-2216,0,0.0513193,"ature functions that consider the child input (word, tag, or other feature), the parent input, and the rest of the sentence. This factorization implies that dependency decisions are independent conditioned on the input sentence x if y is a tree. Computing Zx and the edge expectations needed for partial derivatives requires summing over all possible trees for x. By relating the sum of the scores of all possible trees to counting the number of spanning trees in a graph, it can be shown that Zx is the determinant of the Kirchoff matrix K, which is constructed using the scores of possible edges. (McDonald and Satta, 2007; Smith and Smith, 2007). Computing the determinant takes O(n3 ) time, where n is the length of the sentence. To compute the marginal probability of a particular edge k → i (i.e. yi = k), the score of any edge k 0 → i such that k 0 6= k is set to 0. The determinant of the resulting modified Kirchoff matrix Kk→i is then the sum of the scores of all trees that include the edge k → i. The 5 Note that we could instead define a CRF for projective dependency parse trees and use a variant of the inside outside algorithm for inference. We choose non-projective because it is the more general case. 362"
P09-1041,P05-1012,0,0.182789,"the CRF may consider the distance between head and child, whereas DMV does not model distance. The CRF also models non-projective trees, which when evaluating on English is likely a disadvantage. Consequently, we experiment with two sets of features for the CRF model. The first, restricted set includes features that consider the head and child tags of the dependency conjoined with the direction of the attachment, (parent-POS,childPOS,direction). With this feature set, the CRF model is less expressive than DMV. The second full set includes standard features for edgefactored dependency parsers (McDonald et al., 2005), though still unlexicalized. The CRF cannot consider valency even with the full feature set, but this is balanced by the ability to use distance. “Oracle” constraints For some experiments that follow we use “oracle” constraints that are estimated from labeled data. This involves choosing feature templates (motivated by the linguistic knowledge described above) and estimating target expectations. Oracle methods used in this paper consider three simple statistics of candidate constraint functions: count c˜(g), edge count c˜edge (g), and edge probability p˜(edge|g). Let D be the labeled corpus."
P09-1041,P07-1049,0,0.0183279,"k et al., 1992). While such supervised approaches have yielded accurate parsers (Charniak, 2001), the syntactic annotation of corpora such as the Penn Treebank is extremely costly, and consequently there are few treebanks of comparable size. As a result, there has been recent interest in unsupervised parsing. However, in order to attain reasonable accuracy, these methods have to be carefully biased towards the desired syntactic structure. This weak supervision has been encoded using priors and initializations (Klein and Manning, 2004; Smith, 2006), specialized models (Klein and Manning, 2004; Seginer, 2007; Bod, 2006), and implicit negative evidence (Smith, 2006). These indirect methods for 2 Related Work This work is closely related to the prototypedriven grammar induction method of Haghighi and Klein (2006), which uses prototype phrases to guide the EM algorithm in learning a PCFG. Direct comparison with this method is not possible because we are interested in dependency syntax rather than phrase structure syntax. However, the approach we advocate has several significant 360 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360–368, c Suntec, Singapore,"
P11-1080,P98-1012,0,0.8945,"reference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical"
P11-1080,D08-1029,0,0.392855,"clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. Since many related methods use clustering, there are a number of distributed clustering algorithms that may help scale these approaches. Datta et al. (2006) propose an algorithm for distributed kmeans. Chen et al. (2010) describe a parallel spectral clustering algorithm. We use the Subsqu"
P11-1080,D08-1031,0,0.029693,"Missing"
P11-1080,W02-1001,0,0.0443813,"chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Given labeled data, these parameters can be learned by Perceptron (Collins, 2002), which uses the MAP configuration according to the model (ˆ e). There also exist more efficient training algorithms such as SampleRank (McCallum et al., 2009; Wick et al., 2009b) that update parameters during inference. However, we only focus on inference in this work, and the only parameter that we set manually is the bias b, which indirectly influences the number of entities in ˆ e. Unless specified otherwise, in this work the initial configuration for MCMC is the singleton configuration, i.e. all entities have a size of 1. This MCMC inference technique, which has been used in McCallum and"
P11-1080,N07-1011,1,0.856675,"and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2008; Wellner et al., 2004; Wick et al., 2009a). These models contain pairwise factors between all pairs of mentions capturing similarity between them. Many of these models also enforce transitivity and enable features over 793 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793–803, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ... The Physiological Basis of Politics,” by Kevin B. Smith, Douglas Oxley, Matthew Hibbing... ...during the late 60's and early 70's, Kevin Smith worked with several l"
P11-1080,P07-1094,0,0.00627756,"p is accepted with the following Metropolis-Hastings acceptance probability: !   0 ) 1/t q(e) p(e α(e, e0 ) = min 1, (1) p(e) q(e0 ) 1 Number of possible entities is Bell(n) in the number of mentions, i.e. number of partitions of n items where t is the annealing temperature parameter. MCMC chains efficiently explore the highdensity regions of the probability distribution. By slowly reducing the temperature, we can decrease the entropy of the distribution to encourage convergence to the MAP configuration. MCMC has been used for optimization in a number of related work (McCallum et al., 2009; Goldwater and Griffiths, 2007; Changhe et al., 2004). The proposal function moves a randomly chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Give"
P11-1080,N04-1002,0,0.713737,"ber of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2"
P11-1080,P07-1107,0,0.135709,"Missing"
P11-1080,D09-1120,0,0.0130614,"arge dataset, demonstrating the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years."
P11-1080,N10-1061,0,0.223887,"g the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have bi"
P11-1080,P08-1099,1,0.562065,"esent in real-world applications. By utilizing parallelism across machines, our method can run on very large datasets simply by increasing the number of machines used. Second, approaches that use clustering are limited to using pairwise distance functions for which additional supervision and features are difficult to incorporate. In addition to representing features from all of the related work, graphical models can also use more complex entity-wide features (Culotta et al., 2007; Wick et al., 2009a), and parameters can be learned using supervised (Collins, 2002) or semisupervised techniques (Mann and McCallum, 2008). Finally, the inference for most of the related approaches is greedy, and earlier decisions are not revisited. Our technique is based on MCMC inference and simulated annealing, which are able to escape local maxima. 801 7 Conclusions Motivated by the problem of solving the coreference problem on billions of mentions from all of the newswire documents from the past few decades, we make the following contributions. First, we introduce distributed version of MCMC-based inference technique that can utilize parallelism to enable scalability. Second, we augment the model with hierarchical variables"
P11-1080,W03-0405,0,0.174727,"ent coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervis"
P11-1080,D10-1004,0,0.0166425,"Missing"
P11-1080,P05-1020,0,0.0171755,"Missing"
P11-1080,P04-1076,0,0.246817,"ion for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-"
P11-1080,W04-2406,0,0.00791484,"between pairs of contexts, which are then used for clustering. One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and"
P11-1080,C10-2121,0,0.735477,"ng the entities. mawat, 2004) to manage the distributed computation. This approach to distribution is equivalent to inference with all mentions and entities on a single machine with a restricted proposer, but is faster since it exploits independencies to propose multiple jumps simultaneously. By restricting the jumps as described above, the acceptance probability calculation is exact. Partitioning the entities and proposing local jumps are restrictions to the single-machine proposal distribution; redistribution stages ensure the equivalent Markov chains are still irreducible. See Singh et al. (2010) for more details. 4 Hierarchical Coreference Model The proposal function for MCMC-based MAP inference presents changes to the current entities. Since we use MCMC to reach high-scoring regions of the hypothesis space, we are interested in the changes that improve the current configuration. But as the number of mentions and entities increases, these fruitful samples become extremely rare due to the blowup in the possible space of configurations, resulting in rejection of a large number of proposals. By distributing as described in the previous section, we propose samples in parallel, improving"
P11-1080,W99-0202,0,0.773167,"nentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al."
P11-1080,D10-1001,0,0.0176713,"Missing"
P11-1080,C98-1012,0,\N,Missing
P12-1040,W99-0201,0,0.0416223,"earch, question answering, and knowledge base construction. For example, coreference is vital for determining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level"
P12-1040,D08-1031,0,0.0397098,"etermining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005"
P12-1040,W06-3606,1,0.606287,"s are the factors of the graphical model that encode the pairwise compatibility functions. constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. However, the solution to this problem is intractable, and even approximate inference methods such as loopy belief propagation can be difficult due to the cubic number of deterministic transitivity constraints. 2.2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. (2006), Culotta and McCallum (2006), Poon and Domingos (2007), amongst others), a Markov chain Monte Carlo algorithm traditionally used for marginal inference, but which can also be tuned for MPE inference. MH is a flexible framework for specifying customized local-search transition functions and provides a principled way of deciding which local search moves to accept. A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. The proposed change is accepted with probability α:   P r(y0 ) q(y|y0 ) α = min 1, (2) P r(y) q(y0 |y) 2 We say that a"
P12-1040,N07-1011,1,0.864923,"That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Li"
P12-1040,P07-1107,0,0.047354,"ndicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons:-CMU,LTI., Topics:{WWW,,IR,,SIGIR}, Name:Jamie,Callan, Ins(tu(ons:, Topics:-IR, Jamie,Callan, Jamie,Callan, Topics:-IR, Topics:-IR, Coref?Name:"
P12-1040,N10-1061,0,0.126678,"B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons:-CMU,LTI., Topics:{WWW,,IR,,SIGIR}, Name:Jamie,Callan, Ins(tu(ons:, Topics:-IR, Jamie,Callan, Jamie,Callan, Topics:-IR, Topics:-IR, Coref?Name:,James,Callan, Ins(tu(ons:-"
P12-1040,P05-1020,0,0.0278145,"h, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Associatio"
P12-1040,D09-1101,0,0.0236021,"ng a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons"
P12-1040,C10-2121,0,0.501252,"Missing"
P12-1040,W99-0202,0,0.0690766,"oiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities requires evaluating just a single factor (red square), corresponding to the new child-parent relationship. number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with"
P12-1040,P11-1080,1,0.744781,"elationship. number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a large number of mentions, many of these coreferent mentions are highly similar to each other. For example, in author coreference the two most common strings that refer"
P12-1040,J01-4004,0,0.36667,", and knowledge base construction. For example, coreference is vital for determining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow sy"
P12-1040,P08-1096,0,0.0246889,"s solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jam"
P12-1040,D08-1067,0,\N,Missing
P12-1075,P08-1004,0,0.430077,"ovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into sets of equivalent or simi"
P12-1075,P11-1054,0,0.013915,"(Banko et al., 2007; Banko and Etzioni, 2008). They employ a self-learner to extract relation instances, but no attempt is made to cluster instances into relations. 719 Yates and Etzioni (2009) present RESOLVER for discovering relational synonyms as a post processing step. Our approach falls into the same category. Moreover, we explore path senses and global features for relation discovery. Many generative probabilistic models have been applied to relation extraction. For example, varieties of topic models are employed for both open domain (Yao et al., 2011) and in-domain relation discovery (Chen et al., 2011; Rink and Harabagiu, 2011). Our approach employs generative models for path sense disambiguation, which achieves better performance than directly applying generative models to unsupervised relation discovery. 6 Conclusion We explore senses of paths to discover semantic relations. We employ a topic model to partition entity pairs of a path into different sense clusters and use hierarchical agglomerative clustering to merge senses into semantic relations. Experimental results show our approach discovers precise relation clusters and outperforms a generative model approach and a clustering metho"
P12-1075,P05-1045,0,0.0203123,"defeat” in sense clusters of pattern “A defeat B”. The two theme features are extracted from generative models, and each is a topic number. Our approach produces sense clusters for each path and semantic relation clusters of the whole data. Table 1 and 2 show some example output. 3 P (e1 (pi ), e2 (pi ), . . . , el (pi )|z1 , z2 , . . . , zl ) = α Experiments We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence. We use their lemmas Path A play B doc theme sen theme lexical words entity names 20:sports Americans, Ireland Yankees, Angels Ecuador, England Redskins, Detroit Red Bulls, F.C. Barcelona sports game yankees beat victory num-num won - 30:entertainment Jean-Pierre Bacri, Jacques Rita Benton, Gay Head Dance Jeanie, Scrabble Meryl Streep, Leilah Kevin Kline, Douglas Fairbanks music books television theater production book film show played plays directed artistic r:theater 25:m"
P12-1075,P04-1053,0,0.588726,"ing (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into s"
P12-1075,W04-2407,0,0.00932681,"t B”. The two theme features are extracted from generative models, and each is a topic number. Our approach produces sense clusters for each path and semantic relation clusters of the whole data. Table 1 and 2 show some example output. 3 P (e1 (pi ), e2 (pi ), . . . , el (pi )|z1 , z2 , . . . , zl ) = α Experiments We carry out experiments on New York Times articles from years 2000 to 2007 (Sandhaus, 2008). Following (Yao et al., 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al., 2005) and dependency parsing (Nivre et al., 2004). We extract dependency paths for each pair of named entities in one sentence. We use their lemmas Path A play B doc theme sen theme lexical words entity names 20:sports Americans, Ireland Yankees, Angels Ecuador, England Redskins, Detroit Red Bulls, F.C. Barcelona sports game yankees beat victory num-num won - 30:entertainment Jean-Pierre Bacri, Jacques Rita Benton, Gay Head Dance Jeanie, Scrabble Meryl Streep, Leilah Kevin Kline, Douglas Fairbanks music books television theater production book film show played plays directed artistic r:theater 25:music/art Daniel Barenboim, recital of Mozart"
P12-1075,N07-1071,0,0.353749,"on beats rival Jonathan Tasini for Senate.” It can also indicate that an athlete A beat B in a sports match, as pair “(Dmitry Tursunov, Andy Roddick)” in “Dmitry Tursunov beat the best American player Andy Roddick.” Moreover, it can mean “physically beat” as pair “(Mr. Harris, Mr. Simon)” in “On Sept. 7, 1999, Mr. Harris fatally beat Mr. Simon.” This is known as polysemy. If we work with patterns alone, our extractor will not be able to differentiate between these cases. Most previous approaches do not explicitly address this problem. Lin and Pantel (2001) assumes only one sense per path. In (Pantel et al., 2007), they augment each relation with its selectional pref712 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 712–720, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics erences, i.e. fine-grained entity types of two arguments, to handle polysemy. However, such fine grained entity types come at a high cost. It is difficult to discover a high-quality set of fine-grained entity types due to unknown criteria for developing such a set. In particular, the optimal granularity of entity types depends on the particular p"
P12-1075,P11-1138,0,0.0150996,"DIRT calculates distributional similarities between different paths to find paths which bear the same semantic relation. It does not employ global topic model features extracted from documents and sentences. Local: This system uses our approach (both sense clustering with topic models and hierarchical clustering), but without global features. Local+Type This system adds entity type features to the previous system. This allows us to compare performance of using global features against entity type features. To determine entity types, we link named entities to Wikipedia pages using the Wikifier (Ratinov et al., 2011) package and extract categories from the Wikipedia page. Generally Wikipedia provides many types for one entity. For example, “Mozart” is a person, musician, pianist, composer, and catholic. As we argued in Section 1, it is difficult to determine the right granularity of the entity types to use. In our experiments, we use all of them as features. In hierarchical clustering, for each sense cluster of a path, we pick the most frequent entity type as a feature. This approach can be seen as a proxy to ISP (Pantel et al., 2007), since selectional preferences are one way of distinguishing multiple s"
P12-1075,P02-1006,0,0.105146,"several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial. 1 Introduction Relation extraction (RE) is the task of determining semantic relations between entities mentioned in text. RE is an essential part of information extraction and is useful for question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and"
P12-1075,D11-1130,0,0.0266903,"RT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs of named entities according to the similarity of context words intervening between them. Hachey (2009) uses topic models to perform dimensionality reduction on features when clustering entity pairs into relations. Bollegala et al. (2010) employ co-clustering to find"
P12-1075,D11-1048,0,0.10302,"e “play” relations between two teams, while a few of them express relations of teams acquiring players from 716 other teams. For example, the entity pair ”(Atlanta Hawks, Dallas Mavericks)” mentioned in sentence ”The Atlanta Hawks acquired point guard Anthony Johnson from the Dallas Mavericks.” This is due to that they share many entity pairs of team-team. 3.3 Baselines We compare our approach against several baseline systems, including a generative model approach and variations of our own approach. Rel-LDA: Generative models have been successfully applied to unsupervised relation extraction (Rink and Harabagiu, 2011; Yao et al., 2011). We compare against one such model: An extension to standard LDA that falls into the framework presented by Yao et al. (2011). Each document consists of a list of tuples. Each tuple is represented by features of the entity pair, as listed in 2.1, and the path. For each document, we draw a multinomial distribution over relations. For each tuple, we draw a relation topic and independently generate all the features. The intuition is that each document discusses one domain, and has a particular distribution over relations. In our experiments, we test different numbers of relati"
P12-1075,P10-1044,0,0.0335633,"ther approaches. Our work is closely related to DIRT (Lin and Pantel, 2001). Both DIRT and our approach represent dependency paths using their arguments. Both use distributional similarity to find patterns representing similar semantic relations. Based on DIRT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs"
P12-1075,P10-1045,0,0.0264497,"work is closely related to DIRT (Lin and Pantel, 2001). Both DIRT and our approach represent dependency paths using their arguments. Both use distributional similarity to find patterns representing similar semantic relations. Based on DIRT, Pantel et al. (2007) addresses the issue of multiple senses per path by automatically learning admissible argument types where two paths are similar. They cluster arguments to fine-grained entity types and rank the associations of a relation with these entity types to discover selectional preferences. Selectional preferences discovery (Ritter et al., 2010; Seaghdha, 2010) can help path sense disambiguation, however, we show that using global features performs better than entity type features. Our approach is also related to feature partitioning in cross-cutting model of lexical semantics (Reisinger and Mooney, 2011). And our sense disambiguation model is inspired by this work. There they partition features of words into views and cluster words inside each view. In our case, each sense of a path can be seen as one view. However, we allow different views to be merged since some views overlap with each other. Hasegawa et al. (2004) cluster pairs of named entities"
P12-1075,W04-3206,0,0.0142868,"del, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial. 1 Introduction Relation extraction (RE) is the task of determining semantic relations between entities mentioned in text. RE is an essential part of information extraction and is useful for question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2004) and many other applications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollega"
P12-1075,D11-1135,1,0.926168,"pplications. A common approach to RE is to assume that relations to be extracted are part of a predefined ontology. For example, the relations are given in knowledge bases such as Freebase (Bollacker et al., 2008) or DBpedia (Bizer et al., 2009). However, in many applications, ontologies do not yet exist or have low coverage. Even when they do exist, their maintenance and extension are considered to be a substantial bottleneck. This has led to considerable interest in unsupervised relation discovery (Hasegawa et al., 2004; Banko and Etzioni, 2008; Lin and Pantel, 2001; Bollegala et al., 2010; Yao et al., 2011). Here, the relation extractor simultaneously discovers facts expressed in natural language, and the ontology into which they are assigned. Many relation discovery methods rely exclusively on the notion of either shallow or syntactic patterns that appear between two named entities (Bollegala et al., 2010; Lin and Pantel, 2001). Such patterns could be sequences of lemmas and Part-of-Speech tags, or lexicalized dependency paths. Generally speaking, relation discovery attempts to cluster such patterns into sets of equivalent or similar meaning. Whether we use sequences or dependency paths, we wil"
P12-1075,P08-1000,0,\N,Missing
P13-1104,D12-1133,0,0.528635,"ter w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is the maximum number of transitions performed t"
P13-1104,W06-2920,0,0.427147,"Missing"
P13-1104,W08-2102,0,0.0500522,"Missing"
P13-1104,cer-etal-2010-parsing,0,0.0588043,"Missing"
P13-1104,P11-2121,1,0.8603,"ies as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohne"
P13-1104,P12-2071,1,0.352871,"ith our new hybrid parsing algorithm, A DAG RAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed. It is interesting to see that our greedy parser outperformed most other greedy dependency parsers. This is because our parser used both bootstrapping and Zhang and Nivre (2011)’s non-local features, which had not been used by other greedy parsers. In the future, we will experiment with more advanced dependency representations (de Marneffe and Manning, 2008; Choi and Palmer, 2012b) to show robustness of our approach. Furthermore, we will evaluate individual methods of our approach separately to show impact of each method on parsing performance. We also plan to implement the typical beam search approach to make a direct comparison to our selectional branching.8 Acknowledgments Special thanks are due to Luke Vilnis of the University of Massachusetts Amherst for insights on 8 Our parser is publicly available under an open source project, ClearNLP (clearnlp.googlecode.com). the A DAG RAD derivation. We gratefully acknowledge a grant from the Defense Advanced Research Proj"
P13-1104,W02-1001,0,0.240565,"Missing"
P13-1104,W08-1301,0,0.0264151,"Missing"
P13-1104,D12-1019,0,0.00958663,"ined with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44"
P13-1104,D12-1029,0,0.0246076,"Missing"
P13-1104,N10-1115,0,0.0497612,"Missing"
P13-1104,C12-1059,0,0.0658165,"evelopment set (lines 2-4). Then, the next model Mr is trained on all data but this time, Mr−1 is used to generate multiple transition sequences (line 7-8). Among all transition sequences generated by Mr−1 , training instances from only T1 and Tg are used to train Mr , where T1 is the one-best sequence and Tg is a sequence giving the most accurate parse output compared to the gold-standard tree. The score of Mr is measured (line 9), and repeat the procedure if Sr−1 &lt; Sr ; otherwise, return the previous model Mr−1 . 5 Adaptive subgradient algorithm Alternatively, the dynamic oracle approach of Goldberg and Nivre (2012) can be used to generate multiple transition sequences, which is expected to show similar results. y = y0 otherwise The algorithm takes three hyper-parameters; T is the number of iterations, α is the learning rate, and ρ is the ridge (T &gt; 0, α &gt; 0, ρ ≥ 0). G is our running estimate of a diagonal covariance matrix for the gradients (per-coordinate learning rates). For each instance, scores for all labels are measured by the logistic regression function f (x, y) in Section 3.3. These scores are subtracted from an output of the indicator function I(y, y 0 ), which forces our model to keep learnin"
P13-1104,P06-2041,0,0.0609051,"Missing"
P13-1104,P08-1068,0,0.0798175,"Missing"
P13-1104,J93-2004,0,0.0428576,"Missing"
P13-1104,D10-1004,0,0.036365,"Missing"
P13-1104,E06-1011,0,0.0953577,"Missing"
P13-1104,P05-1012,0,0.423524,"Missing"
P13-1104,W06-2932,0,0.010059,"012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75.66 83.29 77.46 84.65 Swedish LAS UAS 84.58 89.50 82.55 88.93 83.55 89.30 84.66 89.80 86.32 91.12 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. 4.5 Non-pro"
P13-1104,P08-1108,0,0.0104809,"cy parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75.66 83.29 77.46 84.65 Swedish LAS UAS 84.58 89.50 82.55 88.93 83.55 89.30 84.66 89.80 86.32 91.12 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-a"
P13-1104,P10-1110,0,0.705806,"state 5) so it can be compared to other tokens in β (state 10). After w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e t"
P13-1104,P05-1013,0,0.0815568,"on Transition-based dependency parsing has gained considerable interest because it runs fast and performs accurately. Transition-based parsing gives complexities as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-a"
P13-1104,N12-1015,0,0.027483,"Missing"
P13-1104,W06-2933,0,0.052667,"Missing"
P13-1104,P10-1001,0,0.18735,"s for IO, feature extraction and bootstrapping. pendency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach Zhang and Clark (2008) Huang and Sagae (2010) Zhang and Nivre (2011) Bohnet and Nivre (2012) McDonald et al. (2005) Mcdonald and Pereira (2006) Sagae and Lavie (2006) Koo and Collins (2010) Zhang and McDonald (2012) Martins et al. (2010) Rush et al. (2010) Koo et al. (2008) Carreras et al. (2008) Bohnet and Nivre (2012) Suzuki et al. (2009) bt = 80, bd = 80, m = 0.88 bt = 80, bd = 64, m = 0.88 bt = 80, bd = 32, m = 0.88 bt = 80, bd = 16, m = 0.88 bt = 80, bd = 8, m = 0.88 bt = 80, bd = 4, m = 0.88 bt = 80, bd = 2, m = 0.88 bt = 80, bd = 1, m = 0.88 bt = 1, bd = 1, m = 0.88 1,200,000 Transitions 1,000,000 800,000 600,000 400,000 200,000 0 0 10 20 30 40 50 60 70 80 Beam size = 1, 2, 4, 8, 16, 32, 64, 80 Figure 5: The total number of transitions performed during decoding with respe"
P13-1104,W03-3017,0,0.196237,"sent a new transition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2 ). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Transition Current state L EFTl -R EDUCE l ( [σ|i], δ, [j|β], A ) ⇒ ( σ, δ, [j|β], A ∪ {i ← j} ) l ( [σ|i], δ, [j|β], A ) ⇒ ( [σ|i|δ|j], [ ], β, A ∪ {i"
P13-1104,W04-0308,0,0.162124,"hoi and Palmer (2011) who integrated our L EFT-R EDUCE transition into Nivre’s list-based algorithm. Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2 ) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2 ) 1059 for both cases; this is possible because of the new integration of the R IGHT-S HIFT and N O -R EDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a S WAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS ta"
P13-1104,P11-2033,0,0.696046,"ompared to other tokens in β (state 10). After w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is"
P13-1104,J08-4003,0,0.693883,"ansition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2 ). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Transition Current state L EFTl -R EDUCE l ( [σ|i], δ, [j|β], A ) ⇒ ( σ, δ, [j|β], A ∪ {i ← j} ) l ( [σ|i], δ, [j|β], A ) ⇒ ( [σ|i|δ|j], [ ], β, A ∪ {i → j} ) ( [σ|i"
P13-1104,P09-1040,0,0.302801,"ves complexities as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhan"
P13-1104,N12-1054,0,0.0163317,".88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75."
P13-1104,D10-1001,0,0.051107,"Missing"
P13-1104,N06-2033,0,0.100696,"Missing"
P13-1104,D09-1058,0,0.0337446,"Missing"
P13-1104,W03-3023,0,0.51149,"Missing"
P13-1104,D08-1059,0,0.706933,"ch projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about"
P13-1104,D12-1030,0,0.030318,"Missing"
P14-1056,P04-1056,0,0.039695,"respect to the base chain-structured CRF tagger and does not include global constraints. Note that it may make sense to consider a constraint that is sometimes violated in the ground truth, as the penalty learning algorithm can learn a small penalty for it, which 6 Related Work There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belief propagation is prohibitively expensive in our 598 once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In our experiments, we demonstrate that the specific global constraints used by Chang et al. (2012) help on the UMass dataset as well. model due to the high cardinalities of the output variables and of the global factors, which involve all output vari"
P14-1056,W02-1001,0,0.0574249,"of soft-constraint dual decomposition to existing and new NLP problems. 3.1 Learning Penalties 4 One consideration when using soft v.s. hard constraints is that soft constraints present a new training problem, since we need to choose the vector c, the penalties for violating the constraints. An important property of problem (5) in the previous section is that it corresponds to a structured linear model over y and z. Therefore, we can apply known training algorithms for estimating the parameters of structured linear models to choose c. All we need to employ the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al., 2004) is a blackbox procedure for performing MAP inference in the structured linear model given an arbitrary cost vector. Fortunately, the MAP problem for (5) can be solved using Soft-DD, in Algorithm 2. Each penalty ci has to be non-negative; otherwise, the optimization problem in equation (5) is ill-defined. This can be ensured by simple modifications of the perceptron and subgradient descent optimization of the structured SVM objective simply by truncating c coordinate-wise to be non-negative at every learning iteration. Intuitively,"
P14-1056,D10-1001,0,0.0605588,"hor section, accuracy could be improved. One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities. When hard constraints can be encoded as linear equations on the output variables, and the underlying model’s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) (Roth and Yih, 2004). Alternatively, one can employ dual decomposition (Rush et al., 2010). Dual decompositions’s advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples. The above two approaches have previously been applied to impose hard constraints on a model’s output. On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012). Here, the model is not required obey the global constraints, but merely pays a pe"
P14-1056,D12-1131,0,0.499997,"Missing"
P14-1056,P05-1045,0,0.0844947,"obal constraints. Note that it may make sense to consider a constraint that is sometimes violated in the ground truth, as the penalty learning algorithm can learn a small penalty for it, which 6 Related Work There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belief propagation is prohibitively expensive in our 598 once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In our experiments, we demonstrate that the specific global constraints used by Chang et al. (2012) help on the UMass dataset as well. model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously. There are various methods for exploiting the c"
P14-1056,D10-1125,0,0.130803,"Missing"
P14-1056,N12-1024,0,0.0243355,"Missing"
P14-1056,N04-1042,1,0.795779,"roblem can be formulated as a structured linear model similar to equation (1), for which we have a MAP algorithm, but where we have imposed some additional constraints Ay ≤ b that no longer allow us to use the algorithm. In other The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chainstructured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). We produce a prediction by performing MAP inference (Koller and Friedman, 2009). The MAP inference task in a CRF be can expressed as an optimization problem with a lin594 Algorithm 1 DD: projected subgradient for dual decomposition with hard constraints parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables (Cortes and Vapnik, 1995). Note that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model’s score by too much. Consider the optimization problems of t"
P14-1056,W04-2401,0,0.0501838,"e could enforce the global constraint that there should be only one author section, accuracy could be improved. One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities. When hard constraints can be encoded as linear equations on the output variables, and the underlying model’s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) (Roth and Yih, 2004). Alternatively, one can employ dual decomposition (Rush et al., 2010). Dual decompositions’s advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples. The above two approaches have previously been applied to impose hard constraints on a model’s output. On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012). Here, the m"
P15-1015,W08-1301,0,0.0182265,"Missing"
P15-1015,gimenez-marquez-2004-svmtool,0,0.12191,"Missing"
P15-1015,D08-1031,0,0.0803691,"erates a sequence of shift-reduce parsing transitions (Nivre, 2009). The use of sequential prediction to solve these problems and others has a long history in practice as well as theory. Searn (Daum´e III et al., 2009) and DAgger (Ross et al., 2011) are two popular principled frameworks for reducing sequential prediction to classification by learning a classifier on additional synthetic training data. However, as we do in our experiments, practitioners often see good results by training on the gold standard labels with an off-the-shelf classification algorithm, as though classifying IID data (Bengtson and Roth, 2008; Choi and Palmer, 2012). Classifier-based approaches to structured prediction are faster than dynamic programming since they consider only a subset of candidate output structures in a greedy manner. For example, the Stanford CoreNLP classifier-based partof-speech tagger provides a 6.5x speed advantage over their dynamic programming-based model, with little reduction in accuracy. Because our methods are designed for the greedy sequential prediction regime, we can provide further speed increases to the fastest inference methods in NLP. 3 Linear models Our base classifier for sequential predicti"
P15-1015,J93-2004,0,0.0556365,"et), the only difference being Experimental Results We present experiments on three NLP tasks for which greedy sequence labeling has been a successful solution: part-of-speech tagging, transition-based dependency parsing and named entity recognition. In all cases our method achieves multiplicative speedups at test time with little loss in accuracy. 6.1 Model/m Part-of-speech tagging We conduct our experiments on classifier-based greedy part-of-speech tagging. Our baseline tagger uses the same features described in Choi and Palmer (2012). We evaluate our models on the Penn Treebank WSJ corpus (Marcus et al., 1993), employing the typical split of sections used for part-of-speech tagging: 0-18 train, 19-21 development, 22-24 test. The parameters of our models are learned using AdaGrad (Duchi et al., 2011) with `2 regularization via regularized dual averaging (Xiao, 2009), and we used random search on the development set to select hyperparameters. This baseline model (baseline) tags at a rate of approximately 23,000 tokens per second on a 2010 2.1GHz AMD Opteron machine with accuracy comparable to similar taggers (Gim´enez and M`arquez, 2004; Choi and Palmer, 2012; Toutanova et al., 2003). On the same mac"
P15-1015,P11-2121,0,0.0267457,"ubset of templates, we use a different baseline model for each one of the 46 total template prefixes, learned with only those features; we then compare the test accuracy of our dynamic model using template prefix i to the baseline model trained on the fixed prefix i. Our model performs just as well as these separately trained models, demonstrating that our objective learns weights that allow each prefix to act as its own high-quality classifier. 6.2 Transition-based dependency parsing We base our parsing experiments on the greedy, non-projective transition-based dependency parser described in Choi and Palmer (2011). Our model uses a total of 60 feature templates based mainly on the word form, POS tag, lemma and assigned head label of current and previous input and stack tokens, and parses about 300 sentences/second on a modest 2.1GHz AMD Opteron machine. We train our parser on the English Penn TreeBank, learning the parameters using AdaGrad and the parsing split, training on sections 2–21, testing on section 23 and using section 22 for development and the Stanford dependency framework (de 152 Model/m LAS UAS Feat. Speed Baseline Fixed Fixed Fixed Fixed Dynamic/6.5 Dynamic/7.1 Dynamic/10 Dynamic/11 90.31"
P15-1015,D11-1139,0,0.554596,"etermine when to add features. Such heavier-weight approaches are unsuitable for our setting, where the core classifier’s features and scoring are already so cheap that adding complex decision-making would cause too much computational overhead. Other previous work on cascades uses a series of increasingly complex models, such as the Viola-Jones face detection cascade of classifiers (2001), which applies boosted trees trained on 150 exists related work in the field of general (static) feature selection. The most relevant results come from the applications of group sparsity, such as the work of Martins et al. (2011) in Group Lasso for NLP problems. The Group Lasso regularizer (Yuan and Lin, 2006) sparsifies groups of feature weights (e.g. feature templates), and has been used to speed up test-time prediction by removing entire templates from the model. The key difference between this work and ours is that we select our templates based on the test-time difficulty of the inference problem, while the Group Lasso must do so at train time. In Appendix A, we compare against Group Lasso and show improvements in accuracy and speed. Note that non-grouped approaches to selecting sparse feature subsets, such as boo"
P15-1015,P12-2071,0,0.397848,"t-reduce parsing transitions (Nivre, 2009). The use of sequential prediction to solve these problems and others has a long history in practice as well as theory. Searn (Daum´e III et al., 2009) and DAgger (Ross et al., 2011) are two popular principled frameworks for reducing sequential prediction to classification by learning a classifier on additional synthetic training data. However, as we do in our experiments, practitioners often see good results by training on the gold standard labels with an off-the-shelf classification algorithm, as though classifying IID data (Bengtson and Roth, 2008; Choi and Palmer, 2012). Classifier-based approaches to structured prediction are faster than dynamic programming since they consider only a subset of candidate output structures in a greedy manner. For example, the Stanford CoreNLP classifier-based partof-speech tagger provides a 6.5x speed advantage over their dynamic programming-based model, with little reduction in accuracy. Because our methods are designed for the greedy sequential prediction regime, we can provide further speed increases to the fastest inference methods in NLP. 3 Linear models Our base classifier for sequential prediction tasks will be a linea"
P15-1015,P09-1040,0,0.0604219,"Missing"
P15-1015,W09-1119,0,0.271986,"Missing"
P15-1015,N12-1054,0,0.0475702,"sifier-based NLP methods that are already very fast and have relatively cheap features. Some cascaded approaches strive at each stage to prune the number of possible output structures under consideration, whereas in our case we focus on pruning the input features. For example, Xu et al. (2013) learn a tree of classifiers that subdivides the set of classes to minimize average testtime cost. Chen et al. (2012) similarly use a linear cascade instead of a tree. Weiss and Taskar (2010) prune output labels in the context of structured prediction through a cascade of increasingly complex models, and Rush and Petrov (2012) successfully apply these structured prediction cascades to the task of graph-based dependency parsing. In the context of NLP, He et al. (2013) describe a method for dynamic feature template selection at test time in graph-based dependency parsing. Their technique is particular to the parsing task— making a binary decision about whether to lock in edges in the dependency graph at each stage, and enforcing parsing-specific, hard-coded constraints on valid subsequent edges. Furthermore, as described above, they employ an auxiliary model to select features. He and Eisner (2012) share our goal to"
P15-1015,N03-1033,0,0.179084,"reebank WSJ corpus (Marcus et al., 1993), employing the typical split of sections used for part-of-speech tagging: 0-18 train, 19-21 development, 22-24 test. The parameters of our models are learned using AdaGrad (Duchi et al., 2011) with `2 regularization via regularized dual averaging (Xiao, 2009), and we used random search on the development set to select hyperparameters. This baseline model (baseline) tags at a rate of approximately 23,000 tokens per second on a 2010 2.1GHz AMD Opteron machine with accuracy comparable to similar taggers (Gim´enez and M`arquez, 2004; Choi and Palmer, 2012; Toutanova et al., 2003). On the same machine 151 Figure 1: Left-hand plot depicts test accuracy as a function of the average number of templates used to predict. Right-hand plot shows speedup as a function of accuracy. Our model consistently achieves higher accuracy while using fewer templates resulting in the best ratio of speed to accuracy. that we varied the margin at test time. Superior results for m 6= 50 could likely be obtained by optimizing hyperparameters for the desired margin. 6.1.1 Learning the template ordering As described in Section 4.5, we experimented on part-of-speech tagging with three different a"
P15-1015,W13-3518,0,\N,Missing
P15-1015,D13-1152,0,\N,Missing
P15-1016,D10-1115,0,0.0149426,"well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Car"
P15-1016,P11-1062,0,0.00871478,"the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). Table 4: Results comparing the zero-shot model with supervised RNN and a random baseline on 10 types. RNN is the fully supervised model described in section 3 while zero-shot is the model described in section 4. The zero-shot model without explicitly training for the target relation types achieves impressive results by performing significantly (p < 0.05) better than a random baseline. 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inferen"
P15-1016,D13-1080,0,0.112101,"een during training, because of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion. This allows our model to seamlessly perform inference on many millions of paths in the KB graph. In most of our experiments, we learn a separate RNN for predicting each relation type, but alternatively, by learning a single high-capacity composition function for all relation types, our method can perform zero-shot learning—predicting new relation types for which the composition function was never explicitly trained. Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem—but the core mechanism continues to be a classifier based on atomic-path features. In the 2013 work many paths are collapsed by clustering paths according to their relations’ embeddings, and substituting cluster ids for the original relation types. In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is measured using the embeddings. Neither is able to perform zero-shot learning since there must be a classifer for each predicted relati"
P15-1016,P08-1028,0,0.0191586,"heir method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher"
P15-1016,D14-1044,0,0.675327,"ecause of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion. This allows our model to seamlessly perform inference on many millions of paths in the KB graph. In most of our experiments, we learn a separate RNN for predicting each relation type, but alternatively, by learning a single high-capacity composition function for all relation types, our method can perform zero-shot learning—predicting new relation types for which the composition function was never explicitly trained. Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem—but the core mechanism continues to be a classifier based on atomic-path features. In the 2013 work many paths are collapsed by clustering paths according to their relations’ embeddings, and substituting cluster ids for the original relation types. In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is measured using the embeddings. Neither is able to perform zero-shot learning since there must be a classifer for each predicted relation type. Furthermore th"
P15-1016,D14-1113,1,0.782819,"013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014). µλ = arg max vp (π).vr (δ) π∈Φδ (γ) During training, we assign µλ using the current parameter estimates. We use the same procedure to assign µλ for unobserved facts that are used as negative examples during training. We train a separate RNN model for predicting each relation and the parameters of the model for predicting relation δ ∈ ∆ are Θ = {vr (ω)∀ω ∈ ∆, Wδ }. Given a training set consisting of posi1 we did not get significant improvements when we tried more sophisticated ordering schemes for computing the path representations. 159 − 2 tive (Λ+ δ ) and negative (Λδ ) instances for relati"
P15-1016,D14-1070,0,0.0431519,"Missing"
P15-1016,D11-1049,0,0.670121,"representing a sort of inconsistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion. Early work on this problem focused on learning symbolic rules. For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold. (A “path” is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOfHeadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOfHeadquarters(Microsoft, USA) using the predictive feature. In later wor"
P15-1016,D12-1093,0,0.102077,"troduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOfHeadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOfHeadquarters(Microsoft, USA) using the predictive feature. In later work, Lao et al. (2012) greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations (Banko et al., 2007)). However, these symbolic methods can produce many millions of distinct paths, each of which is categorically distinct, treated by PRA as a disKnowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational syn"
P15-1016,N13-1008,1,0.898085,"ns, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. Our compositional approach allow us at test time to make predictions from paths that were unseen during training, because of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion. This allows our mo"
P15-1016,D10-1106,0,0.0584553,"act dentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion. Early work on this problem focused on learning symbolic rules. For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold. (A “path” is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryO"
P15-1016,D12-1110,0,0.0578464,"(Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is seen paths that are very different from the paths seen is training. Empirically, combining the predictions of RNN and PRA Classifier-b achieves a statistically significant gain over PRA Classifier-b. 5.3.1 Zero-shot Table 4 shows the results of the zero-shot model described in section 4 compared with the fully supervised RNN model (section 3) and a baseline that produces a random ordering of the test facts. We evaluate on"
P15-1016,N13-1095,0,0.27397,"Missing"
P15-1016,P09-1113,0,0.124954,"elation δ. In our task, we only observe the set of paths connecting an entity pair but we do not observe which of the path(s) is predictive of the fact. We treat this as a latent variable (µλ for the fact λ) and we assign µλ the path whose vector representation has maximum dot product with the vector representation of the relation to be predicted. For example, µλ for the fact λ = (γ, δ) ∈ Λ+ is given by, Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RN"
P15-1016,D13-1170,0,0.00480651,"nnecting entity pair (Microsoft, USA). Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. Our compositional approach allow us at test time to make predictions from paths that were unseen during training, b"
P15-1016,N07-1016,0,0.015929,"de vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). Table 4: Results comparing the zero-shot model with supervised RNN and a random baseline on 10 types. RNN is the fully supervised model described in section 3 while zero-shot is the model described in section 4. The zero-shot model without explicitly training for the target relation types achieves impressive results by performing significantly (p < 0.05) better than a random baseline. 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et"
P15-1016,D11-1016,0,0.0150004,"epends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering"
P17-2057,N16-1181,0,0.0155087,"uestion answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a). Our work differs from them in two ways: 1) we do not need an explicit database query to retrieve the answers (Neelakantan et al., 2015a; Andreas et al., 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al., 2007; Carlson et al., 2010). Related Work A majority of the QA literature that focused on exploiting KB and text either improves the infer362 6 Conclusions In this work, we showed universal schema is a promising knowledge source for QA than using KB or text alone. Our results conclude though KB is preferred over text when the KB contains the fact of interest, a large portion of queries still attend to text indicating the amalgam of both text and KB is superior than KB alone. Acknowledgment"
P17-2057,D13-1160,0,0.0804093,"rsal schema. For evaluation, literature offers two options: 1) datasets for text-based question answering tasks such as answer sentence selection and reading comprehension; and 2) datasets for KB question answering. Although the text-based question answering datasets are large in size, e.g., SQuAD (Rajpurkar et al., 2016) has over 100k questions, answers to these are often not entities but rather sentences which are not the focus of our work. Moreover these texts may not contain Freebase entities at all, making these skewed heavily towards text. Coming to the alternative option, WebQuestions (Berant et al., 2013) is widely used for QA on Freebase. This dataset is curated such that all questions can be answered on Freebase alone. But since our goal is to explore the impact of universal schema, testing on a dataset completely answerable on a KB is not ideal. WikiMovies dataset (Miller et al., 2016) also has similar properties. Gardner and Krishnamurthy (2017) created a dataset with motivations similar to 360 Model Bisk et al. (2016) O NLY KB O NLY T EXT E NSEMBLE . U NI S CHEMA Dev. F1 Test F1 Question Answer 32.7 39.1 25.3 39.4 41.1 31.4 38.5 26.6 38.6 39.9 1. USA have elected blank , our first african"
P17-2057,P07-1073,0,0.0121583,", e.g., BASEBALL (Green Jr et al., 1961). This problem has matured into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguis"
P17-2057,P15-1127,0,0.0230433,"y sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu"
P17-2057,D15-1038,0,0.0141344,"ce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader"
P17-2057,D14-1117,0,0.0110954,"performs O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logica"
P17-2057,D12-1069,0,0.0101355,"age is less than 16 facts per entity, U NI S CHEMA outperforms O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal withou"
P17-2057,D16-1147,0,0.292628,"r textual relations. 359 Memory Networks MemNNs are neural attention models with external and differentiable memory. MemNNs decouple the memory component from the network thereby allowing it store external information. Previously, these have been successfully applied to question answering on KB where the memory is filled with distributed representation of KB triples (Bordes et al., 2015), or for reading comprehension (Sukhbaatar et al., 2015; Hill et al., 2016), where the memory consists of distributed representation of sentences in the comprehension. Recently, key-value MemNN are introduced (Miller et al., 2016) where each memory slot consists of a key and value. The attention weight is computed only by comparing the question with the key memory, whereas the value is used to compute the contextual representation to predict the answer. We use this variant of MemNN for our model. Miller et al. (2016), in their experiments, store either KB triples or sentences as memories but they do not explicitly model multiple memories containing distinct data sources like we do. 3 Model Our model is a MemNN with universal schema as its memory. Figure 1 shows the model architecture. Memory: Our memory M comprise of b"
P17-2057,P09-1113,0,0.110593,"et al., 1961). This problem has matured into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org"
P17-2057,P15-1016,1,0.327275,"obust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with Op"
P17-2057,D16-1264,0,0.0206994,"aset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 F1 points. 1 The paradigm of exploiting text for questions started in the early 1990s (Kupiec, 1993). With the advent of web, access to text resources became abundant and cheap. Initiatives like TREC QA competitions helped popularizing this paradigm (Voorhees et al., 1999). With the recent advances in deep learning and availability of large public datasets, there has been an explosion of research in a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016). Still, text representation is unstructured and does not allow the compositional reasoning which structured KB supports. Introduction Question Answering (QA) has been a longstanding goal of natural language processing. Two main paradigms evolved in solving this problem: 1) answering questions on a knowledge base; and 2) answering questions using text. Knowledge bases (KB) contains facts expressed in a fixed schema, facilitating compositional reasoning."
P17-2057,Q14-1030,1,0.844524,"ty, U NI S CHEMA outperforms O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the anno"
P17-2057,N13-1008,1,0.92922,"ions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a dquar rg 1 tere d ... of nt_ ide ... res kb: p 1 arg non 2 is t h arg -whit e firs ep t 1 res ide nt o ... f y pan _co m ty has _ci has kb: kb: USA/ Obama k ald rac ry lla USA NYC.. .. .. Ba ama .. .. .. .. .. Don p .. Hi m . . . Ob . . . . . Tru . 1 Affine+Softmax ..."
P17-2057,W14-4504,0,0.0805068,"Missing"
P17-2057,N16-1103,1,0.601109,"question answer pairs are shown in Table 2. Universal Schema Traditionally universal schema is used for relation extraction in the context of knowledge base population. Rows in the schema are formed by entity pairs (e.g. USA, NYC), and columns represent the relation between them. A relation can either be a KB relation, or it could be a pattern of text that exist between these two entities in a large corpus. The embeddings of entities and relation types are learned by low-rank matrix factorization techniques. Riedel et al. (2013) treat textual patterns as static symbols, whereas recent work by Verga et al. (2016) replaces them with distributed representation of sentences obtained by a RNN. Using distributed representation allows reasoning on sentences that are similar in meaning but different on the surface form. We too use this variant to encode our textual relations. 359 Memory Networks MemNNs are neural attention models with external and differentiable memory. MemNNs decouple the memory component from the network thereby allowing it store external information. Previously, these have been successfully applied to question answering on KB where the memory is filled with distributed representation of K"
P17-2057,C16-1226,0,0.0793455,"Missing"
P17-2057,P16-1220,1,0.730734,"as O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Ya"
P17-2057,D10-1099,1,0.857893,"into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a"
P17-2057,P14-1090,0,0.109472,"Missing"
P17-2057,P15-1128,0,0.0642788,"U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated"
P17-2057,D15-1203,0,0.0162448,"ntic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a dquar rg 1 tere d ."
P18-1010,P17-2057,1,0.900326,"Missing"
P18-1010,D15-1103,0,0.275845,"f granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task. A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017). The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012). Del Corro et al. (2015) use manually crafted rules and patterns (Hearst patterns (Hearst, 1992), appositives, etc) to automatiTable 8 shows qualitative predictions for models with and without hierarchy information incorporated. Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity. In the first and second example, the baseline model becomes extremely dependent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences). This shows that modeling the structure of the entity hierarchy helps"
P18-1010,P16-1200,0,0.0153049,"des a context dependent representation. 4 4.1 0≤i≤n−w+1 ci Training Mention-Level Typing Mention level entity typing is treated as multilabel prediction. Given the sentence vector mF , we compute a score for each type in typeset T as: Token Representation Each sentence is made up of s tokens which are mapped to dw dimensional word embeddings. Because sentences may contain mentions of more than one entity, we explicitly encode a distinguished mention in the text using position embeddings which have been shown to be useful in state of the art relation extraction models (dos Santos et al., 2015; Lin et al., 2016) and machine translation (Vaswani et al., 2017). Each word embedding is concatenated with a dp dimensional learned position embedding encoding the token’s relative distance to the target entity. Each token within the distinguished mention span has position 0, tokens to the left have a negative distance from [−s, 0), and tokens to the right of the mention span have a positive distance from (0, s]. We denote the final sequence of token representations as M . 3.2.2 max w W [j]M [i − b c + j]) 2 Each W [j] ∈ Rd×d is a CNN filter, the bias b ∈ Rd , M [i] ∈ Rd is a token representation, and the max"
P18-1010,Q14-1037,0,0.0512989,"ction, knowledge base population, and completion. Our structural, hierarchy-aware loss between types and entities draws on research in Knowledge Base Inference such as Jain et al. (2018), Trouillon et al. (2016) and Nickel et al. (2011). Combining KB completion with hierarchical structure in knowledge bases has been explored in (Dalvi et al., 2015; Xie et al., 2016). Recently, Wu et al. (2017) proposed a hierarchical loss for text classification. Linking mentions to a flat set of entities, often in Freebase or Wikipedia, is a long-standing task in NLP (Bunescu and Pasca, 2006; Cucerzan, 2007; Durrett and Klein, 2014; Francis-Landau et al., 2016). Typing of mentions at varying levels of granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task. A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017). The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarc"
P18-1010,N16-1150,0,0.0200648,"ulation, and completion. Our structural, hierarchy-aware loss between types and entities draws on research in Knowledge Base Inference such as Jain et al. (2018), Trouillon et al. (2016) and Nickel et al. (2011). Combining KB completion with hierarchical structure in knowledge bases has been explored in (Dalvi et al., 2015; Xie et al., 2016). Recently, Wu et al. (2017) proposed a hierarchical loss for text classification. Linking mentions to a flat set of entities, often in Freebase or Wikipedia, is a long-standing task in NLP (Bunescu and Pasca, 2006; Cucerzan, 2007; Durrett and Klein, 2014; Francis-Landau et al., 2016). Typing of mentions at varying levels of granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task. A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017). The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from it"
P18-1010,W02-0109,0,0.536191,"Missing"
P18-1010,P09-1113,0,0.0680749,"t of the CNN is concatenated with the mean of mention surface form embeddings, and then passed through a 2 layer MLP. 3.2.1 w X yj = t j > m F where tj is the embedding for the jth type in T and yj is its corresponding score. The mention is labeled with tm , a binary vector of all types where th tm j = 1 if the j type is in the set of gold types for m and 0 otherwise. We optimize a multi-label binary cross entropy objective: X m Ltype (m) = − tm j log yj + (1 − tj ) log(1 − yj ) j 4.2 Entity-Level Typing In the absence of mention-level annotations, we instead must rely on distant supervision (Mintz et al., 2009) to noisily label all mentions of entity e with all types belonging to e. This procedure inevitably leads to noise as not all mentions of an entity express each of its known types. To alleviate this noise, we use multi-instance multi-label learning (MIML) (Surdeanu et al., 2012) which operates over bags rather than mentions. A bag of mentions Be = {m1 , m2 , . . . , mn } is the set of Sentence Representation The embedded sequence M is then fed into our context encoder. Our context encoder is a single layer CNN followed by a tanh non-linearity to produce C. The outputs are max pooled across 100"
P18-1010,D17-1284,0,0.211853,"ous to learning embeddings for nodes of a knowledge graph with a single hypernym/IS - A relation. To train these embeddings, we sample (c1 , c2 ) pairs, where each pair is a positive link in our hierarchy. For each positive link, we sample a set N of n negative links. We encourage the model to output high scores for positive links, and low scores for negative links via a binary cross entropy (BCE) loss: Lstruct = − log σ(s(c1i , c2i )) X + log(1 − σ(s(c1i , c02i ))) N 6.2.1 Results In Table 5 we see that our base CNN models (CNN and CNN+Complex) match LSTM models of Shimaoka et al. (2017) and Gupta et al. (2017), the L = Ltype/link + γLstruct 1 This step makes the scoring function technically not bilinear, as it commutes with addition but not complex multiplication, but we term it bilinear for ease of exposition. 102 Model Ling and Weld (2012) Shimaoka et al. (2017) † Gupta et al. (2017)† Shimaoka et al. (2017)‡ CNN + hierarchy CNN+Complex + hierarchy Acc 47.4 55.6 57.7 59.6 57.0 58.4 57.2 59.7 Macro F1 69.2 75.1 72.8 78.9 75.0 76.3 75.3 78.3 Micro F1 65.5 71.7 72.1 75.3 72.2 73.6 72.9 75.4 Model CNN + hierarchy + transitive + hierarchy + transitive CNN+Complex + hierarchy + transitive + hierarchy +"
P18-1010,N15-1054,0,0.0567521,"ama is the President of the United States.” with the identified entity string Barack Obama. In the task of entity linking, we want to map m to a specific entity in a knowledge base such as “m/02mjmr” in Freebase. In mention-level typing, we label m with one or more types from our type system T such as tm = {president, leader, politician} (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017). In entity-level typing, we instead consider a bag of mentions Be which are all linked to the same entity. We label Be with te , the set of all types expressed in all m ∈ Be (Yao et al., 2013; Neelakantan and Chang, 2015; Verga et al., 2017; Yaghoobzadeh et al., 2017a). Gold KB links Yes No Yes Yes No Yes No Yes 3.2 Mention Encoder Our model converts each mention m to a d dimensional vector. This vector is used to classify the type or entity of the mention. The basic model depicted in Figure 1 concatenates the averaged word embeddings of the mention string with the output of a convolutional neural network (CNN). The Table 3: Statistics from various type sets. TypeNet is the largest type hierarchy with a gold mapping to KB entities. *The entire WordNet could be added to TypeNet increasing the total size to 17k"
P18-1010,C92-2082,0,0.134946,"e Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task. A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017). The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012). Del Corro et al. (2015) use manually crafted rules and patterns (Hearst patterns (Hearst, 1992), appositives, etc) to automatiTable 8 shows qualitative predictions for models with and without hierarchy information incorporated. Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity. In the first and second example, the baseline model becomes extremely dependent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences). This shows that modeling the structure of the entity hierarchy helps the model disambiguate rare entities. In the third example, structure he"
P18-1010,W03-0419,0,0.379068,"Missing"
P18-1010,D14-1162,0,0.0800293,"Missing"
P18-1010,E17-1058,1,0.845044,"nited States.” with the identified entity string Barack Obama. In the task of entity linking, we want to map m to a specific entity in a knowledge base such as “m/02mjmr” in Freebase. In mention-level typing, we label m with one or more types from our type system T such as tm = {president, leader, politician} (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017). In entity-level typing, we instead consider a bag of mentions Be which are all linked to the same entity. We label Be with te , the set of all types expressed in all m ∈ Be (Yao et al., 2013; Neelakantan and Chang, 2015; Verga et al., 2017; Yaghoobzadeh et al., 2017a). Gold KB links Yes No Yes Yes No Yes No Yes 3.2 Mention Encoder Our model converts each mention m to a d dimensional vector. This vector is used to classify the type or entity of the mention. The basic model depicted in Figure 1 concatenates the averaged word embeddings of the mention string with the output of a convolutional neural network (CNN). The Table 3: Statistics from various type sets. TypeNet is the largest type hierarchy with a gold mapping to KB entities. *The entire WordNet could be added to TypeNet increasing the total size to 17k types. 99 time to g"
P18-1010,P15-1061,0,0.0187187,"cs while the CNN encodes a context dependent representation. 4 4.1 0≤i≤n−w+1 ci Training Mention-Level Typing Mention level entity typing is treated as multilabel prediction. Given the sentence vector mF , we compute a score for each type in typeset T as: Token Representation Each sentence is made up of s tokens which are mapped to dw dimensional word embeddings. Because sentences may contain mentions of more than one entity, we explicitly encode a distinguished mention in the text using position embeddings which have been shown to be useful in state of the art relation extraction models (dos Santos et al., 2015; Lin et al., 2016) and machine translation (Vaswani et al., 2017). Each word embedding is concatenated with a dp dimensional learned position embedding encoding the token’s relative distance to the target entity. Each token within the distinguished mention span has position 0, tokens to the left have a negative distance from [−s, 0), and tokens to the right of the mention span have a positive distance from (0, s]. We denote the final sequence of token representations as M . 3.2.2 max w W [j]M [i − b c + j]) 2 Each W [j] ∈ Rd×d is a CNN filter, the bias b ∈ Rd , M [i] ∈ Rd is a token represent"
P18-1010,P18-1025,1,0.816282,"tly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets. Additionally, we introduce two new humanannotated datasets: MedMentions, a corpus of 246k mentions from PubMed abstracts linked to the UMLS knowledge base, and TypeNet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets. While this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as Box embeddings (Vilnis et al., 2018) and Poincar´e embeddings (Nickel and Kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking. Most of all, we are excited to see new techniques from the NLP community using the resources we have presented. 9 Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. AcM. Razvan C Bunescu and"
P18-1010,E17-1119,0,0.236161,"et al., 2014). This has led to considerable research in automatically identifying entities in text, predicting their types, and linking them to existing structured knowledge sources. Current state-of-the-art models encode a textual mention with a neural network and classify the mention as being an instance of a fine grained type or entity in a knowledge base. Although in many cases the types and their entities are arranged in a hierarchical ontology, most approaches ignore this structure, and previous attempts to incorporate hierarchical information yielded little improvement in performance (Shimaoka et al., 2017). Additionally, existing benchmark entity typing datasets only consider small label sets arranged in very shallow hierarchies. For example, FIGER (Ling and Weld, 2012), the de facto standard fine grained entity type dataset, contains only 113 types in a hierarchy only two levels deep. In this paper we investigate models that explicitly integrate hierarchical information into the embedding space of entities and types, using a hierarchy-aware loss on top of a deep neural network classifier over textual mentions. By using this additional information, we learn a richer, more robust representation,"
P18-1010,spitkovsky-chang-2012-cross,0,0.0296019,"hich are two of the state-of-the-art knowledge graph embedding models. j 4.3 5.1 Entity Linking Hierarchical Structure Models Bilinear: Our standard bilinear model scores a hypernym link between (c1 , c2 ) as: Entity linking is similar to mention-level entity typing with a single correct class per mention. Because the set of possible entities is in the millions, linking models typically integrate an alias table mapping entity mentions to a set of possible candidate entities. Given a large corpus of entity linked data, one can compute conditional probabilities from mention strings to entities (Spitkovsky and Chang, 2012). In many scenarios this data is unavailable. However, knowledge bases such as UMLS contain a canonical string name for each of its curated entities. State-of-the-art biological entity linking systems tend to operate on various string edit metrics between the entity mention string and the set of canonical entity strings in the existing structured knowledge base (Leaman et al., 2013; Wei et al., 2015). For each mention in our dataset, we generate 100 candidate entities ec = (e1 , e2 , . . . , e100 ) each with an associated string similarity score csim. See Appendix A.5.1 for more details on can"
P18-1010,D12-1042,0,0.0646192,"f all types where th tm j = 1 if the j type is in the set of gold types for m and 0 otherwise. We optimize a multi-label binary cross entropy objective: X m Ltype (m) = − tm j log yj + (1 − tj ) log(1 − yj ) j 4.2 Entity-Level Typing In the absence of mention-level annotations, we instead must rely on distant supervision (Mintz et al., 2009) to noisily label all mentions of entity e with all types belonging to e. This procedure inevitably leads to noise as not all mentions of an entity express each of its known types. To alleviate this noise, we use multi-instance multi-label learning (MIML) (Surdeanu et al., 2012) which operates over bags rather than mentions. A bag of mentions Be = {m1 , m2 , . . . , mn } is the set of Sentence Representation The embedded sequence M is then fed into our context encoder. Our context encoder is a single layer CNN followed by a tanh non-linearity to produce C. The outputs are max pooled across 100 all mentions belonging to entity e. The bag is labeled with te , a binary vector of all types where tej = 1 if the jth type is in the set of gold types for e and 0 otherwise. For every entity, we subsample k mentions from its bag of mentions. Each mention is then encoded indepe"
P18-1010,E17-1111,0,0.206884,"Missing"
P18-1010,C12-2133,0,0.110217,"tions at varying levels of granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task. A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017). The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012). Del Corro et al. (2015) use manually crafted rules and patterns (Hearst patterns (Hearst, 1992), appositives, etc) to automatiTable 8 shows qualitative predictions for models with and without hierarchy information incorporated. Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity. In the first and second example, the baseline model becomes extremely dependent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences). This shows that modeling the structure of th"
P18-1025,E17-1068,0,0.331942,"ns is a manifold of probability distributions, the model is not truly probabilistic in that it does not model asymmetries and relations in terms of probEmbedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to"
P18-1025,P14-1098,0,0.0720103,"Missing"
P18-1025,W09-1109,0,0.645253,"Missing"
P19-1355,N18-1202,0,0.46486,"1,023 36,156 126,000 Training one model (GPU) NLP pipeline (parsing, SRL) w/ tuning & experiments Transformer (big) w/ neural arch. search 39 78,468 192 626,155 Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.1 Introduction Advances in techniques and hardware for training deep neural networks have recently enabled impressive accuracy improvements across many fundamental NLP tasks (Bahdanau et al., 2015; Luong et al., 2015; Dozat and Manning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or T"
P19-1355,D18-1548,1,0.905898,", we characterize the dollar cost and carbon emissions that result from training the neural networks at the core of many state-of-the-art NLP models. We do this by estimating the kilowatts of energy required to train a variety of popular off-the-shelf NLP models, which can be converted to approximate carbon emissions and electricity costs. To estimate the even greater resources required to transfer an existing model to a new task or develop new models, we perform a case study of the full computational resources required for the development and tuning of a recent state-of-the-art NLP pipeline (Strubell et al., 2018). We conclude with recommendations to the community based on our findings, namely: (1) Time to retrain and sensitivity to hyperparameters should be reported for NLP machine learning models; (2) academic researchers need equitable access to computational resources; and (3) researchers should prioritize developing efficient models and hardware. 2 Methods To quantify the computational and environmental cost of training deep neural network models for NLP, we perform an analysis of the energy required to train a variety of popular offthe-shelf NLP models, as well as a case study of the complete sum"
P19-1355,N19-1423,0,0.41468,"Training one model (GPU) NLP pipeline (parsing, SRL) w/ tuning & experiments Transformer (big) w/ neural arch. search 39 78,468 192 626,155 Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.1 Introduction Advances in techniques and hardware for training deep neural networks have recently enabled impressive accuracy improvements across many fundamental NLP tasks (Bahdanau et al., 2015; Luong et al., 2015; Dozat and Manning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiti"
P19-1355,D15-1166,0,0.0196644,"Consumption Air travel, 1 person, NY↔SF Human life, avg, 1 year American life, avg, 1 year Car, avg incl. fuel, 1 lifetime CO2 e (lbs) 1984 11,023 36,156 126,000 Training one model (GPU) NLP pipeline (parsing, SRL) w/ tuning & experiments Transformer (big) w/ neural arch. search 39 78,468 192 626,155 Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.1 Introduction Advances in techniques and hardware for training deep neural networks have recently enabled impressive accuracy improvements across many fundamental NLP tasks (Bahdanau et al., 2015; Luong et al., 2015; Dozat and Manning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most NLP model"
P19-1431,D15-1174,0,0.221567,"Missing"
P19-1431,D17-1060,0,0.0276202,"n Fig. 2, refer to the Appendix B for more qualitative analysis. 4 Related Work KG completion is an important research area, with several embedding-based models proposed, such as TransE which scores translations of entities in embedding space (Bordes et al., 2013), DistMult (Toutanova et al., 2015), ComplEx which is an extension to complex space (Trouillon et al., 2016), ConvE which uses 2D convolution layers (Dettmers et al., 2017) as well as recent tensor decomposition methods (Lacroix et al., 2018). Refer to Nickel et al. (2016) for a more comprehensive review. Recently, Das et al. (2017); Xiong et al. (2017) proposed reinforcement learning methods which find paths in KG. We compared with MINERVA (Das et al., 2017), a recent method, and found A2N to perform favorably. Graph Convolution Networks (Kipf and Welling, 2016; Schlichtkrull et al., 2017) and Graph attention networks (Veliˇckovi´c et al., 2017) also learn neighborhood based representations of nodes. However, they do not learn a query-dependent composition of the neighborhood which is sub-optimal as also seen our in experiments and noted previously (Dettmers et al., 2017). They are also computationally expensive. Nguyen et al. (2016) propos"
P19-1592,D18-1214,0,0.0259885,") gives a matrix with its input argument as the diagonal (Cuturi, 2013). We specifically use the regularized objective that has been shown to be effective for training (Cuturi, 2013; Genevay et al., 2018). Optimal transport has been effectively used in several natural language-based applications such as computing the similarity between two documents as the transport cost (Kusner et al., 2015; Huang et al., 2016), in measuring distances between point cloud-based representations of words (Frogner et al., 2019), and learning correspondences between word embedding spaces across domains/languages (Alvarez-Melis and Jaakkola, 2018; Alvarez-Melis et al., 2019). In our case, p1 represents the mention m and p2 represents m0 . The distribution p1 is defined as a point cloud consisting of the character embeddings computed by the LSTM applied to m, i.e., H (m) . Formally, it is a set of evenly weighted Dirac Delta functions in Rd where d is the embedding dimensionality of the character representations. The distribution p2 is defined similarly for m0 . The cost of transporting a character, ci of m to a character cj of m0 has cost, Ci,j = Smax − Si,j where Smax = maxi0 ,j 0 Si0 ,j 0 and Si,j is the inner product of hi and hj ."
P19-1592,D12-1032,0,0.0597741,"Missing"
P19-1592,D15-1075,0,0.0522975,"Missing"
P19-1592,W16-6204,0,0.120906,"classic models of string similarity in alias detection on 5 newly constructed datasets– which we make publicly available. Our results demonstrate that STANCE outperforms all other approaches on 4 out of 5 datasets in terms of Hits@1 and 3 out of 5 datasets in terms of mean average precision. Of the two cases in which STANCE is outperformed by other methods in terms of mean average precision, one is by a variant of STANCE in an ablation study. We also demonstrate STANCE’s capacity for supporting downstream tasks by using it in cross-document coreference for the Twitter at the Grammy’s dataset (Dredze et al., 2016). Using STANCE improves upon the state-of-the-art by 2.8 points of B 3 F1. Analyzing our trained model reveals STANCE effectively learns sequence-aware character similarities, filters noise with optimal transport, and uses the CNN scoring component to detect unconventional similarity-preserving edit patterns. 2 STANCE Our goal is to learn a model, f (·, ·), that measures the similarity between two strings–called mentions. The model should produce a high score when its inputs are aliases of the same entity, where a mention is an alias of an entity if it can be used to refer to that entity. For"
P19-1592,D08-1113,0,0.0727612,"Missing"
P19-1592,D16-1244,0,0.0224268,"plan, Pˆ ∈ RL×L describes how the + characters in m are softly aligned to the characters in m0 . We compute the element-wise product of the similarity matrix, S, and the transport plan: S 0 = S ◦ Pˆ . Cells containing high values in S 0 correspond to similar character pairs from m and m0 that are also well-aligned. Note the distinction between this alignment and the way in which the transport cost can be used as distance measure. The alignment is used as a reweighting of the similarity matrix. In this way, the transport plan is closely related to attention-based models (Bahdanau et al., 2015; Parikh et al., 2016; Vaswani et al., 2017; Kim et al., 2017). Finally, we employ a two dimensional convolutional neural network (CNN) to score S 0 (LeCun et al., 1998). With access to the full matrix S 0 , the CNN is able to detect multiple, aligned, character subsequences from m and m0 that are highly similar. By combining evidence from multiple–potentially non-continguous– aligned character subsequences, the CNN detects long-range similarity-preserving edit patterns. This is crucial, for example, in computing a high score for the pair Obama, Barack and Barack Obama. The architecture of the alignment-scoring CN"
P19-1592,R11-1015,0,0.0228665,"Missing"
S17-2091,P14-1119,0,0.153552,"uction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and informat"
S17-2091,P17-2054,1,0.878568,"pplication domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities. 1 Introduction Empirical"
S17-2091,W09-3611,0,0.0304788,"yphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym r"
S17-2091,D15-1086,1,0.830685,"ion would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 e"
S17-2091,W10-2924,0,0.0334212,"on extraction, and through hypernym prediction would also receive articles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task"
S17-2091,S17-2168,0,0.107723,"Missing"
S17-2091,S17-2167,0,0.0368744,"Missing"
S17-2091,S17-2173,0,0.0252701,"Missing"
S17-2091,C10-1065,0,0.403289,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,S10-1004,0,0.794457,"owledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base po"
S17-2091,D15-1235,0,0.0152438,"om/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task"
S17-2091,W16-5904,0,0.0307461,"ticles on named entity recognition or relation extraction. We expect the outcomes of the task to be relevant to the wider information extraction, knowledge base population and knowledge base construction communities, as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We"
S17-2091,S17-2166,0,0.0299819,"and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 noun phrases generated from the titles and abstracts derived from the Web Of Science corpora9 of the domains Physics, Computer Science, Chemistry and Computer Science. These corpora cannot be distributed publicly and were made available by the IARPA funding agency. Annotation was performed by 3 annotators using 14 fine-grained types, including PROCESS. We measured inter-annotator agreement among the three annotators for the 14 categories using Fleiss’ Kappa. The k value was found to be 0.28"
S17-2091,Q15-1010,0,0.0248553,"cscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publications, partly due to the task organisers’ collab"
S17-2091,D15-1057,0,0.0405374,"ion and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be"
S17-2091,I11-1001,0,0.680204,"led here is mention-level identification and classification of keyphrases, e.g. Keyphrase Extraction (TASK), as well as extracting semantic relations between keywords, e.g. Keyphrase Extraction HYPONYM-OF Information Extraction. These tasks are related to the tasks of named entity recognition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific"
S17-2091,S17-2169,0,0.0534923,"Missing"
S17-2091,P09-1113,0,0.0777004,"elines were refined.8 Annotation Process Mention-level annotation is very time-consuming, and only a handful of semantic relations such as hypernymy and synonymy can be found in each publication. We therefore only annotate paragraphs of publications likely to contain relations. We originally intended to identify suitable documents by automatically extracting a knowledge graph of relations from a large scientific dataset using Hearst-style patterns (Hearst, 1991; Snow et al., 2005), then using those to find potential relations in a distinct set of documents, similar to the distant supervision (Mintz et al., 2009; Snow et al., 2005) heuristic. Documents containing a high number of such potential relations would then be selected. However, this requires automatically learning to identify keyphrases between which those potential relations hold, and requires relations to appear several times in a dataset for such a knowledge graph to be useful. In the end, this strategy was not feasible due to the difficulty of learning to detect keyphrases automatically and only a small overlap between relations in different documents. Instead, keyphrasedense paragraphs were detected automatically using a coarse unsuperv"
S17-2091,C14-1002,0,0.0452535,"Missing"
S17-2091,S17-2172,0,0.092929,"Missing"
S17-2091,L16-1294,0,0.0874962,"In contrast to what we propose, the annotations are more fine-grained and annotations are only available for abstracts. Gupta and Manning (2011) studied keyphrase extraction from ACL Anthology articles, applying a pattern-based bootstrapping approach based on 15 016 documents and assigning the types FOCUS, TECHNIQUE and DOMAIN. Performance was evaluated on 30 manually annotated documents. Although the latter corpus is related to what we propose, manual annotation is only available for a small number of documents and only for the Natural Language Processing domain. The ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016) consists of 300 ACL Anthology abstracts annotated on mention-level with seven different types of keyphrases. Unlike our dataset, it does not contain relation annotations. Note that this corpus was created at the same time as the one SemEval 2017 Task 10 dataset and thus we did not have the chance to build on it. A more in-depth comparison between the two datasets as well as keyphrase identification and classification methods evaluated on them can be found in Augenstein and Søgaard (2017). Existing Resources As part of the FUSE project with IARPA, we created a small annotated corpus of 100 nou"
S17-2091,S17-2161,0,0.0445569,"Missing"
S17-2091,W12-3201,0,0.0249455,".co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impact industrial solutions to making sense of publication"
S17-2091,R09-1086,0,0.0762443,"Missing"
S17-2091,D15-1175,0,0.0291535,"nition, named entity 1 https://scholar.google.co.uk/ http://www.scopus.com/ 3 https://www.semanticscholar.org/ 2 546 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 546–555, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2014; Gupta and Manning, 2011; Marsi and ¨ urk, 2015), topic classification of scientific arOzt¨ ´ S´eaghdha and Teufel, 2014), citation conticles (O text extraction (Teufel, 2006; Kaplan et al., 2009), extracting author and citation graphs (Peng and McCallum, 2006; Chaimongkol et al., 2014; Sim et al., 2015) or a combination of those (Radev and Abu-Jbara, 2012; Gollapalli and Li, 2015; Guo et al., 2015). The expected impact of the task is an interest of the above mentioned research communities beyond the task due to the release of a new corpus, leading to novel research methods for information extraction from scientific documents. What will be particularly useful about the proposed corpus are annotations of hypernym and synonym relations on mention-level, as existing hypernym and synonym relation resources are on type-level, e.g. WordNet.4 Further, we expect that these methods will directly impac"
S17-2091,D16-1198,0,0.0680123,"as it offers a novel application domain for methods researched in that area, while still offering domain-related challenges. Since the dataset is annotated for three tasks dependent on one another, it could also be used as a testbed for joint learning or structured prediction approaches to information extraction (Kate and Mooney, 2010; Singh et al., 2013; Augenstein et al., 2015; Goyal and Dyer, 2016). Furthermore, we expect the task to be interesting for researchers studying tasks aiming at understanding scientific content, such as keyphrase extraction (Kim et al., 2010b; Hasan and Ng, 2014; Sterckx et al., 2016; Augenstein and Søgaard, 2017), semantic relation extraction (Tateisi et al., We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communi"
S17-2091,tateisi-etal-2014-annotation,0,\N,Missing
S17-2091,S17-2170,0,\N,Missing
W03-0430,W02-2018,0,0.0116257,"2 k , log P Λ (l(j) |o(j) ) − 2 2σ j=1 k where the second sum is a Gaussian prior over parameters (with variance σ) that provides smoothing to help cope with sparsity in the training data. When the training labels make the state sequence unambiguous (as they often do in practice), the likelihood function in exponential models such as CRFs is convex, so there are no local maxima, and thus finding the global optimum is guaranteed. It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003). This method approximates the second-derivative of the likelihood by keeping a running, finite-sized window of previous first-derivatives. L-BFGS can simply be treated as a black-box optimization procedure, requiring only that one provide the first-derivative of the function to be optimized. Assuming that the training labels on instance j make its state path unambiguous, let s(j) denote that path, and then the first-derivative of the log-likelihood is   N X δL =  Ck (s(j) , o(j) ) − δλk j=1   N X X λk  P Λ (s|o(j) )Ck (s, o(j) ) − 2 σ j=1 s where Ck (s, o) is t"
W03-0430,W96-0213,0,0.0743788,"Missing"
W03-0430,N03-1028,0,0.496798,"odels, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models’ great flexibility to include a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only"
W03-0430,W98-1118,0,\N,Missing
W03-0430,W99-0613,0,\N,Missing
W05-0636,J02-3001,0,0.135195,"Missing"
W05-0636,A00-2030,0,0.0526061,"ity estimates. 1 Introduction Although much effort has gone into developing statistical parsing models and they have improved steadily over the years, in many applications that use parse trees errors made by the parser are a major source of errors in the final output. A promising approach to this problem is to perform both parsing and the higher-level task in a single, joint probabilistic model. This not only allows uncertainty about the parser output to be carried upward, such as through an k-best list, but also allows information from higher-level processing to improve parsing. For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks. In particular, one suspects that attachment decisions, which are both notoriously hard and extremely important for semantic analysis, could benefit greatly from input from higher-level semantic analysis. The recent interest in semantic role labeling provides an opportunity to explore how higher-level semantic information can inform syntactic parsing. In previous work, it has been shown that SRL systems that use full parse information perform better than those that use shallow parse i"
W05-0636,P03-1002,0,0.0392416,"Missing"
W05-0636,P05-1073,0,0.0148808,"of the decrease in F1 is due to the drop in unlabeled recall. 4.2 Training a reranker using global features One potential solution to this problem is to add features of the entire frame, for example, to vote 227 against predicted frames that are missing key arguments. But such features depend globally on the entire frame, and cannot be represented by local classifiers. One way to train these global features is to learn a linear classifier that selects a parse / frame pair from the ranked list, in the manner of Collins (2000). Reranking has previously been applied to semantic role labeling by Toutanova et al. (2005), from which we use several features. The difference between this paper and Toutanova et al. is that instead of reranking k-best SRL frames of a single parse tree, we are reranking 1-best SRL frames from the k-best parse trees. Because of the the computational expense of training on k-best parse tree lists for each of 30,000 sentences, we train the reranker only on sections 15– 18 of the Treebank (the same subset used in previous CoNLL competitions). We train the reranker using LogLoss, rather than the boosting loss used by Collins. We also restrict the reranker to consider only the top 25 par"
W05-0636,W04-3212,0,0.0276962,"ason being that the ranking over parse trees induced by the semantic role labeling score is unreliable, because the model is trained locally. 2 Base SRL System Our approach to joint parsing and SRL begins with a base SRL system, which uses a standard architecture from the literature. Our base SRL system is a cascade of maximum-entropy classifiers which select the semantic argument label for each constituent of a full parse tree. As in other systems, we use three stages: pruning, identification, and classification. First, in pruning, we use a deterministic preprocessing procedure introduced by Xue and Palmer (2004) to prune many constituents which are almost certainly not arguments. Second, in identification, a binary MaxEnt classifier is used to prune remaining constituents which are predicted to be null with 225 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 225–228, Ann Arbor, June 2005. 2005 Association for Computational Linguistics Base features [GJ02] Path to predicate Constituent type Head word Position Predicate Head POS [SHWA03] All conjunctions of above Base features [GJ02] Head word Constituent type Position Predicate Voice Head POS [SHWA03] From"
W05-0636,J04-4004,0,\N,Missing
W06-1671,P05-1061,0,0.348238,"bility between pairs of fields, not sets of fields. To compute inter-cluster compatibility, the mean of the edges between the clusters is calculated. • McDonald: This method uses the pairwise compatibility function, but instead of calculating the mean of inter-cluster edges, it calculates the geometric mean of all pairs of edges in the potential new cluster. That is, to calculate the compatibility of records Ri and Rj , we construct a new record Rij that contains all fields of Ri and Rj , then calculate the geometric mean of all pairs of fields in Rij . This is analogous to the method used in McDonald et al. (2005) for relation extraction. • Cluster Compatibility (uniform): Intercluster compatibility is calculated directly by the cluster compatibility function. This is the method we advocate in Section 3. Training examples are sampled uniformly as described in Section 3.3. 4.4 Results For these experiments, we compare performance on the true record for each page. That is, we calculate how often each system returns a complete and accurate extraction of the contact record pertaining to the owner of the webpage. We refer to • Cluster Compatibility (iterative): Same as above, but training examples are sampl"
W06-1671,A00-2030,0,0.08909,"Missing"
W06-1671,W06-3606,1,0.675617,"Missing"
W06-1671,P04-1054,1,0.821046,"Missing"
W06-3606,J01-4004,0,0.0527176,"deduplication, object identification, and co-reference resolution) is the problem of determining whether a set of constants (mentions) refer to the same object (entity). Successful identity resolution enables vision systems to track objects, database systems to deduplicate redundant records, and text processing systems to resolve disparate mentions of people, organizations, and locations. Many probabilistic models of object identification have been proposed in the past 40 years in databases (Fellegi and Sunter, 1969; Winkler, 1993) and natural language processing (McCarthy and Lehnert, 1995; Soon et al., 2001). With the introduction of statistical relational learning, more sophisticated models of identity uncertainty have been developed that consider the dependencies between related consolidation decisions. Most relevant to this work are the recent relational 42 models of identity uncertainty (Milch et al., 2005; McCallum and Wellner, 2003; Parag and Domingos, 2004). McCallum and Wellner (2003) present experiments using a conditional random field that factorizes into a product of pairwise decisions about mention pairs (Model 3). These pairwise decisions are made collectively using relational infere"
W11-1807,W09-1402,0,0.705552,"ve results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where"
W11-1807,P05-1022,0,0.0128587,"g δt,Bind p,q µi,p,q ; for the case that j ∈ Prot (x) we P arg1 P arg2 def get cout i,j,r (λ, µ) = λi,j,r + p µi,j,p + q µi,q,j , def out otherwise ci,j,r (λ, µ) = λi,j,r . For bestBind (c) trig arg1 arg2 we set cbind i,p,q (µ) = −µi,p,q − µi,,p,q − µi,,p,q . 3.3 Preprocessing After basic tokenization and sentence segmentation, we generate a set of protein head tokens Prot (x) for each sentence x based on protein span definitions from the shared task. To ensure tokens contain not more than one protein we split them at protein boundaries. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing 1 We refer to Koo et al. (2010) for details on how to set αt . Task 1 Task 1 (abst.) Task 1 (full) Task 2 SVT 73.5 71.5 79.2 71.4 BIND 48.8 50.8 44.4 38.6 REG 43.8 45.5 40.1 39.1 TOT 55.2 56.1 53.1 51.0 Table 1: Results for the GE track, task 1 and 2; abst.=abstract; full=full text. model of McClosky and Charniak (2008). Finally, based on the set of trigger words in the training data, we generate a set of candidate triggers Trig (x). 4 Results We apply the same model to the GE, ID and EPI tracks, with minor modifications in order to deal with the diffe"
W11-1807,W11-1801,0,0.187225,"Missing"
W11-1807,W11-1802,0,0.415252,"rs and outgoing arguments, (b) event triggers and incoming arguments and (c) protein-protein bindings. For efficient decoding we employ dual decomposition. Our results are very competitive: With minimal adaptation of our model we come in second for two of the tasks—right behind a version of the system presented here that includes predictions of the Stanford event extractor as features. We also show that for the Infectious Diseases task using data from the Genia track is a very effective way to improve accuracy. 1 Introduction This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al., 2011a). We introduce a simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should s"
W11-1807,D10-1125,0,0.0124758,"Missing"
W11-1807,P08-2026,0,0.0395691,"d tokens Prot (x) for each sentence x based on protein span definitions from the shared task. To ensure tokens contain not more than one protein we split them at protein boundaries. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing 1 We refer to Koo et al. (2010) for details on how to set αt . Task 1 Task 1 (abst.) Task 1 (full) Task 2 SVT 73.5 71.5 79.2 71.4 BIND 48.8 50.8 44.4 38.6 REG 43.8 45.5 40.1 39.1 TOT 55.2 56.1 53.1 51.0 Table 1: Results for the GE track, task 1 and 2; abst.=abstract; full=full text. model of McClosky and Charniak (2008). Finally, based on the set of trigger words in the training data, we generate a set of candidate triggers Trig (x). 4 Results We apply the same model to the GE, ID and EPI tracks, with minor modifications in order to deal with the different event type sets T and role sets R of each track. Training and testing together took between 30 (EPI) to 120 (GE) minutes using a singlecore implementation. DEV DEV DEV DEV TEST I/G 1/0 0/1 1/1 2/1 2/1 BIND 18.6 18.2 20.0 20.0 34.6 REG 27.1 26.8 33.1 34.5 46.4 PRO 34.3 0.00 49.3 52.0 62.3 TOT 41.5 35.5 47.2 48.5 53.4 Table 2: ID results for different amount"
W11-1807,P11-1163,0,0.283669,"pproach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system. For the Epigenetics and Posttranslational Modifications (EPI) track (Ohta et al., 2011) we achieve the 4th rank, partly because we did not aim to extract speculations, negations or cellular locations. Finally, for Genia Task 2 we rank 3rd— with the 1st rank achieved by the FAUST system. In the following we will briefly describe our model and inference algorithm, as far as this is possible in lim"
W11-1807,W11-1806,0,0.625522,"Missing"
W11-1807,W11-1803,0,0.103825,"Missing"
W11-1807,N10-1123,0,0.338117,"ion. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system"
W11-1807,W11-1804,0,0.10001,"Missing"
W11-1807,W09-1406,1,0.740819,"racks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second"
W11-1807,W11-1808,1,0.647704,"ods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact that it uses predictions of the Stanford system (McClosky et al., 2011a; McClosky et al., 2011b), and hence performs model combination. The same holds for the Infectious Diseases (ID) track (Pyysalo et al., 2011), where we come in as second right behind the FAUST system. For the Epigenetics and Posttranslational Modifications (EPI) track (Ohta et al., 2011) we achieve the 4th rank, partly because we did not aim to extract speculations, negations or cellular locations. Finally, for Genia Task 2 we rank 3rd— with the 1s"
W11-1807,D10-1001,0,0.0229349,"ay to improve accuracy. 1 Introduction This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al., 2011a). We introduce a simple joint model for the extraction of biomedical events, and show competitive results for four tracks of the competition. Our model subsumes three tractable sub-models, one for extracting event triggers and outgoing edges, one for event triggers and incoming edges and one for protein-protein bindings. Fast and accurate joint inference is provided by combining optimizing methods for these three submodels via dual decomposition (Komodakis et al., 2007; Rush et al., 2010). Notably, our model constitutes the first joint approach that explicitly predicts which protein should share the same binding event. So far this has either been done through postprocessing heuristics (Björne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010), or through a local classifier at the end of a pipeline (Miwa et al., 2010). Our model is very competitive. For Genia (GE) Task 1 (Kim et al., 2011b) we achieve the secondbest results. In addition, the best-performing FAUST system (Riedel et al., 2011) is a variant of the model presented here. Its advantage stems from the fact"
W11-1808,W09-1401,0,0.534253,"Missing"
W11-1808,W11-1801,0,0.421255,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1802,0,0.36214,"ctions on test and development sets we used models learned from the the complete training set. Predictions over training data were produced using crossvalidation. This helps to avoid a scenario where the stacking model learns to rely on high accuracy at training time that cannot be matched at test time. Note that, unlike Stanford’s individual submission in this shared task, the stacked models in this paper do not include the Stanford reranker. This is because it would have required making a reranker model for each crossvalidation fold. We made 19 crossvalidation training folds for Genia (GE) (Kim et al., 2011b), 12 for Epigenetics (EPI), and 17 for Infectious Diseases (ID) (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011, respectively). Note that while ID is the smallest and would seem like it would have the fewest folds, we combined the training data of ID with the training and development data from GE. To produce predictions over the test data, we combined the training folds with 6 development folds for GE, 4 for EPI, and 1 for ID. 3 Experiments Table 1 gives an overview of our results on the test sets for all four tasks we submitted to. Note that for the EPI and ID tasks we show the"
W11-1808,P11-1163,1,0.535908,"b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different"
W11-1808,W11-1806,1,0.588093,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,H05-1066,0,0.0337346,") finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different (and thus more useful for stacking) results. Of particular interest to this paper are the four possible decoders in MSTParser. These four decoders come from combinat"
W11-1808,P08-1108,0,0.0519048,"Missing"
W11-1808,W11-1803,0,0.278435,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1804,0,0.36653,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1807,1,0.818921,"edictions from the Stanford system into the UMass system (e.g., as in Nivre and McDonald (2008)). This has the advantage that one model (Umass) determines how to integrate the outputs of the other model (Stanford) into its own structure, whereas in reranking, for example, the combined model is required to output a complete structure produced by only one of the input models. 2 Approach In the following we briefly present both the stacking and the stacked model and some possible ways of integrating the stacked information. 2.1 Stacking Model As our stacking model, we employ the UMass extractor (Riedel and McCallum, 2011). It is based on a discriminatively trained model that jointly predicts trigger labels, event arguments and protein pairs in 51 Proceedings of BioNLP Shared Task 2011 Workshop, pages 51–55, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics binding. We will briefly describe this model but first introduce three types of binary variables that will represent events in a given sentence. Variables ei,t are active if and only if the token at position i has the label t. Variables ai,j,r are active if and only if there is an event with trigger i that has an argument"
W11-1808,D10-1001,0,0.0101169,"(p, q) to include predictions from the systems to be stacked. For example, for every system S to be stacked and every pair of event types (t0 , tS ) we add the features ( 1 hS (i) = tS ∧ t0 = t fS,t0 ,tS (i, t) = 0 otherwise 52 to fT (i, t). Here hS (i) is the event label given to token i according to S. These features allow different weights to be given to each possible combination of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to d"
W11-1808,N10-1091,1,0.0608368,"Missing"
W12-3017,P11-1080,1,0.777878,"Missing"
W12-3017,J01-4004,0,0.040315,"aphic data. Currently, we have supplemented DBLP1 with extra mentions from BibTeX files to create a database with over ten million mentions (6 million authors, 2.3 million papers, 2.2 million venues, and 500k institutions). We perform joint coreference between authors, venues, papers, and institutions at this scale. We describe our coreference model next. 2.1 Hierarchical Coreference inside the DB Entity resolution is difficult at any scale, but is particularly challenging on large bibliographic data sets or other domains where there are large numbers of mentions. Traditional pairwise models (Soon et al., 2001; McCallum and Wellner, 2003) of coreference—that measure compatibility between pairs of mentions—lack both scalability and modeling power to process these datasets. Instead, inspired by a recently proposed three-tiered hierarchical coreference model (Singh et al., 2011), we employ an alternative model that recursively structures entities into trees. Rather than measuring compatibilities between all mention pairs, instead, internal tree nodes might summarize thousands of leaf-level mentions, and compatibilities are instead measured between child and parent nodes. For example, a single intermed"
W12-3017,P12-1040,1,0.817658,"at random. If the nodes happen to be in the same entity tree, then one of the nodes is made the root of a new entity. Otherwise, the two nodes are in different entity trees, then we propose to merge the two sub-tree’s together by either merging the second subtree into the first subtree, or merging the second subtree into the root of the first subtree. If two leaflevel nodes (mentions) are chosen, then a new entity is created and the two mentions are merged into this newly created entity. We describe these proposals and the hierarchical coreference model in more detail in a forthcoming paper (Wick et al., 2012). 2.2 Human edits for entity resolution Broadly speaking, there are two common types of errors for entity coreference resolution: recall errors, and precision errors. A recall error occurs when the coreference system predicts that two mentions do not refer to the same entity when they actually do. Conversely, a precision error occurs when the coreference error incorrectly predicts that two mentions refer to the same entity when in fact they do not. In order to correct these two common error types, we introduce two class of user edits: shouldlink and should-not-link. These edits are analogous t"
W12-3021,D07-1101,0,0.0688272,"Missing"
W12-3021,N07-1011,1,0.875559,"Missing"
W12-3021,P11-1055,0,0.0562764,"Missing"
W12-3021,P11-1080,1,0.861392,"Missing"
W12-3021,D10-1099,1,0.883234,"Missing"
W12-3022,P09-1113,0,0.130763,"icized-by can help predict the profession information in Freebase. Moreover, often users of the database will not need to study a particular schema—they can use their own expressions (say, works-at instead of profession) and still find the right answers. In the previous scenario we could answer more questions than our structured sources alone, because we learn how to predict new Freebase rows. We could answer more questions than the text corpus and OpenIE alone, because we learn how to predict new rows in surface pattern tables. We could also answer more questions than in Distant Supervision (Mintz et al., 2009), because our schema is not limited to the relations in the structured source. We could even go further and import additional structured sources, such as Yago (Hoffart et al., 2012). In this case the probabilistic database would have integrated, and implicitly aligned, several different data sources, in the sense that each helps predict the rows of the other. In this paper we present results of our first technical approach to probabilistic databases with universal schema: collaborative filtering, which has been successful in modeling movie recommendations. Here each entity tuple explicitly “ra"
W12-3022,N07-1071,0,0.054552,"o note that it is easy to incorporate entity representations into the approach, and model selectional preferences. Likewise, we can easily add posterior constraints we know to hold across relations, and learn from unlabeled data. 3 Related Work We briefly review related work in this section. Open IE (Etzioni et al., 2008) extracts how entities and their relations are actually mentioned in text, but does not predict how entities could be mentioned otherwise and hence suffer from reduced recall. There are approaches that learn synonym relations between surface patterns (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001; Yao et 118 al., 2011) to overcome this problem. Fundamentally, these methods rely on a symmetric notion of synonymy in which certain patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed gPCA algorithm (Schoenmackers et al., 2008; Schoenmackers et al., 2010). Such methods learn the structure of a Markov"
W12-3022,D08-1009,0,0.165722,"Missing"
W12-3022,D10-1106,0,0.208632,"ns between surface patterns (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001; Yao et 118 al., 2011) to overcome this problem. Fundamentally, these methods rely on a symmetric notion of synonymy in which certain patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed gPCA algorithm (Schoenmackers et al., 2008; Schoenmackers et al., 2010). Such methods learn the structure of a Markov Network, and are ultimately bounded by limits on tree-width and density. In contrast, the gPCA learns a latent, although not necessarily interpretable, structure. This latent structure can express models of very high treewidth, and hence very complex rules, without loss in efficiency. Moreover, most rule learners work in batch mode while our method continues to learn new associations with the arrival of new data. 4 Experiments Our work aims to predict new rows of source tables, where tables correspond to either surface patterns in natural language"
W12-3022,D11-1135,1,0.905496,"Missing"
W13-3517,N10-1061,0,0.151151,"es iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in"
W13-3517,P98-1013,0,0.00899288,"irs. Coreference resolution forms an important component for natural language processing and information extraction pipelines due to its utility in relation extraction, cross-document coreference, text summarization, and question answering. The task of coreference is challenging for automated systems as the local information contained in the document is often not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of men"
W13-3517,D08-1031,0,0.287666,"stem (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pai"
W13-3517,D07-1074,0,0.0791348,"rs represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links ins"
W13-3517,N07-1011,1,0.351235,"nts in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the development data when trained on the train set. The models we use to evaluate on the test data set are trained on the training and development sets, following the standard evaluation for coreference first used by Culotta et al. (2007). To provide the initial ranked list of entity candidates from Wikipedia, we query the KB Bridge system (Dalton and Dietz, 2013) with the proper name mentions. KB Bridge is an information-retrievalbased entity linking system that connects the query mentions to Wikipedia entities using a sequential dependence model. This system has been shown to match or outperform the top performing systems in the 2012 TAC KBP entity linking task. 4.2 Methods Our experiments investigate a number of baselines that are similar or identical to existing approaches. Wikipedia Linking: As a simple baseline, we direc"
W13-3517,doddington-etal-2004-automatic,0,0.0111976,"ashington State does appear in the candidate entities of the first two mentions, albeit with a lower rank. In our approach, clustering the first two mentions causes the shared candidate Washington State to move to the top of the list. The coreference system is now able to easily identify that the “Washington State” mention is compatible with the Washington State entity formed by the previous two mentions, providing evidence that the final mention should be clustered with either of them in subsequent comparisons. 4 Experiments 4.1 Setup We evaluate our system on the ACE 2004 annotated dataset (Doddington et al., 2004). Following the setup in Bengston and Roth (2008), we split the corpus into training, development, and test sets, resulting in 268 documents in the train set, 107 documents in the test set, and 68 documents in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the developmen"
W13-3517,D09-1120,0,0.222949,"phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on this metric, suggesting that dynamic alignment is beneficial even when the features have not been engineered for events or for different genres. 6 Related Work Within-document coreference has been wellstudied for a number of years. A variety of approaches incorporate linguistic knowledge as rules iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improve"
W13-3517,P11-1055,0,0.051117,"and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO and FrameNet) into a cl"
W13-3517,W11-1902,0,0.0754511,"Missing"
W13-3517,P02-1014,0,0.297829,"outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, he"
W13-3517,P10-1142,0,0.041068,"Missing"
W13-3517,N06-1025,0,0.429605,"ften not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multipl"
W13-3517,W11-1901,0,0.0115112,"ur baseline is shown in Figure 3. Our static linking matches the performance of Ratinov and Roth (2012) on the non-transcripts. Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment. Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than OntoNotes We also run our systems on the OntoNotes dataset, which was used for evaluation in CoNLL 2011 Shared Task (Pradhan et al., 2011). The dataset consists of 2083 documents from a much larger variety of genres, such as conversations, magazines, web text, etc. Further, the dataset also consists of mentions that refer to events, most of which do not appear as Wikipedia pages. Since only the nonsingleton mentions are annotated in the training set, we also include additional noun phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on t"
W13-3517,D10-1048,0,0.807132,"approach stays fairly even as X is varied. Even though the experiments suggest that the larger documents are tougher to coreference,3 dynamic linking provides higher improvements when the documents contain a larger number of mentions. 5.2 Performance on Transcripts The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document. In particular, 158 3 i.e., the absolute values are lower for these splits. The baseline system obtains 83.08, 79.29, 79.64, and 79.77 respectively for X = 10, 33, 40, 50. Method Culotta et al. (2007) Raghunathan et al. (2010) Stoyanov and Eisner (2012) Wiki-linking Bengston and Roth (2008) Baseline Static Linking Dynamic Linking Pairwise P/R F1 71.6 46.2 56.1 64.15 14.99 66.56 47.07 82.53 40.80 72.20 47.40 24.30 55.14 54.61 57.23 MUC P/R 80.4 71.8 74.41 82.7 82.84 88.39 85.07 28.39 69.9 72.02 66.93 72.02 F1 75.8 80.1 41.10 75.8 77.05 76.18 78.01 CEAF P/R 58.54 58.4 75.58 75.40 75.33 75.35 76.55 76.37 F1 58.47 75.49 75.44 76.46 B3 P/R 86.7 73.2 86.3 75.4 92.89 88.3 87.02 93.10 89.37 57.21 74.5 75.97 72.72 76.12 F1 79.3 80.4 81.8 70.81 80.8 81.12 81.66 82.21 Table 2: Evaluation on the ACE test data, with the system"
W13-3517,P11-1082,0,0.568436,"ctive recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features. Although this approach is more robust to noise in the documents, the features of a mention merge the different aspects of the entities, for example a “Michael Jordan” mention will contain features for both the scientist and basketball personas. Instead of fixing the alignment of the mentions to the knowledge base, our proposed approach maintains a ranked list of candidate entities for each mention. To expand the se"
W13-3517,D12-1113,0,0.675058,"isambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge"
W13-3517,P11-1138,0,0.0541057,"a. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in Ratinov and Roth (2012). Dalton and Dietz (2013) introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012). Alignment to an external Conclusions A number of possible avenues for future study are apparent. First, our alignment to a knowledgebase can benefit from more document-aware"
W13-3517,D10-1099,1,0.101154,"Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO"
W13-3517,J01-4004,0,0.231353,"ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features men"
W13-3517,spitkovsky-chang-2012-cross,0,0.0297459,"he large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions. Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system"
W13-3517,C12-1154,0,0.168314,"Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyano"
W13-3517,P10-2029,0,0.230993,", 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pair, mention substring mat"
W13-3517,C98-1013,0,\N,Missing
W14-1609,P05-1001,0,0.0151248,"Missing"
W14-1609,P09-1116,0,0.565252,"use the baseline system as described in Section 2.3.1. NER systems marked as “Skip-gram” consider phrase embeddings; “LexEmb” consider lexicon-infused embeddings; “Brown” use Brown clusters, and “Gaz” use our lexicons as features. 4.1 Dev 92.22 93.39 93.68 93.81 93.69 93.88 94.23 94.46 93.15 94.48 93.50 - Test 87.93 90.05 89.68 89.56 89.27 90.67 90.33 90.90 89.31 89.92 90.57 90.90 Table 2: Final NER F1 scores for the CoNLL 2003 shared task. On the top are the systems presented in this paper, and on the bottom we have baseline systems. The best results within each area are highlighted in bold. Lin and Wu 2009 use massive private industrial query-log data in training. the best, and we use this model for further NER experiments. There was no perceptible difference in computation cost from learning lexicon-infused embeddings versus learning standard Skip-gram embeddings. 4.2 CoNLL 2003 NER We applied our models on CoNLL 2003 NER data set. All hyperparameters were tuned by training on training set, and evaluating on the development set. Then the best hyperparameter values were trained on the combination of training and development data and applied on the test set, to obtain the final results. Table 2"
W14-1609,W03-0434,0,0.0196589,"ssifier,label) pairs. The and share a token. skip-gram model then computes a probability of a context word given a target word as the product of 2.3 Named Entity Recognition the probabilities, given the target word’s embeddings, of all decisions on a path from the root to Named Entity Recognition (NER) is the task of the leaf corresponding to the context word. Figure finding all instances of explicitly named entities 1 shows such a tree structured model. and their types in a given document. While 80 on the predictions made by the first CRF as well as features of the data. Both CRFs, following Zhang and Johnson (2003), have roughly similar features. While local features capture a lot of the clues used in text to highlight named entities, they cannot necessarily disambiguate entity types or detect named entities in special positions, such as the first tokens in a sentence. To solve these problems most NER systems incorporate some form of external knowledge. In our baseline system we use lexicons of months, days, person names, companies, job titles, places, events, organizations, books, films, and some minor others. These lexicons were gathered from US Census data, Wikipedia category pages, and Wikipedia red"
W14-1609,N04-1043,0,0.0133897,"us. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications (Ratinov and Roth, 2009; Koo et al., 2008; Miller et al., 2004). There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993). Background and Related Work Language models and word embeddings One limitation of Brown clusters is their computational complexity, as training takes O(kV 2 + N )x time to train, where k is the number of base clusters, V size of vocabulary, and N number of tokens. This is infeasible for large corpora with millions of word types. A statistical language model is a way to assign probabilities to all possible documents in a given language. Most such models can be"
W14-1609,P93-1024,0,0.168644,"unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications (Ratinov and Roth, 2009; Koo et al., 2008; Miller et al., 2004). There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993). Background and Related Work Language models and word embeddings One limitation of Brown clusters is their computational complexity, as training takes O(kV 2 + N )x time to train, where k is the number of base clusters, V size of vocabulary, and N number of tokens. This is infeasible for large corpora with millions of word types. A statistical language model is a way to assign probabilities to all possible documents in a given language. Most such models can be classified in one of two categories: they can directly assign probabilities to sequences of word types, such as is done in n-gram mode"
W14-1609,W09-1119,0,0.915577,"es of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications (Ratinov and Roth, 2009; Koo et al., 2008; Miller et al., 2004). There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993). Background and Related Work Language models and word embeddings One limitation of Brown clusters is their computational complexity, as training takes O(kV 2 + N )x time to train, where k is the number of base clusters, V size of vocabulary, and N number of tokens. This is infeasible for large corpora with millions of word types. A statistical language model is a way to assign probabilities to all possible documents in a"
W14-1609,P08-1076,0,0.0177072,"Missing"
W14-1609,W03-0419,0,0.438971,"Missing"
W14-1609,P10-1040,0,0.562529,"ivate data (Lin and Wu, 2009). While very attractive due to their simplicity, generality, and hierarchical structure, Brown clusters are limited because the computational complexity of fitting a model scales quadratically with the number of words in the corpus, or the number of “base clusters” in some efficient implementations, making it infeasible to train it on large corpora or with millions of word types. Although some attempts have been made to train named-entity recognition systems with other forms of word representations, most notably those obtained from training neural language models (Turian et al., 2010; Collobert and Weston, 2008), these systems have historically underperformed simple applications of Brown clusters. A disadvantage of neural language models is that, while they are inherently more scalable than Brown clusters, training large neural networks is still often expensive; for example, Turian et al (2010) report that some models took multiple days or weeks to produce acceptable representations. Moreover, language embeddings learned from neural networks tend to behave in a “nonlinear” fashion, as they are trained to encourage a many-layered neural network to assign high probability t"
W14-1609,J92-4003,0,\N,Missing
W14-1609,P08-1068,0,\N,Missing
W16-1304,P11-1062,0,0.0678416,"Missing"
W16-1304,D13-1160,0,0.0929256,"Missing"
W16-1304,D14-1067,0,0.0266584,"Missing"
W16-1304,D14-1165,0,0.147098,"Missing"
W16-1304,D13-1080,0,0.274468,"Missing"
W16-1304,D14-1044,0,0.173769,"Missing"
W16-1304,D15-1038,0,0.13939,"Missing"
W16-1304,D13-1161,0,0.0764652,"Missing"
W16-1304,D11-1049,0,0.29366,"Missing"
W16-1304,D12-1093,0,0.313201,"Missing"
W16-1304,N13-1095,0,0.0159204,"sPlace without considering the fact that Yankee Stadium is not an airport. Introduction Knowledge Bases (KB’s) are structured knowledge sources widely used in applications like question answering (Kwiatkowski et al., 2013; Berant et al., 2013; Bordes et al., 2014) and search engines like Google Search and Microsoft Bing. This has led to the creation of large KB’s like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and NELL (Carlson et al., 2010). KB’s contains millions of facts usually in the form of triples (entity1, relation, entity2). However, KB’s are woefully incomplete (Min et al., 2013), missing important facts, and hence limiting their usefulness in downstream tasks. To overcome this difficulty, Knowledge Base Completion (KBC) methods aim to complete the KB using existing facts. For example, we can infer nationality of a person from their place of birth. A common approach in many KBC methods for relation extraction is reasoning on individual relations (single-hop reasoning) to predict new relations (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013). For example, predicting Nationality(X, Y) from BornIn(X, Y). The performance of relation extr"
W16-1304,P09-1113,0,0.2523,"ELL (Carlson et al., 2010). KB’s contains millions of facts usually in the form of triples (entity1, relation, entity2). However, KB’s are woefully incomplete (Min et al., 2013), missing important facts, and hence limiting their usefulness in downstream tasks. To overcome this difficulty, Knowledge Base Completion (KBC) methods aim to complete the KB using existing facts. For example, we can infer nationality of a person from their place of birth. A common approach in many KBC methods for relation extraction is reasoning on individual relations (single-hop reasoning) to predict new relations (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013). For example, predicting Nationality(X, Y) from BornIn(X, Y). The performance of relation extraction methods have been greatly improved by incorporating selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, both in sentence level (Roth and Yih, 2007; Singh et 18 Proceedings of AKBC 2016, pages 18–23, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics al., 2013) and KB relation extraction (Chang et al., 2014), and in learning enta"
W16-1304,D14-1113,1,0.696384,"e new composition matrices due to the entity and its types respectively. In all of our experiments f is the sigmoid activation function. 20 2.2 Model Training We train a separate RNN model for each target relation1 . The parameters for each model are the embedding of the relations, entities and types, and the various composition matrices (as applicable) . They are trained to maximize the likelihood of the training data.The score of a path π w.r.t to the target relation δ is score(π, δ) = σ (v (π) · v (δ)) (5) We then choose the path which has the highest score similar to (Weston et al., 2013; Neelakantan et al., 2014). Selecting just one path (out of typically hundreds to thousands of paths) between entity pairs might lead to our model ignoring informative paths, especially during the initial stages of training. To alleviate this issue we also experiment by selecting the top k paths that have the highest score for a given entity pair and relation with the resultant score being the average of the top k scores. 3 Experiments & Results In all of our experiments, we set the dimension of the relations, entity and their type embeddings to be 50. For a fair comparison with our model, which has more number of para"
W16-1304,P15-1016,1,0.818468,"Missing"
W16-1304,N13-1008,1,0.880994,"millions of facts usually in the form of triples (entity1, relation, entity2). However, KB’s are woefully incomplete (Min et al., 2013), missing important facts, and hence limiting their usefulness in downstream tasks. To overcome this difficulty, Knowledge Base Completion (KBC) methods aim to complete the KB using existing facts. For example, we can infer nationality of a person from their place of birth. A common approach in many KBC methods for relation extraction is reasoning on individual relations (single-hop reasoning) to predict new relations (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013). For example, predicting Nationality(X, Y) from BornIn(X, Y). The performance of relation extraction methods have been greatly improved by incorporating selectional preferences, i.e., relations enforce constraints on the allowed entity types for the candidate entities, both in sentence level (Roth and Yih, 2007; Singh et 18 Proceedings of AKBC 2016, pages 18–23, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics al., 2013) and KB relation extraction (Chang et al., 2014), and in learning entailment rules (Berant et al., 2011). Anothe"
W16-1304,D10-1106,0,0.0485491,"Missing"
W16-1304,D15-1083,0,0.232657,"f work in relation extraction performs reasoning on the paths (multi-hop reasoning on paths of length ≥ 1) connecting an entity pair (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015). For example, these models can infer the relation PlaysInLeague(Tom Brady, NFL) from the facts PlaysForTeam(Tom Brady, New England Patriots) and PartOf(New England Patriots, NFL). All these methods utilize only the relations in the path and do not include any information about the entities. In this work, we extend the method of Neelakantan (2015) by incorporating entity type information. Their method can generalize to paths unseen in training by composing embeddings of relations in the path non-linearly using a Recurrent Neural Network (RNN) (Werbos, 1990). While entity type information has been successfully incorporated into relation extraction methods that perform single hop reasoning, here, we include them for multi-hop relation extraction. For example, Figure 1 illustrates an example where reasoning without type information would score both the paths equally although the latter path should receive a lesser score since there is an"
W16-1312,D14-1044,0,0.0461826,"Missing"
W16-1312,P15-1016,1,0.896314,"Missing"
W16-1312,N13-1008,1,0.847823,"Missing"
W16-1312,N15-1118,0,0.0698742,"Missing"
W16-1312,D15-1174,0,0.1588,"Missing"
W16-1312,N16-1103,1,0.849887,"Missing"
W16-1317,D14-1082,0,0.0148931,"elation in IE. As a solution to this, patterns which do not make sense can be excluded by a fixed set of rules, e.g. ‘must contain a verb’, ‘must not contain personal pronouns’, ‘must contain at least n words’ etc. A commonly used approach related to fixed rules is to detect appositives as in (Yao et al., 2013). Coming up with a fixed rule set is problematic though as there is always the risk of excluding patterns that are actually usable. As an alternative to fixed rule sets one can come up with learned models to select usable relations. Another approach is to use dependency parsers such as (Chen and Manning, 2014) or openIE style RE (Verga et al., 2015). However, dependency parsers often focus on the syntax of text which is not ideal given the need of propositional information. Hence, important information might get lost. Others use dependency trees (Stanovsky et al., 2016) in order to explore propositional structure of text. This in return might help to decide whether a certain fraction 94 of the text is actually a usable relation. Data from web crawlers is not only noisy, but also there are a lot of relation candidates which are just too seldom to be learned. It would be an option to follow (Riedel e"
W16-1317,N13-1095,0,0.0847197,"Missing"
W16-1317,N13-1008,1,0.916203,"erg’ are connected via the relation rel = ‘sub’. UMass Amherst has several other subsidiary colleges which do exist in FB e.g., ent2 = ‘Engineering’ case of a relation. If this case is in the test set it cannot be learned at all. Thus, evaluation is restricted to relation types within the FB schema. To extend and broaden the coverage of FB many approaches include unlabelled data from text corpora. However the choice of the unlabelled text varies between different contributions, see table 2 for a comparison of dataset statistics between two commonly used datasets. For RE and Universal Schemas (Riedel et al., 2013), data is used from the NYTimes corpus (Sandhaus, 2008) and FB. In comparison to other unlabelled corpora, the NYTimes corpus is quite small. To represent text, data could be used from FB and a (subset of) already existing datasets. For example, FB15k-237 (Toutanova et al., 2015) is a combination of Clueweb(CW) and FB data. CW is a webcrawl that comes annotated with FB entities (Gabrilovich et al., 2013). In the work with FB15k237, relations are expressed as parse paths but not as entire text and in addition the majority of test entity pairs have no textual or other KB evidence. In terms of in"
W16-1317,N15-1118,0,0.0606162,"Missing"
W16-1317,D15-1174,0,0.0738468,"s within the FB schema. To extend and broaden the coverage of FB many approaches include unlabelled data from text corpora. However the choice of the unlabelled text varies between different contributions, see table 2 for a comparison of dataset statistics between two commonly used datasets. For RE and Universal Schemas (Riedel et al., 2013), data is used from the NYTimes corpus (Sandhaus, 2008) and FB. In comparison to other unlabelled corpora, the NYTimes corpus is quite small. To represent text, data could be used from FB and a (subset of) already existing datasets. For example, FB15k-237 (Toutanova et al., 2015) is a combination of Clueweb(CW) and FB data. CW is a webcrawl that comes annotated with FB entities (Gabrilovich et al., 2013). In the work with FB15k237, relations are expressed as parse paths but not as entire text and in addition the majority of test entity pairs have no textual or other KB evidence. In terms of increasing quality of FB, we suggest to increase the number of relations by adding data from other datasets. As mentioned above, this could be CW or other already available datasets. By that, more relations should be covered as well as evaluating against entities which are not incl"
W16-1317,N16-1103,1,\N,Missing
W17-4301,P16-1231,0,0.119591,"rallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing. The best neural dependency parsers generally consist of two stages: First, they employ a recurrent neural network such as a bidirectional LSTM to encode each token in context; next, they compose these token representations into a parse tree. Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (McBecause of their dependency on sequential processing of the sentence, none of these architectures fully exploit the massive parallel processing capability that GPUs possess. If we wish to maximize GPU resources, graph-based dependency parsers are more desirable than their transitionbased counterparts since attention over the edgefactored graph can be parallelized across the entire sentence, unlike the transition-based parser which must sequentially predict and perform each tr"
W17-4301,D14-1082,0,0.341179,"stly accelerating and parallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing. The best neural dependency parsers generally consist of two stages: First, they employ a recurrent neural network such as a bidirectional LSTM to encode each token in context; next, they compose these token representations into a parse tree. Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (McBecause of their dependency on sequential processing of the sentence, none of these architectures fully exploit the massive parallel processing capability that GPUs possess. If we wish to maximize GPU resources, graph-based dependency parsers are more desirable than their transitionbased counterparts since attention over the edgefactored graph can be parallelized across the entire sentence, unlike the transition-based parser which must sequentially predic"
W17-4301,D16-1238,0,0.0169523,"model, and the highest scoring head for each dependent is selected. Previously, (Chen and Manning, 2014) pioneered neural network paring with a transitionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings. Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and ent"
W17-4301,W08-1301,0,0.0278229,"Missing"
W17-4301,P14-1062,0,0.053717,"dd a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks. Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly 5 5.1 Experimental Results Data and Evaluation We train our parser on the English Penn TreeBank on the typical data split: training on sections 2–21, testing on section 23 and using section 22 for development. We convert constituency trees to dependencies using the Stanford dependency framework v3.5 (de Marneffe and Manning, 2008), and use part-of-speech tags from the Stanford left3words part-of-speech tagger. As is th"
W17-4301,P15-1032,0,0.0208048,"ng (2017). Their parser builds token representations with a bidirectional LSTM over word embeddings, followed by head and dependent MLPs. Compatibility between heads and dependents is then scored using a biaffine model, and the highest scoring head for each dependent is selected. Previously, (Chen and Manning, 2014) pioneered neural network paring with a transitionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings. Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representatio"
W17-4301,D14-1181,0,0.00881267,"model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks. Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly 5 5.1 Experimental Results Data and Evaluation We train our parser on the English Penn TreeBank on the typical data split: training on sections 2–21, testing on section 23 and using section 22 for development. We convert constituency trees to dependencies using the Stanford dependency framework v3.5 (de Marneffe and Manning, 2008), and use part-of-speech tags from the Stanford left3words par"
W17-4301,Q16-1023,0,0.278106,"onvolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best neural network parsers. t] [roo My Heads dog also g age likes eatin saus . nmod dog nsubj also advmod likes root g eatin age saus xcomp . punct dobj Figure 1: Receptive field for predicting the headdependent relationship between likes and eating. Darker cell indicates more layers include that cell’s representation. Heads and labels corresponding to gold tree are indicated. Introduction Donald et al., 2005; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) generally employ attention to produce marginals over each possible edge in the graph, followed by a dynamic programming algorithm to find the most likely tree given those marginals. By vastly accelerating and parallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing. The best neural dependency parsers generally consist of two stages: First, the"
W17-4301,D16-1180,0,0.0175434,"ionbased dependency parser which used features from a fast feed-forward neural network over word, token and label embeddings. Many improved upon this work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks. Despite their success in the area of computer vision, in NLP convolutional neural networks"
W17-4301,D15-1180,0,0.019137,"Let r() denote the ReLU activation function (Glorot et al., 2011). Beginning with ct (0) = it we define the stack of layers with the following recurrence:   (k−1) cij (k) = r D2Lc −1 ct (k−1) (3) and add a final dilation-1 layer to the stack:   (L ) cij (Lc +1) = r D1 c ct (Lc ) Training (9) t=1 By rewarding accurate predictions after each application of the block, we learn a model where later blocks are used to refine initial predictions. The loss also helps reduce the vanishing gradient problem (Hochreiter, 1998) for deep architectures. (7) 3 perform a pooling operation over characters. Lei et al. (2015) present a CNN variant where convolutions adaptively skip neighboring words. While the flexibility of this model is powerful, its adaptive behavior is not well-suited to GPU acceleration. More recently, inspired by the success of deep dilated CNNs for image segmentation in computer vision (Yu and Koltun, 2016; Chen et al., 2015), convolutional neural networks have been employed as fast models for tagging, speech generation and machine translation. (van den Oord et al., 2016) use dilated CNNs to efficiently generate speech, and Kalchbrenner et al. (2016) describes an encoder-decoder model for m"
W17-4301,I17-1007,0,0.0118853,"his work by increasing the size of the network and using a structured training objective (Weiss et al., 2015; Andor et al., 2016). (Kiperwasser and Goldberg, 2016) were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs and labels. (Cheng et al., 2016) propose a similar network, except with additional forward and backward encoders to allow for conditioning on previous predictions. (Kuncoro et al., 2016) take a different approach, distilling a consensus of many LSTM-based transition-based parsers into one graph-based parser. (Ma and Hovy, 2017) employ a similar model, but add a CNN over characters as an additional word representation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks. Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a labe"
W17-4301,H05-1066,0,0.246473,"Missing"
W17-4301,P09-1040,0,0.0327286,"ginals. By vastly accelerating and parallelizing the core numeric operations for performing inference and computing gradients in neural networks, recent developments in GPU hardware have facilitated the emergence of deep neural networks as state-ofthe-art models for many NLP tasks, such as syntactic dependency parsing. The best neural dependency parsers generally consist of two stages: First, they employ a recurrent neural network such as a bidirectional LSTM to encode each token in context; next, they compose these token representations into a parse tree. Transition based dependency parsers (Nivre, 2009; Chen and Manning, 2014; Andor et al., 2016) produce a well-formed tree by predicting and executing a series of shiftreduce actions, whereas graph-based parsers (McBecause of their dependency on sequential processing of the sentence, none of these architectures fully exploit the massive parallel processing capability that GPUs possess. If we wish to maximize GPU resources, graph-based dependency parsers are more desirable than their transitionbased counterparts since attention over the edgefactored graph can be parallelized across the entire sentence, unlike the transition-based parser which"
W17-4301,D15-1174,0,0.0695197,"presentation and perform structured training using the Matrix-Tree Theorem. Hashimoto et al. (2017) train a large network which performs many NLP tasks including part-of-speech tagging, chunking, graph-based parsing, and entailment, observing benefits from multitasking with these tasks. Despite their success in the area of computer vision, in NLP convolutional neural networks have mainly been relegated to tasks such as sentence classification, where each input sequence is mapped to a single label (rather than a label for each token) Kim (2014); Kalchbrenner et al. (2014); Zhang et al. (2015); Toutanova et al. (2015). As described above, CNNs have also been used to encode token representations from embeddings of their characters, which similarly 5 5.1 Experimental Results Data and Evaluation We train our parser on the English Penn TreeBank on the typical data split: training on sections 2–21, testing on section 23 and using section 22 for development. We convert constituency trees to dependencies using the Stanford dependency framework v3.5 (de Marneffe and Manning, 2008), and use part-of-speech tags from the Stanford left3words part-of-speech tagger. As is the norm for this dataset, our evaluation exclud"
W17-4301,D17-1283,1,\N,Missing
W18-1706,W06-1669,0,0.132215,"Missing"
W18-1706,Q15-1016,0,0.0479379,"DIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1) where the word embedding w ≥ 0, the context embedding c ≥ 0, cN ≥ 0, #(w, c) are number of Ptimes context word c co-occur with w, #(w) = #(w, c), σ is the logistic sigmoid function, kI c is a constant hyper-parameter,PZ is the average #(w) #(w) of all words (i.e., Z = w|V | and |V |is the size of vocabulary), and PD is the distribution of negative samples. The two modifications do not change the time and space complexity of training skip-gram, which is one of the most scalable word embedding methods (Levy et al., 2015). DIVE is originally designed to perform unsupervised hypernymy detection task, and its goal is to preserve the inclusion relation between two context features in the sparse bag of words 40 Section 3.1 Raw text corpus Section 3.2 Distributional inclusion vector embedding Topics/ bases Skip gram embedding Word embedding EM refinement Refined sense embedding Target-dependent similarity measurement Ego network Graphbased clustering Basis index clusters Section 3.3 Sense embedding Weighted average Figure 1: The flowchart of the proposed method. The blue boxes are processing steps, the orange boxes"
W18-1706,P17-1151,0,0.0170361,"of information such as user intervention or prior knowledge to determine the number of clusters. To efficiently cluster many samples, Sch¨utze (1992) sub-samples the context of mentions; Mu et al. (2017) run principle component analysis (PCA) to compress the contexts of each target word before clustering; other approaches adopt iteratively local search algorithms after random initialization such as expectation maximization (EM) (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Tian et al., 2014; Pi˜na and Johansson, 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) or gradient descent (Athiwaratkun and Wilson, 2017). Although the random initialization and local search methods could be very eflDIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1) where the word embedding w ≥ 0, the context embedding c ≥ 0, cN ≥ 0, #(w, c) are number of Ptimes context word c co-occur with w, #(w) = #(w, c), σ is the logistic sigmoid function, kI c is a constant hyper-parameter,PZ is the average #(w) #(w) of all words (i.e., Z = w|V | and |V |is the size of vocabulary), and PD is the distribution of negative samples. The two modifications do not change the time and space complexity of"
W18-1706,D15-1200,0,0.0233475,"hierarchical sense clustering, it is hard to inject other sources of information such as user intervention or prior knowledge to determine the number of clusters. To efficiently cluster many samples, Sch¨utze (1992) sub-samples the context of mentions; Mu et al. (2017) run principle component analysis (PCA) to compress the contexts of each target word before clustering; other approaches adopt iteratively local search algorithms after random initialization such as expectation maximization (EM) (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Tian et al., 2014; Pi˜na and Johansson, 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) or gradient descent (Athiwaratkun and Wilson, 2017). Although the random initialization and local search methods could be very eflDIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1) where the word embedding w ≥ 0, the context embedding c ≥ 0, cN ≥ 0, #(w, c) are number of Ptimes context word c co-occur with w, #(w) = #(w, c), σ is the logistic sigmoid function, kI c is a constant hyper-parameter,PZ is the average #(w) #(w) of all words (i.e., Z = w|V | and |V |is the size of vocabulary), and PD is the distribution of negative s"
W18-1706,W17-2402,0,0.0500132,"ay saves the time of finding similar words, the samples need to be clustered drastically increase because each target word could have tens of thousands of mentions in the corpus of interest. This makes bottom-up hierarchical clustering or global optimization such as spectral clustering (Stella and Shi, 2003) become infeasible. Without hierarchical sense clustering, it is hard to inject other sources of information such as user intervention or prior knowledge to determine the number of clusters. To efficiently cluster many samples, Sch¨utze (1992) sub-samples the context of mentions; Mu et al. (2017) run principle component analysis (PCA) to compress the contexts of each target word before clustering; other approaches adopt iteratively local search algorithms after random initialization such as expectation maximization (EM) (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Tian et al., 2014; Pi˜na and Johansson, 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) or gradient descent (Athiwaratkun and Wilson, 2017). Although the random initialization and local search methods could be very eflDIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1)"
W18-1706,W06-3812,0,0.0669067,"-the-art graph-based WSI method, without the need of expensive nearest neighbor search. Our method is even better for the words without a dominating sense. 2 Related Work WSI methods can be roughly divided into two categories (Pelevina et al., 2016): clustering words similar to the target/query word or clustering mentions of the target word. We address their general limitations below. 2.1 Clustering Related Words Graph-based clustering for WSI has a long history and many different variations (Lin et al., 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Agirre et al., 2006; Biemann, 2006; Navigli and Crisafulli, 2010; Hope and Keller, 2013; Di Marco and Navigli, 2013; Mitra et al., 2014; Pelevina et al., 2016). In general, the method is to first retrieve words similar or related to each target word as nodes, measure the similarity/relatedness between the words to form an ego graph/network, and either group the nodes by graph clustering or find hubs or representative nodes in the graph using HyperLex (V´eronis, 2004) or PageRank (Agirre et al., 2006). As we mentioned in the introduction section, building word similarity graph and performing graph clustering is usually computat"
W18-1706,biemann-2012-turk,0,0.0406441,"Missing"
W18-1706,N18-1045,1,0.919099,"pic should be clustered together with a city topic if the target word is place. However, if the query word is bank, it makes more sense to group the country topic with the money topic into one sense so that the bank mention in Bank of America will belong to the sense. This means we want to focus on the geographical meaning of country when the target word is more about geography, while focus on the economic meaning of country when the target word is more about economics. In order to tackle the issue, we adopt a recently proposed approach called distributional inclusion vector embedding (DIVE) (Chang et al., 2018). DIVE compresses the sparse bag-of-words while preserving the co-occurrence frequency order, so DIVE is able to model not only the possibility of observing one target word in a topic as typical topic models but also the possibility of observing one topic of a sentence containing a target word mention. This allows us to efficiently identify the topics relevant to each target word, and only focus on an aspect of each of these topics composed of the words relevant to both the topic and the target word. Experiments show that our method performs similarly compared with Pelevina et al. (2016), a st"
W18-1706,P14-1096,0,0.38499,"re, they can also be separated into two different senses, depending on the level of granularity we would like to model. For graph-based clustering methods, it is easy and natural to model the multiple resolutions of senses in a consistent way by hierarchical clustering and defer the difficult problem of choosing the number of clusters to the end. This makes it easier to incorporate other information, such as users’ resolution preference on each hierarchical sense tree. The flexibility is one of the reasons why graph-based methods are widely studied and applied to many downstream applications (Mitra et al., 2014; Mathew et al., 2017; Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). Nevertheless, graph-based WSI methods usually require a substantial amount of computational resources. For example, Pelevina et al. (2016) build the graph by finding the nearest neighbors Word sense induction (WSI), which addresses polysemy by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graphbased method for WSI that builds a global non-negative vector embedding b"
W18-1706,J13-3008,0,0.0608685,"Missing"
W18-1706,E03-1020,0,0.0933274,"rms similarly compared with Pelevina et al. (2016), a state-of-the-art graph-based WSI method, without the need of expensive nearest neighbor search. Our method is even better for the words without a dominating sense. 2 Related Work WSI methods can be roughly divided into two categories (Pelevina et al., 2016): clustering words similar to the target/query word or clustering mentions of the target word. We address their general limitations below. 2.1 Clustering Related Words Graph-based clustering for WSI has a long history and many different variations (Lin et al., 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Agirre et al., 2006; Biemann, 2006; Navigli and Crisafulli, 2010; Hope and Keller, 2013; Di Marco and Navigli, 2013; Mitra et al., 2014; Pelevina et al., 2016). In general, the method is to first retrieve words similar or related to each target word as nodes, measure the similarity/relatedness between the words to form an ego graph/network, and either group the nodes by graph clustering or find hubs or representative nodes in the graph using HyperLex (V´eronis, 2004) or PageRank (Agirre et al., 2006). As we mentioned in the introduction section, building word similarity graph"
W18-1706,D10-1012,0,0.470387,"ifferent senses, depending on the level of granularity we would like to model. For graph-based clustering methods, it is easy and natural to model the multiple resolutions of senses in a consistent way by hierarchical clustering and defer the difficult problem of choosing the number of clusters to the end. This makes it easier to incorporate other information, such as users’ resolution preference on each hierarchical sense tree. The flexibility is one of the reasons why graph-based methods are widely studied and applied to many downstream applications (Mitra et al., 2014; Mathew et al., 2017; Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). Nevertheless, graph-based WSI methods usually require a substantial amount of computational resources. For example, Pelevina et al. (2016) build the graph by finding the nearest neighbors Word sense induction (WSI), which addresses polysemy by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graphbased method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clus"
W18-1706,D14-1113,1,0.907791,"as spectral clustering (Stella and Shi, 2003) become infeasible. Without hierarchical sense clustering, it is hard to inject other sources of information such as user intervention or prior knowledge to determine the number of clusters. To efficiently cluster many samples, Sch¨utze (1992) sub-samples the context of mentions; Mu et al. (2017) run principle component analysis (PCA) to compress the contexts of each target word before clustering; other approaches adopt iteratively local search algorithms after random initialization such as expectation maximization (EM) (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Tian et al., 2014; Pi˜na and Johansson, 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) or gradient descent (Athiwaratkun and Wilson, 2017). Although the random initialization and local search methods could be very eflDIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1) where the word embedding w ≥ 0, the context embedding c ≥ 0, cN ≥ 0, #(w, c) are number of Ptimes context word c co-occur with w, #(w) = #(w, c), σ is the logistic sigmoid function, kI c is a constant hyper-parameter,PZ is the average #(w) #(w) of all words (i.e., Z = w|V | and |V"
W18-1706,S13-2049,0,0.0699489,"Missing"
W18-1706,E12-1060,0,0.189832,"state-of-theart methods while being significantly more efficient. 1 Introduction Word sense induction (WSI) is a challenging task of natural language processing whose goal is to categorize and identify multiple senses of polysemous words from raw text without the help of predefined sense inventory like WordNet (Miller, 1995). The problem is sometimes also called unsupervised word sense disambiguation (Agirre et al., 2006; Pelevina et al., 2016). An effective WSI has wide applications. For example, we can compare different induced senses in different documents to detect novel senses over time (Lau et al., 2012; Mitra et al., 2014) or analyze sense difference in multiple corpora (Mathew et al., 2017). WSI could also be used to group and diversify the documents retrieved from search engine (Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). After identifying senses, we can 38 Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 38–48 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics of the target word in the word embedding space (i.e., ego network). Thus, constructing ego networks for all the wor"
W18-1706,W16-1620,0,0.0948059,"senses in a consistent way by hierarchical clustering and defer the difficult problem of choosing the number of clusters to the end. This makes it easier to incorporate other information, such as users’ resolution preference on each hierarchical sense tree. The flexibility is one of the reasons why graph-based methods are widely studied and applied to many downstream applications (Mitra et al., 2014; Mathew et al., 2017; Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). Nevertheless, graph-based WSI methods usually require a substantial amount of computational resources. For example, Pelevina et al. (2016) build the graph by finding the nearest neighbors Word sense induction (WSI), which addresses polysemy by unsupervised discovery of multiple word senses, resolves ambiguities for downstream NLP tasks and also makes word representations more interpretable. This paper proposes an accurate and efficient graphbased method for WSI that builds a global non-negative vector embedding basis (which are interpretable like topics) and clusters the basis indexes in the ego network of each polysemous word. By adopting distributional inclusion vector embeddings as our basis formation model, we avoid the expe"
W18-1706,R15-1061,0,0.0377961,"Missing"
W18-1706,N10-1013,0,0.0975222,"Missing"
W18-1706,W15-2916,0,0.0213246,"Agrawal1 , Ananya Ganesh1 , Anirudha Desai1 , Vinayak Mathur1 , Alfred Hough2 , Andrew McCallum1 1 CICS, University of Massachusetts, 140 Governors Dr., Amherst, MA 01003 2 Lexalytics, 320 Congress St, Boston, MA 02210 {hschang,amolagrawal,aganesh}@cs.umass.edu, {anirudhadesa,vinayak,mccallum}@cs.umass.edu al.hough@lexalytics.com Abstract train an embedding for each sense of a word. Li and Jurafsky (2015) demonstrate that this multiprototype word embedding is useful in several downstream applications including part-of-speech (POS) tagging, relation extraction, and sentence relatedness tasks. Sumanth and Inkpen (2015) also show that word sense disambiguation could be successfully applied to sentiment analysis. Since word sense induction (WSI) methods are unsupervised, the senses are typically derived from the results of different clustering techniques. Like most of the clustering problems, it is usually challenging to predetermine the number of clusters/senses each word should have. In fact, for many words, the “correct” number of senses is not unique. Setting the number of clusters differently can capture different resolutions of senses. For instance, race in the car context could share the same sense wit"
W18-1706,C14-1016,0,0.0277833,"ella and Shi, 2003) become infeasible. Without hierarchical sense clustering, it is hard to inject other sources of information such as user intervention or prior knowledge to determine the number of clusters. To efficiently cluster many samples, Sch¨utze (1992) sub-samples the context of mentions; Mu et al. (2017) run principle component analysis (PCA) to compress the contexts of each target word before clustering; other approaches adopt iteratively local search algorithms after random initialization such as expectation maximization (EM) (Reisinger and Mooney, 2010; Neelakantan et al., 2014; Tian et al., 2014; Pi˜na and Johansson, 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) or gradient descent (Athiwaratkun and Wilson, 2017). Although the random initialization and local search methods could be very eflDIV E = kI X w XX w c #(w, c) log σ(wT c) + Z X #(w, c) E [log σ(−wT cN )], cN ∼PD #(w) c (1) where the word embedding w ≥ 0, the context embedding c ≥ 0, cN ≥ 0, #(w, c) are number of Ptimes context word c co-occur with w, #(w) = #(w, c), σ is the logistic sigmoid function, kI c is a constant hyper-parameter,PZ is the average #(w) #(w) of all words (i.e., Z = w|V | and |V |is the size of voc"
W18-2607,P17-1171,0,0.380963,"ion along with correct and incorrect answer options. The dataset contains both the source passage as well as the question and answer options. ment to improve existing QA systems. Our annotators were also asked to mark whether individual retrieved sentences were relevant to answering a given question. Our labeling interface logs the reformulated queries issued by each annotator, as well as their relevance annotations. To quantitatively demonstrate the effectiveness of the relevant sentences, we (d) evaluate a subset of questions and the relevant retrieval results with a pre-trained DrQA model (Chen et al., 2017), and find that the performance of the system increases by 42 points. 2 Related Work To explore reading comprehension as a research problem, Hirschman et al. (1999) manually created a dataset of 3rd and 6th grade reading comprehension questions with short answers. The techniques that were explored for this dataset included pattern matching, rules, and logistic regression. More such datasets have been created that include natural language questions: for instance, MCTest (Richardson et al., 2013). MCTest is crowdsourced and comprises of 660 elementarylevel childrens fictional stories, which are"
W18-2607,D16-1244,0,0.0987748,"Missing"
W18-2607,D16-1264,0,0.0417925,"or instance, MCTest (Richardson et al., 2013). MCTest is crowdsourced and comprises of 660 elementarylevel childrens fictional stories, which are the source of questions and multiple choice answers. Questions and answers were constructed with a restrictive vocabulary that a 7 year-old could understand. Half of the questions constructed necessitated the answer to be derived from two sentences, with the motivation being to encourage research in multi-hop (one-hop) reasoning. Recent techniques such as Wang et al. (2015) and Yin et al. (2016) have performed well on this dataset. Currently, SQuAD (Rajpurkar et al., 2016) is one of the most popular datasets for reading comprehension: it uses Wikipedia passages as its source, and question-answer pairs are created using crowdsourcing. While it is stated that SQuAD requires logical reasoning, the complexity of reasoning required is far less than that for the AI2 standardized tests dataset (Clark and Etzioni, 2016; Kembhavi et al., 2017). NewsQA (Trischler et al., 2016) is another dataset that was created using crowdsourcing; it utilizes passages from 10, 000 news articles to create questions. Most of the datasets mentioned above are closed domain, where the answe"
W18-2607,D13-1020,0,0.0340657,"s, we (d) evaluate a subset of questions and the relevant retrieval results with a pre-trained DrQA model (Chen et al., 2017), and find that the performance of the system increases by 42 points. 2 Related Work To explore reading comprehension as a research problem, Hirschman et al. (1999) manually created a dataset of 3rd and 6th grade reading comprehension questions with short answers. The techniques that were explored for this dataset included pattern matching, rules, and logistic regression. More such datasets have been created that include natural language questions: for instance, MCTest (Richardson et al., 2013). MCTest is crowdsourced and comprises of 660 elementarylevel childrens fictional stories, which are the source of questions and multiple choice answers. Questions and answers were constructed with a restrictive vocabulary that a 7 year-old could understand. Half of the questions constructed necessitated the answer to be derived from two sentences, with the motivation being to encourage research in multi-hop (one-hop) reasoning. Recent techniques such as Wang et al. (2015) and Yin et al. (2016) have performed well on this dataset. Currently, SQuAD (Rajpurkar et al., 2016) is one of the most po"
W18-2607,P99-1042,0,0.124434,"xisting QA systems. Our annotators were also asked to mark whether individual retrieved sentences were relevant to answering a given question. Our labeling interface logs the reformulated queries issued by each annotator, as well as their relevance annotations. To quantitatively demonstrate the effectiveness of the relevant sentences, we (d) evaluate a subset of questions and the relevant retrieval results with a pre-trained DrQA model (Chen et al., 2017), and find that the performance of the system increases by 42 points. 2 Related Work To explore reading comprehension as a research problem, Hirschman et al. (1999) manually created a dataset of 3rd and 6th grade reading comprehension questions with short answers. The techniques that were explored for this dataset included pattern matching, rules, and logistic regression. More such datasets have been created that include natural language questions: for instance, MCTest (Richardson et al., 2013). MCTest is crowdsourced and comprises of 660 elementarylevel childrens fictional stories, which are the source of questions and multiple choice answers. Questions and answers were constructed with a restrictive vocabulary that a 7 year-old could understand. Half o"
W18-2607,P17-1147,0,0.0299692,"ed on 2 https://www.elastic.co/products/elasticsearch 1 http://data.allenai.org/arc/ 60 Proceedings of the Workshop on Machine Reading for Question Answering, pages 60–70 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics documents. SearchQA (Dunn et al., 2017) is an effort to create such a dataset; it contains 140K question-answer (QA) pairs. While the motivation was to create an open domain dataset, SearchQA provides text that contains ‘evidence’ (a set of annotated search results) and hence falls short of being a complete open-domain QA dataset. TriviaQA (Joshi et al., 2017) is another reading comprehension dataset that contains 650K QA pairs with evidence. Datasets created from standardized science tests offer some of the few existing examples of questions that require exploration of complex reasoning techniques to find solutions. A number of science-question focused datasets have been released over the past few years. The AI2 Science Questions dataset was introduced by Clark (2015) along with the Aristo Framework, which we build off of. This dataset contains over 1,000 multiple choice questions from state and federal science questions for elementary and middle"
W18-2607,P15-2115,0,0.0126404,"stic regression. More such datasets have been created that include natural language questions: for instance, MCTest (Richardson et al., 2013). MCTest is crowdsourced and comprises of 660 elementarylevel childrens fictional stories, which are the source of questions and multiple choice answers. Questions and answers were constructed with a restrictive vocabulary that a 7 year-old could understand. Half of the questions constructed necessitated the answer to be derived from two sentences, with the motivation being to encourage research in multi-hop (one-hop) reasoning. Recent techniques such as Wang et al. (2015) and Yin et al. (2016) have performed well on this dataset. Currently, SQuAD (Rajpurkar et al., 2016) is one of the most popular datasets for reading comprehension: it uses Wikipedia passages as its source, and question-answer pairs are created using crowdsourcing. While it is stated that SQuAD requires logical reasoning, the complexity of reasoning required is far less than that for the AI2 standardized tests dataset (Clark and Etzioni, 2016; Kembhavi et al., 2017). NewsQA (Trischler et al., 2016) is another dataset that was created using crowdsourcing; it utilizes passages from 10, 000 news"
W18-2607,W17-4413,0,0.0405765,"sed datasets have been released over the past few years. The AI2 Science Questions dataset was introduced by Clark (2015) along with the Aristo Framework, which we build off of. This dataset contains over 1,000 multiple choice questions from state and federal science questions for elementary and middle school students.3 A survey of the knowledge base requirements for accomplishing this task was performed by Clark et al. (2013), and concluded that advanced inference methods were necessary for many of the questions, as they could not be answered by simple fact-based retrieval. The SciQ Dataset (Welbl et al., 2017) contains 13,679 crowdsourced multiple choice science questions. To construct this dataset, workers were shown a passage and asked to construct a question along with correct and incorrect answer options. The dataset contains both the source passage as well as the question and answer options. ment to improve existing QA systems. Our annotators were also asked to mark whether individual retrieved sentences were relevant to answering a given question. Our labeling interface logs the reformulated queries issued by each annotator, as well as their relevance annotations. To quantitatively demonstrat"
W18-2607,W16-0103,0,0.0389985,"Missing"
W18-2607,C16-1278,0,\N,Missing
W18-2607,K17-1010,0,\N,Missing
W18-2607,L18-1433,0,\N,Missing
W18-2904,D08-1008,0,0.0158441,"hich explicitly incorporate syntactic knowledge through features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the data. Zhou and Xu (2015) were the first to achieve state-of-the-art results using 8 layers of bidirectional LSTM combined with inference in a linear-chain conditional The idea that an SRL model should incorporate syntactic structure is not new, since many semantic formalities are defined with respect to syntax. Many of the first approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008; Punyakanok et al., 2008), spearheaded by the CoNLL2005 shared task (Carreras and M`arquez, 2005), achieved success by relying on syntax-heavy linguistic features as input for a linear model, combined with structured inference which could also take syntax into account. T¨ackstr¨om et al. (2015) showed that most of these constraints could more efficiently be enforced by exact inference in a dynamic program. While most techniques required a predicted parse as input, Sutton and McCallum (2005) modeled syntactic parsing and SRL with a joint graphical model, and Lewis et al"
W18-2904,D17-1018,0,0.0478889,"Missing"
W18-2904,W05-0620,0,0.102731,"Missing"
W18-2904,P16-1101,0,0.0709685,"Missing"
W18-2904,K17-1041,0,0.0373407,"ng. FitzGerald et al. (2015) were among the first to successfully employ neural networks, achieving the state-of-the-art by embedding lexicalized features and providing the embeddings as factors in the model of T¨ackstr¨om et al. (2015). Related work Our experiments are based on the LISA model of Strubell and McCallum (2018), who showed that their method for incorporating syntax into a deep neural network architecture for SRL improves SRL F1 with predicted predicates on CoNLL2005 and CoNLL-2012 data, including on out4 https://github.com/allenai/bilm-tf 22 random field (Lafferty et al., 2001). Marcheggiani et al. (2017) and He et al. (2017) also achieved state-of-the-art results using deep LSTMs with no syntactic features. While most previous work assumes that gold predicates are given, like this work and Strubell and McCallum (2018), He et al. (2017) evaluate on predicted predicates, though they train a separate model for predicate detection. Most recently, Tan et al. (2018) achieved the state-of-the art on the CoNLL-2005 and 2012 shared tasks with gold predicates and no syntax using 10 layers of self-attention, and on CoNLL2012 with gold predicates Peters et al. (2018) increase the score of He et al. (2017"
W18-2904,D17-1159,0,0.0699358,"rs followed by two bi-LSTM layers with 4096 hidden units. All three layers are projected down to 512 dimensional representations over which our task-specific parameters are learned. This model is trained on the 1B Word Benchmark (Chelba et al., 2014), which consists of filtered English newswire, news commentary and European parliament proceedings from the WMT ’11 shared task. of-domain test data. Other recent works have also found syntax to improve neural SRL models when evaluated on data from the CoNLL2009 shared task: Roth and Lapata (2016) use LSTMs to embed syntactic dependency paths, and Marcheggiani and Titov (2017) incorporate syntax using graph convolutional neural networks over predicted dependency parse trees. In contrast to this work, Marcheggiani and Titov (2017) found that their syntax-aware model did not out-perform a syntax-agnostic model on out-of-domain data. 3 Recently there has been a move away from SRL models which explicitly incorporate syntactic knowledge through features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the data. Zhou and Xu (2015) were the first to achieve state-of-the-art results usin"
W18-2904,D15-1112,0,0.12094,"Missing"
W18-2904,J05-1004,0,0.238497,"her by experimenting with adding ELMo embeddings to models with and without syntax in order to determine whether ELMo can replace explicit syntax in SRL models, or if they can have a synergistic relationship. D&M LISA Brown Test GLoVe ELMo 92.01 92.56 88.88 89.57 Table 1: Dependency parse accuracy (UAS) on CoNLL-2005. 4.1 Data and pre-processing We evaluate our models on the data from the CoNLL-2005 semantic role labeling shared task (Carreras and M`arquez, 2005). This corpus annotates the WSJ portion of the Penn TreeBank corpus (Marcus et al., 1993) with semantic roles in the PropBank style (Palmer et al., 2005), plus a challenging out-of-domain test set derived from the Brown corpus (Francis and Kuˇcera, 1964). This dataset contains only verbal predicates and 28 distinct role label types. We obtain 105 SRL labels (including continuations) after encoding predicate argument segment boundaries with BIO tags. We use Stanford syntactic dependencies v3.5. 4.2 4 WSJ Test GLoVe ELMo 96.13 96.48 91.47 94.44 Syntactic parsing Table 1 presents the accuracy (UAS) of our dependency parsers. We experiment with adding ELMo embeddings to a strong graph-based dependency parser (Dozat and Manning, 2017), and present"
W18-2904,D14-1162,0,0.0930303,"ill Relevant in a Deep Neural Architecture for SRL? Emma Strubell Andrew McCallum College of Information and Computer Sciences University of Massachusetts Amherst {strubell, mccallum}@cs.umass.edu vide any additional benefits in a deep neural network architecture for e.g. semantic role labeling (SRL). In this work, we aim to begin to answer this question by experimenting with incorporating the ELMo embeddings of Peters et al. (2018) into LISA (Strubell and McCallum, 2018), a “linguistically-informed” deep neural network architecture for SRL which, when given weaker GloVe embeddings as inputs (Pennington et al., 2014), has been shown to leverage syntax to outperform a state-of-the-art, linguistically-agnostic end-to-end SRL model. In experiments on the CoNLL-2005 English SRL shared task, we find that, while the ELMo representations out-perform GloVe and begin to close the performance gap between LISA with predicted and gold syntactic parses, syntacticallyinformed models still out-perform syntax-free models, especially on out-of-domain data. Our results suggest that with the right modeling, incorporating linguistic structures can indeed further improve strong neural network models for NLP. Abstract Do unsup"
W18-2904,P17-1161,0,0.0197288,"instead building up token representations from characters. The ELMo architecture enhances the bidirec(4) With the locally-normalized probability of the correct role label yf t given by: P (yf t |X ) ∝ softmax(sf t ). At test time, we use Viterbi decoding to enforce BIO constraints with fixed transition probabilities between tags obtained from the training data. 2 During training, semantic role predictions are conditioned on the gold predicates. At test time they are conditioned on LISA’s predicted predicates (§2.1.3). 3 https://nlp.stanford.edu/projects/ glove/ 21 tional LM architecture from Peters et al. (2017). The model first composes character embeddings into word type embeddings using a convolutional layer followed by highway layers. Then these token representations are passed to multiple biLSTM layers, all of which are trained end-to-end to jointly optimize forward and backward LM objectives. ELMo additionally learns a small number of task-specific parameters to compose and scale the outputs of each LM, producing a task-specific embedding for each token in context. The intuition behind these task-specific parameters is that different tasks will benefit from different weightings of shallower and"
W18-2904,N18-1202,0,0.394548,"tween LISA with predicted and gold syntactic parses, syntacticallyinformed models still out-perform syntax-free models, especially on out-of-domain data. Our results suggest that with the right modeling, incorporating linguistic structures can indeed further improve strong neural network models for NLP. Abstract Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)? We address this question by incorporating the massively successful ELMo embeddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL. In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntacticallyinformed models still out-perform syntaxfree models when both use ELMo, especially on out-of-domain data. Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP. 1 2 Introduction Models We are interested in assessing"
W18-2904,P05-1072,0,0.0308192,"there has been a move away from SRL models which explicitly incorporate syntactic knowledge through features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the data. Zhou and Xu (2015) were the first to achieve state-of-the-art results using 8 layers of bidirectional LSTM combined with inference in a linear-chain conditional The idea that an SRL model should incorporate syntactic structure is not new, since many semantic formalities are defined with respect to syntax. Many of the first approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008; Punyakanok et al., 2008), spearheaded by the CoNLL2005 shared task (Carreras and M`arquez, 2005), achieved success by relying on syntax-heavy linguistic features as input for a linear model, combined with structured inference which could also take syntax into account. T¨ackstr¨om et al. (2015) showed that most of these constraints could more efficiently be enforced by exact inference in a dynamic program. While most techniques required a predicted parse as input, Sutton and McCallum (2005) modeled syntactic parsing an"
W18-2904,J08-2005,0,0.168053,"ugh features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the data. Zhou and Xu (2015) were the first to achieve state-of-the-art results using 8 layers of bidirectional LSTM combined with inference in a linear-chain conditional The idea that an SRL model should incorporate syntactic structure is not new, since many semantic formalities are defined with respect to syntax. Many of the first approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008; Punyakanok et al., 2008), spearheaded by the CoNLL2005 shared task (Carreras and M`arquez, 2005), achieved success by relying on syntax-heavy linguistic features as input for a linear model, combined with structured inference which could also take syntax into account. T¨ackstr¨om et al. (2015) showed that most of these constraints could more efficiently be enforced by exact inference in a dynamic program. While most techniques required a predicted parse as input, Sutton and McCallum (2005) modeled syntactic parsing and SRL with a joint graphical model, and Lewis et al. (2015) jointly modeled SRL and CCG semantic pars"
W18-2904,P16-1113,0,0.0200464,"r-level convolutional layer with 2048 filters followed by two highway layers followed by two bi-LSTM layers with 4096 hidden units. All three layers are projected down to 512 dimensional representations over which our task-specific parameters are learned. This model is trained on the 1B Word Benchmark (Chelba et al., 2014), which consists of filtered English newswire, news commentary and European parliament proceedings from the WMT ’11 shared task. of-domain test data. Other recent works have also found syntax to improve neural SRL models when evaluated on data from the CoNLL2009 shared task: Roth and Lapata (2016) use LSTMs to embed syntactic dependency paths, and Marcheggiani and Titov (2017) incorporate syntax using graph convolutional neural networks over predicted dependency parse trees. In contrast to this work, Marcheggiani and Titov (2017) found that their syntax-aware model did not out-perform a syntax-agnostic model on out-of-domain data. 3 Recently there has been a move away from SRL models which explicitly incorporate syntactic knowledge through features and structured inference towards models which rely on deep neural networks to learn syntactic structure and longrange dependencies from the"
W18-2904,D18-1548,1,0.74947,"Missing"
W18-2904,W05-0636,1,0.670207,"ny of the first approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008; Punyakanok et al., 2008), spearheaded by the CoNLL2005 shared task (Carreras and M`arquez, 2005), achieved success by relying on syntax-heavy linguistic features as input for a linear model, combined with structured inference which could also take syntax into account. T¨ackstr¨om et al. (2015) showed that most of these constraints could more efficiently be enforced by exact inference in a dynamic program. While most techniques required a predicted parse as input, Sutton and McCallum (2005) modeled syntactic parsing and SRL with a joint graphical model, and Lewis et al. (2015) jointly modeled SRL and CCG semantic parsing. Collobert et al. (2011) were among the first to use a neural network model for SRL, using a CNN over word embeddings combined with globallynormalized inference. However, their model failed to out-perform non-neural models, both with and without multi-task learning with other NLP tagging tasks such as part-of-speech tagging and chunking. FitzGerald et al. (2015) were among the first to successfully employ neural networks, achieving the state-of-the-art by embedd"
W18-2904,Q15-1003,0,0.179578,"Missing"
W18-2904,J08-2002,0,0.0880541,"Missing"
W18-2904,J93-2004,0,\N,Missing
W18-2904,P15-1109,0,\N,Missing
W18-2904,P17-1044,0,\N,Missing
W19-4007,P17-1008,0,0.0217709,"ition Of relations to “used”. Annotations of this kind were created when annotators deemed such an annotation absolutely necessary. Synthesis procedures which required annotation primarily of cross-sentence relations were ignored. 2.5 3 Inter-annotator Agreement Related Work Shallow semantic parsing in NLP: Prior work in the NLP community has defined and annotated semantic structures for text. These structured representations often seek to generalize about sentence level predicate-argument structure, abstracting away from the surface nuances of natural language and representing its semantics (Abend and Rappoport, 2017). A large body of work has created these resources for non-scientific text, as done in PropBank (Palmer et al., 2005; Surdeanu et al., 2008), FrameNet (Fillmore and Baker, 2010), AMR (Banarescu et al., 2013), semantic dependencies (Oepen et al., 2015) and ACE event schemas (Doddington et al., 2004). The GENIA project has defined event structures for biomedical data (Kim et al., 2003) while Garg et al. (2016) extended the AMR framework to biomedical text. Closer still to the work presented here, Mori et al. (2014) have annotated cooking recipes with sentence and discourse level semantic relatio"
W19-4007,W13-2322,0,0.317275,"ne, black solid appeared gradually and the clear solution turned into black slurry finally. should be considered valid and when it can be omitted. As our inter-annotator agreements will demonstrate, experts tend to agree often on what should be considered an operation. The question of what constitutes an operation is analogous to the notion of what constitutes an “event” in the broader NLP literature as highlighted by Mostafazadeh et al. (2016). Argument state and argument re-use: Annotation of semantic structures often allow for argument spans to have multiple parents (Surdeanu et al., 2008; Banarescu et al., 2013; Oepen et al., 2015). For example in Figure 2, the material “Cu” could be considered an argument of the operations “placed” and “heated”. Allowing for arguments to have multiple parents however runs into complications when the operation causes the state of a material to change (incidentally, this is not the case in the example we highlight above). When a materials state changes due to a specific operation, considering the same text span to be the argument of a different operation would not be chemically valid. For example, in the sentence: Most of the entities, “black solid”, “clear solution”"
W19-4007,mori-etal-2014-flow,0,0.163835,"m the surface nuances of natural language and representing its semantics (Abend and Rappoport, 2017). A large body of work has created these resources for non-scientific text, as done in PropBank (Palmer et al., 2005; Surdeanu et al., 2008), FrameNet (Fillmore and Baker, 2010), AMR (Banarescu et al., 2013), semantic dependencies (Oepen et al., 2015) and ACE event schemas (Doddington et al., 2004). The GENIA project has defined event structures for biomedical data (Kim et al., 2003) while Garg et al. (2016) extended the AMR framework to biomedical text. Closer still to the work presented here, Mori et al. (2014) have annotated cooking recipes with sentence and discourse level semantic relations. There has also been an interest in labeling scientific wetlab protoNext we report a host of inter-annotator agreements for the different levels of semantic annotation in our dataset. The agreements we report are based on a collection of 5 synthesis procedures which were annotated separately by all three expert annotators. All the numbers we report are Fleiss’ Kappa scores for the 3 expert annotators. Span-level Labels: Agreements on span level labels correspond to the agreement on entity type labels assigned"
W19-4007,doddington-etal-2004-automatic,0,0.169535,"in NLP: Prior work in the NLP community has defined and annotated semantic structures for text. These structured representations often seek to generalize about sentence level predicate-argument structure, abstracting away from the surface nuances of natural language and representing its semantics (Abend and Rappoport, 2017). A large body of work has created these resources for non-scientific text, as done in PropBank (Palmer et al., 2005; Surdeanu et al., 2008), FrameNet (Fillmore and Baker, 2010), AMR (Banarescu et al., 2013), semantic dependencies (Oepen et al., 2015) and ACE event schemas (Doddington et al., 2004). The GENIA project has defined event structures for biomedical data (Kim et al., 2003) while Garg et al. (2016) extended the AMR framework to biomedical text. Closer still to the work presented here, Mori et al. (2014) have annotated cooking recipes with sentence and discourse level semantic relations. There has also been an interest in labeling scientific wetlab protoNext we report a host of inter-annotator agreements for the different levels of semantic annotation in our dataset. The agreements we report are based on a collection of 5 synthesis procedures which were annotated separately by"
W19-4007,W16-1007,0,0.060805,"Missing"
W19-4007,S15-2153,0,0.0688257,"Missing"
W19-4007,J05-1004,0,0.130967,"cessary. Synthesis procedures which required annotation primarily of cross-sentence relations were ignored. 2.5 3 Inter-annotator Agreement Related Work Shallow semantic parsing in NLP: Prior work in the NLP community has defined and annotated semantic structures for text. These structured representations often seek to generalize about sentence level predicate-argument structure, abstracting away from the surface nuances of natural language and representing its semantics (Abend and Rappoport, 2017). A large body of work has created these resources for non-scientific text, as done in PropBank (Palmer et al., 2005; Surdeanu et al., 2008), FrameNet (Fillmore and Baker, 2010), AMR (Banarescu et al., 2013), semantic dependencies (Oepen et al., 2015) and ACE event schemas (Doddington et al., 2004). The GENIA project has defined event structures for biomedical data (Kim et al., 2003) while Garg et al. (2016) extended the AMR framework to biomedical text. Closer still to the work presented here, Mori et al. (2014) have annotated cooking recipes with sentence and discourse level semantic relations. There has also been an interest in labeling scientific wetlab protoNext we report a host of inter-annotator agre"
W19-4007,W08-2121,0,0.0361685,"Missing"
W19-4007,W19-2609,0,0.149258,"rections In this work we present a shallow semantic parsing dataset consisting of 230 synthesis procedures. The dataset was annotated by domain experts in materials science. We also highlight specific difficulties in the annotation process and present agreement metrics on the different levels of our annotation. We believe the dataset will enable the development of robust supervised entity tagging models and is suitable for evaluating models trained to extract shallow semantic structures. This is evidenced by the adoption of the dataset by work contemporaneous with this work (Kim et al., 2018; Tamari et al., 2019). Future work in the development of this dataset could involve methods for the scaling up of the annotation process, perhaps by adapting the guidelines to enable annotation by non-experts at some stages of the annotation process. Further, we also plan to quantitatively establish the limits of our annotation schema for the kinds of information it isn’t able to capture. We also plan to add additional layers of annotation, including: co-reference relations between synthesis steps, states of argument entities, and linking annotated entities to entries in materials science knowledge bases such as T"
W19-4007,E12-2021,0,\N,Missing
W99-0908,P95-1026,0,0.0232808,"e into naturally redundant and independent parts. For example, web pages can be thought of as the text on the web page, and the collection of text in hyperlink anchors to that page. A recent paper by Riloff and Jones (1999) bootstraps a dictionary of locations from just a small set of known locations. Here, their mutual bootstrap algorithm works by iteratively identifying syntactic constructs indicative of known locations, and identifying new locations using these indicative constructs. The preliminary labeling by keyword matching used in this paper is similar to the seed collocations used by Yarowsky (1995). There, in a word sense disambiguation task, a bootstrapping algorithm is seeded with some examples of common collocations with the particular sense of some word (e.g. the seed &quot;life&quot; for the biological sense of &quot;plant&quot;). (4) The Aj vectors are calculated by the iterations of EM. In the E-step we calculate for each class cj and each word of unlabeled held out data, ~ , the probability that the word was generated by the ith ancestor. In the M-step, we normalize the sum of these expectations to obtain new mixture weights ,kj. Without the use of held out data, all the mixture weight would concen"
