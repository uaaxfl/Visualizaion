2020.aacl-main.4,P11-1016,0,0.338735,"technique is widely used to analyze online posts reviews, mainly from Amazon reviews or Twitter, to help raise the ability to understand consumer needs or experiences with a product, guiding a manufacturer towards product improvement. Aspect-level sentiment classiﬁcation is much more complicated than sentence-level sentiment classiﬁcation. ASC task is necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context"
2020.aacl-main.4,S14-2076,0,0.0231667,"s for words according to their contribution to the ﬁnal classiﬁcation. Experiments are conducted on ﬁve datasets demonstrate how the proposed model outperforms baselines for aspect-level sentiment analysis. The remainder of this paper is organized as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention represen"
2020.aacl-main.4,P18-1087,0,0.0484508,"Missing"
2020.aacl-main.4,D17-1047,0,0.0839424,"necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model ass"
2020.aacl-main.4,P14-2009,0,0.0267853,"s, exp(ei ) α i = n k=1 exp(ek ) sg = n  α i hE i Lcls (θ) = − (11) 4 T E hC i W c hj = τ +m T E hC i W c hj 4.1 (13) exp(ri ) βi = n k=1 exp(rk ) sc = n  β i hE i (14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To compare the proposed model with other aspectlevel sentiment analysis models, we conduct experiments on the following ﬁve commonly used datasets: Twitter was originally proposed by Dong et al. (2014) and contains several Twitter posts, while the other four corpora (Lap14, Rest14, Rest15, Rest16) were respectively retrieved from SemEval 2014 task 4 (Pontiki et al., 2014), SemEval 2015 task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016), which include two types of data, i.e., reviews of laptops and restaurants. The statistical descriptions of these corpora are shown in Table 1. We use accuracy and Macro-average F1 -score as evaluation metrics; these are commonly used in ASC task (Huang and Carley, 2019; Zhang et al., 2019). A higher accuracy or F1 -score indicates"
2020.aacl-main.4,D14-1162,0,0.0858262,"Missing"
2020.aacl-main.4,C18-1066,0,0.0176272,"fy the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model assigns equal weight"
2020.aacl-main.4,S15-2082,0,0.0672426,"Missing"
2020.aacl-main.4,S14-2004,0,0.0444343,"14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To compare the proposed model with other aspectlevel sentiment analysis models, we conduct experiments on the following ﬁve commonly used datasets: Twitter was originally proposed by Dong et al. (2014) and contains several Twitter posts, while the other four corpora (Lap14, Rest14, Rest15, Rest16) were respectively retrieved from SemEval 2014 task 4 (Pontiki et al., 2014), SemEval 2015 task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016), which include two types of data, i.e., reviews of laptops and restaurants. The statistical descriptions of these corpora are shown in Table 1. We use accuracy and Macro-average F1 -score as evaluation metrics; these are commonly used in ASC task (Huang and Carley, 2019; Zhang et al., 2019). A higher accuracy or F1 -score indicates better prediction performance j=τ +1 j=1 Experimental Results This section conducts comparative experiments on ﬁve corpora against several previously proposed methods for as"
2020.aacl-main.4,D13-1170,0,0.0114589,"248 692 2328 638 3608 1120 1204 542 1748 616 Max Length 43 39 81 70 77 68 72 61 72 77 Mean Length 19 19 21 17 18 17 15 17 16 18 Table 1: The summary of datasets ei = n  T E hL i W l hj = τ +m T E hL i W l hj  T set x(t) , y (t) t=1 = 1, where x(t) is a training sample, y (t) is the corresponding actual sentiment label, and T is the number of training samples in the corpus. The training goal is to minimize the cross-entropy Lcls (θ) deﬁned as, (10) j=τ +1 j=1 where Wl ∈ Rdh ×dh is a bilinear term that interacts with these two vectors and captures the speciﬁc semantic relations. According to Socher et al. (2013), such a tensor operator can be used to model complicated compositions between those vectors. Therefore, the attention score weight and ﬁnal representation of G-ATT are computed as, exp(ei ) α i = n k=1 exp(ek ) sg = n  α i hE i Lcls (θ) = − (11) 4 T E hC i W c hj = τ +m T E hC i W c hj 4.1 (13) exp(ri ) βi = n k=1 exp(rk ) sc = n  β i hE i (14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To c"
2020.aacl-main.4,D19-1464,0,0.0575234,"Missing"
2020.aacl-main.4,C16-1311,0,0.289307,"as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention representation of context words and aspect words. Huang et al. (2018) proposed a joint model based on an attention mechanism to model aspects and sentences. Tang et al. (2019) proposed a selfsupervised attention model that can dynamically update at"
2020.aacl-main.4,D16-1021,0,0.224665,"as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention representation of context words and aspect words. Huang et al. (2018) proposed a joint model based on an attention mechanism to model aspects and sentences. Tang et al. (2019) proposed a selfsupervised attention model that can dynamically update at"
2020.aacl-main.4,P19-1053,0,0.111208,"he sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model assigns equal weight to the edges connec"
2020.aacl-main.4,S14-2036,0,0.0129247,"used to analyze online posts reviews, mainly from Amazon reviews or Twitter, to help raise the ability to understand consumer needs or experiences with a product, guiding a manufacturer towards product improvement. Aspect-level sentiment classiﬁcation is much more complicated than sentence-level sentiment classiﬁcation. ASC task is necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Fig"
2020.aacl-main.4,D19-1343,1,0.838617,"ning different weights to edges. Syntactic constraints can be imposed to block the graph convolutional propagation of unrelated words. A convolutional layer and a memory fusion were applied to learn and exploit multiword relations and draw different weights of words to improve performance further. Experimental results on ﬁve datasets show that the proposed method yields better performance than existing methods. The code of this paper is availabled at https://github.com/YuanLi95/ GATT-For-Aspect. 1 Introduction Aspect-level sentiment classiﬁcation is a ﬁnegrained subtask in sentiment analysis (Wang et al., 2019; Peng et al., 2020). Given a sentence and an aspect that appears in the sentence, ASC aims to determine the sentiment polarity of that aspect (e.g., negative, neutral, or positive). For example, a review of a restaurant “The price is reasonable although the service is poor.” expresses a positive sentiment for the price aspect, but also conveys a 27 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 27–36 c December 4 - 7, 2020. 2020 Association for Comp"
2020.semeval-1.110,Q17-1010,0,0.0294379,"Missing"
2020.semeval-1.110,N19-1423,0,0.0620769,"Missing"
2020.semeval-1.110,N19-1012,0,0.046326,"sity Kunming, China Contact:{jtomasulo, wangjin, xjzhang}@ynu.edu.cn Abstract This paper describes a ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use BERT, FastText, Elmo, and Word2Vec to encode these titles then pass them to a Bidirectional GRU with attention. Finally, we used XGBoost on the concatenation of the results of the different models to make predictions. 1 Introduction The Humicroedit dataset was created for research in computational humor (Hossain et al., 2019). The authors employed Amazon Mechanical Turk annotators to edit a single word of some fifteen thousand news headlines in order to make them funny. Five other annotators then graded each edited title on a scale of 0-3, with 3 being the funniest. Titles, despite being very short (the longest were around twenty words), convey a lot of information. Using one word edits is a way to make minimal changes for an expressed purpose. These two factors make the Humicroedit dataset a useful tool for studying computational humor. During the competition, the organizers published the Funlines dataset, which"
2020.semeval-1.110,2020.semeval-1.98,0,0.120435,"o make them funny. Five other annotators then graded each edited title on a scale of 0-3, with 3 being the funniest. Titles, despite being very short (the longest were around twenty words), convey a lot of information. Using one word edits is a way to make minimal changes for an expressed purpose. These two factors make the Humicroedit dataset a useful tool for studying computational humor. During the competition, the organizers published the Funlines dataset, which improved on Humicroedit in terms of cost per title by gamifying the process: they provided cash rewards for the best annotators (Hossain et al., 2020b). The performance of annotators was measured on a mix of how funny their edits were and how well their grades tracked the overall average. Players were able to improve in both categories and the authors posit that this was due to the availability of live feedback. On average, the Funlines dataset is funnier than Humicroedit and there was better agreement on the grades. The Funlines dataset was made available to participants of the competition in January. There were two sub-tasks for SemEval Task 7: 1) for each headline and its edit, predict the mean of the grades assigned by the five annotat"
2020.semeval-1.110,2020.acl-demos.28,0,0.706677,"o make them funny. Five other annotators then graded each edited title on a scale of 0-3, with 3 being the funniest. Titles, despite being very short (the longest were around twenty words), convey a lot of information. Using one word edits is a way to make minimal changes for an expressed purpose. These two factors make the Humicroedit dataset a useful tool for studying computational humor. During the competition, the organizers published the Funlines dataset, which improved on Humicroedit in terms of cost per title by gamifying the process: they provided cash rewards for the best annotators (Hossain et al., 2020b). The performance of annotators was measured on a mix of how funny their edits were and how well their grades tracked the overall average. Players were able to improve in both categories and the authors posit that this was due to the availability of live feedback. On average, the Funlines dataset is funnier than Humicroedit and there was better agreement on the grades. The Funlines dataset was made available to participants of the competition in January. There were two sub-tasks for SemEval Task 7: 1) for each headline and its edit, predict the mean of the grades assigned by the five annotat"
2020.semeval-1.110,N18-1202,0,0.0322928,"Missing"
2020.semeval-1.116,D14-1179,0,0.0391395,"Missing"
2020.semeval-1.116,P19-1053,0,0.0219215,"o overﬁt. Attention mechanism (Bahdanau et al., 2015) breaks the limitation that the traditional encoder-decoder structure depends on a ﬁxed-length vector when encoding and decoding. Its implementation retains the intermediate output results of the input sequence via the LSTM encoder, trains a model to selectively learn these inputs and associates the output sequence with it when the model is output. Attention mechanisms have been widely used in various NLP ﬁelds such as the Transformer (Vaswani et al., 2017), Neural Machine Translation (Yang et al., 2016) and aspect-level sentiment analysis (Tang et al., 2019). 2.3 Image Channel Convolutional neural networks (CNNs) (Krizhevsky et al., 2012) are often used to extract image representations. A CNN is usually divided into convolution layers and pooling layers. The convolution layers are used to extract n-gram features from the picture pixels. Pooling selects a part of the input matrix and chooses the best representative for the region. The max pooling layer selects the max feature. ResNet model (He et al., 2016) is one of the widely used image recognition models, and it solves the deep vanishing gradient problem. The basic structure of the residual is"
2020.semeval-1.116,D19-1343,1,0.560145,"and fed them into the fully connected layer. Bidirectional Long Short-Term Memory (BiLSTM) (Greff et al., 2017) is a special Recurrent Neural Network. The LSTM model can better capture the long-distance dependencies. There are various novel 917 x Weight layer F ( x) ReLU Weight layer F ( x)  x x identity ReLU Figure 2: Residual learning: a building block. models based on LSTM, for instance: Wang (2020) proposed a tree-structured regional CNN-LSTM model for valence-arousal (VA) prediction. A capsule tree LSTM model introduces a dynamic routing algorithm to construct sentence representations (Wang et al., 2019), and experiments prove that the method improves the performance of the tree LSTM and the basic LSTM model. BiLSTM is based on LSTM and can better capture forward and backward semantic dependencies. We show how a memory block calculates the hidden state hTt and output Ct using the following equations. • Gate • Transformation ft = σ(Wf · [hTt−1 , xt ] + bf ) it = σ(Wi · [hTt−1 , xt ] + bi ) ot = σ(Wo · [hTt−1 , xt ] + bo ) (2) C˜t = tanh(Wc · [hTt−1 , xt ] + bc ) (3) Ct = ft ∗ Ct−1 + it ∗ C˜t hTt = ot ∗ tanh(Ct ) (4) • Status update here, xt is the input vector; Ct is the cell state vector; W a"
2020.semeval-1.116,C16-1288,0,0.0285962,"to LSTM but with fewer parameters, and it is not easy to overﬁt. Attention mechanism (Bahdanau et al., 2015) breaks the limitation that the traditional encoder-decoder structure depends on a ﬁxed-length vector when encoding and decoding. Its implementation retains the intermediate output results of the input sequence via the LSTM encoder, trains a model to selectively learn these inputs and associates the output sequence with it when the model is output. Attention mechanisms have been widely used in various NLP ﬁelds such as the Transformer (Vaswani et al., 2017), Neural Machine Translation (Yang et al., 2016) and aspect-level sentiment analysis (Tang et al., 2019). 2.3 Image Channel Convolutional neural networks (CNNs) (Krizhevsky et al., 2012) are often used to extract image representations. A CNN is usually divided into convolution layers and pooling layers. The convolution layers are used to extract n-gram features from the picture pixels. Pooling selects a part of the input matrix and chooses the best representative for the region. The max pooling layer selects the max feature. ResNet model (He et al., 2016) is one of the widely used image recognition models, and it solves the deep vanishing g"
2020.semeval-1.197,S18-1100,0,0.0283256,"se without misusing logical rules. Another set of techniques uses emotional language to induce the audience to agree with the speaker only based on the emotional bond that is being created, provoking the suspension of any rational analysis of the argumentation. The traditional NLP task generally classifies and detects propaganda techniques at the article level, which often fails to meet more detailed requirements. This fact has also been confirmed by previous iterations of the SemEval competition, where leading solutions used convolutional neural networks (CNN), long short-term memory (LSTM) (Baziotis et al., 2018) and transfer learning techniques (Duppada et al., 2018) . The main features of an article are extracted by using the feature capture and pooling of the CNN model, but these methods can only be used at the article level and are coarse-grained detection methods. However, limited research has focused on text classification (Lewis, 1995; Song et al., 2010). News articles have also been classified using the Bi-LSTM-CNN model (Li et al., 2018) . However, there are often many propaganda techniques in one article, and most of these techniques are efficient for propaganda classification but lack the a"
2020.semeval-1.197,P16-2006,0,0.0230635,"(Pennington et al., 2014). 2.1 Span Identification (SI) For the SI subtask requirements, we need to detect which fragments of the news article utilized a propaganda technique. The SemEval organizers provided us with 371 training sets. The data were plain text files, and the SI task was to identify specific pieces that contained at least one propaganda technique. To detect news article propaganda techniques, we tested some deep learning model and integration architectures (Chen and Guestrin, 2016) . For the SI subtask, we also experimented with GloVe-BiLSTM (Li et al., 2017; Luo et al., 2018; Cross and Huang, 2016) , BERT-LSTM, GloVe-LSTM and BERTBiLSTM (Agirre et al., 2016; MacAvaney et al., 2018) . As illustrated in Figure 1, our model includes an embedding layer, an LSTM layer, a fully connected layer and an output layer. First, the embedding layer represents every word using pretrained word embeddings. The LSTM layer is implemented to obtain contextual information. The hidden vector proceeded by each LSTM cell will be further fed into a dense layer with the sigmoid activation function. Then, we can discriminate whether a word is propaganda or not. Finally, we record the index of propaganda words and"
2020.semeval-1.197,2020.semeval-1.186,0,0.150838,"Missing"
2020.semeval-1.197,S16-1173,0,0.0513224,"Missing"
2020.semeval-1.197,S19-2165,0,0.0265609,"chniques in the corpus (Da San Martino et al., 2020) . Since there are overlapping spans, formally, it is a multilabel and multiclass classification problem. However, whenever a span is associated with multiple techniques, the input file will have multiple copies of these fragments; therefore, the problem can be algorithmically treated as a multiclass classification problem. We tried to use GloVe and BERT to generate sentence embeddings, but the experimental results showed that the sentence embedding produced by BERT pretraining was better. After training on the datasets given by the TC task (Drissi et al., 2019; Deriu et al., 2016) , through the model mentioned above, the micro-F1 -score on the development set was 0.561 and that on the test set 1513 System F1 -score BERT+LSTM 0.561 BERT+BiLSTM 0.520 BERT 0.438 BERT+XGBoost 0.476 Table 5: Scores of different models for the TC subtask on the development set. was 0.505. Our system ranked 22th out of 31 teams. In addition to the LSTM model, we also tested some machine learning architectures and some integrated learning methods. Because the performance of the BERT-LSTM model on this task is better than those of other models, we adopted the BERT-LSTM mode"
2020.semeval-1.197,P16-1101,0,0.0604374,", we can be concluded that the LSTM model with GloVe word embeddings obtained the best performance on this task. For the embeddings layer, we implemented GloVe to train the word embeddings (Papagiannopoulou and Tsoumakas, 2018). The input tokens were obtained using the NLTK toolkit on the given articles. After the word embedding representation trained by GloVe is obtained, an LSTM layer is connected. In LSTM, recurrent cells are connected in a special way to avoid vanishing and exploding gradients, and the number of hidden nodes in the LSTM layer is set to 150. We find that the Bi-LSTM model (Ma and Hovy, 2016) does not perform well on this task. Next, the features captured by the LSTM layer are flattened and passed to the hidden dense layer, and the parameters of the dense layer are set to 8, which analyzes the interactions among the obtained vectors. The dropout rate of the dense layer is set to 0.2 to prevent model overfitting. 2.2 Technique Classification (TC) The TC task is a multiclass classification task representing an extension of the SI task, and the TC subtask seeks to classify the various propaganda techniques identified by the SI subtask. Such techniques include the use of logical falla"
2020.semeval-1.197,D19-1565,0,0.0257644,"techniques. The code of this paper is availabled at: https://github.com/daojiaxu/semeval_11. 1 Introduction Propaganda techniques need to attach importance to arouse the emotions of the receivers, sometimes even by temporarily bypassing the intellectual defenses of the receivers Pearlin and Rosenberg (1952) . Propaganda uses psychological and rhetorical techniques to achieve its purpose. Such techniques include using logical fallacies and appealing to the emotions of the audience. Logical fallacies are usually hard to spot since the argumentation, at first, might appear correct and objective (Martino et al., 2019) . However, careful analysis shows that the conclusion cannot be drawn from the premise without misusing logical rules. Another set of techniques uses emotional language to induce the audience to agree with the speaker only based on the emotional bond that is being created, provoking the suspension of any rational analysis of the argumentation. The traditional NLP task generally classifies and detects propaganda techniques at the article level, which often fails to meet more detailed requirements. This fact has also been confirmed by previous iterations of the SemEval competition, where leadin"
2020.semeval-1.197,D14-1162,0,0.0910594,"file will have multiple copies of such fragments; therefore, the problem can be treated as a multiclass classification problem. The techniques include Appeal to Authority, Appeal to fear-prejudice, Black-and-White Fallacy, and so on. The rest of the paper is organized as follows. Section 2 describes the details of the LSTM used in our system. Section 3 presents the experimental results. Conclusions and future works are described in Section 4. 2 System Description We implemented LSTM model to accomplish this task. Meanwhile, the representations of input words are trained by using GloVe model (Pennington et al., 2014). 2.1 Span Identification (SI) For the SI subtask requirements, we need to detect which fragments of the news article utilized a propaganda technique. The SemEval organizers provided us with 371 training sets. The data were plain text files, and the SI task was to identify specific pieces that contained at least one propaganda technique. To detect news article propaganda techniques, we tested some deep learning model and integration architectures (Chen and Guestrin, 2016) . For the SI subtask, we also experimented with GloVe-BiLSTM (Li et al., 2017; Luo et al., 2018; Cross and Huang, 2016) , B"
2020.semeval-1.197,S19-2048,0,0.0417167,"Missing"
2020.semeval-1.224,D19-1082,0,0.0134584,"obability distribution over all classes from the BiLSTM output, which can be weighted into a single-value probability. Finally, we use a voting ensemble to obtain the final probability. 2.1 Multi-granularity Ordinal Classification Multi-granularity ordinal classification is widely applied in many domains. Multi-granularity classification is used to analyze visual objects from subordinate categories, e.g., species of birds or models of cars in computer vision (Wei et al., 2019; Berg et al., 2014). Multi-granularity classification is also used in many NLP tasks such as sentiment classification (Hao et al., 2019), neural machine translation (Mehri and Eskénazi, 2019), Dialog (Mehri and Eskénazi, 2019)and named entity recognition (Mai et al., 2018). In this task, there are two methods of fine-graining, ROC-AUC and ORDER. ROC-AUC: the probability of each word between (0,1) is divided into different parts as category labels 1711 ROC_AUC vs ORDER 0.0 ‘s 0.0 dang erous 0.55 is 0.0 label 0 ROC_AUC ORDER 0.70 0.65 0.60 score what label 1 0.55 0.50 not 0.22 to 0.22 0.45 score1 score2 score3 score class score4 label 2 score0 Figure 2: The comparison between ROC-AUC and ORDER in Binary Classification. evolv e 0"
2020.semeval-1.224,D11-1143,0,0.0325978,"egree of emphasis, which needs to be predicted. In a previous work, a rule-based approach is used (Widera et al., 1997). Later, Text-based features such as part-of-speech (POS), information content, position in the sentence and other information was adopted (Volker Strom, 2007). Some methods have been proposed for predicting emphasized words for expressive Text-To-Speech (TTS) based on a deep neural network (DNN) (Mass et al., 2018; Rosenberg et al., 2015). To address the multiple annotators problem, a majority voting ensemble has been used to transform the problem into single-label learning (Laws et al., 2011). Shirani (2019) suggest that the task should be transformed into label distribution learning (LDL). The main difference between such a method and previous works is that the label is not a single dispersed value, but rather a continuous label distribution. Therefore, the model can fit more accurately to a real label instead of having the distortion of transforming a probability into a single label. The essence of the problem is to use a sequence labeling model to predict a probability value for each word. The existing methods mainly applied either a softmax or conditional random field (CRF) (H"
2020.semeval-1.224,D19-1184,0,0.0270418,"e BiLSTM output, which can be weighted into a single-value probability. Finally, we use a voting ensemble to obtain the final probability. 2.1 Multi-granularity Ordinal Classification Multi-granularity ordinal classification is widely applied in many domains. Multi-granularity classification is used to analyze visual objects from subordinate categories, e.g., species of birds or models of cars in computer vision (Wei et al., 2019; Berg et al., 2014). Multi-granularity classification is also used in many NLP tasks such as sentiment classification (Hao et al., 2019), neural machine translation (Mehri and Eskénazi, 2019), Dialog (Mehri and Eskénazi, 2019)and named entity recognition (Mai et al., 2018). In this task, there are two methods of fine-graining, ROC-AUC and ORDER. ROC-AUC: the probability of each word between (0,1) is divided into different parts as category labels 1711 ROC_AUC vs ORDER 0.0 ‘s 0.0 dang erous 0.55 is 0.0 label 0 ROC_AUC ORDER 0.70 0.65 0.60 score what label 1 0.55 0.50 not 0.22 to 0.22 0.45 score1 score2 score3 score class score4 label 2 score0 Figure 2: The comparison between ROC-AUC and ORDER in Binary Classification. evolv e 0.55 . 0.11 Figure 3: ORDER in Ternary Classification. ("
2020.semeval-1.224,P19-1112,0,0.116249,"xperiment results Figure 4 . 3.3 Evaluation Metrics For evaluation, M atchm is the evaluation metric for this task: For each instance X in the test set Dtest, a (x) set Sm of m∈ (1,. . . , 4) words with the top m probabilities according to the ground truth. Analogously, (x) we select a prediction set Sm for each m, based on the predicted probabilities. The metric M atchm was defined as follows: P (x) (x) |Sm ∩Sˆm |/(min(m,|x|)) scorem = Matchm := x∈Dtext (1) |Dtest| where score0 is the average of score1 , score2 , score3 and score4 . 3.4 Results and Discussion We use the DL-BiLSTM+ELMo model (Shirani et al., 2019) as the baseline. The dev dataset experiment results are shown in Table 2. The results of test data in the post-evaluation period are 0.607, 0.731, 0.802, which is lower than the baseline of test data:0.608, 0.737, 0.807, 0.849, 0.75. Accroding to the dev dataset experiment results, we find that Ternary Classification performs the best, and Binary classifier performs the worst. The ensemble results are higher than any other single classification. The experimental results do not indicate that increasing the granularity of the division leads to better results. This is due to the imbalance of the"
2020.semeval-1.224,2020.semeval-1.184,0,0.0398041,"Missing"
2021.findings-acl.206,D19-1562,0,0.0167468,"-BERT model consisting of 12 layers of transformer encoders was implemented for comparison. ToBERT (Pappagari et al., 2019) was trained non-end2end using a word-to-segment strategy in a two-stage way. The second group includes existing methods incorporating user and product information such as NSC with user (U) and product (P) information incorporated into an attention (A) mechanism (NSC+UPA), user product neural network (UPNN) (Tang et al., 2015), hierarchical model with separate user attention and product attention (HUAPA) (Wu et al., 2018), and the chunkwise importance matrix model (CHIM) (Amplayo, 2019). The third group includes a set of BERT-based methods incorporating user and product information using different strategies, presented in Figs. 1(a)-(c). In detail, an uncased-base-BERT model first extracted fixed feature vectors from texts. Then, the BERT Concat (word) model incorporates attribute features into each word vector and stacks another 6 transformer encoders as the feature extractor. Similarly, the BERT Concat (text) incorporates attribute features into the representation of the special token [CLS] for the classification. Finally, the BERT Attention (bias) applied 6 more MA-Transf"
2021.findings-acl.206,D16-1171,0,0.134892,"low models to attach attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text re"
2021.findings-acl.206,P19-4007,0,0.0231606,"Missing"
2021.findings-acl.206,N19-1423,0,0.195549,"tion To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs were first fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to another task to be fed another smaller task-specific dataset. The abovementioned methods only use features from plain texts. Incorporating attribute information such as users and product"
2021.findings-acl.206,E17-1059,0,0.0211261,"attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text representations by adj"
2021.findings-acl.206,D17-1054,0,0.0183567,"ion to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text representations by adjusting the w"
2021.findings-acl.206,C18-1232,0,0.0116964,"Word Embeddings (d) Bilinear interaction in multi-attribute transformer Figure 1: Different strategies to incorporate external attribute knowledge into deep neural networks. Introduction To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs were first fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to"
2021.findings-acl.206,D14-1181,0,0.0222198,"ntation (a) Word representation Attention map Attr Feature Extractor Attr Attribute Embeddings Attribute Embeddings Scaled Dot-Product Attention Q Text Text K V Word Embeddings Feature Extractor (c) Bias terms in self-attention Word Embeddings (d) Bilinear interaction in multi-attribute transformer Figure 1: Different strategies to incorporate external attribute knowledge into deep neural networks. Introduction To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs wer"
2021.findings-acl.206,2021.ccl-1.108,0,0.0538441,"Missing"
2021.findings-acl.206,P15-1098,0,0.183868,"irst fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to another task to be fed another smaller task-specific dataset. The abovementioned methods only use features from plain texts. Incorporating attribute information such as users and products can improve sentiment analysis task performance. Previous works typically incorporated such external knowledge by concatenating these attributes into word and text representations (Tang et al., 2015), as shown in Figs. 1(a) and (b). Such methods are often introduced in shallow models to attach attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Comp"
2021.rocling-1.51,P16-2037,1,0.831006,"t, Yuan Ze University 2 School of Information Science and Engineering, Yunnan University 3 Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion"
2021.rocling-1.51,D19-1343,1,0.83437,"Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as"
2021.rocling-1.51,W17-5207,0,0.0616408,"Missing"
2021.rocling-1.51,N16-1066,1,0.933056,"tive) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotion"
2021.rocling-1.51,I17-4002,1,0.863212,"n, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotional states of students (Yu et al., 2018). Dimensional sentiment analysis is an effective technique to recognize the valence"
2021.semeval-1.112,2020.acl-main.45,0,0.0192663,"st set. R(Si , Gi ) = |Si ∩ Gi | |Gi | (3) If Gi is empty for some post ti , we set F1 (Si , Gi ) = 1 if Si is also empty and F1 (Si , Gi ) = 0 otherwise. Finally, we averaged F1 (Si , Gi ) over all posts ti . 3.3 Implementation Details Each model was fine-tuned for eight epochs. We used the Adam (Kingma and Ba, 2015), AdamW (Loshchilov and Hutter, 2017), and Stochastic Gradient Descent (SGD) algorithm for optimization. The final one used was AdamW with a learning rate of 5e − 6. In the training process, we attempted to use the cross-entropy loss, focal loss (Lin et al., 2020), and Dice loss (Li et al., 2020). The results on the validation set showed that the focal loss and Dice loss are better than the cross-entropy loss. This may be due to an imbalance between the toxic and nontoxic categories in the text. In order to compare with the baseline model, we finally used the crossentropy loss function to train all models. 3.4 Comparative Results We used BERT, ALBERT, RoBERTa, and XLNET as the transformer-based layers. The model exhibit844 ing the best performance on the validation set in the eight epochs was used to predict the spans on the test set in the competition. The results on the test set are"
2021.semeval-1.112,2021.semeval-1.6,0,0.0180417,"r was a two-classification layer that outputs the probability of a token belonging to a toxic span. 3.2 For this task, we employed the F1 -score metric (da San Martino et al., 2020) to evaluate the responses of a system participating in the challenge. For each post, ti , the predicted span was a set, Si , of character offsets and Gi was the character offset of the groundtruth annotations of ti . The F1 score of ti was calculated as follows: Experimental Results In this section, we present the comparative results of the proposed model. 3.1 Dataset During the competition, we used only the data (Pavlopoulos et al., 2021) provided by the task organizer for the experiments. This task involves trial data (which include 689 posts and spans), training data (which include 7939 posts and spans), and test data (which include 2000 posts). We used the training data as the training set and trial data as the validation set. We needed to find the subscript offset set of the toxic spans of each post in the test Evaluation Metrics F1 (Si , Gi ) = 2 ∗ P (Si , Gi ) ∗ R(Si , Gi ) P (Si , Gi ) + R(Si , Gi ) (1) where P (Si , Gi ) and R(Si , Gi ) are respectively precision and recall scores defined as follows: 843 P (Si , Gi ) ="
2021.semeval-1.112,N19-1423,0,0.150709,"in purpose of this task is to identify the toxic spans in a given text; this can be transformed into a sequence labeling task in natural language processing. Unlike normal sequence labeling tasks, this task is more challenging because the toxic spans in the text may involve a word, phrase, or even a sentence. Traditional methods used to address the problem of sequence labeling include conditional random fields (CRF) (Lafferty et al., 1999), combined models of both long-short-term memory and CRF (LSTM-CRF) (Gupta et al., 2019), and bidirectional encoder representation from transformers (BERT) (Devlin et al., 2019). semeval2021_task5 1 Introduction Existing toxicity detection datasets and models classify the entire comment or document and do not identify the range that makes the text toxic. A system that accurately locates the toxicity range in the text is crucial in achieving semi-automatic review. As a complete submission for the shared task, systems are required to extract a list of toxic spans or an empty list per text. We define a sequence of words that attribute to the text’s toxicity as the toxic span. Table 1 shows two toxic spans, ”stupid” and ”a!@#!@,” which have character offsets from 10 to 1"
2021.semeval-1.112,D19-5012,0,0.0423439,"Missing"
2021.semeval-1.144,2020.semeval-1.186,0,0.0815727,"Missing"
2021.semeval-1.144,2020.semeval-1.197,1,0.738459,"f influencing users through 1 The code of this paper is availabled at: https:// github.com/zxyqujing/SemEval-2021-task6 The detection of propaganda techniques in texts is similar to a text sentiment analysis, and both can be attributed to text classification tasks. In a previous study, Peng et al. (2020) used the adversarial learning of sentiment word representations for a sentiment analysis. A tree-structured regional CNN-LSTM (Wang et al., 2020) and dynamic routing in a tree-structured LSTM (Wang et al., 2019) were used for a dimensional sentiment analysis. In previous SemEval competitions, Dao et al. (2020) used GloVe-LSTM and BERT-LSTM models, and Paraschiv et al. (2020) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles (Da San Martino et al., 2020) . In addition, in multimodal analysis combining images and 1045 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1045–1050 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Input Text Input Text w1,w2,w3,w4,…… ,wn-1,wn w1,w2,w3,w4,……,wn-1,wn ALBERT Hidden Representation (512,768) BERT Hi"
2021.semeval-1.144,N19-1423,0,0.0258832,"proposed model is shown in Figure 4. Subtask 2 was a multi-label sequence-labeling task. We built the model by converting the problem to detect the coverage of each propagation technique separately for the input sequence, and built a multi-task sequence labeling model based on a fine-tuning of BERT. Text Channel. In the text channel, we used the ALBERT–Text-CNN model used in subtask 1, taking the text part of the meme content as an input to obtain a 768-dimensional text feature vector as the output. As illustrated in Figure 3, the input sequence was first obtained using the pre-trained BERT (Devlin et al., 2019) model with a hidden representation matrix with dimensions of 512 × 768. Subsequently, 20 parallel fully connected layers were input separately for the detection of each propaganda technique coverage span (For each propagation technique, the sequence labeling task is performed separately for the input text) . For each technique, the intermediate result of each parallel channel output is a 512 × 41 matrix, and the ensemble layer represents the stacking of 20 matrices from 20 parallel channels, the dimensions of the final output were 20 × 512 × 41, which denote the propaganda technique category,"
2021.semeval-1.144,2021.semeval-1.7,0,0.0293717,", O-Smears and I-Smears. For 20 different propaganda techniques there are 40 different codes, and then add another padding code, so the label code length is 41. • In subtask 3, we normalized the meme image size to 224 × 224 × 3. 3.2 Evaluation Metrics The official evaluation measure for all subtasks is the micro F1 -score, which is defined as follows: F1 − score = 2 ∗ P rec ∗ Rec P rec + Rec (1) where Prec and Rec denote the precision and recall scores of all samples, respectively. For subtask 2, the standard micro F1 -score was slightly modified to account for partial matching between spans (Dimitrov et al., 2021). In addition, the macro F1 -score was also reported for each type of propaganda. 3.3 Implementation Details All models used the TensorFlow2 backend, and all BERT-based models were implemented using Values 5e-6 1e-8 0.3 64 0.3 16 Table 1: Hyper-parameters in our models. 3.4 Results Table 2 presents the results of Subtask 1. We conducted experiments on several pre-trained models including BERT, RoBERTa(Liu et al., 2019), and ALBERT combined with the Text-CNN layer, and observed that the ALBERT and Text-CNN models achieved the best performance, the reason for which may be that the training datas"
2021.semeval-1.144,D14-1181,0,0.00742273,". As illustrated in Figure 2, the proposed model includes an ALBERT layer, a TextCNN layer, a fully connected layer, and an output layer. 1046 • ALBERT (Lan et al., 2020) is a lite BERT for self-supervised learning of language representations, which uses layer-to-layer parameter sharing to reduce the number of parameters of the model, which not only speeds up the model training but also outperforms BERT on certain datasets. With our model, the pretrained ALBERT model is fine-tuned to obtain a 512 × 768 hidden representation matrix for subsequent multi-label classification of text. • Text-CNN (Kim, 2014) is a convolutional neural network applied to a text classification task, using multiple kernels of different sizes to extract key information in sentences, and is thus able to better capture the local relevance. In Image input size(244,244,3) w1,w2,w 3,w 4,……,wn-1,wn Text input Image ALBERT ResNet18/VGG16 … Flatten Hidden Representation (512,768) Text-CNN Text Feature （1,768） Image Feature (1,512) Concatenate(1,1280) Dense (22 units, sigmoid) … Output (1,22) Figure 3: Architecture of parallel channel model. this layer, we used three different sizes of onedimensional convolution kernels, i.e.,"
2021.semeval-1.144,2021.ccl-1.108,0,0.0506425,"Missing"
2021.semeval-1.144,2020.emnlp-demos.6,0,0.0596971,"Missing"
2021.semeval-1.144,2020.semeval-1.116,1,0.770492,"ics Input Text Input Text w1,w2,w3,w4,…… ,wn-1,wn w1,w2,w3,w4,……,wn-1,wn ALBERT Hidden Representation (512,768) BERT Hidden Representation (512,768) … Conv1 Kernel_size = 3 Conv2 Kernel_size = 4 Conv3 Kernel_size = 5 MaxPool_1 MaxPool_2 MaxPool_2 Flatten Flatten Flatten … Dense1 Dense2 Dense3 … Dense20 softmax softmax softmax … softmax Intermediate Results (512, 41) Concatenate Ensemble Dense1 (768 units relu) Output (20, 512, 41) Dense2 (20 units sigmoid) Text-CNN … Figure 2: Architecture of multi-task sequence labeling model. Output (1,20) Figure 1: ALBERT-Text-CNN model architecture. text, Yuan et al. (2020) proposed a parallel channel ensemble model combining BERT embedding, BiLSTM, attention and CNN, and ResNet for a sentiment analysis of memes. Li et al. (2019) proposed a Visual BERT model that aligns and fuses text and image information using transformers (Vaswani et al., 2017) . In this paper, we propose three different systems for the three subtasks in SemEval-2021 Task 6. For subtask 1, we added a Text-CNN layer after the pre-trained model ALBERT to fine-tune it for a multi-label classification of text. For subtask 2, we used the idea of partitioning to transform the problem into the detec"
2021.semeval-1.144,2020.semeval-1.244,0,0.0217166,"labled at: https:// github.com/zxyqujing/SemEval-2021-task6 The detection of propaganda techniques in texts is similar to a text sentiment analysis, and both can be attributed to text classification tasks. In a previous study, Peng et al. (2020) used the adversarial learning of sentiment word representations for a sentiment analysis. A tree-structured regional CNN-LSTM (Wang et al., 2020) and dynamic routing in a tree-structured LSTM (Wang et al., 2019) were used for a dimensional sentiment analysis. In previous SemEval competitions, Dao et al. (2020) used GloVe-LSTM and BERT-LSTM models, and Paraschiv et al. (2020) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles (Da San Martino et al., 2020) . In addition, in multimodal analysis combining images and 1045 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1045–1050 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Input Text Input Text w1,w2,w3,w4,…… ,wn-1,wn w1,w2,w3,w4,……,wn-1,wn ALBERT Hidden Representation (512,768) BERT Hidden Representation (512,768) … Conv1 Kernel_size = 3 Conv2 Kernel"
2021.semeval-1.144,D19-1343,1,0.833107,"aigns (Martino et al., 2020) , and memes applied in a disinformation campaign achieve their purpose of influencing users through 1 The code of this paper is availabled at: https:// github.com/zxyqujing/SemEval-2021-task6 The detection of propaganda techniques in texts is similar to a text sentiment analysis, and both can be attributed to text classification tasks. In a previous study, Peng et al. (2020) used the adversarial learning of sentiment word representations for a sentiment analysis. A tree-structured regional CNN-LSTM (Wang et al., 2020) and dynamic routing in a tree-structured LSTM (Wang et al., 2019) were used for a dimensional sentiment analysis. In previous SemEval competitions, Dao et al. (2020) used GloVe-LSTM and BERT-LSTM models, and Paraschiv et al. (2020) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles (Da San Martino et al., 2020) . In addition, in multimodal analysis combining images and 1045 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1045–1050 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Input Text Inpu"
2021.semeval-1.184,N19-1423,0,0.194277,". For the base model, B-preﬁx and I-preﬁx were used to discriminate the beginning of classiﬁcation and ensure the accuracy of the tokens; therefore, 65 types of entities were used in the model. In this paper, an ensemble model is proposed for time 1289 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1289–1294 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics expression recognition with a hard voting strategy. Each input sample was ﬁrst tokenized using the matched model tokenizer. Base models, such as BERT (Devlin et al., 2019) and its variants, including RoBERTa (Liu et al., 2019), DistilBERT (Sanh et al., 2019) and ALBERT (Lan et al., 2019), were used to learn hidden representations for each token. Then, a fully connected dense layer with a Sof tmax function was used for classiﬁcation. By using a hard voting strategy, the predictions from different base models were merged with the ﬁnal result. Experimental results show that the proposed ensemble models achieve a competitive result with the ofﬁcial baseline model, which ranked ﬁfth in sub-task 2. The remainder of this paper is organized as follows. Section 2 descri"
2021.semeval-1.184,D14-1162,0,0.0915151,"Missing"
2021.semeval-1.184,N18-1202,0,0.0174632,"larger batches, and larger vocabulary than the BERT model. In addition, the provided baseline model was ﬁne-tuned with approximately 25,000 expressions in de-identiﬁed clinical notes as well as the development dataset (Sanh et al., 2019). Four other models were also implemented using a hard voting strategy and a hard voting result for a single submission. 2.1 Tokenization Transforming words to vectors is a necessary step in NLP tasks. Different word representations were used in our implementation, including word2vec (Mikolov et al., 2013; CHURCH, 2017), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019). For the RoBERTa model, we used only the matched RoBERTa tokenizer to build word vectors with a length of 514. Given a sentence x = yN-1 y1 y2 t1 t2 ... ... BERT ... ... Output Layer t[CLS] BERT-based Model Tokenization tN-1 t[SEP] ... ... E[CLS] E1 E2 ... ... EN-1 E[SEP] [CLS] x1 x2 ... ... xN-1 [SEP] Raw Input x Figure 1: Overall architecture of the base model. [x1 , x2 , . . . , xM ] of length M , different lengths of the sentence will result in different lengths of the representation. Therefore, we considered the maximum sentence length as N − 1. If the len"
2021.semeval-1.184,2020.aacl-main.4,1,0.697627,"t be protected. Obtaining annotations about health texts often requires the signing of complex data usage agreements. During the competition, the organizers provided models which ﬁne-tuned on the annotated source domain data, while the source domain data could not be distributed. The organizers also provided labeled data as dev data and unlabeled target domain data as test data. There were two sub-tasks for SemEval Task 10: 1) to classify clinical event mentions (e.g., diseases, symptoms, procedures, etc.) for whether they are being negated by their context. This is a text-classiﬁcation task (Yuan et al., 2020). 2) to ﬁnd time expressions (Laparra et al., 2018) in text, this is a sequence-tagging task. SemEval Task 10 was different from traditional NLP tasks, where training and testing were in the same domain. Predictions could be out of control due to the different target domain. The domain of the dev data is related to the source and target domains, therefore the model can be developed. In SemEval Task 10, it is necessary to develop an existing model along with training the model with labeled data or unlabeled data. Furthermore, the model can be developed by ﬁne-tuning it in different ways. For ce"
2021.semeval-1.58,P16-1200,0,0.0334246,"the word embeddings for each token were passed through a fully connected layer, and thereafter through softmax and argmax, where they were mapped to a class label respectively except the tokens of ‘[CLS]’ and ‘[SEP]’, indicating whether the token was part of a scientific term or predicate phrase fragment. The model architecture is illustrated in Figure 3. Note that the fully connected layer, softmax, and argmax were shared across all tokens. Subtask 3: Triple Extraction This subtask was the most cardinal and complex step in the entire task. This could be considered a relation extraction task (Lin et al., 2016), which was completed by dividing into parts information units classification and triple formation. First, it was necessary to classify all scientific term and predicate phrases in a contribution sentence extracted from Subtask 2 to determine which category of the 12 information units the extracted phrases belonged to. This was a multiclass-sequence classification problem, where we identified the unit information belonging to the scientific term and predicate phrase fragments in a given contribution sentence by concatenating all phrases into a single string to feed our model. The system archit"
2021.semeval-1.58,P16-1101,0,0.048796,"the contribution sentences; and (3) triples: semantic statements that pair scientific terms with a relation, modeled toward the subjectpredicate-object statements for building knowledge graph. The triples were organized under three (mandatory) or more of the twelve total information units (i.e., ResearchProblem, Approach, Model, Code, Dataset, ExperimentalSetup, Hyperparameters, Baselines, Results, Tasks, Experiments, and AblationAnalysis). An illustration of this process is shown in Figure 1. The difficulty of this task lies in text classification (Joulin et al., 2017) and sequence labeling (Ma and Hovy, 2016). Text classification refers to determining which of the two or more labels a one-dimensional linear sequence belongs to. Similarly, sequence labeling is used to tag each element in a one-dimensional linear sequence with a label from a set of labels. Before the popularity of deep learning, the common solutions to the sequencelabeling problem were all based on either the hidden Markov model (Zhou and Su, 2001) or conditional random field (CRF) (Ye and Ling, 2018), with CRF being the mainstream method. With the development of deep learning, convolutional neural networks (CNN) (Kim, 2014) and rec"
2021.semeval-1.58,2020.semeval-1.231,0,0.0299556,"sional input into 2-dimensional numerical values. These values were then input into softmax to calculate the probability of a sentence being a contribution sentence. Finally, the probability outcomes were input into argmax, where, in our experimental setup, an output of 1 indicated a contribution sentence and 0 indicated the contrary. The overall architecture of the system is shown in Figure 2. Figure 3: System of sequence labeling for span identification task 2.3 Figure 2: System of binary classification for sentence classification task 2.2 Subtask 2: Span Identification Span identification (Singh et al., 2020) was a binary sequence tagging task where we classified each token in a contribution sentence to indicate whether it was part of a scientific term or predicate phrase fragment. We passed the contribution sentence identified from Subtask 1 into a pre-trained BERT model and obtained embeddings for each token in the sequence. Next, the word embeddings for each token were passed through a fully connected layer, and thereafter through softmax and argmax, where they were mapped to a class label respectively except the tokens of ‘[CLS]’ and ‘[SEP]’, indicating whether the token was part of a scientif"
2021.semeval-1.58,2020.semeval-1.197,1,0.751354,"s the details of the BERT model used in our system. Section 3 presents the experimental results. Finally, the conclusions are presented in Section 4. 2 System Description We used a pre-trained BERT model to accomplish the task, which was defined in terms of three dataset annotation elements, where the extraction of each data element relied on the extraction of the previous data element. 2.1 Subtask 1: Sentence Classification The first part of this task was to extract sentences that reflected the research contribution in the given NLP scholarly articles. We termed this sentence classification (Dao et al., 2020), where we predicted whether a sentence in an article was a contribution sentence. To this end, our approach was to pass each sentence in an article through the pretrained BERT model to generate 768-dimensional word embeddings for each word in the sentence. The next thing we were going to do was to take the word embeddings of the first token of each sentence 479 1 https://github.com/maxinge8698/ SemEval2021-Task11 (i.e. ‘[CLS]’) to do sentence classification because it integrated the semantic information of the whole sentence. Then this word embeddings acquired from the previous step was conne"
2021.semeval-1.58,2021.semeval-1.44,0,0.0601453,"Missing"
2021.semeval-1.58,E17-2068,0,0.0256116,"s and relational cue phrases extracted from the contribution sentences; and (3) triples: semantic statements that pair scientific terms with a relation, modeled toward the subjectpredicate-object statements for building knowledge graph. The triples were organized under three (mandatory) or more of the twelve total information units (i.e., ResearchProblem, Approach, Model, Code, Dataset, ExperimentalSetup, Hyperparameters, Baselines, Results, Tasks, Experiments, and AblationAnalysis). An illustration of this process is shown in Figure 1. The difficulty of this task lies in text classification (Joulin et al., 2017) and sequence labeling (Ma and Hovy, 2016). Text classification refers to determining which of the two or more labels a one-dimensional linear sequence belongs to. Similarly, sequence labeling is used to tag each element in a one-dimensional linear sequence with a label from a set of labels. Before the popularity of deep learning, the common solutions to the sequencelabeling problem were all based on either the hidden Markov model (Zhou and Su, 2001) or conditional random field (CRF) (Ye and Ling, 2018), with CRF being the mainstream method. With the development of deep learning, convolutional"
2021.semeval-1.58,D14-1181,0,0.0059056,"Ma and Hovy, 2016). Text classification refers to determining which of the two or more labels a one-dimensional linear sequence belongs to. Similarly, sequence labeling is used to tag each element in a one-dimensional linear sequence with a label from a set of labels. Before the popularity of deep learning, the common solutions to the sequencelabeling problem were all based on either the hidden Markov model (Zhou and Su, 2001) or conditional random field (CRF) (Ye and Ling, 2018), with CRF being the mainstream method. With the development of deep learning, convolutional neural networks (CNN) (Kim, 2014) and recurrent neural networks (RNN) (Cho et al., 2014) have achieved great success in text classification and se478 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 478–484 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Example of contribution extraction of NLP scholarly articles quence labeling. Since then, long short-term memory (LSTM) (Wang and Jiang, 2016), Bi-LSTM (Bi-directional long short-term memory), and other models (Yuan et al., 2020) have performed better than CNN and RNN in text cl"
2021.semeval-1.58,N16-1170,0,0.0327615,"field (CRF) (Ye and Ling, 2018), with CRF being the mainstream method. With the development of deep learning, convolutional neural networks (CNN) (Kim, 2014) and recurrent neural networks (RNN) (Cho et al., 2014) have achieved great success in text classification and se478 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 478–484 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Example of contribution extraction of NLP scholarly articles quence labeling. Since then, long short-term memory (LSTM) (Wang and Jiang, 2016), Bi-LSTM (Bi-directional long short-term memory), and other models (Yuan et al., 2020) have performed better than CNN and RNN in text classification and sequence labeling . However, since the introduction of bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018), the accuracy and training efficiency in both text classification and sequence labeling have reached new heights. The SemEval-2021 shared Task 11 (D’Souza et al., 2021) consists of three subtasks: • Subtask 1: identifying contribution sentences; • Subtask 2: identifying scientific terms and predicate phra"
2021.semeval-1.58,P18-2038,0,0.0188454,"is process is shown in Figure 1. The difficulty of this task lies in text classification (Joulin et al., 2017) and sequence labeling (Ma and Hovy, 2016). Text classification refers to determining which of the two or more labels a one-dimensional linear sequence belongs to. Similarly, sequence labeling is used to tag each element in a one-dimensional linear sequence with a label from a set of labels. Before the popularity of deep learning, the common solutions to the sequencelabeling problem were all based on either the hidden Markov model (Zhou and Su, 2001) or conditional random field (CRF) (Ye and Ling, 2018), with CRF being the mainstream method. With the development of deep learning, convolutional neural networks (CNN) (Kim, 2014) and recurrent neural networks (RNN) (Cho et al., 2014) have achieved great success in text classification and se478 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 478–484 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Example of contribution extraction of NLP scholarly articles quence labeling. Since then, long short-term memory (LSTM) (Wang and Jiang, 2016), Bi-LSTM ("
2021.semeval-1.58,2020.aacl-main.4,1,0.719856,"nt of deep learning, convolutional neural networks (CNN) (Kim, 2014) and recurrent neural networks (RNN) (Cho et al., 2014) have achieved great success in text classification and se478 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 478–484 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Example of contribution extraction of NLP scholarly articles quence labeling. Since then, long short-term memory (LSTM) (Wang and Jiang, 2016), Bi-LSTM (Bi-directional long short-term memory), and other models (Yuan et al., 2020) have performed better than CNN and RNN in text classification and sequence labeling . However, since the introduction of bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018), the accuracy and training efficiency in both text classification and sequence labeling have reached new heights. The SemEval-2021 shared Task 11 (D’Souza et al., 2021) consists of three subtasks: • Subtask 1: identifying contribution sentences; • Subtask 2: identifying scientific terms and predicate phrases; • Subtask 3: categorizing, identifying and organizing scientific terms and predica"
A97-1044,H93-1070,0,0.0288142,"head of the subject phrase and the main verb. These types of pairs account for most of the syntactic variants for relating two words (or simple phrases) into pairs carrying compatible semantic content. This also gives the pair-based representation sufficient flexibility to effectively capture content elements even in complex expressions. Long, complex phrases are similarly decomposed into collections of pairs, using corpus statistics to resolve structural ambiguities. 301 3.2 Linguistic Phrase Stream sion, whereas lnc.ntc slightly sacrifices the average precision, but gives better recall (see Buckley, 1993). We used a regular expression pattern matcher on the part-of-speech tagged text to extract noun groups and proper noun sequences. The major rules we used are: 1. a sequence of modifiers (vbnlvbgljj) followed by at least one noun, such as: &quot;cryonic suspend&quot;, &quot;air traffic control system&quot;; 2. proper noun(s) modifying a noun, such as: &quot;u.s. citizen&quot;, &quot;china trade&quot;; 3. proper noun(s) (might contain &apos;&&apos;), such as: &quot;warren commission&quot;, &quot;national air traffic controller&quot;. In these experiments, the length of phrases was limited to maximum 7 words. 3.3 N a m e S t r e a m Proper names, of people, places"
C92-1039,J83-3004,0,0.0341447,"In this paper we are proposing a method to add syntactic preferences to the Preferealce Semantics paradigm while nmiutaining its fundamental philosophy. It will be shown ttmt syntactic preferences can be coded as a set of weights associated with the set of symbolically manipulatable rules of a new grammar formalism. ~121e syntactic preferences such codezt can lye easily used to compute with semantic preferences. With the help of some tectmiques borrowed from Connectioeisln, these weights can be adjusted through tralrting. Another strategy is based on Preference Semantics (Wilks, 1975, 1978) (Fass & Wilks, 1983). The idea is that all the possible sentence readings can (though not necessarily) be produced. All the ""readings are scored according to how mm~y preference satisfactions they contmn"", and ""the best reading (that is, the one with the most preference satisfactions) is taken, even if it contains preference violations."" Selfridge (1986) and Slator (1988) &apos;also use this strategy. One important advantage of this approach is that within a uniform mechanism, the semantic constraints can both be used maximally for the sake of disambiguation m~d be gracefully relaxed when nece,ssmy for the sake of rob"
C92-1039,J86-2001,0,0.134023,"erences such codezt can lye easily used to compute with semantic preferences. With the help of some tectmiques borrowed from Connectioeisln, these weights can be adjusted through tralrting. Another strategy is based on Preference Semantics (Wilks, 1975, 1978) (Fass & Wilks, 1983). The idea is that all the possible sentence readings can (though not necessarily) be produced. All the ""readings are scored according to how mm~y preference satisfactions they contmn"", and ""the best reading (that is, the one with the most preference satisfactions) is taken, even if it contains preference violations."" Selfridge (1986) and Slator (1988) &apos;also use this strategy. One important advantage of this approach is that within a uniform mechanism, the semantic constraints can both be used maximally for the sake of disambiguation m~d be gracefully relaxed when nece,ssmy for the sake of robusmess. However, how to extend the preference philosophy to syntactic constraints have not been addressed. There are two li-equently used approaches to incorporate syntactic constraints in systems using semantic preferences. The first ,~s in (Slao tot, 1988), is to use a weak version of a rather typical syntactic module. The problem w"
C92-1039,J83-3003,0,0.563955,"Missing"
C92-1039,J81-2002,0,\N,Missing
C92-1039,J83-3001,0,\N,Missing
C96-2157,H92-1022,0,0.0400949,"Missing"
C96-2157,P91-1034,0,0.0481289,"Missing"
C96-2157,H91-1065,0,0.0475964,"Missing"
C96-2157,P95-1026,0,0.210527,"tes neutral evidence, which is of little or no consequeuce to the spotter. In general, we take SW(t) &gt; e &gt; 0 as a piece of positive evidence, and SW(t) &lt; - e as a piece of negative evidence, as provided by item t. Weights of evidence items within an evidence set are then combined to arrive at the compound context weight which is used to accept or reject candidate phrase. At this time, we make no claim as to whether (1) is an optimal fornmla for cah:ulating evidence weights. An alternative method we considered was to estimate certain conditional probabilities, similarly to the formula used in (Yarowsky, 1995): The set of evidence items generated for this fl&apos;aginent, i.e., E1 UE2 UEaUE4, contains the following elements: (boys, p), (kicked, p), (the, s), (door, s), (with, f), (rage , f), ((boys, kicked), p), ((the, door)), s), ((with, ,&apos;age), f), (boys, p, 2), (ki&ed, p, 1), (the, s, 2), (door, s, 1), (with, f, 1), (rage, f, 2), ((boys, kicked), p, 1), ((the, door)), s, 1), ((with, ,&apos;age), f, 1) towards or against the hyphothesis that the central unit belongs to the semantic category of interest to the spotter. The significance weights are acquired through corpus-based training. Training Evidence it"
C98-2200,C96-2157,1,0.756279,"here Sh is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; l is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the 4Refer to (Euhn 1958) (Paice 1990) (Kau, Brandow & Mitze 1994) (Kupiec, Pedersen & Chen 1995) for sentence-based summarization approaches. nThe weights Wh are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &: Wang 1996). For the purpose of tile experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. Both the French and Iranian g o v e r n m e n t s acknowledged the I r a n i a n role in the release of the three French hostages, J e a n - P a u l Kauffmann, Marcel C a r t o n and Marcel Fontaine, 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do not occur ofien elsewhere in the database. A weighted frequency score, similar to"
C98-2200,C94-1056,0,\N,Missing
D17-1056,esuli-sebastiani-2006-sentiwordnet,0,0.0333551,"Missing"
D17-1056,N15-1184,0,0.0858873,"Missing"
D17-1056,P15-1162,0,0.00655799,"sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neighbors (i.e., those with an opposite polarity). The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and outperform previously proposed sentiment embeddings. To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) are selected, including convolutional neural networks (CNN) (Kim, 2014), deep averaging network (DAN) (Iyyer et al., 2015) and long-short term memory (LSTM) (Tai et al., 2015; Looks et al., 2017). The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed sentiment embeddings to re-run the classification for performance comparison. The SST is chosen because it can show the effect of using different word embeddings on fine-grained sentiment classification, whereas prior studies only reported binary classification results. The rest of this paper is organized as follows. Section 2 describes the proposed word vector refinement model. Section 3 presents"
D17-1056,D15-1242,0,0.0277778,"Missing"
D17-1056,D14-1181,0,0.0262736,"e closer to a set of both semantically and sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neighbors (i.e., those with an opposite polarity). The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and outperform previously proposed sentiment embeddings. To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) are selected, including convolutional neural networks (CNN) (Kim, 2014), deep averaging network (DAN) (Iyyer et al., 2015) and long-short term memory (LSTM) (Tai et al., 2015; Looks et al., 2017). The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed sentiment embeddings to re-run the classification for performance comparison. The SST is chosen because it can show the effect of using different word embeddings on fine-grained sentiment classification, whereas prior studies only reported binary classification results. The rest of this paper is organized as follows. Section 2 describes the propos"
D17-1056,W16-0410,0,0.0339918,"Missing"
D17-1056,P13-2087,0,0.0223044,"and good-bad in (Tang et al., 2016). Composing these word vectors may produce sentence vectors with similar vector representations but opposite sentiment polarities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations). Building on such ambiguous vectors will affect sentiment classification performance. To enhance the performance of distinguishing words with similar vector representations but opposite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner (Maas et al., 2011; Labutov and Lipson, 2013; Lan et al., 2016; Ren et al., 2016; Tang et al., 2016). The common goal of these methods is to capture both semantic/syntactic and sentiment information such that sentimentally similar words have similar vector representations. They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and negative) given by the training instances. The use of such sentiment embeddings has improved the performance of binary sentiment classification. 534 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages"
D17-1056,P11-1015,0,0.0836917,"mmad et al., 2013) and good-bad in (Tang et al., 2016). Composing these word vectors may produce sentence vectors with similar vector representations but opposite sentiment polarities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations). Building on such ambiguous vectors will affect sentiment classification performance. To enhance the performance of distinguishing words with similar vector representations but opposite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner (Maas et al., 2011; Labutov and Lipson, 2013; Lan et al., 2016; Ren et al., 2016; Tang et al., 2016). The common goal of these methods is to capture both semantic/syntactic and sentiment information such that sentimentally similar words have similar vector representations. They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and negative) given by the training instances. The use of such sentiment embeddings has improved the performance of binary sentiment classification. 534 Proceedings of the 2017 Conference on Empirical Methods in Natural L"
D17-1056,D14-1162,0,0.109279,"rds and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST). 1 Introduction Word embeddings are a technique to learn continuous low-dimensional vector space representations of words by leveraging the contextual information from large corpora. Examples include C&W (Collobert and Weston, 2008; Collobert et al., 2011), Word2vec (Mikolov et al., 2013a; 2013b) and GloVe (Pennington et al., 2014). In addition to the contextual information, characterlevel subwords (Bojanowski et al., 2016) and semantic knowledge resources (Faruqui et al., 2015; Kiela et al., 2015) such as WordNet (Miller, 1995) are also useful information for learning word embeddings. These embeddings have been successfully used for various natural language processing tasks. In general, existing word embeddings are semantically oriented. They can capture semantic and syntactic information from unlabeled data in an unsupervised manner but fail to capture sufficient sentiment information. This makes it difficult to direc"
D17-1056,J11-2001,0,0.0298522,"er et al., 2013). It contains 13,915 words and each word is associated with a real-valued score in [1, 9] for the dimensions of valence, arousal and dominance. The valence represents the degree of positive and negative sentiment, where values of 1, 5 and 9 respectively denote most negative, neutral and most positive sentiment. In Fig. 1, good has a valence score of 7.89, which is greater than 5, and thus can be considered positive. Conversely, bad has a valence score of 3.24 and is thus negative. In addition to the E-ANEW, other lexicons such as SentiWordNet (Esuli and Fabrizio, 2006), SoCal (Taboada et al., 2011), SentiStrength (Thelwall et al., 2012), Vader (Hutto et al., 2014), ANTUSD (Wang and Ku, 2016) and SCL-NMA (Kiritchenko and Mohammad, 2016) also provide 535 real-valued sentiment intensity or strength scores like the valence scores. For each target word to be refined, the top-k semantically similar nearest neighbors are first selected and ranked in descending order of their cosine similarities. In Fig. 1, the left ranked list shows the top 10 nearest neighbors for the target word good. The semantically ranked list is then sentimentally re-ranked based on the absolute difference of the valence"
D17-1056,P15-1150,0,0.0708751,"Missing"
D17-1056,L16-1428,0,0.0266281,"[1, 9] for the dimensions of valence, arousal and dominance. The valence represents the degree of positive and negative sentiment, where values of 1, 5 and 9 respectively denote most negative, neutral and most positive sentiment. In Fig. 1, good has a valence score of 7.89, which is greater than 5, and thus can be considered positive. Conversely, bad has a valence score of 3.24 and is thus negative. In addition to the E-ANEW, other lexicons such as SentiWordNet (Esuli and Fabrizio, 2006), SoCal (Taboada et al., 2011), SentiStrength (Thelwall et al., 2012), Vader (Hutto et al., 2014), ANTUSD (Wang and Ku, 2016) and SCL-NMA (Kiritchenko and Mohammad, 2016) also provide 535 real-valued sentiment intensity or strength scores like the valence scores. For each target word to be refined, the top-k semantically similar nearest neighbors are first selected and ranked in descending order of their cosine similarities. In Fig. 1, the left ranked list shows the top 10 nearest neighbors for the target word good. The semantically ranked list is then sentimentally re-ranked based on the absolute difference of the valence scores between the target word and the words in the list. A smaller difference indicates that"
D19-1343,D14-1080,0,0.0378385,"EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are mor"
D19-1343,P15-1162,0,0.0162101,"ngton et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. As shown in Fig. 1(a), the priority for each word vector will be “fantastic reaall ris rstre this”. This prioritization seems satisfactory for this sentence, but note that the key components could appear anywhere"
D19-1343,P14-1062,0,0.0748188,"on. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these m"
D19-1343,D14-1181,0,0.0117456,"Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belon"
D19-1343,W17-0801,0,0.0511107,"Missing"
D19-1343,D14-1162,0,0.0868791,"proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015;"
D19-1343,N16-1174,0,0.0445282,"N story/NN is/VBZ (c) Capsule Tree-LSTM with Dynamic Routing is/VBZ (b) Tree-LSTM Figure 1: Illustrative examples of different LSTM models for sentiment analysis. A deeper color indicates more weight is assigned to the word according to its contribution to the prediction result. proposed method introduces a dynamic routing algorithm to consider all non-leaf nodes to build sentence vectors, instead of using the root alone in the tree-LSTM. In addition, different nodes will receive different weights according to their contributions to the prediction task. Unlike selfattention (Lin et al., 2017; Yang et al., 2016), which applies a fixed policy without considering the state of the final sentence vectors, the task of assigning weights in the proposed model is considered to be a routing issue to iteratively determine how much information can be passed from non-leaf nodes in the tree to the vector presentation of the sentence, according to the state of final output. For example, in the aforementioned example text, it would be useful for the model to emphasize fantastic that contains the most salient information, even when the word lies at the bottom of the parser tree. Based on the dynamic routing algorith"
D19-1343,N16-1066,1,0.82474,"in an update on the coupling coefficient ctj. 3 Experimental Results Datasets. This experiment used two datasets for evaluation. i) The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is used for sentiment classification. It contains 6920/872/1821 sentences for the train/dev/test sets with binary labels (positive/negative) and 8544/1101/2210 sentences with fine-grained labels (very negative/negative/ neutral/positive/very positive). ii) EmoBank (Buechel and Hahn, 2017; Buechel and Hahn, 2016) is used for sentiment regression to predict valence-arousal (VA) values (Wang et al., 2016b; Yu et al., 2016). It contains 10,000 sentences with real-valued VA ratings in the range of (1, 9), where the valence refers to the degree of positive and negative sentiment and the arousal refers to the degree of calm and excitement. The provided ratings have Reader and Writer perspectives, and the Reader was adopted as the ground-truth ratings due to its superiority reported in (Buechel and Implementation Details. Several deep neural networks were implemented for comparison, including CNN, GRU, LSTM, and tree-LSTM. For the sequential models (GRU and LSTM), we additionally implemented an enhanced version usin"
D19-1343,P15-1150,0,0.173847,"Missing"
D19-1343,P16-2037,1,0.853241,"ich will also result in an update on the coupling coefficient ctj. 3 Experimental Results Datasets. This experiment used two datasets for evaluation. i) The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is used for sentiment classification. It contains 6920/872/1821 sentences for the train/dev/test sets with binary labels (positive/negative) and 8544/1101/2210 sentences with fine-grained labels (very negative/negative/ neutral/positive/very positive). ii) EmoBank (Buechel and Hahn, 2017; Buechel and Hahn, 2016) is used for sentiment regression to predict valence-arousal (VA) values (Wang et al., 2016b; Yu et al., 2016). It contains 10,000 sentences with real-valued VA ratings in the range of (1, 9), where the valence refers to the degree of positive and negative sentiment and the arousal refers to the degree of calm and excitement. The provided ratings have Reader and Writer perspectives, and the Reader was adopted as the ground-truth ratings due to its superiority reported in (Buechel and Implementation Details. Several deep neural networks were implemented for comparison, including CNN, GRU, LSTM, and tree-LSTM. For the sequential models (GRU and LSTM), we additionally implemented an en"
D19-1343,P15-1130,0,0.0198154,"n addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. As shown in Fig. 1(a), the priority"
I17-4002,D14-1162,0,0.0889845,"Missing"
I17-4002,W10-0208,0,0.0330003,", lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are useful resources for the developm"
I17-4002,N16-1066,1,0.779642,"e output format is “term_id, valence_rating, arousal_rating”. Below are the input/output formats of the example words “好” (good), “非常好” (very good), “滿意” (satisfy) and “不滿意” (not satisfy). Example 1: Input: 1, 好 Output: 1, 6.8, 5.2 Example 2: Input: 2, 非常好 Output: 2, 8.500, 6.625 Example 3: Input: 3, 滿意 Output: 3, 7.2, 5.6 Example 4: Input: 4, 不滿意 Output: 4, 2.813, 5.688 Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words (Yu et al, 2016b). Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth. Each multi-word phrase was rated by at least 10 different annotators. Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations. They were then excluded from the calculation of the average ratings for each phrase. The policy of this shared task was implemented as is an open test. That is, in addition to the above of"
I17-4002,P16-2037,1,0.861068,"ontact: lcyu@saturn.yzu.edu.tw, lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are usef"
I17-4002,W11-3704,0,0.0240642,"Missing"
I17-4011,W16-4920,1,0.738079,") (7) The denominator is a normalization item: Z(x) = n XY y i=1 Mi (yi−1 , yi , x) (8) where Mi (y 0 , y 00 , x) is a potential function, x is the input vector produced by the BiLSTM layer and y is the labeling of the input sentence. 3 Experiment This section describes the contents of the experiment, including the training data processing, choice of parameters, experimental results, etc. 3.1 Dataset The word embedding was trained using the word2vec toolkit with the Chinese Wikipedia corpus. According to the experimental results, the word-embedding results from word2vec are better than GloVe (Yang et al., 2016). In addition to the CGED17 training data, the HSK (i.e., Chinese Proficiency Test) training data from CGED16 was used. The number of training sets is 20,048, with 10,447 from CGED17 and 9,601 from CGED16. For the reasons mentioned above, four models for every error type were selected. Thus, we preprocessed training sets for four error type models. For each error type, the position’s label is 0 if correct, or 1 if erroneous. The training-data text was transferred into the word-index sequence, according to the pre-trained word-embedding table. CRF Layer Because of the importance of the relation"
I17-4011,P82-1020,0,0.845858,"Missing"
I17-4029,P12-1092,0,0.0760836,"Missing"
I17-4029,D14-1080,0,0.0190869,"embedding models (word2vec, GloVe, and their concatenate modes). We found that our model using word2vec or GloVe slightly outperformed the baseline methods, whereas the ensemble model using both word2vec and GloVe achieved better performance in comparison to the other models. Dense (1,6) Drop out (1,100) Attend Dense (1,100) Encode Nothing at all ... Embed Our model is based on a bidirectional GRU (Bahdanau et al., 2014) with an attention mechanism (Raffel and Ellis, 2015). GRU was designed to have more persistent memory, thereby making it easier to capture long-term dependencies than an RNN. Irsoy and Cardie (2014) showed that such a bi-directional deep neural network maintains two hidden layers, one for the left-to-right propagation, and the other for the right-to-left propagation. We chose the Bi-GRU model because it could obtain full information through two propagations. In addition, attention mechanisms allow for a more direct dependence between the states of the model at different points in time. In this section, our model is described using the following four steps: embedding, encoding, attending, and prediction. The model architecture is shown in Fig. 1. ... ... ... Bi-GRUATT ... ... ... ... 2 Ou"
I17-4029,N13-1090,0,0.214196,"ined these two vectors, converted from each model, to a new 600-dimensional vector to obtain the advantages of both word2vec and GloVe. Embedding. We took size L tokens of text as input, where L was the maximum length of all training texts. In this English training dataset, L is 117. In addition, every word in the text was embedded into a 300-dimensional vector through the pre-trained embedding model. For those words that cannot be recognized in the pre-trained model, the same dimensional vector of zeros was replaced. This was also used for padding out the sentence when it was shorter than L. Mikolov et al. (2013a) proposed word2vec, which allows training on larger corpora, and Encoding. Through the given sequence of word vectors, the encoding step computes a representation of a sentence matrix, where each row represents the meaning of each token in the context of the rest of the sentence. We used a bidirectional GRU to summarize the contextual information from both directions of a sentence text, and obtained a full sentence matrix vector by concatenating the sentence matrix vector forward and backward at each time step. Similarly to the L175 Comment Complaint Request Bug Meaningless Undefined Total z"
I17-4029,D16-1244,0,0.0694792,"Missing"
I17-4029,D14-1162,0,0.0753881,"ct dependence between the states of the model at different points in time. In this section, our model is described using the following four steps: embedding, encoding, attending, and prediction. The model architecture is shown in Fig. 1. ... ... ... Bi-GRUATT ... ... ... ... 2 Output Predict Drop out (1,234) Attention (1,234) Bi-GRU (L,234) Input (L,300) ... Figure 1: Architecture of Bi-GRUATT model. (the left-hand side ignores dropout layers which marked up on the right hand side, and L is the maximum length of all training texts.) showed how semantic relationships emerge from such training. Pennington et al. (2014) proposed the GloVe approach, which maintains the semantic capacity of word2vec while introducing statistical information from a latent semantic analysis (LSA), which shows improvement in semantic and syntactic tasks. We tested word2vec and GloVe on pre-trained embedding models, and combined these two vectors, converted from each model, to a new 600-dimensional vector to obtain the advantages of both word2vec and GloVe. Embedding. We took size L tokens of text as input, where L was the maximum length of all training texts. In this English training dataset, L is 117. In addition, every word in"
I17-4029,N16-1174,0,0.129732,"Missing"
I17-4035,D15-1237,0,0.059605,"Missing"
I17-4035,D15-1080,0,0.0203363,"o the knowledge and reasoning framework. Clark et al. (2013) proposed a method based on text statistical rules. Clark (2015) described how to obtain more information from the background knowledge base, i.e., they introduced the use of background knowledge to build the best scene. Sachan et al. (2016) presented a unified max-margin framework that learns to detect the hidden structures that explain the correctness of an answer when provided with the question and instructional materials. A system that extracts information from the corpus for automatic generation of test questions was designed by Khot et al. (2015), whereas a structured inference system based on integer linear programming was proposed by Khashabi et al. (2016). A more complex method is presented in (Clark et al., 2016). This model operates at three levels of representation and reasoning: information retrieval, corpus statistics, and simple inference over a semiautomatically constructed knowledge base. In this paper, we mainly focus on an attentionbased long short-term memory (AT-LSTM) model. Two different word embeddings are used to learn the word vectors in both the Chinese and English corpora. Subsequently, the word vectors are fed in"
I17-4035,D15-1168,0,0.0659178,"sult of the AT-LSTM with our own training embedding as the final submission. Table 3 shows our final scores and ranking. Table 2: Optimal parameters Corpura Our score Rank 1 team Baseline score Our rank English 0.353 0.456 0.2945 4 Chinese 0.465 0.581 0.4463 2 All 0.423 0.423 0.39 1 Table 3: Final testing results and ranking m to the LSTM model. The results show that, under the same experimental equipment conditions, the AT-LSTM model can yield better results. Table 1 presents the results of a comparative experiment for an English Subset and a Chinese Subset. The Sklearn grid search function (Liu et al., 2015) is used to determine the best combination of the parameters. Although the same model is used for both the datasets, as the two datasets in the Chinese and English pretreatment are not the same, the parameters that achieve the best results may be different. Table 2 lists the parameters of the model when the best results are obtained. For the English subset, the best-tuned parameters are as follows: number of the filters in CNN is 64, length of a filter is 3, dropout rate is 0.3, dimensionality of the hidden layer in AT-LSTM and LSTM is 300, batch size is 32, and number of epochs is 20. Simulta"
I17-4035,D14-1162,0,0.095915,"ews) AT-LSTM (GoogleNews) CNN (GloVe) LSTM (GloVe) AT-LSTM (GloVe) Chinese Subset CNN (character vector) LSTM (character vector) AT-LSTM (character vector) CNN (word vector) LSTM (word vector) AT-LSTM (word vector) Input Dense Attention α Hidden vectors r LSTM LSTM Dense Word representation Softmax Question Answer Output Table 1: Comparative experiment results Figure 2: Architecture of the proposed AT-LSTM. train the model (Yi et al., 2015). In this experiment, for the English subset, the original corpus is transformed into a word vector by two different word embeddings: GoogleNews and GloVe (Pennington et al., 2014). The results show that GoogleNews can be used to obtain better results. It is used to initialize the weight of the embedding layer in build 300-dimension word vectors for all the questions and answers. For the Chinese subset, we also use a character vector and word vector with two different word embeddings. The character word embedding is trained from the Chinese version of Wikipedia, whereas the word vector embedding is trained from the news (12G), Baidu Encyclopedia (20G), and a novel (90G). The dimensions of the character vector and word vector are 200 and 64, respectively. In the experime"
I17-4035,P16-2076,0,0.0200662,"nnan University Kunming, P.R. China Contact:xjzhang@ynu.edu.cn Abstract determining whether the answer of the candidate is correct or not. In the recent research field of question answering, various methods have proved to be highly useful. The difference between the existing methods is mainly reflected in the access to the knowledge and reasoning framework. Clark et al. (2013) proposed a method based on text statistical rules. Clark (2015) described how to obtain more information from the background knowledge base, i.e., they introduced the use of background knowledge to build the best scene. Sachan et al. (2016) presented a unified max-margin framework that learns to detect the hidden structures that explain the correctness of an answer when provided with the question and instructional materials. A system that extracts information from the corpus for automatic generation of test questions was designed by Khot et al. (2015), whereas a structured inference system based on integer linear programming was proposed by Khashabi et al. (2016). A more complex method is presented in (Clark et al., 2016). This model operates at three levels of representation and reasoning: information retrieval, corpus statisti"
M93-1015,C92-2099,0,0.0640773,"Missing"
M93-1015,J91-4003,1,0.853549,"count of the tokens in the initial corpora wa s used to highlight those words which should be targeted, essentially determining what core vocabular y elements are of rnost importance . In general, the lexical structures used by the system can be though t of as providing for the shallowest possible semantic decomposition while still capturing significan t generalizations about how words relate conceptually to one another . 168 Deriving the Lexicon from Machine-Readable Resource s The lexical knowledge base consists of lexical items called generative lexical structures (GLSs), afte r Pustejovsky[9] . This model of semantic knowledge associated with words is based on a system o f generative devices which is able to recursively define new word senses for lexical items in the language . For this reason, the algorithm and associated dictionary is called a generative lexicon . The lexical structures contain conventional syntactic and morphological information along with detailed typin g information about arguments The creation of the GLS lexicon begins with the printer 's tape of the Longman Dictionary of Contemporary English (LDOCE), Proctor[8] . This was parsed and analysed by the CRL lexi"
M93-1015,H92-1047,1,0.861818,"Missing"
M93-1015,J93-2005,1,0.781633,"rmation about arguments The creation of the GLS lexicon begins with the printer 's tape of the Longman Dictionary of Contemporary English (LDOCE), Proctor[8] . This was parsed and analysed by the CRL lexical grou p to give a tractable formatted version of LDOCE called LEXBASE[5] . LEXBASE contains syntacti c codes, inflectional variants, and boxcodes, selectional information for verbs and nouns, indicatin g generally what kind of arguments are well-formed with that lexical item . A GLS entry is automaticall y derived from LEXBASE by parsing the LEXBASE format for specific semantic information [11] . Th e most novel aspect of this conversion involves parsing the example sentences as well as parenthetical texts in the definition . This gives a much better indication of argument selection for an item than d o the the boxcodes alone . For example, the verb market is converted into the following GLS entry as a result of this initial mapping . gls(market , syn([type(v) , code (gcode_t1) , eventstr([]) , ldoce_id(market_1_1) , caseinfo([subcatl(A1) , subcat2(A2) , case(A1,np) , case(A2,np)]) , inflection([ing(marketing) , pastp(marketed) , plpast(marketed) , singpastl(marketed) , singpast2(ma"
M93-1015,P92-1022,0,0.0125926,"basic transitive) . That is, the cospec encodes explicit information regarding the linear positioning of arguments, as well as semantic constraints on the arguments as imposed by the typing information i n the qualia . The syntactic representation of a word's environment may appear flat, but the semanti c interpretation is based on a unification-like algorithm which creates a much richer functional structure . Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) . Lexically Encoding Idiomatic and Phrasal Structure s One of the advantages to the highly lexical approach being taken here is the ability to encode idiomati c expressions and phrasal expressions as part of the lexicon proper, where motivated by statistical confir mation of the collocations . For example, in the English JV corpus, it so happens that reporting verb s such as announce very often appear with tensed clause complements carrying pronominal subjects, a s below : IBM ; announced that it; had e"
N16-1066,esuli-sebastiani-2006-sentiwordnet,0,0.00308504,"h, users proactively provide their feelings and opinions after browsing the web content. For example, users may read a news article and then offer comments. A user can also review the products available for sale in online stores. In the manual annotation method, trained annotators are asked to create affective annotations for specific language resources for research purposes. Several well-known affective resources are introduced as follows. SentiWordNet is a lexical resource for opinion mining, which assigns to each synset of WordNet three sentiment ratings: positive, negative, and objective (Esuli and Sebastiani, 2006). Linguistic Inquiry and Word Count (LIWC) calculates the degree to which people use different categories of words across a broad range of texts (Pennebaker et al., 2007). In the LIWC 2007 version, the annotators were asked to note their emotions and thoughts about personally relevant topics. The Affective Norms for English Words (ANEW) provides 1,034 English words with ratings in the dimensions of pleasure, arousal and dominance (Bradley and Lang, 1999). In addition to these English-language sentiment lexicons, a few Chinese lexicons have been constructed. The Chinese LIWC (C-LIWC) dictionary"
N16-1066,P14-1147,0,0.00719352,"fective states are generally represented using either categorical or dimensional approaches. The categorical approach represents affective states as several discrete classes such as positive, neutral, negative, and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various practical applications have been developed such as aspect-based sentiment analysis (Schouten and Frasincar, 2016; Pontiki et al., 2015), Twitter sentiment analysis (Saif et al., 2013; Rosenthal et al., 2015), deceptive opinion spam detection (Li et al., 2014), and cross-lingual portability (Banea et al., 2013; Xu et al., 2015). The dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement and calm. Based on this representation, any affective state can be represented as a point in the VA coordinate plane. For many application domains (e.g., product reviews, political stance detect"
N16-1066,W02-1011,0,0.044171,"WC with manual revisions to fit the practical characteristics of Chinese usages (Huang et al., 2012). The NTU Sentiment dictionary (NTUSD) has adopted a combination of manual and automatic methods to include positive and negative emotional words (Ku and Chen, 2007). Among the above affective lexicons, only ANEW is dimensional, providing realvalued scores for three dimensions, and the others are categorical, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the proc"
N16-1066,S15-2082,0,0.0179115,"Missing"
N16-1066,J11-2001,0,0.00435175,"Missing"
N16-1066,P15-2129,1,0.693208,"l, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the process of building Chinese affective resources with valence-arousal ratings, including the CVAW and CAVT. The CVAW is built on the Chinese affective lexicon C-LIWC, and then annotated with VA ratings for each word. Five annotators were trained to rate each word in the valence and arousal dimensions using the Self Assessment Manikin (SAM) model (Lang, 1980). The SAM model provides affective pictures, which c"
P15-2129,W10-0208,0,0.0893279,"ications rely on a handcrafted lexicon ANEW (Af788 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 788–793, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure space. 1. Two-dimensional valence-arousal fective Norms for English Words) (Bradley, 1999) which provides 1,034 English words with ratings in the dimensions of pleasure, arousal and dominance to predict the VA ratings of short and long texts (Paltoglou et al, 2013; Kim et al., 2010). Accordingly, the automatic prediction of VA ratings of affective words is a critical task in building a VA lexicon. Few studies have sought to predict the VA rating of words using regression-based methods (Wei et al., 2011; Malandrakis et al., 2011). This kind of method usually starts from a set of words with labeled VA ratings (called seeds). The VA rating of an unseen word is then estimated from semantically similar seeds. For instance, Wei et al. (2011) trained a linear regression model for each seed cluster, and then predicted the VA rating of an unseen word using the model of the cluste"
P15-2129,P14-1147,0,0.0242538,"(Pang and Lee, 2008; Calvo and D&apos;Mello, 2010; Liu, 2012; Feldman, 2013). In sentiment analysis, representation of affective states is an essential issue and can be generally divided into categorical and dimensional approaches. The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It re"
P15-2129,P07-1054,0,0.0309585,"Missing"
P15-2129,P12-1036,0,0.0195183,"o, 2010; Liu, 2012; Feldman, 2013). In sentiment analysis, representation of affective states is an essential issue and can be generally divided into categorical and dimensional approaches. The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numer"
P15-2129,N13-1123,0,0.00520985,"The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive an"
P15-2129,E09-1077,0,0.0151091,"d seeds). The VA rating of an unseen word is then estimated from semantically similar seeds. For instance, Wei et al. (2011) trained a linear regression model for each seed cluster, and then predicted the VA rating of an unseen word using the model of the cluster to which the unseen word belongs. Malandrakis et al. (2011) used a kernel function to combine the similarity between seeds and unseen words into a linear regression model. Instead of estimating VA ratings of words, another direction is to determine the polarity (i.e., positive and negative) of words by applying the label propagation (Rao and Ravichandran, 2009; Hassan et al., 2011) and pagerank (Esuli et al., 2007) on a graph. Based on these methods, the polarity of an unseen word can be determined/ranked through its neighbor nodes (seeds). Although the pagerank algorithm has been used for polarity ranking, it can still be extended for VA prediction. Therefore, this study extends the idea of pagerank in two aspects. First, we implement pagerank for VA prediction by transforming ranking scores into VA ratings. Second, whereas pagerank assigns an equal weight to the edges connected between an unseen word and its neighbor nodes, we consider their simi"
P15-2129,J11-2001,0,0.0372614,"s, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on such a twodimensional representation, a common research goal is to determine the"
P15-2129,D14-1126,0,0.0051841,"ased on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on such a twodimensional representation, a common research goal is to determine the degrees of valence and arousal of given texts such that a"
P15-2129,P11-2104,0,\N,Missing
P16-2037,D14-1080,0,0.0106674,"ds. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors fo"
P16-2037,D14-1181,0,0.00642009,") used a weighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two"
P16-2037,P14-1062,0,0.0710745,"ighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM"
P16-2037,D15-1168,0,0.0396127,"thod to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors for vocabulary words using word embedding. The regional CNN is then used"
P16-2037,D13-1170,0,0.00315544,"raining phase, a back propagation (BP) algorithm with stochastic gradient descent (SGD) is used to learn model parameters. Details of the BP algorithm can be found in (LeCun et al., 2012). 3 Experiments r 0.406 0.418 0.476 0.468 0.645 0.493 0.641 0.781* r 0.268 0.263 0.286 0.289 0.453 0.290 0.472 0.557* * Regional CNN-LSTM vs LSTM significantly different (p&lt;0.05) This section evaluates the performance of the proposed regional CNN-LSTM model against lexicon-based, regression-based, and NN-based methods. Datasets. This experiment used two affective corpora. i) Stanford Sentiment Treebank (SST) (Socher et al., 2013) contains 8,544 training texts, 2,210 test texts, and 1,101 validation texts. Each text was rated with a single dimension (valence) in the range of (0, 1). ii) Chinese ValenceArousal Texts (CVAT) (Yu et al., 2016) consists of 2,009 texts collected from social forums, manually rated with both valence and arousal dimensions in the range of (1, 9) using the SAM annotation scheme (Bradley et al. 1994). The word vectors for English and Chinese were respectively trained using the Google News and Chinese wiki dumps (zhwiki) datasets. The dimensionality for both word vectors are 300. Experimental Sett"
P16-2037,P15-1130,0,0.0223323,"a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors for vocabulary words using word embedding. The regiona"
P16-2037,P15-2129,1,0.286062,"for Computational Linguistics, pages 225–230, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics … LSTM … L …… Sequential layer …… w1ri r Region ri w2i (xri) Max pooling layer Convolutional layer Word vector wIri Text vector r r L …… LSTM … (xrj) … w1 j r Region rj w2j wJj Text vector LSTM L …… …… … (x ) … w1rk Region rk w2rk rk wKrk Linear decoder V A Figure 1: System architecture of the proposed regional CNN-LSTM model. Research on dimensional sentiment analysis has addressed VA recognition at both the wordlevel (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015) and the sentence-level (Paltoglou et al., 2013; Malandrakis et al., 2013). At the word-level, Wei et al. (2011) used linear regression to transfer VA ratings from English affective words to Chinese words. Malandrakis et al. (2011) used a kernel function to combine the similarity between words for VA prediction. Yu et al. (2015) used a weighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using"
P16-2037,N16-1066,1,0.332709,"0.476 0.468 0.645 0.493 0.641 0.781* r 0.268 0.263 0.286 0.289 0.453 0.290 0.472 0.557* * Regional CNN-LSTM vs LSTM significantly different (p&lt;0.05) This section evaluates the performance of the proposed regional CNN-LSTM model against lexicon-based, regression-based, and NN-based methods. Datasets. This experiment used two affective corpora. i) Stanford Sentiment Treebank (SST) (Socher et al., 2013) contains 8,544 training texts, 2,210 test texts, and 1,101 validation texts. Each text was rated with a single dimension (valence) in the range of (0, 1). ii) Chinese ValenceArousal Texts (CVAT) (Yu et al., 2016) consists of 2,009 texts collected from social forums, manually rated with both valence and arousal dimensions in the range of (1, 9) using the SAM annotation scheme (Bradley et al. 1994). The word vectors for English and Chinese were respectively trained using the Google News and Chinese wiki dumps (zhwiki) datasets. The dimensionality for both word vectors are 300. Experimental Settings. Two lexicon-based methods were used for comparison: weighted arithmetic mean (wAM) and weighted geometric mean (wGM) (Paltoglou et al., 2013), along with two regression-based methods: average values regressi"
P98-2205,C96-2157,1,0.760378,"where Sa is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; l is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the 4Kefer to (Euhn 1958) (Paice 1990) (l~u, Brandow & Mitze 1994) (Kupiec, Pedersen & Chen 1995) for sentence-based summarization approaches. SThe weights w~ are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski & Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in contex~ of an information retrieval system. Both the French and Iranian governments acknowledged the Iranian role in the release ot"" the three French hostages, Jean-Paul Kauffmann, Marcel Carton and Marcel Fontaine. 1. Words and phrases frequergly occurring in a tex~ are likely to be indicative of its content, especially if such words or phrases do not occur olden elsewhere in the database. A weighted frequency score, similar to tf~df used in automatic tex~"
P98-2205,C94-1056,0,\N,Missing
S17-2134,S17-2088,0,0.0749437,"Missing"
S17-2134,P14-1062,0,0.0273292,"ns. Subtask A involves message polarity classification, which requires a system to classify 1 whether a message is of positive, negative, or neutral sentiment. Subtasks B and C involve topicbased message polarity classification, which require a system to classify a message on two- and five-point scales toward a certain topic. Various approaches have been proposed to analyze sentiment of text, and deep neural network has achieved state-of-the-art results in recent years. Proven successful text classification methods include convolutional neural networks (CNN) (LeCun et al., 1990; Y. Kim, 2014; Kalchbrenner et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter et al, 1997; Tai et al., 2015). In general, CNN applies a convolutional layer to extract active local n-gram features, but lost the order of words. By contrast, LSTM can sequentially model texts. However, it focuses only on past information and draws conclusions from the tail part of texts. It fails to capture the local response from temporal data. In this paper, we propose a multi-channel CNNLSTM model for sentiment classification. It consists of two parts: multi-channel CNN, and LSTM. Unlike a conventional CNN model, we apply a multi-channel str"
S17-2134,D14-1181,0,0.00795942,"assifications. Subtask A involves message polarity classification, which requires a system to classify 1 whether a message is of positive, negative, or neutral sentiment. Subtasks B and C involve topicbased message polarity classification, which require a system to classify a message on two- and five-point scales toward a certain topic. Various approaches have been proposed to analyze sentiment of text, and deep neural network has achieved state-of-the-art results in recent years. Proven successful text classification methods include convolutional neural networks (CNN) (LeCun et al., 1990; Y. Kim, 2014; Kalchbrenner et al., 2014) and Long Short-Term Memory (LSTM) (Hochreiter et al, 1997; Tai et al., 2015). In general, CNN applies a convolutional layer to extract active local n-gram features, but lost the order of words. By contrast, LSTM can sequentially model texts. However, it focuses only on past information and draws conclusions from the tail part of texts. It fails to capture the local response from temporal data. In this paper, we propose a multi-channel CNNLSTM model for sentiment classification. It consists of two parts: multi-channel CNN, and LSTM. Unlike a conventional CNN model,"
S17-2134,P15-1150,0,0.1657,"Missing"
S18-1040,D16-1046,0,0.0326199,"and d denotes the dimension of a word vector. The word tokens are then directly fed into the model embedding layer, which was initialized by the pre-trained word embeddings. Domain Adaptation. Domain adaptation enhances learning in target domains by transferring learning from source domains that may have a distribution different from the target domain. Domain adaptation not only addresses the difference between source and target domains, but also pays attention to the relevance of both domains. The method provides an elegant way to access the full resources of similar tasks for target tasks (Mou et al., 2016). BiLSTM Layer. LSTM replaces the nodes of a regular RNN model with special structures (cells). The architecture of the LSTM is shown in Figure 2. It calculates the hidden state ht at time t using the following equations: Ensemble Learning. Ensemble learning is a supervised learning algorithm that ensembles two or more weak learners to amplify system performance (Maclin and Opitz, 1999). The AdaBoost algorithm (Li et al., 2008) is one of the ensemble learning algorithms that repeats training and adjusts the weights of all weak learners continuously to take into consideration the previous itera"
S18-1040,W07-1417,0,0.0442659,"Sentiment analysis is an area of natural language processing (NLP), which aims to systematically identify and study affective state, and to quantify subjective sentiment expressed in texts. Tweets in Twitter always constitute a challenging task among NLP problems because of the colorful writing styles used. In previous work on sentiment analysis tasks, researchers usually used a variety of hand-crafted features and sentiment lexicons to generate the solution system by combining traditional methods such as naive Bayes, support vector machines (SVMs) (Mohammad et al., 2013), and decision trees (Blake, 2007). Recently, many ensemble • An ensemble learning method using the AdaBoost algorithm implemented on the base model is of great use for performing the task with unevenly distributed data. The remainder of this paper is organized as follows. In Section 2, we describe an overview of our system. The details of the model are presented in Section 3. Finally, comparative results of the experiments are discussed, and a conclusion is drawn in Sections 4 and 5, respectively. 273 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 273–278 New Orleans, Louisiana, Ju"
S18-1040,N13-4001,0,0.0482097,"Missing"
S18-1040,W17-5227,1,0.799855,"e SIM with thresholds according to the training sets for V-reg and V-oc. 4.3 Results. On the competition leaderboard, our system placed 22/48 (9/24) for English (Spanish) in task EI-reg, 12/39 (8/16) in task EI-oc, 27/38 (7/14) in task V-reg, 14/31 (6/14) in task V-oc and 7/35 (6/14) in task E-c. Training and Hyper-parameters Experiments and Analysis. We trained our models on the training set and evaluated the prediction with the golden scores of the development set. In order to illustrate the good performance of our methods, we compare the results with baseline models of CNN, LSTM, CNN-LSTM (Zhang et al., 2017) and a regular BiLSTM. From the results shown in Table 1, we can see that our approach achieved a significant result. A regular LSTM tends to ignore future contextual information while processing sequences in a time series. The BiLSTM is able to use both past and future contexts by processing the text from both directions. Not all words make the same contribution to sentiment analysis in the text. The attention mechanism is able to shuffle the word annotation weights according to their importance to the meaning of sentence. We can see that the attention based BiLSTM obtained higher scores than"
S18-1101,K16-1017,0,0.120047,"irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, and iv) non-irony (Cynthia Van Hee and Hoste, 2018). For a more detailed description, please see Carman et al. (2017). In recent years, deep learning techniques have significantly outperformed traditional methods in several natural language processing (NLP) tasks (Cliche, 2017). In such task, several deep learning architecture-based methods have achieved outstanding performance in irony and sarcasm detection in social media. Silvio (Amir et al., 2016) presented a novel convolutional network-based method for learning user embeddings from their previous posts and used the user embeddings with lexical signals to recognize sarcasm. Ghosh and Veale (2016) proposed a combined convolutional neural network (CNN) model and long short-term memory (LSTM) method followed by a deep neural network (DNN), which also achieved an improvement compared to traditional machine learning approaches such as support vector machines (SVM).In this paper, we propose an ensemble of multiple deep learning models with a voting classifier in order to enhance the performa"
S18-1101,S17-2094,0,0.0200204,"must predict whether or not a tweet is ironic. The subtask B is a multiclass classification task where the system has to predict one out of four labels describing i) verbal irony realized through a polarity contrast, ii) verbal irony without such a polarity contrast (i.e., other verbal irony), iii) descriptions of situational irony, and iv) non-irony (Cynthia Van Hee and Hoste, 2018). For a more detailed description, please see Carman et al. (2017). In recent years, deep learning techniques have significantly outperformed traditional methods in several natural language processing (NLP) tasks (Cliche, 2017). In such task, several deep learning architecture-based methods have achieved outstanding performance in irony and sarcasm detection in social media. Silvio (Amir et al., 2016) presented a novel convolutional network-based method for learning user embeddings from their previous posts and used the user embeddings with lexical signals to recognize sarcasm. Ghosh and Veale (2016) proposed a combined convolutional neural network (CNN) model and long short-term memory (LSTM) method followed by a deep neural network (DNN), which also achieved an improvement compared to traditional machine learning"
S18-1101,S18-1005,0,0.0301888,"Missing"
S18-1101,W16-0425,0,0.0184556,"nd Hoste, 2018). For a more detailed description, please see Carman et al. (2017). In recent years, deep learning techniques have significantly outperformed traditional methods in several natural language processing (NLP) tasks (Cliche, 2017). In such task, several deep learning architecture-based methods have achieved outstanding performance in irony and sarcasm detection in social media. Silvio (Amir et al., 2016) presented a novel convolutional network-based method for learning user embeddings from their previous posts and used the user embeddings with lexical signals to recognize sarcasm. Ghosh and Veale (2016) proposed a combined convolutional neural network (CNN) model and long short-term memory (LSTM) method followed by a deep neural network (DNN), which also achieved an improvement compared to traditional machine learning approaches such as support vector machines (SVM).In this paper, we propose an ensemble of multiple deep learning models with a voting classifier in order to enhance the performance of individual neural network models for to detecting the ironic tweets. We trained six individual classifiers, including LSTMs, bi-directional LSTMs, gated recurrent units (GRUs), bi-directional GRUs"
S18-1101,D14-1162,0,0.0879012,"not replace #irony, #sarcasm and #not with word irony for subtask A because it is easy for overfitting to occur while training. We consider that the reason for this is that the searching and labeling of these tweets mostly dependent on their hashtags. In the four-category subtask B, this does not lead to over-fitting, but aids in improving accuracy. Preprocessing Before feeding the tweets to any classifier, they are pre-processed by following procedure: 3.3 Word embedding We obtain word embeddings by training with the corpus of English articles in Wikipedia pages using Global Vector (GloVe) (Pennington et al., 2014). Compared to Word2vec (Mikolov et al., 2013), GloVe achieves superior performance in this task under the same conditions. Moreover, we set the dimension of a single word as 300. Following the above steps, we create a look-up table that allows for most of the words in the training dataset to correspond to word vectors trained in advance, with the dataset containing 9056 unique words. However, 1266 words remain that cannot be matched, with most of these be-ing numbers and certain user-created words. • All uppercase letters are converted to lowercase. • URLs are replaced by <url&gt;; instance of @s"
S18-1177,D15-1168,0,0.0245898,"etter result than other models we used in most cases (Wang et al., 2016). To exclude the experimental error caused by chance, nine such models are assembled together for training. The final accuracy can be raised to 0.714. Table 1 presents the results of a comparative experiment for all models we used. The choice of model parameters has a significant effect on the final accuracy. The main parameters of this model are the word-embedding dimension, the batch size, the epoch, the filter size, the kernel size, the dropout and so on. To get the optimal parameters, the Sklearn grid search function (Liu et al., 2015) is used to determine the best combination of the parameters. Table 2 lists the parameters of the model when the best result is obtained. 3.2 Evaluation Evaluation Metrics. For this experiment, it measures how well a system is capable of correctly anResults. According to the final results provided by the organizers, a total of 199 teams enrolled in the competition. Only 24 teams eventually submitted their results. Our team ranked 13th overall among all teams. As shown in Table 1, the attentionbased CNN-LSTM model can achieve the highest accuracy when using Word2Vec as the wordembedding layer."
S18-1177,D14-1162,0,0.0810358,"dded validation data to the training set to expand the training data. We also tried sorting the training data randomly to expand the data set, but the result was not satisfactory. All input data is converted into word vectors through the word-embedding layer, and the word-embedding model is Word2Vec. Here, all the punctuation is ignored, and all non-English characters are treated as unknown words. In the word vectors, unknown word vectors are randomly generated from a uniform distribution U (-0.25, 0.25). Two different methods of word-embedding are used in this experiment: Word2Vec and GloVe (Pennington et al., 2014). They are used to initialize the weights of the embedding layer in building 300-dimension word vectors for all the texts and question-answer pairs. Word2Vec achieved better performance than GloVe in every model we used. Through the list of unknown words, we know that the use of Word2Vec results in fewer unknown words than GloVe. Implementation Details. All the code involved in this experiment was written in Python 3.5.2. Keras 2.0.4 is used as the framework for the program. The backend used in this experiment is Ten1060 Parameters Filter size Kernel size Dropout rate Epoch Batch size Word emb"
S18-1177,D16-1264,0,0.0506668,"answers. In recent years, many achievements have been made in machine comprehension-based question answering. Among the existing methods, the main differences are in the data processing and the application of the model. A dataset for multi-choice question answering was released by Richardson et al. (2013). Clark (2015) described how to obtain more information from the background knowledge base by introducing the use of background knowledge to build the best scene. A large cloze-style dataset using CNN and Daily Mail news articles was created by Hermann et al. (2015). Unlike previous datasets, Rajpurkar et al. (2016) released a machine comprehension-based dataset (SQuAD dataset). It contains over 1M text-question-answer triples crawled from 536 Wikipedia articles, and the questions and answers are structured primarily through crowdsourcing. It also requires people to submit up to five article-based questions and provide the correct answer that has appeared in the original text. For the open-domain QA dataset, it is even more challenging to get answers because it requires simple word matching and some simple reasoning. In SearchQA (Dunn et al., 2017), the question-answer pairs are crawled from the Jeopardy"
S18-1177,D13-1020,0,0.10658,"answer to many questions does not appear directly in the text, but requires simple reasoning to achieve. In terms of the nature of the problem, this task can be considered as a binary classification. That is, for each question, the candidate answers are divided into two categories: the correct answers and the wrong answers. In recent years, many achievements have been made in machine comprehension-based question answering. Among the existing methods, the main differences are in the data processing and the application of the model. A dataset for multi-choice question answering was released by Richardson et al. (2013). Clark (2015) described how to obtain more information from the background knowledge base by introducing the use of background knowledge to build the best scene. A large cloze-style dataset using CNN and Daily Mail news articles was created by Hermann et al. (2015). Unlike previous datasets, Rajpurkar et al. (2016) released a machine comprehension-based dataset (SQuAD dataset). It contains over 1M text-question-answer triples crawled from 536 Wikipedia articles, and the questions and answers are structured primarily through crowdsourcing. It also requires people to submit up to five article-b"
S18-1177,P16-2037,1,0.840877,"Word2Vec and GloVe as the word-embedding layer. However, the results obtained by the LSTM model have been somewhat improved over the CNN model. Next, we also apply the BiLSTM model and the best result is 0.654, but there are still many points that can be improved. Combining the two models effectively seems to be the perfect choice. In this way, we achieve an accuracy of 0.687. Finally, after adding the attention mechanism, the result is raised to 0.699. Under the same experimental conditions, the attention-based CNN-LSTM model obtained a better result than other models we used in most cases (Wang et al., 2016). To exclude the experimental error caused by chance, nine such models are assembled together for training. The final accuracy can be raised to 0.714. Table 1 presents the results of a comparative experiment for all models we used. The choice of model parameters has a significant effect on the final accuracy. The main parameters of this model are the word-embedding dimension, the batch size, the epoch, the filter size, the kernel size, the dropout and so on. To get the optimal parameters, the Sklearn grid search function (Liu et al., 2015) is used to determine the best combination of the param"
S18-1177,I17-4035,1,0.853807,"or this task. By adding an attention mechanism and combining the two models, the experimental results have been significantly improved. The accuracy of our final submission is 0.7143. 1 Introduction Question answering has long been an important research topic in the field of natural language processing. Prior to this, there have been many similar tasks, and many scholars have made very significant contributions to the research in this field. Such as the Allen AI Science Challenge on the Kaggle (Schoenick et al., 2016) and the IJCNLP2017 shared task 5: Multi-choice Question Answering in Exams (Yuan et al., 2017). Machine comprehension using commonsense knowledge is required to answer multiple-choice questions based on narrative texts about daily activities of human beings. The answer to many questions does not appear directly in the text, but requires simple reasoning to achieve. In terms of the nature of the problem, this task can be considered as a binary classification. That is, for each question, the candidate answers are divided into two categories: the correct answers and the wrong answers. In recent years, many achievements have been made in machine comprehension-based question answering. Amon"
S18-1187,W16-4920,1,0.788944,"ementation details. The third part is to show and analyze the results. 3.1 Dataset The training corpus of the word vector, the training set of the model and the test set must be selected and processed. As mentioned above, reading comprehension focuses more on semantic understanding (Tang et al., 2014), so GoogleNews is a good choice. Because news reports use more cautious words and more rigorous grammar. The mainstream word vector training tools are Word2Vec or GloVe. According to previous experimental results of related tasks, Word2Vec trained vector of words significantly better than GloVe (Yang et al., 2016). Thus, in this experiment, Word2vec was chosen to train the word vector. The form of the task data is complicated. It is more difficult to obtain the data by artificial generation or online acquisition. Thus, the training data and test data are given by the official data set. Each row of the test data set is divided into several sections, including the id, topic, additional information, reason, claim, warrant0, warrant1 and label. For each row of data, it is processed into two test data of the model. Each training data contains four parts. They are fact, warrant, claim and label. Here, fact i"
S18-1187,N18-1175,0,0.0218203,"bedding (Word Embedding Dimension) and the epochs of the training (Training Epoch). Due to the lack of training data, when there are more parameters of the model, it is easy to cause over-fitting. There are two improvements to avoid over-fitting. The first is dropout. Dropout is a classic way to avoid over-fitting. A dropout layer is added behind each Bi-LSTM. Thus, the model has one more hyper-parameter, which is the probability of dropout (Dropout Probability). The second method is ensemble learning. Because of the implicit relationship between claim and reason, this task is very difficult (Habernal et al., 2018). To express all of the features of the 1111 Parameter Bi-LSTM Unit Number Word Embedding Dimension Training Epoch Dropout Probability Ensemble Model Number Pre-set Values 64, 96, 128 200, 300 5, 8 0.3, 0.4, 0.5 5, 7 , 9 , 11 Ensemble Model Number 6 7 9 11 Acc 0.6646 0.6741 0.6803 0.6772 Table 5: Results of Ensemble Learning. Table 2: Pre-set Parameter. Model LSTM Bi-LSTM w/o ATT Bi-LSTM w/ ATT Table 3: Model. Acc 0.5126 0.5253 0.5696 Results of Proposed Model and Baseline input, a sufficiently complex model is required. However, too little training data is not sufficient for the model to lear"
S18-1187,P14-1146,0,0.0103228,"eed. The output of the claim encoder is used twice during decoding. Its output is the attention mechanism to remind the model to focus on the valuable part of the claim. 3 Experiment The experiment contains three parts. The first part is the selection and preprocess of experimental data. The second part is the implementation details. The third part is to show and analyze the results. 3.1 Dataset The training corpus of the word vector, the training set of the model and the test set must be selected and processed. As mentioned above, reading comprehension focuses more on semantic understanding (Tang et al., 2014), so GoogleNews is a good choice. Because news reports use more cautious words and more rigorous grammar. The mainstream word vector training tools are Word2Vec or GloVe. According to previous experimental results of related tasks, Word2Vec trained vector of words significantly better than GloVe (Yang et al., 2016). Thus, in this experiment, Word2vec was chosen to train the word vector. The form of the task data is complicated. It is more difficult to obtain the data by artificial generation or online acquisition. Thus, the training data and test data are given by the official data set. Each r"
S19-2063,S17-2126,0,0.0693686,"Missing"
S19-2063,S19-2005,0,0.0302398,"d ’yesssss’ to ’yes’). • Use the Ekphrasis tool to segment texts. This tool is used to separate special emoticons (for example, convert ’:-(’ to ’unhappy face’, and ’:)’ to ’smiley face’), which is effective for the next step of expression processing. • Traverse the word segmentation and comparison table one by one, replacing logograms, abbreviations, and emojis (for example, ’ /’ to ’unhappy face’ and convert ’,’ to ’smiley face’,). Experiments and Results Datastes and Official Evaluation Metrics Datasets were provided by SemEval 2019 Task 3, EmoContext: Contextual Emotion Detection in Text (Chatterjee et al., 2019). The participants were asked to predict the emotions of a three-turn conversation. The task considered three emotion classes, namely happy, sad, and angry, along with an others category. The number of training and development sets was 30160 and 2755, respectively. The categories of the training and development sets are displayed in Table 1. Owing to the imbalance of the training set data, the official evaluation matrics is the micro-average F 1 -score. 3.2 3.3 Parameter Optimization In order to search the optimal parameters for each model, we used the Scikit-Learn toolkit to perform a grid se"
S19-2063,W14-4012,0,0.0195651,"Missing"
S19-2063,D18-1350,0,0.0312857,"System architecture. bility and is prone to over-fitting. Sampling verification may alleviate the problem of data imbalance (He and Garcia, 2008). In order to enable the model to learn the data characteristics of small samples, we use five-fold cross-validation to verify the model and test its robustness. The system architecture is illustrated in Figure 1. 2.1 combining two different word embedding models, we obtain eight different models. We use four deep learning models, namely LSTM (Hochreiter and Schmidhuber, 1997; Mikolov, 2010), GRU (Cho et al., 2014), Capsule-Net (Sabour et al., 2017; Zhao et al., 2018), and Self-Attention (Luong et al., 2015). We use Dropout (Salakhutdinov et al., 2014) to aid with improved model convergence. Finally, at each model output, we output the four predicted categories of probabilities, instead of the predicted results. Embedding We use a GloVe (Pennington et al., 2014) pretrained word vector: the Twitter 200-dimensional word vector. GloVe is a word representation tool based on the count base and overall statistics. It expresses a word as a vector of real numbers, and captures the semantic properties of words, such as similarity and analogy. Meanwhile, the ELMo al"
S19-2063,D15-1166,0,0.0413298,"Missing"
S19-2142,S17-2126,0,0.0411852,"Missing"
S19-2142,N19-1144,0,0.116096,"our different deep learning architectures. Finally, the fourth subsystem is a bidirectional encoder representations from transformers (BERT) model. Among our models, in Sub-task A, our first subsystem performed the best, ranking 16th among 103 teams; in Sub-task B, the second subsystem performed the best, ranking 12th among 75 teams; in Sub-task C, the fourth subsystem performed best, ranking 4th among 65 teams. 1 • Sub-task B: Automatic categorisation of offense types; • Sub-task C: Offense target identification. In this document, we present four systems that competed at SemEval-2019 Task 6 (Zampieri et al., 2019b). The first model is a 2-layer BiLSTM, equipped with an attention mechanism. The second is voting scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit (BiGRU), and the first model. The third model is a stacking scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit (BiGRU), and the first model. In addition, the above three models, for the word representation, we have used the glove vector. The fourth model is BERT-BASE (Jacob Devlin, 2018), which was released last year by Google AI Language. Introductio"
S19-2142,S19-2010,0,0.0726501,"our different deep learning architectures. Finally, the fourth subsystem is a bidirectional encoder representations from transformers (BERT) model. Among our models, in Sub-task A, our first subsystem performed the best, ranking 16th among 103 teams; in Sub-task B, the second subsystem performed the best, ranking 12th among 75 teams; in Sub-task C, the fourth subsystem performed best, ranking 4th among 65 teams. 1 • Sub-task B: Automatic categorisation of offense types; • Sub-task C: Offense target identification. In this document, we present four systems that competed at SemEval-2019 Task 6 (Zampieri et al., 2019b). The first model is a 2-layer BiLSTM, equipped with an attention mechanism. The second is voting scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit (BiGRU), and the first model. The third model is a stacking scheme that combines a 2-layer BiLSTM, Capsule Network, 2-layer bidirectional gated recurrent unit (BiGRU), and the first model. In addition, the above three models, for the word representation, we have used the glove vector. The fourth model is BERT-BASE (Jacob Devlin, 2018), which was released last year by Google AI Language. Introductio"
S19-2142,W17-3013,0,0.0318939,"Missing"
S19-2142,W18-4401,0,0.0973255,"Missing"
S19-2142,W18-3504,0,0.0204315,", and nonaggression (Kumar et al., 2018). Last year, in a shared task, several participants used deep neural networks and traditional machine learning methods for aggression identification. The best performing systems in this competition used deeplearning approaches based on convolutional neural networks (CNN), BiLSTM, and long short-term memory (LSTM). Offensive Language is commonly defined as hurtful, derogatory or obscene comments made from one person to another. Currently, there is an increasing amount of such language online. Manually monitoring these posts would incur significant costs (Mathur et al., 2018). Therefore, the automatic identification of suspicious posts has emerges as a trend. In recent years, many researchers have studied the use of deeplearning and traditional machine learning methods for this purpose. Their results indicate that, although several deep-learning approaches produce good scores, traditional supervised classifiers can produce similar scores. Word embeddings, character n-grams and lexicons of offensive words are popular features, but all three components are not necessary for a robust system. Ensemble methods mostly help (Wiegand et al., 2018). Many previous studies s"
S19-2142,N18-1202,0,0.0599106,"wo gates in the GRU model, namely the update gate and the reset gate) and backward GRU. For the three Sub-tasks, we used a 2-layer BiGRU. The parameters of our model were chosen to maximise development performance: in Subtasks A and B, we initialised the hidden dimension, recurrent dropout, and batch size as 120, 0.25, and 100, respectively; in Sub-task C, we initialised the hidden dimension, recurrent dropout, and batch size as 120, 0.25, and 128, respectively. • BERT: The BERT model is a language model proposed by Google based on a bidirectional transformer. It is quite different from ELMo (Peters et al., 2018). In existing pre-training models (including word2vec and ELMo), word vectors are generated. This type of pre-training model belongs to domain transfer. The GPT (Karthik Narasimhan and Sutskever, 2018), BERT, etc. proposed in recent years are all examples of model migration. Furthermore, the BERT model combines the pre-training model with the down• BiLSTM with attention: For this, an attention layer was added to the 2-layer BiLSTM. In BiLSTM, we used the output vector of the last time sequence as the feature vector and then performed softmax classification. The attention layer is used to first"
S19-2207,N16-1101,0,0.0804728,"Missing"
S19-2207,P15-1001,0,0.0463967,"d through one calculation step. Thus, the distance between long-distance dependent features is greatly shortened, which is conducive to the effective use of these features. Obviously, it is easier to capture the long-distance interdependent features in sentences after the introduction of Attention. In figure 4, self attention can be described as mapping a query and a set of key-value pairs to an output. The calculation of Attention is mainly divided into three steps. The first step calculates the similarity between query and each key to get the weight. The second step uses a softmax function (Jean et al., 2015) to normalize these weights. Finally, the weight and the corresponding key value are weighted and summed to get the final Attention. Currently, in NLP research, the key and value are always the same, that is, key=value. In this part, we use self-attention, which is denoted as key=value=query (Firat et al., 2016). Attention(Q, K) = n X Similarity(Q, Ki ) ∗ Vi (4) n=1 2.4 MLP Layer This layer is a fully connected layer that multiplies the results of the previous layer with a weight matrix and adds a bias vector. The ReLU (Jarrett et al., 2009) activation function is also applied in this layer. T"
S19-2224,S19-2151,0,0.094921,"Missing"
W16-4920,C14-1028,0,0.0196334,"r the wrong sentence. Besides identifying the error types, the position-level also judges the positions of erroneous range. Some of the previous works have sought to identify Chinese grammatical errors using templateand learning-based methods. Wu et al. (2010) proposed a combination of relative position and analytic template language model to detect Chinese errors written by American learners. Yu and Chen et al. (2012) used simplified Chinese corpus to study word ordering errors (W) in Chinese and proposed syntactic features, external corpus features and perturbation features for W detection. Cheng et al. (2014) detected and corrected word This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 155 Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages 155–161, Osaka, Japan, December 12 2016. ordering errors by using conditional random field (CRF) (Lafferty, 2010) and support vector machine (SVM) together with frequency learning from a large n-gram corpus. Zampieri et al. (2014) used frequent n-grams and news corpus as a reference corpus to detect errors in"
W16-4920,C12-1184,0,0.0490135,"Missing"
W17-5227,D14-1162,0,0.085218,"conditions for emotional intensity. Therefore, these emojis or emoticons are processed into related words with similar meanings. Patterns are applied to every tweet presented in Table 1. We applied the four patterns and lowed all words to map the known pre-trained tokens. Some words that do not exist in the known tokens are treated as unknown words. In the word vectors, unknown word vectors randomly generated from a uniform distribution U (−0.25, 0.25). In this experiment, we used pre-trained word vectors including GoogleNews1 trained by the word2vec toolkit and another one trained by GloVe2 (Pennington et al., 2014). These programs • Gate ft = σ(Wf · [ht−1 , xt ] + bf ) it = σ(Wi · [ht−1 , xt ] + bi ) ot = σ(Wo · [ht−1 , xt ] + bo ) (5) (4) Where xt is the input vector; Ct is the cell state vector; W and b are cell parameters; ft , it , and ot are gate vectors; and σ denotes the sigmoid function. Output Layer. This layer outputs the final regression result, which could be a CNN or CNN-LSTM model. It is a fully connected layer using a linear decoder. A layer output vector defined as, 1 2 202 https://code.google.com/archive/p/word2vec/ https://nlp.stanford.edu/projects/glove/ Model CNNword2vec CNN-LSTMword"
W17-5227,D14-1080,0,0.034548,"greater degree of emotion. In the relevant research field of sentiment analysis, it has been shown that many models are available for both categorical approaches and dimensional approaches. A categorical approach focuses on sentiment classification, while a dimensional approach aims to predict the intensity of emotions. Recently, many methods have been successfully introduced for categorical sentiment analysis, such as word embedding (Liu et al., 2015), convolutional neural networks (CNN) (Kim, 2014; Jiang et al., 2016; Ouyang et al., 2015), recurrent neural networks (RNN) (Liu et al., 2015; Irsoy and Cardie, 2014), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Li and Qian., 2016; Sainath et al., 2015), and bi-directional LSTM (BiLSTM) (Brueckner and Schulter, 2014). We have aimed to employ those methods for dimensional sentiment analysis, and the results show that our approach is feasible. In general, CNN can extract local n-gram features within texts but may fail to capture long-distance dependency. LSTM can address this problem by sequentially modeling texts cross messages (Wang et al., 2016). In this paper (and for this competition), we primarily introduce a CNN-LSTM model combini"
W17-5227,P16-2037,1,0.848598,"t al., 2016; Ouyang et al., 2015), recurrent neural networks (RNN) (Liu et al., 2015; Irsoy and Cardie, 2014), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Li and Qian., 2016; Sainath et al., 2015), and bi-directional LSTM (BiLSTM) (Brueckner and Schulter, 2014). We have aimed to employ those methods for dimensional sentiment analysis, and the results show that our approach is feasible. In general, CNN can extract local n-gram features within texts but may fail to capture long-distance dependency. LSTM can address this problem by sequentially modeling texts cross messages (Wang et al., 2016). In this paper (and for this competition), we primarily introduce a CNN-LSTM model combining CNN and LSTM. First, we construct word vectors from pre-trained word vectors using word embedding. The CNN applies convolutional and maxpooling layers, which are then used to extract ngram features. Finally, LSTM composes those features and outputs the result. By combining CNThe sentiment analysis in this task aims to indicate the sentiment intensity of the four emotions (e.g. anger, fear, joy, and sadness) expressed in tweets. Compared to the polarity classification, such intensity prediction can pro"
W17-5227,D14-1181,0,0.0225725,"resses an emotion including anger, fear, joy, and sadness. The tweets with higher scores correspond to a greater degree of emotion. In the relevant research field of sentiment analysis, it has been shown that many models are available for both categorical approaches and dimensional approaches. A categorical approach focuses on sentiment classification, while a dimensional approach aims to predict the intensity of emotions. Recently, many methods have been successfully introduced for categorical sentiment analysis, such as word embedding (Liu et al., 2015), convolutional neural networks (CNN) (Kim, 2014; Jiang et al., 2016; Ouyang et al., 2015), recurrent neural networks (RNN) (Liu et al., 2015; Irsoy and Cardie, 2014), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Li and Qian., 2016; Sainath et al., 2015), and bi-directional LSTM (BiLSTM) (Brueckner and Schulter, 2014). We have aimed to employ those methods for dimensional sentiment analysis, and the results show that our approach is feasible. In general, CNN can extract local n-gram features within texts but may fail to capture long-distance dependency. LSTM can address this problem by sequentially modeling texts cross m"
W17-5227,D15-1168,0,0.0247157,"tweets are divided into four datasets, each of which expresses an emotion including anger, fear, joy, and sadness. The tweets with higher scores correspond to a greater degree of emotion. In the relevant research field of sentiment analysis, it has been shown that many models are available for both categorical approaches and dimensional approaches. A categorical approach focuses on sentiment classification, while a dimensional approach aims to predict the intensity of emotions. Recently, many methods have been successfully introduced for categorical sentiment analysis, such as word embedding (Liu et al., 2015), convolutional neural networks (CNN) (Kim, 2014; Jiang et al., 2016; Ouyang et al., 2015), recurrent neural networks (RNN) (Liu et al., 2015; Irsoy and Cardie, 2014), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Li and Qian., 2016; Sainath et al., 2015), and bi-directional LSTM (BiLSTM) (Brueckner and Schulter, 2014). We have aimed to employ those methods for dimensional sentiment analysis, and the results show that our approach is feasible. In general, CNN can extract local n-gram features within texts but may fail to capture long-distance dependency. LSTM can address thi"
W17-5227,W17-5205,0,0.079861,"Missing"
X93-1021,C92-2099,0,0.0526046,"Missing"
X93-1021,J91-4003,1,0.918749,"ticular role of a tie-up (the newly formed venturein particular) are also recognized and resolved. For example, a phrase which refers to the child entity, such as &apos;the new company&apos; or &apos;the venture&apos;, will be recognized ann merged with the child of the tie-up event in focus. A stack of entities found in the text is maintained. f(time,[after,&apos;B50i&apos;],wj), f(entity_relationship,l,inf), f(entity_relationship,3,inf)]). final_entity(9, &apos;UNSPEC&apos;), f(entiCy_type,~COMPANY&apos;,&apos;UNSPEC&apos;), f(name,[~&apos;,&apos;~,&apos;,&apos;~&apos;,~Jz&apos;],&apos;UNSPEC&apos;), f(entity_relationship,l,inf), f(entity_relationship,3,inf)]). f i n a l _ r e l ( 1, [9,2], &apos;U N S P E C &apos;, &apos;P A R T N E R &apos;, &apos;U B S P E C &apos; ). Definite noun phrases can only be used for local reference. So they can only be used to refer to entities involved in the tie-up event which is in focus. On the contrary, names can be used for both local and global reference, so they can refer to any entity referred to before in the t e x t . When a reference relation between two entities is resolved they are merged to create one single entity which contains all the information about that particular entity. Since a tie-up is generally referenced by an entire sentence rather than a single no"
X93-1021,H92-1047,1,0.896541,"Missing"
X93-1021,J93-2005,1,0.842458,"Missing"
X93-1021,P92-1022,0,0.0224668,"Missing"
X93-1021,P85-1022,0,0.0572839,"Missing"
X96-1036,P91-1034,0,0.0276899,"Missing"
X96-1036,P95-1026,0,0.0171038,"Missing"
