2006.amta-papers.22,P05-1048,0,0.0243424,"the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because word senses occurring in the test data may never have occurred in similar context in the training data. In this paper we present CL-MT, a hybrid controlled language-statistical MT system where cross-domain SMT is improved by human guided WSD. This study is part of a larger research effort on utilizing machine translation technology to enhance human-human communication. A typical application scenario is on-the-fly automatic email translation, where two users that do not share th"
2006.amta-papers.22,W03-1001,0,0.0133099,"best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because wor"
2006.amta-papers.22,H05-1097,0,0.0173506,"e cases where CLRelated Research There have been several research papers recently on incorporating WSD into SMT. Carpuat and Wu conducted experiments using a WSD classifier for Chinese based on an ensemble of naïve Bayes, maximum entropy, AdaBoost, and Kernel PCAbased classifiers (Carpuat and Wu, 2005). These classifiers had a much richer feature set of contextual information than was available to the phrasal SMT system that Carpuat and Wu used. They found that BLEU scores declined when the WSD system was used to override the translation chosen by the SMT system. A research group at Stanford (Vickrey et al., 2005) applied automatic WSD where the word senses of an English word were taken to be its possible French translations. Their system succeeded in finding the correct translation in a “fill in the blank” experiment, but did not find significant improvements in translation accuracy of full sentences. The use of human-verified WSD has been explored by Translution.com (Orasan et al., 2005). Their method applies only to language pairs where both languages have EuroWordNet thesauri (www.illc.uva.nl/EuroWordNet). They use WordNet’s interlingual index to link word senses in the source language with corresp"
2006.amta-papers.22,W99-0905,0,0.0159966,"e of human-verified WSD has been explored by Translution.com (Orasan et al., 2005). Their method applies only to language pairs where both languages have EuroWordNet thesauri (www.illc.uva.nl/EuroWordNet). They use WordNet’s interlingual index to link word senses in the source language with corresponding senses in the target language. They reported on techniques to prune out irrelevant word senses to avoid overburdening a user, but did not report on how the WSD affected translation accuracy. A promising approach to building a CL lexicon without an MRD available is corpus-based cluster200 ing (Kikui, 1999). Kikui uses distributional clustering to identify the word sense of a source language word, and then tests each translation from a bilingual dictionary to find a translation whose context in the target language corpus best matches the context for that sense in the source language corpus. The controlled language of CL-MT is qualitatively different than that of other research in controlled language. Our CL lexicon is designed to be domain independent and must deal directly with ambiguity of nearly all terms. Other CL systems have been developed for narrow domains, or at best, with a domain-inde"
2006.amta-papers.22,P95-1026,0,0.193642,"Missing"
2006.amta-papers.22,W05-0821,1,0.884247,"Missing"
2006.amta-papers.22,N04-1033,0,0.0174891,"e is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because word senses occurring in"
2006.amta-papers.22,W05-0820,0,0.0205761,"Missing"
2006.amta-papers.22,N03-1017,0,0.00450801,"anslation model. The best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of er"
2006.amta-papers.22,koen-2004-pharaoh,0,0.0465878,"Missing"
2006.amta-papers.22,2003.eamt-1.10,0,0.0442136,"each translation from a bilingual dictionary to find a translation whose context in the target language corpus best matches the context for that sense in the source language corpus. The controlled language of CL-MT is qualitatively different than that of other research in controlled language. Our CL lexicon is designed to be domain independent and must deal directly with ambiguity of nearly all terms. Other CL systems have been developed for narrow domains, or at best, with a domain-independent architecture that relies on domain-specific knowledge. The Kant system (Nyberg and Mitamura, 1996; Mitamura et al., 2003) was developed primarily for one-way translation of Caterpillar Tractor manuals from English. Nearly all of the content words are restricted to a single word sense, and multi-word noun phrases are only allowed if explicitly in the lexicon. Kant would reject “oil filter change” even though “oil filter” and “change” are both in the lexicon (“change of oil filter” is permitted). Attempto Controlled English (ACE) (Fuchs et al. 1998) and Processable English (PENG) (Schwitter 2002) are similarly designed for technical specifications in narrow domains. 6 Conclusions and Future Work We have tested the"
2006.amta-papers.22,W99-0604,0,0.0230932,"ed to train the translation model. The best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more sign"
2007.mtsummit-papers.24,P95-1032,0,0.124968,"Missing"
2007.mtsummit-papers.24,P91-1023,0,0.0312785,"Missing"
2007.mtsummit-papers.24,P97-1063,0,0.0388859,"Missing"
2007.mtsummit-papers.24,P06-2095,0,0.0213313,"Missing"
2009.mtsummit-papers.15,2005.mtsummit-osmtw.3,0,0.072679,"Missing"
2009.mtsummit-papers.15,J93-2003,0,0.0230156,"Missing"
2009.mtsummit-papers.15,2006.amta-papers.3,0,0.0685921,"Missing"
2009.mtsummit-papers.15,J94-4003,0,0.0919634,"Missing"
2009.mtsummit-papers.15,N03-1017,0,0.00900449,"Missing"
2009.mtsummit-papers.15,P09-1030,1,0.834399,"age in PANLINGUAL T RANSLATOR’s dictionary, we found that users can encode sentences nearly as fast as they are able to type. The most obvious word in the source language is usually found in the target language in the intended sense. For language pairs where the coverage is spotty, such as English to Swahili, encoding can become a search for source words that will be translated reliably, adding several minutes to the encoding of each text. 2.3 Context-Free Lexical Translation Although the Lemmatic MT paradigm does not depend on a particular translation dictionary, we selected PAN D ICTIONARY (Mausam et al., 2009) as the basis for PANLINGUAL T RANSLATOR. PAN D ICTIONARY is a sense-distinguished translation dictionary compiled from Wiktionaries1 and more than 600 machine readable bilingual dictionaries. PAN D ICTIONARY uses probabilistic inference to find translations that are not in any of the source dictionaries. The dictionary is organized as a set of word senses s that each have a list of translations w in multiple languages wth an associated probability that w has sense s (denoted pr(w ∈ s)). PAN D ICTIONARY has 81,000 word senses with over 1.6 million translations with an estimated precision of 0."
2009.mtsummit-papers.15,H94-1093,0,0.18893,"Missing"
2009.mtsummit-papers.15,C92-2070,0,0.0563373,"Missing"
2020.nlpcovid19-acl.1,2020.acl-main.207,1,0.909878,"text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pretrained model weights BioBERT and SciBERT have been popular pretrained LMs for C OVID 19-related tasks. DeepSet has released a BERTbase model pretrained on CORD-19.21 SPECTER (Cohan et al., 2020) paper embeddings computed using paper titles and abstracts are being released with each CORD-19 update. SeVeN relation embeddings (Espinosa-Anke and Schockaert, 2018) between word pairs have also been made available for CORD-19.22 Knowledge graphs The Covid Graph project23 releases a C OVID -19 knowledge graph built from mining several public data sources, including 19 https://github.com/allenai/cord19 There are many Search and QA systems to survey. We have chosen to highlight the systems that were made publiclyavailable within a few weeks of the CORD-19 initial release. 21 https://huggingfac"
2020.nlpcovid19-acl.1,N19-1423,0,0.0147555,"l entities can be useful. NER and linking can be performed using NLP toolkits like ScispaCy (Neumann et al., 2019) or language models like BioBERT-base (Lee et al., 2019) and SciBERTbase (Beltagy et al., 2019) finetuned on biomedical NER datasets. Wang et al. (2020) augments CORD-19 full text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pretrained model weights BioBERT and SciBERT have been popular pretrained LMs for C OVID 19-related tasks. DeepSet has released a BERTbase model pretrained on CORD-19.21 SPECTER (Cohan et al., 2020) paper embeddings computed using paper titles and abstracts are being released with each CORD-19 update. SeVeN relation embeddings (Espinosa-Anke and Schockaert, 2018) between word pairs have also been made available for CORD-19.22 Knowledge graphs The Covid Graph project23 releases a C OVID -19 knowledge graph"
2020.nlpcovid19-acl.1,C18-1225,0,0.0454275,"Missing"
2020.nlpcovid19-acl.1,2020.acl-main.447,1,0.934558,"ed paper entry as canonical. If any metadata in the canonical entry are missing, values from other members of the cluster are promoted to fill in the blanks. Cluster filtering Some entries harvested from sources are not papers, and instead correspond to materials like tables of contents, indices, or informational documents. These entries are identified in an ad hoc manner and removed from the dataset. 2.3 Processing full text Most papers are associated with one or more PDFs.12 To extract full text and bibliographies from each PDF, we use the PDF parsing pipeline created for the S2ORC dataset (Lo et al., 2020).13 In (Lo et al., 2020), we introduce the S2ORC JSON format for representing scientific paper full text, 11 This is a conservative clustering policy in which any metadata conflict prohibits clustering. An alternative policy would be to cluster if any identifier matches, under which a, b, and c would form one cluster with identifiers (x, y, [z, z 0 ]). 12 PMC papers can have multiple associated PDFs per paper, separating the main text from supplementary materials. 13 One major difference in full text parsing for CORD-19 is that we do not use ScienceParse,14 as we always derive this metadata fr"
2020.nlpcovid19-acl.1,D18-1211,0,0.0563002,"Missing"
2020.nlpcovid19-acl.1,W19-5034,0,0.0403962,"D-19 have already been developed. Most combine elements of text-based information retrieval and extraction, as illustrated in Figure 3. We have compiled a list of these efforts on the CORD19 public GitHub repository19 and highlight some systems in Table 2.20 4.3 Text mining and NLP research The following is a summary of resources released by the NLP community on top of CORD-19 to support other research activities. Information extraction To support extractive systems, NER and entity linking of biomedical entities can be useful. NER and linking can be performed using NLP toolkits like ScispaCy (Neumann et al., 2019) or language models like BioBERT-base (Lee et al., 2019) and SciBERTbase (Beltagy et al., 2019) finetuned on biomedical NER datasets. Wang et al. (2020) augments CORD-19 full text with entity mentions predicted from several techniques, including weak supervision using the NLM’s Unified Medical Language System (UMLS) Metathesaurus (Bodenreider, 2004). Text classification Some efforts focus on extracting sentences or passages of interest. For example, Liang and Xie (2020) uses BERT (Devlin et al., 2019) to extract sentences from CORD-19 that contain C OVID -19-related radiological findings. Pret"
2020.nlpcovid19-acl.1,D19-1410,0,0.0687887,"Missing"
2020.nlpcovid19-acl.1,P19-1436,0,0.0588806,"Missing"
2020.nlpcovid19-acl.1,P18-4015,1,0.803645,"Missing"
2020.nlpcovid19-acl.1,D19-1371,1,\N,Missing
2020.nlpcovid19-acl.1,2020.nlpcovid19-acl.2,0,\N,Missing
2021.emnlp-main.141,2021.emnlp-main.167,0,0.029922,"meteor <extra_id_1> observatory. Drawer Encoding meteor destroying* an* observatory. [ICON1058] [X10] [Y5] [SCALE5] [ROT0] [MIRROR1] [ICON65] [X13] [Y10]……. Figure 3: Game state encoding for our models. For each encoding method, the upper text is the input and the lower text is the target output. et al., 2020) language model by treating the task as a text-to-text conditional generation task. Interestingly, we find vision-and-language (V+L) models (Tan and Bansal, 2019; Chen et al., 2020) to be less effective, which might be because current V+L models have inferior language-related abilities (Iki and Aizawa, 2021), or because models trained on photographic images are not well-suited to understand the non-literal imagery found in Iconary. 3.1 Guesser experiments found encoding positional information more precisely, or encoding earlier drawings if they exist, did not improve performance when using T5. Next, we append the text ‘phrase:’ and, for each word in the target phrase, either an underscore or the correct word if it is known (see Figure 3, top). We experimented with encoding previous incorrect guesses but found it unnecessary as long as models are prevented from repeating those guesses during gener"
2021.emnlp-main.141,2020.emnlp-main.707,1,0.768191,"ding scheme similar to the pre-training one is effective for O OD generalization. Unsurprisingly, many of these optimizations reduce I ND performance because they increase the usage O OV words, which never appear in the I ND dev sets. Table 7 shows the Drawer ablations. Our initialization strategy proves to be critical, which suggests it is what allows TD RAWER to leverage the T5 parameter initialization even though it does not output natural language. We also get a modest boost by training with the formatting constraints. 6 Related Work produce images from text has been studied for captions (Cho et al., 2020), image specifications (Reed et al., 2016), and dialogue (Sharma et al., 2018). Unlike in these works, the drawings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationale"
2021.emnlp-main.141,D18-1241,1,0.827931,"s (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the Guesser is seeking information from the Drawer. A similar format can be found in dialogue QA datasets (Reddy et al., 2019; Choi et al., 2018; Aliannejadi et al., 2019), and task-oriented dialogue in general similarly requires understanding the intent of a human communicator (Young et al., 2013; Chen et al., 2017). Iconary, however, makes this a multimodal process. There is a long history of using games as a testbed for AI. Traditionally these have been adversarial strategy games like Chess (Silver et al., 2018), Go (Silver et al., 2016), and many others (Moravcík et al., 2017; Vinyals et al., 2017; Mnih et al., 2013) A few cooperative games have been studied, like Codenames (Kim et al., 2019) or Hanabi (WaltonRivers et al., 2019),"
2021.emnlp-main.141,W18-0907,0,0.0264587,"visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Ico"
2021.emnlp-main.141,N19-1423,0,0.0114671,"), and dialogue (Sharma et al., 2018). Unlike in these works, the drawings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for"
2021.emnlp-main.141,2020.acl-main.703,0,0.0156602,"mer Win I ND Soft O OD Win Soft∗ 84.25 85.91 79.34 78.84 79.89 97.62 98.55 97.09 96.69 93.64 37.39 22.67 33.30 27.07 0.00 44.06 27.24 40.61 34.48 0.00 Table 4: Automatic evaluation metrics on the test sets for TG UESSER and our baselines. method of encoding the drawing, we compare the perplexity of each human drawing, averaged over all drawings per game, then averaged over all games in the corpus. 4.3 Baselines We use the following baselines: Identical TGuesser-Large/T5Drawer-Base: models but with smaller versions of T5. BART Guesser/Bart Drawer: Identical models with the BART language model (Lewis et al., 2020). For BART Guesser, we adapt the fill-in-the-blank encoding scheme to generate a copy of the input with the mask tokens replaced, instead of only generating the masked-out tokens, to match BART’s pre-training format. Transformer Guesser/Transformer Drawer: We train a transformer-based model (Vaswani et al., 2017) on this task that does not use a pre-trained language model. This model also encodes the drawings as a sequence of special tokens during both decoding and encoding, in which case we find it important to apply a data-augmentation strategy to help the model learn mappings between icons"
2021.emnlp-main.141,2020.emnlp-main.602,0,0.0202194,"t is the game phrase. During generation, we constrain models to ensure the output contains the right number of words, includes words that are known to be correct from previous guesses, and exclude words that are known to be incorrect. This is non-trivial for wordpiece models, but we leave details in the appendix. 3.2 Handling OOV Words We observe that naively trained models often generate words seen in the training data even when they do not match the drawing. To combat this, we propose several extensions to TG UESSER: Rare Word Boosting: Based on a method from controlled language generation (Ma et al., 2020; Ghosh et al., 2017), we boost the logit score of wordpieces not seen during training. In particular, we add a fixed value (chosen as a hyperparmeter), to the log-probabilities of those wordpieces and then re-apply the softmax operator to get updated word-piece probabilities during generation. Fill-in-the-Blank Encoding: Following the T5 pre-training format (Raffel et al., 2020), we encode the phrase using ‘extra_id’ tokens for sequences of unknown words instead of underscores and train the model to only predict the text that ought to replace those tokens. Figure 3 contains an example. We exp"
2021.emnlp-main.141,P16-1170,0,0.0268231,"awings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skill"
2021.emnlp-main.141,D14-1162,0,0.0839227,"Missing"
2021.emnlp-main.141,Q19-1016,0,0.0242719,"is includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the Guesser is seeking information from the Drawer. A similar format can be found in dialogue QA datasets (Reddy et al., 2019; Choi et al., 2018; Aliannejadi et al., 2019), and task-oriented dialogue in general similarly requires understanding the intent of a human communicator (Young et al., 2013; Chen et al., 2017). Iconary, however, makes this a multimodal process. There is a long history of using games as a testbed for AI. Traditionally these have been adversarial strategy games like Chess (Silver et al., 2018), Go (Silver et al., 2016), and many others (Moravcík et al., 2017; Vinyals et al., 2017; Mnih et al., 2013) A few cooperative games have been studied, like Codenames (Kim et al., 2019) or Hanabi (WaltonRi"
2021.emnlp-main.141,N16-1020,0,0.0288099,"sovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the"
2021.emnlp-main.141,P19-1644,0,0.0537855,"Missing"
2021.emnlp-main.141,D19-1514,0,0.0141506,"rvatory. Fill-in-the-Blank Encoding drawing: huge comet, 2 smoke, telescope, huge dome. phrase: <extra_id_0> destroying a <extra_id_1> <extra_id_0> meteor <extra_id_1> observatory. Drawer Encoding meteor destroying* an* observatory. [ICON1058] [X10] [Y5] [SCALE5] [ROT0] [MIRROR1] [ICON65] [X13] [Y10]……. Figure 3: Game state encoding for our models. For each encoding method, the upper text is the input and the lower text is the target output. et al., 2020) language model by treating the task as a text-to-text conditional generation task. Interestingly, we find vision-and-language (V+L) models (Tan and Bansal, 2019; Chen et al., 2020) to be less effective, which might be because current V+L models have inferior language-related abilities (Iki and Aizawa, 2021), or because models trained on photographic images are not well-suited to understand the non-literal imagery found in Iconary. 3.1 Guesser experiments found encoding positional information more precisely, or encoding earlier drawings if they exist, did not improve performance when using T5. Next, we append the text ‘phrase:’ and, for each word in the target phrase, either an underscore or the correct word if it is known (see Figure 3, top). We expe"
2021.emnlp-main.141,Q14-1006,0,0.164456,"Missing"
2021.emnlp-main.141,D17-1099,0,0.0358424,"Missing"
C04-1021,A00-2018,0,0.0281078,"n the restrictions to form the final SQL queries corresponding to each semantic interpretation. S VP NP PP NP PP NP pronoun verb NP noun PP NP prep noun NP prep noun NP prep noun What are flights fromBoston to Chicago on Monday? Figure 1: Example of an erroneous parse tree corrected by P RECISE’s semantic over-rides. P RECISE detects that the parser attached the PP “on Monday” to “Chicago” in error. P RECISE attempts to re-attach “on Monday” first to the PP “to Chicago”, and then to the NP “flights from Boston to Chicago”, where it belongs. 2.1 Parser Enhancements We used the Charniak parser (Charniak, 2000) for the experiments reported in this paper. We found that the Charniak parser, which was trained on the WSJ corpus, yielded numerous syntactic errors. Our first step was to hand tag a set of 150 questions with Part Of Speech (POS) tags, and re-train the parser’s POS tagger. As a result, the probabilities associated with certain tags changed dramatically. For example, initially, ‘list’ was consistently tagged as a noun, but after re-training it was consistently labeled as a verb. This change occurs because, in the ATIS domain, ‘list’ typically occurs in imperative sentences, such as “List all"
C04-1021,J82-2006,0,0.691045,"s a semantic parser without requiring fully-annotated corpora. HEY uses a hierarchical semantic parser that is trained on a set of questions together with their corresponding SQL queries. HEY is similar to (Tang and Mooney, 2001). Both learning systems require a large set of questions labeled by their SQL queries—an expensive input that P RECISE does not require—and, unlike P RECISE, both systems cannot leverage continuing improvements to statistical parsers. Sublanguages The early work with the most similarities to P RECISE was done in the field of sublanguages. Traditional sublanguage work (Kittredge, 1982) has looked at defining sublanguages for various domains, while more recent work (Grishman, 2001; Sekine, 1994) suggests using AI techniques to learn aspects of sublanguages automatically. Our work can be viewed as a generalization of traditional sublanguage research. We restrict ourselves to the semantically tractable subset of English rather than to a particular knowledge domain. Finally, in addition to offering formal guarantees, we assess the prevalence of our “sublanguage” in the ATIS data. 6 Conclusion This paper is the first to provide evidence that statistical parsers can support NLIs"
C04-1021,H90-1020,0,0.0485346,"eats ‘list’ as a noun, but in the context of the ATIS database it is a verb.1 On the other hand, manually creating and labeling a massive corpus of questions for each database is prohibitively expensive. We consider two methods of resolving this quandary and assess their performance individually and in concert on the ATIS data set. First, we use a strong semantic model to correct parsing errors. We introduce a theoretical framework for discriminating between Semantically Tractable (ST) questions and difficult ones, and we show that ST questions are prevalent in the well-studied ATIS data set (Price, 1990). Thus, we show that the semantic component of the NLI task can be surprisingly easy and can be used to compensate for syntactic parsing errors. Second, we re-train the parser using a relatively small set of 150 questions, where each word is labeled by its part-of-speech tag. To demonstrate how these methods work in practice, we sketch the fully-implemented P RECISE NLI, where a parser is a modular “plug in”. This modularity enables P RECISE to leverage continuing advances in parsing technology over time by plugging in improved parsers as they become available. The remainder of this paper is o"
C04-1021,H94-1039,0,\N,Missing
D08-1002,P08-1004,1,0.273221,"ons We report on the AU C ONTRAIRE CD system, which addresses each of the above challenges. First, AU C ONTRAIRE identifies “functional phrases” statistically (Section 3). Second, AU C ONTRAIRE uses these phrases to automatically create a large corpus of apparent contradictions (Section 4.2). Finally, AU C ONTRAIRE sifts through this corpus to find genuine contradictions using knowledge about synonymy, meronymy, argument types, and ambiguity (Section 4.3). Instead of analyzing sentences directly, AU C ON TRAIRE relies on the T EXT RUNNER Open Information Extraction system (Banko et al., 2007; Banko and Etzioni, 2008) to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them (e.g., was born in(Mozart,Salzburg)). Using extracted tuples greatly simplifies the CD task, because numerous syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are 1 Although we focus on function-based CD in our case study, we believe that our observations apply to other types of CD as well. 12 delegated to T EXT RUNNER or simply ignored. Nevertheless, extracted tuples are a conven"
D08-1002,W03-0906,0,0.0194348,"Missing"
D08-1002,P08-1118,0,0.329103,"Missing"
D08-1002,W07-1431,0,0.0150601,"Etzioni, 2007)— a system that identifies synonyms from T EXT RUNNER extractions. Additionally, AU C ONTRAIRE uses edit-distance and tokenbased string similarity (Cohen et al., 2003) between apparently contradictory values of y to identify synonyms. Meronyms: For some relations, there is no contradiction when y1 and y2 share a meronym, i.e. “part of” relation. For example, in the set born in(Mozart,·) there is no contradiction between the y values “Salzburg” and “Austria”, but “Salzburg” conflicts with “Vienna”. Although this is only true in cases where y occurs in an upward monotone context (MacCartney and Manning, 2007), in practice genuine contradictions between 16 y-values sharing a meronym relationship are extremely rare. We therefore simply assigned contradictions between meronyms a probability close to zero. We used the Tipster Gazetteer4 and WordNet to identify meronyms, both of which have high precision but low coverage. Argument Typing: Two y values are not contradictory if they are of different argument types. For example, the relation born in can take a date or a location for the y value. While a person can be born in only one year and in only one city, a person can be born in both a year and a cit"
D08-1002,P08-1008,0,0.13176,"003) first proposed contradiction detection as an important NLP task, and Harabagiu et al. (2006) were the first to report results on contradiction detection using negation, although their evaluation corpus was a balanced data set built by manually negating entailments in a data set from the Recognizing Textual Entailment conferences (RTE) (Dagan et al., 2005). De Marneffe et al. (2008) reported experimental results on a contradiction corpus created by annotating the RTE data sets. RTE-3 included an optional task, requiring systems to make a 3-way distinction: {entails, contradicts, neither} (Voorhees, 2008). The average performance for contradictions on the RTE-3 was precision 0.11 at recall 0.12, and the best system had precision 0.23 at recall 0.19. We did not run AU C ON TRAIRE on the RTE data sets because they contained 19 Conclusions and Future Work We have described a case study of contradiction detection (CD) based on functional relations. In this context, we introduced and evaluated the AU C ON TRAIRE system and its novel EM-style algorithm for determining whether an arbitrary phrase is functional. We also created a unique “natural” data set of seeming contradictions based on sentences d"
D08-1002,N07-1016,1,0.819609,"ion, in order to estimate the probability that a given pair, {R(x, y1 ), R(x, y2 )} is a genuine contradiction. Synonyms: The set of potential contradictions died from(Mozart,·) may contain assertions that Mozart died from renal failure and that he died from kidney failure. These are distinct values of y, but do not contradict each other, as the two terms are synonyms. AU C ONTRAIRE uses a variety of knowledge sources to handle synonyms. WordNet is a reliable source of synonyms, particularly for common nouns, but has limited recall. AU C ONTRAIRE also utilizes synonyms generated by R ESOLVER (Yates and Etzioni, 2007)— a system that identifies synonyms from T EXT RUNNER extractions. Additionally, AU C ONTRAIRE uses edit-distance and tokenbased string similarity (Cohen et al., 2003) between apparently contradictory values of y to identify synonyms. Meronyms: For some relations, there is no contradiction when y1 and y2 share a meronym, i.e. “part of” relation. For example, in the set born in(Mozart,·) there is no contradiction between the y values “Salzburg” and “Austria”, but “Salzburg” conflicts with “Vienna”. Although this is only true in cases where y occurs in an upward monotone context (MacCartney and"
D08-1009,W02-1033,0,0.0427397,"exts, H OLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, H OLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, H OLMES’s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-defined sense. 1 Introduction and Motivation Numerous researchers have identified the Web as a rich source of answers to factual questions, e.g., (Kwok et al., 2001; Brill et al., 2002), but often the desired information is not stated explicitly even in a textual corpus as massive as the Web. Consider the question “What vegetables help prevent osteoporosis?” Since there is likely no sentence on the Web directly stating “Kale prevents osteoporosis”, a system must infer that kale is an answer by combining facts from multiple sentences, possibly from different pages, which justify that conclusion: i.e., that kale is a vegetable, kale contains calcium, and calcium helps prevent osteoporosis. 1.1 H OLMES: A Scalable TI System This paper describes H OLMES, an implemented system, w"
D08-1009,C00-1043,0,0.00762719,"er— Markov Logic Horn Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows H OLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, H OLMES tries to find all consequents that match a conjunctive query. H OLMES is also related to open-domain questionanswering systems such as Mulder (Kwok et al., 2001), AskMSR (Brill et al., 2002), and others (Harabagiu et al., 2000; Brill et al., 2001). However, these Q/A systems attempt to find individual documents or sentences containing the answer. They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail. In contrast, H OLMES utilizes TI and attempts to combine information from multiple different sentences in a scalable way. While its ability to combine information from multiple sources is promising, H OLMES has several limitations these Q/A systems do not have. Since H OLMES relies on an information extraction system to convert sentences into ground predicat"
D08-1009,C92-2082,0,0.192984,"Missing"
D08-1009,W07-1431,0,0.0405855,"user to limit the number of transitive inference steps for any inference rule. H OLMES also includes a few enhancements for dealing with information extracted from natural language. For example, H OLMES’s inference rules support substring/regex matching of ground assertions, to accommodate simple variations in text. H OLMES also can be restricted to only operate over proper nouns, which is useful for queries involving named entities. 2.3 Markov Logic Inference Rules H OLMES is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by (MacCartney and Manning, 2007). 1. Observed relations are likely to be true: R(X,Y) :- ObservedInCorpus(X, R, Y) 2. Synonym substitution preserves meaning: RTR (X’,Y) :- RTR (X,Y) ∧ Synonym(X, X’) 3. RTR (X,Y’) :- RTR (X,Y) ∧ Synonym(Y, Y’) 4. Generalizations preserve meaning: RTR (X’,Y) :- RTR (X,Y) ∧ IS-A(X, X’) 5. RTR (X,Y’) :- RTR (X,Y) ∧ IS-A(Y, Y’) 6. Transitivity of Part Meronyms: RTR (X,Y’) :- RTR (X,Y) ∧ Part-Of(Y, Y’) where RTR matches ‘* in’ (e.g., ‘born in’). 82 For example, if Q(X):-BornIn(X,‘France’), and we know from WordNet that Paris is in France, then by inference rule 6, we know that BornIn(X,‘Paris’) wi"
D08-1009,P06-2105,0,0.0161962,"scales with KB size. For these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that H OLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T , H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. H OLMES’s representation is much simpler— Markov Logic Horn Clauses for inference rule"
D08-1009,W08-2219,0,0.0522213,"Missing"
D08-1009,N07-1016,1,0.714938,"4.3 Prevalence of APF Relations To determine the prevalence of APF relations in Web text, we examined a sample of 500 binary relations selected randomly from T EXT RUNNER’s ground assertions. The surface forms of the relations and arNutrition: the nine queries issued are instances guments may misrepresent the true properties of the of “What foods prevent disease?” Where a food is underlying concepts, so to better estimate the true a member of one of the classes: fruit, vegetable, or properties we merged synonymous values as given grain, and a disease is one of: anemia, scurvy, or by Resolver (Yates and Etzioni, 2007) or the most osteoporosis. More formally, Q(X, {disease}) :- frequent sense of the word in WordNet. For example, we would consider BornIn(baby, hospital) Prevents(X, {disease}) ∧ IS-A(X, {food}) Our experiments in the nutrition domain utilized and BornAt(infant, infirmary) to represent the two domain-specific inference rules in addition to same concept, and so would merge them into one instance of the ‘Born In’ relation. The largest two rethe ones presented in Section 2.3: lations had over 1.25 million unique instances each, Prevents(X,Y):-HighIn(X,Z) ∧ Prevents(Z,Y) Prevents(X,Y):-Contains(X,"
D10-1106,C92-2082,0,0.300901,"Missing"
D10-1106,N07-1071,0,0.0614125,"Two other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y =⇒ X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions Figure 1: Architecture of S HERLOCK. S HERLOCK learns inference rules offline and provides them to the H OLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by S HERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE incl"
D10-1106,W97-0209,0,0.109925,"rom text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y =⇒ X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions Figure 1: Architecture of S HERLOCK. S HERLOCK learns inference rules offline and provides them to the H OLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by S HERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE include those of Tatu and Moldovan (2007), which ge"
D10-1106,D08-1009,1,0.763391,"e fact that examples provide varying amounts of information. Second, statistical relevance finds rules with large increases in relative probability, not necessarily a large absolute probability. This is crucial in an open domain setting where most facts are false, which means the trivial rule that everything is false will have high accuracy. Even for true rules, the observed estimates p(Head(...)|Body(...))  1 due to missing data and noise. 3.5 Making Inferences In order to benefit from learned rules, we need an inference engine; with its linear-time scalability, H OLMES is a natural choice (Schoenmackers et al., 2008). As input H OLMES requires a target atom H(...), an evidence set E and weighted rule set R as input. It performs a form of knowledge based model construction (Wellman et al., 1992), first finding facts using logical inference, then estimating the confidence of each using a Markov Logic Network (Richardson and Domingos, 2006). Prior to running inference, it is necessary to assign a weight to each rule learned by S HERLOCK. Since 1093 the rules and inferences are based on a set of noisy and incomplete extractions, the algorithms used for both weight learning and inference should capture the fol"
D10-1106,N06-1039,0,0.221453,"et. Beyond this filtering we make no use of a type hierarchy and treat classes independently. In our corpus, we identify 1.1 million distinct, cleaned (instance, class) pairs for 156 classes. whose weighted frequency falls below a threshold, since rare relations are more likely to arise due to extraction errors or word-sense ambiguity. We also remove relations whose pointwise mutual information (PMI) is below a threshold T =exp(2) ≈ 7.4: 3.2 3.3 Discovering Relations between Classes Next, S HERLOCK discovers how classes relate to and interact with each other. Prior work in relation discovery (Shinyama and Sekine, 2006) has investigated the problem of finding relationships between different classes. However, the goal of this work is to learn rules on top of the discovered typed relations. We use a few simple heuristics to automatically identify interesting relations. For every pair of classes (C1 , C2 ), we find a set of typed, candidate relations from the 100 most frequent relations in the corpus where the first argument is an instance of C1 and the second argument is an instance of C2 . For extraction terms with multiple senses (e.g., New York), we split their weight based on how frequently they appear wit"
D10-1106,P06-1101,0,0.0378903,"Missing"
D10-1106,N07-1016,1,0.799312,"d semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, S HERLOCK focuses mainly on learning inference rules, but does so without any manually specified seeds or constraints. Craven et al.(1998) also used ILP to help information extraction on the Web, but required training examples and focused on a single domain. Two other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y =⇒ X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restric"
D10-1106,C02-1144,0,\N,Missing
D10-1106,W07-1404,0,\N,Missing
D10-1123,P08-1004,1,0.606395,"and freelyavailable knowledge resources such as Freebase. It first computes multiple typed functionality scores, representing functionality of the relation phrase when its arguments are constrained to specific types. It then aggregates these scores to predict the global functionality for the phrase. L EIBNIZ outperforms previous work, increasing area under the precisionrecall curve from 0.61 to 0.88. We utilize L EIBNIZ to generate the first public repository of automatically-identified functional relations. 1 Introduction The paradigm of Open Information Extraction (IE) (Banko et al., 2007; Banko and Etzioni, 2008) has scaled extraction technology to the massive set of relations expressed in Web text. However, additional work is needed to better understand these relations, and to place them in richer semantic structures. A step in that direction is identifying the properties of these relations, e.g., symmetry, transitivity and our focus in this paper – functionality. We refer to this problem as functionality identification. A binary relation is functional if, for a given arg1, there is exactly one unique value for arg2. Examples of functional relations are father, death date, birth city, etc. We define"
D10-1123,N06-1038,0,0.0162819,"l-suited for identifying whether a verb phrase can take multiple objects or not. This can be understood as a functionality property of the verb phrase within a sentence, as opposed to functionality of the semantic relation the phrase represents. WIE: In a preliminary study, Popescu (2007) applies an instance based counting approach, but her relations require manually annotated type restrictions, which makes the approach less scalable. Finally, functionality is just one property of relations that can be learned from text. A number of other studies (Guarino and Welty, 2004; Volker et al., 2005; Culotta et al., 2006) have examined detecting other relation properties from text and applying them to tasks such as ontology cleaning. 3 Challenges for Functionality Identification A functional binary relation r is formally defined as one such that ∀x, y1 , y2 : r(x, y1 )∧r(x, y2 ) ⇒ y1 = y2 . We define a relation string to be functional if all semantic relations commonly expressed by the relation string are individually functional. Thus, under our definition, ‘was born in’ and ‘died in’ are functional, even though they can take different arg2s for the same arg1, e.g., year, city, state, country, etc. The definit"
D10-1123,J93-1003,0,0.0250864,"enables L EIBNIZ to identify far more functional relations than T EXT RUNNER. 6.1 Addressing Evidence Sparsity Scaling up to a large collection of typed relations requires us to consider the size of our data sets. For example, consider which relation is more likely to be functional—a relation with 10 instances all of which indicate functionality versus a relation with 100 instances where 95 behave functionally. To address this problem, we adapt the likelihood ratio approach from Schoenmackers et al. (2010). For a typed relation with n instances, f of which indicate functionality, the G-test (Dunning, 1993), G = 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a measure for the likelihood that the relation is not functional. Here k denotes the evidence indicating functionality for the case where the relation is not functional. Setting k = n*0.25 worked well for us. This G-score replaces our previous metric for scoring functional relations. 6.2 Evaluation of the Repository In C LEAN L ISTS a factor that affects the quality of the results is the exact set of lists that is used. If the lists are not clean, results get noisy. For example, Freebase’s list of films contains 73,000 entries, many of which"
D10-1123,P10-1150,0,0.00617076,"Missing"
D10-1123,D08-1002,1,0.823808,"e is exactly one unique value for arg2. Examples of functional relations are father, death date, birth city, etc. We define a relation phrase to be functional if all semantic relations commonly expressed by that phrase are functional. For example, we say that the phrase ‘was born in’ denotes a functional relation, because the different semantic relations expressed by the phrase (e.g., birth city, birth year, etc.) are all functional. Knowing that a relation is functional is helpful for numerous NLP inference tasks. Previous work has used functionality for the tasks of contradiction detection (Ritter et al., 2008), quantifier scope disambiguation (Srinivasan and Yates, 2009), and synonym resolution (Yates and Etzioni, 2009). It could also aid in other tasks such as ontology generation and information extraction. For example, consider two sentences from a contradiction detection task: (1) “George Washington was born in Virginia.” and (2) “George Washington was born in Texas.” As Ritter et al. (2008) points out, we can only determine that the two sentences are contradictory if we know that the semantic relation referred to by the phrase ‘was born in’ is functional, and that both Virginia and Texas are di"
D10-1123,P10-1044,1,0.0728578,"Missing"
D10-1123,D08-1009,1,0.19266,"Missing"
D10-1123,D10-1106,1,0.069093,"erspecified tuple, <Gold, has, an atomic number of 79>, O CCAM extracts <Gold, has an atomic number of, 79>. O CCAM enables L EIBNIZ to identify far more functional relations than T EXT RUNNER. 6.1 Addressing Evidence Sparsity Scaling up to a large collection of typed relations requires us to consider the size of our data sets. For example, consider which relation is more likely to be functional—a relation with 10 instances all of which indicate functionality versus a relation with 100 instances where 95 behave functionally. To address this problem, we adapt the likelihood ratio approach from Schoenmackers et al. (2010). For a typed relation with n instances, f of which indicate functionality, the G-test (Dunning, 1993), G = 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a measure for the likelihood that the relation is not functional. Here k denotes the evidence indicating functionality for the case where the relation is not functional. Setting k = n*0.25 worked well for us. This G-score replaces our previous metric for scoring functional relations. 6.2 Evaluation of the Repository In C LEAN L ISTS a factor that affects the quality of the results is the exact set of lists that is used. If the lists are not c"
D10-1123,D09-1152,0,0.121581,"ctional relations are father, death date, birth city, etc. We define a relation phrase to be functional if all semantic relations commonly expressed by that phrase are functional. For example, we say that the phrase ‘was born in’ denotes a functional relation, because the different semantic relations expressed by the phrase (e.g., birth city, birth year, etc.) are all functional. Knowing that a relation is functional is helpful for numerous NLP inference tasks. Previous work has used functionality for the tasks of contradiction detection (Ritter et al., 2008), quantifier scope disambiguation (Srinivasan and Yates, 2009), and synonym resolution (Yates and Etzioni, 2009). It could also aid in other tasks such as ontology generation and information extraction. For example, consider two sentences from a contradiction detection task: (1) “George Washington was born in Virginia.” and (2) “George Washington was born in Texas.” As Ritter et al. (2008) points out, we can only determine that the two sentences are contradictory if we know that the semantic relation referred to by the phrase ‘was born in’ is functional, and that both Virginia and Texas are distinct states. Automatic functionality identification is essen"
D10-1123,D11-1142,1,\N,Missing
D11-1141,P11-1040,0,0.697784,"Missing"
D11-1141,J92-4003,0,0.0910564,"Missing"
D11-1141,W99-0613,0,0.0450456,"l release the [Nintendo]ORG 3DS in north [America]LOC march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most"
D11-1141,N09-1019,0,0.0205718,"C march 27 for $250 7 http://www.chasen.org/˜taku/software/ TinySVM/ The OOV word ‘Yess’ is mistaken as a named entity. In addition, although the first occurrence of ‘Nintendo’ is correctly segmented, it is misclassified, whereas the second occurrence is improperly segmented – it should be the product “Nintendo 3DS”. Finally “north America” should be segmented as a LOCATION, rather than just ‘America’. In general, news-trained Named Entity Recognizers seem to rely heavily on capitalization, which we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entit"
D11-1141,W10-0713,0,0.605816,"Missing"
D11-1141,P05-1045,0,0.09483,"ue to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared across its mentions. 5 Conclusions We have demonstrated that existing tools for POS tagging, Chunking and Named Entity Recognition perform quite poorly when applied to Tweets. To address this challenge we have annotated tweets and built tools trained on unlabeled, in-domain and outof-domain data, showing substantial improvement"
D11-1141,W02-2010,0,0.0537015,"Missing"
D11-1141,P11-2008,0,0.750872,"Missing"
D11-1141,W11-0704,0,0.045433,"yles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in segmenting Named Entiti"
D11-1141,P11-1038,0,0.693265,"tter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Amazon’s Mechanical Turk for annotating Named Entities in Twitter, Minkov et. al. (2005) investigate person name recognizers in email, and Singh et. al. (2010) apply a minimally supervised approach to extracting entities from text advertisements. In contrast to previous work, we have demonstrated the utility of features based on Twitterspecific POS taggers and Shallow Parsers in se"
D11-1141,C08-1056,0,0.0175354,"Missing"
D11-1141,N10-1087,0,0.0117909,"y annotated data to be very beneficial for named entity segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’"
D11-1141,P11-1037,0,0.762242,"End to End System: Finally we present the end to end performance on segmentation and classification (T- NER) in Table 12. We observe that T- NER again outperforms co-training. Moreover, comparing against the Stanford Named Entity Recognizer on the 3 MUC types, T- NER doubles F1 score. 4 Related Work There has been relatively little previous work on building NLP tools for Twitter or similar text styles. Locke and Martin (2009) train a classifier to recognize named entities based on annotated Twitter data, handling the types PERSON, LOCATION, and ORGANIZATION. Developed in parallel to our work, Liu et al. (2011) investigate NER on the same 3 types, in addition to PRODUCTs and present a semisupervised approach using k-nearest neighbor. Also developed in parallel, Gimpell et al. (2011) build a POS tagger for tweets using 20 coarse-grained tags. Benson et. al. (2011) present a system which extracts artists and venues associated with musical performances. Recent work (Han and Baldwin, 2011; Gouws et al., 2011) has proposed lexical normalization of tweets which may be useful as a preprocessing step for the upstream tasks like POS tagging and NER. In addition Finin et. al. (2010) investigate the use of Ama"
D11-1141,D10-1035,0,0.0451038,"ty segmentation, we were motivated to explore approaches that don’t rely on manual labels for classification due to Twitter’s wide range of named entity types. Additionally, unlike previous work on NER in informal text, our approach allows the sharing of information across an entity’s mentions which is quite beneficial due to Twitter’s terse nature. Previous work on Semantic Bootstrapping has taken a weakly-supervised approach to classifying named entities based on large amounts of unlabeled text (Etzioni et al., 2005; Carlson et al., 2010; Kozareva and Hovy, 2010; Talukdar and Pereira, 2010; McIntosh, 2010). In contrast, rather than predicting which classes an entity belongs to (e.g. a multi-label classification task), LabeledLDA estimates a distribution over its types, which is then useful as a prior when classifying mentions in context. In addition there has been been work on SkipChain CRFs (Sutton, 2004; Finkel et al., 2005) which enforce consistency when classifying multiple occurrences of an entity within a document. Us1532 ing topic models (e.g. LabeledLDA) for classifying named entities has a similar effect, in that information about an entity’s distribution of possible types is shared ac"
D11-1141,H05-1056,0,0.0177837,"Missing"
D11-1141,P09-1113,0,0.173591,"Missing"
D11-1141,D09-1026,0,0.642196,"Missing"
D11-1141,D10-1037,0,0.0260391,"ich we know to be unreliable in tweets. Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al. (2009), we treat classification and segmentation of named entities as separate tasks. This allows us to more easily apply techniques better suited towards each task. For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification. While it might be beneficial to jointly model segmentation and (distantly supervised) classification using a joint sequence labeling and topic model similar to that proposed by Sauper et al. (2010), we leave this for potential future work. Because most words found in tweets are not part of an entity, we need a larger annotated dataset to effectively learn a model of named entities. We therefore use a randomly sampled set of 2,400 tweets for NER. All experiments (Tables 6, 8-10) report results using 4-fold cross validation. 3.1 Segmenting Named Entities Because capitalization in Twitter is less informative than news, in-domain data is needed to train models which rely less heavily on capitalization, and also are able to utilize features provided by T- CAP. We exhaustively annotated our s"
D11-1141,N03-1028,0,0.0319403,"Missing"
D11-1141,N10-1009,0,0.0418606,"Missing"
D11-1141,P10-1149,0,0.0787574,"Missing"
D11-1141,W00-0726,0,0.037791,"Missing"
D11-1141,N03-1033,0,0.0846633,"Missing"
D11-1141,P10-1040,0,0.504898,"Missing"
D11-1141,P95-1026,0,0.0853228,"Missing"
D11-1141,J93-2004,0,\N,Missing
D11-1142,P98-1013,0,0.1065,"recision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as R E V ERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uninformative extraction"
D11-1142,P08-1004,1,0.122213,"tems such as T EXT RUNNER are unable to learn the constraints embedded in R E V ERB. Of course, a learning system, utilizing a different hypothesis space, and an appropriate set of training examples, could potentially learn and refine the constraints in R E V ERB. This is a topic for future work, which we consider in Section 6. The first Open IE system was T EXT RUNNER (Banko et al., 2007), which used a Naive Bayes model with unlexicalized POS and NP-chunk features, trained using examples heuristically generated from the Penn Treebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over T EXT RUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz"
D11-1142,P11-1062,0,0.0631302,"ble at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as T EX T RUNNER and WOE : incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in incomprehensible predictio"
D11-1142,W10-0907,1,0.348589,"them susceptible to parser errors and substantially slower than Open IE systems such as R E V ERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uninformative extractions by capturing relation phrases expressed by a verb-noun combination, including light verb constructions. V |V P |V W ∗P V = verb particle? adv? W = (noun |adj |adv |pron |det) P = (prep |particle |inf. marker) Figure 1: A simple part-of-speech-based regular expression reduces the numb"
D11-1142,E95-1014,0,0.114539,"s of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics ous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil). This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs). An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002). Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors. Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem. Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at. To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought"
D11-1142,P10-1030,0,0.029857,"ic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over T EXT RUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually rely1537 ing on a full dependency parse of the input sentence ("
D11-1142,eichler-etal-2008-unsupervised,0,0.0346979,"rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually rely1537 ing on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation e"
D11-1142,D10-1123,1,0.0689139,"e automatic identification of rela1 The source code for R E V ERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as T EX T RUNNER and WOE : incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include e"
D11-1142,kingsbury-palmer-2002-treebank,0,0.178555,"constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as R E V ERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in particular, relies on its explicit lexical and syntactic constraints, which have no correlate in SRL systems. For a more detailed comparison of SRL and Open IE, see (Christensen et al., 2010). 3 Constraints on Relation Phrases In this section we introduce two constraints on relation phrases: a syntactic constraint and a lexical constraint. 3.1 Syntactic Constraint The syntactic constraint serves two purposes. First, it eliminates incoherent extractions, and second, it reduces uni"
D11-1142,P09-1113,0,0.43748,"2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over T EXT RUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually rely1537 ing on a full dependency parse"
D11-1142,J08-2005,0,0.139557,"trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused on syntactic patterns for relation extraction, we introduce a lexical constraint that boosts precision and recall. Finally, Open IE is closely related to semantic role labeling (SRL) (Punyakanok et al., 2008; Toutanova et al., 2008) in that both tasks extract relations and arguments from sentences. However, SRL systems traditionally rely on syntactic parsers, which makes them susceptible to parser errors and substantially slower than Open IE systems such as R E V ERB. This difference is particularly important when operating on the Web corpus due to its size and heterogeneity. Finally, SRL requires hand-constructed semantic resources like Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) as input. In contrast, Open IE systems require no relation-specific training data. ReVerb, in"
D11-1142,P10-1044,1,0.0553357,"relations in English sentences (Banko et al., 2007). The automatic identification of rela1 The source code for R E V ERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as T EX T RUNNER and WOE : incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor mak"
D11-1142,D10-1106,1,0.105892,"code for R E V ERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as T EX T RUNNER and WOE : incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracted relation phrase has no meaningful interpretation (see Table 1 for examples). Incoherent extractions arise because the learned extractor makes a sequence of decisions about whether to include each word in the relation phrase, often resulting in inco"
D11-1142,P06-2094,0,0.0204135,"equent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over T EXT RUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs"
D11-1142,N06-1039,0,0.0263938,"reebank. Subsequent work showed that utilizing a linear-chain CRF (Banko and Etzioni, 2008) or Markov Logic Network (Zhu et al., 2009) can lead to improved extraction. The WOE systems introduced by Wu and Weld make use of Wikipedia as a source of training data for their extractors, which leads to further improvements over T EXT RUNNER (Wu and Weld, 2010). Wu and Weld also show that dependency parse features result in a dramatic increase in precision and recall over shallow linguistic features, but at the cost of extraction speed. Other approaches to large-scale IE have included Preemptive IE (Shinyama and Sekine, 2006), OnDemand IE (Sekine, 2006), and weak supervision for IE (Mintz et al., 2009; Hoffmann et al., 2010). Preemptive IE and On-Demand IE avoid relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs"
D11-1142,W06-0508,0,0.0254763,"relationspecific extractors, but rely on document and entity clustering, which is too costly for Web-scale IE. Weakly supervised methods use an existing ontology to generate training data for learning relationspecific extractors. While this allows for learning relation-specific extractors at a larger scale than what was previously possible, the extractions are still restricted to a specific ontology. Many systems have used syntactic patterns based on verbs to extract relation phrases, usually rely1537 ing on a full dependency parse of the input sentence (Lin and Pantel, 2001; Stevenson, 2004; Specia and Motta, 2006; Kathrin Eichler and Neumann, 2008). Our work differs from these approaches by focusing on relation phrase patterns expressed in terms of POS tags and NP chunks, instead of full parse trees. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. This paper reports on a substantially improved model of binary relation phrases, which increases the recall of the Banko-Etzioni model (see Section 3.3). Further, while previous work in Open IE has mainly focused"
D11-1142,W04-0401,0,0.0105739,"irical Methods in Natural Language Processing, pages 1535–1545, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics ous Open IE systems return the uninformative (Faust, made, a deal) instead of (Faust, made a deal with, the devil). This type of error is caused by improper handling of relation phrases that are expressed by a combination of a verb with a noun, such as light verb constructions (LVCs). An LVC is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic content of the predicate (Grefenstette and Teufel, 1995; Stevenson et al., 2004; Allerton, 2002). Table 2 illustrates the wide range of relations expressed this way, which are not captured by existing open extractors. Our syntactic constraint leads the extractor to include nouns in the relation phrase, solving this problem. Although the syntactic constraint significantly reduces incoherent and uninformative extractions, it allows overly-specific relation phrases such as is offering only modest greenhouse gas reduction targets at. To avoid overly-specific relation phrases, we introduce an intuitive lexical constraint: a binary relation phrase ought to appear with at least"
D11-1142,J08-2002,0,0.00887911,"Missing"
D11-1142,P10-1013,0,0.935241,"where the target relations cannot be specified in advance. Open IE solves this problem by identifying relation phrases—phrases that denote relations in English sentences (Banko et al., 2007). The automatic identification of rela1 The source code for R E V ERB is available at http:// reverb.cs.washington.edu/ tion phrases enables the extraction of arbitrary relations from sentences, obviating the restriction to a pre-specified vocabulary. Open IE systems have achieved a notable measure of success on massive, open-domain corpora drawn from the Web, Wikipedia, and elsewhere. (Banko et al., 2007; Wu and Weld, 2010; Zhu et al., 2009). The output of Open IE systems has been used to support tasks like learning selectional preferences (Ritter et al., 2010), acquiring common sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010; Berant et al., 2011). In addition, Open IE extractions have been mapped onto existing ontologies (Soderland et al., 2010). We have observed that two types of errors are frequent in the output of Open IE systems such as T EX T RUNNER and WOE : incoherent extractions and uninformative extractions. Incoherent extractions are cases where the extracte"
D11-1142,C98-1013,0,\N,Missing
D12-1048,H05-1091,0,0.283064,"ng approaches in traditional information extraction, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First, and most importantly, these previous systems learn an extractor for each relation of interest, whereas OLLIE is an open extractor. OLLIE’s strength is its ability to generalize from one relation to many other relations that are expressed in similar forms. This happens both via syntactic generalization and type generalization of relation words (sections 3.2.1 and 3.2.2). This capability is essential as many relations in the test set are not even seen in the tra"
D12-1048,de-marneffe-etal-2006-generating,0,0.0480838,"Missing"
D12-1048,D11-1142,1,0.841584,") has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOEparse (Wu and Weld, 2010) suffer from two k"
D12-1048,P10-1030,0,0.0132882,"E V ERB and 1.9 times the AUC of WOEparse . 1 Figure 1: OLLIE (O) has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni e"
D12-1048,P11-1055,0,0.356847,"find this quality to be satisfactory for our needs of learning general patterns. Bootstrapped data has been previously used to generate positive training data for IE (Hoffmann et al., 2010; Mintz et al., 2009). However, previous systems retrieved sentences that only matched the two arguments, which is error-prone, since multiple relations can hold between a pair of entities (e.g., Bill Gates is the CEO of, a co-founder of, and has a high stake in Microsoft). Alternatively, researchers have developed sophisticated probabilistic models to alleviate the effect of noisy data (Riedel et al., 2010; Hoffmann et al., 2011). In our case, by enforcing that a sentence additionally contains some syntactic form of the relation content words, our bootstrapping set is naturally much cleaner. Moreover, this form of bootstrapping is better suited for Open IE’s needs, as we will use this data to generalize to other unseen relations. Since the relation words in the sentence and seed match, we can learn general pattern templates that may apply to other relations too. We discuss this process next. 3.2 Open Pattern Learning OLLIE’s next step is to learn general patterns that encode various ways of expressing relations. OL -"
D12-1048,C08-1050,0,0.0165868,"apped training set. Section 4 discusses the context analysis component, which is based on supervised training with linguistic and lexical features. Section 5 compares OLLIE with R E V ERB and WOE parse on a dataset from three domains: News, Wikipedia, and a Biology textbook. We find that OLLIE obtains 2.7 times the area in precision-yield curves (AUC) as R E V ERB and 1.9 times the AUC as WOEparse . Moreover, for specific relations commonly mediated by nouns (e.g., ‘is the president of’) OLLIE obtains two order of magnitude higher yield. We also compare OLLIE to a state-of-the-art SRL system (Johansson and Nugues, 2008) on an IE-related end task and find that they both have comparable performance at argument identification and have complimentary strengths in sentence analysis. In Section 6 we discuss related work on patternbased relation extraction. 2 Background Open IE systems extract tuples consisting of argument phrases from the input sentence and a phrase 1 Available for download at http://openie.cs.washington.edu 524 from the sentence that expresses a relation between the arguments, in the format (arg1; rel; arg2). This is done without a pre-specified set of relations and with no domain-specific knowled"
D12-1048,kingsbury-palmer-2002-treebank,0,0.0863481,"g1: Macromolecules Macromolecules Macromolecules types of RNA types of RNA arg2: phloem proteins types of RNA sieve tubes plasmodesmata relation term translocated include include enter enter We find an average of 4.0 verb-mediated relations and 0.3 noun-mediated relations per sentence. Evaluating OLLIE against this gold standard helps to answer the question of absolute recall: what percentage of binary relations expressed in a sentence can our systems identify. For comparison, we use a state-of-the-art SRL system from Lund University (Johansson and Nugues, 2008), which is trained on PropBank (Martha and Palmer, 2002) for its verb-frames and NomBank (Meyers et al., 2004) for noun-frames. The PropBank version of the system won the very competitive 2008 CONLL SRL evaluation. We conduct this experiment by manually compar531 Verb relations Noun relations All relations LUND OLLIE 0.58 (0.69) 0.07 (0.33) 0.54 (0.67) 0.49 (0.55) 0.13 (0.13) 0.47 (0.52) union 0.71 (0.83) 0.20 (0.33) 0.67 (0.80) Figure 9: Recall of LUND and OLLIE on binary relations. In parentheses is recall with oracle co-reference. Both systems identify approximately half of all argument pairs, but have lower recall on noun-mediated relations. in"
D12-1048,meyers-etal-2004-annotating,0,0.00488954,"RNA types of RNA arg2: phloem proteins types of RNA sieve tubes plasmodesmata relation term translocated include include enter enter We find an average of 4.0 verb-mediated relations and 0.3 noun-mediated relations per sentence. Evaluating OLLIE against this gold standard helps to answer the question of absolute recall: what percentage of binary relations expressed in a sentence can our systems identify. For comparison, we use a state-of-the-art SRL system from Lund University (Johansson and Nugues, 2008), which is trained on PropBank (Martha and Palmer, 2002) for its verb-frames and NomBank (Meyers et al., 2004) for noun-frames. The PropBank version of the system won the very competitive 2008 CONLL SRL evaluation. We conduct this experiment by manually compar531 Verb relations Noun relations All relations LUND OLLIE 0.58 (0.69) 0.07 (0.33) 0.54 (0.67) 0.49 (0.55) 0.13 (0.13) 0.47 (0.52) union 0.71 (0.83) 0.20 (0.33) 0.67 (0.80) Figure 9: Recall of LUND and OLLIE on binary relations. In parentheses is recall with oracle co-reference. Both systems identify approximately half of all argument pairs, but have lower recall on noun-mediated relations. ing the outputs of LUND and OLLIE against the gold stand"
D12-1048,P09-1113,0,0.325368,"the AUC of WOEparse . 1 Figure 1: OLLIE (O) has a wider syntactic range and finds extractions for the first three sentences where R E V ERB (R) and WOEparse (W) find none. For sentences #4,5, R E V ERB and WOEparse have an incorrect extraction by ignoring the context that OLLIE explicitly represents. Introduction While traditional Information Extraction (IE) (ARPA, 1991; ARPA, 1998) focused on identifying and extracting specific relations of interest, there has been great interest in scaling IE to a broader set of relations and to far larger corpora (Banko et al., 2007; Hoffmann et al., 2010; Mintz et al., 2009; Carlson et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOE"
D12-1048,W04-2407,0,0.0114463,"rce additional dependency restrictions on the sentences. We only allow sentences where the content words from arguments and relation can be linked to each other via a linear path of size four in the dependency parse. To implement this restriction, we only use the subset of content words that are headwords in the parse tree. In the above sentence ‘Ireland’, ‘Boyle’ and ‘born’ connect via a dependency path of length six, and hence this sentence is rejected from the training set. This reduces our set to 4 million (seed tuple, sentence) pairs. In our implementation, we use Malt Dependency Parser (Nivre and Nilsson, 2004) for dependency parsing, since it is fast and hence, easily applicable to a large corpus of sentences. We post-process the parses using Stanford’s CCprocessed algorithm, which compacts the parse structure for easier extraction (de Marneffe et al., 2006). We randomly sampled 100 sentences from our bootstrapping set and found that 90 of them satisfy our bootstrapping hypothesis (64 without dependency constraints). We find this quality to be satisfactory for our needs of learning general patterns. Bootstrapped data has been previously used to generate positive training data for IE (Hoffmann et al"
D12-1048,P06-1015,0,0.0131284,"First, nouns, although less frequently mediating relations, are much harder and both systems are failing significantly on those – OLLIE is somewhat better. Two, neither systems dominates the other; in fact, recall is increased significantly by a union of the two. Three, and probably most importantly, significant information is still being missed by both systems, and more research is warranted. 6 Related Work There is a long history of bootstrapping and pattern learning approaches in traditional information extraction, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First,"
D12-1048,P10-1044,1,0.11327,"7 shows the results of this experiment on our dataset from three domains. As the curves show, OLLIE was correct to add lexical/semantic constraints to these patterns – precision is quite low without the restrictions. This matches our intuition, since these are not completely general patterns and generalizing to all unseen relations results in a large number of errors. OLLIE[lex] performs well though at lower yield. The type generalization helps the yield somewhat, without hurting the precision. We believe that a more data-driven type generalization that uses distributional similarity (e.g., (Ritter et al., 2010)) may help much more. Also, notice that overall precision numbers are lower, since these are the more difficult relations to extract reliably. We conclude that lexical/semantic restrictions are valuable for good performance of OLLIE. We also compare our full system to a version that does not use the context analysis of Section 4. Figure 8 compares OLLIE to a version (OLLIE[pat]) that does not add the AttributedTo and ClausalModifier fields, and, instead of context-sensitive confidence function, uses the pattern frequency in the training 530 Figure 8: Context analysis increases precision, raisi"
D12-1048,N06-1039,0,0.0103107,"ng set and high rule precision while still allowing morphological variants that cover noun-mediated relations. A third difference is in the scale of the training – R E V ERB yields millions of training seeds, where previous systems had orders of magnitude less. This enables OLLIE to learn patterns with greater coverage. The closest to our work is the pattern learning based open extractor WOEparse . Section 3.4 details the differences between the two extractors. Another extractor, StatSnowBall (Zhu et al., 2009), has an Open IE version, which learns general but shallow patterns. Preemptive IE (Shinyama and Sekine, 2006) is a paradigm related to Open IE that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters. The clustering steps make it difficult for it to scale to large corpora. 7 Conclusions Our work describes OLLIE, a novel Open IE extractor that makes two significant advances over the existing Open IE systems. First, it expands the syntactic scope of Open IE systems by identifying relationships mediated by nouns and adjectives. Our experiments found that for some relations this increases the number of correct extract"
D12-1048,P10-1013,0,0.931227,"on et al., 2010; Fader et al., 2011). However, the requirement of having pre-specified relations of interest is a significant obstacle. Imagine an intelligence analyst who recently acquired a terrorist’s laptop or a news reader who wishes to keep abreast of important events. The substantial endeavor in analyzing their corpus is the discovery of important relations, which are likely not pre-specified. Open IE (Banko et al., 2007) is the state-of-the-art approach for such scenarios. However, the state-of-the-art Open IE systems, R E V ERB (Fader et al., 2011; Etzioni et al., 2011) and WOEparse (Wu and Weld, 2010) suffer from two key drawbacks. Firstly, they handle a limited subset of sentence constructions for expressing relationships. Both extract only relations that are mediated by verbs, and R E V ERB further restricts this to a subset of verbal patterns. This misses important information mediated via other syntactic entities such as nouns and adjectives, as well as a wider range of verbal structures (examples #1-3 in Figure 1). 523 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 523–534, Jeju Island,"
D12-1048,P05-1052,0,0.00403611,"on, e.g., DIPRE (Brin, 1998), SnowBall (Agichtein and Gravano, 2000), Espresso (Pantel and Pennacchiotti, 2006), PORE (Wang et al., 2007), SOFIE (Suchanek et al., 2009), NELL (Carlson et al., 2010), and PROSPERA (Nakashole et al., 2011). All these approaches first bootstrap data based on seed instances of a relation (or seed data from existing resources such as Wikipedia) and then learn lexical or lexico-POS patterns to create an extractor. Other approaches have extended these to learning patterns based on full syntactic analysis of a sentence (Bunescu and Mooney, 2005; Suchanek et al., 2006; Zhao and Grishman, 2005). OLLIE has significant differences from the previous work in pattern learning. First, and most importantly, these previous systems learn an extractor for each relation of interest, whereas OLLIE is an open extractor. OLLIE’s strength is its ability to generalize from one relation to many other relations that are expressed in similar forms. This happens both via syntactic generalization and type generalization of relation words (sections 3.2.1 and 3.2.2). This capability is essential as many relations in the test set are not even seen in the training set – in early exper532 iments we found tha"
D12-1082,E06-1002,0,0.27264,"Missing"
D12-1082,D07-1074,0,0.121924,"“EMNLP” “ICAPS” present absent Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Introduction A key challenge in machine reading (Etzioni et al., 2006) is to identify the entities mentioned in text, and associate them with appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1"
D12-1082,C10-1032,0,0.0239483,"not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by R E V ERB (Fader et al., 2011).2 R E V ERB already filters out relative pronouns, WHO-adverbs, and existential “there” nou"
D12-1082,D11-1142,1,0.0433304,"s, but general purpose linking systems all use Wikipedia because of its broad 894 general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed when using entity linking systems is that despite containing over 3 million entities, Wikipedia does not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic"
D12-1082,D09-1099,0,0.00833072,"instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011)"
D12-1082,D11-1011,0,0.342843,"a article, and also that it occurs with textual relations such as “has already announced” and “has released updates for.” For each Wikipedialinked entity in L, we further look up its exact set of Freebase types.4 From U we obtain the set of textual relations that each e ∈ E is in the domain of. We now have a large set of class-labeled instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of o"
D12-1082,N04-1041,0,0.0146658,"y shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding th"
D12-1082,P11-3004,0,0.0115974,"nt number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The R E V ERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by R E V ERB (Fader et al., 2011).2 R E V ERB already filters out relative pronouns, WHO-adverbs, and existential “there” noun phrases that"
D12-1082,C10-1105,0,0.0149933,"approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding the textual relations that e is in the domain of, (2) 4 data available at http://download.freebase.com/wex Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase “Sun Microsystems.” We predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) find"
D12-1082,W09-1119,0,0.0343828,"Missing"
D12-1082,P11-1138,0,0.0209962,"h appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1 (Wang et al., 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: G"
D12-1082,sekine-nobata-2004-definition,0,0.00846455,"ion Named Entity Recognition (NER) is the task of identifying named entities in text. A key difference between our final goals and NER is that in the con2 available at http://reverb.cs.washington.edu text of entity linking and Wikipedia, there are many more entities than just the named entities. For example, “apple juice” and “television” are Wikipedia entities (with Wikipedia articles), but are not traditional named entities. Still, as named entities do comprise a sizable portion of our unlinkable noun phrases, we compare against a NER baseline in our entity detection step. Fine-grained NER (Sekine and Nobata, 2004; Lee et al., 2007) has studied scaling NER to up to 200 semantic types. This differs from our semantic typing of unlinked entities because our approach assumes access to corpora-level relationships between a large set of linked entities (with semantic types) and the unlinked entities. As a result we are able to propagate 1,339 Freebase semantic types from the linked entities to the unlinked entities, which is substantially more types than fine-grained NER. 2.3 Extracting Entity Sets There is a line of research in using Web extraction (Etzioni et al., 2005) and entity set expansion (Pantel et"
D12-1082,P10-1149,0,0.0143317,"on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L ∪ U |, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and F IGER"
D12-1082,D08-1061,0,0.0653835,"Missing"
D12-1082,buscaldi-rosso-2006-mining,0,\N,Missing
D12-1082,D09-1098,0,\N,Missing
D13-1178,W12-3019,1,0.843852,"ect. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sparisty and to improve coherence. We developed a page rank based schema induction algorithm which results in more coherent schemas with several actors. Unlike Chambers’ approach this method does not require explicit parameter tuning for controlling the number of actors. While our event schemas are close to being templates (because of associated types, and actor clustering), they do not have associated extractors. Our future work will focus on buildin"
D13-1178,P08-1090,0,0.659089,"Missing"
D13-1178,P09-1068,0,0.816405,"oni}@cs.washington.edu Actor Rel Actor A1:<person> failed A2:test A1:<person> was suspended for A3:<time period> A1:<person> used A4:<substance, drug> A1:<person> was suspended for A5:<game, activity> A1:<person> was in A6:<location> A1:<person> was suspended by A7:<org, person> Actor Instances: A1: {Murray, Morgan, Governor Bush, Martin, Nelson} A2: {test} A3: {season, year, week, month, night} A4: {cocaine, drug, gasoline, vodka, sedative} A5: {violation, game, abuse, misfeasance, riding} A6: {desert, Simsbury, Albany, Damascus, Akron} A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush} Abstract Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. Table 1: An event schema produced by our system, represented as a set of (Actor, Rel, Actor) triples, and a set of instances for each actor A1, A2, e"
D13-1178,chambers-jurafsky-2010-database,0,0.0326584,"Missing"
D13-1178,P11-1098,0,0.0260073,"pend than, commissioner). 6 Related Work Prior work by Chambers and Jurafsky (2008; 2009; 2010) showed that event sequences (narrative chains) mined from text can be used to induce event schemas in a domain-independent fashion. However, our manual evaluation of their output showed key limitations which may limit applicability. As pointed out earlier, a major weakness in Chambers’ approach is the pair-wise representation of subject-verb and verb-object. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sp"
D13-1178,N13-1104,0,0.0680972,"rom text can be used to induce event schemas in a domain-independent fashion. However, our manual evaluation of their output showed key limitations which may limit applicability. As pointed out earlier, a major weakness in Chambers’ approach is the pair-wise representation of subject-verb and verb-object. Also, their released a set of schemas are limited to two actors, although this number can be increased by setting a chain splitting parameter. Chambers and Jurafsky (2011) extended schema generation to learn domain-specific event templates and associated extractors. In work parallel to ours, Cheung et al. (2013), developed a probabilistic solution for template generation. However, their approach requires performing joint probability estimation using EM, which can limit scaling to large corpora. In this work we developed an Open IE based solution to generate schemas. Following prior work (Balasubramanian et al., 2012), we use Open IE triples for modeling relation co-occurrence. We extend the triple representation with semantic types for arguments to alleviate sparisty and to improve coherence. We developed a page rank based schema induction algorithm which results in more coherent schemas with several"
D13-1178,doddington-etal-2004-automatic,0,0.0167408,"event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead of labeled roles these schemas have a set of relations and actors that serve as arguments.1 Their system is fully automatic, domain-independent, and scales to large text corpora. However, we identify several limitations in the schemas produced by their system.2 Their schemas 1 In the rest of this paper we use event schemas to refer to these"
D13-1178,P05-1045,0,0.0057794,"resented as stemmed head nouns, and we also record semantic types of the arguments. We selected 29 semantic types from WordNet, examining the set of instances on a small development set to ensure that the types are useful, but not overly specific. The set of types are: person, organization, location, time unit, number, amount, group, business, executive, leader, effect, activity, game, sport, device, equipment, structure, building, substance, nutrient, drug, illness, organ, animal, bird, fish, art, book, and publication. To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005)5 , and also look up the argument in WordNet 2.1 and record 4 Available at: http://knowitall.github.io/ ollie/ 5 We used the system downloaded from: http://nlp. stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it. 1723 the first three senses if they map to our target semantic types. We use regular expressions to recognize dates and numeric expressions, and map personal pronouns to <person>. We associate all types found by this mechanism with each argument. The tuples in the example above are normalized to the following: 1. 2. 3. 4. 5. 6. 7. 8. 9. ... (He,"
D13-1178,W13-1202,0,0.0220333,"Missing"
D13-1178,W11-1913,0,0.0910754,"rs’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community. 1 Introduction Event schemas (also known as templates or frames) have been widely used in information extraction. An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead of labeled roles these schemas have a set of relations and actors that serve as"
D13-1178,J13-4004,0,0.0138012,"er relational tuples in a document. For clarity, we show the unstemmed version. Top tuples related to (<person>, convicted of, murder) 1. (<person>, convicted in, <time unit>) 2. (<person>, sentenced to, death) 3. (<person>, sentenced to, year) 4. (<person>, convicted in, <location>) 5. (<person>, sentenced to, life) 6. (<person>, convicted in, <person>) 7. (<person>, convicted after, trial) 8. (<person>, sent to, prison) pair is equal if they are from the same token sequence in the source sentence or one argument is a co-referent mention of the other. We use the Stanford Co-reference system (Lee et al., 2013)6 to detect co-referring mentions. There are four possible equalities depending on the specific pair of arguments in the tuples are the same, shown as E11, E12, E21 and E22 in Figure 1. For example, the E21 column has counts for the number of times the Arg2 of T1 was determined to be the same as the Arg1 of T2. Implementation and Query Language: We populated the Rel-grams database using OLLIE extractions from a set of 1.8 Million New York Times articles drawn from the Gigaword corpus. The database consisted of approximately 320K tuples that have frequency ≥ 3 and 1.1M entries in the bigram tab"
D13-1178,D12-1048,1,0.27349,"at do not make sense in the real world. For example, the assertions “fire caused virus” and “bacteria burned AIDS” are implicit in Table 2. Another limitation in schemas Chambers released is that they restrict schemas to two actors, which can result in combining different actors. Table 4 shows an example of combining perpeterators and victims into a single actor. 1.1 Contributions We present an event schema induction algorithm that overcomes these weaknesses. Our basic representation is triples of the form (Arg1, Relation, Arg2), extracted from a text corpus using Open Information Extraction (Mausam et al., 2012). The use of triples aids in agreement between subject and object of a relation. The use of Open IE leads to more expressive relation phrases (e.g., with prepositions). We also assign semantic types to arguments, both to alleviate data sparsity and to produce coherent actors for our schemas. Table 1 shows an event schema generated by our system. It has six relations and seven actors. The schema makes several related assertions about a person using a drug, failing a test, and getting suspended. The main actors in the schema include the person who failed the test, the drug used, and the agent th"
D13-1178,D09-1016,0,0.0611554,"all Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community. 1 Introduction Event schemas (also known as templates or frames) have been widely used in information extraction. An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event. They provide essential guidance in extracting information related to events from free text (Patwardhan and Riloff, 2009), and can also aid in other NLP tasks, such as coreference (Irwin et al., 2011), summarization (Owczarzak and Dang, 2010), and inference about temporal ordering and causality. Until recently, all event schemas in use in NLP were hand-engineered, e.g., the MUC templates and ACE event relations (ARPA, 1991; ARPA, 1998; Doddington et al., 2004). This led to technology that could only focus on specific domains of interest and has not been applicable more broadly. The seminal work of Chambers and Jurafsky (2009) showed that event schemas can also be induced automatically from text corpora. Instead"
D13-1178,D08-1027,0,0.0407593,"Missing"
D13-1178,E09-1005,0,\N,Missing
D13-1178,P08-1000,0,\N,Missing
D14-1058,P09-1010,0,0.036039,"at https://www.cs. washington.edu/nlp/arithmetic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent w"
D14-1058,P12-1014,1,0.71329,"hington.edu/nlp/arithmetic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equati"
D14-1058,P09-1068,0,0.0400928,"tural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the othe"
D14-1058,de-marneffe-etal-2006-generating,0,0.0379725,"Missing"
D14-1058,D09-1100,0,0.00681362,"ms, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consis"
D14-1058,P05-1045,0,0.0111166,"S automatically identifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments hw1 , . . . , wT , wx i where every wt consists of a verb vt , an entity et , its quantity numt , its attributes at , and up to two containers ct1 , ct2 . wx corresponds to the question sentence inquiring about an unknown entity. A RIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their Attributes: A RIS selects attributes A as modifiers for every entity from the dependency parser (details in Figure 3 step 1a). For example black is an attribute of the entity kitten and is an adjective modifier in the parser. These attributes are 526 1. Grounding into entities and containers: for every problem p in dataset (Section 4.1) (a) he1 , . . . , eT , ex ip ← extract all entities and the question entity i. Extract all numbers and noun phrases (NP)."
D14-1058,D09-1001,0,0.00705295,"addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, container"
D14-1058,P06-2034,0,0.075461,"sy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states i"
D14-1058,D10-1048,0,0.00825614,"ifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments hw1 , . . . , wT , wx i where every wt consists of a verb vt , an entity et , its quantity numt , its attributes at , and up to two containers ct1 , ct2 . wx corresponds to the question sentence inquiring about an unknown entity. A RIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their Attributes: A RIS selects attributes A as modifiers for every entity from the dependency parser (details in Figure 3 step 1a). For example black is an attribute of the entity kitten and is an adjective modifier in the parser. These attributes are 526 1. Grounding into entities and containers: for every problem p in dataset (Section 4.1) (a) he1 , . . . , eT , ex ip ← extract all entities and the question entity i. Extract all numbers and noun phrases (NP). ii. h ← all noun types whi"
D14-1058,P11-1149,0,0.0379957,"problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, containers, attributes, and quanti"
D14-1058,P10-1083,0,0.00746666,"tic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast w"
D14-1058,N13-1103,1,0.644952,"tegorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, containers, attributes, and quantities. Entities: An entity is"
D14-1058,P14-1026,1,0.700373,"in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while A RIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, A RIS is much more robust to diversity in the problem types between the training and test data. a set of entities, their containers, attributes, quantities, and relations. A problem text is split into fragments where each fragment cor"
D14-1058,W04-0902,0,0.154997,"from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while A RIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, A RIS is much more robust to diversity in the problem types between the training and test data. a set of entities, their containers, attributes, quantities, and relations. A problem text is split"
D14-1058,P09-1011,0,0.0251498,"NLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previou"
D14-1058,D13-1029,1,\N,Missing
D15-1080,P14-1114,0,0.0310312,"Missing"
D15-1080,D09-1001,0,0.0215396,"Missing"
D15-1080,S13-1002,0,\N,Missing
D15-1080,P07-2009,0,\N,Missing
D15-1171,D14-1059,0,0.0195865,"tions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the"
D15-1171,Q13-1005,0,0.0108256,"allenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the corresponding text has attracted attention in both vision and NLP (Farhadi et al."
D15-1171,P14-1133,0,0.0123141,". Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition"
D15-1171,D13-1160,0,0.11577,"Missing"
D15-1171,P12-1014,0,0.0126973,"ski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Castin"
D15-1171,P14-1134,0,0.0153331,"5 “A tangent line is drawn to circle O with radius of 5” Figure 4: Hypergraph representation of the sentence “A tangent line is drawn to circle O with radius of 5”. als L and the question text t. This score is the sum of the affinity scores P of individual literals lj ∈ L i.e., Atext (L, t) = j Atext (lj , t) where Atext (lj , t) 7→ [−∞, 0].3 G EO S learns a discriminative model Atext (lj , t; θ) that scores the affinity of every literal lj ∈ L and the question text t through supervised learning from training data. We represent literals using a hypergraph (Figure 4) (Klein and Manning, 2005; Flanigan et al., 2014). Each node in the graph corresponds to a concept in the geometry language (i.e. constants, variables, functions, or predicates). The edges capture the relations between concepts; concept nodes are connected if one concept is the argument of the other in the geometry language. In order to interpret the question text (Figure 3 step 1), G EO S first identifies concepts evoked by the words or phrases in the input text. Then, it learns the affinity scores which are the weights of edges in the hypergraph. It finally completes relations so that type matches are satistfied in the formal language. 4.1"
D15-1171,P06-2034,0,0.165077,"Missing"
D15-1171,D13-1029,1,0.84185,"Missing"
D15-1171,D14-1082,0,0.00336714,"etup Logical Language Ω: Ω consists of 13 types of entities and 94 function and predicates observed in our development set of geometry questions. Implementation details: Sentences in geometry questions often contain in-line mathematical expressions, such as “If AB=x+5, what is x?”. These mathematical expressions cause general purpose parsers to fail. G EO S uses an equation analyzer and pre-processes question text by replacing “=” with “equals”, and replacing mathematical terms (e.g., “x+5”) with a dummy noun so that the dependency parser does not fail. G EO S uses Stanford dependency parser (Chen and Manning, 2014) to obtain syntactic information, which is used to compute features for relation identification (Table 1). For diagram parsing, similar to Seo et al. (2014), we assume that G EO S has access to ground truth optical character recognition for labels in the diagrams. For optimization, we tune the parameters λ to 0.5, based on the training examples.4 Dataset: We built a dataset of SAT plane geometry questions where every question has a tex4 In our dataset, the number of all possible literals for each sentence is at most 1000. 1472 Questions Sentences Words Literals Binary relations Unary relations"
D15-1171,N15-1086,1,0.11084,"red to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the corresponding text"
D15-1171,P04-1054,0,0.0104618,"oney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., c"
D15-1171,D14-1058,1,0.463238,"correct a) 15 b) 30 c) 45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential information absent from questio"
D15-1171,D09-1100,0,0.0429332,"Missing"
D15-1171,P13-1022,0,0.0100165,"vailable geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instruction"
D15-1171,D14-1043,1,0.835908,"geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsi"
D15-1171,P14-1026,0,0.380456,"what, MeasureOf(BAC)) correct a) 15 b) 30 c) 45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential informat"
D15-1171,D13-1161,0,0.042558,"ude: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al.,"
D15-1171,P09-1011,0,0.00771506,"tical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be genera"
D15-1171,D09-1001,0,0.00968221,"irst results of this kind. Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of ground"
D15-1171,Q14-1030,0,0.0116589,"lementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jura"
D15-1171,Q15-1001,0,0.0621838,"45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential information absent from question text. In Figure 1"
D15-1171,P10-1083,0,0.00953773,"et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation probl"
E14-4003,P08-1004,1,0.944569,"lves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis – meaning i"
E14-4003,D11-1142,1,0.917387,"n2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relation Extraction (CORE) system"
E14-4003,W00-1205,0,0.017299,"Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists o"
E14-4003,W12-0702,0,0.0486469,"(‘Apples nutritious’), “ 蘋 果 是 營 養 豐 富 的 ” (‘Apples are nutritious’), and “蘋果富含營養” 12 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–16, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics (‘Apples are rich in nutrition’) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one’s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. This paper introduces the Chinese Open Relation Extraction"
E14-4003,P10-1013,0,0.482828,", Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relat"
E14-4003,W04-1116,0,0.0178753,"riple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and"
E14-4003,W12-6338,0,0.0116822,"correctly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two"
E14-4003,C02-1049,0,0.0327284,"source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner. Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applica"
E14-4003,O04-2005,0,0.0104221,"nese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese. invented the light bulb’) for instance. It was segmented and tagged as follows: 愛迪生/Nb 發 明/VC 了/Di 燈泡/Na. Among these words, t"
E14-4003,W12-6335,1,0.892043,"ith other character, such as “了解” meaning “understand”. 會/Nc, 的/DE, 民主黨/Nb, 星期一/Nd, 發怖/VE, Therefore, “ 愛 迪 生 發 明 了 解 藥 ” (‘Edison 報 告 /Na. Next, “ 星 期 一 /Nd 發 佈 /VE” is invented a cure’) would be segmented incorrectly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, whic"
H05-1043,P97-1023,0,0.586036,"w, v), o(v, w0 ) ∃v s.t. m(w, v), o(w0 , v) 3. Given the set of SO labels for (w, f ) pairs, OPINE finds a SO label for each (w, f , s) input tuple. Each of these subtasks is cast as an unsupervised collective classification problem and solved using the same mechanism. In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of labels to objects. In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)). A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g., model-based matching). OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983), in order to solve the three subtasks described above. Table 5: Dependency Rule Templates For Finding Words 3.3.3 w, w’ with Related SO Labels . OPINE instantiates these templates in order to obtain extraction rules. Notation"
H05-1043,C04-1200,0,0.357858,"Missing"
H05-1043,W03-0404,0,0.350402,"c approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE’s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997). Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper’s global optimization approach and use of multiple sources of constraints on a word’s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE’s"
H05-1043,W04-3221,0,0.0521553,"improves on both. (Hu and Liu, 2004) doesn’t assess candidate features, so its precision is lower than OPINE’s. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE’s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997). Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper’s global optimization approach and use of multiple sources of constraints on a word’s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; P"
H05-1043,P99-1008,0,0.0515095,"eviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn’t assess candidate features, so its precision is lower than OPINE’s. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE’s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997). Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper’s global optimization approach and use of multiple sources of constraints on a word’s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phra"
H05-1043,W02-1011,0,0.0766489,"Missing"
H05-1043,P05-1017,0,0.451614,"esses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE’s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997). Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper’s global optimization approach and use of multiple sources of constraints on a word’s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE’s focus has been on extracting and analyzing opinion phrases corresponding to specific feat"
H05-1043,P02-1053,0,0.130441,"to the tuple (head(o), f, s) (3.3.6 shows how OPINE takes into account negation modifiers). 3.4 Experiments In this section we evaluate OPINE’s performance on the following tasks: finding SO labels of words in the context of known features and sentences (SO label extraction); distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarity of extracted opinion phrases in the context of known features and sentences (opinion phrase polarity extraction). While other systems, such as (Hu and Liu, 2004; Turney, 2002), have addressed these tasks to some degree, OPINE is the first to report results. We first ran OPINE on 13841 sentences and 538 previously extracted features. OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences. We compared OPINE against two baseline methods, PMI++ and Hu++. PMI++ is an extended version of (Turney, 2002)’s method for finding the SO label of a phrase (as an attempt to deal with context-sensitive words). For a given (word, feature, sentence) tuple, PMI++ ignores the sentence, generates a phrase based on the word a"
H05-1043,C00-1044,0,\N,Missing
H05-1043,H05-1044,0,\N,Missing
H05-1043,P93-1023,0,\N,Missing
H05-1043,C04-1121,0,\N,Missing
H05-1071,C92-2082,0,0.0212884,"number of daily queries a program can issue to the API. Other search engines have also introduced mechanisms to limit programmatic queries, forcing applications to introduce “courtesy waits” between queries and to limit the number of queries they issue. To understand these efficiency problems in more detail, consider the K NOW I TA LL information extraction system (Etzioni et al., 2005). K NOW I TA LL has a generateand-test architecture that extracts information in two stages. First, K NOW I TA LL utilizes a small set of domainindependent extraction patterns to generate candidate facts (cf. (Hearst, 1992)). For example, the generic pattern “NP1 such as NPList2” indicates that the head of each simple noun phrase (NP) in NPList2 is a member of the class named in NP1. By instantiating the pattern for class City, K NOW I TA LL extracts three candidate cities from the sentence: “We provide tours to cities such as Paris, London, and Berlin.” Note that it must also fetch each document that contains a potential candidate. Next, extending the PMI-IR algorithm (Turney, 2001), K NOW I TA LL automatically tests the plausibility of the candidate facts it extracts using pointwise mutual information (PMI) st"
H05-1071,M92-1036,0,0.0277625,"time analysis) would remain the same. We thus expect that with a larger corpus we can construct a K NOW I T N OW system that reproduces K NOW I TA LL levels of precision and recall while still executing in the order of a few minutes. 5 Related Work There has been very little work published on how to make NLP computations such as PMI-IR and IE fast for large corpora. Indeed, extraction rate is not a metric typically used to evaluate IE systems, but we believe it is an important metric if IE is to scale. Hobbs et al. point out the advantage of fast text processing for rapid system development (Hobbs et al., 1992). They could test each change to system parameters and domain-specific patterns on a large sample of documents, having moved from a system that took 36 hours to process 100 documents to FASTUS, which took only 11 minutes. This allowed them to develop one of the highest performing MUC-4 systems in only one month. While there has been extensive work in the IR and Web communities on improvements to the standard inverted index scheme, there has been little work on efficient large-scale search to support natural language applications. One exception is Resnik’s Linguist’s Search Engine (Elkiss and R"
H05-1071,P02-1053,0,0.0295688,"rmation about its indexing system, but the user manual suggests its corpus is a combination of indexed sentences and user-specific document collections driven by the user’s AltaVista queries. In contrast, the BE system has a single index, constructed just once, that serves all queries. There is no published performance data available for Resnik’s system. 6 Conclusions and Future Directions In previous work, statistical NLP computation over large corpora has been a slow, offline process, as in K NOWI TA LL (Etzioni et al., 2005) and also in PMI-IR applications such as sentiment classification (Turney, 2002). Technology trends, and open source search engines such as Nutch, have made it feasible to create “private” search engines that index large collections of documents; but as shown in Figure 2, firing large numbers of queries at private search engines is still slow. This paper described a novel and practical approach towards substantially speeding up IE. We described K NOW I T N OW, which extracts thousands of facts in minutes instead of days. Furthermore, we sketched URNS, a probabilistic model that both obviates the need for search-engine queries and outputs more accurate probabilities than P"
H05-2017,C00-1044,0,0.610529,"Missing"
H05-2017,H05-1044,0,\N,Missing
H05-2017,W03-0404,0,\N,Missing
H05-2017,C04-1200,0,\N,Missing
H05-2017,P02-1053,0,\N,Missing
H05-2017,P99-1008,0,\N,Missing
H05-2017,W02-1011,0,\N,Missing
H05-2017,P93-1023,0,\N,Missing
H05-2017,P97-1023,0,\N,Missing
H05-2017,W04-3221,0,\N,Missing
H05-2017,C04-1121,0,\N,Missing
H05-2017,P05-1017,0,\N,Missing
N07-1016,P90-1034,0,0.222533,"theirs, but we make many of the same modeling assumptions. Second, we follow Snow et al.’s work (2006) on taxonomy induction in incorporating transitive closure constraints in our probability calculations, as explained below. 3 Probabilistic Model Our probabilistic model provides a formal, rigorous method for resolving synonyms in the absence of training data. It has two sources of evidence: the similarity of the strings themselves (i.e., edit distance) and the similarity of the assertions they appear in. This second source of evidence is sometimes referred to as “distributional similarity” (Hindle, 1990). Section 3.2 presents a simple model for predicting whether a pair of strings co-refer based on string similarity. Section 3.3 then presents a model called the Extracted Shared Property (ESP) Model for predicting whether a pair of strings co-refer based on their distributional similarity. Finally, a method is presented for combining these models to come up with an overall prediction for coreference decisions between two clusters of strings. 3.1 Terminology and Notation We use the following notation to describe the probabilistic models. The input is a data set D containing extracted assertions"
N07-1016,W97-0319,0,0.0256258,"ample, many (Dong et al., 2005; Bhattacharya and Getoor, 2005; Bhattacharya and Getoor, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is extremely rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. See (Winkler, 1999) for a survey of this area. Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum and Wellner, 2004; Singla and Domingos, 2006), but of course these systems depend on the availability of training data, and often on a significant number of labeled examples per relation of interest. These approaches also depend on complex probabilistic models and learning algorithms, and they have order O(n3 ) time complexity, or worse. They currently do not scale to the amounts of data extracted from the Web. Previous systems were tested on at most a few thousand examples, compared with millions or hundreds of millions of extractions from WIE systems such as T EXT RUNNER. Corefere"
N07-1016,J94-4002,0,0.0227887,"ingla and Domingos, 2006), but of course these systems depend on the availability of training data, and often on a significant number of labeled examples per relation of interest. These approaches also depend on complex probabilistic models and learning algorithms, and they have order O(n3 ) time complexity, or worse. They currently do not scale to the amounts of data extracted from the Web. Previous systems were tested on at most a few thousand examples, compared with millions or hundreds of millions of extractions from WIE systems such as T EXT RUNNER. Coreference resolution systems (e.g., (Lappin and Leass, 1994; Ng and Cardie, 2002)), like SR systems, try to merge references to the same object (typically pronouns, but potentially other types of noun phrases). This problem differs from the SR problem in several ways: first, it deals with unstructered text input, possibly with syntactic annotation, rather than relational input. Second, it deals only with resolving objects. Finally, it requires local decisions about strings; that is, the same word may appear twice in a text and refer to two different things, so each occurrence of a word must be treated separately. The PASCAL Recognising Textual Entailm"
N07-1016,P02-1014,0,0.00501146,"), but of course these systems depend on the availability of training data, and often on a significant number of labeled examples per relation of interest. These approaches also depend on complex probabilistic models and learning algorithms, and they have order O(n3 ) time complexity, or worse. They currently do not scale to the amounts of data extracted from the Web. Previous systems were tested on at most a few thousand examples, compared with millions or hundreds of millions of extractions from WIE systems such as T EXT RUNNER. Coreference resolution systems (e.g., (Lappin and Leass, 1994; Ng and Cardie, 2002)), like SR systems, try to merge references to the same object (typically pronouns, but potentially other types of noun phrases). This problem differs from the SR problem in several ways: first, it deals with unstructered text input, possibly with syntactic annotation, rather than relational input. Second, it deals only with resolving objects. Finally, it requires local decisions about strings; that is, the same word may appear twice in a text and refer to two different things, so each occurrence of a word must be treated separately. The PASCAL Recognising Textual Entailment Challenge proposes"
N07-1016,P06-1101,0,0.00956235,"this simplifying assumption, we can combine the evidence to find the probability of a coference relationship by applying Bayes’ Rule to both sides (we omit the i, j indices for brevity): P (Rt |E s , E e ) = By our assumptions, (1) nj P (Rt |E s )P (Rt |E e )(1 − P (Rt )) i s i e i i∈{t,f } P (R |E )P (R |E )(1 − P (R )) P 3.5 Comparing Clusters of Strings Our algorithm merges clusters of strings with one another, using one of the above models. However, these models give probabilities for coreference decisions between two individual strings, not two clusters of strings. We follow the work of Snow et al. (2006) in incorporating transitive closure constraints in probabilistic modeling, and make the same independence assumptions. The benefit of this approach is that the calculation for merging two clusters depends only on coreference decisions between individual strings, which can be calculated independently. Let a clustering be a set of coreference relationships between pairs of strings such that the coreference relationships obey the transitive closure property. We let the probability of a set of assertions D given a clustering C be: P (D|C) = Y t P (Di ∪ Dj |Ri,j )× t ∈C Ri,j Y S := set of all stri"
N07-4013,N06-1038,0,0.00627965,"ration of the TextRunner system shows the results of performing OIE on a set of 117 million web pages. It demonstrates the power of TextRunner in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a fast user interface for querying the results. 2 Previous Work The bulk of previous information extraction work uses hand-labeled data or hand-crafted patterns to enable relation-specific extraction (e.g., (Culotta et al., 2006)). OIE seeks to avoid these requirements for human input. Shinyama and Sekine (Shinyama and Sekine, 2006) describe an approach to “unrestricted relation discovery” that does away with many of the requirements for human input. However, it requires clustering of the documents used for extraction, and thus scales in quadratic time in the number of documents. It does not scale to the size of the Web. For a full discussion of previous work, please see (Banko et al., 2007), or see (Yates and Etzioni, 2007) for work relating to synonym resolution. 3 Open IE in TextRunner OIE presents significant new"
N07-4013,N06-1039,0,0.0200679,"s. It demonstrates the power of TextRunner in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a fast user interface for querying the results. 2 Previous Work The bulk of previous information extraction work uses hand-labeled data or hand-crafted patterns to enable relation-specific extraction (e.g., (Culotta et al., 2006)). OIE seeks to avoid these requirements for human input. Shinyama and Sekine (Shinyama and Sekine, 2006) describe an approach to “unrestricted relation discovery” that does away with many of the requirements for human input. However, it requires clustering of the documents used for extraction, and thus scales in quadratic time in the number of documents. It does not scale to the size of the Web. For a full discussion of previous work, please see (Banko et al., 2007), or see (Yates and Etzioni, 2007) for work relating to synonym resolution. 3 Open IE in TextRunner OIE presents significant new challenges for information extraction systems, including Automation of relation extraction, which in trad"
N07-4013,N07-1016,1,0.725663,"rk uses hand-labeled data or hand-crafted patterns to enable relation-specific extraction (e.g., (Culotta et al., 2006)). OIE seeks to avoid these requirements for human input. Shinyama and Sekine (Shinyama and Sekine, 2006) describe an approach to “unrestricted relation discovery” that does away with many of the requirements for human input. However, it requires clustering of the documents used for extraction, and thus scales in quadratic time in the number of documents. It does not scale to the size of the Web. For a full discussion of previous work, please see (Banko et al., 2007), or see (Yates and Etzioni, 2007) for work relating to synonym resolution. 3 Open IE in TextRunner OIE presents significant new challenges for information extraction systems, including Automation of relation extraction, which in traditional information extraction uses handlabeled inputs. Corpus Heterogeneity on the Web, which makes tools like parsers and named-entity taggers less accurate because the corpus is different from the data used to train the tools. Scalability and efficiency of the system. Open IE systems are effectively restricted to a single, fast pass over the data so that they can scale to huge document collecti"
N13-1136,P11-1051,0,0.0250394,"ght CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while our approach performs joint selection and ordering. Discourse models have also been used for evaluating summary quality (Barzilay and Lapata, 2008; Louis and Nenkova, 2009; Pitler et al., 2010). Finally, there is work on generating coherent summaries in specific domains, such as scientific articles (Saggion and Lapalme, 2002; Abu-Jbara and Radev, 2011) using domain-specific cues like citations. In contrast, our work generates summaries without any domain-specific knowledge. Other research has focused on identifying coherent threads of documents rather than sentences (Shahaf and Guestrin, 2010). 3 Discourse Graph As described in Section 1, our goal is to identify pairwise ordering constraints over a set of input sentences. These constraints specify a multi-document discourse graph, which is used by G-F LOW to evaluate the coherence of a candidate summary. In this graph G, each vertex is a sentence and an edge from si to sj indicates that sj"
N13-1136,D10-1047,0,0.020387,"Missing"
N13-1136,W97-0703,0,0.0626535,"reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an"
N13-1136,J08-1001,0,0.657666,"doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joint model. 1 doc2: Hamas clai"
N13-1136,H01-1065,0,0.0217437,"nce as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespec"
N13-1136,P10-1084,0,0.0265154,"Missing"
N13-1136,chang-manning-2012-sutime,0,0.00654009,"m the New York Times and Reuters, and filter out candidate pairs scoring below a threshold identified over a small training set. We construct edges in the ADG between pairs of sentences containing these verb to noun mappings. To our knowledge, we are the first to use deverbal nouns for summarization. 3.2 Event/Entity Continuation Our second indicator is related to lexical chains (Barzilay and Lapata, 2008). We add an edge in the ADG from a sentence si to sj if they contain the same event or entity and the timestamp of si is less than or equal to the timestamp of sj (timestamps generated with (Chang and Manning, 2012)). 3.3 Discourse Markers We use 36 explicit discourse markers (e.g., ‘but’, ‘however’, ‘moreover’) to identify edges between two adjacent sentences of a document (Marcu and Echihabi, 2002). This indicator lets us learn an edge from s4 to s5 below: s4 Arafat condemned the bombing. s5 However, Netanyahu suspended peace talks. 3.4 Inferred Edges We exploit the redundancy of information in MDS documents to infer edges to related sentences. An edge (s, s00 ) can be inferred if there is an existing edge (s, s0 ) and s0 and s00 express similar information. As an example, the edge (s6 , s7 ) can be in"
N13-1136,J86-3001,0,0.646458,"out regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure The"
N13-1136,N09-1041,0,0.376363,": Suspension of peace accord due to bombing doc4: Mubarak urges peace accord doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering c"
N13-1136,P03-1069,0,0.0209978,"peace accord doc3: Clinton urges peace accord Figure 1: An example of a discourse graph covering a bombing and its aftermath, indicating the source document for each node. A coherent summary should begin with the bombing and then describe the reactions. Sentences are abbreviated for compactness. Introduction The goal of multi-document summarization (MDS) is to produce high quality summaries of collections of related documents. Most previous work in extractive MDS has studied the problems of sentence selection (e.g., (Radev, 2004; Haghighi and Vanderwende, 2009)) and sentence ordering (e.g., (Lapata, 2003; Barzilay and Lapata, 2008)) separately, but we believe that a joint model is necessary to produce coherent summaries. The intuition is simple: if the sentences in a summary are first selected—without regard to coherence—then a satisfactory ordering of the selected sentences may not exist. 1 doc5: Palestinians condemn attack doc1: Anger from Israelis We evaluate G-F LOW on Mechanical Turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joi"
N13-1136,W11-1902,0,0.0178603,"Missing"
N13-1136,I11-1118,0,0.126842,"Missing"
N13-1136,D11-1105,0,0.0661184,"Missing"
N13-1136,P11-1052,0,0.215437,"se colony occupied by Indonesia since 1975. • Indonesia invaded East Timor in 1975 and annexed it the following year. • Thailand won host rights for the quadrennial games in 1995, but setbacks in preparations led officials of the Olympic Council of Asia late last year to threaten to move the games to another country. • Thailand showed its nearly complete facilities for the Asian Games to a tough jury Thursday - the heads of the organizing committees from the 43 nations competing in the December event. Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and sentence orderer (Li et al., 2011a), and by G-F LOW. graph to estimate coherence of a candidate summary. Second, G-F LOW introduces a novel methodology for joint sentence selection and ordering (Section 4). It casts MDS as a constraint optimization problem where salience and coherence are soft constraints, and redundancy and summary length are hard constraints. Because this optimization problem is NPhard, G-F LOW uses local search to approximate it. We report on a Mechanical Turk evaluation that directly compares G-F LOW to state-of-the-art MDS systems. Using DUC’04 as our test set, we co"
N13-1136,W04-1013,0,0.0207662,"d Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et"
N13-1136,W10-4310,0,0.0120046,"tion, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an instance of summariza"
N13-1136,W01-0100,0,0.335287,"ive of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation"
N13-1136,P02-1047,0,0.0191987,"ontaining these verb to noun mappings. To our knowledge, we are the first to use deverbal nouns for summarization. 3.2 Event/Entity Continuation Our second indicator is related to lexical chains (Barzilay and Lapata, 2008). We add an edge in the ADG from a sentence si to sj if they contain the same event or entity and the timestamp of si is less than or equal to the timestamp of sj (timestamps generated with (Chang and Manning, 2012)). 3.3 Discourse Markers We use 36 explicit discourse markers (e.g., ‘but’, ‘however’, ‘moreover’) to identify edges between two adjacent sentences of a document (Marcu and Echihabi, 2002). This indicator lets us learn an edge from s4 to s5 below: s4 Arafat condemned the bombing. s5 However, Netanyahu suspended peace talks. 3.4 Inferred Edges We exploit the redundancy of information in MDS documents to infer edges to related sentences. An edge (s, s00 ) can be inferred if there is an existing edge (s, s0 ) and s0 and s00 express similar information. As an example, the edge (s6 , s7 ) can be inferred based on edge (s4 , s5 ): s6 Arafat condemned the attack. s7 Netanyahu has suspended the talks. To infer edges we need an algorithm to identify sentences expressing similar informat"
N13-1136,W97-0713,0,0.102958,"ry, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST"
N13-1136,D12-1048,1,0.239931,"he complete list of features and learned weights is in Table 2. The classifier finds a sentence more salient if it mentions nouns or verbs that are present in more sentences across the documents. The highest ranked features are the last three – number of other sentences that mention a noun or a verb in the given sentence. We use the same procedure as in deverbal nouns for detecting verb mentions that appear as nouns in other sentences (Section 3.1). 4.3 Redundancy We also wish to avoid redundancy. G-F LOW first processes each sentence with a state-of-the-art Open Information extractor O LLIE (Mausam et al., 2012), which converts a sentence into its component relational tuples of the form (arg1, relational phrase, arg2).3 For example, it finds (Militants, bombed, a marketplace) as a tuple from sentence s12 . Two sentences will express redundant information if they both contain the same or synonymous component fact(s). Unfortunately, detecting synonymy even at relational tuple level is very hard. G-F LOW approximates this synonymy by considering two relational tuples synonymous if the relation phrases contain verbs that are synonyms of each other, have at least one synonymous argument, and are timestamp"
N13-1136,C04-1108,0,0.0113353,"erata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succee"
N13-1136,P10-1056,0,0.0289057,"Missing"
N13-1136,prasad-etal-2008-penn,0,0.31003,"selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge an"
N13-1136,C10-1101,0,0.0448864,"Missing"
N13-1136,J98-3005,0,0.0243244,"ry, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang et al., 2002; Jorge and Pardo, 2010). However, these systems require a stronger input, such as a manual CST-annotation of the set of documents. Our work can be seen as an instance of summarization based on lightweight CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while"
N13-1136,J02-4005,0,0.0424646,"arization based on lightweight CST. However, a key difference is that our proposed algorithm is completely automated and does not require any additional human annotation. Additionally, while incorporating coherence into selection, this work does not attempt to order the sentences coherently, while our approach performs joint selection and ordering. Discourse models have also been used for evaluating summary quality (Barzilay and Lapata, 2008; Louis and Nenkova, 2009; Pitler et al., 2010). Finally, there is work on generating coherent summaries in specific domains, such as scientific articles (Saggion and Lapalme, 2002; Abu-Jbara and Radev, 2011) using domain-specific cues like citations. In contrast, our work generates summaries without any domain-specific knowledge. Other research has focused on identifying coherent threads of documents rather than sentences (Shahaf and Guestrin, 2010). 3 Discourse Graph As described in Section 1, our goal is to identify pairwise ordering constraints over a set of input sentences. These constraints specify a multi-document discourse graph, which is used by G-F LOW to evaluate the coherence of a candidate summary. In this graph G, each vertex is a sentence and an edge from"
N13-1136,C10-1111,0,0.0779683,"of single documents. There is some preliminary work on the use of manually-created discourse models in MDS. Our approach is fully automated. 2.1 Subset Selection in MDS Most extractive summarization research aims to increase the coverage of concepts and entities while reducing redundancy. Approaches include the use of maximum marginal relevance (Carbonell and Goldstein, 1998), centroid-based summarization (Saggion and Gaizauskas, 2004; Radev et al., 2004), covering weighted scores of concepts (Takamura and Okumura, 2009; Qazvinian et al., 2010), formulation as minimum dominating set problem (Shen and Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the selected sentences into an intelligible summary. They are often evaluted by ROUGE (Lin, 2004),"
N13-1136,E09-1089,0,0.0316897,"riven summarization (Section 2.3). There is prior work in this area, but primarily for summarization of single documents. There is some preliminary work on the use of manually-created discourse models in MDS. Our approach is fully automated. 2.1 Subset Selection in MDS Most extractive summarization research aims to increase the coverage of concepts and entities while reducing redundancy. Approaches include the use of maximum marginal relevance (Carbonell and Goldstein, 1998), centroid-based summarization (Saggion and Gaizauskas, 2004; Radev et al., 2004), covering weighted scores of concepts (Takamura and Okumura, 2009; Qazvinian et al., 2010), formulation as minimum dominating set problem (Shen and Li, 2010), and use of submodularity in sentence selection (Lin and Bilmes, 2011). Graph centrality has also been used to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010; Li et al., 2011b), and discriminative models (Aker et al., 2010). These approaches do not consider coherence as one of the desiderata in sentence selection. Moreover, they do not attempt to organize the se"
N13-1136,J05-2005,0,0.0102645,"eads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coherence Models and Summarization Research on discourse analysis of documents provides a basis for modeling coherence in a document. Several theories have been developed for modeling discourse, e.g., Centering Theory, Rhetorical Structure Theory (RST), Penn Discourse Tree1165 Bank (Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005; Prasad et al., 2008). Numerous discourse-guided summarization algorithms have been developed (Marcu, 1997; Mani, 2001; Taboada and Mann, 2006; Barzilay and Elhadad, 1997; Louis et al., 2010). However, these approaches have been applied to single document summarization and not to MDS. Discourse models have seen some application to summary generation in MDS, for example, using a detailed semantic representation of the source texts (McKeown and Radev, 1995; Radev and McKeown, 1998). A multi-document extension of RST is Cross-document Structure Theory (CST), which has been applied to MDS (Zhang"
N13-1136,C10-2170,0,0.0148097,"ry. They are often evaluted by ROUGE (Lin, 2004), which is coherence-insensitive. In practice, these approaches often result in incoherent summaries. 2.2 Sentence Reordering A parallel thread of research has investigated taking a set of summary sentences as input and reordering them to make the summary fluent. Various algorithms use some combination of topic-relatedness, chronology, precedence, succession, and entity coherence for reordering sentences (Barzilay et al., 2001; Okazaki et al., 2004; Barzilay and Lapata, 2008; Bollegala et al., 2010). Recent work has also used event-based models (Zhang et al., 2010) and context analysis (Li et al., 2011a). The hypothesis in this research is that a pipelined combination of subset selection and reordering will produce high-quality summaries. Unfortunately, this is not true in practice, because sentences are selected primarily for coverage without regard to coherence. This methodology often leads to an inadvertent selection of a set of disconnected sentences, which cannot be put together in a coherent summary, irrespective of how the succeeding algorithm reorders them. In our evaluation, reordering had limited impact on the quality of the summaries. 2.3 Coh"
N13-1136,D09-1032,0,\N,Missing
N18-3011,S17-2097,1,0.733321,"et al., 2014). We also compute context-sensitive word embeddings, denoted as lmk D Œlm! k I lmk , by concatenating the projected outputs of forward and backward recurrent neural network language models (RNN-LM) at position k. The language model (LM) for each direction is trained independently and consists of a single layer long short-term memory (LSTM) network followed by a linear project layer. While training the LM parameters, lm! k is used to predict tkC1 and lmk is used to predict tk 1 . We fix the LM parameters during training of the entity extraction model. See Peters et al. (2017) and Ammar et al. (2017) for more details. Given the xk and lmk embeddings for each token k 2 f1; : : : ; N g, we use a two-layer bidirectional LSTM to encode the sequence with xk and lmk feeding into the first and second layer, respectively. That is, ! ! g! k D LSTM.xk ; gk 1 /; gk D Œgk I gk ; Description Without LM With LM Avg. of 15 models with LM F1 49.9 54.1 55.2 Table 3: Results of the entity extraction model on the development set of SemEval-2017 task 10. with a similar architecture, but trained on different datasets. Two instances are trained on the BC5CDR (Li et al., 2016) and the CHEMDNER datasets (Kralli"
N18-3011,N16-1030,0,0.0752076,"Missing"
N18-3011,S17-2091,0,0.0429031,"order to exclude irrelevant entities, e.g., “Lord of the Rings” in DBpedia. ! ! h! k D LSTM.Œgk I lmk ; hk 1 /; hk D Œhk I hk ; where gk and hk are defined similarly to g! and k h! but process token sequences in the opposite k direction. Similar to the model described in §3, we feed the output of the second LSTM into a dense layer to predict unnormalized label weights for each token and learn label bigram feature weights to account for dependencies between labels. Results. We use the standard data splits of the SemEval-2017 Task 10 on entity (and relation) extraction from scientific papers (Augenstein et al., 2017). Table 3 compares three variants of our entity extraction model. The first line omits the LM embeddings lmk , while the second line is the full model (including LM embeddings) showing a large improvement of 4.2 F1 points. The third line shows that creating an ensemble of 15 models further improves the results by 1.1 F1 points. Model instances. In the deployed system, we use three instances of the entity extraction model 4.4 Entity Linking Models Given a text span s identified by the entity extraction model in §4.2 (or with heuristics) and a reference KB, the goal of the entity linking model i"
N18-3011,N18-1022,1,0.790646,"le, in order to allow users to better understand what impact a paper had and effectively navigate its citations, we experimented with methods for classifying a citation as important or incidental, as well as more finegrained classes (Valenzuela et al., 2015). The citation information also enables us to develop models for estimating the potential of a paper or an author. In Weihs and Etzioni (2017), we predict citationbased metrics such as an author’s h-index and the citation rate of a paper in the future. Also related is the problem of predicting which papers should be cited in a given draft (Bhagavatula et al., 2018), which can help improve the quality of a paper draft before it is submitted for peer review, or used to supplement the list of references after a paper is published. 6 Conclusion and Future Work In this paper, we discuss the construction of a graph, providing a symbolic representation of the scientific literature. We describe deployed models for identifying authors, references and entities in the paper text, and provide experimental results to evaluate the performance of each model. Three research directions follow from this work and other similar projects, e.g., Hahn-Powell et al. (2017); Wu"
N18-3011,Q15-1023,0,0.0243296,"rc /; Baseline Neural CS Bio 84.2 84.6 54.2 85.8 Table 4: The Bag of Concepts F1 score of the baseline and neural model on the two curated datasets. earlier. We compute two scores based on the word overlap of (i) mention’s context and candidate’s definition and (ii) mention’s surface span and the candidate entity’s name. Finally, we feed the concatenation of the cosine similarity between f.m/ and g.e/ and the intersection-based scores into an affine transformation followed by a sigmoid nonlinearity to compute the final score for the pair (m, e). Results. We use the Bag of Concepts F1 metric (Ling et al., 2015) for comparison. Table 4 compares the performance of the most-frequent-entity baseline and our neural model described above. 5 Other Research Problems In the previous sections, we discussed how we construct the main components of the literature graph. In this section, we briefly describe several other related challenges we are actively working on. Author disambiguation. Despite initiatives to have global author IDs ORCID and ResearcherID, most publishers provide author information as names (e.g., arXiv). However, author names cannot be used as a unique identifier since several people often sha"
N18-3011,P09-1113,0,0.0473225,"in a hand-curated KB comes at the cost of limited coverage. Introduction of new concepts and relations in the scientific literature occurs at a faster pace than KB curation, resulting in a large gap in KB coverage of scientific concepts. In order to close this gap, we need to develop models which can predict textual relations as well as detailed concept descriptions in scientific papers. For the same reasons, we also need to augment the relations imported from the KB with relations extracted from text. Our approach to address both entity and relation coverage is based on distant supervision (Mintz et al., 2009). In short, we train two models for identifying entity definitions and relations expressed in natural language in scientific documents, and automatically generate labeled data for training these models using known definitions and relations in the KB. We note that the literature graph currently lacks coverage for important entity types (e.g., affiliations) and domains (e.g., physics). Covering affiliations requires small modifications to the metadata extraction model followed by an algorithm for matching author names with their affiliations. In order to cover additional scientific domains, more"
N18-3011,D14-1162,0,0.0823175,"Missing"
N18-3011,P07-1033,0,0.0326079,"c concept discussed in the literature, with attributes such as ‘canonical name’, ‘aliases’ and ‘description’. Our literature graph has 0.4M nodes of this type. We describe how we populate entity nodes in §4.3. Entity mentions. Each node of this type represents a textual reference of an entity in one of the papers, with attributes such as ‘mention text’, ‘context’, and ‘confidence’. We describe how we populate the 237M mentions in the literature graph in §4.1. 2016), and assume that entity types in the test set match those labeled in the training set (including work on domain adaptation, e.g., Daumé, 2007). These assumptions, while useful for developing and benchmarking new methods, are unrealistic for many domains and applications. The paper also serves as an overview of the approach we adopt at www.semanticscholar.org in a step towards more intelligent academic search engines (Etzioni, 2011). In the next section, we start by describing our symbolic representation of the literature. Then, we discuss how we extract metadata associated with a paper such as authors and references, then how we extract the entities mentioned in paper text. Before we conclude, we briefly describe other research chal"
N18-3011,P17-1161,1,0.763803,"e embeddings (Pennington et al., 2014). We also compute context-sensitive word embeddings, denoted as lmk D Œlm! k I lmk , by concatenating the projected outputs of forward and backward recurrent neural network language models (RNN-LM) at position k. The language model (LM) for each direction is trained independently and consists of a single layer long short-term memory (LSTM) network followed by a linear project layer. While training the LM parameters, lm! k is used to predict tkC1 and lmk is used to predict tk 1 . We fix the LM parameters during training of the entity extraction model. See Peters et al. (2017) and Ammar et al. (2017) for more details. Given the xk and lmk embeddings for each token k 2 f1; : : : ; N g, we use a two-layer bidirectional LSTM to encode the sequence with xk and lmk feeding into the first and second layer, respectively. That is, ! ! g! k D LSTM.xk ; gk 1 /; gk D Œgk I gk ; Description Without LM With LM Avg. of 15 models with LM F1 49.9 54.1 55.2 Table 3: Results of the entity extraction model on the development set of SemEval-2017 task 10. with a similar architecture, but trained on different datasets. Two instances are trained on the BC5CDR (Li et al., 2016) and the C"
N18-3011,P17-4018,0,0.0269059,"ft (Bhagavatula et al., 2018), which can help improve the quality of a paper draft before it is submitted for peer review, or used to supplement the list of references after a paper is published. 6 Conclusion and Future Work In this paper, we discuss the construction of a graph, providing a symbolic representation of the scientific literature. We describe deployed models for identifying authors, references and entities in the paper text, and provide experimental results to evaluate the performance of each model. Three research directions follow from this work and other similar projects, e.g., Hahn-Powell et al. (2017); Wu et al. (2014): i) improving quality and enriching content of the literature graph (e.g., ontology matching and knowledge base population). ii) aggregating domain-specific extractions across many papers to enable a better understanding of the literature as a whole (e.g., identifying demographic biases in clinical trial participants and summarizing empirical results on important tasks). iii) exploring the literature via natural language interfaces. In order to help future research efforts, we make the following resources publicly available: metadata for over 20 million papers,10 meaningful"
N18-3011,P17-1089,0,0.0141096,"hmic discovery in the scientific literature. Despite notable advances in scientific search engines, data mining and digital libraries (e.g., Wu et al., 2014), researchers remain unable to answer simple questions such as:  What is the percentage of female subjects in depression clinical trials?  Which of my co-authors published one or more papers on coreference resolution?  Which papers discuss the effects of Ranibizumab on the Retina? In this paper, we focus on the problem of extracting structured data from scientific documents, which can later be used in natural language interfaces (e.g., Iyer et al., 2017) or to improve ranking of results in academic search (e.g., Xiong et al., 84 Proceedings of NAACL-HLT 2018, pages 84–91 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics Entities. Each node of this type represents a unique scientific concept discussed in the literature, with attributes such as ‘canonical name’, ‘aliases’ and ‘description’. Our literature graph has 0.4M nodes of this type. We describe how we populate entity nodes in §4.3. Entity mentions. Each node of this type represents a textual reference of an entity in one of the papers, with attrib"
P07-1088,P06-1102,0,0.079051,"Missing"
P07-1088,P05-1077,0,0.0177657,"kes the simplifying assumption that each sentence in the corpus is generated independently. 699 are to those of the seed arguments.3 Specifically, we define a function: P 0 X w0 ∈SRi P (t|w ) , P (t|ei )) (2) f (e) = KL( |SRi | e ∈e i where KL represents KL divergence, and the outer sum is taken over the arguments ei of the extraction e. We rank the elements of UR in ascending order of f (e). H MM -T has two advantages over a more traditional type checking approach of simply counting the number of times in the corpus that each extraction appears in a context in which a seed also appears (cf. (Ravichandran et al., 2005)). The first advantage of H MM -T is efficiency, as the traditional approach involves a computationally expensive step of retrieving the potentially large set of contexts in which the extractions and seeds appear. In our experiments, using H MM -T instead of a context-based approach results in a 10-50x reduction in the amount of data that is retrieved to perform type checking. Secondly, on sparse data H MM -T has the potential to improve type checking accuracy. For example, consider comparing Pickerington, a sparse candidate argument of the type City, to the seed argument Chicago, for which th"
P08-1004,P07-1073,0,0.0611867,"Missing"
P08-1004,N04-4028,0,0.0290016,"tor, referred to as H- CRF. Treating the output of an O- CRF and R1- CRF as black boxes, H- CRF learns to predict which, if any, tokens found between a pair of entities (e1 , e2 ), indicates a relationship. Due to the sequential nature of our RE task, H- CRF employs a CRF as the metalearner, as opposed to a decision tree or regressionbased classifier. H- CRF uses the probability distribution over the set of possible labels according to each O- CRF and R1- CRF as features. To obtain the probability at each position of a linear-chain CRF, the constrained forward-backward technique described in (Culotta and McCallum, 2004) is used. H- CRF also computes the Monge Elkan distance (Monge and Elkan, 1996) between the relations predicted by O- CRF and R1CRF and includes the result in the feature set. An additional meta-feature utilized by H- CRF indicates whether either or both base extractors return “no relation” for a given pair of entities. In addition to these numeric features, H- CRF uses a subset of the base features used by O- CRF and R1- CRF. At each Category Verb Noun+Prep Verb+Prep Infinitive Other All P 93.9 89.1 95.2 95.7 0 88.3 O-CRF R 65.1 36.0 50.0 46.8 0 45.2 F1 76.9 51.3 65.6 62.9 0 59.8 P 100 100 95"
P08-1004,N06-1038,0,0.356077,"(Lafferty et al., 2001), are undirected graphical models trained to maximize the conditional probability of a finite set of labels Y given a set of input observations X. By making a first-order Markov assumption about the dependencies among the output variables Y , and arranging variables sequentially in a linear chain, RE can be treated as a sequence labeling problem. Linear-chain CRFs have been applied to a variety of sequential text processing tasks including named-entity recognition, part-of-speech tagging, word segmentation, semantic role identification, and recently relation extraction (Culotta et al., 2006). 3.1.1 Training As with O- NB, O- CRF’s training process is selfsupervised. O- CRF applies a handful of relationindependent heuristics to the PennTreebank and obtains a set of labeled examples in the form of relational tuples. The heuristics were designed to capture dependencies typically obtained via syntactic parsing and semantic role labelling. For example, a heuristic used to identify positive examples is the extraction of noun phrases participating in a subjectverb-object relationship, e.g., “<Einstein> received <the Nobel Prize> in 1921.” An example of a heuristic that locates negative"
P08-1004,J06-1005,0,0.057563,"Missing"
P08-1004,C92-2082,0,0.29372,"Missing"
P08-1004,P03-1054,0,0.0040614,"uiring any relation-specific human input. Open IE’s extraction process is linear in the number of documents in the corpus, and constant in the number of relations. Open IE is ideally suited to corpora such as the Web, where the target relations are not known in advance, and their number is massive. The relationship between standard RE systems and the new Open IE paradigm is analogous to the relationship between lexicalized and unlexicalized parsers. Statistical parsers are usually lexicalized (i.e. they make parsing decisions based on n-gram statistics computed for specific lexemes). However, Klein and Manning (2003) showed that unlexicalized parsers are more accurate than previously believed, and can be learned in an unsupervised manner. Klein and Manning analyze the tradeoffs beTraditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that"
P08-1004,W04-2401,0,0.0610151,"reduce the amount of effort involved when porting IE systems to new domains. Shinyama and Sekine’s “preemptive” IE system (2006) discovers relationships from sets of related news articles. Until recently, most work in RE has been carried out on a per-relation basis. Typically, RE is framed as a binary classification problem: Given a sentence S and a relation R, does S assert R between two entities in S? Representative approaches include (Zelenko et al., 2003) and (Bunescu and Mooney, 2005), which use support-vector machines fitted with language-oriented kernels to classify pairs of entities. Roth and Yih (2004) also described a classification-based framework in which they jointly learn to identify named entities and relations. Culotta et al. (2006) used a CRF for RE, yet their task differs greatly from open extraction. RE was performed from biographical text in which the topic of each document was known. For every entity found in the document, their goal was to predict what relation, if any, it had relative to the page topic, from a set of given relations. Under these restrictions, RE became an instance of entity labeling, where the label assigned to an entity (e.g. Father) is its relation to the to"
P08-1004,P06-2094,0,0.140069,"Missing"
P08-1004,N06-1039,0,0.59229,"Missing"
P08-1004,N07-1016,1,0.580564,"Missing"
P09-1030,P08-1088,0,0.0355916,"l in English. For each suggested translation they discussed the various senses of words in their respective languages and tag a translation correct if they found some sense that is shared by both words. For this study we tagged 7 language pairs: Hindi-Hebrew, English Wiktionary PanDictionary (0.90) PanDictionary (0.85) PanDictionary (0.70) # languages with distinct words ≥ 1000 ≥ 100 ≥1 49 107 505 67 146 608 75 175 794 107 607 1066 Table 1: PAN D ICTIONARY covers substantially more languages than the English Wiktionary. 268 lingual corpora, which may scale to several language pairs in future (Haghighi et al., 2008). Little work has been done in combining multiple dictionaries in a way that maintains word senses across dictionaries. Gollins and Sanderson (2001) explored using triangulation between alternate pivot languages in cross-lingual information retrieval. Their triangulation essentially mixes together circuits for all word senses, hence, is unable to achieve high precision. Dyvik’s “semantic mirrors” uses translation paths to tease apart distinct word senses from inputs that are not sense-distinguished (Dyvik, 2004). However, its expensive processing and reliance on parallel corpora would not scal"
P09-1030,2005.mtsummit-osmtw.3,0,0.0226508,". Because lexical translation does not require aligned corpora as input, it is feasible for a much broader set of languages than statistical Machine Translation (SMT). Of course, lexical translation cannot replace SMT, but it is useful for several applications including translating search-engine queries, library classifications, meta-data tags,2 and recent applications like cross-lingual image search (Etzioni et al., 2007), and enhancing multi-lingual Wikipedias (Adar et al., 2009). Furthermore, lexical translation is a valuable component in knowledge-based Machine Translation systems, e.g., (Bond et al., 2005; Carbonell et al., 2006). PAN D ICTIONARY currently contains over 200 million pairwise translations in over 200,000 language pairs at precision 0.8. It is constructed from information harvested from 631 online dictionaries and Wiktionaries. This necessitates matchIntroduction and Motivation In the era of globalization, inter-lingual communication is becoming increasingly important. Although nearly 7,000 languages are in use today (Gordon, 2005), most language resources are mono-lingual, or bi-lingual.1 This paper investigates whether Wiktionaries and other translation dictionaries available o"
P09-1030,P97-1063,0,0.129933,"Missing"
P09-1030,2006.amta-papers.3,0,0.0405188,"ranslation does not require aligned corpora as input, it is feasible for a much broader set of languages than statistical Machine Translation (SMT). Of course, lexical translation cannot replace SMT, but it is useful for several applications including translating search-engine queries, library classifications, meta-data tags,2 and recent applications like cross-lingual image search (Etzioni et al., 2007), and enhancing multi-lingual Wikipedias (Adar et al., 2009). Furthermore, lexical translation is a valuable component in knowledge-based Machine Translation systems, e.g., (Bond et al., 2005; Carbonell et al., 2006). PAN D ICTIONARY currently contains over 200 million pairwise translations in over 200,000 language pairs at precision 0.8. It is constructed from information harvested from 631 online dictionaries and Wiktionaries. This necessitates matchIntroduction and Motivation In the era of globalization, inter-lingual communication is becoming increasingly important. Although nearly 7,000 languages are in use today (Gordon, 2005), most language resources are mono-lingual, or bi-lingual.1 This paper investigates whether Wiktionaries and other translation dictionaries available over the Web can be automa"
P09-1030,P95-1032,0,0.211724,"Missing"
P09-1030,P91-1023,0,0.168357,"Missing"
P09-1030,J93-1004,0,\N,Missing
P09-2049,P09-1030,1,0.781508,"Missing"
P10-1044,J03-3005,0,0.292451,"Missing"
P10-1044,P08-1004,1,0.0945607,"2 in LinkLDA) are sampled sequentially conditioned on a fullassignment to all others, integrating out the parameters (Griffiths and Steyvers, 2004). This produces robust parameter estimates, as it allows computation of expectations over the posterior distribution LinkLDA Figure 2 illustrates the LinkLDA model in the plate notation, which is analogous to the model in (Erosheva et al., 2004). In particular note that each ai is drawn from a different hidden topic zi , however the zi ’s are drawn from the same distribution θr for a given relation r. To facilitate learn427 tracted by T EXT RUNNER (Banko and Etzioni, 2008) from 500 million Web pages. To create a generalization corpus from this large dataset. We first selected 3,000 relations from the middle of the tail (we used the 2,0005,000 most frequent ones)3 and collected all instances. To reduce sparsity, we discarded all tuples containing an NP that occurred fewer than 50 times in the data. This resulted in a vocabulary of about 32,000 noun phrases, and a set of about 2.4 million tuples in our generalization corpus. We inferred topic-argument and relation-topic multinomials (β, γ, and θ) on the generalization corpus by taking 5 samples at a lag of 50 aft"
P10-1044,P08-1119,0,0.0281168,"Missing"
P10-1044,D08-1007,0,0.732714,"ing multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical co"
P10-1044,J98-2002,0,0.720098,"s used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce"
P10-1044,E09-1013,0,0.0251596,"e. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in Section 4, it is the best model for our relation da"
P10-1044,N09-1042,0,0.010883,".washington.edu/research/ ldasp. 425 these relations. 2 Our task is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is"
P10-1044,D09-1092,0,0.0188142,"vor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z. LinkLDA (Figure 2) lies between these two extremes, and as demonstrated in"
P10-1044,J02-2003,0,0.533616,"e parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable"
P10-1044,N10-1012,0,0.0218463,"Missing"
P10-1044,P06-1039,0,0.0314514,"Missing"
P10-1044,P10-1045,0,0.631386,"Missing"
P10-1044,N07-1071,0,0.755472,"del. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suf"
P10-1044,P07-1028,0,0.746694,"rd pseudodisambiguation task. Additionally, because L DA - SP is based on a formal probabilistic model, it has the advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Erk (2007) showed the advantages of this approach over Resnik’s information-theoretic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbase"
P10-1044,P09-1070,0,0.0561091,"k is to compute, for each argument ai of each relation r, a set of usual argument values (noun phrases) that it takes. For example, for the relation is headquartered in the first argument set will include companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generate"
P10-1044,J02-3001,0,0.173292,"Missing"
P10-1044,W97-0209,0,0.907626,"Missing"
P10-1044,P99-1014,0,0.952895,"retic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbased approaches. Probably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Previous Work Previous work on selectiona"
P10-1044,W03-0902,0,0.0148719,"dels to similar tasks. O poses a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system. Van Durme and Gildea (2009) proposed applying LDA to general knowledge templates extracted using the K NEXT system (Schubert and Tong, 2003). In contrast, our work uses LinkLDA and focuses on modeling multiple arguments of a relation (e.g., the subject and direct object of a verb). 3 Topic Models for Selectional Prefs. 426 3.2 JointLDA α As a more tightly coupled alternative, we first propose JointLDA, whose graphical model is depicted in Figure 1. The key difference in JointLDA (versus LDA) is that instead of one, it maintains two sets of topics (latent distributions over words) denoted by β and γ, one for classes of each argument. A topic id k represents a pair of topics, βk and γk , that co-occur in the arguments of extracted r"
P10-1044,N09-1054,0,0.00743456,"companies like Microsoft, Intel, General Motors and second argument will favor locations like New York, California, Seattle. Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). 3.1 IndependentLDA We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2 . At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated based on a single hidden variable z"
P10-2053,P08-1004,1,0.866372,"Missing"
P10-2053,P03-1028,0,0.0165086,"quences from Web text. S EQ nearly doubles the area under the precision-recall curve compared to an extractor that does not exploit these regularities. 1 Table 1: Examples of sequences extracted by S EQ from unstructured Web text. its position (e.g., (Washington, 1) and (JFK, 35)). The task of sequence extraction is to automatically instantiate sequence frames given a corpus of unstructured text. Introduction Classical IE systems fill slots in domain-specific frames such as the time and location slots in seminar announcements (Freitag, 2000) or the terrorist organization slot in news stories (Chieu et al., 2003). In contrast, open IE systems are domainindependent, but extract “flat” sets of assertions that are not organized into frames and slots (Sekine, 2006; Banko et al., 2007). This paper reports on S EQ—an open IE system that leverages a domain-independent frame to extract ordered sequences of objects from Web text. We show that the novel, domain-independent sequence frame in S EQ substantially boosts the precision and recall of the system and yields coherent sequences filtered from low-precision extractions (Table 1). Sequence extraction is distinct from set expansion (Etzioni et al., 2004; Wang"
P10-2053,C92-2082,0,0.023295,"s resulted in a statistically significant improvement over using either type of feature individually (paired t-test of area under the curve, p &lt; 0.05). We measure S EQ’s efficacy on the complete sequence-extraction task by contrasting it with two baseline systems. The first is L OCAL, which ranks extractions by localConf .3 The second is 3 4 Related Work There has been extensive work in extracting lists or sets of entities from the Web. These extractors rely on either (1) HTML features (Cohen et al., 2002; Wang and Cohen, 2007) to extract from structured text or (2) lexico-syntactic patterns (Hearst, 1992; Etzioni et al., 2005) to extract from unstructured text. S EQ is most similar to this second type of extractor, but additionally leverages the sequence regularities of functionality and density. These regularities allow the system to overcome the poor performance of the purely syntactic extractor L OCAL and the redundancy-based extractor R EDUND. 5 Conclusions We have demonstrated that an extractor leveraging sequence regularities can greatly outperform extractors without this knowledge. Identifying likely sequence names and then filling in sequence items proved to be an effective approach t"
P10-2053,P06-2094,0,0.0230381,"le 1: Examples of sequences extracted by S EQ from unstructured Web text. its position (e.g., (Washington, 1) and (JFK, 35)). The task of sequence extraction is to automatically instantiate sequence frames given a corpus of unstructured text. Introduction Classical IE systems fill slots in domain-specific frames such as the time and location slots in seminar announcements (Freitag, 2000) or the terrorist organization slot in news stories (Chieu et al., 2003). In contrast, open IE systems are domainindependent, but extract “flat” sets of assertions that are not organized into frames and slots (Sekine, 2006; Banko et al., 2007). This paper reports on S EQ—an open IE system that leverages a domain-independent frame to extract ordered sequences of objects from Web text. We show that the novel, domain-independent sequence frame in S EQ substantially boosts the precision and recall of the system and yields coherent sequences filtered from low-precision extractions (Table 1). Sequence extraction is distinct from set expansion (Etzioni et al., 2004; Wang and Cohen, 2007) because sequences are ordered and because the extraction process does not require seeds or HTML lists as input. The domain-independe"
P13-1158,P05-1074,0,0.236624,"ave yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are en"
P13-1158,N03-1003,0,0.0100333,"ets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 n"
P13-1158,P01-1008,0,0.0129458,"ng and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2"
P13-1158,W02-1033,0,0.233788,"n extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yate"
P13-1158,P13-1042,0,0.425016,"t al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al.,"
P13-1158,D08-1021,0,0.0105295,"neral, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabula"
P13-1158,W10-2903,0,0.0253509,"made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question int"
P13-1158,W02-1001,0,0.0430583,"perceptron approach. We first randomly partition the training data T into K equally-sized subsets T1 , . . . , TK . We then perform perceptron learning on each partition in parallel. Finally, the learned weights from each parallel run are aggregated by taking a uniformly weighted average of each partition’s parameter vector. This procedure is repeated for T iterations. The training data consists of (question x, query z) pairs, but our scoring model is over (question x, derivation y) pairs, which are unobserved in the training data. We use a hidden variable version of the perceptron algorithm (Collins, 2002), where the model parameters are updated using the highest scoring derivation y ∗ that will generate the correct query z using the learned lexicon L. 6 Data For our database D, we use the publicly available set of 15 million R E V ERB extractions (Fader et al., 2011).3 The database consists of a set of triples r(e1 , e2 ) over a vocabulary of approximately 600K relations and 2M entities, extracted from the ClueWeb09 corpus.4 The R E V ERB database contains a large cross-section of general world-knowledge, and thus is a good testbed for developing an open-domain QA system. However, the extracti"
P13-1158,D11-1142,1,0.437775,"problem. The central challenge is to automate every step of QA system construction, including gathering large databases and answering questions against these databases. While there has been significant work on large-scale information extraction (IE) from unstructured text (Banko et al., 2007; Hoffmann et al., 2010; Riedel et al., 2010), the problem of answering questions with the noisy knowledge bases that IE systems produce has received less attention. In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al., 2011). Our system learns from a large, noisy, questionparaphrase corpus, where question clusters have a common but unknown query, and can span a diverse set of topics. Table 1 shows example paraphrase clusters for a set of factual questions. Such data provides strong signal for learning about lexical variation, but there are a number Table 1: Examples of paraphrase clusters from the WikiAnswers corpus. Within each cluster, there is a wide range of syntactic and lexical variations. of challenges. Given that the data is communityauthored, it will inevitably be incomplete, contain incorrectly tagged p"
P13-1158,P10-1030,0,0.0256217,"Missing"
P13-1158,P11-1055,1,0.487038,"and show that it outperforms baseline systems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle an"
P13-1158,N03-1017,0,0.0397793,"Missing"
P13-1158,P11-1060,0,0.0666912,"ext into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The lear"
P13-1158,D09-1040,0,0.0132155,"stion answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the Approach In this section, we give a high-level overview of the rest of the paper. Problem Our goal is to learn a function that will map a natural language question x to a query z over a database D. The database D is a collection of assertions in the form r(e1 , e2 ) where r is a bi1609 nary relation from a vocabulary R, and e1 and e2 are entities from a vocabulary E. We assume that t"
P13-1158,P09-1113,0,0.0109104,"questions from WikiAnswers using a database of web extractions, and show that it outperforms baseline systems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to aut"
P13-1158,P00-1056,0,0.0222414,"Missing"
P13-1158,W08-0509,0,0.0122163,"Missing"
P13-1158,C04-1021,1,0.607014,"2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these sys"
P13-1158,W04-3219,0,0.218387,"Missing"
P13-1158,D12-1035,0,0.167604,"rom data (Zelle and Mooney, 1996; Popescu et al., 2004; Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). These systems have the ability to handle questions with complex semantics on small domain-specific databases like GeoQuery (Tang and Mooney, 2001) or subsets of Freebase (Cai and Yates, 2013), but have yet to scale to the task of general, open-domain question answering. In contrast, our system answers questions with more limited semantics, but does so at a very large scale in an open-domain manner. Some work has been made towards more general databases like DBpedia (Yahya et al., 2012; Unger et al., 2012), but these systems rely on hand-written templates for question interpretation. The learning algorithms presented in this paper are similar to algorithms used for paraphrase extraction from sentence-aligned corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Marton et al., 2009). However, we use a paraphrase corpus for extracting lexical items relating natural language patterns to database concepts, as opposed to relationships between pairs of natural language utterances. Overview of the A"
P13-1158,P12-1075,0,0.00682637,"ems. • We release our learned lexicon question-paraphrase dataset to research community, available http://openie.cs.washington.edu. 2 and the at 3 Related Work Our work builds upon two major threads of research in natural language processing: information extraction (IE), and natural language interfaces to databases (NLIDB). 1 http://wiki.answers.com/ Research in IE has been moving towards the goal of extracting facts from large text corpora, across many domains, with minimal supervision (Mintz et al., 2009; Hoffmann et al., 2010; Riedel et al., 2010; Hoffmann et al., 2011; Banko et al., 2007; Yao et al., 2012). While much progress has been made in converting text into structured knowledge, there has been little work on answering natural language questions over these databases. There has been some work on QA over web text (Kwok et al., 2001; Brill et al., 2002), but these systems do not operate over extracted relational data. The NLIDB problem has been studied for decades (Grosz et al., 1987; Katz, 1997). More recently, researchers have created systems that use machine learning techniques to automatically construct question answering systems from data (Zelle and Mooney, 1996; Popescu et al., 2004; Z"
P13-1158,N10-1069,0,\N,Missing
Q13-1030,P11-1040,0,0.046149,"Missing"
Q13-1030,P07-1073,0,0.0926997,"Missing"
Q13-1030,W99-0613,0,0.16261,"all curve (from 0.16 to 0.34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, s"
Q13-1030,W02-1001,0,0.0880796,"the sentences in our text corpus: θ∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this app"
Q13-1030,P11-1055,1,0.951987,"e. To address this challenge, we propose a joint model of extraction from text and the process by which propositions are observed or missing in both the database and text. Our approach provides a natural way to incorporate side information in the form of a missing data model. For instance, popular entities such as Barack Obama already have good coverage in Freebase, so new extractions are more likely to be errors than those involving rare entities with poor coverage. Our approach to missing data is general and can be combined with various IE solutions. As a proof of concept, we extend MultiR (Hoffmann et al., 2011), a recent model for distantly supervised information extraction, to explicitly model missing data. These extensions complicate the MAP inference problem which is used as a subroutine in learning. This motivated us to explore a variety of approaches to inference in the joint extraction and missing data model. We explore both exact inference based on A* search and efficient approximate inference using local search. Our experiments demonstrate that with a carefully designed set of search operators, local search produces optimal solutions in most cases. Experimental results demonstrate large perf"
Q13-1030,P06-1096,0,0.0137234,"Missing"
Q13-1030,N10-1082,0,0.0108074,"penalty for extracting a fact not in Freebase, and produce an overall higher score. To avoid the problem of getting stuck in local optima, we propose an additional search operator which considers changing all variables, zi , which are currently assigned to a specific relation r, to a new relation r0 , resulting in an additional (k − 1)2 possible neighbors, in addition to the n × (k − 1) neighbors which come from the standard search operator. This aggregate-level search operator allows for more global moves which help to avoid local optima, similar to the type-level sampling approach for MCMC (Liang et al., 2010). At each iteration, we consider all n × (k − 1) + (k−1)2 possible neighboring solutions generated by both search operators, and pick the one with biggest overall improvement, or terminate the algorithm if no improvements can be made over the current solution. 20 random restarts were used for each infer372 ence problem. We found this approach to almost always find an optimal solution. In over 100,000 problems with 200 or fewer variables from the New York Times dataset used in Section 7, an optimal solution was missed in only 3 cases which was verified by comparing against optimal solutions fou"
Q13-1030,N13-1095,0,0.0522778,"on, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approach assuming a fixed proportion of true positives for each entity pair. The Min et al. (2013) approach is perhaps the most closely related of the recent approaches for distant supervision. However, there are a number of key differences: (1) They impose a hard constraint on the proportion of true positive examples for each entity pair, whereas we jointly model relation extraction and missing data in the text and KB. (2) They only handle the case of missing information in the database and not in the text. (3) Their mo"
Q13-1030,P09-1113,0,0.938168,"Missing"
Q13-1030,W04-2407,0,0.0215227,"Times text, features and Freebase relations developed by Riedel et. al. (2010) which was also used by Hoffmann et. al. (2011). This dataset is constructed by extracting named entities from 1.8 million New York Times articles, which are then match against entities in Freebase. Sentences which contain pairs of entities participating in one or more relations are then used as training examples for those relations. The sentencelevel features include word sequences appearing in context with the pair of entities, in addition to part 373 of speech sequences, and dependency paths from the Malt parser (Nivre et al., 2004). 7.1.1 Baseline To evaluate the effect of modeling missing data in distant supervision, we compare against the MultiR model for distant supervision (Hoffmann et al., 2011), a state of the art approach for binary relation extraction which is the most similar previous work, and models facts in Freebase as hard constraints disallowing the possibility of missing information in either the text or the database. To make our experiment as controlled as possible and ruleout the possibility of differences in performance due to implementation details, we compare against our own re-implementation of Mult"
Q13-1030,N13-1008,0,0.0576422,"Missing"
Q13-1030,D11-1141,1,0.107054,"Missing"
Q13-1030,D12-1042,0,0.677887,"Missing"
Q13-1030,P12-1076,0,0.336988,"distant supervision to train event extractors from Twitter. Mintz et. al. (2009) used a set of relations from Freebase as a distant source of supervision to learn to extract information from Wikipedia. Ridel et. al. (2010), Hoffmann et. al. (2011), and Surdeanu et. al. (2012) presented a series of models casting distant supervision as a multiple-instance learning problem (Dietterich et al., 1997). Recent work has begun to address the challenge of noise in heuristically labeled training data generated by distant supervision, and proposed a variety of strategies for correcting erroneous labels. Takamatsu et al. (2012) present a generative model of the labeling process, which is used as a preprocessing step for improving the quality of labels before training relation extractors. Independently, Xu et. al. (2013) analyze a random sample of 1834 sentences from the New York Times, demonstrating that most entity pairs expressing a Freebase relation correspond to false negatives. They apply pseudo-relevance feedback to add missing entries in the knowledge base before applying the MultiR model (Hoffmann et al., 2011). Min et al. (2013) extend the MIML model of Surdeanu et. al. (2012) using a semi-supervised approa"
Q13-1030,P10-1149,0,0.0213479,"34), but still falls short of the results presented by Ritter et. al. (2011). Intuitively this makes sense, because the model used by Ritter et. al. is based on latent Dirichlet allocation which is better suited to this highly ambiguous unary relation data. 376 0.8 0.6 0.0 0.2 As mentioned previously, the problem of missing data in distant (weak) supervision is a very general issue; so far we have investigated this problem in the context of extracting binary relations using distant supervision. We now turn to the problem of weakly supervised named entity recognition (Collins and Singer, 1999; Talukdar and Pereira, 2010). 0.4 precision 7.2 Named Entity Categorization 7.2.1 NER_MultiR NER_DNMAR 0.0 0.2 0.4 0.6 0.8 1.0 recall Figure 7: Precision and Recall at the named entity categorization task 8 Conclusions In this paper we have investigated the problem of missing data in distant supervision; we introduced a joint model of information extraction and missing data which relaxes the hard constraints used in previous work to generate heuristic labels, and provides a natural way to incorporate side information through a missing data model. Efficient inference breaks in the new model, so we presented an approach ba"
Q13-1030,P13-2117,0,0.593462,"Missing"
Q13-1030,D07-1071,1,0.364681,"∗ 369 arg max P (d|s; θ) θ Y X arg max P (z, d|s; θ) = P (z, d|s; θ) θ = n Y i=1 = n Y i=1 e1 ,e2 z φ(zi , si ; θ) × eθ·f (zi ,si ) × k Y ω(z, dj ) j=1 k Y 1¬dj ⊕∃i:j=zi j=1 Where 1x is an indicator variable which takes the value 1 if x is true and 0 otherwise, the ω(z, dj ) factors are hard constraints corresponding to the deterministic-OR function, and f (zi , si ) is a vector of features extracted from sentence si and relation zi . An iterative gradient-ascent based approach is used to tune θ using a latent-variable perceptronstyle additive update scheme (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007). The gradient of the conditional log likelihood, for a single pair of entities, e1 and e2 , is as follows:3 ∂ log P (d|s; θ) ∂θ =  EP (z|s,d;θ)  X j  f (sj , zj )   X −EP (z,d|s;θ)  f (sj , zj ) j 2 These variables indicate which relation is mentioned between e1 and e2 in each sentence. = 3 For details see Koller and Friedman (2009), Chapter 20. (Barack Obama, Honolulu) ?1 ?2 ?3 … ?1 ?2 ?3 … ?? ?? ?2 … ?? These expectations are too difficult to compute in practice, so instead they are approximated as maximizations. Computing this approximation to the gradient requires solving two infe"
Q15-1042,Q13-1005,0,0.00664686,"Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). W"
Q15-1042,D14-1159,0,0.0163322,"aking advantage of a richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on glob"
Q15-1042,P09-1010,0,0.0315966,"t any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015)"
Q15-1042,J05-1003,0,0.0221186,"generating and ranking equation trees; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often r"
Q15-1042,de-marneffe-etal-2006-generating,0,0.0321535,"Missing"
Q15-1042,P10-1126,0,0.00889558,"scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra wor"
Q15-1042,W05-0602,0,0.0364487,"Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on global problem structure. 3 On Monday, 375 students went on a trip to the zoo. All 7 buses were ﬁlled and 4 students had to travel in cars. How many students were in each bus ? 1. Ground text w into base Qsets (sec(on 5) Qnt: 375 Ent: Student Qnt: x Ent: Student Ctr: Bus Tl (w): subset of T(w) yielding correct solu(on = = -s 375s 2 Problems involving simultaneous equations require combining multiple equation"
Q15-1042,P06-2034,0,0.0261535,"ranking equation trees; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning"
Q15-1042,N15-1086,1,0.0320073,"ch aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et"
Q15-1042,D14-1058,1,0.84117,"t al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond"
Q15-1042,P14-1026,0,0.572616,"et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than T"
Q15-1042,D10-1119,0,0.0108677,"s; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence"
Q15-1042,P09-1011,0,0.00650665,"; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word pr"
Q15-1042,N15-1114,0,0.00562616,"richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on global problem structur"
Q15-1042,D15-1118,0,0.112838,"(Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well fo"
Q15-1042,W04-2401,0,0.0641659,"ve been observed during training. Instead, our method maps word problems to equation trees, taking advantage of a richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Simila"
Q15-1042,D15-1202,0,0.536055,"al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance,"
Q15-1042,D15-1171,1,0.671041,"nd Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than Tina. How many cats do they have together?” and “Tina has 2 cats. Jo"
Q15-1042,D15-1135,0,0.519108,"in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. Whil"
Q15-1042,S14-1015,0,0.013061,"to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works c"
Q15-1042,D15-1096,0,0.509188,"k et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than Tina. How many cats d"
Q15-1042,Q15-1001,0,\N,Missing
Q15-1042,D11-1012,0,\N,Missing
Q15-1042,D14-1043,1,\N,Missing
W06-1604,C04-1180,0,0.0168436,"anking. Collins (2000) describes statistical techniques for reranking alternative parses for a sentence. Implicitly, a reranking method detects parser errors, in that if the reranking method picks a new parse over the original one, it is classifying the original one as less likely to be correct. Collins uses syntactic and lexical features and trains on the Penn Treebank; in contrast, W OODWARD uses semantic features derived from the web. See section 3 for a comparison of our results with Collins’. Several systems produce a semantic interpretation of a sentence on top of a parser. For example, Bos et al. (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanova et al., 2005) provides a semantic representation using minimal recursion semantics. Toutanova et al. also include semantic features in their parse selection mechanism, although it is mostly syntaxdriven. The ERG is a hand-built grammar and thus 2 Semantic Filtering This section describes semantic filtering as implemented in the W OODWARD system. W OODWARD consists of two components: a semantic interpreter that takes a parse tree and converts it to a conjunction of first"
W06-1604,J05-1003,0,0.033731,"earch engines tightly limit the number of queries an application may issue per day. On the other hand, the TextRunner index is at present 2.6 Question Answering Filter When parsing questions, an additional method of detecting incorrect parses becomes available: use a question answering (QA) system to find answers. If a QA system using the parse can find an answer to the question, then the question was probably parsed correctly. To test this theory, we implemented a lightweight, simple, and fast QA system that directly mirrors the semantic interpretation. It relies on TextRunner and KnowItNow (Cafarella et al., 2005) to quickly find possible answers, given the relational conjunction (RC) of the question. KnowItNow is a state of the art Information Extraction system that uses a set of domain independent patterns to efficiently find hyponyms of a class. We formalize the process as follows: define a question as a set of variables Xi corresponding to noun phrases, a set of noun type predicates T i (Xi ), and a set of relational predicates P i (Xi1 , ..., Xik ) which relate one or more variables and constants. The conjunction of type and relational predicates is precisely the RC. We define an answer as a set o"
W06-1604,W03-2606,0,0.107268,"Missing"
W06-1604,N03-1022,0,0.155234,"of the relation and the head words of each argument. For example, the prepositional attachment “operations of 65 cents” would yield phrases like “operations of” and “operations of * cents”. (The ‘*’ character is a wildcard in the Google interface; it can match any single word.) We then collect hitcounts for each discriminator phrase, as well as for the relation name and each argument head word, and compute a PMI score for each phrase, using the phrase’s hitcount as the numerator in Equation 1. Due to space constraints, we omit details about the algorithm for converting a parse into an RC, but Moldovan et al. (2003) describe a method similar to ours. 29 Given a set of such PMI scores for a single relation, we apply a learned classifier to decide if the PMI scores justify calling the relation implausible. This classifier (as well as all of our other ones) is trained on a set of sentences from TREC and the Penn Treebank; our training and test sets are described in more detail in section 3. We parsed each sentence automatically using Daniel Bikel’s implementation of the Collins parsing model, 1 trained on sections 2–21 of the Penn Treebank, and then applied our semantic interpreter algorithm to come up with"
W06-1604,H05-1071,1,\N,Missing
W06-1702,H05-1071,1,0.846998,"Missing"
W06-2208,H05-1056,0,\N,Missing
W06-2208,P05-1052,0,\N,Missing
W06-2208,P02-1006,0,\N,Missing
W06-2208,W99-0613,0,\N,Missing
W10-0907,P98-1013,0,0.0227497,"ible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate SRL-IE’s effectiveness"
W10-0907,P08-1004,1,0.816759,"eneous, unstructured text. The traditional approaches to information extraction (e.g., (Soderland, 1999; Agichtein and Gravano, 2000)) do not operate at these scales, since they focus attention on a well-defined small set of relations and require large amounts of training data for each relation. The recent Open Information Extraction paradigm (Banko et al., 2007) attempts to overcome the knowledge acquisition bottleneck with its relation-independent nature and no manually annotated training data. 52 We are interested in the best possible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain"
W10-0907,W09-2201,0,0.0118398,"le for extracting common sense knowledge as opposed to factual information. Another Open IE system, Kylin (Weld et al., 2008), suggests automatically building an extractor for each relation using self-supervised training, with training data generated using Wikipedia infoboxes. This work has the limitation that it can only extract relations expressed in Wikipedia infoboxes. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). While one goal of Preemptive IE is to avoid relation-specificity, Preemptive IE does not emphasize Web scalability, which is essential to Open IE. (Carlson et al., 2009) presents a semi-supervised approach to information extraction on the Web. It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar, resulting in 46 general linkpaths as patterns for relation extraction. With these patterns WANDERLUST extracts binary relations from link grammar linkages. In contrast to"
W10-0907,N09-2022,0,0.0115482,"actor. Because not all roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constituents that are potentia"
W10-0907,C08-1050,0,0.012304,"tient and ‘son’ is the benefactor. Because not all roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constitu"
W10-0907,kingsbury-palmer-2002-treebank,0,0.731996,"terested in the best possible technique for Open IE. The T EXT RUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Avoiding the expensive processing of deep syntactic analysis allowed T EXT RUNNER to process at Web scale. In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. SRL is a popular NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate S"
W10-0907,J08-2003,0,0.0108118,"roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank (Martha and Palmer, 2002), which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years. Recently, there have been many advances in SRL (Toutanova et al., 2008; Johansson and Nugues, 2008; Coppola et al., 2009; Moschitti et al., 2008). We use UIUC-SRL as our base SRL system (Punyakanok et al., 2008). Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art (it achieved the highest F1 score on the CoNLL-2005 shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference. Pruning involves using a full parse tree and heuristic rules to eliminate constituents that are unlikely to be arguments. Argument identification uses a classifier to identify constituents that are potential arguments. In argument"
W10-0907,D09-1001,0,0.0269047,"It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar, resulting in 46 general linkpaths as patterns for relation extraction. With these patterns WANDERLUST extracts binary relations from link grammar linkages. In contrast to our approaches, this requires a large set of hand-labeled examples. USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion. They evaluate their work on biomedical text, so its applicability to general Web text is not yet clear. 8 Discussion and Future Work The Heavy Tail: It is well accepted that information on the Web is distributed according to Zipf’s 0 10 20 30 40 Time (hours) 0.0 50 0 10 20 30 40 Time (hours) F−measure 0.4 0.8 TextRunner SRL−IE RecallHybrid PrecHybrid TextRunner SRL−IE RecallHybrid PrecHybrid 0.0 Recall 0.4 0.8 Precision 0.2 0.4 0.6 0.0 TextRunner SRL−IE RecallHybrid PrecHybrid 50 0 10 20"
W10-0907,J08-2005,0,0.120937,"lar NLP task that has seen significant progress over the last few years. The advent of hand-constructed semantic resources such as Propbank and Framenet (Martha and Palmer, 2002; Baker et al., 1998) have resulted in semantic role labelers achieving high in-domain precisions. Our first observation is that semantically labeled arguments in a sentence almost always correspond to the arguments in Open IE extractions. Similarly, the verbs often match up with Open IE relations. These observations lead us to construct a new Open IE extractor based on SRL. We use UIUC’s publicly available SRL system (Punyakanok et al., 2008) that is known to be competitive with the state of the art and construct a novel Open IE extractor based on it called SRL-IE. We first need to evaluate SRL-IE’s effectiveness in the context of large scale and heterogeneous input data as found on the Web: because SRL uses deeper analysis we expect SRL-IE to be much slower. Second, SRL is trained on news corpora using a resource like Propbank, and so may face recall loss due to out of vocabulary verbs and precision loss due to different writing styles found on the Web. In this paper we address several empirical quesProceedings of the NAACL HLT 2"
W10-0907,N06-1039,0,0.0123911,"008). A version of K NEXT uses heuristic rules and syntactic parses to convert a sentence into an unscoped logical form (Van Durme and Schubert, 2008). This work is more suitable for extracting common sense knowledge as opposed to factual information. Another Open IE system, Kylin (Weld et al., 2008), suggests automatically building an extractor for each relation using self-supervised training, with training data generated using Wikipedia infoboxes. This work has the limitation that it can only extract relations expressed in Wikipedia infoboxes. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). While one goal of Preemptive IE is to avoid relation-specificity, Preemptive IE does not emphasize Web scalability, which is essential to Open IE. (Carlson et al., 2009) presents a semi-supervised approach to information extraction on the Web. It learns classifiers for different relations and couples the training of those classifiers with ontology defining constraints. While we attempt to learn unknown relations, it learns a pre-defined set of relations. Another related system is WANDERLUST (Akbik and Broß, 2009). The authors of this system annotated 10,000 sentences parsed with LinkGrammar,"
W10-0907,J08-2002,0,0.0441675,"Missing"
W10-0907,W08-2219,0,0.0204424,"Missing"
W10-0907,C98-1013,0,\N,Missing
W10-0911,W10-0907,1,0.72391,"wledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage b"
W10-0911,P07-1088,1,0.752966,"leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KO"
W10-0911,P10-1030,1,0.76682,"omingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned inference patterns (Holmes, Sherlock), and cotraining using relation-specific and relation-independent (open) extraction to reinforce semantic coherence (Wu et al., 2008)"
W10-0911,D08-1068,1,0.90344,"radictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausi"
W10-0911,D09-1001,1,0.413926,"edge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Scho"
W10-0911,H05-1043,1,0.0833929,"ic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContrair"
W10-0911,D08-1002,1,0.769247,"nd Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via rel"
W10-0911,P10-1044,1,0.578046,"ustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and Text"
W10-0911,D08-1009,1,0.92382,"usters (e.g., in USP, LOFT). Similarly, a variety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to 90 machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better than more expressive ones. A key direction"
W10-0911,D10-1106,1,0.437126,"009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and ofte"
W10-0911,P10-1013,1,0.458065,"n and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned"
W10-0911,N07-1016,1,0.79964,"ioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs s"
W10-0911,H05-2017,1,\N,Missing
W10-0911,P10-1031,1,\N,Missing
W12-3014,W11-2504,0,0.0217989,"Missing"
W12-3014,W08-2221,1,0.836258,"ess can be bootstrapped: QA can help build the KB, and the KB can provide the evidence for QA. We kickstart the process by initially seeding the KB with extractions from individual sentences in the book, and then use QA over those extractions to rescore and refine the knowledge (&quot;introspective QA&quot;). 2.1 Information Extraction Our first step is to process the textbook text and extract semi-structured representations of its content. We extract two forms of the textbook's information: Logical Forms (LFs): A parse-based logical form (LF) representation of the book sentences using the BLUE system (Clark and Harrison, 2008), e.g., from &quot;Metabolism sets limits on cell size&quot; we obtain: (S (SUBJ (&quot;metabolism&quot;)) (V (&quot;set&quot;)) (SOBJ (&quot;limit&quot; (&quot;on&quot; (&quot;size&quot; (MOD (&quot;cell&quot;))))) Triples: A set of arg1-predicate-arg2 triples extracted via a chunker applied to the book sentences, using Univ. Washington's ReVerb system (Fader et al, 2011), e.g., from &quot;Free ribosomes are suspended in the cytosol and synthesize proteins there.&quot; we obtain: [&quot;ribosomes&quot;] [&quot;are suspended in&quot;] [&quot;the cytosol&quot;] These extractions are the raw material for the initial textual KB. 2.2 Knowledge-Base Construction and Introspective Question-Answering As the"
W12-3014,D11-1142,1,0.634026,"tion Extraction Our first step is to process the textbook text and extract semi-structured representations of its content. We extract two forms of the textbook's information: Logical Forms (LFs): A parse-based logical form (LF) representation of the book sentences using the BLUE system (Clark and Harrison, 2008), e.g., from &quot;Metabolism sets limits on cell size&quot; we obtain: (S (SUBJ (&quot;metabolism&quot;)) (V (&quot;set&quot;)) (SOBJ (&quot;limit&quot; (&quot;on&quot; (&quot;size&quot; (MOD (&quot;cell&quot;))))) Triples: A set of arg1-predicate-arg2 triples extracted via a chunker applied to the book sentences, using Univ. Washington's ReVerb system (Fader et al, 2011), e.g., from &quot;Free ribosomes are suspended in the cytosol and synthesize proteins there.&quot; we obtain: [&quot;ribosomes&quot;] [&quot;are suspended in&quot;] [&quot;the cytosol&quot;] These extractions are the raw material for the initial textual KB. 2.2 Knowledge-Base Construction and Introspective Question-Answering As the ontology for the TKB, we are using the preexisting biology taxonomy (isa hierarchy) from the hand-build biology KB (part of the formal knowledge project). Initially, for each concept in that ontology, all the extractions &quot;about&quot; that concept are gathered together. An extraction is considered &quot;about&quot; a co"
W12-3014,P11-1062,0,\N,Missing
W12-3014,P10-1124,0,\N,Missing
W12-3016,E06-1002,0,0.182439,"Missing"
W12-3016,D07-1074,0,0.0237753,"t strings to meaningful entities that hold properties, semantic types, and relationships with each other. Introduction Information Extraction techniques such as Open IE (Banko et al., 2007; Weld et al., 2008) operate at unprecedented scale. The R E V ERB extractor (Fader et al., 2011) was run on 500 million Web pages, and extracted 6 billion (Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on anothe"
W12-3016,C10-1032,0,0.118648,"(Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on another document. This relates to cross-document coreference (Singh et al., 2011), but is not the same because cross-document coreference does not offer all the benefits of linking to Wikipedia. Another opportunity is that after linking a million documents, we can discover systematic linking errors when particular entities are linked to many more times"
W12-3016,D11-1142,1,0.0869527,"ng at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities. 1 Figure 1: Entity Linking elevates textual argument strings to meaningful entities that hold properties, semantic types, and relationships with each other. Introduction Information Extraction techniques such as Open IE (Banko et al., 2007; Weld et al., 2008) operate at unprecedented scale. The R E V ERB extractor (Fader et al., 2011) was run on 500 million Web pages, and extracted 6 billion (Subject, Relation, Object) extractions such as (“Orange Juice”, “is rich in”, “Vitamin C”), over millions of textual relations. Linking each textual argument string to its corresponding Wikipedia entity, known as entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), would offer benefits such as semantic type information, integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into"
W12-3016,D11-1072,0,0.0431029,"Missing"
W12-3016,P11-1138,0,0.0641982,"ambiguity is high. ing at this scale. 2 Entity Linking Given a textual assertion, we aim to find the Wikipedia entity that corresponds to the argument. For example, the assertion (“New York”, “acquired”, “Pineda”) should link to the Wikipedia article for New York Yankees, rather than New York City. Speed is a practical concern when linking this many assertions, so instead of designing a system with sophisticated features that rely on the full Wikipedia graph structure, we instead start with a faster system leveraging linking features such as string matching, prominence, and context matching. (Ratinov et al., 2011) found that these “local” features already provide a baseline that is very difficult to beat with the more sophisticated “global” features that take more time to compute. For efficient highquality entity linking of Web scale corpora, we focus on the faster techniques, and then later incorporate corpus-level features to increase precision. 2.1 Our Basic Linker Given an entity string, we first obtain the most prominent Wikipedia entities that meet string matching criteria. As in (Fader et al., 2009), we measure prominence using inlink count, which is the number of Wikipedia pages that link to a"
W12-3016,D10-1106,1,0.617096,"at, school)” are returned first. When results are sorted by link score, the top hundred results are all specific instances of professors and the schools they teach at, and are noticeably more specific and generally correct than the top frequency-sorted instances. 3.3 Inference Disambiguated and typed entities are especially valuable for inference applications over extracted data. For example if we observe enough instances like “Orange Juice is rich in Vitamin C,” “Vitamin C helps prevent scurvy,” and “Orange Juice helps prevent scurvy,” then we can learn the inference rule shown in Figure 3. (Schoenmackers et al., 2010) explored this, but without entity linking they had to rely on heavy filtering against hypernym data, losing most of their extraction instances in the process. We plan to explore how much gain we get in inference rule learning when using entity linking instead of hypernym filtering. Linked instances would also be higher precision input than what is currently available for learning implicit common sense properties of textual relations (Lin et al., 2010). 4 Conclusions While numerous entity-linking systems have been developed in recent years, we believe that going forward, researchers will incre"
W12-3016,P11-1080,0,0.0289028,", integration with linked data resources (Bizer et al., 2009), and disambiguation (see Figure 1). Existing entity linking research has focused primarily on linking all the entities within individual documents into Wikipedia (Milne and Witten, 2008; Kulkarni et al., 2009; Dredze et al., 2010). To link a million documents they would repeat a million times. However, there are opportunities to do better when we know ahead of time that the task is large scale linking. For example, information on one document might help link an entity on another document. This relates to cross-document coreference (Singh et al., 2011), but is not the same because cross-document coreference does not offer all the benefits of linking to Wikipedia. Another opportunity is that after linking a million documents, we can discover systematic linking errors when particular entities are linked to many more times than expected. In this paper we entity link millions of highprecision extractions from the Web, and present our initial methods for addressing some of the opportunities and practical challenges that arise when link84 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-W"
W12-3019,J92-4003,0,0.37258,"the future, we will automatically determine semantic types for the slots. We will also split slots that have a mixture of semantic types, as in the example of the arguments {percent, year} for the extraction (sale; increase; ?) in Table 2. Table 3: Both Rel-clusters and Chambers system discovered clusters that covered most of the extraction slots for MUC-4 terrorism events. Fraction of slots Chambers Rel-clusters Bombing 0.50 1.00 Attack 0.67 1.00 Kidnapping 0.50 0.50 Arson 0.60 0.60 Average 0.57 0.77 4 Related Work There has been extensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extrac"
W12-3019,P08-1090,0,0.113394,"xtensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram language models, but is computed over relations rather than over words. We construct the Rel-grams probabilistic graphical model based on statistics stored in the Rel-grams database and demonstrate the model’s use in identifying event templates from c"
W12-3019,P09-1068,0,0.18985,"del language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram language models, but is computed over relations rather than over words. We construct the Rel-grams probabilistic graphical model based on statistics stored in the Rel-grams database and demonstrate the model’s use in identifying event templates from clusters of co-occurring relati"
W12-3019,P11-1098,0,0.416297,"ntation is linear in the size of the corpus and easily scaled to far larger corpora. Rel-grams database facilitates several tasks including: Relational Language Models: We define a relational language model, which encodes the probability of relational tuple R, having observed R0 in the k previous tuples. This can be used for discourse coherence, sentence order in summarization, etc. Event Template Construction: We cluster commonly co-occuring relational tuples as in Figure 1 and use them as the basis for open event templates (see Table 2). Our work builds on and generalizes earlier efforts by Chambers and Jurafsky (2011). Expectation-driven Extraction: The probabilities output by the relational language model may be 101 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 101–105, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics used to inform an information extractor. As has been the case with n-gram models and resources such as DIRT (Lin and Pantel, 2001), we expect the community to suggest additional applications leveraging this large scale, public resource. An intriguing possibility is to use"
W12-3019,W04-3205,0,0.0896575,"discovered clusters that covered most of the extraction slots for MUC-4 terrorism events. Fraction of slots Chambers Rel-clusters Bombing 0.50 1.00 Attack 0.67 1.00 Kidnapping 0.50 0.50 Arson 0.60 0.60 Average 0.57 0.77 4 Related Work There has been extensive use of n-grams to model language at the word level (Brown et al., 1992; Bergsma et al., 2009; Momtazi and Klakow, 2009; Yu et al., 2007; Lin et al., 2010). Rel-grams model language at the level of relations. Unlike DIRT (Lin and Pantel, 2001), Rel-grams counts relation cooccurrence rather than argument co-occurence. And unlike VerbOcean (Chklovski and Pantel, 2004), Rel-grams handles arbitrary relations rather than a small set of pre-determined relations between verbs. We build on prior work that learns narrative chains and narrative schema that link actions by the same protagonists (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and work that extracts event templates from a narrowly focused corpus (Chambers and Jurafsky, 2011). Rel-grams finds more general associations between relations, and has made a first step towards learning event templates at scale. 5 Conclusions This paper introduces the Rel-grams model, which is analogous to n-gram"
W12-3019,D11-1142,1,0.617966,"LP tasks such as extraction. 102 probability given (treasury bond; fall; ?). Our model matches well with human intuition about semantic relatedness. Predicted tuples Argument values 1.(bond; fall; ?) point, percent 2.(yield; rise; ?) point, percent 3.(report; show; ?) economy, growth 4.(bond; yield; ?) percent, point 5.(index; rise; ?) percent, point 6.(federal reserve; raise; ?) rate, interest rate The Rel-grams Database As a first step towards building the relational language model, we extract relational tuples from each sentence in our corpus using ReVerb, a state-of-theart Open IE system (Fader et al., 2011). This extracts relational tuples in the format (arg1; rel; arg2) where each tuple element is a phrase from the sentence. We construct a relational database to hold cooccurrence statistics for pairs of tuples found in each document as shown in Figure 2. The database consists of three tables: a Tuples table maps each tuple to a unique identifier; BigramCounts stores the cooccurrence frequency, a count for a pair of tuples and a window k; UniCounts counts the number of times each tuple was observed in the corpus. We refer to this resource as the Rel-grams database. Query Language: The relational"
W12-3019,P07-2045,0,0.0025624,"s. We make this database freely available and hope it will prove a useful resource for a wide variety of NLP tasks. 1 0.38 (bomb; explode at; ?) 0.58 0.40 (bomb; explode in; ?) 0.54 0.72 0.53 (bomb; kill; ?) 0.48 0.60 0.51 (?; kill; people) (bomb; wound; ?) 0.43 0.57 (?; detonate; bomb) 0.54 0.40 (bomb; destroy; ?) Figure 1: Part of a sub-graph that Rel-grams discovers showing relational tuples strongly associated with (bomb; kill; ?) Introduction The Google N-grams corpus (Brants and Franz, 2006) has enjoyed immense popularity in NLP and has proven effective for a wide range of applications (Koehn et al., 2007; Bergsma et al., 2009; Lin et al., 2010). However, it is a lexical resource and provides only local, sentence-level information. It does not capture the flow of semantic content within a larger document or even in neighboring sentences. We introduce the novel Rel-grams database1 containing corpus statistics on frequently occurring sequences of open-domain relational tuples. Relgrams is analogous to n-grams except that instead of word sequences within a sentence, it tabulates relation sequences within a document. Thus, we expect Rel-grams to model semantic and discourselevel regularities in th"
W12-3019,lin-etal-2010-new,0,0.0630824,"and hope it will prove a useful resource for a wide variety of NLP tasks. 1 0.38 (bomb; explode at; ?) 0.58 0.40 (bomb; explode in; ?) 0.54 0.72 0.53 (bomb; kill; ?) 0.48 0.60 0.51 (?; kill; people) (bomb; wound; ?) 0.43 0.57 (?; detonate; bomb) 0.54 0.40 (bomb; destroy; ?) Figure 1: Part of a sub-graph that Rel-grams discovers showing relational tuples strongly associated with (bomb; kill; ?) Introduction The Google N-grams corpus (Brants and Franz, 2006) has enjoyed immense popularity in NLP and has proven effective for a wide range of applications (Koehn et al., 2007; Bergsma et al., 2009; Lin et al., 2010). However, it is a lexical resource and provides only local, sentence-level information. It does not capture the flow of semantic content within a larger document or even in neighboring sentences. We introduce the novel Rel-grams database1 containing corpus statistics on frequently occurring sequences of open-domain relational tuples. Relgrams is analogous to n-grams except that instead of word sequences within a sentence, it tabulates relation sequences within a document. Thus, we expect Rel-grams to model semantic and discourselevel regularities in the English language. 1 (?; claim; responsi"
W12-3019,P08-1000,0,\N,Missing
W16-1303,H05-1071,1,0.915744,"Missing"
W16-1303,W09-2201,0,0.0312736,"Missing"
W16-1303,W99-0613,0,0.548716,"Missing"
W16-1303,D11-1133,0,0.0672581,"Missing"
W16-1303,W12-0702,0,0.0450558,"Missing"
W16-1303,W14-1611,0,0.0787971,"Missing"
W16-1303,C92-2082,0,0.445913,"Missing"
W16-1303,P11-1055,0,0.123146,"Missing"
W16-1303,P12-3019,0,0.0706265,"Missing"
W16-1303,E14-1048,0,0.0501259,"Missing"
W16-1303,J06-3003,0,0.0561851,"Missing"
W16-1303,P15-1034,0,\N,Missing
