2020.findings-emnlp.164,Continual Learning Long Short Term Memory,2020,-1,-1,7,0,11737,xin guo,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task{'}s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks."
2020.coling-main.214,{M}ed{W}riter: Knowledge-Aware Medical Text Generation,2020,-1,-1,4,0,21313,youcheng pan,Proceedings of the 28th International Conference on Computational Linguistics,0,"To exploit the domain knowledge to guarantee the correctness of generated text has been a hot topic in recent years, especially for high professional domains such as medical. However, most of recent works only consider the information of unstructured text rather than structured information of the knowledge graph. In this paper, we focus on the medical topic-to-text generation task and adapt a knowledge-aware text generation model to the medical domain, named MedWriter, which not only introduces the specific knowledge from the external MKG but also is capable of learning graph-level representation. We conduct experiments on a medical literature dataset collected from medical journals, each of which has a set of topic words, an abstract of medical literature and a corresponding knowledge graph from CMeKG. Experimental results demonstrate incorporating knowledge graph into generation model can improve the quality of the generated text and has robust superiority over the competitor methods."
D19-5706,A Deep Learning-Based System for {P}harma{C}o{NER},2019,0,1,6,0,4717,ying xiong,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"The Biological Text Mining Unit at BSC and CNIO organized the first shared task on chemical {\&} drug mention recognition from Spanish medical texts called PharmaCoNER (Pharmacological Substances, Compounds and proteins and Named Entity Recognition track) in 2019, which includes two tracks: one for NER offset and entity classification (track 1) and the other one for concept indexing (track 2). We developed a pipeline system based on deep learning methods for this shared task, specifically, a subsystem based on BERT (Bidirectional Encoder Representations from Transformers) for NER offset and entity classification and a subsystem based on Bpool (Bi-LSTM with max/mean pooling) for concept indexing. Evaluation conducted on the shared task data showed that our system achieves a micro-average F1-score of 0.9105 on track 1 and a micro-average F1-score of 0.8391 on track 2."
N18-1188,{LSDSCC}: a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics,2018,0,4,8,0,8061,zhen xu,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"It has been proven that automatic conversational agents can be built up using the Endto-End Neural Response Generation (NRG) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation metrics for testing. This paper proposes a Large Scale Domain-Specific Conversational Corpus (LSDSCC) composed of high-quality queryresponse pairs extracted from the domainspecific online forum, with thorough preprocessing and cleansing procedures. Also, a testing set, including multiple diverse responses annotated for each query, is constructed, and on this basis, the metrics for measuring the diversity of generated results are further presented. We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed dataset. The experimental results have shown that our proposed corpus can be taken as a new benchmark dataset for the NRG task, and the presented metrics are promising to guide the optimization of NRG models by quantifying the diversity of the generated responses reasonably."
I17-1072,Predicting Users{'} Negative Feedbacks in Multi-Turn Human-Computer Dialogues,2017,16,0,4,1,8040,xin wang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"User experience is essential for human-computer dialogue systems. However, it is impractical to ask users to provide explicit feedbacks when the agents{'} responses displease them. Therefore, in this paper, we explore to predict users{'} imminent dissatisfactions caused by intelligent agents by analysing the existing utterances in the dialogue sessions. To our knowledge, this is the first work focusing on this task. Several possible factors that trigger negative emotions are modelled. A relation sequence model (RSM) is proposed to encode the sequence of appropriateness of current response with respect to the earlier utterances. The experimental results show that the proposed structure is effective in modelling emotional risk (possibility of negative feedback) than existing conversation modelling approaches. Besides, strategies of obtaining distance supervision data for pre-training are also discussed in this work. Balanced sampling with respect to the last response in the distance supervision data are shown to be reliable for data augmentation."
D17-1065,Neural Response Generation via {GAN} with an Approximate Embedding Layer,2017,19,31,5,0,8061,zhen xu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a Generative Adversarial Network (GAN) to model single-turn short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network for response generation simultaneously with a discriminative classifier that measures the differences between human-produced responses and machine-generated ones. In addition, the proposed method introduces an approximate embedding layer to solve the non-differentiable problem caused by the sampling-based output decoding procedure in the Seq2Seq generative model. The GAN setup provides an effective way to avoid noninformative responses (a.k.a {``}safe responses{''}), which are frequently observed in traditional neural response generators. The experimental results show that the proposed approach significantly outperforms existing neural response generation models in diversity metrics, with slight increases in relevance scores as well, when evaluated on both a Mandarin corpus and an English corpus."
C16-1117,Incorporating Label Dependency for Answer Quality Tagging in Community Question Answering via {CNN}-{LSTM}-{CRF},2016,5,7,6,1,14586,yang xiang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In community question answering (cQA), the quality of answers are determined by the matching degree between question-answer pairs and the correlation among the answers. In this paper, we show that the dependency between the answer quality labels also plays a pivotal role. To validate the effectiveness of label dependency, we propose two neural network-based models, with different combination modes of Convolutional Neural Net-works, Long Short Term Memory and Conditional Random Fields. Extensive experi-ments are taken on the dataset released by the SemEval-2015 cQA shared task. The first model is a stacked ensemble of the networks. It achieves 58.96{\%} on macro averaged F1, which improves the state-of-the-art neural network-based method by 2.82{\%} and outper-forms the Top-1 system in the shared task by 1.77{\%}. The second is a simple attention-based model whose input is the connection of the question and its corresponding answers. It produces promising results with 58.29{\%} on overall F1 and gains the best performance on the Good and Bad categories."
Y15-1006,Computing Semantic Text Similarity Using Rich Features,2015,27,6,4,0,1457,yang liu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Semantic text similarity (STS) is an essential problem in many Natural Language Processing tasks, which has drawn a considerable amount of attention by research community in recent years. In this paper, our work focused on computing semantic similarity between texts of sentence length. We employed a Support Vector Regression model with rich effective features to predict the similarity scores between short English sentence pairs. Our model used WordNet-Based features, CorpusBased features, Word2Vec-based features, Alignment-Based feature and Literal-Based features to cover various aspects of sentences. And the experiment conducted on SemEval 2015 task 2a shows that our method achieved a Pearson correlation: 80.486% which outperformed the wining system (80.15%) by a small margin, the results indicated a high correlation with human judgments. Specially, among the five test sets which come from different domains used in the estimation, our method got better results than the top team on two of them whose domain-related data is available for training, while comparable results were achieved on the rest three unseen test sets. The experiments results indicated that our solution is more competitive when the domain-specific training data is available and our method still keeps good generalization ability on novel data."
W15-4415,{C}hinese Grammatical Error Diagnosis Using Ensemble Learning,2015,10,13,2,1,14586,yang xiang,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"Automatic grammatical error detection for Chinese has been a big challenge for NLP researchers for a long time, mostly due to the flexible and irregular ways in the expressing of this language. Strictly speaking, there is no evidence of a series of formal and strict grammar rules for Chinese, especially for the spoken Chinese, making it hard for foreigners to master this language. The CFL shared task provides a platform for the researchers to develop automatic engines to detect grammatical errors based on a number of manually annotated Chinese spoken sentences. This paper introduces HITSZxe2x80x99s system for this yearxe2x80x99s Chinese grammatical error diagnosis (CGED) task. Similar to the last yearxe2x80x99s task, we put our emphasis mostly on the error detection level and error type identification level but did little for the position level. For all our models, we simply use supervised machine learning methods constrained to the given training corpus, with neither any heuristic rules nor any other referenced materials (except for the last yearsxe2x80x99 data). Among the three runs of results we submitted, the one using the ensemble classifier Random Feature Subspace (HITSZ_Run1) gained the best performance, with an optimal F1 of 0.6648 for the detection level and 0.2675 for the identification level."
S15-2014,yi{G}ou: A Semantic Text Similarity Computing System Based on {SVM},2015,16,3,4,0,1457,yang liu,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the yiGou system we developed to compute the semantic similarity of two English sentences, which we submitted to the SemEval 2015 Task 2 (English subtask). The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38 th and 42 th , with mean Pearson correlation 0.7114 and 0.6964 respectively."
S15-2035,{HITSZ}-{ICRC}: Exploiting Classification Approach for Answer Selection in Community Question Answering,2015,16,20,3,0,37200,yongshuai hou,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the participation of the HITSZ-ICRC team on the Answer Selection Challenge in SemEval-2015. Our team participated in English subtask A, English subtask B and Arabic task. Two approaches, ensemble learning and hierarchical classification were proposed for answer selection in each task. Bag-of-words features, lexical features and non-textual features were employed. For the Arabic task, features were extracted from both Arabic data and English data that translated from the Arabic data. Evaluation demonstrated that the proposed methods were effective, achieving a macro-averaged F1 of 56.41% (rank 2 nd ) in English subtask A, 53.60 % (rank 3 rd ) in English subtask B and 67.70% (rank 3 rd ) in Arabic task, respectively."
S15-2037,{ICRC}-{HIT}: A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge,2015,14,17,5,1,35742,xiaoqiang zhou,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper, we present a comment labeling system based on a deep learning strategy. We treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments. In the recurrent architecture of our system, our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question-comment pair, and assigns the labels to the comment sequence with a recurrent neural network over CNN. Compared with the conditional random fields based method, our approach performs better performance on Macro-F1 (53.82%), and achieves the highest accuracy (73.18%), F1-value (79.76%) on predicting the Good class in this answer selection challenge."
S15-2140,{HITSZ}-{ICRC}: An Integration Approach for {QA} {T}emp{E}val Challenge,2015,11,0,4,0,37200,yongshuai hou,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper presents the HITSZ-ICRC system designed for the QA TempEval challenge in SemEval-2015. The system used an integration approach to annotate temporal information by merging temporal annotation results from different temporal annotators. TIPSemB, ClearTK and TARSQI were used as temporal annotators to get candidate temporal annotation results. Evaluation demonstrated that our system was effective for improving the performance of temporal information annotation, and achieved recalls of 0.18, 0.26 and 0.19 on Blog, News and Wikipedeia test sets."
P15-2117,Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering,2015,18,17,5,1,35742,xiaoqiang zhou,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, the answer selection problem in community question answering (CQA) is regarded as an answer sequence labeling task, and a novel approach is proposed based on the recurrent architecture for this problem. Our approach applies convolution neural networks (CNNs) to learning the joint representation of question-answer pair firstly, and then uses the joint representation as input of the long short-term memory (LSTM) to learn the answer sequence of a question for labeling the matching quality of each answer. Experiments conducted on the SemEval 2015 CQA dataset shows the effectiveness of our approach."
P15-1130,Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory,2015,33,95,5,1,8040,xin wang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In this paper, we introduce Long ShortTerm Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases."
W14-6808,Problematic Situation Analysis and Automatic Recognition for {C}hinese Online Conversational System,2014,12,10,4,1,14586,yang xiang,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Automatic problematic situation recognition (PSR) is important for an online conversational system to constantly improve its performance. A PSR module is responsible of automatically identifying usersxe2x80x99 un-satisfactions and then sending feedbacks to conversation managers. In this paper, we collect dialogues from a Chinese online chatbot, annotate the problematic situations and propose a framework to predict utterance-level problematic situations by integrating intent and sentiment factors. Different from previous work, the research field is set as open-domain in which very few domain specific textual features could be used and the method is easy to be adapted to other domains. Experimental results show that integrating both intent and sentiment factors gains the best performance."
P14-5005,{WINGS}:Writing with Intelligent Guidance and Suggestions,2014,8,2,3,0,39091,xianjun dai,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Without inspirations, writing may be a frustrating task for most people. In this study, we designed and implemented WINGS, a Chinese input method extended on IBus-Pinyin with intelligent writing assistance. In addition to supporting common Chinese input, WINGS mainly attempts to spark usersxe2x80x99 inspirations by recommending both word level and sentence level writing suggestions. The main strategies used by WINGS, including providing syntactically and semantically related words based on word vector representation and recommending contextually related sentences based on LDA, are discussed and described. Experimental results suggest that WINGS can facilitate Chinese writing in an effective and creative manner."
P14-2139,Cross-lingual Opinion Analysis via Negative Transfer Detection,2014,27,19,7,0,3936,lin gui,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used."
C14-1095,Identification of Basic Phrases for {K}azakh Language using Maximum Entropy Model,2014,12,3,2,0,40262,gulila altenbek,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper proposes the definition, classification and structure of the Kazakh basic phrases, and sets up a framework for classifying them according to their syntactic functions. Meanwhile, the structure of the Kazakh basic phrases were analyzed; and the determination of the Kazakh basic phrases collocation and extraction of the Kazakh basic phrases based on rules were followed. The Maximum Entropy (ME) model uses for the identification of the phrases from texts and achieved a result of automatic identification of Kazakh phrases with an accuracy of 78.22% based on rules System and additional artificial modification. Design feature of this ME model join rely on templates of Kazakh Word, part of speech, affixes. Experimental results show that the accuracy rate reached 87.89xefxbcx85xefxbcx8e"
C14-1127,Hybrid Deep Belief Networks for Semi-supervised Sentiment Classification,2014,58,11,3,0,40283,shusen zhou,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we develop a novel semi-supervised learning algorithm called hybrid deep belief networks (HDBN), to address the semi-supervised sentiment classification problem with deep learning. First, we construct the previous several hidden layers using restricted Boltzmann machines (RBM), which can reduce the dimension and abstract the information of the reviews quickly. Second, we construct the following hidden layers using convolutional restricted Boltzmann machines (CRBM), which can abstract the information of reviews effectively. Third, the constructed deep architecture is fine-tuned by gradient-descent based supervised learning with an exponential loss function. We did several experiments on five sentiment classification datasets, and show that HDBN is competitive with previous semi-supervised learning algorithm. Experiments are also conducted to verify the effectiveness of our proposed method with different number of unlabeled reviews."
W13-3616,A Hybrid Model For Grammatical Error Correction,2013,16,11,4,1,14586,yang xiang,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"This paper presents a hybrid model for the CoNLL-2013 shared task which focuses on the problem of grammatical error correction. This yearxe2x80x99s task includes determiner, preposition, noun number, verb form, and subject-verb agreement errors which is more comprehensive than previous error correction tasks. We correct these five types of errors in different modules where either machine learning based or rule-based methods are applied. Preprocessing and post-processing procedures are employed to keep idiomatic phrases from being corrected. We achieved precision of 35.65%, recall of 16.56%, F1 of 22.61% in the official evaluation and precision of 41.75%, recall of 20.29%, F1 of 27.3% in the revised version. Some further comparisons employing different strategies are made in our experiments."
P13-4012,{PAL}: A Chatterbot System for Answering Domain-specific Questions,2013,9,4,3,1,15070,yuanchao liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"In this paper, we propose PAL, a prototype chatterbot for answering non-obstructive psychological domain-specific questions. This system focuses on providing primary suggestions or helping people relieve pressure by extracting knowledge from online forums, based on which the chatterbot system is constructed. The strategies used by PAL, including semantic-extension-based question matching, solution management with personal information consideration, and XML-based knowledge pattern construction, are described and discussed. We also conduct a primary test for the feasibility of our system."
P13-2146,Multimodal {DBN} for Predicting High-Quality Answers in c{QA} portals,2013,18,24,5,0,6700,haifeng hu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we address the problem for predicting cQA answer quality as a classification task. We propose a multimodal deep belief nets based approach that operates in two stages: First, the joint representation is learned by taking both textual and non-textual features into a deep learning network. Then, the joint representation learned by the network is used as input features for a linear classifier. Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach."
I13-1086,Automatic Corpora Construction for Text Classification,2013,15,1,3,0,36734,dandan wang,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Since the machines become more and more intelligent, it is reasonable to expect the automatic construction of text classifiers by given just the objective categories. As trade-off solutions, existing researches usually provide additional information to the category terms to enhance the performance of a classifier. Unique from them, in this paper, we construct the standard corpora from the web by just providing text categories. Since there are millions of manually constructed websites, it is hopeful to find out proper text categorization (TC) knowledge. So we directly go to the web and use the hierarchies implied in navigation bars to extract and verify TC resources. By addressing the issues of navigation bar recognition and text filtering, the corpora are constructed for given text categories and the classifiers are trained based on them. We conduct our experiments on the large scale of webpages collected from the 500 top English websites on Alexa. The Open Directory Project (ODP) is used as testing corpus. Experimental results show that, being compared with the classifier based on manually labeled corpus, the classifier trained on auto constructed corpora reaches comparable performance for the categories that are well covered by the training corpus."
I13-1148,Grammatical Error Correction Using Feature Selection and Confidence Tuning,2013,13,1,3,1,14586,yang xiang,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper proposes a novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors. Most previous machine learning based researches empirically collected features which may bring about noises and increase the computational complexity. Meanwhile, the predicted result is largely affected by the threshold setting of a classifier which can easily lead to low performance but hasnxe2x80x99t been well developed yet. To address these problems, we employ genetic algorithm for feature selection and confidence tuning to reinforce the motivation of correction. Comparative experiments on the NUCLE corpus show that our approach could efficiently reduce feature dimensionality and enhance the final F1 value for the article error correction problem."
W12-4507,A Mixed Deterministic Model for Coreference Resolution,2012,14,8,4,0,40848,bo yuan,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"This paper presents a mixed deterministic model for coreference resolution in the CoNLL-2012 shared task. We separate the two main stages of our model, mention detection and coreference resolution, into several sub-tasks which are solved by machine learning method and deterministic rules based on multi-filters, such as lexical, syntactic, semantic, gender and number information. We participate in the closed track for English and Chinese, and also submit an open result for Chinese using tools to generate the required features. Finally, we reach the average F1 scores 58.68, 60.69 and 61.02 on the English closed task, Chinese closed and open tasks."
C12-3059,Generating Questions from Web Community Contents,2012,9,1,4,1,22525,baoxun wang,Proceedings of {COLING} 2012: Demonstration Papers,0,"Large amounts of knowledge exist in the user-generated contents of web communities. Generating questions from such community contents to form the question-answer pairs is an effective way to collect and manage the knowledge in the web. The parser or rule based question generation (QG) methods have been widely studied and applied. Statistical QG aims to provide a strategy to handle the rapidly growing web data by alleviating the manual work. This paper proposes a deep belief network (DBN) based approach to address the statistical QG problem. This problem is considered as a three-step task: question type determination, concept selection and question construction. The DBNs are introduced to generate the essential words for question type determination and concept selection. Finally, a simple rule based method is used to construct questions with the generated words. The experimental results show that our approach is promising for the web community oriented question generation."
W11-1724,Instance Level Transfer Learning for Cross Lingual Opinion Analysis,2011,14,4,3,0.401423,1816,ruifeng xu,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language. Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the influence of low quality translated examples. Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. These two algorithms are applied to sentence- and document-level cross lingual opinion analysis tasks, respectively. The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data."
I11-1168,Diversifying Information Needs in Results of Question Retrieval,2011,15,0,2,1,34339,yaoyun zhang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Information need is an important factor in question retrieval. This paper proposes a method to diversify the results of question retrieval in term of types of information needs. CogQTaxo, a question hierarchy is leveraged to represent usersxe2x80x99 information needs cognitively from three linguistic levels. Based on a prediction model of question types, three factors, i.e., scores of IR model, question type similarity and question type novelty are linearly combined to re-rank the retrieved questions. Preliminary experimental results show that the proposed method enhances the question retrieval performance in information coverage and diversity."
W10-4124,{K}azakh Segmentation System of Inflectional Affixes,2010,3,16,2,0,40262,gulila altenbek,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-3002,A Cascade Method for Detecting Hedges and their Scope in Natural Language Text,2010,5,35,2,1,14656,buzhou tang,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"Detecting hedges and their scope in natural language text is very important for information inference. In this paper, we present a system based on a cascade method for the CoNLL-2010 shared task. The system composes of two components: one for detecting hedges and another one for detecting their scope. For detecting hedges, we build a cascade subsystem. Firstly, a conditional random field (CRF) model and a large margin-based model are trained respectively. Then, we train another CRF model using the result of the first phase. For detecting the scope of hedges, a CRF model is trained according to the result of the first subtask. The experiments show that our system achieves 86.36% F-measure on biological corpus and 55.05% F-measure on Wikipedia corpus for hedge detection, and 49.95% F-measure on biological corpus for hedge scope detection. Among them, 86.36% is the best result on biological corpus for hedge detection."
P10-1125,Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities,2010,20,45,2,1,22525,baoxun wang,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven question-answering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora."
C10-2173,Active Deep Networks for Semi-Supervised Sentiment Classification,2010,21,54,3,0,40283,shusen zhou,Coling 2010: Posters,0,"This paper presents a novel semi-supervised learning algorithm called Active Deep Networks (ADN), to address the semi-supervised sentiment classification problem with active learning. First, we propose the semi-supervised learning method of ADN. ADN is constructed by Restricted Boltzmann Machines (RBM) with unsupervised learning using labeled data and abundant of unlabeled data. Then the constructed structure is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Second, we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data. Then ADN architecture is trained by the selected labeled data and all unlabeled data. Experiments on five sentiment classification datasets show that ADN outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification."
W09-1217,A Joint Syntactic and Semantic Dependency Parsing System based on Maximum Entropy Models,2009,11,1,5,1,14656,buzhou tang,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"A joint syntactic and semantic dependency parsing system submitted to the CoNLL-2009 shared task is presented in this paper. The system is composed of three components: a syntactic dependency parser, a predicate classifier and a semantic parser. The first-order MSTParser is used as our syntactic dependency pasrser. Projective and non-projective MSTParsers are compared with each other on seven languages. Predicate classification and semantic parsing are both recognized as classification problem, and the Maximum Entropy Models are used for them in our system. For semantic parsing and predicate classifying, we focus on finding optimized features on multiple languages. The average Macro F1 Score of our system is 73.97 for joint task in closed challenge."
Y08-1051,Chunking with Max-Margin {M}arkov Networks,2008,14,0,3,1,14656,buzhou tang,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we apply Max-Margin Markov Networks (M3Ns) to English base phrases chunking, which is a large margin approach combining both the advantages of graphical models(such as Conditional Random Fields, CRFs) and kernel-based approaches (such as Support Vector Machines, SVMs) to solve the problems of multi-label multi-class supervised classification. To show the efficiency of M3Ns, we compare it with CRFs and other relative systems on the data set of CoNLL-2000 comprehensively. The experiment results show that M3Ns achieves state-of-the-art performance with strong generalization ability, which is better than CRFs."
W08-2130,Discriminative Learning of Syntactic and Semantic Dependencies,2008,5,3,4,0,19839,lu li,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"A Maximum Entropy Model based system for discriminative learning of syntactic and semantic dependencies submitted to the CoNLL-2008 shared task (Surdeanu, et al., 2008) is presented in this paper. The system converts the dependency learning task to classification issues and reconstructs the dependent relations based on classification results. Finally F1 scores of 86.69, 69.95 and 78.35 are obtained for syntactic dependencies, semantic dependencies and the whole system respectively in closed challenge. For open challenge the corresponding F1 scores are 86.69, 68.99 and 77.84."
W08-1601,Semantic Chunk Annotation for complex questions using Conditional Random Field,2008,15,4,5,0,44803,shixi fan,Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions,0,"This paper presents a CRF (Conditional Random Field) model for Semantic Chunk Annotation in a Chinese Question and Answering System (SCACQA). The model was derived from a corpus of real world questions, which are collected from some discussion groups on the Internet. The questions are supposed to be answered by other people, so some of the questions are very complex. Mutual information was adopted for feature selection. The training data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences. The result shows an F-score of 93.07%."
I08-4026,A Study of {C}hinese Lexical Analysis Based on Discriminative Models,2008,5,2,4,1,45149,guanglu sun,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper briefly describes our system in The Fourth SIGHAN Bakeoff. Discriminative models including maximum entropy model and conditional random fields are utilized in Chinese word segmentation and named entity recognition with different tag sets and features. Transformation-based learning model is used in part-of-speech tagging. Evaluation shows that our system achieves the F-scores: 92.64% and 92.73% in NCC Word Segmentation close and open tests, 89.11% in MSRA name entity recognition open test, 91.13% and 91.97% in PKU part-of-speech tagging close and open tests. All the results get medium performances on the bakeoff tracks."
I08-1008,Name Origin Recognition Using Maximum Entropy Model and Diverse Features,2008,6,2,6,0,3694,min zhang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Name origin recognition is to identify the source language of a personal or location name. Some early work used either rulebased or statistical methods with single knowledge source. In this paper, we cast the name origin recognition as a multi-class classification problem and approach the problem using Maximum Entropy method. In doing so, we investigate the use of different features, including phonetic rules, ngram statistics and character position information for name origin recognition. Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44% for names written in English and 98.10% for names written in Chinese, which are significantly and consistently better than those in reported work."
O07-5006,Exploiting {P}inyin Constraints in {P}inyin-to-Character Conversion Task: a Class-Based Maximum Entropy {M}arkov Model Approach,2007,28,4,3,1,19520,jinghui xiao,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 3, September 2007: Special Issue on Invited Papers from {ISCSLP} 2006",0,"The Pinyin-to-Character Conversion task is the core process of the Chinese pinyin-based input method. Statistical language model techniques, especially ngram-based models, are mostly adopted to solve that task. However, the ngram model only focuses on the constraints between characters, ignoring the pinyin constraints in the input pinyin sequence. This paper improves the performance of the Pinyin-to-Character Conversion system through exploitation of the pinyin constraints. The MEMM framework is used to describe the pinyin constraints and the character constraints. A Class-based MEMM (C-MEMM) model is proposed to address the MEMM efficiency problem in the Pinyin-to-Character Conversion task. The C-MEMM probability functions are strictly deduced and well formulized according to the Bayes rule and the Markov property. Both the cases of hard class and soft class are well discussed. In the experiments, C-MEMM outperforms the traditional ngram model significantly by exploitation of the pinyin constraints in the Pinyin-to-Character Conversion task. In addition, C-MEMM can well utilize the syntax and semantic information in word class and further improve the system performance."
O07-4002,An Empirical Study of Non-Stationary Ngram Model and its Smoothing Techniques,2007,18,2,3,1,19520,jinghui xiao,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 2, June 2007",0,"Recently many new techniques have been proposed for language modeling, such as ME, MEMM and CRF. However, the ngram model is still a staple in practical applications. It is well worthy of studying how to improve the performance of the ngram model. This paper enhances the traditional ngram model by relaxing the stationary hypothesis on the Markov chain and exploiting the word positional information. Such an assumption is made that the probability of the current word is determined not only by history words but also by the words positions in the sentence. The non-stationary ngram model (NS ngram model) is proposed. Several related issues are discussed in detail, including the definition of the NS ngram model, the representation of the word positional information and the estimation of the conditional probability. In addition, three smoothing approaches are proposed to solve the data sparseness problem of the NS ngram model. Several smoothing algorithms are presented in each approach. In the experiments, the NS ngram model is evaluated on the pinyin-to-character conversion task which is the core technique of the Chinese text input method. Experimental results show that the NS ngram model outperforms the traditional ngram model significantly by the exploitation of the word positional information. In addition, the proposed smoothing techniques solve the data sparseness problem of the NS ngram model effectively with great error rate reduction."
W06-0134,A Pragmatic {C}hinese Word Segmentation System,2006,6,5,3,0,24937,wei jiang,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents our work for participation in the Third International Chinese Word Segmentation Bakeoff. We apply several processing approaches according to the corresponding sub-tasks, which are exhibited in real natural language. In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task. The experiment indicates that this system achieves Fmeasure 96.8% in MSRA open test in the third SIGHAN-2006 bakeoff."
O06-5005,A Pragmatic {C}hinese Word Segmentation Approach Based on Mixing Models,2006,13,6,3,0,24937,wei jiang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 4, {D}ecember 2006",0,"A pragmatic Chinese word segmentation approach is presented in this paper based on mixing language models. Chinese word segmentation is composed of several hard sub-tasks, which usually encounter different difficulties. The authors apply the corresponding language model to solve each special sub-task, so as to take advantage of each model. First, a class-based trigram is adopted in basic word segmentation, which applies the Absolute Discount Smoothing algorithm to overcome data sparseness. The Maximum Entropy Model (ME) is also used to identify Named Entities. Second, the authors propose the application of rough sets and average mutual information, etc. to extract special features. Finally, some features are extended through the combination of the word cluster and the thesaurus. The authors' system participated in the Second International Chinese Word Segmentation Bakeoff, and achieved 96.7 and 97.2 in F-measure in the PKU and MSRA open tests, respectively."
O06-3002,{C}hinese Chunking Based on Maximum Entropy {M}arkov Models,2006,30,13,3,1,45149,guanglu sun,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 2, June 2006",0,"This paper presents a new Chinese chunking method based on maximum entropy Markov models. We firstly present two types of Chinese chunking specifications and data sets, based on which the chunking models are applied. Then we describe the hidden Markov chunking model and maximum entropy chunking model. Based on our analysis of the two models, we propose a maximum entropy Markov chunking model that combines the transition probabilities and conditional probabilities of states. Experimental results for two types of data sets show that this approach achieves impressive accuracy in terms of the F-score: 91.02% and 92.68%, respectively. Compared with the hidden Markov chunking model and maximum entropy chunking model, based on the same data set, the new chunking model achieves better performance."
I05-3001,Detecting Segmentation Errors in {C}hinese Annotated Corpus,2005,6,3,3,0,1800,chengjie sun,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation error candidates found in a segmented corpus and then on which the segmentation errors are identified manually. Segmentation error rate of a gold standard corpus can be given using our method. In Peking University (PK) and Academic Sinica (AS) test corpora of Special Interest Group for Chinese Language Processing (SIGHAN) Bakeoff1, 1.29% and 2.26% segmentation error rates are detected by our method. These errors decrease the F-measure of SIGHAN Bakeoff1 baseline test by 1.36% in PK test data and 1.93% in AS test data respectively. This work was done while Chengjie Sun was visiting Microsoft Research Asia."
I05-1072,Principles of Non-stationary Hidden {M}arkov Model and Its Applications to Sequence Labeling Task,2005,9,6,3,0,19520,jinghui xiao,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Hidden Markov Model (Hmm) is one of the most popular language models. To improve its predictive power, one of Hmm hypotheses, named limited history hypothesis, is usually relaxed. Then Higher-order Hmm is built up. But there are several severe problems hampering the applications of high-order Hmm, such as the problem of parameter space explosion, data sparseness problem and system resource exhaustion problem. From another point of view, this paper relaxes the other Hmm hypothesis, named stationary (time invariant) hypothesis, makes use of time information and proposes a non-stationary Hmm (NSHmm). This paper describes NSHmm in detail, including its definition, the representation of time information, the algorithms and the parameter space and so on. Moreover, to further reduce the parameter space for mobile applications, this paper proposes a variant form of NSHmm (VNSHmm). Then NSHmm and VNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-character conversion. Experiment results show that compared with Hmm, NSHmm and VNSHmm can greatly reduce the error rate in both of the two tasks, which proves that they have much more predictive power than Hmm does."
