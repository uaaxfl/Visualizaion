2021.scil-1.55,How to marry a star: Probabilistic constraints for meaning in context,2021,-1,-1,1,1,2274,katrin erk,Proceedings of the Society for Computation in Linguistics 2021,0,None
2021.naacl-main.129,Did they answer? Subjective acts and intents in conversational discourse,2021,-1,-1,4,1,3638,elisa ferracane,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective{\_}discourse."
2020.emnlp-main.427,Help! Need Advice on Identifying Advice,2020,-1,-1,4,0,18040,venkata govindarajan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research."
2020.conll-1.17,When is a bishop not like a rook? When it{'}s like a rabbi! Multi-prototype {BERT} embeddings for estimating semantic relationships,2020,-1,-1,2,0,20963,gabriella chronis,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT{'}s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness."
2020.coling-main.268,Leveraging {W}ord{N}et Paths for Neural Hypernym Prediction,2020,-1,-1,4,0,21364,yejin cho,Proceedings of the 28th International Conference on Computational Linguistics,0,"We formulate the problem of hypernym prediction as a sequence generation task, where the sequences are taxonomy paths in WordNet. Our experiments with encoder-decoder models show that training to generate taxonomy paths can improve the performance of direct hypernym prediction. As a simple but powerful model, the hypo2path model achieves state-of-the-art performance, outperforming the best benchmark by 4.11 points in hit-at-one (H@1)."
W19-2704,From News to Medical: Cross-domain Discourse Segmentation,2019,20,0,4,1,3638,elisa ferracane,Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019,0,"The first step in discourse analysis involves dividing a text into segments. We annotate the first high-quality small-scale medical corpus in English with discourse segments and analyze how well news-trained segmenters perform on this domain. While we expectedly find a drop in performance, the nature of the segmentation errors suggests some problems can be addressed earlier in the pipeline, while others would require expanding the corpus to a trainable size to learn the nuances of the medical domain."
P19-1062,Evaluating Discourse in Structured Text Representations,2019,22,1,4,1,3638,elisa ferracane,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text{'}s discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance."
D19-1273,Query-focused Scenario Construction,2019,0,0,3,1,10665,su wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"The news coverage of events often contains not one but multiple incompatible accounts of what happened. We develop a query-based system that extracts compatible sets of events (scenarios) from such data, formulated as one-class clustering. Our system incrementally evaluates each event{'}s compatibility with already selected events, taking order into account. We use synthetic data consisting of article mixtures for scalable training and evaluate our model on a new human-curated dataset of scenarios about real-world news topics. Stronger neural network models and harder synthetic training settings are both important to achieve high performance, and our final scenario construction system substantially outperforms baselines based on prior work."
N18-2049,Modeling Semantic Plausibility by Injecting World Knowledge,2018,12,3,3,1,10665,su wang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested. However both are physically plausible events. This paper introduces the task of semantic plausibility: recognizing plausible but possibly novel events. We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball. Simple models based on distributional representations perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost. Our error analysis shows that our new dataset is a great testbed for semantic plausibility models: more sophisticated knowledge representation and propagation could address many of the remaining errors."
N18-1044,Deep Neural Models of Semantic Shift,2018,0,16,2,0,27050,alex rosenfeld,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Diachronic distributional models track changes in word use over time. In this paper, we propose a deep neural network diachronic distributional model. Instead of modeling lexical change via a time series as is done in previous work, we represent time as a continuous variable and model a word{'}s usage as a function of time. Additionally, we have also created a novel synthetic task which measures a model{'}s ability to capture the semantic trajectory. This evaluation quantitatively measures how well a model captures the semantic trajectory of a word over time. Finally, we explore how well the derivatives of our model can be used to measure the speed of lexical change."
N18-1076,Implicit Argument Prediction with Event Knowledge,2018,9,3,2,0,29431,pengxiang cheng,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale. This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions. We show that our model has superior performance on both synthetic and natural data."
D18-1175,Picking Apart Story Salads,2018,0,3,4,1,10665,su wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"During natural disasters and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce \textit{Story Salads}, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence."
I17-1021,Distributional Modeling on a Diet: One-shot Word Learning from Text Only,2017,0,3,3,1,10665,su wang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative."
W16-6112,Leveraging coreference to identify arms in medical abstracts: An experimental study,2016,29,2,4,1,3638,elisa ferracane,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
N16-1131,{PIC} a Different Word: A Simple Model for Lexical Substitution in Context,2016,15,3,2,1,3621,stephen roller,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The Lexical Substitution task involves selecting and ranking lexical paraphrases for a target word in a given sentential context. We present PIC, a simple measure for estimating the appropriateness of substitutes in a given context. PIC outperforms another simple, comparable model proposed in recent work, especially when selecting substitutes from the entire vocabulary. Analysis shows that PIC improves over baselines by incorporating frequency biases into predictions."
J16-4007,Representing Meaning with a Combination of Logical and Distributional Models,2016,87,25,4,0,34477,beltagy,Computational Linguistics,0,"NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.n n We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system:1 1 Logical representation focuses on representing the input problems in probabilistic logic; 2 knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3 probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.2"
J16-2003,Word Sense Clustering and Clusterability,2016,52,11,3,0,9803,diana mccarthy,Computational Linguistics,0,"Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.n n We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster. We test two ways of measuring clusterability: 1 existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and 2 the idea that if a lemma is more clusterable, two clusterings based on two different views of the same data points will be more congruent. The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations. We apply automatic clustering to the manual annotations. We use manual annotations because we want the representations of the instances that we cluster to be as informative and clean as possible. We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-1 clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering."
D16-1234,Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment,2016,23,15,2,1,3621,stephen roller,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"We consider the task of predicting lexical entailment using distributional vectors. We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs. We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations. We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis. Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets."
W15-0119,On the Proper Treatment of Quantifiers in Probabilistic Logic Semantics,2015,26,5,2,1,37162,islam beltagy,Proceedings of the 11th International Conference on Computational Semantics,0,"As a format for describing the meaning of natural language sentences, probabilistic logic combines the expressivity of first-order logic with the ability to handle graded information in a principled fashion. But practical probabilistic logic frameworks usually assume a finite domain in which each entity corresponds to a constant in the logic (domain closure assumption). They also assume a closed world where everything has a very low prior probability. These assumptions lead to some problems in the inferences that these systems make. In this paper, we show how to formulate Textual Entailment (RTE) inference problems in probabilistic logic in a way that takes the domain closure and closed-world assumptions into account. We evaluate our proposed technique on three RTE datasets, on a synthetic dataset with a focus on complex forms of quantification, on FraCas and on one more natural dataset. We show that our technique leads to improvements on the more natural dataset, and achieves 100% accuracy on the synthetic dataset and on the relevant part of FraCas."
W14-3009,Who Evoked that Frame? Some Thoughts on Context Effects and Event Types,2014,17,0,1,1,2274,katrin erk,Proceedings of Frame Semantics in {NLP}: A Workshop in Honor of Chuck {F}illmore (1929-2014),0,"Lexical substitution is an annotation task in which annotators provide one-word paraphrases (lexical substitutes) for individual target words in a sentence context. Lexical substitution yields a fine-grained characterization of word meaning that can be done by non-expert annotators. We discuss results of a recent lexical substitution annotation effort, where we found strong contextual modulation effects: Many substitutes were not synonyms, hyponyms or hypernyms of the targets, but were highly specific to the situation at hand. This data provides some food for thought for framesemantic analysis."
W14-2402,Semantic Parsing using Distributional Semantics and Probabilistic Logic,2014,22,15,2,1,37162,islam beltagy,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference. Instead, we use distributional semantics to generate only the relevant part of an on-the-fly ontology. Sentences and the on-the-fly ontology are represented in probabilistic logic. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL). This semantic parsing approach is evaluated on two tasks, Textual Entitlement (RTE) and Textual Similarity (STS), both accomplished using inference in probabilistic logic. Experiments show the potential of the approach."
S14-2141,{UT}exas: Natural Language Semantics using Distributional Semantics and Probabilistic Logic,2014,25,12,4,1,37162,islam beltagy,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We represent natural language semantics by combining logical and distributional information in probabilistic logic. We use Markov Logic Networks (MLN) for the RTE task, and Probabilistic Soft Logic (PSL) for the STS task. The system is evaluated on the SICK dataset. Our best system achieves 73% accuracy on the RTE task, and a Pearsonxe2x80x99s correlation of 0.71 on the STS task."
P14-1114,Probabilistic Soft Logic for Semantic Textual Similarity,2014,28,43,2,1,37162,islam beltagy,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Probabilistic Soft Logic (PSL) is a recently developed framework for probabilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach."
E14-1057,What Substitutes Tell Us - Analysis of an {``}All-Words{''} Lexical Substitution Corpus,2014,33,23,2,0,40088,gerhard kremer,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first large-scale English xe2x80x9callwords lexical substitutionxe2x80x9d corpus. The size of the corpus provides a rich resource for investigations into word meaning. We investigate the nature of lexical substitute sets, comparing them to WordNet synsets. We find them to be consistent with, but more fine-grained than, synsets. We also identify significant differences to results for paraphrase ranking in context reported for the SEMEVAL lexical substitution data. This highlights the influence of corpus construction approaches on evaluation results."
C14-1097,Inclusive yet Selective: Supervised Distributional Hypernymy Detection,2014,31,80,2,1,3621,stephen roller,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. We find that this hypothesis only holds when it is applied to relevant dimensions. We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion."
W13-0109,Towards a semantics for distributional representations,2013,30,15,1,1,2274,katrin erk,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"Distributional representations have recently been proposed as a general-purpose representation of natural language meaning, to replace logical form. There is, however, one important difference between logical and distributional representations: Logical languages have a clear semantics, while distributional representations do not. In this paper, we propose a semantics for distributional representations that links points in vector space to mental concepts. We extend this framework to a joint semantics of logic and distributions by linking intensions of logical expressions to mental concepts."
S13-1002,{M}ontague Meets {M}arkov: Deep Semantics with Probabilistic Logical Form,2013,42,57,5,1,37162,islam beltagy,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance."
J13-3003,Measuring Word Meaning in Context,2013,78,29,1,1,2274,katrin erk,Computational Linguistics,0,"Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators. Recently there have been several proposals for representing word meaning in context that diverge from the traditional use of a single best sense for each occurrence. They represent word meaning in context through multiple paraphrases, as points in vector space, or as distributions over latent senses. New methods of evaluating and comparing these different representations are needed.In this paper we propose two novel annotation schemes that characterize word meaning in context in a graded fashion. In WSsim annotation, the applicability of each dictionary sense is rated on an ordinal scale. Usim annotation directly rates the similarity of pairs of usages of the same lemma, again on a scale. We find that the novel annotation schemes show good inter-annotator agreement, as well as a strong correlation with traditional single-sense annotation and ..."
W11-0112,Integrating Logical Representations with Probabilistic Information using {M}arkov {L}ogic,2011,17,48,2,0,8780,dan garrette,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"First-order logic provides a powerful and flexible mechanism for representing natural language semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning. This paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of Statistical Relational AI. Specifically, we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context."
P11-1108,Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models,2011,34,23,3,0,44683,elias ponvert,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing---the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsu-pervised parser, Seginer's (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL."
W10-2803,"What Is Word Meaning, Really? (And How Can Distributional Models Help Us Describe It?)",2010,60,26,1,1,2274,katrin erk,Proceedings of the 2010 Workshop on {GE}ometrical Models of Natural Language Semantics,0,"In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. Or move away from dictionary senses completely, and only model similarities between individual word usages. We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models."
P10-2017,Exemplar-Based Models for Word Meaning in Context,2010,20,70,1,1,2274,katrin erk,Proceedings of the {ACL} 2010 Conference Short Papers,0,"This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vector-per-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models."
J10-4007,"A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences",2010,75,62,1,1,2274,katrin erk,Computational Linguistics,0,"We present a vector space-based model for selectional preferences that predicts plausibility scores for argument headwords. It does not require any lexical resources (such as WordNet). It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads.n n We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes. We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus. As for parameters, we identify settings that yield good performance across a range of experimental conditions. However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items."
D10-1020,"Crouching {D}irichlet, Hidden {M}arkov Model: Unsupervised {POS} Tagging with Context Local Tag Generation",2010,32,12,2,1,46389,taesun moon,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We define the crouching Dirichlet, hidden Markov model (CDHMM), an HMM for part-of-speech tagging which draws state prior distributions for each local document context. This simple modification of the HMM takes advantage of the dichotomy in natural language between content and function words. In contrast, a standard HMM draws all prior distributions once over all states and it is known to perform poorly in unsupervised and semi-supervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM) is surprisingly effective."
W09-3711,Supporting inferences in semantic space: representing words as regions,2009,22,16,1,1,2274,katrin erk,Proceedings of the Eight International Conference on Computational Semantics,0,"Semantic space models represent the meaning of a word as a vector in high-dimensional space. They offer a framework in which the meaning representation of a word can be computed from its context, but the question remains how they support inferences. While there has been some work on paraphrase-based inferences in semantic space, it is not clear how semantic space models would support inferences involving hyponymy, like horse ran xe2x86x92 animal moved. In this paper, we first discuss what a point in semantic space stands for, contrasting semantic space with Gardenforsian conceptual space. Building on this, we propose an extension of the semantic space representation from a point to a region. We present a model for learning a region representation for word meaning in semantic space, based on the fact that points at close distance tend to represent similar meanings. We show that this model can be used to predict, with high precision, when a hyponymy-based inference rule is applicable. Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences."
W09-3207,Measuring semantic relatedness with vector space models and random walks,2009,-1,-1,2,0,46912,amacc herdaugdelen,Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4),0,None
W09-1109,Representing words as regions in vector space,2009,28,33,1,1,2274,katrin erk,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009),0,Vector space models of word meaning typically represent the meaning of a word as a vector computed by summing over all its corpus occurrences. Words close to this point in space can be assumed to be similar to it in meaning. But how far around this point does the region of similar meaning extend? In this paper we discuss two models that represent word meaning as regions in vector space. Both representations can be computed from traditional point representations in vector space. We find that both models perform at over 95% F-score on a token classification task.
W09-0208,Paraphrase Assessment in Structured Vector Space: Exploring Parameters and Datasets,2009,26,30,1,1,2274,katrin erk,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"The appropriateness of paraphrases for words depends often on context: grab can replace catch in catch a ball, but not in catch a cold. Structured Vector Space (SVS) (Erk and Pado, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases. This paper investigates best-practice parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation."
P09-1002,Investigations on Word Senses and Word Usages,2009,27,57,1,1,2274,katrin erk,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The vast majority of work on word senses has relied on predefined sense inventories and an annotation schema where each word instance is tagged with the best fitting sense. This paper examines the case for a graded notion of word meaning in two experiments, one which uses WordNet senses in a graded fashion, contrasted with the winner takes all annotation, and one which asks annotators to judge the similarity of two usages. We find that the graded responses correlate with annotations from previous datasets, but sense assignments are used in a way that weakens the case for clear cut sense boundaries. The responses from both experiments correlate with the overlap of paraphrases from the English lexical substitution task which bodes well for the use of substitutes as a proxy for word sense. This paper also provides two novel datasets which can be used for evaluating computational systems."
D09-1046,Graded Word Sense Assignment,2009,32,39,1,1,2274,katrin erk,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Word sense disambiguation is typically phrased as the task of labeling a word in context with the best-fitting sense from a sense inventory such as WordNet. While questions have often been raised over the choice of sense inventory, computational linguists have readily accepted the best-fitting sense methodology despite the fact that the case for discrete sense boundaries is widely disputed by lexical semantics researchers. This paper studies graded word sense assignment, based on a recent dataset of graded word sense annotation."
D09-1070,Unsupervised morphological segmentation and clustering with document boundaries,2009,28,8,2,1,46389,taesun moon,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and affixes. This typically involves heuristic search procedures and calibrating multiple arbitrary thresholds. We present a simple approach that uses no thresholds other than those involved in standard application of X2 significance testing. A key part of our approach is using document boundaries to constrain generation of candidate stems and affixes and clustering morphological variants of a given word stem. We evaluate our model on English and the Mayan language Uspanteko; it compares favorably to two benchmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values.
W08-0201,"Teaching Computational Linguistics to a Large, Diverse Student Body: Courses, Tools, and Interdepartmental Interaction",2008,11,3,2,0.225483,1071,jason baldridge,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We describe course adaptation and development for teaching computational linguistics for the diverse body of undergraduate and graduate students the Department of Linguistics at the University of Texas at Austin. We also discuss classroom tools and teaching aids we have used and created, and we mention our efforts to develop a campus-wide computational linguistics program."
D08-1094,A Structured Vector Space Model for Word Meaning in Context,2008,34,285,1,1,2274,katrin erk,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account.n n We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
W07-1528,{IGT}-{XML}: An {XML} Format for Interlinearized Glossed Text,2007,7,33,2,0,1316,alexis palmer,Proceedings of the Linguistic Annotation Workshop,0,"We propose a new XML format for representing interlinearized glossed text (IGT), particularly in the context of the documentation and description of endangered languages. The proposed representation, which we call IGT-XML, builds on previous models but provides a more loosely coupled and flexible representation of different annotation layers. Designed to accommodate both selective manual reannotation of individual layers and semi-automatic extension of annotation, IGT-XML is a first step toward partial automation of the production of IGT."
S07-1018,{S}em{E}val-2007 Task 19: Frame Semantic Structure Extraction,2007,10,82,3,0,1267,collin baker,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http://framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects). The training data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation."
P07-1028,"A Simple, Similarity-based Model for Selectional Preferences",2007,17,123,1,1,2274,katrin erk,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnikxe2x80x99s WordNet-based model and the EM-based clustering model, but has coverage problems."
D07-1042,"Flexible, Corpus-Based Modelling of Human Plausibility Judgements",2007,23,35,3,0,411,sebastian pado,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone."
N06-1017,Unknown word sense detection as outlier detection,2006,17,20,1,1,2274,katrin erk,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We address the problem of unknown word sense detection: the identification of corpus occurrences that are not covered by a given sense inventory. We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82."
burchardt-etal-2006-salsa,The {SALSA} Corpus: a {G}erman Corpus Resource for Lexical Semantics,2006,23,144,2,0,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications."
burchardt-etal-2006-salto,{SALTO} - A Versatile Multi-Level Annotation Tool,2006,8,67,2,0,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control."
erk-pado-2006-shalmaneser,Shalmaneser - A Toolchain For Shallow Semantic Parsing,2006,19,54,1,1,2274,katrin erk,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents Shalmaneser, a software package for shallow semantic parsing, the automatic assignment of semantic classes and roles to free text. Shalmaneser is a toolchain of independent modules communicating through a common XML format. System output can be inspected graphically. Shalmaneser can be used either as a Âblack boxÂ to obtain semantic parses for new datasets (classifiers for English and German frame-semantic analysis are included), or as a research platform that can be extended to new parsers, languages, or classification paradigms."
H05-1084,Analyzing Models for Semantic Role Assignment using Confusability,2005,19,5,1,1,2274,katrin erk,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We analyze models for semantic role assignment by defining a meta-model that abstracts over features and learning paradigms. This meta-model is based on the concept of role confusability, is defined in information-theoretic terms, and predicts that roles realized by less specific grammatical functions are more difficult to assign. We find that confusability is strongly correlated with the performance of classifiers based on syntactic features, but not for classifiers including semantic features. This indicates that syntactic features approximate a description of grammatical functions, and that semantic features provide an independent second view on the data."
W04-2413,Semantic Role Labelling With Chunk Sequences,2004,3,10,2,0,51408,ulrike baldewein,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"We describe a statistical approach to semantic role labelling that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set."
W04-0817,Semantic role labelling with similarity-based generalization using {EM}-based clustering,2004,8,20,2,0,51408,ulrike baldewein,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment. Our final score is Precision=73.6%, Recall=59.4% (F=65.7)."
erk-pado-2004-powerful,A Powerful and Versatile {XML} Format for Representing Role-semantic Annotation,2004,2,35,1,1,2274,katrin erk,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We present two XML formats for the description and encoding of semantic role information in corpora. The TIGER/SALSA XML format provides a modular representation for semantic roles and syntactic structure. The Text-SALSA XML format is a lightweight version of TIGER/SALSA XML designed for manual annotation with an XML editor rather than a special tool. Both formats can deal with underspecification, roles crossing the sentence boundary, compound splitting, and whole-sentence tags for meta-level comments."
heid-etal-2004-querying,Querying Both Time-aligned and Hierarchical Corpora with {NXT} Search,2004,6,20,5,0,24867,ulrich heid,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"One problem of the (re-)usability and exchange of annotated corpora is in the lack of standards in corpus formats and corpus query tools. This paper reports on the NXT Search tool, which was used to query two corpora with very different annotation formats. It is shown that with automatic data format conversion both corpora can be accessed and searched with NXT Search."
P03-1068,Towards a Resource for Lexical Semantics: A Large {G}erman Corpus with Extensive Semantic Annotation,2003,10,68,1,1,2274,katrin erk,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information, e.g. the construction of domain-independent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation."
E03-1027,Well-Nested Parallelism Constraints for Ellipsis Resolution,2003,15,2,1,1,2274,katrin erk,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The Constraint Language for Lambda Structures (CLLS) is an expressive tree description language. It provides a uniform framework for underspecified semantics, covering scope, ellipsis, and anaphora. Efficient algorithms exist for the sublanguage that models scope. But so far no terminating algorithm exists for sublanguages that model ellipsis. We introduce well-nested parallelism constraints and show that they solve this problem."
P01-1011,Underspecified Beta Reduction,2001,15,6,2,0,53863,manuel bodirsky,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"For ambiguous sentences, traditional semantics construction produces large numbers of higher-order formulas, which must then be xcexb2-reduced individually. Underspecified versions can produce compact descriptions of all readings, but it is not known how to perform xcexb2-reduction on these descriptions. We show how to do this using xcexb2-reduction constraints in the constraint language for xcexbb-structures (CLLS)."
