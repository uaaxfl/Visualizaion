2011.eamt-1.11,P07-2045,0,0.00648783,"Missing"
2011.eamt-1.11,N06-1014,0,0.0304209,"ents. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the num"
2011.eamt-1.11,W96-0201,0,0.0261497,"flows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a language pair, which is internally assigned a configuration file. GMA provides configuration fil"
2011.eamt-1.11,P97-1063,0,0.0245084,"for a range of widely-used state-of-the-art aligners which can be easily exchanged in user workflows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a"
2011.eamt-1.11,moore-2002-fast,0,0.0299673,"This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webservice will create and use an empty file. Hunalign also provides a set of optional parameters. Some of them are offered by the webservice (bisent, cautious and text), while two of them are activated internally (realign and 4 2.1.3 BSA BSA – Bilingual Sentence Aligner7 (Moore, 2002) – is a three-step hybrid approach. First, sentence-length based alignment is performed; second, statistical word alignment model is trained on the high probability aligned sentences and third, all sentences are realigned based on the word alignments. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://"
2011.eamt-1.11,P07-1003,0,0.0126663,"nly 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run"
2011.eamt-1.11,J03-1002,0,0.00596422,"Missing"
2011.eamt-1.11,P91-1023,0,0.187551,"d for English, French, German, Spanish and Italian have been obtained from Universit´e de Neuchˆatel.6 Type 2 character ISO code text 2 character ISO code text Table 1: Shared mandatory parameters. Webservices created for sentential aligners (Hunalign, GMA and BSA) are covered in section 2.1, while section 2.2 deals with sub-sentential aligners (GIZA++, BerkeleyAligner and OpenMaTrEx chunk aligner). 2.1 2.1.1 Sentential alignment Hunalign Hunalign (Varga et al., 2005)4 can work in two modes. If a bilingual dictionary is available, this information is combined with sentence-length information (Gale and Church, 1991) and used to identify sentence alignment. In the absence of a bilingual dictionary, it first identifies the alignment using sentence-length information only, then builds an automatic dictionary based on this alignment and finally realigns the text using this dictionary. This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webs"
2011.eamt-1.11,P09-1104,0,0.0224796,"tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run the model (default valu"
2011.eamt-1.40,W05-0909,0,0.038572,"was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English → French French → English English → Greek Greek → English dom env"
2011.eamt-1.40,2010.amta-papers.16,1,0.556343,"Missing"
2011.eamt-1.40,baroni-bernardini-2004-bootcat,0,0.0135991,"extracted 209 terms for the env domain and 86 for the lab domain. The weights assigned to the terms were signed integers indicating the relevance of each term to a topic-class. Topic-classes correspond to possible sub-categories of the domain. The other input for the crawler is a list of seed URLs relevant to the domain. The seeds for the env domain were selected from relevant lists in the Open Directory Project,5 a repository maintained by volunteer editors. For the lab domain, similar lists were not so easy to find. We therefore adopted a different method, namely using the BootCat toolkit (Baroni and Bernardini, 2004) to create random tuples (i.e. n-combinations of terms) from the terms included in the topic definition. We then ran a query for each tuple on the Yahoo! search engine,6 kept the first five URLs returned for each query and finally constructed the seed list with these URLs. Normalization, the next step in the workflow, concerned encoding identification based on the content_charset header of each document, and, if needed, conversion to UTF-8. Language identification was performed by a modified version of the n-gram-based Lingua::Identify7 tool, which was 4 http://eurovoc.europa.eu/ http://www.dm"
2011.eamt-1.40,eck-etal-2004-language,0,0.413861,"same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p."
2011.eamt-1.40,W08-0334,0,0.0158822,"for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific data and their type. If the data is available, it can be directly used to improve components of the MT system: word alignment and phrase extraction (Wu and Wang, 2004), language models (Koehn and Schroeder, 2007), and translation models (Nakov, 2008), usually by m"
2011.eamt-1.40,2005.eamt-1.19,0,0.704171,"e enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens"
2011.eamt-1.40,P05-1058,0,0.192602,"Missing"
2011.eamt-1.40,W07-0733,0,0.0609231,"utomatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general mode"
2011.eamt-1.40,P07-2045,0,0.00586506,".) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Doma"
2011.eamt-1.40,2005.mtsummit-papers.11,0,0.00681904,"eprocessing, training, tuning, decoding, postprocessing, and evaluation. 3.1 General-domain data As for other data-driven MT systems, MaTrEx requires certain data to be trained on, namely parallel data for translation models, monolingual data for language models, and parallel development data for tuning of system parameters. Parameter tuning is not strictly required but has a big influence on system performance. For the baseline system we decided to exploit the widely used data provided by the organizers of the series of SMT workshops (WPT 2005, WMT 2006–2010)1 : the Europarl parallel corpus (Koehn, 2005) version 5 as training data for translation models and language models, and WPT 2005 test set as the development data for parameter optimization. The Europarl parallel corpus is extracted from the proceedings of the European Parliament. For practical reasons we consider this corpus to contain general-domain texts. Version 5 released in Spring 2010 includes texts in 11 European languages including all languages of our interest (English, French, and Greek; see Table 1). Note that the amount of parallel data for English and Greek is only about one half of what is available for English and French."
2011.eamt-1.40,W02-1405,0,0.604769,"nology but also in grammar. In order to achieve optimal performance, an SMT system must be trained on data from the same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere,"
2011.eamt-1.40,J05-4003,0,0.0388536,"lsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus"
2011.eamt-1.40,W08-0320,0,0.0609529,"gium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific da"
2011.eamt-1.40,P03-1021,0,0.0040274,"8.1 Table 2: Web-crawled monolingual data statistics. lowercased) versions of the target sides of the parallel data are kept for training the Moses recaser. The lowercased versions of the target sides are used for training an interpolated 5-gram language model with Kneser-Ney discounting using the SRILM toolkit (Stolcke, 2002). Translation models are trained on the relevant parts of the Europarl corpus, lowercased and filtered on sentence level; we kept all sentence pairs having less than 100 words on each side and with length ratio within the interval h0.11,9.0i. Minimum error rate training (Och, 2003, MERT) is employed to optimize the model parameters on the development set. For decoding, test sentences are tokenized, lowercased, and translated by the trained system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4 4.1 Acquisition of in-domain resources Web crawling of monolingual data Our workflow for acquiring in-domain monolingual data consists of the following steps: focused web crawling, text normalization, language identification, document clean-up and near-duplicate detection. For"
2011.eamt-1.40,P02-1040,0,0.114242,"a different domain. The correctors confirmed that the process was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English →"
2011.eamt-1.40,wu-wang-2004-improving-domain,0,0.512028,"ll amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl c"
2011.eamt-1.40,W10-1720,1,\N,Missing
2012.eamt-1.10,P09-1104,0,0.0252006,"r for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence pairs. The aligners are executed using the default values for their parameters. All the experiments"
2012.eamt-1.10,moore-2002-fast,0,0.0491488,"eir feedback on Hunalign and Anymalign, respectively. We would like to thank Joachim Wagner for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence"
2012.eamt-1.10,J03-1002,0,0.00898825,"We would like to thank Joachim Wagner for his help on using the cluster. This research has been partially funded by the EU project PANACEA (7FP-ITC-248064). ∗ c 2012 European Association for Machine Translation. 57 100,000 proving the architecture by implementing limitation mechanisms that take into account the results. Time (seconds) 2 10,000 Evaluation We have integrated a range of state-of-the-art sentence and word aligners into the web service architecture. The sentence aligners included are Hunalign (Varga et al., 2005), GMA1 and BSA (Moore, 2002). As for word aligners, they are GIZA++ (Och and Ney, 2003), BerkeleyAligner (Haghighi et al., 2009) and Anymalign (Lardilleux and Lepage, 2009). For a detailed description of the integration please refer to (Toral et al., 2011). In order to evaluate the efficiency of the aligners, we have run them over different amounts of sentences of a bilingual corpus (from 5k to 100k adding 5k at a time for sentence alignment and from 100k to 1.7M adding 100k at a time for word alignment). For all the experiments we use sentences from the Europarl English–Spanish corpus,2 which contains over 1.7M sentence pairs. The aligners are executed using the default values"
2012.eamt-1.10,2011.eamt-1.11,1,0.815927,"ional resources consumed (e.g. execution time, use of memory). However, this assessment is critical if the aligners are to be exploited in an industrial scenario. This work is part of a wider project, whose objective is to automate the stages involved in the acquisition, production, updating and maintenance of language resources required by MT systems. This is done by creating a platform, designed as a dedicated workflow manager, for the composition of a number of processes for the production of language resources, based on combinations of different web services. The present work builds upon (Toral et al., 2011), where we presented a web service architecture for sentence and word alignment. Here we extend this proposal by evaluating the efficiency of the aligners integrated, and subsequently imThis paper presents a novel efficiencybased evaluation of sentence and word aligners. This assessment is critical in order to make a reliable use in industrial scenarios. The evaluation shows that the resources required by aligners differ rather broadly. Subsequently, we establish limitation mechanisms on a set of aligners deployed as web services. These results, paired with the quality expected from the aligne"
2012.eamt-1.38,W05-0909,0,0.00939362,"s confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089 3,333 test 2,000 62,953 4,022 66,770 7,056 English–French EN–EL / env 53.49 34.15 3.00 5.09 4.28 Sys"
2012.eamt-1.38,2010.amta-papers.16,1,0.890714,"Missing"
2012.eamt-1.38,2008.tc-1.1,0,0.351485,"proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation mo"
2012.eamt-1.38,eck-etal-2004-language,0,0.0421934,"st of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as"
2012.eamt-1.38,W08-0334,0,0.0350418,"an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from different domains or even from the web, which is also the case in our work. 3 Resources and their acquisition In this"
2012.eamt-1.38,2009.mtsummit-commercial.5,0,0.0940661,"Missing"
2012.eamt-1.38,2005.eamt-1.19,0,0.0499412,"those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-d"
2012.eamt-1.38,P05-1058,0,0.0700041,"Missing"
2012.eamt-1.38,W07-0733,0,0.108639,"2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to"
2012.eamt-1.38,P07-2045,0,0.0439871,"cally contain vocabulary that is not likely to be found in texts from other domains (Banerjee et al., 2010). Other problems can be caused by divergence in style or genre where the difference is not only in lexis but also in grammar. In order to achieve optimal performance, an SMT system should be trained on data from the same domain, genre, and style as it is applied to. For many domains, though, in-domain data of a size sufficient to train a full system is hard to find. Recent experiments have shown that even small amounts of such data can be used to adapt a system to the domain of interest (Koehn et al., 2007). In this work, we present a strategy for automatic web-crawling and cleaning of domain-specific data. Further, our exhaustive experiments, carried out for the Natural Environment (env) and Labour Legislation (lab) domains and English– French (EN–FR) and English–Greek (EN–EL) language pairs (in both directions), demonstrate how the crawled data improves SMT quality. After an overview of related work, we discuss the possibility of adapting a general-domain SMT system by using various types of in-domain data. Then, we present our web-crawling procedure followed by a description of a series of ex"
2012.eamt-1.38,2005.mtsummit-papers.11,0,0.192466,"Missing"
2012.eamt-1.38,W02-1405,0,0.111907,"that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs (Désilets et al., 2008), or employ a predefined bilingual wordlist (Chen et al., 2004), or a naive aligner (Zhang et al., 2006) in order to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn"
2012.eamt-1.38,J05-4003,0,0.0461806,"er to estimate the content similarity of candidate parallel web pages. 2.2 Domain adaptation in SMT The first attempt towards domain adaptation in SMT was made by Langlais (2002) who integrated in-domain lexicons into the translation model. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and decl"
2012.eamt-1.38,W08-0320,0,0.0403881,". Hildebrand et al. (2005) applied the same approach on the translation model. Wu et al. (2005) proposed an align146 ment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) phrase-based SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models into Moses. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. In general, all approaches to domain adaptation of SMT depend on the availability of domain-specific data. If the data is available, it can be directly used to improve components of the MT system. Otherwise, it can be extracted from a pool of texts from dif"
2012.eamt-1.38,P03-1021,0,0.009744,"th length ratio within the interval h0.11,9.0i. The maximum 149 English–Greek category 1. perfect translation 2. minor corrections done 3. major corrections needed 4. misaligned sentence pair 5. wrong domain Table 6: Details of the in-domain parallel data sets obtained by web-crawling and manual correction: sentence pairs (sents), source (L1 ) and target (L2 ) tokens and vocabulary size (voc). length of aligned phrases is set to 7 and the reordering models are generated using parameters: distance, orientation-bidirectional-fe. The model parameters are optimized by Minimum Error Rate Training (Och, 2003, MERT) on development sets. For decoding, test sentences are tokenized, lowercased, and translated by the tuned system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4.2 Using out-of-domain test data A number of previous experiments (Wu et al., 2008; Banerjee et al., 2010, e.g.) showed significant degradation of translation quality if an SMT system was applied to out-of-domain data. In order to verify this observation we trained and tuned our system on general-domain data and compared its pe"
2012.eamt-1.38,P02-1040,0,0.0968791,"ected for corrections were used as training sets. See further statistics in Table 6. The correctors confirmed that the manual corrections were about 5–10 times faster than translating the sentences from scratch, so this can be viewed as low-cost method for acquiring in-domain test and development sets for SMT. 4 Domain adaptation experiments In this section, we present experiments that exploit all the acquired in-domain data in eight different evaluation scenarios involving two domains (env, lab) and two language pairs (EN–FR, EN–EL) in both directions. Our primary evaluation measure is BLEU (Papineni et al., 2002). For detailed analysis we also present NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) in Table 8. 4.1 pair dom set sents L1 tokens / voc L2 tokens / voc env train 10,240 300,760 10,963 362,899 14,209 dev 1,392 41,382 4,660 49,657 5,542 test 2,000 58,865 5,483 70,740 6,617 lab train 20,261 709,893 12,746 836,634 17,139 dev 1,411 52,156 4,478 61,191 5,535 test 2,000 71,688 5,277 84,397 6,630 env train 9,653 240,822 10,932 267,742 20,185 dev 1,000 27,865 3,586 30,510 5,467 test 2,000 58,073 4,893 63,551 8,229 lab train 7,064 233,145 7,136 244,396 14,456 dev 506 15,129 2,227 16,089"
2012.eamt-1.38,2011.eamt-1.40,1,0.250717,"Missing"
2012.eamt-1.38,J03-3002,0,0.202473,"ent as relevant to a domain or not also affects the acquisition of domain-specific resources, on the assumption that relevant pages are more likely to contain links to more pages in the same domain. Qi and Davison (2009) review features and algorithms used in web page classification. In most of the algorithms reviewed, on-page features (i.e. textual content and HTML tags) are used to construct a corresponding feature vector and then, several machine-learning approaches, such as SVMs, Decision Trees, and Neural Networks, are employed (Yu et al., 2004). Considering the Web as a parallel corpus, Resnik and Smith (2003) proposed the STRAND system, in which they used Altavista to search for multilingual websites and examined the similarity of the HTML structures of the fetched web pages in order to identify pairs of potentially parallel pages. Similarly, Esplà-Gomis and Forcada (2010) proposed Bitextor, a system that exploits shallow features (file size, text length, tag structure, and list of numbers in a web page) to mine parallel documents from multilingual web sites. Besides structure similarity, other systems either filter fetched web pages by keeping only those containing language markers in their URLs"
2012.eamt-1.38,C08-1125,0,0.0633981,"Missing"
2012.eamt-1.38,wu-wang-2004-improving-domain,0,\N,Missing
2012.eamt-1.38,W04-3250,0,\N,Missing
2012.eamt-1.38,W10-1720,1,\N,Missing
2012.eamt-1.38,2006.eamt-1.31,0,\N,Missing
2012.eamt-1.38,2010.amta-papers.27,1,\N,Missing
2020.acl-main.613,N13-1073,0,0.0158717,", 2002) as this was shown to be more effective for tuning SMT parameters for translating search queries (Pecina et al., 2014). The system is denoted as QT-SMT-form. 6851 For the DT experiments, we train two SMT systems: DT-SMT-form, which is a replication of the SMT system that translates standard sentences by Duˇsek et al. (2014), and our own system DT-SMTpre-lem that translates English sentences into lemmatized sentences in the target language. This is done by lemmatizing the monolingual data and the target side of the parallel data prior to training. In both the systems, we use fast align (Dyer et al., 2013) to train word alignment model on the lowercased word forms between English and the target language, then we replace the word forms in the target language with word lemmas. Moses (with its default settings) is used to train a phrasetable model using the tokenized and lowercased English word forms, and the tokenized and lemmatized data in the target language plus a 5-gram language model. Minimum Error Rate Training (MERT, Och, 2003) is used to tune the model parameter weights using the development data sets. We also experiment with another system (DT-SMTpost-lem), which produces lemmatized outp"
2020.acl-main.613,D18-1045,0,0.0150725,"rms of vocabulary size). 5.2 Neural Machine Translation Neural Machine Translation (NMT) has become the state-of-the-art approach in MT and recently achieved superior results and lead to a significant improvement over the SMT systems (Jean et al., 2015). We implement two types of NMT systems: one for query translation (denoted as QT-NMTform) and one for document translation (denoted as DT-NMT-form). Both produce standard (nonlemmatized) output. The systems are based on the Marian (JunczysDowmunt et al., 2018) implementation of the Transformer (Vaswani et al., 2017) model with backtranslation (Edunov et al., 2018). SMT has an advantage over NMT in employing monolingual data in its language model. This gap can be bridged Initial Training NMT Model EN -> Target Parallel Corpus (Target Side) NMT Model Source -> EN Parallel Corpus (Authentic) CLEF eHealth Collection (EN) Parallel Corpus (English Side) Iterative Training NMT Model Source -> EN NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model Source -> EN Figure 1: A schema of the iterative back-translation mechanism for NMT training. by back-translation, a technique that exploits another MT mode"
2020.acl-main.613,fujii-ishikawa-2000-applying,0,0.520701,"rmance of DT, QT, and a hybrid system combining both. They found that the system translating English queries into German (the document language) outperformed the system translating the documents from German into English (the query language). They hypothesized that documents, which are typically longer than queries, provide more contextual and linguistic information that helps reduce translation ambiguity and thus improves translation quality. McCarley (1999) presented a hybrid DT/QT system, which averaged the retrieved document scores from DT and QT systems and thus outperformed both of them. Fujii and Ishikawa (2000) employed a two-step method where QT was first used to retrieve a limited number of documents that were translated into the query language and reranked by their DT retrieval scores. Pirkola (1998) presented a new method for CLIR, which was referred to as structured queries. The idea was that a document containing one possible translation candidate of a query term is more relevant than a document that contains multiple translations of that term. This probabilistic structured queries approach was also applied to CrossLanguage Speech Retrieval (Nair et al., 2020). Darwish and Oard (2003) also exp"
2020.acl-main.613,W18-2703,0,0.0160805,"k-translation approach in this work iteratively. 5.2.1 Task-Oriented NMT Training The NMT systems are trained using the same training data as the SMT systems. However, in NMT, all data sets (monolingual and parallel) are encoded into Byte-Pair Encoding (BPE), which helps reduce the out-of-vocabulary problem in NMT by encoding rare words as sequences of subword units (Sennrich et al., 2016b). We train the Transformer model using the same parameters as reported by Vaswani et al. (2017). Figure 1 shows the architecture of the proposed iterative back-translation NMT model, inspired by the work of Hoang et al. (2018): for each language pair, we first train initial models for both directions, English to target, and source to English. We use the authentic (non-synthetic) parallel data that is presented in Section 3.1 for training the initial models. During training the Transformer models, multiple epochs (iterations through the entire training data) are needed. It is known that too many training epochs can cause over-fitting of the model, and a few iterations might cause under-fitting (Popel and Bojar, 2018). To avoid this, the early-stopping of the training is employed to terminate the process when the int"
2020.acl-main.613,P15-1001,0,0.0298237,"the output of DT-SMT-form. This is to allow better comparison of the DT and QT approaches. Translating documents into a morphologically richer language enlarge the vocabulary (term diversity) and thus make retrieval more difficult. The three systems produce morphologically reduced translations of documents and thus make them comparable to the English ones (in terms of vocabulary size). 5.2 Neural Machine Translation Neural Machine Translation (NMT) has become the state-of-the-art approach in MT and recently achieved superior results and lead to a significant improvement over the SMT systems (Jean et al., 2015). We implement two types of NMT systems: one for query translation (denoted as QT-NMTform) and one for document translation (denoted as DT-NMT-form). Both produce standard (nonlemmatized) output. The systems are based on the Marian (JunczysDowmunt et al., 2018) implementation of the Transformer (Vaswani et al., 2017) model with backtranslation (Edunov et al., 2018). SMT has an advantage over NMT in employing monolingual data in its language model. This gap can be bridged Initial Training NMT Model EN -> Target Parallel Corpus (Target Side) NMT Model Source -> EN Parallel Corpus (Authentic) CLE"
2020.acl-main.613,P07-2045,0,0.0118292,"For retrieval, we use Terrier’s implementation of the language model with Bayesian smoothing and Dirichlet prior (Smucker and Allan, 2005) with the default value of the smoothing parameter. 5 Machine Translation Systems In this section, we provide details on training the SMT and NMT systems used in the CLIR experiments. The SMT systems fully replicate the work by Duˇsek et al. (2014); we only provide the most important information. The NMT systems are described in full detail. 5.1 Statistical Machine Translation The SMT systems are based on the phrase-based SMT paradigm implemented in Moses (Koehn et al., 2007). The system for the QT experiments was developed within the Khresmoi project (Duˇsek et al., 2014). The system was tuned to translate medical search queries (using the Khresmoi Query development set) and optimized towards PER (Position-independent word Error Rate, Tillmann et al., 1997) instead of the traditionally preferred BLEU (Papineni et al., 2002) as this was shown to be more effective for tuning SMT parameters for translating search queries (Pecina et al., 2014). The system is denoted as QT-SMT-form. 6851 For the DT experiments, we train two SMT systems: DT-SMT-form, which is a replica"
2020.acl-main.613,P99-1027,0,0.708277,"he query language (DT). Not many studies and experiments have been conducted in order to compare these two approaches. Oard (1998) investigated the performance of DT, QT, and a hybrid system combining both. They found that the system translating English queries into German (the document language) outperformed the system translating the documents from German into English (the query language). They hypothesized that documents, which are typically longer than queries, provide more contextual and linguistic information that helps reduce translation ambiguity and thus improves translation quality. McCarley (1999) presented a hybrid DT/QT system, which averaged the retrieved document scores from DT and QT systems and thus outperformed both of them. Fujii and Ishikawa (2000) employed a two-step method where QT was first used to retrieve a limited number of documents that were translated into the query language and reranked by their DT retrieval scores. Pirkola (1998) presented a new method for CLIR, which was referred to as structured queries. The idea was that a document containing one possible translation candidate of a query term is more relevant than a document that contains multiple translations of"
2020.acl-main.613,E12-1012,0,0.0218692,"T retrieval scores. Pirkola (1998) presented a new method for CLIR, which was referred to as structured queries. The idea was that a document containing one possible translation candidate of a query term is more relevant than a document that contains multiple translations of that term. This probabilistic structured queries approach was also applied to CrossLanguage Speech Retrieval (Nair et al., 2020). Darwish and Oard (2003) also exploited alternative translations of query terms. Their experiments showed that combining multiple translations outperformed the selection of one best translation. Nikoulina et al. (2012) investigated reranking SMT translation hypotheses towards better CLIR performance and showed that SMT systems are usually trained to give the best results in terms of translation accuracy, adequacy, and fluency. However, an improvement will be achieved when they are optimized towards retrieval quality. We followed this approach in our previous work and introduced a richer set of features and adopted the hypothesis reranker for multiple languages in the medical domain (Saleh and Pecina, 2016b,a). Several recent papers employed methods based on Deep Learning. Litschko et al. (2018) presented an"
2020.acl-main.613,oard-1998-comparative,0,0.906027,"users to find information in documents that are written in the language that they use to write their queries. This ignores a vast amount of information that is represented in other languages. Cross-Lingual Information Retrieval (CLIR) breaks this language barrier by allowing users to look up information that is represented in documents written in languages different from the language of the query. We reinvestigate the effectiveness of two principal approaches to CLIR: document translation (DT) and query translation (QT). The existing comparison studies of the two approaches are outdated (e.g. Oard, 1998) and do not reflect the current advances in Machine Translation (MT). Even in very recent works, the authors have blindly assumed that DT is superior to QT (Khiroun et al., 2018), giving the argument that in DT, the text is translated in a larger context compared to the translation of short isolated queries in QT. The larger context should help in translation disambiguation and better lexical selection during translation, which should subsequently lead to better retrieval results. This hypothesis needs to be revised, taking into consideration the significant improvement of machine translation"
2020.acl-main.613,P03-1021,0,0.0626478,"nguage. This is done by lemmatizing the monolingual data and the target side of the parallel data prior to training. In both the systems, we use fast align (Dyer et al., 2013) to train word alignment model on the lowercased word forms between English and the target language, then we replace the word forms in the target language with word lemmas. Moses (with its default settings) is used to train a phrasetable model using the tokenized and lowercased English word forms, and the tokenized and lemmatized data in the target language plus a 5-gram language model. Minimum Error Rate Training (MERT, Och, 2003) is used to tune the model parameter weights using the development data sets. We also experiment with another system (DT-SMTpost-lem), which produces lemmatized output but obtained as post-lemmatization of the output of the DT-SMT-form system, and a system (DT-SMT-poststem) which produces stemmed output obtained by the Snowball stemmer applied again to the output of DT-SMT-form. This is to allow better comparison of the DT and QT approaches. Translating documents into a morphologically richer language enlarge the vocabulary (term diversity) and thus make retrieval more difficult. The three sys"
2020.acl-main.613,P02-1040,0,0.107623,"e the work by Duˇsek et al. (2014); we only provide the most important information. The NMT systems are described in full detail. 5.1 Statistical Machine Translation The SMT systems are based on the phrase-based SMT paradigm implemented in Moses (Koehn et al., 2007). The system for the QT experiments was developed within the Khresmoi project (Duˇsek et al., 2014). The system was tuned to translate medical search queries (using the Khresmoi Query development set) and optimized towards PER (Position-independent word Error Rate, Tillmann et al., 1997) instead of the traditionally preferred BLEU (Papineni et al., 2002) as this was shown to be more effective for tuning SMT parameters for translating search queries (Pecina et al., 2014). The system is denoted as QT-SMT-form. 6851 For the DT experiments, we train two SMT systems: DT-SMT-form, which is a replication of the SMT system that translates standard sentences by Duˇsek et al. (2014), and our own system DT-SMTpre-lem that translates English sentences into lemmatized sentences in the target language. This is done by lemmatizing the monolingual data and the target side of the parallel data prior to training. In both the systems, we use fast align (Dyer et"
2020.acl-main.613,2011.mtsummit-plenaries.5,0,0.0592253,"ources Parallel data is essential for training both SMT and NMT systems. We exploited the UFAL Medical Corpus1 which was assembled during the course of several EU projects aiming at more reliable machine translation of medical texts and used for the purposes of WMT Biomedical Translation Task (Bojar et al., 2014). It mainly includes the EMEA corpus by Tiedemann (2009), UMLS metathesaurus (Humphreys et al., 1998), titles from Wikipedia articles in the medical categories mapped to other languages using Wikipedia Interlingual links, medical domain patent applications (W¨aschle and Riezler, 2012; Pouliquen and Mazenc, 2011), and various web-crawled data. 1 http://ufal.mff.cuni.cz/ufal_medical_ corpus 6850 Monolingual data is used to build a language model during the development of SMT systems. The language model helps select a candidate translation that is as coherent and fluent as possible in the target language (which is certainly important for document translation, but less important for query translation). Our procedure of data selection (both parallel and monolingual data) follows the work of Pecina et al. (2014), where two language models are trained on in-domain and generaldomain data respectively, then e"
2020.acl-main.613,P16-1009,0,0.0241667,"arget Parallel Corpus (Target Side) NMT Model Source -> EN Parallel Corpus (Authentic) CLEF eHealth Collection (EN) Parallel Corpus (English Side) Iterative Training NMT Model Source -> EN NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model Source -> EN Figure 1: A schema of the iterative back-translation mechanism for NMT training. by back-translation, a technique that exploits another MT model to translate monolingual data from the target language into the source language and adds this “synthetic” data to the original parallel data(Sennrich et al., 2016a). This approach also helps for domain adaption of NMT when the monolingual data is taken from a specific domain. We follow the back-translation approach in this work iteratively. 5.2.1 Task-Oriented NMT Training The NMT systems are trained using the same training data as the SMT systems. However, in NMT, all data sets (monolingual and parallel) are encoded into Byte-Pair Encoding (BPE), which helps reduce the out-of-vocabulary problem in NMT by encoding rare words as sequences of subword units (Sennrich et al., 2016b). We train the Transformer model using the same parameters as reported by V"
2020.acl-main.613,P16-1162,0,0.0239563,"arget Parallel Corpus (Target Side) NMT Model Source -> EN Parallel Corpus (Authentic) CLEF eHealth Collection (EN) Parallel Corpus (English Side) Iterative Training NMT Model Source -> EN NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model EN -> Target Parallel Corpus (Synthetic) NMT Model Source -> EN Figure 1: A schema of the iterative back-translation mechanism for NMT training. by back-translation, a technique that exploits another MT model to translate monolingual data from the target language into the source language and adds this “synthetic” data to the original parallel data(Sennrich et al., 2016a). This approach also helps for domain adaption of NMT when the monolingual data is taken from a specific domain. We follow the back-translation approach in this work iteratively. 5.2.1 Task-Oriented NMT Training The NMT systems are trained using the same training data as the SMT systems. However, in NMT, all data sets (monolingual and parallel) are encoded into Byte-Pair Encoding (BPE), which helps reduce the out-of-vocabulary problem in NMT by encoding rare words as sequences of subword units (Sennrich et al., 2016b). We train the Transformer model using the same parameters as reported by V"
2020.acl-main.613,K17-3009,0,0.0465674,"Missing"
avramidis-etal-2012-richly,steinberger-etal-2006-jrc,0,\N,Missing
avramidis-etal-2012-richly,vandeghinste-etal-2008-evaluation,1,\N,Missing
avramidis-etal-2012-richly,W09-0424,0,\N,Missing
avramidis-etal-2012-richly,P07-2045,0,\N,Missing
avramidis-etal-2012-richly,C04-1072,0,\N,Missing
avramidis-etal-2012-richly,W08-0309,0,\N,Missing
avramidis-etal-2012-richly,2005.mtsummit-papers.11,0,\N,Missing
avramidis-etal-2012-richly,W10-1720,1,\N,Missing
avramidis-etal-2012-richly,P03-1021,0,\N,Missing
C12-1135,2010.amta-papers.16,1,0.91367,"Missing"
C12-1135,E06-1032,0,0.101057,"Missing"
C12-1135,P11-2071,0,0.0229814,"Missing"
C12-1135,eck-etal-2004-language,0,0.0341917,"), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain m"
C12-1135,W08-0334,0,0.0173916,"can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their re"
C12-1135,D10-1044,0,0.0271916,"ain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train d"
C12-1135,2010.amta-papers.27,1,0.903095,"Missing"
C12-1135,2005.eamt-1.19,0,0.0282124,"guage models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek"
C12-1135,D11-1125,0,0.0279752,"e model significantly prefers hypotheses with altered word order (which is consistent with the two preceding observations). 4. Language model weights (h8 ) do not change substantially, its importance remains similar on general-domain and specific-domain data. These findings are highly consistent across domains and language pairs. The weight vectors of the systems tuned on specific-domain data are quite similar but differ substantially from the parameters obtained by tuning on general-domain. This observation can be quantified by measuring cosine similarity (see Figure 2, right) as proposed by Hopkins and May (2011). Lower scores, as in the first rows/columns of each table, indicate low similarity of the vectors – specific-domain tuned weights differ a lot from the general-domain tuned ones; and vice versa – specific-domain tuned parameters are quite similar when compared to each other. 5.5 Analysis of phrase-length distribution From the analysis presented above, we conclude that a PB-SMT system tuned on data from the same domain as the training data strongly prefers to construct translations consisting of long phrases. Such phrases are usually of good translation quality (local mistakes of word alignmen"
C12-1135,W04-3250,0,0.190801,"Missing"
C12-1135,2005.mtsummit-papers.11,0,0.0315462,"et al. (2011) exploit automatically web-crawled in-domain resources for parameter optimization and improving language models. Pecina et al. (2012) extend the work by using the web-crawled resources to also improve translation models. 4 Experimental setup Our experimental setup follows and extends the one used in Pecina et al. (2011). In addition to the two evaluation domains (env, lab) used in that work, and in order to corroborate their earlier findings, we also carry out experiments on medical domain data (med). 4.1 Data Our general-domain system is trained on the Europarl parallel corpus (Koehn, 2005, v5) extracted from the proceedings of the European Parliament and for the purposes of this work considered to contain general-domain texts (it covers a very broad range of topics and it is to a considerable extent spoken language). The general-domain development and test data used for parameter optimization and testing, respectively, are adopted from the WPT 20051 machine translation shared task. These sets were extracted from the same source as Europarl and contain 2,000 sentence pairs each. The specific-domain development and test data for the env and lab domains were acquired by domain-fo"
C12-1135,P07-2045,0,0.0376072,"tem to a specific domain. Finally, given the fact that a general-domain system can only use limited length translation units when translating specific-domain data, we explore limited length training and decoding. After a brief overview of the log-linear model including its parameter optimization and an overview of the state-of-the-art in domain adaptation for SMT, we describe our experiments, present the results, the analysis, explore the resulting research questions with additional experiments, and conclude. 2 Phrase-Based Statistical Machine Translation In PB-SMT, implemented e.g. in Moses (Koehn et al., 2007), an input sentence is segmented into sequences of consecutive words, called phrases. Each phrase is then translated into a target language phrase, which may be reordered with other translated phrases to produce the output. 2210 Formally, the model is based on the noisy channel model. The translation e of an input sentence f is searched for by maximizing the translation probability p(e|f) formulated as a log-linear combination of a set of feature functions hi and their weights λi : Qn p(e|f) = i hi (e, f)λi Typically, the components include features of the following models: phrase translation"
C12-1135,N03-1017,0,0.0327235,"en longer phrases improve translation quality is for the systems trained, tuned and tested on the same (general) domain. In all other cases, the results for phrases up to three words long are as good as for longer phrases. If the domain of the test data does not match the domain of training and tuning data, the maximum phrase length set to three is enough in all scenarios. Longer phrases lead to degradation of translation quality and increase time for training and decoding, as well as memory requirements for building and storing the translation models. A similar result was reported already by Koehn et al. (2003). They observed that limiting the maximum length of a phrase to only three words achieved top performance. However, current state-of-the-art SMT systems usually benefit from longer phrases than three (see e.g. the top curve in Figure 5 which refers to a general-domain system applied to a general-domain test set), and our result applies only to scenarios where the training and test domains do not match; in that case setting the maximum phrase length to three is sufficient. 6 Conclusions In this work, we have analysed domain adaptation of PB-SMT by tuning parameters of the underlying log-linear"
C12-1135,W07-0733,0,0.0683277,"se, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting simila"
C12-1135,W02-1405,0,0.0358254,"lore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. E"
C12-1135,J05-4003,0,0.0384329,"odel combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the translation model. Foster et al. (2010) weigh phrase pairs from out-of-domain corpora according to their relevance to the target domain. Munteanu and Marcu (2005) extract in-domain sentence pairs from comparable corpora. Daumé III and Jagarlamudi (2011) attempt to reduce out-of-vocabulary terms when targeting a specific domain by mining their translations from comparable corpora. Bertoldi et al. (2009) rely on large in-domain monolingual data to create synthetic parallel corpora for training. 2211 languages (L1-L2) dom English–French gen env lab med English–Greek gen env lab med set train dev test dev test dev test dev test train dev test dev test dev test dev test sentences 1,725,096 2,000 2,000 1,392 2,000 1,411 2,000 1,064 2,000 964,242 2,000 2,000"
C12-1135,W08-0320,0,0.0424553,"Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identified: (i) combination of in-domain and out-of-domain resources for training, (ii) training data selection, and (iii) acquisition of specific-domain data. Below we briefly review a selection of relevant work that falls into these topics. The first attempt to perform domain adaptation was carried out by Langlais (2002), who integrated in-domain lexicons in the translation model. Koehn and Schroeder (2007) integrate in-domain and out-of-domain language models as log-linear features in Moses. Nakov (2008) combines in-domain translation and reordering models with out-of-domain models. Finch and Sumita (2008) use a probabilistic mixture model combining two models for questions and declarative sentences with a general model. Training data selection is another approach to domain-adaptation. The assumption is that a general-domain corpus, if sufficiently broad, includes sentences that resemble the target domain. Eck et al. (2004) present a technique for adapting the language model by selecting similar sentences from available training data. Hildebrand et al. (2005) extended this approach to the tra"
C12-1135,P03-1021,0,0.0119307,"odel, which ensures that the translations are fluent, reordering (distortion) model, which allows to reorder phrases in the input sentences (e.g. distance-based and lexicalized reordering) and word penalty, which prevents the translations from being too long or too short. These models are trained on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm can"
C12-1135,P02-1040,0,0.0883898,"ned on either parallel or monolingual training data. The weights of the log-linear combination influence overall translation quality; however, the optimal setting depends on the translation direction and data. A common solution to optimise weights is to use Minimum Error Rate Training (MERT), proposed by Och (2003), which automatically searches for the values that minimize a given error measure (or maximize a given translation quality measure) on a development set of parallel sentences. Theoretically, any automatic measure can be used for this purpose; however, the most commonly used is BLEU (Papineni et al., 2002). The search algorithm is a type of coordinate ascent: considering n-best translation hypotheses for each input sentence, it updates the feature weight most likely promising to improve the objective and iterates until convergence. The error surface is highly non-convex and as the algorithm cannot explore the whole parameter space, it may converge to a local maximum; in practise, it often produces good results (Bertoldi et al., 2009). 3 Domain adaptation in Statistical Machine Translation Domain-adaptation is a very active research topic within the area of SMT. Three main topics can be identifi"
C12-1135,2012.eamt-1.38,1,0.722345,"Missing"
C12-1135,2011.eamt-1.40,1,0.781939,"alizace parametr˚u. Proceedings of COLING 2012: Technical Papers, pages 2209–2224, COLING 2012, Mumbai, December 2012. 2209 1 Introduction Statistical Machine Translation (SMT) is an instance of a machine learning application and, in general, will work best if the data for training and testing are drawn from the same distribution (i.e. domain, genre, and style). In practice, however, it is often difficult to obtain sufficient amounts of in-domain data (in particular parallel data required for translation and distortion models) to train a well performing system for a specific domain. Recently, Pecina et al. (2011) showed that just using in-domain development data for parameter tuning improves output quality of a Phrase-Based SMT (PB-SMT) system trained on general-domain data but applied to a specific domain. Although further additional improvements can be realized by using in-domain parallel and/or monolingual training data, parameter tuning on in-domain data requires only a relatively small set of parallel sentences, which is often easier to obtain. They report on a series of experiments carried out on the domains of Natural Environment (env) and Labour Legislation (lab) and two language pairs: Englis"
C12-1135,2011.mtsummit-papers.58,0,0.0536528,"Missing"
C12-1135,C08-1125,0,0.0344385,"Missing"
C12-2011,W11-4417,1,0.911901,"Missing"
C12-2011,P00-1037,0,0.145659,"Missing"
C12-2011,J92-4003,0,0.0590624,"now use language models trained on different corpora to finally choose the single best correction. We compare the results against the Microsoft Spell Checker in Office 2010, Ayaspell used in OpenOffice, and Google Docs. 4.1 Correction Procedure For automatic spelling correction (or first order ranking) we use n-gram language models. Language modelling assumes that the production of a human language text is characterized by a set of conditional probabilities, , where is the history and is the prediction, so that the probability of a sequence of k words P(w1, …, wk) is formulated as a product (Brown et al., 1992): We use the SRILM toolkit2 (Stolcke et al., 2011) to train 2-, 3- and 4-gram language models on our data sets. As we have two types of candidates, normal words and split words, we use two SRILM tools: disambig and ngram. We use the disambig tool to choose among the normal candidates. Handling split words is done as a posterior step where we use the ngram tool to score the chosen candidate from the first round and the various split-word options. Then the candidate with the least perplexity score is selected. The perplexity of a language model is the reciprocal of the geometric average of the p"
C12-2011,P05-1071,0,0.165899,"Missing"
C12-2011,E09-2008,0,0.0607402,"r, the next step is to generate possible and plausible corrections for that error. For a spelling error and a dictionary , the purpose of the error model is to generate the correction , or list of corrections where ∊ , and are most likely to have been erroneously typed as . In order to do this, the error model generates a list of candidate corrections that bear the highest similarity to the spelling error . We use a finite-state transducer to propose candidate corrections within edit distance 1 and 2 measured by Levenshtein Distance (Levenshtein, 1966) from the misspelled word (Oflazer, 1996; Hulden, 2009b; Norvig, 2009; Mitton, 1996). The transducer works basically as a character-based generator that replaces each character with all possible characters in the alphabet as well as deleting, inserting, and transposing neighbouring characters. There is also the problem of merged (or run-on) words that need to be split, such as >‘ أوأيw>y’ “or any”. Candidate generation using edit distance is a brute-force process that ends up with a huge list of candidates. Given that there are 35 alphabetic letters in Arabic, for a word of length , there will be deletions, − 1 transpositions, 35 replaces, 35 +"
C12-2011,C90-2036,0,0.361216,"Missing"
C12-2011,W06-1648,0,0.17186,"Missing"
C12-2011,P08-2030,0,0.0557039,"nd a corpus of news articles crawled from the Al-Jazeera web site. The Gigaword corpus is a collection of news articles from nine news sources: Agence France-Presse, Xinhua News Agency, An Nahar, Al-Hayat, Al-Quds Al-Arabi, Al-Ahram, Assabah, Asharq Al-Awsat and Ummah Press. Before we start using our available corpora in training the language model, we analyse the data to measure the amount of noise in each subset of the data. In order to do this, we create a list of the most common spelling errors. This list of spelling errors is created by analysing the data using MADA (Habash et al., 2005; Roth et al., 2008) and checking instances where words have been normalized. In this case the original word is considered to be a suboptimal variation of the spelling of the diacritized form. We collect these suboptimal forms and sort them by frequency. 2 http://www.speech.sri.com/projects/srilm/ 108 Then we take the top 100 misspelt forms and see how frequent they are in the different subsets of data in relation to the word count in each data set. The analysis shows that the data has a varying degree of cleanness, ranging from the very clean to the very noisy. Data in the Agence France-Presse (AFP) is the noisi"
C12-2011,J96-1003,0,\N,Missing
C12-2011,shaalan-etal-2012-arabic,1,\N,Missing
C12-2011,I08-2131,0,\N,Missing
cinkova-etal-2006-semi,pearce-2002-comparative,0,\N,Missing
cinkova-etal-2006-semi,W03-1810,0,\N,Missing
cinkova-etal-2006-semi,P05-2003,1,\N,Missing
cinkova-etal-2006-semi,W03-1809,0,\N,Missing
cinkova-etal-2006-semi,P98-2210,0,\N,Missing
cinkova-etal-2006-semi,C98-2205,0,\N,Missing
federmann-etal-2012-ml4hmt,vandeghinste-etal-2008-evaluation,1,\N,Missing
federmann-etal-2012-ml4hmt,federmann-2010-appraise,1,\N,Missing
federmann-etal-2012-ml4hmt,E06-1005,0,\N,Missing
federmann-etal-2012-ml4hmt,W09-0424,0,\N,Missing
federmann-etal-2012-ml4hmt,P02-1040,0,\N,Missing
federmann-etal-2012-ml4hmt,W02-1019,0,\N,Missing
federmann-etal-2012-ml4hmt,W05-0909,0,\N,Missing
federmann-etal-2012-ml4hmt,W08-0309,0,\N,Missing
federmann-etal-2012-ml4hmt,W10-1720,1,\N,Missing
federmann-etal-2012-ml4hmt,W11-2101,0,\N,Missing
I13-1177,A00-1031,0,0.153464,"seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The target language tagger, T agger(t), is then the last model, Tm . T agger(s) is trained from manually annotated data Data(s) which is mainly derived from the CoNLL 2006 and CoNLL 2007 Shared Tasks. Using the matching provided by Petrov et al., we map the individual tagsets to the Universal Tagset. We train a supervised POS tagger T agger(s) on the annotated data using the TNT tagger (Brants, 2000). Table 3 shows the source and size of annotated data, and the 5 fold cross-validation accuracy of T agger(s), for each language. We evaluate each T agger(t) using Data(t); results are shown in Table 4. The average tagger per3 NOUN, VERB, ADJ, ADV, PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), “.” (punctuation), and X (all other categories, e.g., foreign words, abbreviations). 1245 Source language en da nl pt sv el it es de Baseline en 55.73 75.70 72.40 66.56 47.67 74.50 68.76 72.24 30.28 da 76.17 76"
I13-1177,P11-1061,0,0.31602,"ta is not available for our target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/"
I13-1177,P13-2112,1,0.741841,"r target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/filtering on the (automa"
I13-1177,W11-3603,0,0.403144,"Missing"
I13-1177,D08-1109,0,0.072329,"Missing"
I13-1177,steinberger-etal-2006-jrc,0,0.0711628,"Missing"
I13-1177,W04-3229,0,0.801596,"Missing"
I13-1177,2005.mtsummit-papers.11,0,0.0294774,"the source or target language role. From Table 4, it seems that taggers perform better if the source and target language are in the same language family. For example, the top four source languages for Danish are Dutch, English, Swedish, and German, and the top two source languages for Portuguese are Italian and Spanish. This confirms the intuition in adding language relatedness features in section 4. Duong et al. (2013) used English as the source language to build taggers for the same eight other languages. The only difference between these two experiments is that Duong et al. used Europarl (Koehn, 2005) data instead of JRC-Acquis. Table 2 also compares the size of parallel data with JRC-Acquis 76.2 73.0 79.6 73.8 50.4 72.2 75.4 74.0 71.8 Europarl 85.6 84.0 86.3 81.0 80.0 81.4 83.3 85.4 83.4 Table 5: Accuracy on JRC-Acquis and Europarl using English as the source language. English as the source language for JRC-Acquis and Europarl. Given that Europarl is larger, higher performance is expected. Table 5 compares the tagger accuracy for each target language using English as the source language, for the two datasets. As expected, the accuracies are higher for Europarl. However, there is a strong"
I13-1177,2009.mtsummit-papers.7,0,0.0300127,"Missing"
I13-1177,petrov-etal-2012-universal,0,0.0786767,"he entropy for each lexical entry is calculated as X H(s) = − p(t|s) × log2 p(t|s) t∈T where T is the set of possible translations of word s, and t is a translation. For each language, we pick a fixed amount of text (1 million words) and calculate the average entropy for all words. 5 Build taggers In this section we construct 72 taggers, using parallel data for 72 language pairs, and then evaluate the performance of each pair. We use an open source unsupervised cross-lingual POS tagger (UMPOS) from Duong et al. (2013), a stateof-the-art system. UMPOS employs the consensus 12 Universal Tagset (Petrov et al., 2012),3 to avoid the problem of transliterating between different tagsets for different languages, and to enable comparison across languages. The input for UMPOS is a tagger for the source language, T agger(s), along with parallel data (s–t). The source language s is tagged using T agger(s), and then the tagged labels are projected to the target language t. Sentences are then ranked, and a seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The"
P05-2003,J93-1003,0,0.295586,"Missing"
P05-2003,P01-1025,0,0.0667056,"Missing"
P05-2003,pearce-2002-comparative,0,0.392973,"s, stock phrases, technological expressions, and proper names. Collocations are of high importance for many applications in the field of NLP. The most desirable ones are machine translation, word sense disambiguation, language generation, and information retrieval. The recent availability of large amounts of textual data has attracted interest in automatic collocation extraction from text. In the last thirty years a number of different methods employing various association measures have been proposed. Overview of the most widely used techniques is given e.g. in (Manning and Schütze, 1999) or (Pearce, 2002). Several researches also attempted to compare existing methods and suggested different evaluation schemes, e.g Kita (1994) or Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in (Evert, 2004). In this paper we present a compendium of 84 methods for automatic collocation extraction. They came from different research areas and some of them have not been used for this purpose yet. A brief overview of these methods is followed by their comparative evaluation against manually annotated data by the means of precision and recall measures. In the end we pr"
P05-2003,P97-1061,0,0.0546773,"is taken directly from the definition that a collocation is a syntactic and semantic unit. For each bigram occurring in the corpus, information of its empirical context (frequencies of open-class words occurring within a specified context window) and left and right immediate contexts (frequencies of words immediately preceding or following the bigram) is extracted (Table 1b). By determining the entropy of the immediate contexts of a word sequence, the association measures rank collocations according to the assumption that they occur as units in a (informationtheoretically) noisy environment (Shimohata et al., 1997) (58–62). By comparing empirical contexts of a word sequence and its components, the association measures rank collocations according to the as14 y ) f (x∗) b) Cw a) a = f (xy) b = f (x¯ Cxy c = f (¯ xy) d = f (¯ xy¯) f (¯ x∗) l Cxy r f (∗y) f (∗¯ y) N Cxy empirical context of w empirical context of xy left immediate context of xy right immediate context of xy Table 1: a) A contingency table with observed frequencies and marginal frequencies for a bigram xy; w ¯ stands for any word except w; ∗ stands for any word; N is a total number of bigrams. The table cells are sometimes referred as fij ."
P05-2003,thanopoulos-etal-2002-comparative,0,0.0268409,"Missing"
P06-1119,2006.eamt-1.18,1,0.694848,"Missing"
P06-1119,J93-2003,0,0.00721652,"Missing"
P06-1119,cmejrek-etal-2004-prague,1,0.901336,"Missing"
P06-1119,W05-1010,0,0.0143561,"and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation"
P06-1119,C96-1006,0,0.0262257,"Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward pro"
P06-1119,J03-1002,0,0.00292812,"us translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun & Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och & Ney, 2003; Wantanabe & Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine tra"
P06-1119,P02-1040,0,0.0751386,"rily phrase by phrase/word by word translation. Our evaluation scores below will partially reflect the simplicity of our system. Our system is simple by design. Any improvement or degradation to the input of our system has direct influence on the output. Thus, measures of translation accuracy for our system can be directly interpreted as quality measures for the lexical resources used and the process by which they were developed. 4 Evaluation We performed two different types of evaluation to validate our process. First, we compared our system output to human reference translations using Bleu (Papineni, et al., 2002), a widelyaccepted objective metric for evaluation of machine translations. Second, we showed corrected and uncorrected machine translations to Czech speakers and collected subjective judgments of fluency and accuracy. For evaluation purposes, we selected 418 keyword phrases to be used as target translations. These phrases were selected using a stratified sampling technique so that different levels of thesaurus value would be represented. There was no overlap between these keyword phrases and the 3000 prioritized keyword phrases used to build our lexicon. Prior to machine translation we obtain"
P06-1119,P99-1067,0,0.00918922,"have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domai"
P06-1119,P03-2025,0,0.0476662,"Missing"
P06-1119,C96-2098,0,0.0202396,"knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to sp"
P06-2084,P01-1025,0,0.0311329,"Missing"
P06-2084,W02-0909,0,0.0215002,"f recall (assuming uniform distribution) and mean average precision (MAP) as a mean of this measure computed for each data fold. Significance testing in this case can be realized by paired t-test or by more appropriate nonparametric paired Wilcoxon test. Due to the unreliable precision scores for low recall and their fast changes for high recall, estimation of AP should be limited only to some narrower recall interval, e.g. h0.1,0.9i 3.2 Precision-recall curves Since choosing a classification threshold depends primarily on the intended application and there is no principled way of finding it (Inkpen and Hirst, 2002), we can measure performance of association measures by precision–recall scores within the entire interval of possible threshold values. In this manner, individual association measures can be thoroughly compared by their two-dimensional precision-recall curves visualizing the quality of ranking without committing to a classification threshold. The closer the curve stays to the top and right, the better the ranking procedure is. 654 0.7 77 38 30 5 4 29 22 45 20 18 6 76 48 44 73 26 11 72 53 49 41 40 81 12 51 79 57 67 0.5 0.4 77 39 80 38 32 31 30 13 10 5 42 37 4 27 28 29 63 16 22 24 23 45 7 33 20"
P06-2084,W03-0301,0,0.0291361,"and comparison with individual measures. Finally, we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation. 1 Introduction Lexical association measures are mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus. They have a wide spectrum of applications in the field of natural language processing and computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. W"
P06-2084,W04-3243,0,0.0400611,"Missing"
P06-2084,pearce-2002-comparative,0,0.220146,"measures are mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus. They have a wide spectrum of applications in the field of natural language processing and computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. We also address the issue of the evaluation of association measures by precision-recall graphs and mean av2 Reference data The first step in our work was to create a reference data set. Krenn (2000) suggests th"
P06-2084,P05-2003,1,0.498308,"Missing"
P06-2084,P97-1061,0,0.00892009,"imation of joint and conditional bigram probabilities (Table 1, 1–3), mutual information and derived measures (4–9), statistical tests of independence (10–14), likelihood measures (15– 16), and various other heuristic association measures and coefficients (17–55) originating in different research fields. By determining the entropy of the immediate context of a word sequence (words immediately preceding or following the bigram), the association measures (56–60) rank collocations according to the assumption that they occur as (syntactic) units in a (information-theoretically) noisy environment (Shimohata et al., 1997). By comparing empirical contexts of a word sequence and of its components (open-class words occurring within 2.1 Manual annotation The list of collocation candidates was manually processed by three trained linguists in parallel and independently with the aim of identifying collocations as defined by Choueka. To simplify and clarify the work they were instructed to select those bigrams that can be assigned to these categories: ∗ idiomatic expressions ∗ ∗ ∗ ∗ Association measures - studená válka (cold war) - visí otazník (question mark is hanging ∼ open question) technical terms - pˇredseda vlá"
P13-2112,P11-2000,0,0.650374,"her language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of-speech (POS) taggers a"
P13-2112,2005.mtsummit-papers.11,0,0.351921,"it achieves comparable results. 3 We eliminate many-to-one alignments (Step 2). Keeping these would give more POS-tagged tokens for the target side, but also introduce noise. For example, suppose English and French were the source and target language, respectively. In this case alignments such as English laws (NNS) to French les (DT) lois (NNS) would be expected (Yarowsky and Ngai, 2001). However, in Step 3, where tags are projected from the source to target language, this would incorrectly tag French les as NN. We build a French tagger based on English– French data from the Europarl Corpus (Koehn, 2005). We also compare the accuracy and coverage of the tags obtained through direct projection using the French Melt POS tagger (Denis and Sagot, 2009). Table 1 conﬁrms that the one-to-one alignments indeed give higher accuracy but lower coverage than the many-to-one alignments. At this stage of the model we hypothesize that highconﬁdence tags are important, and hence eliminate the many-to-one alignments. In Step 4, in an effort to again obtain higher quality target language tags from direct projection, we eliminate all but the top n sentences based on their alignment scores, as provided by the al"
P13-2112,A00-1031,0,0.840096,"Missing"
P13-2112,N06-1020,0,0.0488021,"stimated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger. 4.2 language word wis is aligned with target language word wjt with probability p(wjt |wis ), Tis is the tag for wis using the tagger available for the source language, and Tjt is the tag for wjt using the tagger learned for the target language. If p(wjt |wis ) &gt; S, where S is a threshold which we heuristically set to 0.7, we replace Tjt by Tis . Self-training can suffer from over-ﬁtting, in which errors in the original model are repeated and ampliﬁed in the new model (McClosky et al., 2006). To avoid this, we remove the tag of any token that the model is uncertain of, i.e., if p(wjt |wis ) &lt; S and Tjt �= Tis then Tjt = Null. So, on the target side, aligned words have a tag from direct projection or no tag, and unaligned words have a tag assigned by our model. Step 4 estimates the emission and transition probabilities as in Algorithm 1. In Step 5, emission probabilities for lexical items in the previous model, but missing from the current model, are added to the current model. Later models therefore take advantage of information from earlier models, and have wider coverage. Self"
P13-2112,P11-1061,0,0.687686,"ne language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of"
P13-2112,D08-1109,0,0.0462258,"(Das and Petrov, 2011), but is substantially less-sophisticated (speciﬁcally not requiring convex optimization or a feature-based HMM). The complexity of our algorithm is O(nlogn) compared to O(n2 ) for that of Das and Petrov 637 (2011) where n is the size of training data.3 We made our code are available for download.4 In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy. Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language. Using our ﬁnal model with unsupervised HMM methods might improve the ﬁnal performance too, i.e. use our ﬁnal model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. In many cases, GS outperformed other methods, t"
P13-2112,N03-1033,0,0.239791,"Missing"
P13-2112,N01-1026,0,0.781294,"s for an HMM tagger for one language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tag"
P13-2112,Y09-1013,0,\N,Missing
P13-2112,petrov-etal-2012-universal,0,\N,Missing
P13-2112,feldman-etal-2006-cross,0,\N,Missing
P13-2112,W04-3229,0,\N,Missing
P13-2112,D08-1036,0,\N,Missing
R15-1040,D11-1033,0,0.0417068,"Missing"
R15-1040,N12-1033,0,0.031071,"ive language can be used as a feature for authorship analysis (Stamatatos, 2009). The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) presents the potential of authorship analysis in various applications including criminal law (e.g., identifying writers of harassing messages, verifying the authenticity of suicide notes), civil law (e.g., copyright disputes), and forensic linguistics. In the end, it includes the traditional applications to literary research (e.g., attributing anonymous or disputed literary works to known authors). Bergsma et al. (2012) consider the NLI task as a sub-task of the authorship analysis task. Relatively similar to NLI is the task of Language Variety Identification. It has been recently addressed by the research community (Zampieri and Gebre, 2012; Sadat et al., 2014; Maier and G´omez-Rodr´ıguez, 2014). Introduction We present a system for identifying the native language (L1) of a writer based solely on a sample of their writing in a second language (L2). In this work we focus on English as the second language. According to the weak Contrastive Analysis Hypothesis (Lado, 1957), speakers and writers of the same L1"
R15-1040,W13-1714,0,0.237633,"Missing"
R15-1040,W13-1726,0,0.0383212,"Missing"
R15-1040,W13-1734,0,0.0348285,"Missing"
R15-1040,P96-1041,0,0.417414,"Missing"
R15-1040,W14-4204,0,0.065283,"Missing"
R15-1040,W15-0606,0,0.238206,"Missing"
R15-1040,W13-1728,0,0.16607,"Missing"
R15-1040,W13-1716,0,0.0336378,"Missing"
R15-1040,P10-2041,0,0.0849985,"Missing"
R15-1040,D14-1142,0,0.190425,"Missing"
R15-1040,W14-5904,0,0.0214206,"rious applications including criminal law (e.g., identifying writers of harassing messages, verifying the authenticity of suicide notes), civil law (e.g., copyright disputes), and forensic linguistics. In the end, it includes the traditional applications to literary research (e.g., attributing anonymous or disputed literary works to known authors). Bergsma et al. (2012) consider the NLI task as a sub-task of the authorship analysis task. Relatively similar to NLI is the task of Language Variety Identification. It has been recently addressed by the research community (Zampieri and Gebre, 2012; Sadat et al., 2014; Maier and G´omez-Rodr´ıguez, 2014). Introduction We present a system for identifying the native language (L1) of a writer based solely on a sample of their writing in a second language (L2). In this work we focus on English as the second language. According to the weak Contrastive Analysis Hypothesis (Lado, 1957), speakers and writers of the same L1 can sometimes be identified by similar L2 errors. These errors may be a result of linguistic interference. Common tendencies of a speaker’s L1 are superimposed onto their L2. Native Language Identification (NLI) is an attempt to exploit these err"
R15-1040,P12-2038,0,0.228166,"Missing"
R15-1040,C12-1158,0,0.226075,"Missing"
R15-1040,W13-1706,0,0.136232,"Missing"
R15-1040,N03-1033,0,0.0145131,"tokens in the scoring text were excluded from the computation, so the measurements are not strictly comparable. We believe that this drawback is reasonable: (1) it allows us to compute scores for all training instances, and (2) we do not have to split the training data into two parts – one for building the language model and the other for the cross-entropy calculation. 4.4 • Suffixes (Sn ). Language models built on token suffixes of the length n ∈ {2, ..., 6}. • POS tags (P). Language model built on POS tags. We tagged the TOEFL11 corpus as well as the whole Wikipedia by the Stanford tagger (Toutanova et al., 2003). For each feature family we built and compared the performance of two language models: one from the original text, and the other using the same, but lower-cased text. Moreover, we experimented with and compared different smoothing methods, as described in details in Section 5.2. 4.6 Other features To complete the list of feature families, we added 9 statistical (ST) and two categorical (PR) features: Text length characteristics include the number of sentences, number of tokens and number of characters for the given instance. It also includes the average sentence length (# of tokens / # of sen"
shaalan-etal-2012-arabic,J96-1003,0,\N,Missing
shaalan-etal-2012-arabic,C90-2036,0,\N,Missing
shaalan-etal-2012-arabic,P00-1037,0,\N,Missing
shaalan-etal-2012-arabic,W11-4417,1,\N,Missing
shaalan-etal-2012-arabic,I08-2131,0,\N,Missing
shaalan-etal-2012-arabic,W06-1648,0,\N,Missing
spoustova-etal-2008-validating,W02-1001,0,\N,Missing
spoustova-etal-2008-validating,J08-4004,0,\N,Missing
spoustova-etal-2010-building,W02-1001,0,\N,Missing
spoustova-etal-2010-building,E09-1087,1,\N,Missing
spoustova-etal-2010-building,P02-1006,0,\N,Missing
spoustova-etal-2010-building,halacsy-etal-2008-parallel,0,\N,Missing
strakova-pecina-2010-czech,H05-1066,0,\N,Missing
strakova-pecina-2010-czech,W08-0325,0,\N,Missing
uresova-etal-2014-multilingual,E12-1012,0,\N,Missing
uresova-etal-2014-multilingual,W12-3102,0,\N,Missing
uresova-etal-2014-multilingual,P02-1040,0,\N,Missing
uresova-etal-2014-multilingual,P07-2045,0,\N,Missing
uresova-etal-2014-multilingual,W04-3250,0,\N,Missing
uresova-etal-2014-multilingual,P03-1021,0,\N,Missing
uresova-etal-2014-multilingual,2011.iwslt-papers.5,0,\N,Missing
W09-0403,W05-0909,0,0.369132,"Missing"
W09-0403,E06-1032,0,0.0882327,"elation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed. Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33–36, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 33 vantage concerns the use of a proprietary matching algorithm which has not been made public and which requires the actual use of the Trados software. the paper (Callison-Burch et al., 2006). The authors showed examples where changing a good word order into an unacceptable one did not affect the BLEU score. We may add a different example documenting the phenomenon that a pair of syntactically correct Czech sentences with the same word forms, differing only in the word order whose n-gram score for n = 2, 3, and 4 differs greatly. Let us take one of the sentences from the 2008 SMT workshop and its reference translation: Nevertheless, the matching algorithm of Trados gives results which to a great extent correspond to a much simpler traditional metric, to the Levenshtein’s edit dist"
W09-0403,W08-0309,0,0.0865598,"Missing"
W09-0403,P02-1040,0,0.0875457,"lignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs. 1 Introduction The problem of finding a reliable machine translation metrics corresponding with a human judgment has recently returned to the centre of attention. After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs. The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NI"
W09-0403,W08-0312,0,\N,Missing
W10-1720,P96-1041,0,0.070396,"M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 145combination output is detokenised and true-cased. 3.2 English–Czech (en–cs) Experiments ˇ The CzEng corpus (Bojar and Zabokrtsk´ y, 2009) is"
W10-1720,J07-2003,0,0.0791111,"ngually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to l"
W10-1720,W07-0712,0,0.00757178,"Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 1"
W10-1720,2009.freeopmt-1.3,1,0.877177,"Missing"
W10-1720,2004.tmi-1.11,1,0.0884387,"English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we"
W10-1720,Y09-1019,1,0.818274,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,2009.eamt-1.32,1,0.0893869,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,W04-3250,0,0.127192,"Missing"
W10-1720,2005.mtsummit-papers.11,0,0.0279389,"gth posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Nations parallel data. We used a maximum sentence length of 80 for en–es and 40 for en–cs. Detailed statistics are shown in Table 1. Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages"
W10-1720,P07-2045,0,0.0171046,"Examples). This system exploits example-based pects of both the EBMT and SMT paradigms. MT, statistical MT (SMT), and system combinaThe architecture includes various individual systion techniques. tems: phrase-based, example-based, hierarchical We participated in the English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based"
W10-1720,N04-1022,0,0.0614202,"ribe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English– Spanish and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architect"
W10-1720,P03-1021,0,0.0273052,"output of each individual system. The CN is built by aligning other hypotheses against the backbone, based on the TER metric. Null words are allowed in the alignment. Either votes or different confidence measures are assigned to each word in the network. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length rat"
W10-1720,P02-1038,0,0.00732471,"xtracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). We used three factors in our factored translation model, which are used in two different decoding paths: a surface form (SF) to SF translation factor, a lemma to lemma translation factor, and a part-ofspeech (PoS) to PoS translation factor. Finally, we used two decoding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI"
W10-1720,P02-1040,0,0.113346,"Missing"
W10-1720,2006.amta-papers.25,0,0.0454791,"and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architecture In this paper, we present the DCU multi-engine The M AT R E X system is a combination-based MT system M AT R E X (Machine Translation using multi-engine architecture, which explo"
W10-1720,2007.tmi-papers.28,1,0.632786,"Missing"
W10-1720,2006.iwslt-evaluation.4,1,0.38824,"tion factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it as the conditional feature (h probability of the target phrases (ˆ ek ) given the ˆ source phrase (fk ) and its source-side context information (CI): ˆ MBL = log P (ˆ h ek |fˆk , CI(fˆk )) Figure 1: System Framework. tion probabilities and the amount of word-to-word cognates (Stroppa and Way, 2006). Once these phrase pairs were obtained they were merged with the phrase pairs extracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it"
W10-1720,W06-3110,0,0.03498,"word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Natio"
W10-1720,W96-0213,0,0.0699016,"ng the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we us"
W10-1720,N07-1029,0,0.00800063,"d Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it"
W10-1720,D07-1091,0,\N,Missing
W10-1720,2003.mtsummit-systems.3,0,\N,Missing
W10-1720,W10-1742,1,\N,Missing
W10-1742,N04-1022,0,0.159363,"Missing"
W10-1742,E06-1005,0,0.0258044,". Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and Frenchto-English combination tasks. Section 7 gives our conclusions. 2 Hypothesis Alignment Methods Hypothesis alignment plays a vital role in the CN, as the backbone sentence determines the skeleton and the word order of the consensus output. In the combination evaluation task, we integrated TER (Snover et al., 2006), HMM (Matusov et al., 2006) and TERp (Snover et al., 2009) into our augmented three-pass combination framework. In this section, we briefly describe these three methods. 2.1 TER The TER (Translation Edit Rate) metric measures the ratio of the number of edit operations between the hypothesis E 0 and the reference Eb to the total number of words in Eb . Here the backbone Eb is assumed to be the reference. The allowable edits include insertions (Ins), deletions (Del), substitutions (Sub), and phrase shifts (Shft). The TER of E 0 compared to Eb is computed as in (1): where Nb is the total number of words in Eb . The differe"
W10-1742,P03-1021,0,0.0351504,"hmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated in the English–Czech and French–English system combination tasks. In our system combination framework, we use a large-scale monolingual data to train language models and carry out POS-tagging. 5.1 English-Czech Training Data The statistics of the data used for language models training are shown in Table 1. Monolingual tokens (Cz) 2,214,757 81,161,278 205,600,053 288,976,088 Number of sentences 84,706 8,027,391 13,042,040 21,154,137 Table 1: Statistics of data in the En–Cz task All the data are provided by the workshop organisers. 1 In Table 1, “News-Comm” indicates the data"
W10-1742,P02-1040,0,0.0871932,"nd we used the default configuration of optimised edit costs. 3 Augmented Three-Pass Combination Framework The construction of the augmented three-pass combination framework is shown in Figure 1. N Single MT Systems BLEU TER Top M Single TERp BLEU TER TERp Alignment HMM TER TERp Pass 1 Individual CNs Nbest Re-ranking Super CN Networks Pass 1: Specific Metric-based Single Networks 1. Merge all the 1-best hypotheses from single MT systems into a new N -best set Ns . 2. Utilise the standard MBR decoder to select one from the Ns as the backbone given some specific loss function such as TER, BLEU (Papineni et al., 2002) and TERp; Additionally, in order to increase the diversity of candidates used for Pass 2 and Pass 3, we also use the 1-best hypotheses from the top M single MT systems as the backbone. Add the backbones generated by MBR into Ns . 3. Perform the word alignment between the different backbones and the other hypotheses via the TER, HMM, TERp (only for English) metrics. 4. Carry out word reordering based on word alignment (TER and TERp have completed the reordering in the process of scoring) and build individual CNs (Rosti et al., 2007); 5. Decode the single networks and export the 1best outputs a"
W10-1742,W96-0213,0,0.659158,"oding to search for the The lines with arrows pointing to “mConMBR” best final result from Ncon . In this step, we represent adding outputs into the mConMBR deset a uniform distribution between the candicoding component. “Top M Single” indicates that dates in Ncon . the 1-best results from the best M individual MT 292 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated"
W10-1742,P07-1040,0,0.0900202,"et of hypotheses, and then component which is used to re-rank the the remaining hypotheses are aligned against the N -best lists generated from the individual backbone by a specific alignment approach. CurCNs and the super network, 2) a new hyrently, most research in system combination has pothesis alignment metric – TERp – that focused on hypothesis alignment due to its signifis used to carry out English-targeted hyicant influence on combination quality. pothesis alignment, and 3) more differA multiple CN or “super-network” framework ent backbone-based CNs which are emwas firstly proposed in Rosti et al. (2007) who ployed to increase the diversity of the used each of all individual system results as the mConMBR decoding phase. We took backbone to build CNs based on the same alignpart in the combination tasks of Englishment metric, TER (Snover et al., 2006). A consento-Czech and French-to-English. Expersus network MBR (ConMBR) approach was preimental results show that our proposed sented in (Sim et al., 2007), where MBR decodcombination framework achieved 2.17 abing is employed to select the best hypothesis with solute points (13.36 relative points) and the minimum cost from the original single syste"
W10-1742,2006.amta-papers.25,0,0.404187,", most research in system combination has pothesis alignment metric – TERp – that focused on hypothesis alignment due to its signifis used to carry out English-targeted hyicant influence on combination quality. pothesis alignment, and 3) more differA multiple CN or “super-network” framework ent backbone-based CNs which are emwas firstly proposed in Rosti et al. (2007) who ployed to increase the diversity of the used each of all individual system results as the mConMBR decoding phase. We took backbone to build CNs based on the same alignpart in the combination tasks of Englishment metric, TER (Snover et al., 2006). A consento-Czech and French-to-English. Expersus network MBR (ConMBR) approach was preimental results show that our proposed sented in (Sim et al., 2007), where MBR decodcombination framework achieved 2.17 abing is employed to select the best hypothesis with solute points (13.36 relative points) and the minimum cost from the original single system 1.52 absolute points (5.37 relative points) outputs compared to the consensus output. in terms of BLEU score on English-toDu and Way (2009) proposed a combination Czech and French-to-English tasks restrategy that employs MBR, super network, and spe"
W10-1742,W09-0441,0,0.01692,"ranslation and MetricsMATR, pages 290–295, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pass, all the 1-best hypotheses coming from single MT systems, individual networks, and the super network are combined to select the final result using the mConMBR decoder. In the system combination task of WMT 2010, we adopted an augmented framework by extending the strategy in (Du and Way, 2009). In addition to the basic three-pass architecture, we augment our combination system as follows: • We add a rescoring component in Pass 1 and Pass 2. • We introduce the TERp (Snover et al., 2009) alignment metric for the English-targeted combination. • We employ different backbones and hypothesis alignment metrics to increase the diversity of candidates for our mConMBR decoding. The remainder of this paper is organised as follows. In Section 2, we introduce the three hypothesis alignment methods used in our framework. Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and"
W10-1742,W06-3110,0,0.0584497,"onMBR” best final result from Ncon . In this step, we represent adding outputs into the mConMBR deset a uniform distribution between the candicoding component. “Top M Single” indicates that dates in Ncon . the 1-best results from the best M individual MT 292 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated in the English–Czech and French–English system combination tas"
W10-1742,W10-1720,1,\N,Missing
W10-3704,attia-etal-2010-automatically,1,0.604545,"Missing"
W10-3704,deksne-etal-2008-dictionary,0,0.29076,"een applied to bigrams and trigrams, and it becomes more problematic to extract MWEs of more than three words. As a consequence, each approach requires specific resources and is suitable for dealing with only one side of a multifaceted problem. Pecina (2010) evaluates 82 lexical association measures for the ranking of collocation candidates and concludes that it is not possible to select a single best universal measure, and that different measures give different results for different tasks depending on data, language, and the types of MWE that the task is focused on. Similarly, Ramisch et al. (2008) investigate the hypothesis that MWEs can be detected solely by looking at the distinct statistical properties of their individual words and conclude that the association measures can only detect trends and preferences in the co-occurrences of words. A lot of effort has concentrated on the task of 2 Data Resources In this project we use three data resources for extracting MWEs. These resources differ widely in nature, size, structure and the main purpose they are used for. In this section we give a brief introduction to each of these data resources. Wikipedia (WK) is a freely-available multili"
W10-3704,elkateb-etal-2006-building,0,0.0380663,"Missing"
W10-3704,W09-2905,0,0.0428556,"Missing"
W10-3704,W04-0411,0,0.0213009,"ne Translation (Deksne, 2008). There are two basic criteria for identifying MWEs: first, component words exhibit statistically significant co-occurrence, and second, they show a certain level of semantic opaqueness or non-compositionality. Statistically significant cooccurrence can give a good indication of how likely a sequence of words is to form an MWE. This is particularly interesting for statistical techniques which utilize the fact that a large number of MWEs are composed of words that co-occur together more often than can be expected by chance. The compositionality, or decomposability (Villavicencio et al. 2004), of MWEs is also a core issue that presents a challenge for NLP applications because the meaning of the expression is not directly predicted from the meaning of the component words. In this respect, compositionalily varies between phrases that are highly comIntroduction A lexicon of multiword expressions (MWEs) has a significant importance as a linguistic resource because MWEs cannot usually be analyzed literally, or word-for-word. In this paper we apply three approaches to the extraction of Arabic MWEs from multilingual, bilingual, and monolingual data sources. We rely on linguistic informat"
W10-3704,W97-0311,0,0.144125,"two well-known among them are “non-substitutability”, when a word in the expression cannot be substituted by a semantically equivalent word, and “single-word paraphrasability”, when the expression can be paraphrased or translated by a single word. These two indications have been exploited differently by different researchers. Van de Cruys and Moiro´ n (2006) develop an unsupervised method for detecting MWEs using clusters of semantically related words and taking the ratio of the word preference over the cluster preference as an indication of how likely a particular expression is to be an MWE. Melamed (1997) investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take noncompositional compounds into account are more accurate. Moir´on and Tiedemann (2006) use word alignment of parallel corpora to locate the translation of an MWE in a target language and decide whether the original expression is idiomatic or literal. The technique used here is inspired by that of Zarrieß and Kuhn (2009) who rely on the linguistic intuition that if a group of words in one language is translated as a single word in another lang"
W10-3704,vintar-fiser-2008-harvesting,0,0.0593775,"Missing"
W10-3704,W06-2405,0,0.0453337,"expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs. Given the heterogeneity of MWEs, different approaches were devised. Broadly speaking, work on the extraction of MWEs revolves around four approaches: (a) statistical metho"
W10-3704,W09-2904,0,0.321046,"ase”, and those  that show a degree of idiomaticity, such as,          madiynatu ’l-mal¯ ahiy, “amusement park”, lit. “city of amusements”. In extreme cases the meaning of the expression as a whole is utterly  unrelated to the component words, such as,         !  farasu ’l-nabiyyi, “grasshopper”, lit. “the horse of the Prophet”. automatically extracting MWEs for various languages besides English, including Slovene (Vintar and Fiˇser, 2008), Chinese (Duan et al., 2009), Czech (Pecina, 2010), Dutch (Van de Cruys and Moir´on, 2006), Latvian (Deksne, 2008) and German ( Zarrieß and Kuhn, 2009). A few papers, however, focus on Arabic MWEs. Boulaknadel et al. (2009) develop a hybrid multiword term extraction tool for Arabic in the “environment” domain. Attia (2006) reports on the semi-automatic extraction of various types of MWEs in Arabic and how they are used in an LFG-based parser. In this paper we report on three different methods for the extraction of MWEs for Arabic, a less resourced language. Our approach is linguistically motivated and can be applied to other languages. 1.2 Related Work A considerable amount of research has focused on the identification and extraction of MWEs"
W10-3704,W03-1812,0,\N,Missing
W10-3704,boulaknadel-etal-2008-multi,0,\N,Missing
W10-3707,C04-1114,0,0.441408,"ither. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionaries, which in effect are instances of atomic translation pairs, to the parallel corpus is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008). We modify the parallel corpus by converting the MWEs into single tokens and adding the aligned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtai"
W10-3707,W09-3539,1,0.741895,"Missing"
W10-3707,W05-0909,0,0.209211,"Missing"
W10-3707,W04-3248,0,0.280025,"Missing"
W10-3707,J93-2003,0,0.0150925,"ed MWEs can bring about any further improvement on top of that. We carried out our experiments on an English—Bangla translation task, a relatively hard task with Bangla being a morphologically richer language. 3 System Description 3.1 PB-SMT Translation is modeled in SMT as a decision process, in which the translation e1I = e1 . . . ei . . . eI of a source sentence f1J = f1 . . . fj . . . fJ is chosen to maximize (1): (1) arg max P(e1I |f1J ) = arg max P( f1J |e1I ).P(e1I ) I ,e1I I ,e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modeled as a log-linear combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P(e1I |f 1J ) = ∑ λ m hm ( f 1J , e1I , s1K ) m =1 + λLM log P(e1I ) (2) where s = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3): k 1 ∀1 ≤ k ≤ K , sk = (ik, bk, jk), eˆk = eik −1 +1...eik , fˆ = f ... f . k bk jk (3) and eac"
W10-3707,W03-1502,0,0.128214,"r simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilin"
W10-3707,N10-1029,0,0.108076,"reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other approaches to SMT) does not generally treat MWEs as special tokens. Another problem SMT suffers from is that verb phrases are often wrongly translated, or even sometimes deleted in the output in or"
W10-3707,C08-2007,0,0.104429,"Missing"
W10-3707,P07-2045,0,0.0291762,"gual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since"
W10-3707,2006.amta-papers.25,0,0.0728121,"Missing"
W10-3707,C96-2141,0,0.388305,"(or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionar"
W10-3707,W04-3250,0,0.31373,"Missing"
W10-3707,W06-1204,0,0.435823,"Missing"
W10-3707,P01-1050,0,0.00961498,"pus. Multi-word expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration me"
W10-3707,C08-1125,0,0.154066,"Missing"
W10-3707,E03-1035,0,0.114084,"igned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtained, together with some analysis. Section 5 concludes, and provides avenues for further work. 2 Related Work Moore (2003) presented an approach for simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical N"
W10-3707,P03-1021,0,0.0160755,"guages to Indian Languages Machine Translation (ILILMT) System”. NEs in Bangla are identified using the NER system of Ekbal and Bandyopadhyay (2008). We use the Stanford Parser, Stanford NER and the NER for Bangla along with the default model files provided, i.e., with no additional training. The effectiveness of the MWE-aligned parallel corpus developed in the work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and 1 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 2 http://nlp.stanford.edu/software/lex-parser.shtml 3 4 Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). http://crfchunker.sourceforge.net/ http://nlp.stanford.edu/software/CRF-NER.shtml 50 Experiments and Results We randomly extracted 500 sentences each for the development set and testset fr"
W10-3707,P02-1040,0,0.0819899,"Missing"
W10-3707,W09-2907,0,0.376728,"re, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWE"
W10-3707,N03-1017,0,\N,Missing
W11-4417,2006.bcs-1.5,1,0.693113,"ttested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a historical vers"
W11-4417,2003.mtsummit-semit.5,0,0.72348,"exical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC’s SAMA (Standard Arabic Morphological Analyser). 1 Introduction Due to its complexity, Arabic morphology has always been a challenge for computational processing and a hard testing ground for morphological analysis technologies. A lexicon is a core component of any morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001). The quality and coverage of the lexical database determines the quality and coverage of the morphological analyser, and limitations in the lexicon will cascade through to higher levels of processing. In this paper, we present an approach to automatically construct a corpus-based lexical database for Modern Standard Arabic (MSA), focusing on the 1 http://sourceforge.net/projects/aracomlex/ 125 This paper is structured as follows. In the introduction, we differentiate between MSA, the focus of this research, and Classical Arabic (CA) which is a hi"
W11-4417,E09-2008,0,0.126089,"technology that makes it especially attractive in dealing with human language morphologies; among these are the ability to handle concatenative and nonconcatenative morphotactics, and the high speed and efficiency in handling large automata of lexicons with their derivations and inflections that can run into millions of paths. The Xerox XFST System (Beesley and Karttunen, 2003) is a well-known finite state compiler, but the disadvantage of this tool is that it is a proprietary software, which limits its use in the larger research community. Fortunately, there is an alternative, namely Foma, (Hulden, 2009), which is an opensource finite-state toolkit that implements the Xerox lexc and xfst utilities. We have developed an opensource morphological analyser for Arabic using the Foma compiler allowing us to share our morphology with third parties. The lexical database, which is being edited and validated, is used to automatically extend and update the morphological analyser, allowing for greater coverage and better capabilities. Arabic words are formed through the amalgamation of two tiers, namely root and pattern. A root is a sequence of three consonants and the pattern is a template of vowels (or"
W11-4417,P08-2030,0,0.204709,"Missing"
W13-1006,P99-1041,0,0.471317,"the semantic compositionality of expressions. The analysis can be found in Section 2. A special attention is paid to the evaluation of the proposed models that is described in Section 3. Section 4 presents our first intuitive experimental setup and results of LSA applied to the DISCO 2011 task. Section 5 concludes the paper. 42 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42–50, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics 2 Semantic Compositionality of Word Expressions Determined by WSMs Several recent works, including Lin (1999), Schone and Jurafsky (2001), Baldwin et al. (2003), McCarthy et al. (2003), Katz (2006), Johannsen et al. (2011), Reddy et al. (2011a), and Krˇcm´aˇr et al. (2012), show the ability of methods based on WSMs to capture the degree of semantic compositionality of word expressions. We analyse the proposed methods and discuss their differences. As further described in detail and summarized in Table 1, the approaches differ in the type of WSMs, corpora, preprocessing techniques, methods for determining the compositionality, datasets for evaluation, and methods of evaluation itself. Our understandin"
W13-1006,W03-1810,0,0.583199,"Missing"
W13-1006,P08-1028,0,0.0644062,"tional characteristics of expressions and their components. The context vectors expected to be different from each other are e.g. the vector representing the expression “hot dog” and the vector representing the word “dog”. 3) The compositionality-based methods compare two vectors of each analysed expression: the true co-occurrence vector of an expression and the vector obtained from vectors corresponding to the components of the expression using a compositionality function (Reddy et al., 2011a). The most common compositionality functions are vector addition or pointwise vector multiplication (Mitchell and Lapata, 2008). For example, the vectors for “hot dog” and “hot”⊕“dog” are supposed to be different. Evaluation datasets There is still no consensus on how to evaluate models determining semantic compositionality. However, by examining the discussed papers, we have observed an increasing tenPaper Lin (1999) Schone+Jurafsky(2001) Baldwin et al. (2003) McCarthy et al. (2003) Katz (2006) Krˇcm´aˇr et al. (2012) Johannsen et al. (2011) Reddy et al. (2011a) Corpora 125m, triples 6.7m TREC BNC+POS BNC+GR GNC ukWaC+POS ukWaC ukWaC+POS WSMs own LSA LSA own LSA COALS COALS own Methods SY SY, CY CT CTn CY SY SY, CT C"
W13-1006,pearce-2002-comparative,0,0.124566,"eprocessing, Lin (1999) extracts triples with dependency relationships, Baldwin et al. (2003), Reddy et al. (2011a), and Krˇcm´aˇr et al. (2012) concatenate word lemmas with their POS categories. Johannsen et al. (2011) use word lemmas and remove low-frequency words while Reddy et al. (2011a), for example, keep only frequent content words. Methods We have identified three basic methods for determining semantic compositionality: 1) The substitutability-based methods exploit the fact that replacing components of noncompositional expressions by words which are similar leads to anti-collocations (Pearce, 2002). Then, frequency or mutual information of such expressions (anti-collocations) is compared with the frequency or mutual information of the original expressions. For example, consider expected occurrence counts of “hot dog” and its anti-collocations such as “warm dog” or “hot terrier”. 2) The component-based methods, utilized for example by Baldwin et al. (2003) or Johannsen et al. (2011), compare the distributional characteristics of expressions and their components. The context vectors expected to be different from each other are e.g. the vector representing the expression “hot dog” and the"
W13-1006,W12-3311,0,0.0228065,"by the hypothesis that a special treatment of semantically non-compositional expressions can improve results in various Natural Language Processing (NPL) tasks, as shown for example by Acosta et al. (2011), who utilized MWEs in Information Retrieval (IR). Besides that, there are other NLP applications that can benefit from knowing the degree of compositionality of expressions such as machine translation (Carpuat and Diab, 2010), lexicography (Church and Hanks, 1990), word sense disambiguation (Finlayson and Kulkarni, 2011), part-of-speech (POS) tagging and parsing (Seretan, 2008) as listed in Ramisch (2012). The main goal of this paper is to present an analysis of previous approaches using WSMs for determining the semantic compositionality of expressions. The analysis can be found in Section 2. A special attention is paid to the evaluation of the proposed models that is described in Section 3. Section 4 presents our first intuitive experimental setup and results of LSA applied to the DISCO 2011 task. Section 5 concludes the paper. 42 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42–50, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistic"
W13-1006,I11-1024,0,0.28635,"evaluation of the proposed models that is described in Section 3. Section 4 presents our first intuitive experimental setup and results of LSA applied to the DISCO 2011 task. Section 5 concludes the paper. 42 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42–50, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics 2 Semantic Compositionality of Word Expressions Determined by WSMs Several recent works, including Lin (1999), Schone and Jurafsky (2001), Baldwin et al. (2003), McCarthy et al. (2003), Katz (2006), Johannsen et al. (2011), Reddy et al. (2011a), and Krˇcm´aˇr et al. (2012), show the ability of methods based on WSMs to capture the degree of semantic compositionality of word expressions. We analyse the proposed methods and discuss their differences. As further described in detail and summarized in Table 1, the approaches differ in the type of WSMs, corpora, preprocessing techniques, methods for determining the compositionality, datasets for evaluation, and methods of evaluation itself. Our understanding of WSM is in agreement with Sahlgren (2006): “The word space model is a computational model of word meaning that utilizes the distr"
W13-1006,W11-1310,0,0.539629,"evaluation of the proposed models that is described in Section 3. Section 4 presents our first intuitive experimental setup and results of LSA applied to the DISCO 2011 task. Section 5 concludes the paper. 42 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42–50, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics 2 Semantic Compositionality of Word Expressions Determined by WSMs Several recent works, including Lin (1999), Schone and Jurafsky (2001), Baldwin et al. (2003), McCarthy et al. (2003), Katz (2006), Johannsen et al. (2011), Reddy et al. (2011a), and Krˇcm´aˇr et al. (2012), show the ability of methods based on WSMs to capture the degree of semantic compositionality of word expressions. We analyse the proposed methods and discuss their differences. As further described in detail and summarized in Table 1, the approaches differ in the type of WSMs, corpora, preprocessing techniques, methods for determining the compositionality, datasets for evaluation, and methods of evaluation itself. Our understanding of WSM is in agreement with Sahlgren (2006): “The word space model is a computational model of word meaning that utilizes the distr"
W13-1006,W01-0513,0,0.0609881,"Missing"
W13-1006,J90-1003,0,\N,Missing
W13-1006,W11-1304,0,\N,Missing
W13-1006,W06-1203,0,\N,Missing
W13-1006,W03-1812,0,\N,Missing
W13-1006,W11-0805,0,\N,Missing
W13-1006,N10-1029,0,\N,Missing
W13-1006,W03-1809,0,\N,Missing
W13-1006,W11-1307,0,\N,Missing
W13-1006,W11-1305,0,\N,Missing
W13-1006,W11-0815,0,\N,Missing
W13-1006,P10-4006,0,\N,Missing
W13-1016,P05-1038,0,0.0231723,"ependency Treebank and includes deep syntactic dependency trees of all MWEs. 1 Introduction 2 Multiword expressions (MWEs) exist on the interface of syntax, semantics, and lexicon, yet they are almost completely absent from major syntactic theories and semantic formalisms. They also have interesting morphological properties and for all these reasons, they are important, but challenging for Natural Language Processing (NLP). Recent advances show that taking MWEs into account can improve NLP tasks such as dependency parsing (Nivre and Nilsson, 2004; Eryi˘git et al., 2011), constituency parsing (Arun and Keller, 2005), text generation (Hogan et al., 2007), or machine translation (Carpuat and Diab, 2010). The Prague Dependency Treebank (PDT) of Czech and the associated lexicon of MWEs SemLex1 offer a unique opportunity for experimentation 1 http://ufal.mff.cuni.cz/lexemann/mwe/semlex.zip Processing of Multiword Expressions and Related Work Automatic processing of multiword expressions includes two distinct (but interlinked) tasks. Most of the effort has been put into acquisition of MWEs appearing in a particular text corpus into a lexicon of MWEs (types) not necessarily linked with their occurrences (instan"
W13-1016,N10-1029,0,0.0389917,"ction 2 Multiword expressions (MWEs) exist on the interface of syntax, semantics, and lexicon, yet they are almost completely absent from major syntactic theories and semantic formalisms. They also have interesting morphological properties and for all these reasons, they are important, but challenging for Natural Language Processing (NLP). Recent advances show that taking MWEs into account can improve NLP tasks such as dependency parsing (Nivre and Nilsson, 2004; Eryi˘git et al., 2011), constituency parsing (Arun and Keller, 2005), text generation (Hogan et al., 2007), or machine translation (Carpuat and Diab, 2010). The Prague Dependency Treebank (PDT) of Czech and the associated lexicon of MWEs SemLex1 offer a unique opportunity for experimentation 1 http://ufal.mff.cuni.cz/lexemann/mwe/semlex.zip Processing of Multiword Expressions and Related Work Automatic processing of multiword expressions includes two distinct (but interlinked) tasks. Most of the effort has been put into acquisition of MWEs appearing in a particular text corpus into a lexicon of MWEs (types) not necessarily linked with their occurrences (instances) in the text. The bestperforming methods are usually based on lexical association m"
W13-1016,W07-1106,0,0.023972,"characterized by more or less non-compositional (figurative) meaning. Their components, however, can also occur with the same syntax but compositional (literal) semantics, and therefore not acting as MWEs (Jedinou branku dal aˇz v posledn´ı minutˇe z´apasu / He scored his only goal in the last minute of the match. vs. Rozhodˇc´ı dal branku zpˇet na sv´e m´ısto / The referee put a goal back to its place). Automatic discrimination between figurative and literal meaning is a challenging task similar to 107 word sense disambiguation which has been studied extensively: Katz and Giesbrecht (2006), Cook et al. (2007), Hashimoto and Kawahara (2008), Li and Sporleder (2009), and Fothergill and Baldwin (2011). Seretan (2010) includes MWE identification (based on a lexicon) in a syntactic parser and reports an improvement of parsing quality. As a by-product, the parser identified occurrences of MWEs from a lexicon. Similarly, Green et al. (2013) embed identification of some MWEs in a Tree Substitution Grammar and achieve improvement both in parsing quality and MWE identification effectiveness. None of these works, however, attempt to identify all MWEs, regardless their length or complexity, which is the main"
W13-1016,W11-3806,0,0.0689413,"Missing"
W13-1016,I11-1102,0,0.0125213,"ponents, however, can also occur with the same syntax but compositional (literal) semantics, and therefore not acting as MWEs (Jedinou branku dal aˇz v posledn´ı minutˇe z´apasu / He scored his only goal in the last minute of the match. vs. Rozhodˇc´ı dal branku zpˇet na sv´e m´ısto / The referee put a goal back to its place). Automatic discrimination between figurative and literal meaning is a challenging task similar to 107 word sense disambiguation which has been studied extensively: Katz and Giesbrecht (2006), Cook et al. (2007), Hashimoto and Kawahara (2008), Li and Sporleder (2009), and Fothergill and Baldwin (2011). Seretan (2010) includes MWE identification (based on a lexicon) in a syntactic parser and reports an improvement of parsing quality. As a by-product, the parser identified occurrences of MWEs from a lexicon. Similarly, Green et al. (2013) embed identification of some MWEs in a Tree Substitution Grammar and achieve improvement both in parsing quality and MWE identification effectiveness. None of these works, however, attempt to identify all MWEs, regardless their length or complexity, which is the main goal of this paper. 3 Definition of Multiword Expressions We can use the rough definition o"
W13-1016,J13-1009,0,0.166108,"Missing"
W13-1016,D08-1104,0,0.0902005,"re or less non-compositional (figurative) meaning. Their components, however, can also occur with the same syntax but compositional (literal) semantics, and therefore not acting as MWEs (Jedinou branku dal aˇz v posledn´ı minutˇe z´apasu / He scored his only goal in the last minute of the match. vs. Rozhodˇc´ı dal branku zpˇet na sv´e m´ısto / The referee put a goal back to its place). Automatic discrimination between figurative and literal meaning is a challenging task similar to 107 word sense disambiguation which has been studied extensively: Katz and Giesbrecht (2006), Cook et al. (2007), Hashimoto and Kawahara (2008), Li and Sporleder (2009), and Fothergill and Baldwin (2011). Seretan (2010) includes MWE identification (based on a lexicon) in a syntactic parser and reports an improvement of parsing quality. As a by-product, the parser identified occurrences of MWEs from a lexicon. Similarly, Green et al. (2013) embed identification of some MWEs in a Tree Substitution Grammar and achieve improvement both in parsing quality and MWE identification effectiveness. None of these works, however, attempt to identify all MWEs, regardless their length or complexity, which is the main goal of this paper. 3 Definitio"
W13-1016,D07-1028,0,0.0320397,"Missing"
W13-1016,W06-1203,0,0.123792,"oint of view, MWEs are often characterized by more or less non-compositional (figurative) meaning. Their components, however, can also occur with the same syntax but compositional (literal) semantics, and therefore not acting as MWEs (Jedinou branku dal aˇz v posledn´ı minutˇe z´apasu / He scored his only goal in the last minute of the match. vs. Rozhodˇc´ı dal branku zpˇet na sv´e m´ısto / The referee put a goal back to its place). Automatic discrimination between figurative and literal meaning is a challenging task similar to 107 word sense disambiguation which has been studied extensively: Katz and Giesbrecht (2006), Cook et al. (2007), Hashimoto and Kawahara (2008), Li and Sporleder (2009), and Fothergill and Baldwin (2011). Seretan (2010) includes MWE identification (based on a lexicon) in a syntactic parser and reports an improvement of parsing quality. As a by-product, the parser identified occurrences of MWEs from a lexicon. Similarly, Green et al. (2013) embed identification of some MWEs in a Tree Substitution Grammar and achieve improvement both in parsing quality and MWE identification effectiveness. None of these works, however, attempt to identify all MWEs, regardless their length or complexity"
W13-1016,D09-1033,0,0.0144326,"igurative) meaning. Their components, however, can also occur with the same syntax but compositional (literal) semantics, and therefore not acting as MWEs (Jedinou branku dal aˇz v posledn´ı minutˇe z´apasu / He scored his only goal in the last minute of the match. vs. Rozhodˇc´ı dal branku zpˇet na sv´e m´ısto / The referee put a goal back to its place). Automatic discrimination between figurative and literal meaning is a challenging task similar to 107 word sense disambiguation which has been studied extensively: Katz and Giesbrecht (2006), Cook et al. (2007), Hashimoto and Kawahara (2008), Li and Sporleder (2009), and Fothergill and Baldwin (2011). Seretan (2010) includes MWE identification (based on a lexicon) in a syntactic parser and reports an improvement of parsing quality. As a by-product, the parser identified occurrences of MWEs from a lexicon. Similarly, Green et al. (2013) embed identification of some MWEs in a Tree Substitution Grammar and achieve improvement both in parsing quality and MWE identification effectiveness. None of these works, however, attempt to identify all MWEs, regardless their length or complexity, which is the main goal of this paper. 3 Definition of Multiword Expression"
W13-1016,P05-2003,1,0.779108,"z/lexemann/mwe/semlex.zip Processing of Multiword Expressions and Related Work Automatic processing of multiword expressions includes two distinct (but interlinked) tasks. Most of the effort has been put into acquisition of MWEs appearing in a particular text corpus into a lexicon of MWEs (types) not necessarily linked with their occurrences (instances) in the text. The bestperforming methods are usually based on lexical association measures that exploit statistical evidence of word occurrences and co-occurrences acquired from a corpus to determine degree of lexical association between words (Pecina, 2005). Expressions that consist of words with high association are then 2 We do not aim at disambiguating the occurrences as figurative or literal. We have not observed enough literal uses to substantiate working on this step. There are bigger improvements to be gained from better identification of syntactic occurrences. 106 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 106–115, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics denoted as MWEs. Most of the current approaches are limited to bigrams despite the fact that higherorder MWEs"
W13-3208,P08-1028,0,0.0949181,"head and modifying constituent, respectively, with regards to a certain WSM. Function f stands for a combination of its parameters: 0.5xh + 0.5xm (avg), 0xh + 1xm (mOnly), 1xh + 0xm (hOnly), min(xh , xm ) (min), and max(xh , xm ) (max). CO The compositionality-based Measure compares the true co-occurrence vector of the examined expression and the vector obtained from the vectors corresponding to the constituents of the expression using some compositionality function (Reddy et al., 2011a). Commonly used compositionality functions are vector addition (⊕) and pointwise vector multiplication (⊗) (Mitchell and Lapata, 2008). The vectors expected to be different from each other are e.g. “hot dog” and “hot”⊕“dog”. Formally, dataset TrValD TestD AN 68 77 VO 68 62 SV 39 35 NN 45 45 Table 1: Numbers of expressions of all the different types from the DISCO and Reddy datasets. cco = s(ve , vh ∗ vm ) , where ve , vh , and vm stand for vectors of an examined expression, its head and modifying constituents, respectively. ∗ stands for a vector operation. WSM construction Since the DISCO and Reddy data were extracted from the ukWaC corpus (Baroni et al., 2009), we also build our WSMs from the same corpus. We use our own mod"
W13-3208,I11-1024,0,0.0904635,"s could also benefit from a set of non-compositional expressions. Specifically, WSMs could treat semantically non-compositional expressions as single units. As an example, consider “kick the bucket”, “hot dog”, or “zebra crossing”. Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents (“kick”, “bucket”, “dog” or “zebra”), but instead to the whole expressions. Recent works, including that of Lin (1999), Baldwin et al. (2003), Biemann and Giesbrecht (2011), Johannsen et al. (2011), Reddy et al. (2011a), Krˇcm´aˇr et al. (2012), and Krˇcm´aˇr et al. (2013), show the applicability of WSMs in determining the compositionality of word expressions. The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets. First, this leads to non-directly comparable results and second, many combinations of This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of w"
W13-3208,W11-1310,0,0.118386,"s could also benefit from a set of non-compositional expressions. Specifically, WSMs could treat semantically non-compositional expressions as single units. As an example, consider “kick the bucket”, “hot dog”, or “zebra crossing”. Treating such expressions as single units might improve the quality of WSMs since the neighboring words of these expressions should not be related to their constituents (“kick”, “bucket”, “dog” or “zebra”), but instead to the whole expressions. Recent works, including that of Lin (1999), Baldwin et al. (2003), Biemann and Giesbrecht (2011), Johannsen et al. (2011), Reddy et al. (2011a), Krˇcm´aˇr et al. (2012), and Krˇcm´aˇr et al. (2013), show the applicability of WSMs in determining the compositionality of word expressions. The proposed methods exploit various types of WSMs combined with various measures for determining the compositionality applied to various datasets. First, this leads to non-directly comparable results and second, many combinations of This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of w"
W13-3208,pearce-2002-comparative,0,0.0190737,"but the most frequent m columns reflecting the most common open-class words are discarded; COALS transforms weighted counts in the co-occurrence matrix in a special way (all the word pair correlations are calculated, negative values are set to 0, and non-negative ones are square rooted – corr); and optionally, Singular Value Decomposition (Deerwester et al., 1990) can be applied to the COALS co-occurrence matrix. SU The substitutability-based Measure is based on the fact that the replacement of noncompositional expressions’ constituents by the words similar to them leads to anti-collocations (Pearce, 2002). The compositionality of expressions is calculated as the ratio between the number of occurrences of the expression in a corpora and the sum of occurrences of its alternatives – possibly anti-collocations. In a similar way, we can compare pointwise mutual information scores (Lin, 1999). As an example, consider the possible occurrences of “hot dog” and “warm dog” in the corpora. Formally, adopted from Krˇcm´aˇr et al. (2012), we calculate the compositionality score csu for an examined expression as follows: PM PH m h j=1 W hh, aj i i=1 W hai , mi ∗ csu = , W hh, mi RI is described in Sahlgren"
W13-3208,W11-1304,0,\N,Missing
W13-3208,W03-1810,0,\N,Missing
W13-3208,W03-1812,0,\N,Missing
W13-3208,N10-1029,0,\N,Missing
W13-3208,W13-1006,1,\N,Missing
W13-3208,P10-4006,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W14-3326,D11-1033,0,0.426651,"ain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general dom"
W14-3326,2011.iwslt-evaluation.18,0,0.0458693,"ction 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et"
W14-3326,bojar-etal-2012-joy,1,0.843764,"Missing"
W14-3326,N13-1073,0,0.0271109,"s are trained on the monolingual data in the target language (constrained or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables"
W14-3326,C04-1114,0,0.358032,", 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provi"
W14-3326,E12-3006,0,0.0294557,"Missing"
W14-3326,2005.eamt-1.19,0,0.105464,"nd Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the stan"
W14-3326,2011.iwslt-papers.5,0,0.0956886,"aining phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general-domain texts provided as constrained data for the standard task (“general domain” here is used to denote data Statistical"
W14-3326,P10-2041,0,0.508413,") or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et al., 2005; Axelrod et al., 2011) or their combination (Mansour et al., 2011). Similar approaches to domain adaptation are also applied in other tasks, e.g., automatic speech recognition (Byrne et al., 2004). 2.2 3 Data description This section includes an overview of the parallel and monolingual data sources used to train our systems. Following the task specification, they are split into constrained and unconstrained sections. The constrained section includes medicaldomain data provided for this task (extracted by the provided scripts), and general"
W14-3326,W08-0320,0,0.122804,"paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; M"
W14-3326,W07-0733,0,0.0769909,"work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation mo"
W14-3326,P03-1021,0,0.0285359,"Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expected, the unconstrained systems outperform the constrained ones. Linear interpolation outperforms data concatenation quite reliably"
W14-3326,P07-2045,0,0.00503854,"ined or unconstrained, depending on the setting). The general-domain models are trained on the WMT News data. Compared to the approach of Moore and Lewis (2010) and Axelrod et al. (2011), we prune the model vocabulary more aggressively – we discard not only the singletons, but also all words with non-Latin characters, which helps clean the models from noise introduced by the automatic process of data acquisition by web crawling. 4.2 4.3 System details We compute word alignment on lowercase 4-character stems using fast align (Dyer et al., 2013). We create phrase tables using the Moses toolkit (Koehn et al., 2007) with standard settings. We train 5-gram language models on the target-side lowercase forms using SRILM. We use MERT (Och, 2003) to tune model weights in our systems on the development data provided for the task. The only difference between the system variants for query and summary translation is the tuning set. In both cases, we use the respective sets provided offcially for the shared task. Data combination 4.4 For both parallel and monolingual data, we obtain two data sets after applying the data selection: Results Tables 3 and 4 show case-insensitive BLEU scores of our systems.7 As expecte"
W14-3326,2011.mtsummit-plenaries.5,0,0.0420922,"xts of nonmedical patents in the PatTR collection. Parallel data The parallel data summary is presented in Table 1. The main sources of the medical-domain data for all the language pairs include the EMEA corpus (Tiedemann, 2009), the UMLS metathesaurus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12"
W14-3326,2005.mtsummit-papers.11,0,0.0172802,"and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distribution of the domain-specificity s"
W14-3326,C10-2124,0,0.432112,"arch queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt l"
W14-3326,W02-1405,0,0.131062,"ranslation of search queries and document summaries. Section 5 concludes the paper. 2 Related work To put our work in the context of other approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training co"
W14-3326,E12-1055,0,0.0150313,"n con unc unc concat interpol concat interpol cs→en 30.87±4.70 32.46±5.05 34.88±5.04 33.82±5.16 de→en 33.21±5.03 33.74±4.97 31.24±5.59 34.19±5.27 en→cs 23.25±4.85 21.56±4.80 22.61±4.91 23.93±5.16 en→de 17.72±4.75 16.90±4.39 19.13±5.66 15.87±11.31 en→fr 28.64±3.77 29.34±3.73 33.08±3.80 31.19±3.73 fr→en 35.56±4.94 35.28±5.26 36.73±4.88 40.25±5.14 Table 4: BLEU scores of query translations. each section and use linear interpolation to combine them into a single model. For language models, we use the SRILM linear interpolation feature (Stolcke, 2002). We interpolate phrase tables using Tmcombine (Sennrich, 2012). In both cases, the held-out set for minimizing the perplexity is the system development set. The two language models for sentence scoring are trained with a restricted vocabulary extracted from the in-domain training data as words occurring at least twice (singletons and other words are treated as out-of-vocabulary). In our experiments, we apply this technique to select both monolingual data for language models and parallel data for translation models. Selection of parallel data is based on the English side only. The in-domain models are trained on the monolingual data in the target language"
W14-3326,P13-1135,0,0.0121052,"rus of health and biomedical vocabularies and standards (U.S. National Library of Medicine, 2009), and bilingual titles of Wikipedia articles belonging to the categories identified to be medical domain. Additional medical-domain data comes from the MAREC patent collection: PatTR (W¨aschle and Riezler, 2012) available for DE–EN and FR–EN, and COPPA (Pouliquen and Mazenc, 2011) for FR–EN (only patents from the medical categories A61, C12N, and C12P are allowed in the constrained systems). The constrained general-domain data include three parallel corpora for all the language pairs: CommonCrawl (Smith et al., 2013), Europarl version 6 (Koehn, 2005), the News Commentary corpus (Callison-Burch et al., 2012). Further, the constrained data include CzEng (Bojar et al., 2012) for CS–EN and the UN corpus for FR–EN. For our unconstrained experiments, we also employ parallel data from the non-medical patents from the PatTR and COPPA collections (other categories than A61, C12N, and C12P). 4 https://www.hon.ch/ https://sites.google.com/site/ shareclefehealth/ 5 223 10 10 5 5 0 0 −5 −5 −10 −10 −15 −15 15 10 general 5 0 −5 −10 15 constrained 15 unconstrained medical unconstrained constrained 15 −15 Figure 1: Distri"
W14-3326,wu-wang-2004-improving-domain,0,0.0358057,"approaches, we first describe previous work on domain adaptation in Statistical Machine Translation (SMT), then focus specifically on SMT in the medical domain. 2.1 Domain adaptation of Statistical machine translation Many works on domain adaptation examine the usage of available in-domain data to directly improve in-domain performance of SMT. Some authors attempt to combine the predictions of two separate (in-domain and general-domain) translation models (Langlais, 2002; Sanchis-Trilles and Casacuberta, 2010; Bisazza et al., 2011; Nakov, 2008) or language models (Koehn and Schroeder, 2007). Wu and Wang (2004) use in-domain data to improve word alignment in the training phase. Carpuat et al. (2012) explore the possibility of using word sense disambiguation to discriminate between domains. Other approaches concentrate on the acquisition of larger in-domain corpora. Some of them exploit existing general-domain corpora by selecting data that resemble the properties of in-domain data (e.g., using cross-entropy), thus building a larger pseudo-in-domain training corpus. This technique is used to adapt language models (Eck et al., 2004b; Moore and Lewis, 2010) as well as translation models (Hildebrand et"
W14-3326,W12-3151,1,0.842853,"difference) in the FR–EN parallel data and FR monolingual data is illustrated in Figures 1 and 2, respectively.6 The scores (Y axis) are presented for each sentence in increasing order from left to right (X axis). The rest of the preprocessing procedure was applied to all the datasets mentioned above, both parallel and monolingual. The data were tokenized and normalized by converting or omitting some (mostly punctuation) characters. A set of language-dependent heuristics was applied in an attempt to restore and normalize the opening/closing quotation marks, i.e. convert ""quoted"" to “quoted” (Zeman, 2012). The motivation here is twofold: First, we hope that paired quotation marks could occasionally work as brackets and better denote parallel phrases for Moses; second, if Moses learns to output directed quotation marks, the subsequent detokenization will be easier. For all systems which translate from German, decompounding is employed to reduce source-side data sparsity. We used BananaSplit for this task (M¨uller and Gurevych, 2006). We perform all training and internal evaluation on lowercased data; we trained recasers to postprocess the final submissions. 6 For the medical domain, constrained"
W14-3326,W12-3102,0,\N,Missing
W14-3326,eck-etal-2004-language,0,\N,Missing
W14-3353,2006.amta-papers.25,0,0.0960506,"e corrected test sentence with the words weights. The bottom part of the figure shows computation of the unigram and bigram precisions. The first column contains the original translation n-grams, the second one the corrected n-grams, the third one the n-gram weights and the last one indicates whether a matching ngram is contained in the reference sentence. edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distance of sentences treated as bags of words. The Translation Edit Rate (TER) (Snover et al., 2006) is a number of edit operation needed to change the test sentence to the most similar reference sentence. In this case, the allowed editing operations are insertions, deletions and substitutions and also shifting words within a sentence. reference translation is usually available and therefore the BLEU scores are often underestimated. The main disadvantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Therefore, words which are inflectional variants of each other are treated as completely different words although their meaning is similar (e.g."
W14-3353,P10-2016,0,0.0375265,"Missing"
W14-3353,2009.mtsummit-papers.3,0,0.0330995,"ranslation is usually available and therefore the BLEU scores are often underestimated. The main disadvantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Therefore, words which are inflectional variants of each other are treated as completely different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the A different approach is used in TerrorCat (Fishel et al., 2012). It u"
W14-3353,W11-2107,0,0.047856,"different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the A different approach is used in TerrorCat (Fishel et al., 2012). It uses frequencies of automatically obtained translation error categories as base for machine-learned pairwise comparison of translation hypotheses. In the Workshop of Machine Translation (WMT) Metrics Task, several new MT metrics compete annually (Mach´acˇ ek and Bojar, 2013). In the comptetition, METEO"
W14-3353,W12-3105,0,0.0266321,"Missing"
W14-3353,2003.mtsummit-papers.32,0,0.0752214,"rs in inflection. Between the second and the third line, the matching with respect to the affix distance is shown. The fourth line contains the corrected test sentence with the words weights. The bottom part of the figure shows computation of the unigram and bigram precisions. The first column contains the original translation n-grams, the second one the corrected n-grams, the third one the n-gram weights and the last one indicates whether a matching ngram is contained in the reference sentence. edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distance of sentences treated as bags of words. The Translation Edit Rate (TER) (Snover et al., 2006) is a number of edit operation needed to change the test sentence to the most similar reference sentence. In this case, the allowed editing operations are insertions, deletions and substitutions and also shifting words within a sentence. reference translation is usually available and therefore the BLEU scores are often underestimated. The main disadvantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Th"
W14-3353,W13-2202,0,0.359335,"judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the A different approach is used in TerrorCat (Fishel et al., 2012). It uses frequencies of automatically obtained translation error categories as base for machine-learned pairwise comparison of translation hypotheses. In the Workshop of Machine Translation (WMT) Metrics Task, several new MT metrics compete annually (Mach´acˇ ek and Bojar, 2013). In the comptetition, METEOR and TerrorCat scored better that the other mentioned metrics. 410 Metric Description Pearson&apos;s correlation coeffitient 3 tBLEU is computed in in two steps. Similarly to the METEOR score, we first make a monolingual alignment between the reference and the test sentences and then apply an algorithm similar to the standard BLEU but with modified n-gram precisions. The monolingual alignment is computed as a minimum weighted maximum bipartite matching between words in a reference sentence and a translation sentence1 using the Munkres assignment algorithm (Munkres, 1957"
W14-3353,P03-1021,0,0.0267378,"et al., 2002) is an established and the most widely used automatic metric for evaluation of MT quality. It is computed as a harmonic mean of the n-gram precisions multiplied by the brevity penalty coefficient which ensures also high recall. Formally: ! 4 X 1 BLEU = BP · exp log pn , 4 Introduction Automatic evaluation of machine translation (MT) quality is an important part of the machine translation pipeline. The possibility to run an evaluation algorithm many times while training a system enables the system to be optimized with respect to such a metric (e.g., by Minimum Error Rate Training (Och, 2003)). By achieving a high correlation of the metric with human judgment, we expect the system performance to be optimized also with respect to the human perception of translation quality. In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU (Papineni et al., 2002) and designed to suit better when translation into morphologically richer languages. We aim to have a simple language independent metric that correlates with human judgment better than the standard BLEU. n=1 where BP is the brevity penaly defined as follows:  1 if c &gt; r r BP = , e1− c ot"
W14-3353,P02-1040,0,0.0973243,"ignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes. The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages. 1 2 Previous Work BLEU (Papineni et al., 2002) is an established and the most widely used automatic metric for evaluation of MT quality. It is computed as a harmonic mean of the n-gram precisions multiplied by the brevity penalty coefficient which ensures also high recall. Formally: ! 4 X 1 BLEU = BP · exp log pn , 4 Introduction Automatic evaluation of machine translation (MT) quality is an important part of the machine translation pipeline. The possibility to run an evaluation algorithm many times while training a system enables the system to be optimized with respect to such a metric (e.g., by Minimum Error Rate Training (Och, 2003))."
W16-2361,J92-4003,0,0.391299,"Missing"
W16-2361,W14-4012,0,0.287119,"Missing"
W16-2361,W11-2107,0,0.036755,"ways in which the image annotation were collected also lead to two sub-tasks. The first one is called Multimodal Translation and its goal is to generate a translation of an image caption to the target language given the caption in source language and the image itself. The second task is the Cross-Lingual Image Captioning. In this setting, the system is provided five captions in the source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual ca"
W16-2361,W16-3210,0,0.125555,"Missing"
W16-2361,P16-1154,0,0.16715,"es are extracted from the 4096-dimensional penultimate layer (fc7) of the VGG-16 Imagenet network Simonyan and Zisserman (2014) before applying non-linearity. We keep the weights of the convolutional network fixed during the training. We do not use attention over the image features, so the image information is fed to the network only via the initial state. We also try a system combination and add an encoder for the phrase-based output. The SMT encoder shares the vocabulary and word embeddings with the decoder. For the combination with SMT output, we experimented with the CopyNet architecture (Gu et al., 2016) and with encoding the sequence the way as in the APE task (see Section 3.2). Since neither of these variations seems to have any effect on the performance, we report only the results of the simple encoder combina650 Source Reference Moses 2 Errors: MMMT Gloss: CLC Gloss: Source Reference Moses MMMT CLC A group of men are loading cotton onto a truck Eine Gruppe von M¨annern l¨adt Baumwolle auf einen Lastwagen eine Gruppe von M¨annern l¨adt cotton auf einen Lkw untranslated “cotton” and capitalization of “LKW” Eine Gruppe von M¨annern l¨adt etwas auf einem Lkw. ::::: A group of men are loading"
W16-2361,D15-1293,0,0.0314596,"s’ ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago. The distributed representations of words, sentences and images can be understood as a kind of common data type for language and images within the models. This is then used in tasks like automatic image captioning (Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015) or in attempts to ground lexical semantics in vision (Kiela and Clark, 2015). Model Description We use the neural translation model with attention (Bahdanau et al., 2014) and extend it to include multiple encoders, see Figure 1 for an illustration. Each input sentence enters the system simultaneously in several representations xi . An encoder used for the i-th representation Xi = (x1i , . . . , xki ) of k words, each stored as a one-hot vector xji , is a bidirectional RNN implementing a function f (Xi ) = Hi = (h1i , . . . , hki ) (1) where the states hji are concatenations of the outputs of the forward and backward networks after processing the j-th token in the resp"
W16-2361,D15-1044,0,0.0336949,"es the architecture of the networks we have used. Section 3 summarizes related work on the task of automatic post-editing of machine translation output and describes our submission to the Workshop of Machine Translation (WMT) competition. In a similar fashion, Section 4 refers to the task of multimodal translation. Conclusions and ideas for further work are given in Section 5. Introduction 2 Neural sequence to sequence models are currently used for variety of tasks in Natural Language Processing including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), text summarization (Rush et al., 2015), natural language generation (Wen et al., 2015), and others. This was enabled by the capability of recurrent neural networks to model temporal structure in data, including the long-distance dependencies in case of gated networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). The deep learning models’ ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago. The distributed representations of words, senten"
W16-2361,P07-2045,1,0.0121854,"source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual captioning BLEU METEOR Moses baseline MM baseline 32.2 54.4 27.2 11.3 33.8 32.6 tuned Moses NMT NMT + Moses NMT + image NMT + Moses + image — ” —, submitted 36.8 37.1 36.5 34.0 37.3 31.9 57.4 54.6 54.3 51.6 55.2 49.6 12.3 13.6 13.7 13.3 13.6 13.0 35.0 34.6 35.1 34.4 34.9 33.5 9.1 22.7 24.6 14.0 25.3 38.5 39.3 31.6 captioning only 5 en captions 5 en captions + image — ” —, subm"
W16-2361,W07-0728,0,0.0666219,"LEU score. The system was able to deal very well with the frequent error of keeping a word from the source in the translated sentence. Although neural sequential models usually learn the basic output structure very quickly, in this case it made a lot of errors in pairing parentheses correctly. We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences. Related Work In the previous year’s competition (Bojar et al., 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007). There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013). Although the use of neural sequential model is very straightforward in this case, to the best of our knowledge, there have not been experiments with RNNs for this task. 3.2 method 4 Experiments & Results Multimodal Translation The goal of the multimodal translation task is to generate an image caption in a target language (German) given the image itself and one or more captions in the source"
W16-2361,W13-4703,0,0.0288696,"tructure very quickly, in this case it made a lot of errors in pairing parentheses correctly. We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences. Related Work In the previous year’s competition (Bojar et al., 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007). There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013). Although the use of neural sequential model is very straightforward in this case, to the best of our knowledge, there have not been experiments with RNNs for this task. 3.2 method 4 Experiments & Results Multimodal Translation The goal of the multimodal translation task is to generate an image caption in a target language (German) given the image itself and one or more captions in the source language (English). The input sentence is fed to our system in a form of multiple input sequences without explicitly telling which sentence is the source one and which one 648 Source Choose Uncached Refr"
W16-2361,2006.amta-papers.25,0,0.13055,"k, the organizers provided tokenized data from the IT domain (Turchi et al., 2016). The training data consist of 12,000 triplets of the source sentence, its automatic translation and a reference sentence. The reference sentences are manually post-edited automatic translations. Additional 1,000 sentences were provided for validation, and another 2,000 sentences for final evaluation. Throughout the paper, we report scores on the validation set; reference sentences for final evaluation were not released for obvious reasons. The performance of the systems is measured using Translation Error Rate (Snover et al., 2006) from the manually post-edited sentences. We thus call the score HTER. This means that the goal of the task is more to simulate manual post-editing, rather than to reconstruct the original unknown reference sentence. 3.1 HTER BLEU baseline edit operations edit operations+ .2481 .2438 .2436 62.29 62.70 62.62 Table 1: Results of experiments on the APE task on the validation data. The ‘+’ sign indicates the additional regular-expression rules – the system that has been submitted. is the MT output. It is up to the network to discover their best use when producing the (single) target sequence. The"
W16-2361,P02-1040,0,0.0965578,"ages and a test set with 1,000 images. The two ways in which the image annotation were collected also lead to two sub-tasks. The first one is called Multimodal Translation and its goal is to generate a translation of an image caption to the target language given the caption in source language and the image itself. The second task is the Cross-Lingual Image Captioning. In this setting, the system is provided five captions in the source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system"
W16-2361,2014.amta-researchers.3,0,0.359423,"he image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions. 4.2 Phrase-Based System For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word 649 system Multimodal translation BLEU METEOR Cross-lingual captioning BLEU METEOR Moses baseline MM baseline 32.2 54.4 27.2 11.3 33.8 32.6 tuned Moses NMT NMT + Moses NMT + image NMT + Moses + image — ” —, submitted 36.8 37.1 36.5 34.0 37.3 31.9 57.4 54.6 54.3 51.6 55.2 49.6 12.3 13.6 13.7 13.3 13.6 13.0 35.0 34.6 35.1 34.4 34.9 33.5 9.1 22.7 24.6 14.0 25.3 38.5 39.3 31.6 captioning only 5 en captions 5 en captions + image — ” —, submitted Table 2: Results of experiments with the multimodal translation task on the validation data. At the time of"
W16-2361,P16-5005,0,0.0205225,"is a distribution estimated as  T m αm i = softmax v · tanh(s + WHi Hi ) (3) with sm being the hidden state of the decoder in time m. Vector v and matrix WHi are learned parameters for projecting the encoder states. The probability of the decoder emitting the word ym in the j-th step, denoted as P (ym |H1 , . . . , Hn , Y0..m−1 ), is proportional to ! n X exp Wo sj + (4) Wai aji We experimented with recently published improvements of neural sequence to sequence learning: scheduled sampling (Bengio et al., 2015), noisy activation function (G¨ulc¸ehre et al., 2016), linguistic coverage model (Tu et al., 2016). None of them were able to improve the systems’ performance, so we do not include them in our submissions. i=1 where Hi are hidden states from the i-th encoder and Y0..m−1 is the already decoded target sentence (represented as matrix, one-hot vector for each produced word). Matrices Wo and Wai are learned parameters; Wo determines the recurrent dependence on the decoder’s state and Wai determine the dependence on the (attention-weighted) encoders’ states. For image captioning, we do not use the attention model because of its high computational demands and rely on the basic model by Vinyals Si"
W16-2361,D15-1199,0,0.0540366,"Missing"
W16-2361,W15-3001,1,\N,Missing
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W19-5337,W19-5301,1,0.839454,"Missing"
W19-5337,N13-1073,0,0.0558634,"ed T2T system achieved an insignificant improvement (+0.1 BLEU) over our last year’s sentence-level T2T system, but applying this system on sentences led to a significant worsening (−0.6 BLEU). We hypothesized that by providing the translation model with larger attendable context, the resulting translations display larger lexical consistency. We could demonstrate it by finding less examples where an English polysemous word is translated to two or more Czech non-synonymous lemmata within one document. To evaluate the hypothesis, we word-aligned the source and target sentences using fast_align (Dyer et al., 2013).5 We then lemmatized the aligned words (both English and Czech) using MorphoDiTa (Straková et al., 2014) and considered all instances where a single English lemma was aligned to at least two Czech lemmata in a single document. Since our focus was on evaluating the difference between non-context and document-level models, we selected only the English lemmata with different number of aligned Czech lemmata in the two types of systems. Two pairs of models were compared: “DocTransformer T2T” vs. “Transformer T2T 2019” and “DocTransformer Marian” vs. “Transformer Marian”. The final pool of examples"
W19-5337,P18-4020,0,0.0759104,"Missing"
W19-5337,D18-2012,0,0.0320416,"mup 20000 --lr-decay-inv-sqrt 20000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --label-smoothing 0.1 --learn-rate 0.0002 --exponential-smoothing 3 Document-Level Systems Our document-level models were created by training on the context-augmented data described in Section 2.2. We used different strategies for document-level decoding in Marian and in T2T. We used the same learning rate as T2T and estimated the number of warmup training steps so the model consumed approximately the same number of sentences as T2T in warmup. Instead of T2T’s default SubwordTextEncoder, we used SentencePiece (Kudo and Richardson, 2018) with its default parameters to obtain a shared vocabulary of 32,000 entries from untokenized training data. We set the maximal sentence length to 150 and decoded with beam size 4. We could not use Adafactor (Shazeer and Stern, 2018) optimizer as in T2T, because it is not implemented in Marian. We used Adam instead. We did not set the batch size manually, but used the --mini-batch-fit parameter to determine the mini-batch size automatically based on sentence lengths to fit the available memory. We estimated the workspace memory to 13,900 MB as the largest possible on our hardware. We shuffled"
W19-5337,D18-1512,0,0.0561563,"Missing"
W19-5337,W18-6424,1,0.812702,"coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence. 1 Since assessing the performance of documentlevel systems is one of the goals of WMT19 (Barrault et al., 2019), we decided to build NMT systems trained for translation of longer segments than single sentences. In this paper, we describe our five NMT systems submitted to WMT19 English→Czech news translation task (see Table 1). They are based on the Transformer model (Vaswani et al., 2017) and on our submission from WMT18 (Popel, 2018). Our new contributions are (i) adaptation of the baseline single-sentence models to translate multiple adjacent sentences in a document at once, so the Transformer can attend to inter-sentence relations and achieve better document-level translation quality, as was already showed to be effective by Jean et al. (2017); and (ii) reimplementation of our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the qual"
W19-5337,W18-6319,0,0.0236236,"6 29.20 28.13 0.5474 29.00 27.89 0.5516 Table 4: Automatic evaluation on newstest2019. Significantly different BLEU scores (p < 0.05 bootstrap resampling) are separated by a horizontal line. „lower and upper“. This is considered as standard in Czech formal texts. For Marian, we applied only the conversion of quotation symbols. 4 Results 4.1 Automatic Evaluation Table 4 reports the automatic metrics of our English→Czech systems submitted to WMT2019, plus the best other system – UEdin (Marian system trained by University of Edinburgh). The automatic metrics are calculated using sacreBLEU 1.3.2 (Post, 2018) and their signatures are: • pre-context: sentences which are ignored in the translation and serve only as a context for better translation of the main content, • BLEU+case.mixed+lang.encs+numrefs.1+smooth.exp+tok.13a, • main content: sentences which are used for the final translation, • BLEU+case.lc+lang.encs+numrefs.1+smooth.exp+tok.intl and • post-context: sentences which are ignored, similarly to the pre-context. • chrF2+case.mixed+lang.encs+numchars.6+numrefs.1+space.False. Based on a small dev-set BLEU hyperparameter search, we selected the following length limits: pre-context of up to 2"
W19-5337,P14-5003,0,0.0292515,"Missing"
W19-5337,W18-6312,0,0.0177422,"f our last year’s submission in the Marian framework (Junczys-Dowmunt et al., 2018). Introduction Neural machine translation has reached a point, where the quality of automatic translation measured on isolated sentences is similar on average to the quality of professional human translations. Hassan et al. (2018) report achieving a “human parity” on Chinese→English news translation. Bojar et al. (2018, p. 291) report that our last year’s English→Czech system (Popel, 2018) was evaluated as significantly better (p < 0.05) than the human reference. However, it has been shown (Läubli et al., 2018; Toral et al., 2018) that evaluating the quality of translation of news articles on isolated sentences without the context of the whole document is not sufficient. It can bias the evaluation results because systems that ignore the context are not penalized in the evaluation for these context-related errors; and vice versa: sysThis paper is organized as follows: In Section 2, we describe our training data and its augmentation to overlapping multi-sentence sequences. We describe also the hyper-parameters of our models in the two frameworks. Section 3 follows with a description of the document-level decoding strateg"
W19-5337,W18-1819,0,0.0615439,"use only the data allowed in WMT2018, which does not include CS NewsCrawl 2018 and WikiTitles. All the data were preprocessed, filtered and backtranslated by the same process as in Popel (2018). We selected the originally English part of newstest2016 for validation, following the idea of CZ/nonCZ tuning in Popel (2018), but excluding the CZ tuning because the WMT2019 test set was announced to contain only original English sentences and no translationese. 2.2 2.3 2.3.1 Model Hyper-parameters Tensor2Tensor Our three systems with “T2T” in the name are implemented in the Tensor2Tensor framework (Vaswani et al., 2018), version 1.6.0. The model and training parameters this year are identical to our last year’s (WMT18) submission (Popel, 2018), with just two exceptions: First, we trained on 10 GPUs instead of 8 GPUs, thus using the effective batch size of 29k subwords instead of 23k subwords. Second, we used max_length=200 instead of 150. This means we discard all training sequences longer than 200 subwords. With our 32k joint subword vocabulary, a word contains on average 1.5 subwords. Thus effectively, the sequence-length limit used in T2T training was in most cases lower than 1000 characters – on average"
W19-5337,W18-6401,1,\N,Missing
