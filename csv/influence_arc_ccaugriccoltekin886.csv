2020.lrec-1.688,W19-4409,0,0.013197,"urse cohesion (see Zesch et al. (2015) for an overview of features in the litera5604 ture). The emergence of Neural Networks and Deep Learning has also prompted an appearance of a body of work that uses deep learning to automatically score essays (Alikaniotis et al., 2019; Taghipour and Tou Ng, 2016; Nadeem et al., 2019). For instance, Alikaniotis et al. (2019) use score-specific word embeddings for which they use pretrained embeddings and further train them on predicting essay scores. A common question raised in the literature is whether to treat AES as a regression or a classification task. Berggren et al. (2019) tackle this issue by experimenting with both regression and classification and also non-neural and neural models for Norwegian Essay Scoring. Both corpora used in this paper have previously been applied for AES. Weiß (2017) demonstrated the power of complexity features to predict CEFR levels of German essays using the MERLIN corpus. Yannakoudakis et al. (2011), in the paper introducing the CLC corpus, also present AES experiments in which they consider the task as a rank preference learning problem. They use features such as phrase structure rules and error rate to predict essay scores. 2.3."
2020.lrec-1.688,boyd-etal-2014-merlin,0,0.15659,"//pearsonpte.com/wp-content/uploads/2015/05/7.-PTEA_ Automated_Scoring.pdf). 2 to do with the fact that AES is a text classification (or regression) task, which is a well-studied and straightforward NLP task. This allows us to focus less on the specifics of the systems and the original task, and more on issues regarding reproducibility. The target paper we attempt to reproduce by Vajjala and Rama (2018) presents a series of models for predicting the Common European Framework of Reference (CEFR) levels of essays written by German, Czech and Italian learners. The authors use the MERLIN corpus (Boyd et al., 2014) for the task, and present multilingual and cross-lingual models along with monolingual models. We present three sets of experiments in this paper. First, we use their publicly-available code to replicate their results. The second set of experiments are performed using the same dataset, the same machine learning models and the same features, but we use our own implementation, and perform additional tuning of the model hyperparameters. And third, we test whether their features can be used to successfully predict essay scores of a fourth language, namely English. We use the Cambridge Learner Cor"
2020.lrec-1.688,Q17-1033,0,0.0237115,"bject in the NLP community. Most early reproduction or replication studies in NLP is concerned with subfields whose applications may have high potential impact, such as biomedical NLP (Névéol et al., 2016; Cohen et al., 2018). A number of studies include surveys of papers in prominent NLP conferences for quantifying the properties related to reproduction and replication, such as release of the data and code, actual availability of the resources used, authors’ willingness to share their code if it is not already available, or whether the reported results are statistically sound (Mieskes, 2017; Dror et al., 2017; Wieling et al., 2018). There has also been a number of interesting papers with case studies. Fokkens et al. (2013) focus on (a lack of) system descriptions (e.g., preprocessing, versions of the resources used) that cause reproduction attempts to fail. They further note that replication attempts are also useful for gaining further insight into the original problem and the solution. Cohen et al. (2018) also includes three case studies, which are analyzed carefully according to dimensions summarized above. Moore and Rayson (2018) perform reproduction experiments of over 10 sentiment analysis me"
2020.lrec-1.688,P13-1166,0,0.0805033,"Missing"
2020.lrec-1.688,W17-1603,0,0.0216337,"erest in the subject in the NLP community. Most early reproduction or replication studies in NLP is concerned with subfields whose applications may have high potential impact, such as biomedical NLP (Névéol et al., 2016; Cohen et al., 2018). A number of studies include surveys of papers in prominent NLP conferences for quantifying the properties related to reproduction and replication, such as release of the data and code, actual availability of the resources used, authors’ willingness to share their code if it is not already available, or whether the reported results are statistically sound (Mieskes, 2017; Dror et al., 2017; Wieling et al., 2018). There has also been a number of interesting papers with case studies. Fokkens et al. (2013) focus on (a lack of) system descriptions (e.g., preprocessing, versions of the resources used) that cause reproduction attempts to fail. They further note that replication attempts are also useful for gaining further insight into the original problem and the solution. Cohen et al. (2018) also includes three case studies, which are analyzed carefully according to dimensions summarized above. Moore and Rayson (2018) perform reproduction experiments of over 10 se"
2020.lrec-1.688,C18-1097,0,0.0240347,"or whether the reported results are statistically sound (Mieskes, 2017; Dror et al., 2017; Wieling et al., 2018). There has also been a number of interesting papers with case studies. Fokkens et al. (2013) focus on (a lack of) system descriptions (e.g., preprocessing, versions of the resources used) that cause reproduction attempts to fail. They further note that replication attempts are also useful for gaining further insight into the original problem and the solution. Cohen et al. (2018) also includes three case studies, which are analyzed carefully according to dimensions summarized above. Moore and Rayson (2018) perform reproduction experiments of over 10 sentiment analysis methods. Their focus is on the reproduction of results on multiple datasets, emphasizing the use of as many datasets as possible in evaluation of NLP systems, which is in line with the other studies where careful, less biased data is shown to change the conclusions of earlier reports (Pirina and Çöltekin, 2018). 10 case studies reported by Wieling et al. (2018) involve only replication. The authors tested whether the code released by earlier studies could be run in a limited ‘human time’ and, if so, whether the values output by th"
2020.lrec-1.688,W19-4450,0,0.0145372,"Attali and Burstein, February 2006) and Intelligent Essay Assessor (Foltz et al., 1999) based on Latent Semantic Analysis (Deerwester et al., 1990). A wide range of features have been developed to analyse essays, from as simple as document length to more complex ones involving, for instance, discourse cohesion (see Zesch et al. (2015) for an overview of features in the litera5604 ture). The emergence of Neural Networks and Deep Learning has also prompted an appearance of a body of work that uses deep learning to automatically score essays (Alikaniotis et al., 2019; Taghipour and Tou Ng, 2016; Nadeem et al., 2019). For instance, Alikaniotis et al. (2019) use score-specific word embeddings for which they use pretrained embeddings and further train them on predicting essay scores. A common question raised in the literature is whether to treat AES as a regression or a classification task. Berggren et al. (2019) tackle this issue by experimenting with both regression and classification and also non-neural and neural models for Norwegian Essay Scoring. Both corpora used in this paper have previously been applied for AES. Weiß (2017) demonstrated the power of complexity features to predict CEFR levels of Ger"
2020.lrec-1.688,W16-6110,0,0.0207928,"ps2019/) and ICLR (https://reproducibility-challenge.github.io/iclr_2019/). are verified with a different experimental setting. However, there are some cases where replication experiments are useful, especially when it means that one shares the code and data used in the experiments. Although it is not always clear what is exactly being reproduced (or replicated), there is clearly an increasing interest in the subject in the NLP community. Most early reproduction or replication studies in NLP is concerned with subfields whose applications may have high potential impact, such as biomedical NLP (Névéol et al., 2016; Cohen et al., 2018). A number of studies include surveys of papers in prominent NLP conferences for quantifying the properties related to reproduction and replication, such as release of the data and code, actual availability of the resources used, authors’ willingness to share their code if it is not already available, or whether the reported results are statistically sound (Mieskes, 2017; Dror et al., 2017; Wieling et al., 2018). There has also been a number of interesting papers with case studies. Fokkens et al. (2013) focus on (a lack of) system descriptions (e.g., preprocessing, version"
2020.lrec-1.688,C67-1032,0,0.284123,"The replication or reproduction attempts listed above report mostly negative results, mirroring the results reported in other fields (Open Science Collaboration, 2015, for example). As a result, it is clear that there is need for mechanisms or guidelines to increase the confirmability of the studies in the field, as well as more reproduction studies and further discussion on fruitful ways to perform replication and reproduction studies. 2.2. Automated Essay Scoring Automated essay scoring, also called automatic text scoring, goes back to the 60s when Ellis Page developed Project Essay Grader (Page, 1967), A number of AES schemes have emerged since then, some of the most prominent in the field being e-rater (Attali and Burstein, February 2006) and Intelligent Essay Assessor (Foltz et al., 1999) based on Latent Semantic Analysis (Deerwester et al., 1990). A wide range of features have been developed to analyse essays, from as simple as document length to more complex ones involving, for instance, discourse cohesion (see Zesch et al. (2015) for an overview of features in the litera5604 ture). The emergence of Neural Networks and Deep Learning has also prompted an appearance of a body of work tha"
2020.lrec-1.688,W18-5903,1,0.747627,"tion attempts are also useful for gaining further insight into the original problem and the solution. Cohen et al. (2018) also includes three case studies, which are analyzed carefully according to dimensions summarized above. Moore and Rayson (2018) perform reproduction experiments of over 10 sentiment analysis methods. Their focus is on the reproduction of results on multiple datasets, emphasizing the use of as many datasets as possible in evaluation of NLP systems, which is in line with the other studies where careful, less biased data is shown to change the conclusions of earlier reports (Pirina and Çöltekin, 2018). 10 case studies reported by Wieling et al. (2018) involve only replication. The authors tested whether the code released by earlier studies could be run in a limited ‘human time’ and, if so, whether the values output by the software are the same as the ones reported in the original papers. The replication or reproduction attempts listed above report mostly negative results, mirroring the results reported in other fields (Open Science Collaboration, 2015, for example). As a result, it is clear that there is need for mechanisms or guidelines to increase the confirmability of the studies in the"
2020.lrec-1.688,W17-5028,1,0.82041,"s our earlier results for monolingual and multilingual models were presented. The scores in Table 4 do not present any surprises. The scores are reasonably close to the reported values. Similar to the original findings, the transfer seems to work better for Italian than Czech, and as expected the scores are lower than the corresponding monolingual models. 4. Reproduction: Same Data, Different Code In this section, we report our first reproduction results, where our experiments differ from theirs in the text classification software employed. The software we use is based on our earlier studies (Rama and Çöltekin, 2017; Çöltekin and Rama, 2018; Wu et al., 2019). Here, we only use traditional classifiers used by Vajjala and Rama (2018), namely, support vector machines (SVMs), logistic regression, and random forests. Since we use the same underlying library (Pedregosa et al., 2011), our experiments should arguably not deviate strongly from theirs. The main difference in our implementation is that we optimise parameters. Although Vajjala and Rama (2018) try a few alternative classification methods, each model’s parameters are set to library defaults. For the results presented here, we tune each model using ran"
2020.lrec-1.688,L16-1680,0,0.188274,"ution of essays in the MERLIN corpus. An identical table can be found in Vajjala and Rama (2018, Table 1). 1. word n-grams and POS n-grams where n is 1 to 5; 2. dependency n-grams where n is 1 to 3; 3. domain features which consist of features that are specific to AES research. These include document length, lexical richness features and error features (see their paper for a more detailed description); 4. combined features where domain features are concatenated with each of the three n-gram features. The texts were POS-tagged and automatically annotated with dependency relations using UDPipe (Straka et al., 2016). The annotated files are readily available in their git repository. They combine the dense domain features and the sparse n-gram features by first training a classifier on the sparse features to get the probability distribution of CEFR classes for each essay. As a second step, they use those probability distributions as features together with the dense features to train the final classifier. The authors train and test three ‘traditional’ classifiers, random forests, linear support vector machines (SVMs) and logistic regression, as well as a multi-layer perceptron (MLP) with word embeddings tr"
2020.lrec-1.688,D16-1193,0,0.0192469,"Missing"
2020.lrec-1.688,W18-0515,0,0.0617658,"2019). As a field that heavily relies on experimental work, the same concerns apply to computational linguistics and natural language processing (NLP), and there have been recent efforts to understand the extent of the problem and identify potential solutions. The present work is conducted in the context of such an effort, REPROLANG 2020 shared task on reproducibility of results in computational studies of language.1 Our study, in particular, is concerned with the reproduction of a study of automatic assay scoring (AES) for determining language proficiency levels of second language learners (Vajjala and Rama, 2018). In AES, the aim is to assign a score, mark or level to a text by means of an automatic system. The motivation behind the development of such systems lies in the time, cost and reliability that are involved in manual essay correction marking (Dikli, 2006). AES is one of the NLP applications that could find its way into real life applications, and hence, have a high potential impact on the society. For example, some of these systems are currently being used in high stake examinations.2 Since the quality of such high-impact applications is an important concern, reproducibility is especially des"
2020.lrec-1.688,J18-4003,0,0.034235,"Missing"
2020.lrec-1.688,W19-1406,1,0.836401,"gual models were presented. The scores in Table 4 do not present any surprises. The scores are reasonably close to the reported values. Similar to the original findings, the transfer seems to work better for Italian than Czech, and as expected the scores are lower than the corresponding monolingual models. 4. Reproduction: Same Data, Different Code In this section, we report our first reproduction results, where our experiments differ from theirs in the text classification software employed. The software we use is based on our earlier studies (Rama and Çöltekin, 2017; Çöltekin and Rama, 2018; Wu et al., 2019). Here, we only use traditional classifiers used by Vajjala and Rama (2018), namely, support vector machines (SVMs), logistic regression, and random forests. Since we use the same underlying library (Pedregosa et al., 2011), our experiments should arguably not deviate strongly from theirs. The main difference in our implementation is that we optimise parameters. Although Vajjala and Rama (2018) try a few alternative classification methods, each model’s parameters are set to library defaults. For the results presented here, we tune each model using random search. The hyperparamters tuned for al"
2020.lrec-1.688,P11-1019,0,0.263724,"ual models. We present three sets of experiments in this paper. First, we use their publicly-available code to replicate their results. The second set of experiments are performed using the same dataset, the same machine learning models and the same features, but we use our own implementation, and perform additional tuning of the model hyperparameters. And third, we test whether their features can be used to successfully predict essay scores of a fourth language, namely English. We use the Cambridge Learner Corpus (CLC) which includes texts that were written as part of a First Cambridge exam (Yannakoudakis et al., 2011). The first two sets of experiments verify the reproducibility of the original results, while the additional dataset allows for testing whether the model is general enough to be extended to another language with a different label granularity. This paper is structured as follows. In Section 2., we give some relevant pointers to existing literature on reproducibility in NLP. Additionally, a brief overview of previous work on AES, and a brief summary of the target paper is given. Section 3. includes our reproduction experiments of Vajjala and Rama (2018) where we discuss our results in comparison"
2020.lrec-1.758,S19-2007,0,0.456142,"uage. For example, it can be useful in moderation of social media platforms, or more generally, on Internet sites allowing user content; it may be used by law enforcement agencies for detecting unlawful content; it may facilitate study of psychological effects of abusive language; and it could be useful for parents for preventing children from being exposed to such content. The recent interest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kumar et al., 2018b). This paper presents a corpus of Turkish offensive l"
2020.lrec-1.758,W18-3906,1,0.931793,"l., 2015; Sanguinetti et al., 2018; Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Waseem, 2016). Although there is no clear definition of hate speech, it typically covers offensive language targeting a group (or sometimes a person) based on features such as race, ethnicity, gender, sexual orientation, socio-economic class, political affiliation or religion. The motivation is generally based on practical concerns, as hate speech is illegal under some jurisdictions, and there has been recent attempts to actively counteract online hate speech (European Commission, 2018, see also Article 19 (2018) for a discussion of effects of hate speech prevention mandates on freedom of speech). Some of these studies further narrow the scope down to a specific target, commonly race (Basile et al., 2019; Kwok and Wang, 2013), 3 According to Twitter transparency report (Twitter, 2019), Turkey also has the highest rate of content removal requests by a country on Twitter. women (Basile et al., 2019; Fersini et al., 2018b; Fersini et al., 2018a) refugees (Ross et al., 2017), hate speech with a particular ideology (Jaki and De Smedt, 2018), or even hate speech related to a single significant event (Burnap"
2020.lrec-1.758,gao-huang-2017-detecting,0,0.109401,"d tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kumar et al., 2018b). This paper presents a corpus of Turkish offensive language on the social media platform Twitter, and initial results on automatic identification of offensive language on this corpus. Turkish is a language with relatively large number of speakers.1 It is mainly spoken in Turkey, but sizable communities of native speakers live also in other countries in1 Approximately 90 million L1 and L2 speakers according to https://en.wikipedia.org/wiki/Turkish_language [accessed: 25 November 2019]. cluding Germany, some Balkan countries and Cyprus. The language"
2020.lrec-1.758,W18-4401,0,0.659335,"efit from successful automatic identification of inappropriate language. For example, it can be useful in moderation of social media platforms, or more generally, on Internet sites allowing user content; it may be used by law enforcement agencies for detecting unlawful content; it may facilitate study of psychological effects of abusive language; and it could be useful for parents for preventing children from being exposed to such content. The recent interest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kuma"
2020.lrec-1.758,L18-1226,0,0.316802,"efit from successful automatic identification of inappropriate language. For example, it can be useful in moderation of social media platforms, or more generally, on Internet sites allowing user content; it may be used by law enforcement agencies for detecting unlawful content; it may facilitate study of psychological effects of abusive language; and it could be useful for parents for preventing children from being exposed to such content. The recent interest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kuma"
2020.lrec-1.758,S19-2011,0,0.359291,"Missing"
2020.lrec-1.758,L18-1585,0,0.0367742,"Missing"
2020.lrec-1.758,W17-3008,0,0.221077,"fer online communication for children (Chen et al., 2012). Although the applications are different, there is a considerable overlap. For example, cyberbullying often employs expressions and statements that are considered hate speech. Furthermore, both the linguistic properties of texts, and the methods to detect them are similar. As a result, some recent the studies use annotations schemes that cover a broad spectrum of offensive, abusive or aggressive language (ÁlvarezCarmona et al., 2018; Álvarez-Carmona et al., 2018; Djuric et al., 2015; Kumar et al., 2018b; Mojica de la Vega and Ng, 2018; Mubarak et al., 2017; Nobata et al., 2016; Spertus, 1997; Zampieri et al., 2019a). Sometimes the term trolling is used for a subset of online uses of offensive language. A point raised frequently in many recent studies is the lack of consensus on the definition of offensive language and its subcategories, and, as a result, the incompatibility of annotations in different corpora. There are attempts to provide clear definitions and taxonomies for (online) offensive language (Waseem et al., 2017; Ruppenhofer et al., 2018). A common trend is to use a set of classes based on the target of the offensive language, (Wieg"
2020.lrec-1.758,S19-2123,0,0.0684088,"ermEval 2018 (three-way classification: profanity, abuse or insult). Figure 1: Distribution of tweets normalized by population. The locations are based on the location names indicated in Twitter users’ profiles. The graph is based on 16 860 tweets for which a location name in Turkey can be identified. Locations outside Turkey and ambiguous or unidentifiable location declarations are ignored. The figure is created using Gabmap (Nerbonne et al., 2011; Leinonen et al., 2015). Best performing systems in general use external resources, such as pre-trained (contextual) embeddings (Liu et al., 2019; Nikolov and Radivchev, 2019; Montani and Schüller, 2018) or pre-training or classification/clustering results on auxiliary tasks on large data sets, such as sentiment analysis or emoji classification (Wiedemann et al., 2018). 3. Data 3.1. Data Collection The data was collected from Twitter using Twitter streaming API. As it is a common practice, the stream was filtered based on a list of frequent words in Turkish tweets and by Twitter’s language identification mechanism. The data collection covers a wide time span from March 2018 to September 2019, with a gap of two weeks during November 2018. We obtained approximately"
2020.lrec-1.758,I13-1066,0,0.01701,"ding to Twitter transparency report (Twitter, 2019), Turkey also has the highest rate of content removal requests by a country on Twitter. women (Basile et al., 2019; Fersini et al., 2018b; Fersini et al., 2018a) refugees (Ross et al., 2017), hate speech with a particular ideology (Jaki and De Smedt, 2018), or even hate speech related to a single significant event (Burnap and Williams, 2014). Another common sub-area of offensive language whose automatic detection has potential practical applications is detection of cyberbullying (Dadvar et al., 2013; Dadvar et al., 2014; Dinakar et al., 2012; Nitta et al., 2013; Van Hee et al., 2015; Xu et al., 2012). Unlike hate speech, target of cyberbullying is generally a single person, often a child. Bullying, and its online version cyberbullying, is considered as a serious health issue (American Psychological Association, 2004; Smith et al., 2009). Hence, a typical application of automatic detection of cyberbullying is providing safer online communication for children (Chen et al., 2012). Although the applications are different, there is a considerable overlap. For example, cyberbullying often employs expressions and statements that are considered hate speech."
2020.lrec-1.758,W17-5028,1,0.895243,"Missing"
2020.lrec-1.758,L18-1443,0,0.290259,"e a review focusing on studies reporting collection and annotation of corpora, discussing rather few of the systems or methods for automatically identifying examples of such language. Most corpus collection studies in the earlier literature annotate a particular form of offensive language, and, often, they are intended for use in a particular application. By far, the most common application is hate speech detection (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017; Davidson et al., 2017; Del Vigna et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Gitari et al., 2015; Sanguinetti et al., 2018; Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Waseem, 2016). Although there is no clear definition of hate speech, it typically covers offensive language targeting a group (or sometimes a person) based on features such as race, ethnicity, gender, sexual orientation, socio-economic class, political affiliation or religion. The motivation is generally based on practical concerns, as hate speech is illegal under some jurisdictions, and there has been recent attempts to actively counteract online hate speech (European Commission, 2018, see also Article 19 (2018) for a discussion of effects"
2020.lrec-1.758,W17-1101,0,0.124288,"fectly fit into a practical purpose. However, an offense targeted to a group is likely to be an instance of hate speech, and cyberbullying involves offensive language toward one or more individuals. Furthermore, the more general nature of the annotation scheme is likely to be more suitable for studying properties of offensive language samples from a linguistic or sociological perspective. The above scheme does not cover all aspects of the offensive language one may be interested to study or to annotate. Earlier studies investigated other aspects or dimensions of (forms of) offensive language (Schmidt and Wiegand, 2017). In our initial experiments, similar to Basile et al. (2019), we also tried annotating the offensive statements for their ‘aggressiveness’. However, the number of tweets that contained aggression was rather low with very low levels of inter annotator agreement. Hence we decided not to use this dimension in our final annotation scheme. Even without an indication of aggression, not all offensive statements are equal. An offensive statement may range from (unfriendly) teasing to a clearly aggressive form of offensive language. Similar to some earlier studies (Sanguinetti et al., 2018), we also f"
2020.lrec-1.758,R15-1086,0,0.0555649,"Missing"
2020.lrec-1.758,W12-2103,0,0.0453155,"dies reporting collection and annotation of corpora, discussing rather few of the systems or methods for automatically identifying examples of such language. Most corpus collection studies in the earlier literature annotate a particular form of offensive language, and, often, they are intended for use in a particular application. By far, the most common application is hate speech detection (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017; Davidson et al., 2017; Del Vigna et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Gitari et al., 2015; Sanguinetti et al., 2018; Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Waseem, 2016). Although there is no clear definition of hate speech, it typically covers offensive language targeting a group (or sometimes a person) based on features such as race, ethnicity, gender, sexual orientation, socio-economic class, political affiliation or religion. The motivation is generally based on practical concerns, as hate speech is illegal under some jurisdictions, and there has been recent attempts to actively counteract online hate speech (European Commission, 2018, see also Article 19 (2018) for a discussion of effects of hate speech prevention ma"
2020.lrec-1.758,N16-2013,0,0.22142,"erest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kumar et al., 2018b). This paper presents a corpus of Turkish offensive language on the social media platform Twitter, and initial results on automatic identification of offensive language on this corpus. Turkish is a language with relatively large number of speakers.1 It is mainly spoken in Turkey, but sizable communities of native speakers live also in other countries in1 Approximately 90 million L1 and L2 speakers according to https://en.wikipedia.org/wiki/T"
2020.lrec-1.758,W17-3012,0,0.207452,"ona et al., 2018; Álvarez-Carmona et al., 2018; Djuric et al., 2015; Kumar et al., 2018b; Mojica de la Vega and Ng, 2018; Mubarak et al., 2017; Nobata et al., 2016; Spertus, 1997; Zampieri et al., 2019a). Sometimes the term trolling is used for a subset of online uses of offensive language. A point raised frequently in many recent studies is the lack of consensus on the definition of offensive language and its subcategories, and, as a result, the incompatibility of annotations in different corpora. There are attempts to provide clear definitions and taxonomies for (online) offensive language (Waseem et al., 2017; Ruppenhofer et al., 2018). A common trend is to use a set of classes based on the target of the offensive language, (Wiegand et al., 2018; Struß et al., 2019; Zampieri et al., 2019a). Particularly, if the target is an individual (or a number of loosely related individuals), or a group of people based on their race, gender, political/ideological affiliation, religion or a similar property. The former target category often includes acts of cyberbullying, while the latter is likely to be an instance of hate speech. Zampieri et al. (2019a) also include an ‘other’ category, where the target is no"
2020.lrec-1.758,W16-5618,0,0.0486516,"discussing rather few of the systems or methods for automatically identifying examples of such language. Most corpus collection studies in the earlier literature annotate a particular form of offensive language, and, often, they are intended for use in a particular application. By far, the most common application is hate speech detection (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017; Davidson et al., 2017; Del Vigna et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Gitari et al., 2015; Sanguinetti et al., 2018; Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Waseem, 2016). Although there is no clear definition of hate speech, it typically covers offensive language targeting a group (or sometimes a person) based on features such as race, ethnicity, gender, sexual orientation, socio-economic class, political affiliation or religion. The motivation is generally based on practical concerns, as hate speech is illegal under some jurisdictions, and there has been recent attempts to actively counteract online hate speech (European Commission, 2018, see also Article 19 (2018) for a discussion of effects of hate speech prevention mandates on freedom of speech). Some of"
2020.lrec-1.758,W19-1406,1,0.841225,"et type (individual, group or other) of a targeted offensive tweet. In addition, we also present experiments with a model trained to distinguish all labels simultaneously in a 5-way classification setup. We use linear support vector machine (SVM) classifiers with bag of n-grams as features. We use both word and character n-grams, and concatenate both type of features in a flat manner. The features are weighted using BM25. The same method was used in a number of earlier shared tasks with different objectives, and obtained top or near-top results (Çöltekin and Rama, 2018; Çöltekin et al., 2018; Wu et al., 2019). Hence, we believe that the results presented in this paper will be indicative of the amount of the signal in the data. However, since we do not use any external data, such as word embeddings, and/or other techniques, such as ensembles of classifiers, that are known to improve the results in similar tasks, there is considerable room for improvement. 5.1. Experimental Setup For each task, we tune a classifier using both character and word n-grams. We tune the models for a number of preprocessing and model parameters, using a random search through the parameter space. Namely, we tune the system"
2020.lrec-1.758,N12-1084,0,0.540392,"Missing"
2020.lrec-1.758,N19-1144,0,0.273868,"on of inappropriate language. For example, it can be useful in moderation of social media platforms, or more generally, on Internet sites allowing user content; it may be used by law enforcement agencies for detecting unlawful content; it may facilitate study of psychological effects of abusive language; and it could be useful for parents for preventing children from being exposed to such content. The recent interest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kumar et al., 2018b). This paper presents a corpus"
2020.lrec-1.758,S19-2010,0,0.192777,"on of inappropriate language. For example, it can be useful in moderation of social media platforms, or more generally, on Internet sites allowing user content; it may be used by law enforcement agencies for detecting unlawful content; it may facilitate study of psychological effects of abusive language; and it could be useful for parents for preventing children from being exposed to such content. The recent interest in automatic identification of various forms of offensive language is also evidenced by a number of shared tasks on the related topics (Kumar et al., 2018a; Wiegand et al., 2018; Zampieri et al., 2019b; Basile et al., 2019), and high number of participating groups in these shared tasks (the recent SemEval shared tasks OffensEval (Zampieri et al., 2019b) and HatEval (Basile et al., 2019) attracted submissions from 115 and 108 groups respectively). Furthermore, an increasing number of corpora annotated for some aspects or subtypes of offensive language has been published (Xu et al., 2012; Waseem and Hovy, 2016; Agarwal and Sureka, 2017; Davidson et al., 2017; ElSherief et al., 2018; Fortuna, 2017; Gao and Huang, 2017; Ibrohim and Budi, 2018; Kumar et al., 2018b). This paper presents a corpus"
2020.lrec-1.758,S18-1004,1,0.833786,"ly a three-way classifier that predicts the target type (individual, group or other) of a targeted offensive tweet. In addition, we also present experiments with a model trained to distinguish all labels simultaneously in a 5-way classification setup. We use linear support vector machine (SVM) classifiers with bag of n-grams as features. We use both word and character n-grams, and concatenate both type of features in a flat manner. The features are weighted using BM25. The same method was used in a number of earlier shared tasks with different objectives, and obtained top or near-top results (Çöltekin and Rama, 2018; Çöltekin et al., 2018; Wu et al., 2019). Hence, we believe that the results presented in this paper will be indicative of the amount of the signal in the data. However, since we do not use any external data, such as word embeddings, and/or other techniques, such as ensembles of classifiers, that are known to improve the results in similar tasks, there is considerable room for improvement. 5.1. Experimental Setup For each task, we tune a classifier using both character and word n-grams. We tune the models for a number of preprocessing and model parameters, using a random search through the pa"
2020.semeval-1.188,2020.semeval-1.206,0,0.0947199,"Missing"
2020.semeval-1.188,C18-1139,0,0.018965,"ømberg-Derczynski et al., 2020), etc. 4 1428 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic. Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words. Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018). Machine learning models In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), ALBERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular. Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014). Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles. 5 English Track A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of th"
2020.udw-1.6,2020.lrec-1.680,0,0.28915,"Missing"
2020.udw-1.6,L18-1025,0,0.227666,"Missing"
2020.udw-1.6,K18-2001,0,0.0309937,"Missing"
2020.vardial-1.17,P19-1068,0,0.324502,"0) Romanian Dialect Identification (RDI) task. Romanian and Moldavian are two closely related language varieties spoken in Romania and the Republic of Moldavia respectively. The languages, particularly in written form, are very similar – to the extent that discrimination by human annotators is barely above chance levels (G˘aman and Ionescu, 2020). However, as evidenced by the last year’s evaluation campaign (Zampieri et al., 2019), the machine learning methods applied to the task seem to be more successful (Chifu, 2019; Onose et al., 2019; Tudoreanu, 2019; Wu et al., 2019). The MOROCO corpus (Butnaru and Ionescu, 2019) used in last year’s RDI shared task consists of texts from online news. Although the data is fairly balanced with respect to the topics, and some of the obvious non-linguistic cues (e.g., named entities) are removed from the data, the data may still contain some unintended non-dialectal cues (e.g., style differences between the newspapers in two countries). A natural question that arises is whether the machine learning methods tap into such non-obvious cues not relevant to linguistic differences, or the data contains a strong signal for identifying the linguistic variation. To this end, the p"
2020.vardial-1.17,W19-1414,0,0.0136591,"The present study is conducted within the scope of the VarDial 2020 (G˘aman et al., 2020) Romanian Dialect Identification (RDI) task. Romanian and Moldavian are two closely related language varieties spoken in Romania and the Republic of Moldavia respectively. The languages, particularly in written form, are very similar – to the extent that discrimination by human annotators is barely above chance levels (G˘aman and Ionescu, 2020). However, as evidenced by the last year’s evaluation campaign (Zampieri et al., 2019), the machine learning methods applied to the task seem to be more successful (Chifu, 2019; Onose et al., 2019; Tudoreanu, 2019; Wu et al., 2019). The MOROCO corpus (Butnaru and Ionescu, 2019) used in last year’s RDI shared task consists of texts from online news. Although the data is fairly balanced with respect to the topics, and some of the obvious non-linguistic cues (e.g., named entities) are removed from the data, the data may still contain some unintended non-dialectal cues (e.g., style differences between the newspapers in two countries). A natural question that arises is whether the machine learning methods tap into such non-obvious cues not relevant to linguistic differen"
2020.vardial-1.17,2020.vardial-1.1,0,0.072284,"Missing"
2020.vardial-1.17,W18-3929,0,0.0332169,"Missing"
2020.vardial-1.17,W18-3907,0,0.0368156,"Missing"
2020.vardial-1.17,W19-1418,0,0.0163279,"tudy is conducted within the scope of the VarDial 2020 (G˘aman et al., 2020) Romanian Dialect Identification (RDI) task. Romanian and Moldavian are two closely related language varieties spoken in Romania and the Republic of Moldavia respectively. The languages, particularly in written form, are very similar – to the extent that discrimination by human annotators is barely above chance levels (G˘aman and Ionescu, 2020). However, as evidenced by the last year’s evaluation campaign (Zampieri et al., 2019), the machine learning methods applied to the task seem to be more successful (Chifu, 2019; Onose et al., 2019; Tudoreanu, 2019; Wu et al., 2019). The MOROCO corpus (Butnaru and Ionescu, 2019) used in last year’s RDI shared task consists of texts from online news. Although the data is fairly balanced with respect to the topics, and some of the obvious non-linguistic cues (e.g., named entities) are removed from the data, the data may still contain some unintended non-dialectal cues (e.g., style differences between the newspapers in two countries). A natural question that arises is whether the machine learning methods tap into such non-obvious cues not relevant to linguistic differences, or the data con"
2020.vardial-1.17,W19-1422,0,0.0227692,"thin the scope of the VarDial 2020 (G˘aman et al., 2020) Romanian Dialect Identification (RDI) task. Romanian and Moldavian are two closely related language varieties spoken in Romania and the Republic of Moldavia respectively. The languages, particularly in written form, are very similar – to the extent that discrimination by human annotators is barely above chance levels (G˘aman and Ionescu, 2020). However, as evidenced by the last year’s evaluation campaign (Zampieri et al., 2019), the machine learning methods applied to the task seem to be more successful (Chifu, 2019; Onose et al., 2019; Tudoreanu, 2019; Wu et al., 2019). The MOROCO corpus (Butnaru and Ionescu, 2019) used in last year’s RDI shared task consists of texts from online news. Although the data is fairly balanced with respect to the topics, and some of the obvious non-linguistic cues (e.g., named entities) are removed from the data, the data may still contain some unintended non-dialectal cues (e.g., style differences between the newspapers in two countries). A natural question that arises is whether the machine learning methods tap into such non-obvious cues not relevant to linguistic differences, or the data contains a strong si"
2020.vardial-1.17,W19-1406,1,0.908498,"the VarDial 2020 (G˘aman et al., 2020) Romanian Dialect Identification (RDI) task. Romanian and Moldavian are two closely related language varieties spoken in Romania and the Republic of Moldavia respectively. The languages, particularly in written form, are very similar – to the extent that discrimination by human annotators is barely above chance levels (G˘aman and Ionescu, 2020). However, as evidenced by the last year’s evaluation campaign (Zampieri et al., 2019), the machine learning methods applied to the task seem to be more successful (Chifu, 2019; Onose et al., 2019; Tudoreanu, 2019; Wu et al., 2019). The MOROCO corpus (Butnaru and Ionescu, 2019) used in last year’s RDI shared task consists of texts from online news. Although the data is fairly balanced with respect to the topics, and some of the obvious non-linguistic cues (e.g., named entities) are removed from the data, the data may still contain some unintended non-dialectal cues (e.g., style differences between the newspapers in two countries). A natural question that arises is whether the machine learning methods tap into such non-obvious cues not relevant to linguistic differences, or the data contains a strong signal for identifyi"
2020.vardial-1.17,W17-1201,0,0.171995,"Missing"
2020.vardial-1.17,W18-3901,0,0.0448106,"Missing"
2021.cmcl-1.17,K17-3009,0,0.0192534,"sion model include the main word-level features, frequency, word length and predictability discussed above. Word frequencies are computed using a large news corpus of approximately 2.7 million articles.1 The predictability features are obtained using the recurrent network described in Section 2.2. Besides these features, we also include some linguistic features including the POS tag, dependency relation, and signed distance from the head, as well as named entity tag. The POS and dependency information is obtained using version 1.2 of UDPipe using the pretrained models released by the authors (Straka and Straková, 2017; Straka and Straková, 2019). We used Apache OpenNLP (Apache Software Foundation, 2014) for named entity recognition. The model input also included indicator features for beginning and end of sentence, and whether the word is combined with a punctuation mark or not (see Table 3). We also included the features from EZ-reader described in Section 2.1.1 as additional inputs to the regression model. The predictions were based on a symmetric window around the target word, where all the above features for the target word and ±k words were concatenated. We selected the optimal window size as well as"
C16-1325,W03-2405,0,0.250537,"me the first Turkish treebank to be included in a UD release. The treebank was created by automatic conversion of the IMST This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://universaldependencies.org/ License details: http:// 3444 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3444–3454, Osaka, Japan, December 11-17 2016. Treebank (Sulubacak et al., 2016), which is itself a reannotation of the METU-Sabancı Turkish Treebank (Oflazer et al., 2003; Atalay et al., 2003). Although the annotation framework of the IMST Treebank was revised, it is still fundamentally similar to that of the METU-Sabancı Treebank and radically different from the UD framework in both morphology and syntax. In this paper, we describe the procedures employed in converting the annotation schemes of the IMST Treebank to the corresponding UD-compliant schemes. We also provide comparative statistics on the composition of the IMST Treebank before and after the conversion. Afterwards, we report our initial parsing results on the new IMST-UD Treebank in comparison with the original IMST Tre"
C16-1325,de-marneffe-etal-2006-generating,0,0.185725,"Missing"
C16-1325,de-marneffe-etal-2014-universal,1,0.918476,"Missing"
C16-1325,E06-1012,0,0.552875,"Missing"
C16-1325,J08-3003,1,0.900686,"Missing"
C16-1325,W11-3806,1,0.873339,"Missing"
C16-1325,P99-1033,0,0.107905,"word has a separate row, each containing a tabdelimited array of morphosyntactic data pertaining to the word. In compliance with the UD standard, the converted sentences were output in the CoNLL-U format.2 The sections to follow present explanations and discussions on the procedures of mapping morphological and syntactic data, as well as some idiosyncratic linguistic phenomena. Quick reference tables were also provided where applicable, showing what conditions on the source unit are required to assign which properties to the target unit. 2.1 Segmentation The inflectional group (IG) formalism (Oflazer, 1999; Hakkani-T¨ur et al., 2002) was designed to make the highly agglutinative typology of Turkish tractable for language processing. Since then, it has seen usage in many influential works (Oflazer, 2003; Eryi˘git and Oflazer, 2006) and has become the de facto standard in parsing Turkish. According to the formalism, orthographic tokens are divided into morphosyntactic words from derivational boundaries.3 These units are called the inflectional groups (IGs) of the token. The IG formalism establishes these, rather than orthographic tokens, as the syntactic units of the sentence. The original IMST t"
C16-1325,J03-4001,0,0.0451041,"mat.2 The sections to follow present explanations and discussions on the procedures of mapping morphological and syntactic data, as well as some idiosyncratic linguistic phenomena. Quick reference tables were also provided where applicable, showing what conditions on the source unit are required to assign which properties to the target unit. 2.1 Segmentation The inflectional group (IG) formalism (Oflazer, 1999; Hakkani-T¨ur et al., 2002) was designed to make the highly agglutinative typology of Turkish tractable for language processing. Since then, it has seen usage in many influential works (Oflazer, 2003; Eryi˘git and Oflazer, 2006) and has become the de facto standard in parsing Turkish. According to the formalism, orthographic tokens are divided into morphosyntactic words from derivational boundaries.3 These units are called the inflectional groups (IGs) of the token. The IG formalism establishes these, rather than orthographic tokens, as the syntactic units of the sentence. The original IMST treebank also follows its predecessors in using the IG formalism. The rightmost IG governs the word, while every other IG depends on the next one in line with the exclusive relation DERIV. Though a com"
C16-1325,petrov-etal-2012-universal,0,0.0603661,"the UD framework is at least as viable for Turkish as the original annotation framework of the IMST Treebank. 1 Introduction The Universal Dependencies (UD)1 project is an international collaborative project to make cross-linguistically consistent treebanks available for a wide variety of languages. Currently in version 1.3, the UD project covers 40 languages, including two Turkic languages: Kazakh, which was annotated from scratch, and Turkish, the creation of which is described in this paper. The universal annotation guidelines of UD are based on the Google Universal Part-of-Speech Tagset (Petrov et al., 2012) for parts of speech, the Interset framework (Zeman, 2008) for morphological features, and Stanford Dependencies (De Marneffe et al., 2006; Tsarfaty, 2013; De Marneffe et al., 2014) for dependency relations. The objective of harmonizing annotation guidelines as far as possible is to make comparison of parsing results and investigating cross-linguistic methods across languages easier. This is achieved by a number of principles, including the primacy of content words, distinguishing core arguments from modifiers and distinguishing clausal constituents from nominals. The IMST-UD Treebank was firs"
C16-1325,W13-4915,1,0.888478,"Missing"
C16-1325,P13-2103,0,0.0332315,"roject is an international collaborative project to make cross-linguistically consistent treebanks available for a wide variety of languages. Currently in version 1.3, the UD project covers 40 languages, including two Turkic languages: Kazakh, which was annotated from scratch, and Turkish, the creation of which is described in this paper. The universal annotation guidelines of UD are based on the Google Universal Part-of-Speech Tagset (Petrov et al., 2012) for parts of speech, the Interset framework (Zeman, 2008) for morphological features, and Stanford Dependencies (De Marneffe et al., 2006; Tsarfaty, 2013; De Marneffe et al., 2014) for dependency relations. The objective of harmonizing annotation guidelines as far as possible is to make comparison of parsing results and investigating cross-linguistic methods across languages easier. This is achieved by a number of principles, including the primacy of content words, distinguishing core arguments from modifiers and distinguishing clausal constituents from nominals. The IMST-UD Treebank was first released in UD version 1.3 and became the first Turkish treebank to be included in a UD release. The treebank was created by automatic conversion of the"
C16-1325,zeman-2008-reusable,0,0.0261613,"l annotation framework of the IMST Treebank. 1 Introduction The Universal Dependencies (UD)1 project is an international collaborative project to make cross-linguistically consistent treebanks available for a wide variety of languages. Currently in version 1.3, the UD project covers 40 languages, including two Turkic languages: Kazakh, which was annotated from scratch, and Turkish, the creation of which is described in this paper. The universal annotation guidelines of UD are based on the Google Universal Part-of-Speech Tagset (Petrov et al., 2012) for parts of speech, the Interset framework (Zeman, 2008) for morphological features, and Stanford Dependencies (De Marneffe et al., 2006; Tsarfaty, 2013; De Marneffe et al., 2014) for dependency relations. The objective of harmonizing annotation guidelines as far as possible is to make comparison of parsing results and investigating cross-linguistic methods across languages easier. This is achieved by a number of principles, including the primacy of content words, distinguishing core arguments from modifiers and distinguishing clausal constituents from nominals. The IMST-UD Treebank was first released in UD version 1.3 and became the first Turkish"
C16-1325,J08-4010,1,\N,Missing
coltekin-2010-freely,schmid-etal-2004-smor,0,\N,Missing
coltekin-2014-set,coltekin-2010-freely,1,\N,Missing
coltekin-2014-set,N06-1042,0,\N,Missing
coltekin-2014-set,E09-2008,0,\N,Missing
coltekin-2014-set,A94-1024,0,\N,Missing
coltekin-2014-set,J08-4010,0,\N,Missing
coltekin-2014-set,J08-3003,0,\N,Missing
coltekin-2014-set,W13-1817,0,\N,Missing
coltekin-2014-set,P97-1029,0,\N,Missing
coltekin-2014-set,W14-0505,1,\N,Missing
coltekin-2014-set,R13-2014,0,\N,Missing
coltekin-2014-set,washington-etal-2012-finite,0,\N,Missing
coltekin-2014-set,2009.freeopmt-1.3,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-3014,K17-2001,0,0.0379892,"Missing"
K18-3014,K17-2002,0,0.0495487,"Missing"
K18-3014,K18-3001,0,0.0242806,"Missing"
K18-3014,N16-1077,0,0.0638924,"Missing"
K18-3014,W18-3401,0,0.047629,"Missing"
K18-3014,K17-2003,0,0.0380235,"Missing"
K18-3014,W16-2010,0,0.0427657,"Missing"
K18-3014,L18-1293,0,0.0503691,"Missing"
K18-3014,K17-2010,0,0.0229137,"ell. The success of the system seems to depend highly on ‘training data enhancement’. For different tracks (with different restrictions on data used) of the 2016 shared task, Kann and Sch¨utze (2016) developed new techniques to increase the number of training instances. The methods used mostly work well for re-inflection task, since the re-inflection task is symmetric, and one can invert the source and target forms. In the subsequent year’s shared task for 2017 (Cotterell et al., 2017), multiple authors explored new data enhancement techniques (Kann and Sch¨utze, 2017; Bergmanis et al., 2017; Silfverberg et al., 2017) to improve the performance of the seq2seq models in medium and low resource scenarios. The work presented in this paper is based on the work of the simple encoder-decoder system of Faruqui et al. (2016). In this paper, we describe our three submissions to the inflection track of SIGMORPHON shared task. We experimented with three models: namely, sequence to sequence model (popularly known as seq2seq), seq2seq model with data augmentation, and a multilingual multi-tasking seq2seq model that is multilingual in nature. Our results with the multilingual model are below the baseline in the case of"
L18-1608,W06-2920,0,0.370592,"Missing"
L18-1608,W02-1503,0,0.157572,"tactic parsing, in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages. Keywords: Morphology, Universal Dependencies, Morphological Analysis, Morphological Ambiguity 1. Introduction The development of the universal dependencies (UD) framework and its treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows many shared tasks and multilingual evaluation campaigns in which the linguistic representation schemes across different languages vary (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Butt et al., 2002; Zeman et al., 2012). The UD treebanks collection, in contrast, obeys a single set of annotation guidelines, and respects the discrepancies between surface input tokens and the output nodes in the syntax trees (a.k.a., the two-level representation principle.)1 The UD initiative has paved the way to the development of cross-lingual models for word segmentation, part-of-speech tagging and dependency parsing (Straka and Strakov´a, 2017), as well as cross-linguistic typological investigations (Futrell et al., 2015). Recently, the CoNLL 2017 Shared Task on Multilingual UD Parsing (Zeman et al., 20"
L18-1608,coltekin-2010-freely,1,0.858208,"Missing"
L18-1608,C16-1033,1,0.861239,"4), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different se"
L18-1608,K17-3027,1,0.780254,"004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different segmentation approach compared to the METU-Sabancı Turkish Treebank (Oflazer et al., 2003). In addition, form and lemma representations, POS tags and morphological tag sets have changed. The existing morphological analyzers are not compatible with this new representation. Thus we intro"
L18-1608,L16-1262,1,0.908557,"Missing"
L18-1608,pasha-etal-2014-madamira,1,0.772852,"w, and Turkish. For these morphological analyzers, their pre-existing morphological analyses adhere to schemes that differ from those employed in the respective UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of Mo"
L18-1608,L18-1292,1,0.841663,"L format. 3.2. Converted Morphological Lexicons As a complement to the CoNLL-UL-compatible analyzers described above, we have created a set of 53 CoNLL-ULcompatible morphological lexicons covering 38 languages, based on existing freely available resources.9 The source lexicons, the conversion processes and the resulting inventory of freely available CoNLL-UL lexicons are described https://conllul.github.io 3850 6 https://camel.abudhabi.nyu.edu/calima-star/ https://github.com/habeanf/yap 8 https://github.com/coltekin/TRmorph/tree/trmorph2 9 http://pauillac.inria.fr/˜sagot/udlexicons.html 7 in (Sagot, 2018).10 Here we only provide in Table 3 two examples converted from the Lefff , the Alexina lexicon for French. The first one illustrates the 1-to-1 case, with an entry converted from the following original entry: encodent encoder v P3p, which includes the wordform (i.e. the [source and tree] token) encodent ‘encode3pl.pres.ind ’, its lemma, its Lefff POS and its Lefff morphosyntactic tag. The other example illustrates the 1-to-m case with the source token auxquels, which is analyzable as reflecting the sequence of two tree tokens a` lesquels ‘to which’. 4. Related Work and Perspective Our work ov"
L18-1608,W13-4917,1,0.923267,"Missing"
L18-1608,Q15-1026,1,0.906774,"Missing"
L18-1608,K17-3009,0,0.0605054,"Missing"
L18-1608,C16-1325,1,0.883776,"Missing"
L18-1608,W17-1320,1,0.835643,"ctive UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices"
L18-1608,P12-2002,1,0.826771,"y. As a result of the latter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to main"
L18-1608,P13-2103,1,0.786276,"atter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to maintain compatibilit"
L18-1608,zeman-etal-2012-hamledt,0,0.319438,"Missing"
L18-1608,K17-3001,1,0.904845,"Missing"
S18-1004,D17-1169,0,0.0966468,"Missing"
S18-1004,E17-2017,0,0.117698,"Missing"
S18-1004,W15-5407,0,0.0665747,"Missing"
S18-1004,S18-1003,0,0.0819006,"Missing"
S18-1004,W17-1219,0,0.0750768,"Missing"
S18-1004,L16-1626,0,0.140226,"Missing"
S18-1004,W14-4012,0,0.184411,"Missing"
S18-1004,W17-5028,1,0.783725,"Missing"
W14-0505,N09-1036,0,0.0483944,"l, since the input is often compatible with multiple segmentations spanning the complete utterance. The problem, however, is even more difficult for a learner who starts with no lexicon. Fortunately, the lexicon is not the only aid for segmentation. Experimental research within last two decades has revealed an array of cues that are used by adults and children for lexical segmentation. These cues include, but are not Models that use explicit representations in combination with statistical procedures (e.g., Brent and Cartwright, 1996; Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; M. Johnson and Goldwater, 2009) avoid both problems: these models perform better, and it is easier to reason about what they learn. Although these models were also instrumental in our understanding of the problem, they lack at least two aspects of con19 Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 19–28, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics ity voting, will be used for combining multiple boundary indicators. Majority voting is a common (and arguably effective) method in everyday social and political life. As a result"
W14-0505,W11-0304,0,0.014332,"updated for each voter. Then, the weight, wi , of each voter is updated using,  ei  wi ← 2 0.5 − N nectionist models that fit human processing better. First, even though we know that human segmentation is incremental and predictive, most of these models process their input either in a batch fashion, or they require the complete utterance to be presented before attempting to segment the input. Second, it is generally difficult to incorporate arbitrary cues into most of these models. Models that use explicit representations with incremental models exist (e.g., Monaghan and Christiansen, 2010; Lignos, 2011), but are rather rare. Furthermore, the investigation of cues and cue combination in segmentation is also relatively scarce within the recent studies (exceptions include the investigation of various suprevised models by Jarosz and J. A. Johnson, 2013). The present paper introduces a strictly incremental, unsupervised method for learning segmentation where the learning method and internal representations are explicitly defined. Crucially, we use a set of cues demonstrated to be used by humans in solving the segmentation problem. The simulations results that we present are based on the same chil"
W14-0505,P08-1016,0,0.522366,"fore, we define a set of boundary indicators and operationalize lexical stress as another cue for segmentation. All models of segmentation in the literature use utterance boundaries implicitly by assuming that the words cannot straddle utterance boundaries. The explicit use of utterance boundaries to discover regularities about words is common in connectionist models (e.g., Aslin et al., 1996; Christiansen et al., 1998; Stoianov and Nerbonne, 2000). Similar use of utterance boundaries in nonconnectionist models is rather rare. Three exceptions to this are the models described by Brent (1996), Fleck (2008) and Monaghan and Christiansen (2010). The method described in this section is similar to Fleck’s method, where the model estimates the probability of observing a boundary given its left and right context, P(b|l, r), where b represents boundary, and as before, l and r represent left and right contexts, respectively. If this probability is greater than 0.5, the model inserts a boundary. Using utterance boundaries and the pauses, Fleck (2008) presents a batch algorithm with a few ad hoc corrections that estimates the probabilities P(b), P(l|b), P(r|b), P(l), P(r), and uses Bayesian inversion to"
W14-0505,J01-3002,0,0.299648,"f the acoustic input, the problem is not trivial, since the input is often compatible with multiple segmentations spanning the complete utterance. The problem, however, is even more difficult for a learner who starts with no lexicon. Fortunately, the lexicon is not the only aid for segmentation. Experimental research within last two decades has revealed an array of cues that are used by adults and children for lexical segmentation. These cues include, but are not Models that use explicit representations in combination with statistical procedures (e.g., Brent and Cartwright, 1996; Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; M. Johnson and Goldwater, 2009) avoid both problems: these models perform better, and it is easier to reason about what they learn. Although these models were also instrumental in our understanding of the problem, they lack at least two aspects of con19 Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 19–28, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics ity voting, will be used for combining multiple boundary indicators. Majority voting is a common (and arguably effective)"
W15-2409,P08-1016,0,0.0568729,"Missing"
W15-2409,J01-2001,0,0.106621,"the earlier literature, we do not split our data as test and training set since we are using an unsupervised learning method. 2.3 P(wi ) (1) P(w) = (1 − α)f(w) ∏ α m j=1 f(aj ) if w is known if w is unknown (2) Brent, 1999 Venkataraman, 2001 Goldwater, Griffiths, and M. Johnson (2009) Blanchard, Heinz, and Golinkoff (2010) 82.3 82.1 85.2 81.9 68.2 68.3 72.3 66.1 52.4 55.7 59.1 56.3 Current model (incremental) Current model (final) 83.4 86.6 71.6 76.3 55.3 70.7 One way to view this model is as an instance of minimum description length (MDL) principle (Rissanen, 1978). (Creutz and Lagus, 2007; Goldsmith, 2001; Marcken, 1996; Rissanen, 1978). Equation 1 imposes a preference for short utterances (in number of words). Assuming each word is represented by an index or pointer in the lexicon, this leads to a preference towards a representation that minimizes the corpus length. Let alone, this preference would result in no segmentation, and corpus size would be equal to the number of utterance types. Despite small corpus representation, this would lead to a large lexicon containing all the utterance types. The second part of Equation 2, on the other hand, imposes a preference for short words and, since s"
W15-2409,coltekin-2010-freely,1,0.868438,"Missing"
W15-2409,coltekin-2014-set,1,0.874577,"Missing"
W15-2409,J01-3002,0,0.565999,"er, and Redanz, 1993), allophonic differences (Jusczyk, Hohne, and Bauman, 1999), vowel harmony (Kampen et al., 2008; Suomi, McQueen, and Cutler, 1997) and coarticulation (E. K. Johnson and Jusczyk, 2001). Computational models offer a complementary method to the psycholinguistic experiments. There have been an increasing number of computational models of segmentation in the literature, particularly within the last two decades (just to exemplify a few, Elman, 1990, Aslin, 1993, Cairns et al., 1994, Christiansen, Allen, and Seidenberg, 1998, Fleck, 2008, Brent and Cartwright, 1996, Brent, 1999, Venkataraman, 2001, Xanthos, 2004, Goldwater, Griffiths, and M. Johnson, 2009, M. Johnson and Goldwater, 2009, Monaghan and Christiansen, 2010, Çöltekin and Nerbonne, 2014). In this paper, we investigate a recurring issue in the segmentation literature: the use of syllable or phoneme as the basic input unit in computational models of segmentation.1 Most psycholinguistic research is based on syllable as the basic unit. The likely reason behind this choice is the early research pointing to syllable as a salient perceptual unit for adults (Cutler, Mehler, et al., 1986; Mehler et al., 1981; Savin and Bever, 1970),"
W15-2409,W10-2912,0,0.0189888,"lable-internal. This study contrasts the use of phoneme and syllable as the basic units in speech segmentation. To this end, we use a simple state-of-the-art segmentation model, and run a set of simulations on two typologically different languages, English and Turkish. We evaluate the results based on word- and morpheme-segmented gold standards. The next section describes the model and the data used in this study, Section 3 presents results from a series of computational simulations, we discuss the results in Section 4 and conclude in Section 5. A few exceptions aside (Gambell and Yang, 2006; Lignos and Yang, 2010; Phillips and Pearl, 2014; Swingley, 2005), most of the computational models in the literature take phonetic segments as the basic unit. For some of the models, the syllable is a natural choice as the basic unit because they are based on information associated with syllables rather than sub-syllabic units. For example both lexical stress (Gambell and Yang, 2006; Swingley, 2005), and vowel harmony (Ketrez, 2013) operates at the level of syllable. Even when such information, e.g., lexical stress, is used in phoneme-based models (e.g., by Christiansen, Allen, and Seidenberg, 1998, Çöltekin, 2011"
W15-2409,P96-1044,0,0.380233,"Missing"
W15-2409,W14-0505,1,\N,Missing
W16-1714,W14-3902,0,0.374656,"Çetino˘glu IMS University of Stuttgart Germany ozlem@ims.uni-stuttgart.de Ça˘grı Çöltekin Department of Linguistics University of Tübingen Germany ccoltekin@sfs.uni-tuebingen.de Abstract 2014; cf. Solorio et al., 2014), predicting codeswitching points (Solorio and Liu, 2008a; Elfardy et al., 2013), and POS tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish"
W16-1714,L16-1667,1,0.7341,"Missing"
W16-1714,W14-5152,0,0.0127886,"sity of Stuttgart Germany ozlem@ims.uni-stuttgart.de Ça˘grı Çöltekin Department of Linguistics University of Tübingen Germany ccoltekin@sfs.uni-tuebingen.de Abstract 2014; cf. Solorio et al., 2014), predicting codeswitching points (Solorio and Liu, 2008a; Elfardy et al., 2013), and POS tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already"
W16-1714,I11-1100,1,0.917666,"Missing"
W16-1714,P11-2008,0,0.10711,"Missing"
W16-1714,P99-1033,0,0.118156,"s as zu and der. The segmentation of Turkish syntactic words is more involved, and at present, the UD guidelines for Turkish tokenization are still a moving target. We describe the approach we employed for segmentation of Turkish below. Turkish is a morphologically complex language. In addition to a large set of inflectional morphemes that can attach to verbal or nominal stems, some productive (derivational) morphemes may change the POS tag of an already inflected word. In Turkish NLP literature, this phenomenon is addressed with sub-word units that are often called inflectional groups (IGs) (Oflazer, 1999), which correspond to one or more morphemes grouped by derivational boundaries. In this work, we also follow the same convention, however, similar to Çöltekin (2016), we follow a more conservative approach to segmentation in comparison to most earlier work. Instead of segmenting a word into IGs after each derivation, we segment only before the morphemes that introduce a new syntactic word, such that parts of the word may carry conflicting morphological features, or participate in separate syntactic relations. In other words, we segment words to avoid potential ambiguous or conflicting morphosy"
W16-1714,R15-1033,0,0.0488142,"u, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already annotated with language information (Çetino˘glu, 2016). We add the POS tag layer following Universal Dependencies (UD) (Nivre et al., 2016). German is one of the languages UD already covers. Turkish on the other hand is under development. Therefore, our work also contributes to the discussions on POS tagging"
W16-1714,W15-1610,0,0.176559,"Missing"
W16-1714,C82-1023,0,0.215414,"perspective (Gumperz, 1964; Sankoff, 1968; Lipski, 1978). Some linguists make distinctions in the terminology according to the level of the language mixing, e.g. use codemixing for sentence-internal alternations, some others use either code-mixing or code-switching for all types of mixing (Poplack, 1980; MyersScotton, 1997). In this paper we use codeswitching (CS) as an umbrella term. Unlike linguistic studies, computational research on code-switching has recently accelerated, although the first theoretical framework to parse code-switched sentences has been proposed by Joshi back in the 80s (Joshi, 1982). Several studies has emerged on word-level language identification (Nguyen and Do˘gruöz, 2013; Das and Gambäck, 1 There are some POS-annotated corpora that contain CS instances although the intention of collection is different. For instance the KiezDeutsch corpus (Rehbein et al., 2014) has a small number of utterances with Turkish-German CS. Old German Reference Corpus (Dipper et al., 2004) has examples of mixing Old High German and Latin. 120 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 120–130, c Berlin, Germany, August 11, 2016. 2016 Association for Computational L"
W16-1714,W15-1608,0,0.440658,"ny ozlem@ims.uni-stuttgart.de Ça˘grı Çöltekin Department of Linguistics University of Tübingen Germany ccoltekin@sfs.uni-tuebingen.de Abstract 2014; cf. Solorio et al., 2014), predicting codeswitching points (Solorio and Liu, 2008a; Elfardy et al., 2013), and POS tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already annotated with language"
W16-1714,petrov-etal-2012-universal,0,0.148065,"Missing"
W16-1714,J93-2004,0,0.0548092,"Missing"
W16-1714,rehbein-etal-2014-kiezdeutsch,0,0.0694546,"Missing"
W16-1714,D11-1141,0,0.0842406,"Missing"
W16-1714,D13-1084,0,0.125539,"Missing"
W16-1714,D08-1102,0,0.174993,"nd Liu, 2008a; Elfardy et al., 2013), and POS tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already annotated with language information (Çetino˘glu, 2016). We add the POS tag layer following Universal Dependencies (UD) (Nivre et al., 2016). German is one of the languages UD already covers. Turkish on the other hand is under development. T"
W16-1714,D08-1110,0,0.627216,"nd Liu, 2008a; Elfardy et al., 2013), and POS tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already annotated with language information (Çetino˘glu, 2016). We add the POS tag layer following Universal Dependencies (UD) (Nivre et al., 2016). German is one of the languages UD already covers. Turkish on the other hand is under development. T"
W16-1714,W14-3907,0,0.112609,"12 Universal POS tags (Petrov et al., 2011) and three additional tags for named entities (people, location, organisation). They have 3 Data We use the data that Çetino˘glu (2016) has collected on code-switching Turkish-German tweets. It consists of 1029 tweets, each having at least one code-switching point. Tweets are automatically collected and manually filtered. Before adding language identification annotation tokenisation and normalisation is applied based on Turkish and German orthography rules. The tag set is based on the 2014 Shared Task on Language Identification in Code-Switched Data (Solorio et al., 2014; Maharjan et al., 2015): TR (Turkish), DE (German), LANG 3 (third language), MIXED (intra-word CS), NE (named entity), AM BIG (words belong to both languages and cannot be disambiguated with the given context), OTHER (punctuation, numbers, URLs, emoticons, sym121 bols, any other token that do not belong to previous classes). The Shared Task labels the tokens that belong to a third language as OTHER, Çetino˘glu (2016) introduces the LANG 3 tag for them. Additionally, named entities are tagged both as NE as in the Shared Task, and with their language label TR, DE, or LANG 3. MIXED tokens are al"
W16-1714,D14-1105,0,0.142132,"ing (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015). Computational approaches often need annotated data. The number of CS corpora annotated with language identification information has also increased proportional to the interest in the field (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Das and Gambäck, 2014; Maharjan et al., 2015). Part of speech (POS) annotation of CS data, on the other hand, is not very common yet. To our knowledge, there are only three code-switching corpora with POS annotation:1 one on SpanishEnglish (Solorio and Liu, 2008b) and two on Hindi-English (Vyas et al., 2014; Jamatia et al., 2015). These are valuable resources as part of speech tags can provide more insight on the nature of code-switching and pave the way for syntactic annotation. Here in this work, we present a fourth CS corpus annotated with POS information. The corpus contains 1029 Turkish-German tweets, already annotated with language information (Çetino˘glu, 2016). We add the POS tag layer following Universal Dependencies (UD) (Nivre et al., 2016). German is one of the languages UD already covers. Turkish on the other hand is under development. Therefore, our work also contributes to the dis"
W16-1908,N09-1036,0,0.0541335,"Missing"
W16-1908,W14-0505,1,0.841297,"Missing"
W16-1908,P15-1167,1,0.833007,"purely symbolic representations, such distributed representations allow input units that appear in similar contexts to share similar vectors (embeddings). The model can, then, exploit the similarities between the embeddings during segmentation and learning. This paper studies the learning and use of embeddings of phone1 uni- and bi-grams for computational models of word segmentation in child language acquisition. Our work is inspired by recent success of embeddings in NLP (Devlin et al., 2014; Socher et al., 2013), especially in Chinese word segmentation (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015). However, this work differs from Chinese word segmentaThis paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings significantly improves performance. We make use of extensive visualization to understand what the model has learned. We show that the learned embeddings are informative fo"
W16-1908,J01-3002,0,0.198198,"Missing"
W16-1908,D13-1061,0,0.032469,"es that are learned from data. Unlike purely symbolic representations, such distributed representations allow input units that appear in similar contexts to share similar vectors (embeddings). The model can, then, exploit the similarities between the embeddings during segmentation and learning. This paper studies the learning and use of embeddings of phone1 uni- and bi-grams for computational models of word segmentation in child language acquisition. Our work is inspired by recent success of embeddings in NLP (Devlin et al., 2014; Socher et al., 2013), especially in Chinese word segmentation (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015). However, this work differs from Chinese word segmentaThis paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings significantly improves performance. We make use of extensive visualization to understand what the model has learned. We show that"
W16-1908,P14-1028,0,0.0205512,"from data. Unlike purely symbolic representations, such distributed representations allow input units that appear in similar contexts to share similar vectors (embeddings). The model can, then, exploit the similarities between the embeddings during segmentation and learning. This paper studies the learning and use of embeddings of phone1 uni- and bi-grams for computational models of word segmentation in child language acquisition. Our work is inspired by recent success of embeddings in NLP (Devlin et al., 2014; Socher et al., 2013), especially in Chinese word segmentation (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015). However, this work differs from Chinese word segmentaThis paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings significantly improves performance. We make use of extensive visualization to understand what the model has learned. We show that the learned embed"
W16-1908,P13-1045,0,0.0210797,"of low-dimensional, real-valued vector representation of features that are learned from data. Unlike purely symbolic representations, such distributed representations allow input units that appear in similar contexts to share similar vectors (embeddings). The model can, then, exploit the similarities between the embeddings during segmentation and learning. This paper studies the learning and use of embeddings of phone1 uni- and bi-grams for computational models of word segmentation in child language acquisition. Our work is inspired by recent success of embeddings in NLP (Devlin et al., 2014; Socher et al., 2013), especially in Chinese word segmentation (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015). However, this work differs from Chinese word segmentaThis paper presents a novel model that learns and exploits embeddings of phone ngrams for word segmentation in child language acquisition. Embedding-based models are evaluated on a phonemically transcribed corpus of child-directed speech, in comparison with their symbolic counterparts using the common learning framework and features. Results show that learning embeddings significantly improves performance. We make use of extensive visuali"
W16-1908,P09-1054,0,0.0384984,"r utterance boundaries and sampled intra-utterance positions, respectively. To offset over-fitting, we add an L2 regularization term (||ij ||2 + ||w||2 ) to the loss function, as follows:  λ ||ij ||2 + ||w||2 (3) 2 The λ is a factor that adjusts the contribution of the regularization term. To minimize the regularized loss function, which is is still convex, we perform stochastic gradient descent to iteratively update the embeddings and the weight vector in turn, each time considering the other as constant. The gradients and update rules are similar to that of logistic regression model as in Tsuruoka et al. (2009), except that the input embeddings i are also updated besides the standard weight vector. In particular, the gradient of input embeddings ij for each particular position j is computed according to (4), where w is the weight vector and yj is the assumed label. The input embeddings are then updated by (5), where α is the learning rate. 3.1 Jj ← Jj + ∂Jj = (f (j) − yj ) · w + λij ∂ij ∂Jj ij ← ij − α ∂ij 2.3 Data In the experiments reported in this paper, we use the de facto standard corpus for evaluating segmentation models. The corpus was collected by Bernstein Ratner (1987) and converted to a p"
W16-1908,P08-1016,0,\N,Missing
W16-1908,P14-1129,0,\N,Missing
W16-4802,W15-5409,0,0.0842082,"res. This observation is supported by the fact that one of the most popular and successful approaches in earlier DSL shared tasks has been hierarchical systems that use different models This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 15 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 15–24, Osaka, Japan, December 12 2016. for discriminating language groups and individual languages within each group (Goutte et al., 2014; Goutte and Léger, 2015; Fabra Boluda et al., 2015; Ács et al., 2015). The potential benefit of a deep learning model here is the possibility of building a single (hierarchical) model that addresses both issues jointly. The second potential benefit of deep learning architectures comes from the fact that, unlike linear models, they can capture non-additive non-linear interactions between input features. For example, although none of the features marked with boldface in (1) below are conclusive for 3-way discrimination between Bosnian (1a), Croatian (1b) and Serbian (1c), a non-linear combination of these features are definitely useful.1 (1) a."
W16-4802,W15-5403,0,0.40173,"ord embeddings (dense vectors) and then employ average the vectors across the sentence to yield a single dense representation of a sentence. Formally, given a sentence of length N , each word wn is represented as a vector en ∈ RK where K is the dimension k×n . The of the dense representation. At this step, the sentence is now represented as matrix P E ∈ R Ek k average pooling step transforms the matrix into a single vector X ∈ R and Xk = n where, Xk is the k th element in X. We tested with both character embeddings and word embeddings in our experiments. This model is similar to the system of Franco-Salvador et al. (2015) in the DSL 2015 shared task, since it represents the documents as an average vectors of their components (characters or words). However, crucially, the embeddings used in this model are learned specifically for the discrimination task rather than being general word vectors capturing syntactic/semantic properties. Hierarchical character + word models. The general architecture used for our hierarchical network model is presented in Figure 1. In this model, we use both character and word embeddings to train our model. Similar to FastText model discussed above, the embeddings are task-specific. T"
W16-4802,W15-5413,0,0.518804,"t types of models and/or features. This observation is supported by the fact that one of the most popular and successful approaches in earlier DSL shared tasks has been hierarchical systems that use different models This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 15 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 15–24, Osaka, Japan, December 12 2016. for discriminating language groups and individual languages within each group (Goutte et al., 2014; Goutte and Léger, 2015; Fabra Boluda et al., 2015; Ács et al., 2015). The potential benefit of a deep learning model here is the possibility of building a single (hierarchical) model that addresses both issues jointly. The second potential benefit of deep learning architectures comes from the fact that, unlike linear models, they can capture non-additive non-linear interactions between input features. For example, although none of the features marked with boldface in (1) below are conclusive for 3-way discrimination between Bosnian (1a), Croatian (1b) and Serbian (1c), a non-linear combination of these features are"
W16-4802,W14-5316,0,0.133656,"ages require different types of models and/or features. This observation is supported by the fact that one of the most popular and successful approaches in earlier DSL shared tasks has been hierarchical systems that use different models This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 15 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 15–24, Osaka, Japan, December 12 2016. for discriminating language groups and individual languages within each group (Goutte et al., 2014; Goutte and Léger, 2015; Fabra Boluda et al., 2015; Ács et al., 2015). The potential benefit of a deep learning model here is the possibility of building a single (hierarchical) model that addresses both issues jointly. The second potential benefit of deep learning architectures comes from the fact that, unlike linear models, they can capture non-additive non-linear interactions between input features. For example, although none of the features marked with boldface in (1) below are conclusive for 3-way discrimination between Bosnian (1a), Croatian (1b) and Serbian (1c), a non-linear combinati"
W16-4802,Q14-1003,0,0.0338165,"Missing"
W16-4802,W16-4801,0,0.114712,"Missing"
W16-4802,D13-1084,0,0.0557822,"Missing"
W16-4802,W14-5318,0,0.0336049,"vs-rest and one-vs-one multi-class strategies. In our experiments, different linear models performed comparably. However, the SVM models always performed slightly better than logistic regression models. In this paper, we only describe the SVM models and discuss the results obtained using them. We did not apply any filtering (e.g., case normalization, tokenization) except truncating the input documents to 70 white-space-separated tokens. In all experiments reported in this paper, we fixed the single model parameter (SVM margin or regularization parameter, C) at 1.00. Unlike results reported by Purver (2014), in our experiments, the model accuracy was not affected drastically with the changes in the regularization parameter within a reasonable range (not reported here). Although stronger regularization was useful for models employing larger number of features, the effect was not noteworthy. All linear models were implemented with scikit-learn (Pedregosa et al., 2011) and trained and tested using Liblinear back end (Fan et al., 2008). 1 The example (article 3 of the Universal Declaration of Human Rights) is taken from https://en.wikipedia.org/ wiki/Comparison_of_standard_Bosnian,_Croatian,_Montene"
W16-4802,C12-1160,0,0.179442,"Missing"
W16-4802,W14-5307,0,0.593388,"Missing"
W16-4802,W15-5401,0,0.608357,"both models – linear and deep learning – and compare them in Section 3. The implementations of all models described below are available at https://doi.org/10.5281/zenodo.163812. 2.1 Linear models The results we submitted to the shared task use a multi-class (one-vs-one) support vector machine (SVM) model with linear kernel. Although we experimented with various features, our final model included character ngrams of length one to seven. The features are weighted using sub-linear tf-idf scaling (Jurafsky and Martin, 2009, p.805). The models we describe here are almost identical to the model of Zampieri et al. (2015) in the DSL 2015 shared task. Similar to them, we also experimented with logistic regression as well, using both one-vs-rest and one-vs-one multi-class strategies. In our experiments, different linear models performed comparably. However, the SVM models always performed slightly better than logistic regression models. In this paper, we only describe the SVM models and discuss the results obtained using them. We did not apply any filtering (e.g., case normalization, tokenization) except truncating the input documents to 70 white-space-separated tokens. In all experiments reported in this paper,"
W16-4803,D14-1162,0,0.0840354,"cture where the lower half of the hour glass architecture is used for learning a hidden representation whereas, the upper half of the architecture (a mirror of the lower half) learns to reconstruct the input through back-propagation. The learned intermediate representation of each word is a concise representation its pronunciation. The network is forced to use informationdense representations (by removing or reducing redundant features in the input) to be able to construct the original pronunciation. These representations, which are similar to well-known word embeddings (Mikolov et al., 2013; Pennington et al., 2014) in spirit, can then be used for many tasks. In our case, we use the similarities between these internal vector representations for quantifying the similarities of alternative pronunciations. Although the lengths of the pronunciations vary, the embedding representations are fixed. Hence each pronunciation is mapped to a low dimensional space, Rk , such that similar pronunciations are mapped to close proximity of each other. In this paper, we employ a LSTM based autoencoder (cf. figure 1) to learn an internal representation 26 of a word. The LSTM autoencoder has two parts: encoder and decoder."
W16-4803,W12-0211,1,0.900594,"Missing"
W16-4803,W09-0304,0,0.454855,"Missing"
W17-0404,P13-2017,0,0.0977457,"Missing"
W17-0404,foth-etal-2014-size,0,0.32101,"Missing"
W17-0404,seeker-kuhn-2012-making,0,0.0324346,"¨uBa-D/Z and TIGER treebanks were used in their original form, and as converted dependency treebanks. There have been other constituency-todependency conversion efforts for German treebanks. Bohnet (2003) and Daum et al. (2004) present methods for converting NEGRA to a dependency treebank. Hajiˇc et al. (2009) convert TIGER to a dependency treebank for use in the CoNLL-2009 multi-lingual dependency parsing shared task. The same conversion method is also used in Zeman et al. (2012), again in a multilingual setting, but also with an effort to unify the annotation scheme. In a more recent study, Seeker and Kuhn (2012) convert TIGER to a dependency treebank. They focus on representation of empty nodes in resulting dependency annotations. In all of the earlier studies listed above, with the exception of Zeman et al. (2012), the target dependency treebanks share the tagsets for POS and morphological annotations, and to a large extent the dependency heads already annotated in the source treebank. However, in the present study the morphosyntactic annotations have to diverge, sometimes in non-trivial manner, from the source annotations. We will describe these differences in detail below. 28 Figure 1: An example"
W17-0404,telljohann-etal-2004-tuba,1,0.849413,"Missing"
W17-0404,W04-3231,1,0.751987,"Missing"
W17-1203,I11-1097,0,0.0325053,"PMI value indicates that an alignment of x with y is more likely the result of chance than of shared inheritance. The PMI based computation requires a prior list of plausible cognates for computing a weighted similarity matrix between sound segments. In the initial step, we extract cross-lingual word pairs that have a Levenshtein distance less than 0.5 and treat them as a list of plausible cognates. The PMI estimation procedure is described as followed: Phylogenetic approaches Automatic cognate detection Given a multilingual word list for a concept, the automatic cognate detection procedure (Hauer and Kondrak, 2011) can be broken into two parts: 1. Compute a pairwise similarity score for all word pairs in the concept. 2. Supply the pairwise similarity matrix to a clustering algorithm to output clusters that show high similarity with one another. 1. Compute alignments between the word pairs using a vanilla Needleman-Wunsch algorithm.3 2. The computed alignments from step 1 are then used to compute similarity between segments x, y according to the following formula: Needleman-Wunsch algorithm (NW, Needleman and Wunsch (1970); the similarity counterpart of Levenshtein distance) is a possible choice for comp"
W17-1203,P16-2097,0,0.0306239,"Missing"
W17-1203,W12-0216,0,0.0177904,"o the Gondi language word lists. In the related field of computational historical linguistics, Gray and Atkinson (2003) applied Bayesian phylogenetic methods from computational biology to date the age of Proto-IndoEuropean language tree. The authors use cognate judgments given by historical linguists to infer both the topology and the root age of the IndoEuropean family. In parallel to this work, Kondrak (2009) applied phonetically motivated string similarity measures and word alignment inspired methods for the purpose of determining if two words are cognates or not. This work was followed by List (2012) and Rama (2015) who employed statistical and string kernel methods for determining cognates in multilingual word lists. In typical dialectometric studies (Nerbonne, 2009), the assumption that all the pronunciations of a particular word are cognates is often justified by the data. However, we cannot assume that this is the case in Gondi dialects since there are sigThis paper presents a computational analysis of Gondi dialects spoken in central India. We present a digitized data set of the dialect area, and analyze the data using different techniques from dialectometry, deep learning, and compu"
W17-1203,W16-4803,1,0.648032,"Missing"
W17-1203,N15-1130,1,0.856124,"uage word lists. In the related field of computational historical linguistics, Gray and Atkinson (2003) applied Bayesian phylogenetic methods from computational biology to date the age of Proto-IndoEuropean language tree. The authors use cognate judgments given by historical linguists to infer both the topology and the root age of the IndoEuropean family. In parallel to this work, Kondrak (2009) applied phonetically motivated string similarity measures and word alignment inspired methods for the purpose of determining if two words are cognates or not. This work was followed by List (2012) and Rama (2015) who employed statistical and string kernel methods for determining cognates in multilingual word lists. In typical dialectometric studies (Nerbonne, 2009), the assumption that all the pronunciations of a particular word are cognates is often justified by the data. However, we cannot assume that this is the case in Gondi dialects since there are sigThis paper presents a computational analysis of Gondi dialects spoken in central India. We present a digitized data set of the dialect area, and analyze the data using different techniques from dialectometry, deep learning, and computational biology"
W17-1203,W09-0304,0,0.0861991,"Missing"
W17-1218,W16-4827,0,0.0379573,"Missing"
W17-1218,W16-4819,0,0.0398011,"Missing"
W17-1218,W16-4818,0,0.0470832,"Missing"
W17-1218,W16-4816,0,0.0266935,"Missing"
W17-1218,W16-4820,0,0.130159,"Missing"
W17-1218,W16-4802,1,0.91439,"methods and results for the CLP task as well. Although language identification is a mostly solved problem, closely related languages and dialects still pose a challenge for the language identification systems (Tiedemann and Ljubeˇsi´c, 2012; Zampieri et al., 2014; Zampieri et al., 2015; Zampieri et al., 2017). For this task, we experimented with two different families of models: linear support vector machines (SVM), and (deep) neural network models. For both models we used combination of character and word (n-gram) features. Similar to our earlier experiments in VarDial 2016 shared task (C¸o¨ ltekin and Rama, 2016), the linear models performed better than the neural network models in all language identification tasks. We describe both families of models, and compare the results obtained. In the VarDial 2017 shared task campaign, the DSL and ADI shared tasks had both open and closed track submissions, while GDI had only closed tracks. For all the tasks, we only participate in the closed track. While discriminating closely related languages is a challenge for the language identification task, the similarities can be useful in other tasks. By using information or resources available for a related (source)"
W17-1218,W16-4831,0,0.0424786,"Missing"
W17-1218,P07-2045,0,0.00495217,"Missing"
W17-1218,W16-4828,0,0.0661857,"Missing"
W17-1218,W16-4814,0,0.0402255,"Missing"
W17-1218,W16-4822,0,0.0354655,"Missing"
W17-1218,W16-4823,0,0.033611,"Missing"
W17-1218,D11-1006,0,0.0608221,"Missing"
W17-1218,L16-1284,0,0.043804,"Missing"
W17-1218,W16-4825,0,0.0435552,"Missing"
W17-1218,W16-4815,0,0.0399987,"Missing"
W17-1218,P12-1066,0,0.0130342,"al., 2011; Tiedemann et al., 2014a, just to name a few). Particularly, it has been shown that these approaches tend to perform better than purely unsupervised methods, which can be another natural choice for parsing a language without a treebank. There are two common approaches for transfer parsing. The first one is often called model transfer, which typically involves training a delexicalized parser on the source language treebank, and using it on the target language, with further adaptation or lexicalization with the help of additional monolingual or parallel corpora (McDonald et al., 2011; Naseem et al., 2012). The second method is annotation transfer, which utilizes parallel resources to map the existing annotations for the source language to the target language (Yarowsky et al., 2001; Hwa et al., 2005; Tiedemann, 2014). In this work, we use a straightforward annotationtransfer method using freely available tools. Similar to the language identification, we only participated in the closed track of the CLP task. The remainder of the paper is organized as follows. The next section provides brief descriptions of the tasks and the data sets. Section 3 describes the methods and the systems we used for b"
W17-1218,W16-4830,0,0.0526627,"Missing"
W17-1218,H01-1035,0,0.0187619,"tion task, aims to identify closely related languages or dialects. VarDial 2017 hosted three related language identification tasks: Discriminating between similar languages (DSL) shared task which includes closely related languages in six 146 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 146–155, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics and effort to create. Hence, transferring knowledge from one or more (not necessarily related) languages is studied extensively in some recent work and found to be useful (Yarowsky et al., 2001; Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Tiedemann et al., 2014a, just to name a few). Particularly, it has been shown that these approaches tend to perform better than purely unsupervised methods, which can be another natural choice for parsing a language without a treebank. There are two common approaches for transfer parsing. The first one is often called model transfer, which typically involves training a delexicalized parser on the source language treebank, and using it on the target language, with further adaptation or lexicalization with the help of additional"
W17-1218,W14-5307,0,0.228833,"Missing"
W17-1218,W15-5401,0,0.0944302,"Missing"
W17-1218,W17-1201,0,0.0979115,"Missing"
W17-1218,I08-3008,0,0.0366447,"ed languages or dialects. VarDial 2017 hosted three related language identification tasks: Discriminating between similar languages (DSL) shared task which includes closely related languages in six 146 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 146–155, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics and effort to create. Hence, transferring knowledge from one or more (not necessarily related) languages is studied extensively in some recent work and found to be useful (Yarowsky et al., 2001; Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Tiedemann et al., 2014a, just to name a few). Particularly, it has been shown that these approaches tend to perform better than purely unsupervised methods, which can be another natural choice for parsing a language without a treebank. There are two common approaches for transfer parsing. The first one is often called model transfer, which typically involves training a delexicalized parser on the source language treebank, and using it on the target language, with further adaptation or lexicalization with the help of additional monolingual or parallel corpora (McDonald"
W17-1218,L16-1680,0,0.0672031,"Missing"
W17-1218,W16-4804,0,0.098713,"ADI tasks and used 10-fold cross validation for tuning. We also used 10-fold cross validation for tuning the parameters of the system for the GDI task for which no designated development data was provided. language variety. Although the data is balanced with respect to the number of documents, there is a slight variation with respect to the number of characters and tokens among different language varieties as presented in Table 1. These differences may explain some of the biases towards certain varieties within groups. Further details about the task and the data can be found in Goutte et al. (2016). The ADI data includes transcriptions of speech from five different Arabic varieties. Besides the transcribed words, the ADI data also includes ivectors, fixed-length vectors representing some acoustic properties of whole utterances. The ADI data shows slightly more class imbalance than the DSL data, as shown in Table 2. The lengths of the documents in the ADI data is also more varied. More information on the data and the task can be found in Malmasi et al. (2015). The GDI task includes data from four Swiss German dialects. This data set includes much shorter documents compared to the DSL and"
W17-1218,C12-1160,0,0.0731739,"Missing"
W17-1218,W14-1614,0,0.0270693,"Missing"
W17-1218,tiedemann-2012-parallel,0,0.0113096,"ows slightly more class imbalance than the DSL data, as shown in Table 2. The lengths of the documents in the ADI data is also more varied. More information on the data and the task can be found in Malmasi et al. (2015). The GDI task includes data from four Swiss German dialects. This data set includes much shorter documents compared to the DSL and ADI data sets. The GDI data statistics are also presented in Table 3. 2.2 The source language treebanks are part of the Universal Dependencies (UD) version 1.4 (Nivre et al., 2016). The parallel texts are subtitles from the OPUS corpora collection (Tiedemann, 2012). Cross-lingual parsing The cross lingual parsing tasks involved using one or more source language treebanks along with 148 Language / variety prediction Language / variety classifier Group prediction Group classifier Character features Word features Character embeddings Word embeddings Characters Words Figure 1: The schematic representation of our neural network architecture. Task word DSL ADI GDI 3 3 2 char 7 10 7 In this study, we use both task-specific character and word embeddings to train our model. They are trained during learning to discriminate the languages varieties. As opposed to g"
W17-1218,C14-1175,0,0.0134512,"anguage without a treebank. There are two common approaches for transfer parsing. The first one is often called model transfer, which typically involves training a delexicalized parser on the source language treebank, and using it on the target language, with further adaptation or lexicalization with the help of additional monolingual or parallel corpora (McDonald et al., 2011; Naseem et al., 2012). The second method is annotation transfer, which utilizes parallel resources to map the existing annotations for the source language to the target language (Yarowsky et al., 2001; Hwa et al., 2005; Tiedemann, 2014). In this work, we use a straightforward annotationtransfer method using freely available tools. Similar to the language identification, we only participated in the closed track of the CLP task. The remainder of the paper is organized as follows. The next section provides brief descriptions of the tasks and the data sets. Section 3 describes the methods and the systems we used for both tasks, Section 4 presents our results and we conclude in Section 5 after a brief discussion. 2 variety tokens mean sd bs hr sr 196.53 236.91 209.13 90.80 102.32 97.47 30.86 36.56 33.64 14.18 15.59 15.45 es-ar es"
W17-5028,W17-5007,0,0.0652497,"Missing"
W17-5028,C14-1185,0,0.130693,"Missing"
W17-5028,W16-4802,1,0.891722,"Missing"
W17-5028,W16-4801,0,0.133777,"Missing"
W17-5028,W13-1712,0,0.115745,"Missing"
W17-5028,W17-1201,0,0.135797,"Missing"
W17-5028,D14-1142,0,0.0851991,"LI with a single classifier In this paper, we extracted character n-grams, word n-grams, and word skip-grams from essays and speech transcriptions for training our classifiers. Specifically, we used the following features in our experiments. We used a simple regular expression based tokenizer for extracting words and did not apply any filtering (e.g., case normalization). • Word n-grams: Unigrams and bigrams. • Character n-grams: We extracted character substrings of length from 1–9. • Word skip-grams: We extracted word bigrams by skipping a intermediary word for extracting 1-skip word bigram (Ionescu et al., 2014). For each task, we extracted the following features: • Essays task: Each document is represented as a combination of word and character n-grams which are weighted using sub-linear tf-idf scaling (Jurafsky and Martin, 2009, p.805). • Speech task: We used a combination of ivectors, word and character n-grams (extracted from speech transcriptions). The word/character n-grams are weighted separately using sublinear tf-idf scaling and then combined with the i-vectors. Methodology and Data Task description In this subsection, we provide a description of the three subtasks in NLI shared task 2017 (M"
W17-5028,W13-1714,0,0.22683,"Missing"
W18-3906,W17-1214,0,0.034617,"Missing"
W18-3906,W16-4802,1,0.929872,"German Dialects (Basel, Bern, Lucerne, Zurich). This year’s edition also included a surprise dialect. Finally, the ILI task, another newcomer, is about identifying five closely related Indo-Aryan languages (Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magah). A simple approach to language/dialect identification is to treat it as a text (or document) classification task. Two well-known methods for solving this task are linear classifiers with bag-of-n-gram representations, and, recently popularized, recurrent neural networks. As in our participation at earlier VarDial evaluation campaigns (C ¸ o¨ ltekin and Rama, 2016; C¸o¨ ltekin and Rama, 2017), we experiment with both This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 55 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 55–65 Santa Fe, New Mexico, USA, August 20, 2018. the methods. Although our main participation is based on the SVM models, we also report the performance of the RNN models for comparison, and provide further analyses regarding the feature sets used in the SVM models. We outline our approach in the"
W18-3906,W14-4012,0,0.0919,"Missing"
W18-3906,W17-1221,0,0.333239,"Missing"
W18-3906,J18-3003,0,0.12221,"Missing"
W18-3906,W17-1222,0,0.110842,"Missing"
W18-3906,W17-1220,0,0.257541,"Missing"
W18-3906,W16-4801,0,0.0560333,"Missing"
W18-3906,W17-1219,0,0.129414,"Missing"
W18-3906,W17-5028,1,0.775706,"Missing"
W18-3906,L16-1641,0,0.0638603,"Missing"
W18-3906,W17-1224,0,0.0918073,"Missing"
W18-3906,W14-5307,0,0.0667712,"Missing"
W18-3906,W17-1201,0,0.108151,"Missing"
W18-3906,W18-3901,0,0.111294,"Missing"
W18-3906,S18-1004,1,0.894432,"Missing"
W18-5812,A00-2038,0,0.176453,"netic Vector Representations for Sound Sequence Alignment Pavel Sofroniev and Çağrı Çöltekin Department of Linguistics University of Tübingen pavel.sofroniev@student.uni-tuebingen.de, ccoltekin@sfs.uni-tuebingen.de Abstract either on its own (demonstrating differences between language varieties) or as a necessary step in a larger application, for example, for inferring the cognacy of these words or finding synchronic or diachronic sound correspondences. The use of similarities between the sound segments has been common in computational studies of historical linguistics (Covington, 1996, 1998; Kondrak, 2000; Kondrak and Hirst, 2002; Kondrak, 2003; List, 2012; Jäger, 2013; Jäger and Sofroniev, 2016). These studies rely on scoring functions most of which are based on the linguistic knowledge about the sound changes that typically occur across languages. Another trend shared by all of the earlier studies is the use of a reduced alphabet for representing the sound segments. Even though the standard way to encode sound sequences is the International Phonetic Alphabet (IPA), using a smaller set of symbols, such as ASJP (Brown et al., 2013; Wichmann et al., 2016), seem to help creating scoring function"
W18-5812,J96-4002,0,0.340364,"as (Chollet et al., 2015). The source code used for the experiments reported here is publicly available.3 Experiments and Results Data In order to evaluate the performance of the methods put forward in the previous section, we use the Benchmark Database for Phonetic Alignments (BDPA, List and Prokić, 2014). The database contains 7198 aligned pairs of IPA sequences collected from 12 source datasets, covering languages and dialects from 6 language families (detailed information about the data set is provided in the Appendix). The database also features the small set of 82 selected pairs used by Covington (1996) to evaluate his method, encoded in IPA. Our training data is sourced from NorthEuraLex, a comprehensive lexicostatistical database that provides IPA-encoded lexical data for languages of, primarily but not exclusively, Northern Eurasia (Dellert and Jäger, 2017). At the time of writing the database covers 1016 concepts from 107 languages, resulting in 121 614 IPA transcriptions. 3.2 3.3 Evaluation In order to quantify the methods’ performance, we employ an intuitive evaluation scheme similar to the one used by Kondrak and Hirst (2002): if, for a given word pair, m is the number of alternative"
W18-5812,P98-1043,0,0.397202,"Missing"
W18-5812,list-prokic-2014-benchmark,0,0.0164215,"diction task they perform, not for good alignment performance. The implementation is realized in the Python programming language, and makes use of a number of libraries, including NumPy (Walt et al., 2011), SciPy (Jones et al., 2001), scikit-learn (Pedregosa et al., 2011), Gensim (Řehůřek and Sojka, 2010), and Keras (Chollet et al., 2015). The source code used for the experiments reported here is publicly available.3 Experiments and Results Data In order to evaluate the performance of the methods put forward in the previous section, we use the Benchmark Database for Phonetic Alignments (BDPA, List and Prokić, 2014). The database contains 7198 aligned pairs of IPA sequences collected from 12 source datasets, covering languages and dialects from 6 language families (detailed information about the data set is provided in the Appendix). The database also features the small set of 82 selected pairs used by Covington (1996) to evaluate his method, encoded in IPA. Our training data is sourced from NorthEuraLex, a comprehensive lexicostatistical database that provides IPA-encoded lexical data for languages of, primarily but not exclusively, Northern Eurasia (Dellert and Jäger, 2017). At the time of writing the"
W18-5812,W16-1908,1,0.829037,"ology, and Morphology, pages 111–116 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 phon2vec embeddings are the well-known word2vec method (Mikolov et al., 2013) applied to IPA-encoded phonetic segments. The method learns dense vector representations that maximize the similarity of segments that appear in similar contexts. As in original word2vec models, the context is treated as a bag of words, ignoring the relative position of each context element. earlier research, e.g., for word segmentation (Ma et al., 2016), transfer learning of named entity recognition (Mortensen et al., 2016) and morphological inflection (Silfverberg et al., 2018). We compare our methods to a one-hot-encoding baseline (which is equivalent to symbolic representations), linguistically-motivated vectors, and alignments produced using state-of-the-art scoring methods. We compare the alignment performance of these methods on a manually-annotated goldstandard corpus, using the same alignment algorithm and the same training data where applicable. 2 Position sensitive neural network embeddings (NN embeddings) are obtained using a simp"
W18-5812,C16-1328,0,0.0304745,"31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 phon2vec embeddings are the well-known word2vec method (Mikolov et al., 2013) applied to IPA-encoded phonetic segments. The method learns dense vector representations that maximize the similarity of segments that appear in similar contexts. As in original word2vec models, the context is treated as a bag of words, ignoring the relative position of each context element. earlier research, e.g., for word segmentation (Ma et al., 2016), transfer learning of named entity recognition (Mortensen et al., 2016) and morphological inflection (Silfverberg et al., 2018). We compare our methods to a one-hot-encoding baseline (which is equivalent to symbolic representations), linguistically-motivated vectors, and alignments produced using state-of-the-art scoring methods. We compare the alignment performance of these methods on a manually-annotated goldstandard corpus, using the same alignment algorithm and the same training data where applicable. 2 Position sensitive neural network embeddings (NN embeddings) are obtained using a simple feed-forward neural network architecture. Similar to word2vec skip-gr"
W18-5812,W18-0314,0,0.0290413,"ional Morphology and Phonology https://doi.org/10.18653/v1/P17 phon2vec embeddings are the well-known word2vec method (Mikolov et al., 2013) applied to IPA-encoded phonetic segments. The method learns dense vector representations that maximize the similarity of segments that appear in similar contexts. As in original word2vec models, the context is treated as a bag of words, ignoring the relative position of each context element. earlier research, e.g., for word segmentation (Ma et al., 2016), transfer learning of named entity recognition (Mortensen et al., 2016) and morphological inflection (Silfverberg et al., 2018). We compare our methods to a one-hot-encoding baseline (which is equivalent to symbolic representations), linguistically-motivated vectors, and alignments produced using state-of-the-art scoring methods. We compare the alignment performance of these methods on a manually-annotated goldstandard corpus, using the same alignment algorithm and the same training data where applicable. 2 Position sensitive neural network embeddings (NN embeddings) are obtained using a simple feed-forward neural network architecture. Similar to word2vec skip-gram method, the neural network tries to predict the conte"
W18-5903,D17-1322,0,0.272917,"-Esparza et al., 2008; Coppersmith et al., 2015). Most of the earlier works have been focused on analyzing the language used by depressed individuals, and/or finding linguistic correlates of the depression. A more applicable approach to monitoring public or individual mental health requires explicit identification of depression from the linguistic samples. Such an application can complement the conventional diagnosis methods, and, if proven successful, it can be useful for diagnosis where conventional methods are not applicable. Similar to some of the recent studies (Coppersmith et al., 2015; Yates et al., 2017; Lynn et al., 2018), our aim in this paper is identifying depression from linguistic data. Using (mainly) corpora we gather from the social media platform Reddit, we experiment with a number of different classification models. Our focus here is on selection of corpora for reliable and generalizable analysis or identification of depression from the social media data. This paper presents a set of classification experiments for identifying depression in posts gathered from social media platforms. In addition to the data gathered previously by other researchers, we collect additional data from th"
W18-5903,W16-4802,1,0.88832,"Missing"
W18-5903,S18-1004,1,0.855263,"Missing"
W18-5903,W15-1204,0,0.0547319,"Missing"
W18-5914,W17-1218,1,0.815303,"combined in a flat manner as a single text-feature matrix. We experimented with two feature weighting methods: tf-idf (Jurafsky and Martin, 2009, p.805) and BM25 (Robertson et al., 2009). The weighted features are then used for training an SVM classifier. We used one-vs-rest multi-class strategy when training the SVM classifier for task 2. All models were implemented in Python, using scikit-learn machine learning library (Pedregosa et al., 2011). The models are similar to the models we used in a few other text classification tasks (C¸o¨ ltekin and Rama, 2018; Rama and C¸o¨ ltekin, 2017; C¸o¨ ltekin and Rama, 2017), where the models are explained in detail. We tuned the models for each task separately, changing the maximum order of character and word n-gram features, case normalization, and SVM margin parameter ‘C’. The parameter ranges explored during tuning was 0–12 for maximum character n-gram order, 0–7 for maximum word n-gram order, and 0.1–2.0 with steps of 0.1 for ‘C’. We used 5-fold cross validation during tuning, using random search through the space of hyIntroduction The increasing use of social media platforms world wide offers an interesting application of natural language processing tools f"
W18-5914,S18-1004,1,0.827104,"certain upper limit (specified below). All features are combined in a flat manner as a single text-feature matrix. We experimented with two feature weighting methods: tf-idf (Jurafsky and Martin, 2009, p.805) and BM25 (Robertson et al., 2009). The weighted features are then used for training an SVM classifier. We used one-vs-rest multi-class strategy when training the SVM classifier for task 2. All models were implemented in Python, using scikit-learn machine learning library (Pedregosa et al., 2011). The models are similar to the models we used in a few other text classification tasks (C¸o¨ ltekin and Rama, 2018; Rama and C¸o¨ ltekin, 2017; C¸o¨ ltekin and Rama, 2017), where the models are explained in detail. We tuned the models for each task separately, changing the maximum order of character and word n-gram features, case normalization, and SVM margin parameter ‘C’. The parameter ranges explored during tuning was 0–12 for maximum character n-gram order, 0–7 for maximum word n-gram order, and 0.1–2.0 with steps of 0.1 for ‘C’. We used 5-fold cross validation during tuning, using random search through the space of hyIntroduction The increasing use of social media platforms world wide offers an inter"
W18-5914,W18-5904,0,0.0868648,"m features, case normalization, and SVM margin parameter ‘C’. The parameter ranges explored during tuning was 0–12 for maximum character n-gram order, 0–7 for maximum word n-gram order, and 0.1–2.0 with steps of 0.1 for ‘C’. We used 5-fold cross validation during tuning, using random search through the space of hyIntroduction The increasing use of social media platforms world wide offers an interesting application of natural language processing tools for monitoring public health and health-related events on the social media. The social media mining for health applications (SMM4H) shared task (Weissenbacher et al., 2018) hosts four tasks aiming to identify mentions of different aspects medication use on Twitter. Briefly, the tasks and their descriptions are: Task 1: Automatic detection of posts mentioning drug names. Task 2: Automatic classification of posts describing medication intake. Task 3: Automatic classification of adverse drug reaction mentioning posts. Task 4: Automatic detection of posts mentioning vaccination behavior. All tasks, except Task 2 are binary classification tasks. Task 2 requires three-way classification, including an uncertain class indicating posts mentioning possible medication inta"
W18-6002,W15-5301,0,0.0522692,"Missing"
W19-1406,W17-1221,0,0.0328896,"language identification. However, the MRC subtasks on cross-lingual topic identification can be solved by the very same text classification models used for language identification. Hence, we also participated in the crosslingual classification subtasks of the MRC. Our base model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks"
W19-1406,W18-3919,0,0.0277747,"Missing"
W19-1406,W19-1409,0,0.0928757,"Missing"
W19-1406,W18-3918,0,0.036109,"Missing"
W19-1406,W18-3929,0,0.148547,"Missing"
W19-1406,W18-3925,0,0.0252646,"Missing"
W19-1406,W18-3907,0,0.252123,"Missing"
W19-1406,P19-1068,0,0.169686,"t is assigned to only one class. Named entities in the data set were anonymized. The training data for subtask 1 consisted of 21 701 texts with a slight class imbalance (11 740 Romanian, 9 961 Moldavian), with a development set of 11 834 instances approximately following the same class distribution. Training sets for subtasks 1 and 2 included 9 961 and 11 740 texts, and development sets included 5 432 and 6 402 texts, respectively. All subtasks shared a test set of 5 918 texts, although subtasks 2 and 3 were evaluated on subsets of the test set. Further information on the data can be found in Butnaru and Ionescu (2019). 3 3.1 SVM with flat combinations of features For all tasks, we submitted predictions generated by SVM classifiers where a range of overlapping character and word n-grams are combined into a single feature matrix. The features are weighted using BM25, although a plain tf-idf weighing scheme produced similar results on the development set. In all tasks, we optimized the model hyperparameters through random search, using 5fold cross validation on combined training and development sets. Random search was stopped after approximately 1 000 draws from the space of random parameters, and picking the"
W19-1406,W16-4820,0,0.073915,"Missing"
W19-1406,W18-3906,1,0.808923,"tures are weighted using BM25, although a plain tf-idf weighing scheme produced similar results on the development set. In all tasks, we optimized the model hyperparameters through random search, using 5fold cross validation on combined training and development sets. Random search was stopped after approximately 1 000 draws from the space of random parameters, and picking the best average F1-score over the 5 folds. This is simply the same approach taken in a series of earlier VarDial evaluation campaigns (C¸o¨ ltekin and Rama, 2016; Rama and C¸o¨ ltekin, 2017; C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin et al., 2018). Following the adaptation idea used by Jauhiainen et al. (2018a,b) in last year’s VarDial evaluation campaign, we also employed an adaptation approach in some of the tasks. At test time, we produced a set of first-level predictions based on the best model tuned for the task on the training/development set, and retrained the model after adding the predictions with high-confidence to the training set. In our case, predictions with high-confidence means the test instances that are farther than a threshold — in this case, 0.50 — from the decision boundary for binary classification, and the instan"
W19-1406,W16-4802,1,0.944957,"ly different task than language identification. However, the MRC subtasks on cross-lingual topic identification can be solved by the very same text classification models used for language identification. Hence, we also participated in the crosslingual classification subtasks of the MRC. Our base model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language"
W19-1406,W15-5407,0,0.0565362,"i-vectors with the n-gram vectors weighted Methods and experimental setting Our main submissions were based on two SVM systems that differ in the way they combine the n-gram features: SVM with flat feature combinations and SVM ensembles. We employed both character and word n-gram features. Depending on the task, the character n-grams varied between 1 to 9 and the word n-grams varied from 1 to 3. The features were weighted with either tf-idf or BM25 (Robertson et al., 2009) weighting schemes. The flat combination is similar to C¸o¨ ltekin and Rama (2016) and the ensemble approach is similar to Malmasi and Dras (2015). Both methods were implemented in Python using the scikit-learn library (Pedregosa et al., 2011). We also experimented with recurrent neural classifiers and considered a system similar to HeLi 56 by BM25, before feeding them to the SVM classifiers. As SVMs are sensitive to the scale of the data, we introduced a weight parameter and searched for its optimum value during tuning. 3.2 SVM ensembles SVM ensembles are generally considered more robust than single classifiers (Oza and Tumer, 2008). An ensemble system makes use of decisions from multiple classifiers on every input entity. The decision"
W19-1406,Y96-1018,0,0.037805,"performance variations based on different system designs. A text in traditional Chinese can always be transformed verbatim into its simplified counterpart without any content change and vice versa. Two corpora, one using traditional script and one simplified, were provided to investigate the performance of the discrimination task on the two different scripts, which will be further discussed in Section 5. The DMT data comes from the news domain for both varieties. The datasets contained a training and development set for both simplified Chinese (McEnery and Xiao, 2003) and traditional Chinese (Chen et al., 1996). The training set consisted of 18 770 samples for both Chinese varieties, whereas the development set contained 2 000 samples each. The texts contained no punctuation and were (automatically) segmented by the task organizers. the overlapping n-gram features, we also used an ensemble approach in some of the tasks, providing a tentative comparison between these two related methods. An interesting result of last year’s VarDial evaluation campaign was SUKI team’s success on Indo-Aryan language identification (Jauhiainen et al., 2018b) and GDI (Jauhiainen et al., 2018a) tasks with a rather large m"
W19-1406,J18-3003,0,0.0258219,"se model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the simplified variant of Discriminating between Mainland and Taiwan"
W19-1406,W17-1222,0,0.0209615,"pated in the crosslingual classification subtasks of the MRC. Our base model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the"
W19-1406,W18-3933,0,0.0328104,"Missing"
W19-1406,W17-1220,0,0.208984,"pated in the crosslingual classification subtasks of the MRC. Our base model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the"
W19-1406,W18-3901,0,0.201625,"Missing"
W19-1406,W16-4801,0,0.0609109,"Missing"
W19-1406,W17-1219,0,0.0468098,"ver, the MRC subtasks on cross-lingual topic identification can be solved by the very same text classification models used for language identification. Hence, we also participated in the crosslingual classification subtasks of the MRC. Our base model is a linear support vector machine (SVM) classifier with sparse character and word n-gram features. These models have been found to be successful in earlier instances of VarDial language identification tasks; in fact, they were found to be more effective than more recent neural classifiers (C¸o¨ ltekin and Rama, 2016; Clematide and Makarov, 2017; Medvedeva et al., 2017). A successful variation of these linear classifiers is an ensemble of classifiers with different n-gram orders used both for language discrimination (Malmasi and Zampieri, 2017b,a), and native language identification (Malmasi and Dras, 2018). Besides the simple, ‘flat’ concatenation of This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks, as well as the Moldavia"
W19-1406,W14-0505,1,0.806158,"ensemble systems are particularly effective when the individual classifiers are independent, and features from text and audio provide more independent predictions in comparison to the overlapping n-gram features.1 4.3 CLI We submitted predictions using only the flat feature combination for the cuneiform language identification task. Our submission with adaptation came in a close second with an F1-score of 76.32. Since the data did not include any word boundaries, our system combines only character n-grams (of order 1 to 5). We also experimented with two unsupervised segmentation methods (C¸o¨ ltekin and Nerbonne, 2014; Virpioja et al., 2013). However, GDI The same models used for DMT were slightly modified for the German Dialect Identification task. Our flat model using character n-grams of order 1 to 5, word unigrams and bigrams, and the i-vector features achieved first place with an F1score of 75.93, which was very closely followed by the second and third place entries. The confusion matrix presented in Figure 1 demonstrates that Basel was most easily identified (recall: 91.99). Lucerne was the dialect most of1 Our official score on the test with the flat combination is higher than the ensemble submissio"
W19-1406,W17-5028,1,0.892243,"Missing"
W19-1406,L16-1641,0,0.0632067,"Missing"
W19-1406,W17-1201,0,0.157707,"Missing"
W19-1416,D18-1366,0,0.0212729,"dels can take advantage of labeled data in source languages to predict the morphological analysis in a similar target language. While the results presented here are competitive with others obtained in this shared task, the analysis scores are admittedly low. However, there are multiple ways to improve the results as our models do not incorporate much in terms of cross-lingual signal. In the future, it would be worth integrating this cross-lingual signal in the form of pretrained cross-lingual word embeddings (Artetxe et al., 2016; Lample et al., 2018) or sub-word, e.g., character, embeddings (Chaudhary et al., 2018; Sofroniev and Çöltekin, 2018), as this could lead to better generalization to new languages. Similarly, typological distance between source and target language often correlates with performance (Cotterell and Heigold, 2017), which could be exploited for weighting the contribution of source-language examples when learning a multilingual model. TRK Similar to the ROA development set, both our models make fewer predictions on average than the gold standard predictions provided for Crimean Tatar. As noted in Section 3 the (optimum) linear model makes only a single prediction for each of the 999"
W19-1416,D14-1179,0,0.0177595,"Missing"
W19-1416,N15-1151,0,0.0500058,"to find. A potential solution to aid developing morphological analysis tools is to use unsupervised methods. Earlier attempts to develop unsupervised morphological analysis tools, mostly within Morpho Challenge shared tasks (Kurimo et al., 2010), returned rather mixed, often sub-optimal results (see Hammarström and Borin, 2011, for a survey). Another approach for obtaining morphological analyses for languages without a morphological analyzer is based on transfer learning, which has become a widespread approach in NLP and related disciplines rather recently (Yarowsky et al., 2001; Faruqui and Kumar, 2015; Johnson et al., 2017; Barnes et al., 2018). The general idea is to train a supervised machine learning model that predicts analyses of word forms in a target language using gold-standard analyses that exist in other related languages. This paper describes Tübingen-Oslo team’s participation in the cross-lingual morphological analysis task in the VarDial 2019 evaluation campaign. We participated in the shared task with a standard neural network model. Our model achieved analysis F1-scores of 31.48 and 23.67 on test languages KarachayBalkar (Turkic) and Sardinian (Romance) respectively. The sco"
W19-4209,N15-1107,0,0.0182546,"s. For example, given the German lemma aufgeben ‘to give up’ and the morphological tags {V.PTCP, PST}, the task is to predict the inflected form aufgegeben (morphological tags are described in McCarthy et al., 2019; Kirov et al., 2018). Traditionally, finitestate methods (Koskenniemi, 1985) are used for morphological generation (and analysis). Since such systems typically require man-months of expert work, and difficult to maintain and adapt to changes in the language, data driven approaches to inflection generation have recently become popular (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The task is further popularized by the past three SIGMORPHON morphological (re)inflection shared tasks (Cotterell et al., 2016, 2017, 2018). The primary focus of the task tackled in this paper, the task 1 of the present SIGMORPHON shared task (McCarthy et al., 2019), is the cross-lingual transfer 71 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 71–79 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics ing, the data or resources that exist for a related language are leveraged to imp"
W19-4209,N13-1138,0,0.0161683,"ord based on its lemma and morphological features. For example, given the German lemma aufgeben ‘to give up’ and the morphological tags {V.PTCP, PST}, the task is to predict the inflected form aufgegeben (morphological tags are described in McCarthy et al., 2019; Kirov et al., 2018). Traditionally, finitestate methods (Koskenniemi, 1985) are used for morphological generation (and analysis). Since such systems typically require man-months of expert work, and difficult to maintain and adapt to changes in the language, data driven approaches to inflection generation have recently become popular (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The task is further popularized by the past three SIGMORPHON morphological (re)inflection shared tasks (Cotterell et al., 2016, 2017, 2018). The primary focus of the task tackled in this paper, the task 1 of the present SIGMORPHON shared task (McCarthy et al., 2019), is the cross-lingual transfer 71 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 71–79 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics ing, the data or resources that exist"
W19-4209,W16-2004,0,0.0267272,"phological tags, and the output produced so far. The general idea is similar to transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) where the aim is to predict the parsing action in a given state of the parser. The similar ideas were used in the past for morphological inflection generation as well. The system presented here is most similar to the baseline system of SIGMORPHON 2016 shared task (Cotterell et al., 2016), and also shares many aspects of the inflection generation systems that follow an align-and-transduce strategy in the earlier SIGMORPHON shared tasks (e.g., Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representations, such as the parser state in transition based parsing, or hidden representations learned in a recurrent neural network. 2.1 e e a u a f u g f e g g e e b b e e n n a a u u f f g e g g e e b b e e n n Figure 1: Example alignments of two lemma–form pairs from Turkish (top) and German (bottom). In each box, upper part shows the alignment based on longest common substring, while lower part shows minimum edit distance solution. ally assigned weights based on linguistic knowledge/intuitions ("
W19-4209,N15-1151,0,0.019612,"erts, and maybe even native speakers, are hard to come by. As past SIGMORPHON shared tasks demonstrated, however, satisfactory results in the morphological inflection task requires relatively large amount of data. The low-resource settings in earlier SIGMORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings. A potential solution to this problem, the focus of the current inflection shared task, is cross-lingual or transfer learning, which has been demonstrated to be useful a number of language processing tasks (e.g., Yarowsky et al., 2001; Faruqui and Kumar, 2015; Johnson et al., 2017; Barnes et al., 2018). In cross-lingual learnThis paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation. Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set. The performance of the system does not reach the performance of the top two systems in the competition."
W19-4209,P18-1231,0,0.0171908,"s, are hard to come by. As past SIGMORPHON shared tasks demonstrated, however, satisfactory results in the morphological inflection task requires relatively large amount of data. The low-resource settings in earlier SIGMORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings. A potential solution to this problem, the focus of the current inflection shared task, is cross-lingual or transfer learning, which has been demonstrated to be useful a number of language processing tasks (e.g., Yarowsky et al., 2001; Faruqui and Kumar, 2015; Johnson et al., 2017; Barnes et al., 2018). In cross-lingual learnThis paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation. Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set. The performance of the system does not reach the performance of the top two systems in the competition. However, we show that results can be improv"
W19-4209,W19-1416,1,0.829044,"ritten with a non-Latin script to some version of the Latin script. However, the standard methods are often designed for easy reading/phonetization by English speakers. Even in cases of target languages that use a version of the Latin script, there are significant differences to hinder cross-lingual learning considerably. As a result, we report below some of the experiments with transliterations between Latin and Cyrillic scripts for only eight language pairs (all Turkic languages) to demonstrate the potential gains that can be obtained with transliteration. The transliteration method follows Çöltekin and Barnes (2019). The method does not follow any transliteration standards (e.g., one set by ISO), but tries to maximize the similarities of the writing traditions in these particular languages. 2.4 The shared task data used in this study consists of 100 language pairs, which is described in detail in McCarthy et al. (2019). Here we only provide a basic overview that is relevant to our discussion below. All language pairs feature a high-resource training set from the source language, a low-resource training set and a development set, both from the target language. Number of unique source and target languages"
W19-4209,K18-3001,0,0.0394452,"Missing"
W19-4209,K17-2001,0,0.107154,"Missing"
W19-4209,W16-2010,0,0.0380311,"Missing"
W19-4209,L18-1293,0,0.0365387,"Missing"
W19-4209,W04-2407,0,0.0162586,"ta or resources that exist for a related language are leveraged to improve the learning in low-resource setting. The method we use for cross-lingual learning is rather simple. We only use the (related) high-resource language as additional training data. 2 r r i i m y e e k c e k l e r e e r r i i m y e c e e k k l e r The method The inflection systems in this study operate by predicting a number of transduction actions based on current position in the lemma, morphological tags, and the output produced so far. The general idea is similar to transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) where the aim is to predict the parsing action in a given state of the parser. The similar ideas were used in the past for morphological inflection generation as well. The system presented here is most similar to the baseline system of SIGMORPHON 2016 shared task (Cotterell et al., 2016), and also shares many aspects of the inflection generation systems that follow an align-and-transduce strategy in the earlier SIGMORPHON shared tasks (e.g., Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representations, such as the par"
W19-4209,W16-2006,0,0.0117137,"general idea is similar to transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) where the aim is to predict the parsing action in a given state of the parser. The similar ideas were used in the past for morphological inflection generation as well. The system presented here is most similar to the baseline system of SIGMORPHON 2016 shared task (Cotterell et al., 2016), and also shares many aspects of the inflection generation systems that follow an align-and-transduce strategy in the earlier SIGMORPHON shared tasks (e.g., Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representations, such as the parser state in transition based parsing, or hidden representations learned in a recurrent neural network. 2.1 e e a u a f u g f e g g e e b b e e n n a a u u f f g e g g e e b b e e n n Figure 1: Example alignments of two lemma–form pairs from Turkish (top) and German (bottom). In each box, upper part shows the alignment based on longest common substring, while lower part shows minimum edit distance solution. ally assigned weights based on linguistic knowledge/intuitions (Sofroniev and Çöltekin, 2018). We tried a"
W19-4209,W18-5812,1,0.809869,"; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representations, such as the parser state in transition based parsing, or hidden representations learned in a recurrent neural network. 2.1 e e a u a f u g f e g g e e b b e e n n a a u u f f g e g g e e b b e e n n Figure 1: Example alignments of two lemma–form pairs from Turkish (top) and German (bottom). In each box, upper part shows the alignment based on longest common substring, while lower part shows minimum edit distance solution. ally assigned weights based on linguistic knowledge/intuitions (Sofroniev and Çöltekin, 2018). We tried a few of these more informed weighted alignment methods. However, in preliminary experiments, a simple alignment mechanism based on longest common substring (LCS) worked best. Hence, in all the experiments reported here, alignments are performed first by finding the longest common substring of lemma and the word from, and aligning the two strings such that the LCS is aligned correctly. The rest of the characters are aligned disregarding whether they match or not. The method introduces gaps only at the beginning and end of the sequences. If there are two matching substrings of equal"
W19-4209,W09-0304,0,0.0358614,"roduces gaps only at the beginning and end of the sequences. If there are two matching substrings of equal length, we pick the first sequence. Alignment During training, we need to determine the goldstandard transduction actions, which requires aligning the lemmas and word forms. Better sequence alignment is one of the concerns for the similar inflection systems cited above, as well as the sequence-to-sequence models that operate with hard monotonic attention. Better alignments are also a common concern and studied extensively in other areas of computational linguistics such as dialectometry (Wieling et al., 2009; Prokić, 2010) and historical linguistics (List, 2012; Jäger, 2013). Standard alignment algorithms that use equal penalties for edit operations often fail to capture the similarities and differences between characters (or phonetic segments). As a result, often a weighted method is used such that similar characters in one of the sequences are more likely to be aligned with the similar characters in the other. The weights are most often learned from the data using an unsupervised method. The data-driven weights are found to be more effective than manuFigure 1 presents two example alignments bas"
W19-4209,P19-1148,0,0.0403829,"ke up of the data. Not all language pairs are close enough to facilitate the transfer learning. However, there are many possible directions for exploiting the cross-lingual signal better. The simple method used in this study can be improved in many ways. Although our initial experiments were not successful. We believe cross-lingual character embeddings, and ’translating’ transduction actions from source language to target language may be potential ways to get a better cross-lingual input. Table 2: Overall results obtained by our systems in comparison to the official state-of-the-art baseline (Wu and Cotterell, 2019). The scores are word-form accuracy and mean edit distance (MED) averaged over all 100 language pairs. The rows marked with asterisk indicate post-evaluation scores obtained using the linear predictor, after fixing a bug and further tuning. performing systems, the post-evaluation fixes and tuning results in a dramatic increase in the performance of the linear model. The more interesting result, however, is the small difference between the transfer learning results and the ‘target only’ results. We present the target-only and transfer accuracy scores for each language pair in Figure 2. In gener"
W19-4209,W03-3023,0,0.0247892,"onal Linguistics ing, the data or resources that exist for a related language are leveraged to improve the learning in low-resource setting. The method we use for cross-lingual learning is rather simple. We only use the (related) high-resource language as additional training data. 2 r r i i m y e e k c e k l e r e e r r i i m y e c e e k k l e r The method The inflection systems in this study operate by predicting a number of transduction actions based on current position in the lemma, morphological tags, and the output produced so far. The general idea is similar to transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) where the aim is to predict the parsing action in a given state of the parser. The similar ideas were used in the past for morphological inflection generation as well. The system presented here is most similar to the baseline system of SIGMORPHON 2016 shared task (Cotterell et al., 2016), and also shares many aspects of the inflection generation systems that follow an align-and-transduce strategy in the earlier SIGMORPHON shared tasks (e.g., Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representat"
W19-4209,H01-1035,0,0.32668,"or lowresource languages, where experts, and maybe even native speakers, are hard to come by. As past SIGMORPHON shared tasks demonstrated, however, satisfactory results in the morphological inflection task requires relatively large amount of data. The low-resource settings in earlier SIGMORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings. A potential solution to this problem, the focus of the current inflection shared task, is cross-lingual or transfer learning, which has been demonstrated to be useful a number of language processing tasks (e.g., Yarowsky et al., 2001; Faruqui and Kumar, 2015; Johnson et al., 2017; Barnes et al., 2018). In cross-lingual learnThis paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation. Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set. The performance of the system does not reach the performance of the top two sy"
W19-4209,N15-1093,0,0.0177317,"morphological features. For example, given the German lemma aufgeben ‘to give up’ and the morphological tags {V.PTCP, PST}, the task is to predict the inflected form aufgegeben (morphological tags are described in McCarthy et al., 2019; Kirov et al., 2018). Traditionally, finitestate methods (Koskenniemi, 1985) are used for morphological generation (and analysis). Since such systems typically require man-months of expert work, and difficult to maintain and adapt to changes in the language, data driven approaches to inflection generation have recently become popular (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The task is further popularized by the past three SIGMORPHON morphological (re)inflection shared tasks (Cotterell et al., 2016, 2017, 2018). The primary focus of the task tackled in this paper, the task 1 of the present SIGMORPHON shared task (McCarthy et al., 2019), is the cross-lingual transfer 71 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 71–79 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics ing, the data or resources that exist for a related languag"
W19-4209,W16-2005,0,0.0188178,"t produced so far. The general idea is similar to transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) where the aim is to predict the parsing action in a given state of the parser. The similar ideas were used in the past for morphological inflection generation as well. The system presented here is most similar to the baseline system of SIGMORPHON 2016 shared task (Cotterell et al., 2016), and also shares many aspects of the inflection generation systems that follow an align-and-transduce strategy in the earlier SIGMORPHON shared tasks (e.g., Alegria and Etxeberria, 2016; Nicolai et al., 2016; Liu and Mao, 2016). Our current models do not make use of any hidden representations, such as the parser state in transition based parsing, or hidden representations learned in a recurrent neural network. 2.1 e e a u a f u g f e g g e e b b e e n n a a u u f f g e g g e e b b e e n n Figure 1: Example alignments of two lemma–form pairs from Turkish (top) and German (bottom). In each box, upper part shows the alignment based on longest common substring, while lower part shows minimum edit distance solution. ally assigned weights based on linguistic knowledge/intuitions (Sofroniev and Çöltekin"
W19-7809,N18-1090,0,0.23705,"hem with data-driven tools, we are in the process of annotating Turkish–German transcriptions with part-of-speech, morphology, and dependency layers. We have chosen Universal Dependencies (UD) (Nivre et al., 2016) as our annotation scheme. The UD project aims to define morphosyntactic annotation guidelines that are consistent across languages. Its unified tag sets and annotation standards facilitate the annotation of multiple languages within a single treebank. Furthermore, annotations parallel to monolingual resources are useful for making use of these resources, e.g., for transfer learning (Bhat et al., 2018). Despite clear advantages of the UD framework for annotating CS treebanks, the annotation of multiple languages in a single treebank needs additional considerations that have not been studied before. Although there has been a few UD treebanks with code-switching (Bhat et al., 2018; Partanen et al., 2018), the papers describing these treebanks do not document or discuss the code-switching aspects of the annotation process. In this paper we address this gap and outline some of the challenges and interesting phenomena that surface during the annotation of a Turkish–German code-switching treebank"
W19-7809,L16-1667,1,0.910124,"Missing"
W19-7809,L16-1248,0,0.173856,"ross-lingual consistency within our multilingual treebank. None of the treebanks noted above include spoken language, let alone code-switching. Quite a few UD treebanks, on the other hand, contain spoken language partially (Danish DDT, Greek GDT, Latvian LVTB, Persian Seraji, Polish LFG, Swedish LinES) or fully (Cantonese HK, Chinese HK, French Spoken, Naija NSC, Norwegian NynorskLIA, Slovenian SST). These treebanks have extended the UD dependency relations with subtypes in addition to using the existing ones to cover linguistic phenomena mainly observed in speech. For example, Slovenian SST (Dobrovoljc and Nivre, 2016) annotates correcting disfluencies either with reparandum or parataxis:restart. Another parataxis subtype, parataxis:discourse is defined to cover sentential parentheticals with fixed semantics that serve as discourse elements (e.g., you know). French Spoken (Gerdes and Kahane, 2017) and Naija NSC (Courtin et al., 2018) employ the same tag too. They define a separate tag parataxis:dislocated for clauses that precede the sentence they are dislocated from. The other relation that is commonly extended is discourse. Slovenian SST separates filler sounds from other discourse elements and assigns th"
W19-7809,foth-etal-2014-size,0,0.0313392,"3). Turkish GB is a manually annotated treebank consisting of grammar book examples (Çöltekin, 2015). There are PUD treebanks consisting of parallel (translated) sentences for both languages. The PUD treebanks were automatically converted from another dependency scheme for the CoNLL 2017 multi-lingual parsing shared task (Zeman et al., 2017). The first German UD treebank is the GSD treebank (McDonald et al., 2013), which is also automatically converted from a different dependency formalism. There are also two new additions to German treebanks; HDT, a conversion of Hamburg Dependency Treebank (Foth et al., 2014; Hennig and Köhn, 2017), and LIT, a treebank of German literary history. Most of our annotation decisions and the discussions below are based on the version 2.3 of the UD treebanks, particularly Turkish IMST and German GSD. There are, however, inconsistencies across languages, and across treebanks of the same language. For most annotation decisions, we follow the annotations in the monolingual treebanks as much of possible. In case of inconsistencies across treebanks, our policy is to choose the alternative closest to the general UD guidelines, so as to ensure cross-lingual consistency within"
W19-7809,W16-1715,0,0.0302172,"aim any consistency with the annotations of the monolingual Russian UD treebank. Similar to these treebanks, we also assign a language ID to each token following the tag set in Çetino˘glu (2016). Many other treebanks include words or phrases from a foreign language. Most of them mark foreign tokens with Foreign=Yes, and annotate the internal structure of foreign phrases with flat relations. However, a few treebanks, e.g., Irish IDT (Lynn and Foster, 2016), annotate foreign tokens according to their respective language. 3 Annotations Any annotation project is bound to make non-trivial choices (Gerdes and Kahane, 2016). Most non-trivial choices for a code-switching treebank comes either because of the multilingual nature of the resource, or, as noted earlier, the fact that code-switching is prevalent in informal language, and annotation of informal or spoken language has been more challenging than more standard/written language. Most of the problems related to multilingual nature of the data stem from different annotations choices established for individual languages. Although one of the main motivations behind the UD project is multilingual consistency across treebanks, multilingualism within a treebank ha"
W19-7809,W17-0407,0,0.0162281,"manually annotated treebank consisting of grammar book examples (Çöltekin, 2015). There are PUD treebanks consisting of parallel (translated) sentences for both languages. The PUD treebanks were automatically converted from another dependency scheme for the CoNLL 2017 multi-lingual parsing shared task (Zeman et al., 2017). The first German UD treebank is the GSD treebank (McDonald et al., 2013), which is also automatically converted from a different dependency formalism. There are also two new additions to German treebanks; HDT, a conversion of Hamburg Dependency Treebank (Foth et al., 2014; Hennig and Köhn, 2017), and LIT, a treebank of German literary history. Most of our annotation decisions and the discussions below are based on the version 2.3 of the UD treebanks, particularly Turkish IMST and German GSD. There are, however, inconsistencies across languages, and across treebanks of the same language. For most annotation decisions, we follow the annotations in the monolingual treebanks as much of possible. In case of inconsistencies across treebanks, our policy is to choose the alternative closest to the general UD guidelines, so as to ensure cross-lingual consistency within our multilingual treeba"
W19-7809,C82-1023,0,0.562841,"resting phenomena that surface during the annotation of a Turkish–German code-switching treebank. Our contributions are in two levels. The observations on code-switching, independent of the annotation scheme, help in understanding in what forms it occurs. The annotation solutions we propose explore how to handle CS within the UD framework. Working with spoken data brings another aspect and opens also speech annotation under UD to discussion. 2 Related Work Many well-known linguistic theories on CS syntax, e.g. Free Morpheme and Equivalence Constraints (Poplack, 1980), Closed-class Constraint (Joshi, 1982), Matrix Language Frame (Myers-Scotton, 1993), Functional Head Constraint (Belazi et al., 1994) define their formalism and constraints on constituency structures . Eppler (2005) argues that these constraints are too restrictive from a data-driven perspective and favours Word Grammar (Hudson, 1990), a dependency-based formalism, where the scope of the constraints is head-dependent pairs. Her annotations on German–English transcriptions and the Chinese– English treebank (Wang and Liu, 2013), which also follows Word Grammar, are the only CS dependency treebanks that do not follow UD to the best o"
W19-7809,W16-5403,0,0.0214982,"se is defined to cover sentential parentheticals with fixed semantics that serve as discourse elements (e.g., you know). French Spoken (Gerdes and Kahane, 2017) and Naija NSC (Courtin et al., 2018) employ the same tag too. They define a separate tag parataxis:dislocated for clauses that precede the sentence they are dislocated from. The other relation that is commonly extended is discourse. Slovenian SST separates filler sounds from other discourse elements and assigns them discourse:filler. Norwegian NynorskLIA (Øvrelid and Hohle, 2016) follows the same approach. Cantonese HK and Chinese HK (Leung et al., 2016) define discourse:sp for sentence particles common in spoken language. So far we are more conservative in extending relations with subtypes and have introduced one that is described in Section 3.2. The Hindi-English UD treebank (Bhat et al., 2018) annotates the mixed language of social media and has no extension to UD dependencies. The major annotation augmentation is the language IDs assigned to each token. Komi-Zyrian IKDP (Partanen et al., 2018) consists of spoken language, and some utterances include Russian phrases. In those utterances mixed and Russian tokens are marked with respective l"
W19-7809,W18-6015,0,0.152699,"lines that are consistent across languages. Its unified tag sets and annotation standards facilitate the annotation of multiple languages within a single treebank. Furthermore, annotations parallel to monolingual resources are useful for making use of these resources, e.g., for transfer learning (Bhat et al., 2018). Despite clear advantages of the UD framework for annotating CS treebanks, the annotation of multiple languages in a single treebank needs additional considerations that have not been studied before. Although there has been a few UD treebanks with code-switching (Bhat et al., 2018; Partanen et al., 2018), the papers describing these treebanks do not document or discuss the code-switching aspects of the annotation process. In this paper we address this gap and outline some of the challenges and interesting phenomena that surface during the annotation of a Turkish–German code-switching treebank. Our contributions are in two levels. The observations on code-switching, independent of the annotation scheme, help in understanding in what forms it occurs. The annotation solutions we propose explore how to handle CS within the UD framework. Working with spoken data brings another aspect and opens als"
W19-7809,D18-1344,0,0.0186471,"s to the German syntax as well, where the noun should be plural, when switching to German on the surface. Such CS-specific constructions vary from non-canonical morphological marking to creating new syntactic representations, to applying a linguistic phenomenon of one language to the other. They make structural analysis of code-switching linguistically interesting and computationally challenging. Several approaches tackle these challenges by utilising labelled and unlabelled monolingual and parallel data, e.g. by creating artificial CS data and using them in training models for processing CS (Pratapa et al., 2018; Zhang et al., 2018). However to be able to capture unique cases like the singular-to-plural mapping for ‘week’ in (1), those models need to see such instances. Thus, to observe the characteristics of CS and address them with data-driven tools, we are in the process of annotating Turkish–German transcriptions with part-of-speech, morphology, and dependency layers. We have chosen Universal Dependencies (UD) (Nivre et al., 2016) as our annotation scheme. The UD project aims to define morphosyntactic annotation guidelines that are consistent across languages. Its unified tag sets and annotation"
W19-7809,D18-1030,0,0.0138181,"as well, where the noun should be plural, when switching to German on the surface. Such CS-specific constructions vary from non-canonical morphological marking to creating new syntactic representations, to applying a linguistic phenomenon of one language to the other. They make structural analysis of code-switching linguistically interesting and computationally challenging. Several approaches tackle these challenges by utilising labelled and unlabelled monolingual and parallel data, e.g. by creating artificial CS data and using them in training models for processing CS (Pratapa et al., 2018; Zhang et al., 2018). However to be able to capture unique cases like the singular-to-plural mapping for ‘week’ in (1), those models need to see such instances. Thus, to observe the characteristics of CS and address them with data-driven tools, we are in the process of annotating Turkish–German transcriptions with part-of-speech, morphology, and dependency layers. We have chosen Universal Dependencies (UD) (Nivre et al., 2016) as our annotation scheme. The UD project aims to define morphosyntactic annotation guidelines that are consistent across languages. Its unified tag sets and annotation standards facilitate"
W19-7809,L16-1250,0,0.0200437,"ith reparandum or parataxis:restart. Another parataxis subtype, parataxis:discourse is defined to cover sentential parentheticals with fixed semantics that serve as discourse elements (e.g., you know). French Spoken (Gerdes and Kahane, 2017) and Naija NSC (Courtin et al., 2018) employ the same tag too. They define a separate tag parataxis:dislocated for clauses that precede the sentence they are dislocated from. The other relation that is commonly extended is discourse. Slovenian SST separates filler sounds from other discourse elements and assigns them discourse:filler. Norwegian NynorskLIA (Øvrelid and Hohle, 2016) follows the same approach. Cantonese HK and Chinese HK (Leung et al., 2016) define discourse:sp for sentence particles common in spoken language. So far we are more conservative in extending relations with subtypes and have introduced one that is described in Section 3.2. The Hindi-English UD treebank (Bhat et al., 2018) annotates the mixed language of social media and has no extension to UD dependencies. The major annotation augmentation is the language IDs assigned to each token. Komi-Zyrian IKDP (Partanen et al., 2018) consists of spoken language, and some utterances include Russian phrase"
