2008.eamt-1.18,W03-2008,0,0.212445,"focal position detecting circuit. A sentence of this length and complexity is difficult to process even for native speakers of English, let alone for foreigners who do not master English well. Given that professionals have to sift through the claims of a large number of patents returned as response to a search in a patent DB (which makes a quick assessment of the relevance of patent essential), it is not surprising that multilingual summarization of patent claims is very attractive to practitioners in the field. Nonetheless, only little work has been done so far in the area; cf. as an example [1], who proposes a reading aid based on the segmentation of claims into smaller and simpler sentences. The focus has been on the machine translation – especially in the light of the recently dramatically increased prominence of patents in languages not widely spoken in the West (e.g., Korean and Chinese). As far as summarization of patent material is concerned, up to date, the overwhelming share of it is manual.1 One explication for this unsatisfactory state of affairs is that the peculiarities of the genre of patent claims require new approaches to summarization: the application of surface leve"
2008.eamt-1.18,W97-0713,0,0.0775597,"orithms are not able to cope with a reasonable outcome with sentences of such a length, a prior two-step simplification procedure of the original is needed: (i) segmentation into simpler chunks and (ii) repair of chunks which are not grammatical clauses by introducing missing constituents or referential links, or by modifying available constituents. The output of the simplification can serve for two extraction based summarization strategies: (a) discourse structure oriented summarization; (b) syntactic structure oriented summarization. Discourse structure oriented summarization as proposed by [3] uses the depth of the subtree “controlled” by an element of a discourse relation in the sense of the Rhetorical Structure Theory [4] – under the assumption that the nucleus of a relation controls an elementary tree formed by the nucleus and satellite of a relation. See [5] for the application of this strategy to the summarization of patent claims. The syntactic structure based summarization often uses syntactic dependency criteria which indicate the importance of syntactic tree branches, drawing on dependency relations [6,7]. To the best of our knowledge, the syntax oriented strategy has not"
2008.eamt-1.18,W03-2007,0,0.353926,"ng constituents or referential links, or by modifying available constituents. The output of the simplification can serve for two extraction based summarization strategies: (a) discourse structure oriented summarization; (b) syntactic structure oriented summarization. Discourse structure oriented summarization as proposed by [3] uses the depth of the subtree “controlled” by an element of a discourse relation in the sense of the Rhetorical Structure Theory [4] – under the assumption that the nucleus of a relation controls an elementary tree formed by the nucleus and satellite of a relation. See [5] for the application of this strategy to the summarization of patent claims. The syntactic structure based summarization often uses syntactic dependency criteria which indicate the importance of syntactic tree branches, drawing on dependency relations [6,7]. To the best of our knowledge, the syntax oriented strategy has not been applied so far to patents. In PATExpert, three different summarization strategies are implemented: (i) a strategy based on the claim structure, (ii) a strategy based on the discourse structure, and (iii) a strategy based on the deep-syntactic (or shallow semantic) stru"
2008.eamt-1.18,W00-1436,1,0.744236,"marization is most suitable for multilingual summarization. It presupposes two preprocessing stages: (a) claim 122 12th EAMT conference, 22-23 September 2008, Hamburg, Germany dependency structure determination, simplification, and discourse analysis, and (b) full parsing of the simplified claim sentences. For parsing, we use MINIPAR [8]. Despite some shortcomings such as systematic right-attachment, we chose MINIPAR since it produces syntactic structures which roughly correspond to the Surface Syntactic Structures (SSyntSs) of the linguistic framework underlying the linguistic workbench MATE [9] we use for generation: the Meaning-Text Theory, MTT [10]. The summarization and multilingual transfer stages are performed on the DeepSyntactic Structures (DSyntSs) of the MTT, such that prior to these stages, the MINIPAR structures are mapped onto SSyntSs and the SSyntSs onto DSyntSs; for details on the preprocessing stages, see [11]. The abstract nature of the DSyntS, which eliminates the surface-syntactic idiosyncrasies of the linguistic constructions, ensures, on the one hand, quasi-semantic criteria for summarization, and, on the other hand, simplified transfer between the structures of"
2008.eamt-1.18,mille-wanner-2008-making,1,0.911241,"[8]. Despite some shortcomings such as systematic right-attachment, we chose MINIPAR since it produces syntactic structures which roughly correspond to the Surface Syntactic Structures (SSyntSs) of the linguistic framework underlying the linguistic workbench MATE [9] we use for generation: the Meaning-Text Theory, MTT [10]. The summarization and multilingual transfer stages are performed on the DeepSyntactic Structures (DSyntSs) of the MTT, such that prior to these stages, the MINIPAR structures are mapped onto SSyntSs and the SSyntSs onto DSyntSs; for details on the preprocessing stages, see [11]. The abstract nature of the DSyntS, which eliminates the surface-syntactic idiosyncrasies of the linguistic constructions, ensures, on the one hand, quasi-semantic criteria for summarization, and, on the other hand, simplified transfer between the structures of different languages; cf., e.g., [12]. 3 Multilingual Summarization of Patent Claims Starting from the DSyntSs of the simplified claims, the multilingual summarization of patents consists of the following steps: (1) summarization of the original claims, (2) transfer of DSyntS of the source language to the target language, (3) generation"
2008.eamt-1.18,W04-1013,0,0.00474356,"Missing"
2008.eamt-1.18,J85-2001,0,0.247219,"-23 September 2008, Hamburg, Germany tongue produced by PATExpert with summarization switched off (such that only simplification, transfer and regeneration were effective) against the online-Google translation of the original claims as baseline.5 Given that the recall of our multilingual generator is still very much hampered by the shortage of multilingual resources, we consider this evaluation a general indication of the potential of “deep” translation techniques when combined with the preprocessing of the claims. The evaluation was based on a questionnaire which has been largely inspired by [14]. It consists of three categories: “intelligibility”, “simplicity” and “accuracy”. The first two deal with the quality of the transferred text; both have a five value scale. The third category, which has a seven value scale, captures how the content from the English input is conveyed in the transferred text. Due to the lack of space, we do not cite here the questionnaire itself. Table 1 shows the accuracy regarding each of the three quality categories for PATExpert and the baseline. Table 1. Accuracy of the PATExpert Multilingual Summarizer against a baseline Intelligibility Simplicity Accurac"
2008.eamt-1.18,lenci-etal-2002-multilingual,0,0.0229069,"one hand, to perform the summarization at a rather abstract level and thus to use “deep” summarization criteria, and, on the other hand, to reduce the transfer to a large extent to lexical transfer. The results are encouraging. Still, the three central components involved in the process: summarization, transfer and generation, are continuously being extended and improved, such that in the full paper, we will be able to present evaluation figures that are likely to be considerably superior to those presented above. There are some related works. The most similar ones are the MUSI-summarizer by [15] and the summarizer within VERBMOBIL described in [16]. As our strategy, 5 Since our goal was to evaluate the multilingual output of our system with the original claims as input, we consider it correct to run the Google translator on the original claims. 128 12th EAMT conference, 22-23 September 2008, Hamburg, Germany MUSI implies a deep analysis stage and a regeneration stage. However, MUSI’s summarization strategy consists in sentence extraction using surface-oriented criteria (cue phases and positions of sentences). The analysis is applied to the extracted sentences and the resulting syntac"
2008.eamt-1.18,W00-1420,0,0.0305217,"stract level and thus to use “deep” summarization criteria, and, on the other hand, to reduce the transfer to a large extent to lexical transfer. The results are encouraging. Still, the three central components involved in the process: summarization, transfer and generation, are continuously being extended and improved, such that in the full paper, we will be able to present evaluation figures that are likely to be considerably superior to those presented above. There are some related works. The most similar ones are the MUSI-summarizer by [15] and the summarizer within VERBMOBIL described in [16]. As our strategy, 5 Since our goal was to evaluate the multilingual output of our system with the original claims as input, we consider it correct to run the Google translator on the original claims. 128 12th EAMT conference, 22-23 September 2008, Hamburg, Germany MUSI implies a deep analysis stage and a regeneration stage. However, MUSI’s summarization strategy consists in sentence extraction using surface-oriented criteria (cue phases and positions of sentences). The analysis is applied to the extracted sentences and the resulting syntactic structures are mapped onto conceptual representati"
2008.eamt-1.20,J90-2002,0,0.304879,"in the target language. The unit of the target language that appears more frequently across the sets of multi-word units is usually the correct translation of the initial single-word source language entry. Keywords: Bilingual Lexicon Extraction, Specialized Terminology, Machine Translation, Corpus Linguistics, Knowledge-poor methods, statistical methods. 1 Introduction Strategies that involve the use of parallel corpora were among the first attempts to extract bilingual lexicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for t"
2008.eamt-1.20,H91-1026,0,0.152781,"he target language. The unit of the target language that appears more frequently across the sets of multi-word units is usually the correct translation of the initial single-word source language entry. Keywords: Bilingual Lexicon Extraction, Specialized Terminology, Machine Translation, Corpus Linguistics, Knowledge-poor methods, statistical methods. 1 Introduction Strategies that involve the use of parallel corpora were among the first attempts to extract bilingual lexicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for transl"
2008.eamt-1.20,W95-0114,0,0.0757426,"to extract bilingual lexicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for translation in the target language. The rationale behind this strategy is that both the source language word and its equivalent are supposed to share the same profile of co-occurrence, in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents wit"
2008.eamt-1.20,W97-0119,0,0.0127103,"t bilingual lexicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for translation in the target language. The rationale behind this strategy is that both the source language word and its equivalent are supposed to share the same profile of co-occurrence, in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help"
2008.eamt-1.20,P99-1067,0,0.0562384,"ingual lexicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for translation in the target language. The rationale behind this strategy is that both the source language word and its equivalent are supposed to share the same profile of co-occurrence, in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help of t"
2008.eamt-1.20,1999.tmi-1.11,0,0.0632851,"exicons, using measures of statistical association to study the cooccurrence of pairs of entries in the aligned sentences ([1]; [2]; among others). This methodology has yielded accurate results. However, the shortcoming is that parallel corpora are not easy to compile, particularly in the case of specialized domains. There have also been a number of attempts to extract bilingual lexicons without the need of parallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for translation in the target language. The rationale behind this strategy is that both the source language word and its equivalent are supposed to share the same profile of co-occurrence, in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help of the initi"
2008.eamt-1.20,J86-2003,0,0.0293083,"arallel corpora, but using bilingual dictionaries as seed words. In this line of research there are two main trends. The first one is represented by authors such as [3]; [4]; [5]; [6] and [7]. Briefly, most of these approaches involve a similarity metric between a word in the source language and a candidate for translation in the target language. The rationale behind this strategy is that both the source language word and its equivalent are supposed to share the same profile of co-occurrence, in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help of the initial bilingual lexicon. Once this information is gathered, the next step is to select as a candidate for translation the unit of the target language that co-occurs more often (in some corpus) with the greatest number of the translations obtained with the bilingual lexicon. The other trend in the literature ([11]; [12]) is more related to the present proposal. In order to obtain documents where equivalents co-oc"
2008.eamt-1.20,W01-1413,0,0.0159443,"in the same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help of the initial bilingual lexicon. Once this information is gathered, the next step is to select as a candidate for translation the unit of the target language that co-occurs more often (in some corpus) with the greatest number of the translations obtained with the bilingual lexicon. The other trend in the literature ([11]; [12]) is more related to the present proposal. In order to obtain documents where equivalents co-occur, [11] exploited search engines using pairs of equivalent terms in Japanese and English obtained from a bilingual dictionary as queries. This yields bilingual glossaries as well as other partially parallel texts among the downloaded collection. In the case of [12], they mine English-to-Chinese bilingual translations and transliterations from monolingual Chinese Web pages. Their idea is to extract equivalent pairs searching for a pattern of an English expression enclosed by parenthesis in a C"
2008.eamt-1.20,2007.mtsummit-papers.9,0,0.0242449,"same 140 12th EAMT conference, 22-23 September 2008, Hamburg, Germany manner that synonyms do ([8]; [9]; [10]). The process is then to study the units that co-occur significantly with the input word, and then try to translate as many as possible co-occurrents with the help of the initial bilingual lexicon. Once this information is gathered, the next step is to select as a candidate for translation the unit of the target language that co-occurs more often (in some corpus) with the greatest number of the translations obtained with the bilingual lexicon. The other trend in the literature ([11]; [12]) is more related to the present proposal. In order to obtain documents where equivalents co-occur, [11] exploited search engines using pairs of equivalent terms in Japanese and English obtained from a bilingual dictionary as queries. This yields bilingual glossaries as well as other partially parallel texts among the downloaded collection. In the case of [12], they mine English-to-Chinese bilingual translations and transliterations from monolingual Chinese Web pages. Their idea is to extract equivalent pairs searching for a pattern of an English expression enclosed by parenthesis in a Chinese"
2020.lrec-1.126,N15-1042,1,0.744855,"terlinked theme-rheme (also referred to as topic–focus) sequence, such that the reader/listener can follow the development of the narration easily.1 The linkage follows specific, theoretically wellstudied, theme-rheme patterns referred to as thematic progression (Daneˇs, 1974). Thematic progression determines the organization of the discourse and is, therefore, of great relevance to a number of natural language processing (NLP) applications. However, while the clausal theme–rheme structure, also known as Information Structure, has been used in natural language generation (Wanner et al., 2003; Ballesteros et al., 2015), text summarization (Bouayad-Agha et al., 2012) and even prosody generation (Dom´ınguez et al., 2017), the automatic detection of thematic progression patterns in discourse, let alone the potential of thematic progression patterns in downstream applications, has not been explored yet. Among the few who have tackled the topic of thematic progression under a somewhat broader angle (and under different headings) that includes, e.g., anaphoric links, are, for instance, Kruijff-Korbayov´a and Kruijff (1996) and Hajiˇcov´a and M´ırovsk´y (2018). In this paper, we present an implementation that auto"
2020.lrec-1.126,I13-1178,1,0.72151,"and curly brackets respectively. The label of the span is placed when the bracket is closed. Any span may also include further embedded spans; for instance, a specifier may be subdivided into theme and rheme. Consequently, both proposition and thematicity spans may contain different levels of thematicity. (2) [After nine months]SP1 [a little boy {[who]T1 [was named Johny]R1 }P1.1]T1 [was born]R1. In our implementation, we follow the interpretation of thematicity as defined by Mel’ˇcuk. The conventions for annotating thematicity, explained in broad terms above, are explained in more detail in (Bohnet et al., 2013). In what follows, we will exemplify this annotation schema for the reader to have a grasp of the fundamentals of thematicity annotation in connection with thematic progression. An example of thematicity annotation in a longer text is provided as example (3). The fragment is the beginning of the tale number 1 from the evaluation corpus described in Section 3.2.2.3 The title and first paragraph of this tale have been included in the fragment annotated with thematicity; cf. example (3). Sentences have been numbered from 1 to 5 (S1 to S5) to make a clearer reference to these units in the text. On"
2020.lrec-1.126,L18-1259,0,0.0629779,"Missing"
2020.lrec-1.126,D14-1162,0,0.0879123,"ty structure is predicted for each sentence using a thematicity parser. This dimension is introduced as the seventh column in the CoNLL file. 4. The thematic progression is computed selecting a set of contiguous spans and a hypertheme to establish the TP of a given theme labeled as such by the thematicity parser. Theme spans may include open class words or pronouns (personal pronouns, demonstrative pronouns, relative pronouns, etc.). Two different strategies are devised to deal with each of these categories, namely: (a) Open class words: a word2vec representation (using Glove word embeddings (Pennington et al., 2014) 8 ) is derived for each word of the span, and its centroid is computed. The cosine similarity between the current span and the candidate reference spans (hypertheme, previous theme and previous rheme, if exists) are computed and the distance is displayed in the graph representation of the thematic progression. In the future, we plan the use of distance thresholds to prune themes and to only link closely-related elements. (b) Pronouns (personal and demonstrative pronouns) are treated with co-reference resolution (using the SpaCy extension neuralcoref.9 ) Only candidate spans are included in th"
2020.lrec-1.126,W03-2315,1,0.61609,"hich implements an interlinked theme-rheme (also referred to as topic–focus) sequence, such that the reader/listener can follow the development of the narration easily.1 The linkage follows specific, theoretically wellstudied, theme-rheme patterns referred to as thematic progression (Daneˇs, 1974). Thematic progression determines the organization of the discourse and is, therefore, of great relevance to a number of natural language processing (NLP) applications. However, while the clausal theme–rheme structure, also known as Information Structure, has been used in natural language generation (Wanner et al., 2003; Ballesteros et al., 2015), text summarization (Bouayad-Agha et al., 2012) and even prosody generation (Dom´ınguez et al., 2017), the automatic detection of thematic progression patterns in discourse, let alone the potential of thematic progression patterns in downstream applications, has not been explored yet. Among the few who have tackled the topic of thematic progression under a somewhat broader angle (and under different headings) that includes, e.g., anaphoric links, are, for instance, Kruijff-Korbayov´a and Kruijff (1996) and Hajiˇcov´a and M´ırovsk´y (2018). In this paper, we present"
2020.lrec-1.838,S19-2007,0,0.082426,"Missing"
2020.lrec-1.838,S19-2131,1,0.765579,"category definitions. That is, definitions should be more similar to the definition of hate speech in the Waseem dataset (Waseem and Hovy, 2016), misogyny in the Amievalita dataset (Fersini et al., 2018) and hate speech, misogyny and aggression in the Hateval (Basile et al., 2019) than to definitions in (Davidson et al., 2017; Jigsaw, 2019a)). • Position new categories in the map of existing categories when annotating new datasets, for instance, by following a similar method to the one provided in this 6792 work. • Develop hierarchical multiclass annotation schemas, as already called for in (Fortuna et al., 2019; Zampieri et al., 2019a; Jigsaw, 2019b). Multiclass schemas facilitate the development of targeted classifiers for the different types of pejorative online behavior. Generic classifiers, trained on binary annotation schemas, do not address the coverage of fine-grained categories (such as, e.g., ’threat’). • Collect information that allows to control possible dataset bias, such as, e.g., the profile of the author of the message. Cf. (Arango et al., 2019), where it is shown that hate speech related datasets are collected from a limited set of authors, such that when messages from the same autho"
2020.lrec-1.838,W19-3510,1,0.842505,"category definitions. That is, definitions should be more similar to the definition of hate speech in the Waseem dataset (Waseem and Hovy, 2016), misogyny in the Amievalita dataset (Fersini et al., 2018) and hate speech, misogyny and aggression in the Hateval (Basile et al., 2019) than to definitions in (Davidson et al., 2017; Jigsaw, 2019a)). • Position new categories in the map of existing categories when annotating new datasets, for instance, by following a similar method to the one provided in this 6792 work. • Develop hierarchical multiclass annotation schemas, as already called for in (Fortuna et al., 2019; Zampieri et al., 2019a; Jigsaw, 2019b). Multiclass schemas facilitate the development of targeted classifiers for the different types of pejorative online behavior. Generic classifiers, trained on binary annotation schemas, do not address the coverage of fine-grained categories (such as, e.g., ’threat’). • Collect information that allows to control possible dataset bias, such as, e.g., the profile of the author of the message. Cf. (Arango et al., 2019), where it is shown that hate speech related datasets are collected from a limited set of authors, such that when messages from the same autho"
2020.lrec-1.838,W18-4401,0,0.231944,"ons of key categories is a critical issue. The authors argue that researchers use different, sometimes theoretically ambiguous or misleading terms for equivalent categories. Thus, ‘abusive’ has been defined based on the speakers’ intention to harm, which cannot always be determined by just looking at the content. Furthermore, definitions also make assumptions on the effect of the messages on the reader, which, obviously, depends entirely on the personality of the reader. The authors conclude that accurately defining key terms will result in better communication and collaboration in the field. Kumar et al. (2018) also point out that there is a large amount of terminology as well as different understandings of this terminology in the context of abusive speech. The fact that there are so many different definitions and interpretations of the same terms results in duplicated research, lack of clear goals and difficulties in reusing the data. The authors stress that it is of utmost importance that a common understanding of the problem is achieved such that standard datasets and different compatible approaches to solve the problem are developed. In another recent study (Swamy et al., 2019), it is also highl"
2020.lrec-1.838,L18-1008,0,0.0122984,"ggle insult severe toxic obscene identity hate threat toxic standardized category misoginy-sexism racism hate speech hate speech offensive toxicity misoginy-sexism hate speech aggressive hate speech overt aggression covert aggression aggression insult severe toxic obscene hate speech threat toxicity Table 3: Category standardization. their similarity to the other categories and their homogeneity, i.e., variation of the samples of one single category. Each category is represented as a centroid vector using Fast Text (Bojanowski et al., 2016) and pretrained word embeddings trained on wikipedia (Mikolov et al., 2018). We decided to follow this approach because the majority of the datasets contain short texts generated on social networks and Fast Text along with pretrained word embeddings has been providing good results in diverse works applied to the automatic detection of hate speech and related concepts with similar data; see, e.g., (Santucci et al., 2018; Fortuna and Nunes, 2019). The process that we use to compute the aforementioned centroids is as follows: • Pre-process the messages by lowercasing all words, removing IPs, Twitter elements such as hashtags, usernames, and stop words using NLTK. • Trai"
2020.lrec-1.838,D14-1162,0,0.0989458,"ection 3.3.. 2.4. Experiment 2: Classifying with Perspective API In order to analyse the generalization potential of a stateof-the-art model over the considered datasets and their categories, we use Perspective API. Perspective API was created by Jigsaw and Google’s Counter Abuse Technology team in the context of the Conversation-AI project. The API provides several classifiers that compute scores between 0 and 1 for different categories (among others, ‘toxicity’ (Jigsaw, 2019a)), given an input text. The classifier uses Convolutional Neural Networks (CNNs) trained with GloVe word embeddings (Pennington et al., 2014) finetuned during training on data from online sources such as Wikipedia and The New York Times. Perspective API provides the following definitions of the relevant categories: • toxicity is a “rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.” • severe toxicity is a “very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective.” • Compute the average of every message centroid that belongs to each category, obtaining the centroid of each category. • identity attack"
2020.lrec-1.838,K19-1088,0,0.197048,"Missing"
2020.lrec-1.838,W19-3509,0,0.319562,"t number of scientific publications focused on two different (albeit related) tasks: (i) the compilation and annotation of corpora and (ii) the automatic detection of different types of offensive speech, among them, e.g., toxicity, hate, abuse, using generic state-of-the-art (i.e., machine learning-based) natural language processing techniques. However, critics on this duality are starting to emerge in the community. The main concern resides in the restricted generalization potential of the trained machine learning (ML) models for classification of offensive speech. For instance, Swamy et al. (2019) find that by training on top of BERT models (Devlin et al., 2018), it is possible to obtain a language model that performs very competitively for different datasets, however, the generalization of the model depends highly on the training data. The authors hypothesize that a model will generalize better if it is used on data that is more similar to the data used for training, and worse if the data contains more non-offensive samples. In general, the limited generalization potential is a problem that can have two different origins. Firstly, online offensive speech varies depending on the target"
2020.lrec-1.838,N16-2013,0,0.684377,"s. 6787 Dataset id Classes Data Collection Strategy Number of Instances Source Reference Waseem Mutually Exclusive Classes no racism, sexism 16.914 Twitter Davidson yes hate speech, offensive Initial search based on common slurs and terms used pertaining to religious, sexual, gender, and ethnic minorities. Begining with the hatebase lexicon. 24.802 Twitter Amievalita partially Search with representative slurs, monitoring of potential victims’ and perpetrators’ accounts. 4.000 Twitter Hateval no misogynous, discredit, sexual harassment, stereotype, dominance, derailing hate speech, aggression (Waseem and Hovy, 2016) (Davidson et al., 2017) (Fersini et al., 2018) Against immigrants and women. 9.000 Twitter TRAC yes covert aggression, overt aggression 12.000 Facebook Toxkaggle no threat, identity hate, severe toxic, insult, obscene, toxic Searching for keywords and constructions that are often included in offensive messages, such as “she is”, “antifa”, “conservatives”. Not provided 159.571 Wikipedia (Basile et al., 2019) (Zampieri et al., 2019b) (Jigsaw, 2019b) Table 1: Dataset properties dataset id Waseem definitions “Hate speech: 1. uses a sexist or racial slur. 2. attacks a minority. 3. seeks to silence"
2020.lrec-1.838,N19-1144,0,0.0865003,"al victims’ and perpetrators’ accounts. 4.000 Twitter Hateval no misogynous, discredit, sexual harassment, stereotype, dominance, derailing hate speech, aggression (Waseem and Hovy, 2016) (Davidson et al., 2017) (Fersini et al., 2018) Against immigrants and women. 9.000 Twitter TRAC yes covert aggression, overt aggression 12.000 Facebook Toxkaggle no threat, identity hate, severe toxic, insult, obscene, toxic Searching for keywords and constructions that are often included in offensive messages, such as “she is”, “antifa”, “conservatives”. Not provided 159.571 Wikipedia (Basile et al., 2019) (Zampieri et al., 2019b) (Jigsaw, 2019b) Table 1: Dataset properties dataset id Waseem definitions “Hate speech: 1. uses a sexist or racial slur. 2. attacks a minority. 3. seeks to silence a minority. 4. criticizes a minority (without a well founded argument). 5. promotes, but does not directly use, hate speech or violent crime. 6. criticizes a minority and uses a straw man argument. 7. blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims. 8. shows support of problematic hash tags. E.g. #BanIslam, #whoriental, #whitegenocide 9. negatively stereotypes a minority. 10. defends xe"
2020.lrec-1.838,S19-2010,0,0.170204,"al victims’ and perpetrators’ accounts. 4.000 Twitter Hateval no misogynous, discredit, sexual harassment, stereotype, dominance, derailing hate speech, aggression (Waseem and Hovy, 2016) (Davidson et al., 2017) (Fersini et al., 2018) Against immigrants and women. 9.000 Twitter TRAC yes covert aggression, overt aggression 12.000 Facebook Toxkaggle no threat, identity hate, severe toxic, insult, obscene, toxic Searching for keywords and constructions that are often included in offensive messages, such as “she is”, “antifa”, “conservatives”. Not provided 159.571 Wikipedia (Basile et al., 2019) (Zampieri et al., 2019b) (Jigsaw, 2019b) Table 1: Dataset properties dataset id Waseem definitions “Hate speech: 1. uses a sexist or racial slur. 2. attacks a minority. 3. seeks to silence a minority. 4. criticizes a minority (without a well founded argument). 5. promotes, but does not directly use, hate speech or violent crime. 6. criticizes a minority and uses a straw man argument. 7. blatantly misrepresents truth or seeks to distort views on a minority with unfounded claims. 8. shows support of problematic hash tags. E.g. #BanIslam, #whoriental, #whitegenocide 9. negatively stereotypes a minority. 10. defends xe"
2020.msr-1.1,W13-3520,0,0.0265768,". Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track are UD structures as in T1 from"
2020.msr-1.1,P11-2040,0,0.0129219,"R’18 (Mille et al., 2018) and SR’19 (Mille et al., 2019). The evaluation method is Direct Assessment (DA) (Graham et al., 2016), as used by the WMT competitions to produce the official ranking of machine translation systems (Barrault et al., 2020) and video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019). We ran the evaluation on Mechanical Turk,6 assessing two quality criteria, in separate evaluation experiments, but using the same method: Readability and Meaning Similarity. We used continuous sliders as rating tools, the evidence being that raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the gray text’), was:7,8 The meaning of the gray text is adequately expressed by t"
2020.msr-1.1,W11-2832,0,0.0178935,"of the SR’20 tracks, data and evaluation methods, as well as brief summaries of the participating systems. Full descriptions of the participating systems can be found in separate system reports elsewhere in this volume. 1 Introduction SR’20 is the fourth in a line of shared tasks focused on surface realisation, the name originally given to the last stage in the first-generation (pre-statistical and pre-neural) Natural Language Generation (NLG) pipeline, mapping from semantic representations to fully realised surface word strings. When we ran the first Surface Realisation Shared Task in 2011 (Belz et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al.,"
2020.msr-1.1,K17-3005,0,0.0265528,"tense, verbal finiteness, etc.). The test data sets can be grouped into three types: (i) in-domain test data, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some histori"
2020.msr-1.1,W19-8652,0,0.0338279,"map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back o"
2020.msr-1.1,W18-3604,1,0.862544,"redicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1, and 13 for T2 (for a summary overview, see Table 2, top 3 sections of the table). The datasets were selected from the available collection of 4"
2020.msr-1.1,D19-1055,0,0.0142412,"ear, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system"
2020.msr-1.1,D19-6304,0,0.216682,"er proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs word order restoration by learning Interpreted Regular Tree Grammar (IRTG) rules encoding the correspondence between UD-subgraphs and word orderings. The grammars build strings and UD graphs simultaneously, using pairs of operations each connecting a set of dependents to their common head while concatenating the corresponding words. Rule 3 Data type Dataset Track train dev test In-domain arabic_padt (ar) chinese_gsd (zh) english_ewt (en) english_gum (en) english_lines (en) english_partut (en) french_gsd (fr) french_partut (fr) french_sequoia (fr) hindi_hdtb (hi) indonesian_gsd (id) j"
2020.msr-1.1,2020.acl-main.703,0,0.0144601,"mmon head while concatenating the corresponding words. The approach extends the team’s 2019 system by allowing rules to reference lemmas in addition to POS-tags and by giving preference to derivations that use a smaller number of more specific rules to construct a particular UD graph. Word order restoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augm"
2020.msr-1.1,K18-2014,0,0.0243276,"ta, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some historical figures. On the extracted sentences, we applied extensive filtering to achieve reasonably good text qu"
2020.msr-1.1,D17-1262,1,0.833898,"look at improvements this year compared to 2019, we see for instance, on the English_ewt test set, last year’s top BLEU score in T1 (the Shallow Track) was 82.98 (IMS); in 2020, it goes up to 86.16 in the restricted track (IMS), and 87.5 in the open track (ADAPT). In T2 (the Deep Track), top BLEU scores also increased, from 54.75 (IMS) to 58.84 in the restricted track, and 58.66 in the unrestricted track (both IMS). We next look at overall improvements of team submissions across all test sets they submitted outputs bias into the evaluation revealing no significant evidence of reference-bias (Ma et al., 2017). 8 ADAPT 20a 20b –BLEU-4– T1_ar_padt T1_en_ewt T2_en_ewt T1_en_gum T2_en_gum T1_en_lines T2_en_lines T1_en_partut T2_en_partut T1_es_ancora T2_es_ancora T1_es_gsd T2_es_gsd T1_fr_gsd T2_fr_gsd T1_fr_partut T2_fr_partut T1_fr_sequoia T2_fr_sequoia T1_hi_hdtb T1_id_gsd T1_ja_gsd T1_ko_gsd T1_ko_kaist T1_pt_bosque T1_pt_gsd T1_ru_gsd T1_ru_syntagrus T1_zh_gsd BME 20a 19 26 57.25 26.4 59.22 60.77 57.57 55.98 48.78 57.96 61.37 59.32 61.09 54.6 53.74 43.21 43.8 52.46 49.17 45.25 46.72 57.2 59.16 50.89 58.37 57.05 39.89 30.68 54.28 54.79 50.58 63.63 54.22 49.53 46.08 47.23 39.53 30.39 54.58 50.91 58"
2020.msr-1.1,W17-3517,1,0.829222,"et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al., 2017), Universal Dependencies (UDs) had emerged as a convenient standard in parsing, with many associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisatio"
2020.msr-1.1,W18-3601,1,0.732438,"ny associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Mult"
2020.msr-1.1,D19-6301,1,0.741899,"ets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are"
2020.msr-1.1,W15-4719,0,0.0195724,"rojective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1,"
2020.msr-1.1,P02-1040,0,0.108533,"-20-multilingual 2 5 Figure 1: Sample UD structure (without the last two columns). Figure 2: Sample T1 input structure (without the last two columns). Figure 3: Sample T2 input structure (without the last two columns). and Deep Track inputs is available on GitLab.4 Figures 1, 2 and 3 shown sample UD, Track 1 and Track 2 structures respectively, taken from the parsed Wikipedia English dataset. 5 Evaluation Methods 5.1 Automatic methods We used BLEU, NIST, BERT, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST5 is a related n-gram similarity metric weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text. The resulting number is then divided by the number of characters"
2020.msr-1.1,N18-1202,0,0.0268272,"rd order and inflecting words. a. Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track"
2020.msr-1.1,2020.acl-demos.14,0,0.0252606,"xt quality. We skipped sentences that include special characters, contain unusual tokens (e.g. ISBN), or have unbalanced quotation marks or brackets. Furthermore, we took only sentences with more than 5 tokens and shorter than 50 tokens. After the initial filtering, quite a few malformed sentences remained. In order to remove those, we scored the sentences with BERT and kept only the best scored half. Finally, via manual inspection we identified patterns and expressions to reduce the number of malformed sentences still further. We parsed the cleaned Wikipedia sentences with the Stanza parser (Qi et al., 2020), using the trained models provided for the respective languages; the Stanza parser gets very competitive results on a large set of languages (see Table 3). For each language, we executed the parser with the processors for Tokenisation and Sentence Split, Multi-word Token Expansion, Part-of-Speech and Morphological Tagging, Lemmatisation and Dependency Parsing. The performance of the parser for all six languages in terms of Labelled Attachment Score and lemmatisation, two of the crucial aspects for our task, is provided in Table 3; for reference, we also provide the LAS and lemma scores of the"
2020.msr-1.1,K18-2011,1,0.865135,"Missing"
2020.msr-1.1,D19-6306,0,0.147951,", T2 T1, T2 T1 T1 T1 T1, T2 - - 1,795 1,032 1,675 2,287 471 1,723 Automatically parsed Wikipedia english_wikiST Z (en) french_wikiST Z (fr) korean_wikiST Z (ko) portuguese_wikiST Z (pt) russian_wikiST Z (ru) spanish_wikiST Z (es) T1, T2 T1, T2 T1 T1 T1 T1, T2 - - 1,313 1,313 530 1,135 1,291 1,280 Table 2: SR’20 dataset sizes for training, development and test sets (number of sentences). weights are proportional to the observed frequency of each pattern in the training data. The inflection step uses a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search, then combining the trees into a full projective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group w"
2020.msr-1.1,2020.acl-main.134,0,0.342113,"estoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs wo"
2020.msr-1.1,W19-3404,0,0.023759,"ferent again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system training, and cros"
2020.mwe-1.1,ramos-etal-2010-towards,1,0.707197,"Missing"
2020.mwe-1.1,P19-1318,1,0.891962,"Missing"
2020.mwe-1.1,O09-5002,0,0.0195391,"he meaning ‘have’ and ‘show’ are grouped under the same category; in the entries problem and admiration, the categories ‘cause’ and ‘show’ are explicitly distinguished; and so on. In computational lexicography, on the other hand, semantic categories of different granularity have been used for automatic classification of collocations; cf., e.g., Wanner et al. (2016), who use 16 categories for automatic classification of verb+noun collocations and 5 categories for the classification of adj+noun collocations; Moreno et al. (2013), who work with 5 broader categories for verb+noun collocations, or Chung-Chi et al. (2009), who also use very coarse-grained semantic categories of the type ‘goodness’, ‘heaviness’, ‘measures’, etc. In contrast, for instance, Wanner (2004), Wanner et al. (2006), Gelbukh and Kolesnikova (2012), and Garcia et al. (2019) use the most fine-grained semantic typology of collocations available in the field: the typology of lexical functions (LFs) developed in the context of the Explanatory Combinatorial Lexicology (ECL) (Mel’ˇcuk, 1996). LFs have the advantage that due to their level of detail, they can be used as semantic units in semantic structures and, if needed, for particular applic"
2020.mwe-1.1,C18-1225,0,0.0580242,"Missing"
2020.mwe-1.1,C16-1323,1,0.896117,"Missing"
2020.mwe-1.1,P19-1576,1,0.84301,"iled and semantically annotated large scale collocation datasets are scarce.1 Even more scarce are aligned multilingual collocation resources, which are instrumental for any crosslanguage application. In what follows, we present a manually compiled and semantically annotated bilingual (English–French) collocation resource. In order to facilitate its uptake in different applications, we enrich the collocations in this resource with additional information: each collocation is assigned its semantic category in terms of a lexical function (Mel’ˇcuk, 1996) and its corresponding relation embedding (Espinosa-Anke et al., 2019). The individual collocation elements are also embedded using Mikolov et al. (2013)’s skipgram algorithm. The functional head of each collocation is furthermore disambiguated against BabelNet (Navigli and Ponzetto, 2012), which facilitates the alignment between the equivalent English and French heads (as, e.g., between Eng. charges and Fr. accusations in dismiss the charges and rejeter les accusations). To allow for the consultation of the use of a collocation in context (be it for second language learning or model training), for each collocation, sentences from large scale English and French"
2020.mwe-1.1,P19-1392,0,0.0205115,"mantic categories of different granularity have been used for automatic classification of collocations; cf., e.g., Wanner et al. (2016), who use 16 categories for automatic classification of verb+noun collocations and 5 categories for the classification of adj+noun collocations; Moreno et al. (2013), who work with 5 broader categories for verb+noun collocations, or Chung-Chi et al. (2009), who also use very coarse-grained semantic categories of the type ‘goodness’, ‘heaviness’, ‘measures’, etc. In contrast, for instance, Wanner (2004), Wanner et al. (2006), Gelbukh and Kolesnikova (2012), and Garcia et al. (2019) use the most fine-grained semantic typology of collocations available in the field: the typology of lexical functions (LFs) developed in the context of the Explanatory Combinatorial Lexicology (ECL) (Mel’ˇcuk, 1996). LFs have the advantage that due to their level of detail, they can be used as semantic units in semantic structures and, if needed, for particular applications they can be generalized.2 Moreover, their cross-language consistency has been validated on a large number of language families. Following the tradition in ECL, in the resource we introduce, collocations are categorized acc"
2020.mwe-1.1,W09-2107,0,0.0996558,"Missing"
2020.mwe-1.1,D19-1359,0,0.0223498,"or each individual word as well as their joint collocation embedding), a subcategorization pattern of both its elements, as well as their corresponding BabelNet id, and finally, indices of their occurrences in large scale reference corpora. 1 Introduction Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words are central to second language (L2) learning (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010) and various NLP applications – including, e.g., word sense disambiguation (Maru et al., 2019), parsing and machine translation (Seretan, 2013), and natural language generation (Wanner and Bateman, 1990; Smadja and McKeown, 1991). However, manually compiled and semantically annotated large scale collocation datasets are scarce.1 Even more scarce are aligned multilingual collocation resources, which are instrumental for any crosslanguage application. In what follows, we present a manually compiled and semantically annotated bilingual (English–French) collocation resource. In order to facilitate its uptake in different applications, we enrich the collocations in this resource with additi"
2020.mwe-1.1,N13-1090,0,0.00770472,"carce are aligned multilingual collocation resources, which are instrumental for any crosslanguage application. In what follows, we present a manually compiled and semantically annotated bilingual (English–French) collocation resource. In order to facilitate its uptake in different applications, we enrich the collocations in this resource with additional information: each collocation is assigned its semantic category in terms of a lexical function (Mel’ˇcuk, 1996) and its corresponding relation embedding (Espinosa-Anke et al., 2019). The individual collocation elements are also embedded using Mikolov et al. (2013)’s skipgram algorithm. The functional head of each collocation is furthermore disambiguated against BabelNet (Navigli and Ponzetto, 2012), which facilitates the alignment between the equivalent English and French heads (as, e.g., between Eng. charges and Fr. accusations in dismiss the charges and rejeter les accusations). To allow for the consultation of the use of a collocation in context (be it for second language learning or model training), for each collocation, sentences from large scale English and French corpora in which they occur are also released. The remainder of the paper is struct"
2020.mwe-1.1,P16-2081,1,0.896216,"Missing"
2020.mwe-1.1,C14-1097,0,0.0416272,"Missing"
2020.mwe-1.1,W11-0807,0,0.0471119,"Missing"
2020.mwe-1.1,S12-1010,0,0.0466335,"Missing"
2020.mwe-1.1,W90-0105,1,0.583531,"both its elements, as well as their corresponding BabelNet id, and finally, indices of their occurrences in large scale reference corpora. 1 Introduction Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words are central to second language (L2) learning (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010) and various NLP applications – including, e.g., word sense disambiguation (Maru et al., 2019), parsing and machine translation (Seretan, 2013), and natural language generation (Wanner and Bateman, 1990; Smadja and McKeown, 1991). However, manually compiled and semantically annotated large scale collocation datasets are scarce.1 Even more scarce are aligned multilingual collocation resources, which are instrumental for any crosslanguage application. In what follows, we present a manually compiled and semantically annotated bilingual (English–French) collocation resource. In order to facilitate its uptake in different applications, we enrich the collocations in this resource with additional information: each collocation is assigned its semantic category in terms of a lexical function (Mel’ˇcu"
2020.mwe-1.1,wanner-etal-2004-enriching,1,0.70634,"Missing"
2020.webnlg-1.1,W11-2832,0,0.0388806,"f., e.g., (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018) for overviews and the WebNLG challenge (Gardent et al., 2017a) for state-of-the-art works. In general, there are three main approaches to generating texts from ontologies: (i) filling slot values in predefined sentence templates (McRoy et al., 2003), (ii) applying grammars that encode different types of linguistic knowledge (Varges and Mellish, 2001; Wanner et al., 2010; Bouayad-Agha et al., 2012; Androutsopoulos et al., 2013), and (iii) predicting the most appropriate output based on machine learning models (Gardent et al., 2017b; Belz et al., 2011). Template-based generators are very robust, but also limited in terms of portability since new templates need to be defined for ev3 System and dataset overview Let us first introduce the architecture of our system and then outline the creation of the datasets used for development and testing. 3.1 General system architecture The workflow of our system is illustrated in Figure 1. The initial input is the topic entity on which the text is to be generated. Based on this, the data collection module harvests relevant content from the Web. The resources of interest are images from the Flickr website"
2020.webnlg-1.1,N01-1001,0,0.19552,"d via a multi-modality ontology, which is further enriched by DBpedia triples for the purpose of semantics-driven image retrieval. On the other side, text generation from ontological structures is on the rise; cf., e.g., (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018) for overviews and the WebNLG challenge (Gardent et al., 2017a) for state-of-the-art works. In general, there are three main approaches to generating texts from ontologies: (i) filling slot values in predefined sentence templates (McRoy et al., 2003), (ii) applying grammars that encode different types of linguistic knowledge (Varges and Mellish, 2001; Wanner et al., 2010; Bouayad-Agha et al., 2012; Androutsopoulos et al., 2013), and (iii) predicting the most appropriate output based on machine learning models (Gardent et al., 2017b; Belz et al., 2011). Template-based generators are very robust, but also limited in terms of portability since new templates need to be defined for ev3 System and dataset overview Let us first introduce the architecture of our system and then outline the creation of the datasets used for development and testing. 3.1 General system architecture The workflow of our system is illustrated in Figure 1. The initial i"
2021.eacl-main.120,D10-1115,0,0.0731826,"ations (with and without conditioning) in Section 4, with the aim to understand whether these predictions can be used to measure the idiosyncrasy of the underlying semantics of a lexical collocation, i.e., whether the restrictions imposed by a collocation’s base are due to the frozenness of the phrase itself or, on the contrary, sentential context is neccessary. 2.2 Distributional Lexical Composition Building representations that account for noncompositional meanings within the broader spectrum of encoding semantic relations between words is a long-standing problem in computational semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013). Interestingly, there seems to be little agreement on how these representations should be defined, with recent attempts focusing on verbal multiword expressions (see an overview of approaches in Ramisch et al. (2018)), phrases of variable length encoded via LSTMs, based on their definitions (Hill et al., 2016), or arbitrary lexical and commonsense relations between word pairs for downstream NLP. As a testimony of the broad methods explored in the most recent literature, let us refer to, for instance, the combination of word vector averages with"
2021.eacl-main.120,W13-0104,0,0.0545731,"h the aim to understand whether these predictions can be used to measure the idiosyncrasy of the underlying semantics of a lexical collocation, i.e., whether the restrictions imposed by a collocation’s base are due to the frozenness of the phrase itself or, on the contrary, sentential context is neccessary. 2.2 Distributional Lexical Composition Building representations that account for noncompositional meanings within the broader spectrum of encoding semantic relations between words is a long-standing problem in computational semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013). Interestingly, there seems to be little agreement on how these representations should be defined, with recent attempts focusing on verbal multiword expressions (see an overview of approaches in Ramisch et al. (2018)), phrases of variable length encoded via LSTMs, based on their definitions (Hill et al., 2016), or arbitrary lexical and commonsense relations between word pairs for downstream NLP. As a testimony of the broad methods explored in the most recent literature, let us refer to, for instance, the combination of word vector averages with conditional autoencoders (Espinosa-Anke and Scho"
2021.eacl-main.120,P89-1010,0,0.621715,"expressions (in our case, only lexical collocations) to leverage the contextual nature of current LMs. However, our goal is not to compare different combinations of feature-extraction and training/finetuning methods, but rather to understand lexical collocations’ learnability, idiosyncrasy and their internal vector-space representations. 3 3.1 Data and Resources Lexical Collocations Let us first introduce the notion of lexical collocation and LF. The term collocation has been used in computational linguistics research to denote two different concepts. On the one hand, following Firth (1957), Church and Hanks (1989); Evert (2007); Pecina (2008) and others, a collocation has been assumed to be a combination of words that have the tendency to occur together in discourse. Typical examples are doctor – hospital, mop – bucket, real – estate, look – for, etc. On the other hand, for instance, Wanner et al. (2006); Gelbukh and Kolesnikova. (2012); Rodr´ıguez Fern´andez et al. (2016); Garcia et al. (2017) adopt the definition that is common in lexicography and phraseology (Hausmann, 1985; Cowie, 1994; Mel’ˇcuk, 1995), according to which, a collocation is an idiosyncratic combination of two lexical items, the base"
2021.eacl-main.120,W17-1703,0,0.0275299,"us first introduce the notion of lexical collocation and LF. The term collocation has been used in computational linguistics research to denote two different concepts. On the one hand, following Firth (1957), Church and Hanks (1989); Evert (2007); Pecina (2008) and others, a collocation has been assumed to be a combination of words that have the tendency to occur together in discourse. Typical examples are doctor – hospital, mop – bucket, real – estate, look – for, etc. On the other hand, for instance, Wanner et al. (2006); Gelbukh and Kolesnikova. (2012); Rodr´ıguez Fern´andez et al. (2016); Garcia et al. (2017) adopt the definition that is common in lexicography and phraseology (Hausmann, 1985; Cowie, 1994; Mel’ˇcuk, 1995), according to which, a collocation is an idiosyncratic combination of two lexical items, the base and the collocate, as defined above in Section 1. This interpretation states that collocations are phraseological units, although their degree of compositionality can vary. For instance, win [a] war is perceived to possess a higher degree of (free) composition than, e.g., hold [a] meeting, and heavy rain is less compositional than [a] well-justified argument. We adopt this definition"
2021.eacl-main.120,N19-1419,0,0.0205176,"19), and can be leveraged in a wide array of NLP tasks almost out-of-thebox (see, e.g., the GLUE and SuperGLUE results in Wang et al. (2019b) and Wang et al. (2019a), respectively). They have also been harnessed as supporting resources for knowledge-based NLP (Petroni et al., 2019), as they capture a wealth of 1 The resources associated with this paper are available at https://github.com/luisespinosaanke/ lexicalcollocations. linguistic phenomena (Rogers et al., 2020). Recently, a great deal of research analyzed the degree to which they encode, e.g., morphological (Edmiston, 2020), syntactic (Hewitt and Manning, 2019), or lexico-semantic structures (Joshi et al., 2020). However, less work explored so far how LMs interpret phraseological units at various degrees of compositionality. This is crucial for understanding the suitability of different text representations (e.g., static vs contextualized word embeddings) for encoding different types of multiword expressions (Shwartz and Dagan, 2019), which, in turn, can be useful for extracting latent world or commonsense information (Zellers et al., 2018). One central type of phraselogical units are lexical collocations, defined as restricted cooccurrences of two"
2021.eacl-main.120,Q16-1002,0,0.0273183,"istributional Lexical Composition Building representations that account for noncompositional meanings within the broader spectrum of encoding semantic relations between words is a long-standing problem in computational semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013). Interestingly, there seems to be little agreement on how these representations should be defined, with recent attempts focusing on verbal multiword expressions (see an overview of approaches in Ramisch et al. (2018)), phrases of variable length encoded via LSTMs, based on their definitions (Hill et al., 2016), or arbitrary lexical and commonsense relations between word pairs for downstream NLP. As a testimony of the broad methods explored in the most recent literature, let us refer to, for instance, the combination of word vector averages with conditional autoencoders (Espinosa-Anke and Schockaert, 2018), expectation maximization (Camacho-Collados et al., 2019), LSTMs for predicting word pair contexts (Joshi et al., 2019), and explicit encoding of generalized lexico-syntactic patterns (Washio and Kato, 1407 LF Oper1 Real1 Magn BERT input Masked sentence Pred. collocates Orig. collocate Masked sent"
2021.eacl-main.120,N19-1362,0,0.0163879,"tempts focusing on verbal multiword expressions (see an overview of approaches in Ramisch et al. (2018)), phrases of variable length encoded via LSTMs, based on their definitions (Hill et al., 2016), or arbitrary lexical and commonsense relations between word pairs for downstream NLP. As a testimony of the broad methods explored in the most recent literature, let us refer to, for instance, the combination of word vector averages with conditional autoencoders (Espinosa-Anke and Schockaert, 2018), expectation maximization (Camacho-Collados et al., 2019), LSTMs for predicting word pair contexts (Joshi et al., 2019), and explicit encoding of generalized lexico-syntactic patterns (Washio and Kato, 1407 LF Oper1 Real1 Magn BERT input Masked sentence Pred. collocates Orig. collocate Masked sent Orig sent [SEP] Masked sent iran feared that the u.s and israel may [MASK] an air raid on its controversial nuclear facilities iran feared that the u.s and israel may launch an air raid on its controversial nuclear facilities [SEP] iran feared that the u.s and israel may [MASK] an air raid on its controversial nuclear facilities perform, conduct, mount, launch launch, conduct, order launch Masked sent Orig sent [SEP]"
2021.eacl-main.120,C16-1323,1,0.897344,"Missing"
2021.eacl-main.120,C18-1225,0,0.0383424,"Missing"
2021.eacl-main.120,P19-1576,1,0.903721,"llegitimate Bon(performance) = good AntiBon(performance) = poor Table 2: LFs used in this paper. The ‘semantic gloss’ column provides both a definition and the actantial structure, which is required in cases where one LF may express the same semantics but with a different syntactic structure (e.g., Real1 vs. Real2). specifically focus on the existing (and learnable) knowledge LMs have concerning lexical collocations, and whether they can be used to recognize and categorize LFs in free text. For our experiments, we use, as initial lexical collocation source, a collocations dataset, L EX F UNC (Espinosa-Anke et al., 2019), which we have extended to cover a wider range of LFs (listed in Table 2). The original L EX F UNC dataset and this extended version are both the result of an initial collection of collocations categorized into LFs made available by Igor Mel’ˇcuk. Each collocation has been manually lemmatized, and bases and collocates have been manually annotated with part-of-speech tags and their syntactic dependency relation. With the lexical collocations of the extended L EX F UNC dataset at hand, we first compile from the English Gigaword3 a collocations corpus, which contains the occurrences of these lex"
2021.eacl-main.120,N15-1098,0,0.0683347,"Missing"
2021.eacl-main.120,2021.ccl-1.108,0,0.0263369,"Missing"
2021.eacl-main.120,D19-1359,0,0.0233434,"stics, pages 1406–1417 April 19 - 23, 2021. ©2021 Association for Computational Linguistics pervised in-context collocation categorization, where we fine-tune LMs on the task of predicting a semantic category of a collocation in terms of its lexical function (LF), given its sentential context; cf. Section 3.1. Modeling, recognizing, and classifying collocations in corpora has obvious applications for automatically creating and expanding lexicographic resources, as well as for various downstream NLP applications, among them, e.g., machine translation (Seretan, 2014), word sense disambiguation (Maru et al., 2019), or natural language generation (Wanner and Bateman, 1990). The two main contributions of this paper thus are: 1. A “collocations-in-context” dataset, with instances of collocations of 16 different semantic categories (in terms of LFs) in context, and with a fixed and lexical (i.e., no overlapping) train/dev/test split (Section 3). 2. An evaluation framework for assessing the degree of compositionality of lexical collocations, pivoting around two tasks: unsupervised collocate retrieval (Section 4) and incontext collocation categorization (Section 5). Our results suggest that modeling collocat"
2021.eacl-main.120,P16-2074,0,0.0231823,"restingly, also collocations that convey opposite semantics, such as AntiMagn or AntiBon. Figure 2: Confusion matrix for the two best performing models on average on Oper2 (Xlnet-base, left) and Oper1 (DistilBERT, right). 6 Subspace Analysis In this section, we further explore the semantics of some selected LFs. We generate visualizations of PCA-projected BERT vectors for all collocation mentions of Magn, AntiMagn, Oper1 and Oper2. These four LFs are sufficiently frequent, and they encode different morphosyntactic structures.9 We can see that antonymy (Ono et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016) is relatively well captured in contextualized models, although the subspaces are clearly different between the embedding and the last transformer layer. More specifically, as the representations of collocates for Magn 9 <Magn,AntiMagn> are most frequently expressed by adj+noun combinations, whereas <Oper1,Oper2> are always realized by a verb+noun pattern. Figure 3: Oper1 (red) and Oper2 (blue) collocate embeddings for BERT’s embedding layer (top row, left), and for the 1st (top row, right), and 5th and 12th transformer layers (second row, left and right, respectively). The bottom quadrant cor"
2021.eacl-main.120,N15-1100,0,0.0216794,"on of amplification (e.g., Bon), but interestingly, also collocations that convey opposite semantics, such as AntiMagn or AntiBon. Figure 2: Confusion matrix for the two best performing models on average on Oper2 (Xlnet-base, left) and Oper1 (DistilBERT, right). 6 Subspace Analysis In this section, we further explore the semantics of some selected LFs. We generate visualizations of PCA-projected BERT vectors for all collocation mentions of Magn, AntiMagn, Oper1 and Oper2. These four LFs are sufficiently frequent, and they encode different morphosyntactic structures.9 We can see that antonymy (Ono et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016) is relatively well captured in contextualized models, although the subspaces are clearly different between the embedding and the last transformer layer. More specifically, as the representations of collocates for Magn 9 <Magn,AntiMagn> are most frequently expressed by adj+noun combinations, whereas <Oper1,Oper2> are always realized by a verb+noun pattern. Figure 3: Oper1 (red) and Oper2 (blue) collocate embeddings for BERT’s embedding layer (top row, left), and for the 1st (top row, right), and 5th and 12th transformer layers (second row, left and"
2021.eacl-main.120,D14-1162,0,0.0939256,"he quality of BERT’s predictions can be obtained by measuring the semantic similarity (for instance, by cosine distance) between the original masked collocate and the predicted collocates. In the example we already referred to above, heavy rain, the similarity between ‘the’ and ‘heavy’ is low, whereas, if the model predicts hard or even any other adjective, it should be considered less wrong. We obtain a broad picture of the quality of BERT’s predictions by plotting a histogram (Figure 1) of the similarities obtained by comparing the original collocate’s and BERT’s predicted GloVe embeddings (Pennington et al., 2014) under both settings (MASKED and CONDITIONED) for the same three LFs as in Table 1), namely Magn, Oper1 and Real1. The conditioning strategy is helpful; it con5 https://spacy.io/api/lemmatizer. Figure 1: Histograms showing the distribution of similarities between gold and predicted (lemmatized) collocates for the three LFs Magn, Oper1 and Real1 (from left to right). M ASKED AntiBon AntiMagn AntiReal2 AntiVer Bon Caus1Func0 CausFunc0 IncepOper1 IncepPredPlus LiquFunc0 Magn Oper1 Oper2 Real1 Real2 Ver C ONDITIONED MRR MAP MRR 13.78 30.16 39.14 11.38 25.13 52.77 61.14 54.38 7.27 45.51 33.43 73.12"
2021.eacl-main.120,D19-1250,0,0.0550452,"Missing"
2021.eacl-main.120,P16-2081,1,0.89509,"Missing"
2021.eacl-main.120,2020.tacl-1.54,0,0.0135172,"t al., 2019), RoBERTa (Liu et al., 2019), etc. have proven extremely flexible, as they behave as unsupervised multitask learners (Radford et al., 2019), and can be leveraged in a wide array of NLP tasks almost out-of-thebox (see, e.g., the GLUE and SuperGLUE results in Wang et al. (2019b) and Wang et al. (2019a), respectively). They have also been harnessed as supporting resources for knowledge-based NLP (Petroni et al., 2019), as they capture a wealth of 1 The resources associated with this paper are available at https://github.com/luisespinosaanke/ lexicalcollocations. linguistic phenomena (Rogers et al., 2020). Recently, a great deal of research analyzed the degree to which they encode, e.g., morphological (Edmiston, 2020), syntactic (Hewitt and Manning, 2019), or lexico-semantic structures (Joshi et al., 2020). However, less work explored so far how LMs interpret phraseological units at various degrees of compositionality. This is crucial for understanding the suitability of different text representations (e.g., static vs contextualized word embeddings) for encoding different types of multiword expressions (Shwartz and Dagan, 2019), which, in turn, can be useful for extracting latent world or comm"
2021.eacl-main.120,K15-1026,0,0.0137219,"n (e.g., Bon), but interestingly, also collocations that convey opposite semantics, such as AntiMagn or AntiBon. Figure 2: Confusion matrix for the two best performing models on average on Oper2 (Xlnet-base, left) and Oper1 (DistilBERT, right). 6 Subspace Analysis In this section, we further explore the semantics of some selected LFs. We generate visualizations of PCA-projected BERT vectors for all collocation mentions of Magn, AntiMagn, Oper1 and Oper2. These four LFs are sufficiently frequent, and they encode different morphosyntactic structures.9 We can see that antonymy (Ono et al., 2015; Schwartz et al., 2015; Nguyen et al., 2016) is relatively well captured in contextualized models, although the subspaces are clearly different between the embedding and the last transformer layer. More specifically, as the representations of collocates for Magn 9 <Magn,AntiMagn> are most frequently expressed by adj+noun combinations, whereas <Oper1,Oper2> are always realized by a verb+noun pattern. Figure 3: Oper1 (red) and Oper2 (blue) collocate embeddings for BERT’s embedding layer (top row, left), and for the 1st (top row, right), and 5th and 12th transformer layers (second row, left and right, respectively). T"
2021.eacl-main.120,Q19-1027,0,0.162191,"ithub.com/luisespinosaanke/ lexicalcollocations. linguistic phenomena (Rogers et al., 2020). Recently, a great deal of research analyzed the degree to which they encode, e.g., morphological (Edmiston, 2020), syntactic (Hewitt and Manning, 2019), or lexico-semantic structures (Joshi et al., 2020). However, less work explored so far how LMs interpret phraseological units at various degrees of compositionality. This is crucial for understanding the suitability of different text representations (e.g., static vs contextualized word embeddings) for encoding different types of multiword expressions (Shwartz and Dagan, 2019), which, in turn, can be useful for extracting latent world or commonsense information (Zellers et al., 2018). One central type of phraselogical units are lexical collocations, defined as restricted cooccurrences of two syntactically bound lexical items (Kilgarriff, 2006), such that one of the items (the base) conditions the selection of the other item (the collocate) to express a specific meaning. For instance, the base lecture conditions the collocates give or deliver to express the meaning ‘perform’, the base applause conditions the selection of the collocate thunderous to express the meani"
2021.eacl-main.120,W90-0105,1,0.542313,"sociation for Computational Linguistics pervised in-context collocation categorization, where we fine-tune LMs on the task of predicting a semantic category of a collocation in terms of its lexical function (LF), given its sentential context; cf. Section 3.1. Modeling, recognizing, and classifying collocations in corpora has obvious applications for automatically creating and expanding lexicographic resources, as well as for various downstream NLP applications, among them, e.g., machine translation (Seretan, 2014), word sense disambiguation (Maru et al., 2019), or natural language generation (Wanner and Bateman, 1990). The two main contributions of this paper thus are: 1. A “collocations-in-context” dataset, with instances of collocations of 16 different semantic categories (in terms of LFs) in context, and with a fixed and lexical (i.e., no overlapping) train/dev/test split (Section 3). 2. An evaluation framework for assessing the degree of compositionality of lexical collocations, pivoting around two tasks: unsupervised collocate retrieval (Section 4) and incontext collocation categorization (Section 5). Our results suggest that modeling collocations in context is a challenge, even for widely used LMs, a"
2021.eacl-main.120,D18-1058,0,0.0362079,"Missing"
2021.eacl-main.120,2020.emnlp-demos.6,0,0.0526386,"Missing"
2021.eacl-main.120,D18-1009,0,0.0217482,"eal of research analyzed the degree to which they encode, e.g., morphological (Edmiston, 2020), syntactic (Hewitt and Manning, 2019), or lexico-semantic structures (Joshi et al., 2020). However, less work explored so far how LMs interpret phraseological units at various degrees of compositionality. This is crucial for understanding the suitability of different text representations (e.g., static vs contextualized word embeddings) for encoding different types of multiword expressions (Shwartz and Dagan, 2019), which, in turn, can be useful for extracting latent world or commonsense information (Zellers et al., 2018). One central type of phraselogical units are lexical collocations, defined as restricted cooccurrences of two syntactically bound lexical items (Kilgarriff, 2006), such that one of the items (the base) conditions the selection of the other item (the collocate) to express a specific meaning. For instance, the base lecture conditions the collocates give or deliver to express the meaning ‘perform’, the base applause conditions the selection of the collocate thunderous to express the meaning ‘intense’, and so on. Lexical collocations are of high relevance to lexicography, NLP and second language"
2021.eacl-main.120,P19-1328,0,0.0283123,"LM) can be used as a proxy for gaining insights into how language is encoded by the weights of the (usually transformerbased) LM architecture. Moreover, simply asking an LM to predict words in context (without taskspecific fine-tuning) has proved useful in NLP applications dealing with lexical items (affixes, words or phrases). For example, Wu et al. (2019) use BERT’s MLM for augmenting their training data in sentiment analysis tasks; Qiang et al. (2019) use BERT for lexical simplification by conditioning the predictions over the [MASK] token by providing the original sentence as context; and Zhou et al. (2019) obtain SotA results in lexical substitution by conditioning BERT via embedding dropout on the target (unmasked) word. Inspired by the findings in these works (especially Qiang et al. (2019)), we will explore the predictions of BERT over masked lexical collocations (with and without conditioning) in Section 4, with the aim to understand whether these predictions can be used to measure the idiosyncrasy of the underlying semantics of a lexical collocation, i.e., whether the restrictions imposed by a collocation’s base are due to the frozenness of the phrase itself or, on the contrary, sentential"
2021.eacl-main.191,P17-1080,0,0.0313539,"dely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitivel"
2021.eacl-main.191,C10-3009,0,0.0660577,"Missing"
2021.eacl-main.191,W05-0620,0,0.23958,"Missing"
2021.eacl-main.191,2020.acl-main.561,0,0.0248025,"paraphrase identification. The first three inherently deal with (morpho-)syntactic information while the latter three, which traditionally draw upon the output of syntactic parsing (Carreras and M`arquez, 2005; Bj¨orkelund et al., 2010; Strubell et al., 2018; Wang et al., 2019, inter-alia), deal with higher level, semantic information. Almost all of our experiments are on English corpora; one is on multilingual dependency parsing. 2 Related work BERT has become the default baseline in NLP, and consequently, numerous studies analyze its linguistic capabilities in general (Rogers et al., 2020; Henderson, 2020), and its syntactic capabilities in particular (Linzen and Baroni, 2020). Even if syntactic information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019"
2021.eacl-main.191,N19-1419,0,0.429062,"ral Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitively, we expect morpho-syntactic tasks to clearly reinforce the encoded syntactic information, while tasks that are not explicitly syntactic in nature should maintain it in case they bene"
2021.eacl-main.191,P19-1356,0,0.0185575,"istic capabilities in general (Rogers et al., 2020; Henderson, 2020), and its syntactic capabilities in particular (Linzen and Baroni, 2020). Even if syntactic information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no dec"
2021.eacl-main.191,D19-1445,0,0.0194995,"ation in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Universal Dependencies Corpus for English (Silveira et al., 2014). It can generally also distinguish good from bad"
2021.eacl-main.191,2020.acl-main.375,0,0.0180508,"information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Univers"
2021.eacl-main.191,2020.tacl-1.50,0,0.201783,"little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitively, we expect morpho-syntactic tasks to clearly reinforce the encoded syntactic information, while tasks that are not explicitly syntactic in nature should maintain it in case they benefit from syntax (Kuncoro et al., 2020) and lose it if they do not. In order to cover the three main levels of the linguistic description (morphology, syntax and semantics), we select six different tasks: PoS tagging, constituency parsing, syntactic dependency parsing, semantic role labeling (SRL), question answering (QA) and paraphrase identification. The first three inherently deal with (morpho-)syntactic information while the latter three, which traditionally draw upon the output of syntactic parsing (Carreras and M`arquez, 2005; Bj¨orkelund et al., 2010; Strubell et al., 2018; Wang et al., 2019, inter-alia), deal with higher le"
2021.eacl-main.191,N19-1112,0,0.0180436,"nly on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Universal Dependencies Corpus for English (Silveira et al., 2014). It can generally also distinguish good from bad completions and robustly retrieves noun hypernyms, but shows insensitivity to the contextual impacts of negation (Ettinger, 2020). Different supervised probing models have been used to test for the presence of a wide range of linguistic phenomena in the BERT model (Conneau et al., 2018; Liu et al., 2019; Tenney et al., 2019b; Voita and Titov, 2020; Elazar et al., 2020). Hewitt and Manning (2019)’s structural probe shows that entire syntax trees are embedded implicitly in BERT’s vector geometry. Extending their work, Chi et al. (2020) show that multilingual BERT recovers syntactic tree distances in languages other than English and learns representations of syntactic dependency labels. Regarding how fine-tuning affects the representations of BERT, Gauthier and Levy (2019) found a significant divergence between the final representations of models fine-tuned on different tasks when using the str"
2021.eacl-main.191,J93-2004,0,0.0740579,"and training data order (Dodge et al., 2020), we repeat this process 5 times per task with different random seeds and average results. PoS tagging. We fine-tune BERT with a linear layer on top of the hidden-states output for token classification.3 Dataset: Universal Dependencies Corpus for English (UD 2.5 EN EWT Silveira et al. (2014)). Constituency parsing. Following Vilares et al. (2020), we cast constituency parsing as a sequence labeling problem, and use a single feed-forward layer on top of BERT to directly map word vectors to labels that encode a linearized tree. Dataset: Penn Treebank (Marcus et al., 1993). Dependency parsing. We fine-tune a Deep Biaffine neural dependency parser (Dozat and Manning, 2016) on three different datasets: i) UD 2.5 English EWT (Silveira et al., 2014); ii) a multilingual benchmark generated by concatenating the UD 2.5 standard data splits for German, English, Spanish, French, Italian, Portuguese, and Swedish (Zeman et al., 2019), with gold PoS tags; iii) PTB SD 3.3.0 (De Marneffe et al., 2006). Semantic role labeling. Following Shi and Lin (2019), we decompose the task into i) predicate sense disambiguation and argument identification, and ii) classification. Both su"
2021.eacl-main.191,2020.blackboxnlp-1.4,0,0.0210519,"zar et al., 2020). Hewitt and Manning (2019)’s structural probe shows that entire syntax trees are embedded implicitly in BERT’s vector geometry. Extending their work, Chi et al. (2020) show that multilingual BERT recovers syntactic tree distances in languages other than English and learns representations of syntactic dependency labels. Regarding how fine-tuning affects the representations of BERT, Gauthier and Levy (2019) found a significant divergence between the final representations of models fine-tuned on different tasks when using the structural probe of Hewitt and Manning (2019), while Merchant et al. (2020) concluded that fine-tuning is conservative and does not lead to catastrophic forgetting of linguistic phenomena – which our experiments do not confirm. However, we find that the encoded syntactic information is forgotten, reinforced or preserved differently along the fine-tuning process depending on the task. 3 Experimental setup We study the evolution of the syntactic structures discovered during pretraining along the fine-tuning of BERT-base (cased) (Devlin et al., 2019)1 on six different tasks, drawing upon the structural probe of Hewitt and Manning (2019).2 We fine-tune the whole model on"
2021.eacl-main.191,N18-1202,0,0.0463525,"g. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task. 1 Introduction Adapting unsupervised pretrained language models (LMs) to solve supervised tasks has become a widely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sen"
2021.eacl-main.191,W19-4302,0,0.0522777,"Missing"
2021.eacl-main.191,2020.acl-main.420,0,0.0133724,"aster/examples/ text-classification/run_glue.py. 6 Cf. also Supplementary Material. Figure 1: Tree distance evaluation. UUAS evolution. Figure 2: Tree distance evaluation. Dspr evolution. assumed. As many words have a clear preference towards a specific PoS, especially in English, and most of the ambiguous cases can be resolved using information in the close vicinity (e.g., a simple 3gram sequence tagger is able to achieve a very high accuracy (Manning, 2011)), syntactic structure information may not be necessary and, therefore, the model does not preserve it. This observation is aligned with Pimentel et al. (2020), who found that PoS-tagging is not an ideal task for contemplating the syntax contained in contextual word embeddings. The loss is less pronounced on depth-related metrics, maybe because the root of the sentence usually corresponds to the verb, which may also help in identifying the PoS of surrounding words. Constituency parsing and dependency parsing share a very similar tendency, with a big improvement in the first fine-tuning steps preserved along the rest of the process. As both tasks heavily rely on syntactic information, this improvement intuitively makes sense. Dependency parsing fine-"
2021.eacl-main.191,P18-2124,0,0.128433,"show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task. 1 Introduction Adapting unsupervised pretrained language models (LMs) to solve supervised tasks has become a widely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change"
2021.emnlp-main.118,P17-1080,0,0.0303724,"with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However, large volumes of pretraining data are not always available and pretraining is costly, such that the following questions need to be answered: (i) Do we always ne"
2021.emnlp-main.118,2020.acl-main.493,0,0.0135195,"tion 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources necessary for their development and, consequently, also the inclusivity gap. The financial cost of the required hardware and electricity favors industry-powered resear"
2021.emnlp-main.118,P18-1198,0,0.0247188,"BERTas models and the syntactic tests as well as the downstream applications we explore. Section 4 presents the outcome of our experiments. Section 5 offers a cost-benefit analysis of the pretraining of the different models, and Section 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language mo"
2021.emnlp-main.118,N19-1423,0,0.0353809,"and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 20"
2021.emnlp-main.118,I05-5002,0,0.115912,"Missing"
2021.emnlp-main.118,N19-1004,1,0.899159,"Missing"
2021.emnlp-main.118,W18-2501,0,0.0122756,"follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al., 2020) and AllenNLP (Gardner et al., 2018). For of the original sequence, and select the probability implementation details, pretrained weights and hyperparameassigned by the model to the original word. Note ter values, cf. the documentation of the libraries. 3 Source: https://github.com/Tarpelite/ that this setup allows the models to know how many UniNLP/blob/master/examples/run_pos.py tokens there are in the sentences, and therefore the 4 Source: https://github.com/huggingface/ results are not directly comparable with those of transformers/blob/master/examples/ unidirectional models, that do not have any infor- text-classification/r"
2021.emnlp-main.118,2020.acl-demos.10,0,0.0185617,"ic capabilities of RoBERTa by means of the MiniBERTas models, a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amou"
2021.emnlp-main.118,N18-1108,0,0.0218954,"and propose actionable recommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively large data size (e.g., 10M words), such as n"
2021.emnlp-main.118,2020.acl-main.158,0,0.256772,"a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amount of syntax according to Hewitt and Manning (2019b)’s"
2021.emnlp-main.118,W19-4825,0,0.0274722,"Missing"
2021.emnlp-main.118,Q16-1037,0,0.0336098,"cessful neural network models for NLP, and propose actionable recommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively la"
2021.emnlp-main.118,N19-1112,0,0.0823389,"complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi e"
2021.emnlp-main.118,2021.ccl-1.108,0,0.0255846,"Missing"
2021.emnlp-main.118,D18-1151,0,0.346375,"ecommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively large data size (e.g., 10M words), such as negative polarity items an"
2021.emnlp-main.118,2020.emnlp-main.632,0,0.0659525,"Missing"
2021.emnlp-main.118,N18-1202,0,0.02345,"lin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However, large volumes of pretraining data are not always available and pretraining is costly, such that the following questions need to be answered: (i) Do we always need models pretrained on internetscale corpora"
2021.emnlp-main.118,2020.acl-main.420,0,0.0145241,"ave also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources necessary for their development and, consequently, also the inclusivity gap. The financial cost of the required hardware and electricity favors industry-powered research, and harms academics, students, and nonindustry researchers, particularly those from emerging economies. Moreover, the training of such models is not only financially expensive, but has also a large carbon footprint. Schwartz et al. (2019) propose to report the financial cost of deve"
2021.emnlp-main.118,P18-2124,0,0.0275291,"a encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-t"
2021.emnlp-main.118,D17-1035,0,0.0147493,"als, mask, mask], and compare the surprisal of the model predicting a dot ‘.’ for the first masked position in each case. 3.5 Downstream applications To compare the performance of the models on downstream applications, we analyze their learning curves along the fine-tuning process on two morpho-syntactic tasks (PoS tagging and dependency parsing) and a non-syntactic task (paraphrase identification). Each task is fine-tuned for 3 epochs, with the default learning rate of 5e−5 . To mitigate the variance in performance induced by weight initialization and training data order (Dodge et al., 2020; Reimers and Gurevych, 2017), we repeat this process 5 times per task with different random seeds and average results.2 For PoS tagging, we fine-tune RoBERTa with a linear layer on top of the hidden-states output for token classification.3 Dataset: Universal Dependencies Corpus for English (UD 2.5 English EWT (Silveira et al., 2014)). For Dependency parsing, we fine-tune a Deep Biaffine neural dependency parser (Dozat and Manning, 2016). Dataset: UD 2.5 English EWT (Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Micro"
2021.emnlp-main.118,2020.tacl-1.54,0,0.0173918,"onmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However"
2021.emnlp-main.118,silveira-etal-2014-gold,0,0.0537333,"Missing"
2021.emnlp-main.118,P19-1355,0,0.316243,"form a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amount of syntax according to Hewitt and Manning (2019b)’s metrics, but do not always lead to a better syntactic generalization. Indeed, we find that models pretrained on less data perform equally good or even better than those pretrained on more data on 3 out of 6 syntactic test suites. When applied to downstream tasks, the models pretrained on more data perform generally better. However, the analysis of the trade-off between the cost of training a model and its performance sho"
2021.emnlp-main.118,D19-1592,0,0.0323774,"Missing"
2021.emnlp-main.118,2020.emnlp-main.14,0,0.015276,"m applications we explore. Section 4 presents the outcome of our experiments. Section 5 offers a cost-benefit analysis of the pretraining of the different models, and Section 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources n"
2021.emnlp-main.118,W19-2304,0,0.0289405,"(Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Microsoft Research Paraphrase Corpus (MRPC) The tests in SyntaxGym evaluate whether models are able to assign a higher probability to grammatical and natural continuations of sentences. As RoBERTa is a bidirectional model, to be able to ask it to predict the probability of a token given the context of previous tokens we test it in a left-to-right generative setup, as done in (Rongali et al., 2020; Zhu et al., 2020). More precisely, we follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al., 2020) and AllenNLP (Gardner et al., 2018). For"
2021.emnlp-main.118,W18-5446,0,0.0592616,"Missing"
2021.emnlp-main.118,2020.emnlp-main.16,0,0.183139,"ch that the following questions need to be answered: (i) Do we always need models pretrained on internetscale corpora? (ii) As the models are pretrained on more data, and their perplexity improves, do they encode more syntactic information and offer a better syntactic generalization? (iii) Do the models with more pretraining perform better when applied in downstream tasks? To address these questions, we explore the relation between the size of the pretraining data and the syntactic capabilities of RoBERTa by means of the MiniBERTas models, a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Metho"
2021.emnlp-main.118,W19-4819,0,0.0271355,"Missing"
2021.emnlp-main.118,W18-5423,0,0.0203788,"ithout a second clause less probable, and should make a second clause more probable. The circuit is composed of 4 Subordination tests from Futrell et al. (2018). 5. Licensing: Measures when a particular token must exist within the scope of an upstream licensor token. The circuit is composed of 4 Negative Polarity Item Licensing (NPI) tests and 6 Reflexive Pronoun Licensing tests, all from Marvin and Linzen (2018). 6. Long-Distance Dependencies: Measures covariations between two tokens that span long distances in tree depth. The circuit is composed of 6 Filler-Gap Dependencies (FGD) tests from Wilcox et al. (2018) and Wilcox et al. (2019b), and 2 Cleft tests from (Hu et al., 2020). 3.4 Encoding unidirectional context with bidirectional models mation regarding the length of the sequence. For example, in a Subordination test with the examples ‘Because the students did not like the material.’ and ‘The students did not like the material.’, we expect the model to assign a higher surprisal (Wilcox et al., 2019c) to the first example, because the initial ""Because"" implies that the immediately following clause is not the main clause of the sentence, but instead is a subordinate that must be followed by the mai"
2021.emnlp-main.118,N19-1334,1,0.901988,"sh finite present tense verbs. It is composed of 3 Subject-Verb Number Agreement tests from Marvin and Linzen (2018), 2. Center Embedding: Tests the ability to embed a phrase in the middle of another phrase of the same type. Subject and verbs must match in 3.2 Structural probing a first-in-last-out order, meaning models must apHewitt and Manning (2019b)’s structural probes as- proximate a stack-like data-structure in order to sess how well syntax trees are embedded in a linear successfully process them. The circuit is composed transformation of the network representation space of 2 tests from Wilcox et al. (2019a). applying two different evaluations: Tree distance 3. Garden-Path Effects: Measures the syntacevaluation, in which squared L2 distance encodes tic phenomena that result from tree structural ambi1 https://huggingface.co/nyu-mll guities that give rise to locally coherent but globally 1573 implausible syntactic parses. The circuit is composed of 2 Main Verb / Reduced Relative Clause (MVRR) tests and 4 NP/Z Garden-paths (NPZ) tests, all from Futrell et al. (2018). 4. Gross Syntactic Expectation: Tests the ability of the models to distinguish between coordinate and subordinate clauses: introduci"
2021.emnlp-main.118,2020.conll-1.40,0,0.020115,"d Manning, 2016). Dataset: UD 2.5 English EWT (Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Microsoft Research Paraphrase Corpus (MRPC) The tests in SyntaxGym evaluate whether models are able to assign a higher probability to grammatical and natural continuations of sentences. As RoBERTa is a bidirectional model, to be able to ask it to predict the probability of a token given the context of previous tokens we test it in a left-to-right generative setup, as done in (Rongali et al., 2020; Zhu et al., 2020). More precisely, we follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al.,"
2021.findings-acl.333,Q16-1037,0,0.215026,"al., 2019b), DistilBERT (Sanh et al., 2019), XLNet (Yang et al., 2019), etc. are excellent learners. They have been shown to capture a range of different types of linguistic information, from morphological (Edmiston, 2020) over syntactic (Hewitt and Manning, 2019) to lexico-semantic (Joshi et al., 2020). A particularly significant number of works study the degree to which these models capture and generalize over (i.e., learn to instantiate correctly in different contexts) syntactic phenomena, including, e.g., subject-verb agreement, long distance dependencies, garden path constructions, etc. (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2019; Wilcox et al., 2019a). However, most of these works focus on monolingual models, and, if the coverage of syntactic phenomena is considered systematically and in detail, it is mainly for English, as, e.g., (Hu et al., 2020a). This paper aims to shift the attention from monolingual to multilingual models and to emphasize the importance to also consider the syntactic phenomena of languages other than English when assessing the generalization potential of a model. More specifically, it systematically assesses how well multilingual models are capable"
2021.findings-acl.333,N19-1112,0,0.331757,"otential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform. 1 Introduction Transformer-based neural models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), DistilBERT (Sanh et al., 2019), XLNet (Yang et al., 2019), etc. are excellent learners. They have been shown to capture a range of different types of linguistic information, from morphological (Edmiston, 2020) over syntactic (Hewitt and Manning, 2019) to lexico-semantic (Joshi et al., 2020). A particularly significant number of works study the degree to which these models capture and generalize over (i.e., learn to instantiate correctly in different contexts) syntactic phenomena, including, e.g., subject-verb agreement, long distance dependencies, garden path constructions, etc. (Linzen et"
2021.nlp4posimpact-1.3,P16-2096,0,0.0722857,"Missing"
2021.nlp4posimpact-1.3,2020.acl-tutorials.2,0,0.0993811,"Missing"
2021.nlp4posimpact-1.3,2020.lrec-1.402,0,0.0113777,"sed on a list of keywords retrieved from the literature and revised in view of the task, we retrieve from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them. The result of the analysis is a map of the current NLP4SG research and insights concerning the white spots on this map. 1 Introduction Measuring the social impact of NLP is not a trivial task. A priori, the range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. It goes from more theoretical works (Cowls et al., 2021), language resources (Midrigan Ciochina et al., 2020; El-Haj et al., 2015) and models (Devlin et al., 2019) to concrete technologies of which many target the identification of hate speech (Fortuna et al., 2021) or fake news (Shu et al., 2017). But there are also others that address, e.g., text simplification or paraphrasing, which can be used to alleviate consequences of dyslexia (Rello et al., 2015), conversational agents for mental health treatment (Gaffney et al., 2019), (i) we offer a working definition of NLP4SG and related concepts that can serve as a first orientation in the field; (ii) we provide an analysis of the current state and the"
2021.nlp4posimpact-1.3,2020.lrec-1.109,0,0.0347236,"sets: areas for social good and other dimensions of social good; cf. Tables 1 and 2. Areas for social good keywords correspond to social good applications. As previously outlined in Section 2.2, the main areas are Agriculture, Education, Environmental sustainability, Healthcare, Public safety, Social care, Transportation and Urban planning. Evaluation of social good in NLP Evaluating the current state of NLP for social good is a crucial step towards the identification of the gaps and promotion of a more impactful technology development. For this purpose, we build upon the NLP Scholar Dataset (Mohammad, 2020) and analyse existent features together with new classifications on social good aspects. In what follows, we describe in detail the data and the procedure of our analysis. We make the code available to the community1 . 3.1 Methodology Data The NLP Scholar Dataset provides access to more than 50k instances from both ACL Anthology (AA) and Google Scholar (GS), and includes authors’ names, year of publication, venue of publication, etc. We use the version of this dataset from June 2020 (Mohammad, 2020). The dataset includes some entries that are not really papers (e.g., forewords, prefaces, progr"
2021.nlp4posimpact-1.3,N19-1423,0,0.00474565,"and revised in view of the task, we retrieve from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them. The result of the analysis is a map of the current NLP4SG research and insights concerning the white spots on this map. 1 Introduction Measuring the social impact of NLP is not a trivial task. A priori, the range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. It goes from more theoretical works (Cowls et al., 2021), language resources (Midrigan Ciochina et al., 2020; El-Haj et al., 2015) and models (Devlin et al., 2019) to concrete technologies of which many target the identification of hate speech (Fortuna et al., 2021) or fake news (Shu et al., 2017). But there are also others that address, e.g., text simplification or paraphrasing, which can be used to alleviate consequences of dyslexia (Rello et al., 2015), conversational agents for mental health treatment (Gaffney et al., 2019), (i) we offer a working definition of NLP4SG and related concepts that can serve as a first orientation in the field; (ii) we provide an analysis of the current state and the tendencies of the research on NLP4SG. The remainder of"
2021.woah-1.19,P19-1271,0,0.0160485,"’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e.g., ‘denouncing’, ‘facts’, ‘humor’, ‘hypocrisy’ and others) and themes (e.g., ‘crimes’, ‘culture’, ‘islamization’, 4 In (4), feminist is a classifying attribute of novels (see also Section 4) and should thus be part of the target. 5 Cf. (Fortuna et al., 2020) for a list of categories used in the most common hate speech datasets. 180 ‘rapism’ and others) as in (Chung et al., 2019)), and classification of different focuses of abuse (e.g., ‘stereotype & objectification’, ‘dominance’, ‘derailing’, ‘sexual harassment’, ‘threats of violence’, and ‘discredit’ as in (Fersini et al., 2018)). All these works do not aim to identify the specific targeted group of individuals or the individual and neither do they aim to identify characteristics of the targets that provoked hate. Rather, they identify posts related to hate speech in general or to one of its more specific categories – which is a step prior to detection of targets and aspects, where we start. Some previous works use"
2021.woah-1.19,N19-1423,0,0.00940235,"et expanding (α5 =1) and 0.6 0.76 0.53 w/o tf and α4 =0 w/ target expanding (α5 =1) and 0.61 0.75 0.55 w/o tf and α3 =0 w/o all subwords 0.63 0.73 0.6 w/o nominal subwords 0.63 0.74 0.61 w/ target expanding (α5 =1) and 0.63 0.79 0.57 w/o tf w/ target expanding (α5 =1) 0.63 0.79 0.56 GetTA Pair (Tref , p, ~abest , ~vbest ) 0.68 0.79 0.65 Algorithm setup Table 3: Evaluation of the quality of the detected targets and aspects on the development and test set with a hypernym person or group that is a relevant candidate entity according to the definition of a target. We also fine-tuned a BERT model (Devlin et al., 2019) on the development set for target recognition in order to compare our pointer-generatorbased model to transformer-based models. We can observe that target identification as invoked by the GetTA Pair (Algorithm 2) achieves a rather good performance. Thus, the accuracy for the exact match between the ground truth targets and predicted targets is 0.65 for the development set and 0.57 for the test set. With BERT, we achieve somewhat lower accuracy. It is interesting to observe that combining GetTA Pair with BERT results in lower accuracy for the exact match, but in considerably higher accuracy (o"
2021.woah-1.19,P14-2009,0,0.0215597,"rehensive survey of aspect-oriented sentiment analysis. In some (more traditional) works, aspects and their values are identified in separate stages (Hu and Liu, 2004; Hai et al., 2011). In more recent works, both tasks are addressed by one model, with aspects being partially identified by attention mechanisms realized, e.g., in an LSTM (Wang et al., 2016), CNN (Liu and Shen, 2020) or an alternative common deep NN model. The targets are, as a rule, predefined, such that the challenge consists in analysing the sentiment of tweets towards these predefined targets; cf., e.g., (Tang et al., 2016; Dong et al., 2014). The problem of open-class target identification has not been broadly investigated and sometimes solved simply as a named entity recognition problem due to the nature of the data in wihch the targets are often represented by proper names (Mitchell et al., 2013; Ma et al., 2018). However, targets in hate speech texts go far beyond named 181 entities, and the overall task is inverse to targetoriented sentiment classification: given a known category (hate speech of negative sentiment as a rule), we have to identify the hate target and its corresponding “opinioned” aspect. Still, our proposal is"
2021.woah-1.19,2020.lrec-1.838,1,0.711476,"s in (Alfina et al., 2017; Ross et al., 2017)), multi-class classification into several hate speech categories (e.g., ‘racism’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e.g., ‘denouncing’, ‘facts’, ‘humor’, ‘hypocrisy’ and others) and themes (e.g., ‘crimes’, ‘culture’, ‘islamization’, 4 In (4), feminist is a classifying attribute of novels (see also Section 4) and should thus be part of the target. 5 Cf. (Fortuna et al., 2020) for a list of categories used in the most common hate speech datasets. 180 ‘rapism’ and others) as in (Chung et al., 2019)), and classification of different focuses of abuse (e.g., ‘stereotype & objectification’, ‘dominance’, ‘derailing’, ‘sexual harassment’, ‘threats of violence’, and ‘discredit’ as in (Fersini et al., 2018)). All these works do not aim to identify the specific targeted group of individuals or the individual and neither do they aim to identify characteristics of the targets that provoked hate. Rather, they identify posts related to hate speech in general or to one of its mor"
2021.woah-1.19,2020.acl-main.483,0,0.0153643,"pects can be identified; see, e.g., (8). (8) I asked that question recently and actually got an answer http://t.co/oD98sptcGT. We discard such posts in our current experiments. 3 Related Work As mentioned in Section 1, most of the works on online hate speech focused on the task of classifying social media posts with respect to predefined typologies of rather coarse-grained hate speech categories, such as ‘hate speech’, ‘racism’, ‘sexism’, ‘offense’, etc. (Schmidt and Wiegand, 2017; Davidson et al., 2017a; Fortuna and Nunes, 2018; Swamy et al., 2019; Arango et al., 2019; Salminen et al., 2020; Kennedy et al., 2020; Rajamanickam et al., 2020).5 Vidgen and Derczynski (2020) distinguish between binary classification (as in (Alfina et al., 2017; Ross et al., 2017)), multi-class classification into several hate speech categories (e.g., ‘racism’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e.g., ‘denouncing’, ‘facts’, ‘humor’, ‘hypocrisy’ and others) and themes (e.g., ‘crimes’, ‘culture’, ‘islamization’, 4 In (4), feminist"
2021.woah-1.19,D07-1114,0,0.0601715,"assume predefined target categories and do not identify which characteristics of the targets are concerned. In contrast, open-class target and aspect extraction may allow for modeling of the particular forms of discrimination and hate experienced by individuals or groups of individuals covered or not covered by previously identified target categories. As already mentioned in Section 1, our work is also related to aspect-oriented sentiment analysis, in which “targets” are specific entities (e.g., products, sights, celebrities) and “aspects” are characteristics or components of a given entity (Kobayashi et al., 2007; Nikoli´c et al., 2020). For each identified aspect, the “sentiment value” aligned with it is extracted; see, e.g., (Nazir et al., 2020) for a recent comprehensive survey of aspect-oriented sentiment analysis. In some (more traditional) works, aspects and their values are identified in separate stages (Hu and Liu, 2004; Hai et al., 2011). In more recent works, both tasks are addressed by one model, with aspects being partially identified by attention mechanisms realized, e.g., in an LSTM (Wang et al., 2016), CNN (Liu and Shen, 2020) or an alternative common deep NN model. The targets are, as"
2021.woah-1.19,D15-1166,0,0.0557858,"nalysis in the sense that we also use an NN model (in our case, LSTM-based encoder) with attention mechanisms for initial hate speech target and aspect candidates identification, before a domain-adaptation post-processing stage. 4 come, which is especially relevant to our work, as the hate speech dataset includes specific words unseen during generic training, such as proper names, hashtags, and Twitter names. The generator implies the ability to adjust internal vocabulary distribution for selecting the next word (which might be a termination token “*”) based on weights of global attention at (Luong et al., 2015), which are updated at each generation step t. The probability of generating the next word instead of copying one is defined as follows: T ∗ pgen = σ(wh∗ ht + wxT xt + wsT st + bptr ) Outline of the Model (1) The study of social media hate speech posts reveals that targets are entities that are, as a rule, verbalized in terms of classifying nominal groups (Halliday, 2013). Aspects may also be expressed by classifying nominal groups, but adjectival (attributive) and participle groups (actions) are also common. In other words, overall, targets can be considered concepts (Waldis et al., 2018). Th"
2021.woah-1.19,D18-1504,0,0.0156249,"tified by attention mechanisms realized, e.g., in an LSTM (Wang et al., 2016), CNN (Liu and Shen, 2020) or an alternative common deep NN model. The targets are, as a rule, predefined, such that the challenge consists in analysing the sentiment of tweets towards these predefined targets; cf., e.g., (Tang et al., 2016; Dong et al., 2014). The problem of open-class target identification has not been broadly investigated and sometimes solved simply as a named entity recognition problem due to the nature of the data in wihch the targets are often represented by proper names (Mitchell et al., 2013; Ma et al., 2018). However, targets in hate speech texts go far beyond named 181 entities, and the overall task is inverse to targetoriented sentiment classification: given a known category (hate speech of negative sentiment as a rule), we have to identify the hate target and its corresponding “opinioned” aspect. Still, our proposal is similar to the modern approaches to aspectoriented sentiment analysis in the sense that we also use an NN model (in our case, LSTM-based encoder) with attention mechanisms for initial hate speech target and aspect candidates identification, before a domain-adaptation post-proces"
2021.woah-1.19,D13-1171,0,0.0774037,"Missing"
2021.woah-1.19,D19-1474,0,0.0128617,"ous works use a similar terminology as we do, but with a different meaning. For instance, Zainuddin et al. (2017, 2018, 2019) aim to identify the sentiment (positive or negative) of the author of a given post towards a range of specific hate speech categories (e.g., ‘race’ and ‘gender’), which they call “aspect”. In (Gautam et al., 2020), tweets related to the MeToo movement are annotated manually with respect to five different linguistic “aspects”: relevance, stance, hate speech, sarcasm, and dialogue acts. In this case, too, the interpretation of the notion of aspect is different from ours. Ousidhoum et al. (2019) define five different “aspects” that include specific targets, among others: (i) whether the text is direct or indirect; (ii) whether it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (iii) whether it is against an individual or a group of people; (iv) the name of the targeted group (16 common target groups are identified); and (v) the annotators’ sentiment. Fersini et al. (2018) are also concerned with target detection in that they determine whether the messages were purposely sent to a specific target or to many potential receivers (e.g., groups of women"
2021.woah-1.19,2020.acl-main.394,0,0.0298933,"d; see, e.g., (8). (8) I asked that question recently and actually got an answer http://t.co/oD98sptcGT. We discard such posts in our current experiments. 3 Related Work As mentioned in Section 1, most of the works on online hate speech focused on the task of classifying social media posts with respect to predefined typologies of rather coarse-grained hate speech categories, such as ‘hate speech’, ‘racism’, ‘sexism’, ‘offense’, etc. (Schmidt and Wiegand, 2017; Davidson et al., 2017a; Fortuna and Nunes, 2018; Swamy et al., 2019; Arango et al., 2019; Salminen et al., 2020; Kennedy et al., 2020; Rajamanickam et al., 2020).5 Vidgen and Derczynski (2020) distinguish between binary classification (as in (Alfina et al., 2017; Ross et al., 2017)), multi-class classification into several hate speech categories (e.g., ‘racism’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e.g., ‘denouncing’, ‘facts’, ‘humor’, ‘hypocrisy’ and others) and themes (e.g., ‘crimes’, ‘culture’, ‘islamization’, 4 In (4), feminist is a classifying attribute o"
2021.woah-1.19,2020.acl-main.486,0,0.0177421,"of people; (iv) the name of the targeted group (16 common target groups are identified); and (v) the annotators’ sentiment. Fersini et al. (2018) are also concerned with target detection in that they determine whether the messages were purposely sent to a specific target or to many potential receivers (e.g., groups of women). In (Silva et al., 2016), targets are identified using a short list of offensive words built drawing upon Hatebase6 and a single template “&lt;one word&gt; people” to capture “black people”, “stupid people”, “rude people”, etc. Our work also aligns with Mathew et al. (2020) and Sap et al. (2020) in the sense that Mathew et al. (2020) annotate a hate speech dataset at the word and phrase level, capturing human rationales for the labelling (which is similar to the target–aspect labelling), while Sap et al. (2020) propose to understand and fight hate speech prejudices with accurate underlying explanations. However, Mathew et al. (2020) take into account only three labels (‘hate’, 6 http://www.hatebase.org/ ‘offensive’, and ‘normal’) and ten target communities performing supervised classification, while we aim at retrieving and distinguishing open-class targets and aspects in a semi-supe"
2021.woah-1.19,W17-1101,0,0.0170602,", as can’t cook in (7): (7) Scoring like a Cunt because you can’t cook for shit isn’t fighting hard Kat. In some posts, no targets and/or aspects can be identified; see, e.g., (8). (8) I asked that question recently and actually got an answer http://t.co/oD98sptcGT. We discard such posts in our current experiments. 3 Related Work As mentioned in Section 1, most of the works on online hate speech focused on the task of classifying social media posts with respect to predefined typologies of rather coarse-grained hate speech categories, such as ‘hate speech’, ‘racism’, ‘sexism’, ‘offense’, etc. (Schmidt and Wiegand, 2017; Davidson et al., 2017a; Fortuna and Nunes, 2018; Swamy et al., 2019; Arango et al., 2019; Salminen et al., 2020; Kennedy et al., 2020; Rajamanickam et al., 2020).5 Vidgen and Derczynski (2020) distinguish between binary classification (as in (Alfina et al., 2017; Ross et al., 2017)), multi-class classification into several hate speech categories (e.g., ‘racism’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e"
2021.woah-1.19,P16-1162,0,0.0263931,"Missing"
2021.woah-1.19,K19-1088,0,0.0401513,"Missing"
2021.woah-1.19,C16-1311,0,0.0201241,") for a recent comprehensive survey of aspect-oriented sentiment analysis. In some (more traditional) works, aspects and their values are identified in separate stages (Hu and Liu, 2004; Hai et al., 2011). In more recent works, both tasks are addressed by one model, with aspects being partially identified by attention mechanisms realized, e.g., in an LSTM (Wang et al., 2016), CNN (Liu and Shen, 2020) or an alternative common deep NN model. The targets are, as a rule, predefined, such that the challenge consists in analysing the sentiment of tweets towards these predefined targets; cf., e.g., (Tang et al., 2016; Dong et al., 2014). The problem of open-class target identification has not been broadly investigated and sometimes solved simply as a named entity recognition problem due to the nature of the data in wihch the targets are often represented by proper names (Mitchell et al., 2013; Ma et al., 2018). However, targets in hate speech texts go far beyond named 181 entities, and the overall task is inverse to targetoriented sentiment classification: given a known category (hate speech of negative sentiment as a rule), we have to identify the hate target and its corresponding “opinioned” aspect. Sti"
2021.woah-1.19,2020.acl-main.110,0,0.0290129,"Missing"
2021.woah-1.19,D16-1058,0,0.0275855,"s, celebrities) and “aspects” are characteristics or components of a given entity (Kobayashi et al., 2007; Nikoli´c et al., 2020). For each identified aspect, the “sentiment value” aligned with it is extracted; see, e.g., (Nazir et al., 2020) for a recent comprehensive survey of aspect-oriented sentiment analysis. In some (more traditional) works, aspects and their values are identified in separate stages (Hu and Liu, 2004; Hai et al., 2011). In more recent works, both tasks are addressed by one model, with aspects being partially identified by attention mechanisms realized, e.g., in an LSTM (Wang et al., 2016), CNN (Liu and Shen, 2020) or an alternative common deep NN model. The targets are, as a rule, predefined, such that the challenge consists in analysing the sentiment of tweets towards these predefined targets; cf., e.g., (Tang et al., 2016; Dong et al., 2014). The problem of open-class target identification has not been broadly investigated and sometimes solved simply as a named entity recognition problem due to the nature of the data in wihch the targets are often represented by proper names (Mitchell et al., 2013; Ma et al., 2018). However, targets in hate speech texts go far beyond named 1"
2021.woah-1.19,N16-2013,0,0.422319,"k of classifying social media posts with respect to predefined typologies of rather coarse-grained hate speech categories, such as ‘hate speech’, ‘racism’, ‘sexism’, ‘offense’, etc. (Schmidt and Wiegand, 2017; Davidson et al., 2017a; Fortuna and Nunes, 2018; Swamy et al., 2019; Arango et al., 2019; Salminen et al., 2020; Kennedy et al., 2020; Rajamanickam et al., 2020).5 Vidgen and Derczynski (2020) distinguish between binary classification (as in (Alfina et al., 2017; Ross et al., 2017)), multi-class classification into several hate speech categories (e.g., ‘racism’, ‘sexism’, and ‘none’ in (Waseem and Hovy, 2016b)), different strengths of abuse classification (e.g., ‘hateful’, ‘offensive’ and ‘neutral’ contents as in (Davidson et al., 2017b)), classification into different types of statements (e.g., ‘denouncing’, ‘facts’, ‘humor’, ‘hypocrisy’ and others) and themes (e.g., ‘crimes’, ‘culture’, ‘islamization’, 4 In (4), feminist is a classifying attribute of novels (see also Section 4) and should thus be part of the target. 5 Cf. (Fortuna et al., 2020) for a list of categories used in the most common hate speech datasets. 180 ‘rapism’ and others) as in (Chung et al., 2019)), and classification of diffe"
bohnet-wanner-2010-open,levy-andrew-2006-tregex,0,\N,Missing
bohnet-wanner-2010-open,C00-1007,0,\N,Missing
bohnet-wanner-2010-open,P03-1011,0,\N,Missing
bohnet-wanner-2010-open,P03-1003,0,\N,Missing
bohnet-wanner-2010-open,knight-al-onaizan-1998-translation,0,\N,Missing
bohnet-wanner-2010-open,P03-2041,0,\N,Missing
bohnet-wanner-2010-open,J97-3002,0,\N,Missing
bohnet-wanner-2010-open,W02-2105,0,\N,Missing
bohnet-wanner-2010-open,A97-1039,0,\N,Missing
bohnet-wanner-2010-open,W01-0807,1,\N,Missing
bohnet-wanner-2010-open,N03-1019,0,\N,Missing
bouayad-agha-etal-2014-exercise,recasens-etal-2010-typology,0,\N,Missing
bouayad-agha-etal-2014-exercise,W11-1902,0,\N,Missing
bouayad-agha-etal-2014-exercise,J13-4004,0,\N,Missing
bouayad-agha-etal-2014-exercise,D10-1048,0,\N,Missing
C10-1012,C00-1007,0,0.036318,"ent. Its disadvantage is that it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators"
C10-1012,P98-1026,0,0.035183,"Missing"
C10-1012,P01-1024,0,0.0300827,"om the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the sec"
C10-1012,W96-0501,0,0.0268062,"ubsequent integration of other generation tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to"
C10-1012,D08-1019,0,0.0243456,"syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, mul"
C10-1012,N09-2057,0,0.382889,"is the head, 2 if w2 is the head, etc. and else 0; dist is the position within the constituent; contains-? is a boolean value which is true if the sentence contains a question mark and false otherwise; pos-head is the position of the head in the constituent) 4.2 Dependency Tree Linearization Since we use unordered dependency trees as syntactic structures, our realizer has to find the optimal linear order for the lexemes of each dependency tree. Algorithm 4 shows our linearization algorithm. To order the dependency tree, we use a one classifier-approach for all languages—in contrast to, e.g., Filippova and Strube (2009), who use a two-classifier approach for German.1 The algorithm is again a beam search. It starts with an elementary list for each node of the dependency tree. Each elementary list is first extended by the children of the node in the list; then, the lists are extended stepwise by the children of the newly added nodes. If the number of lists during this procedure exceeds the threshold of 1000, the lists are sorted in accordance with their score, and the first 1000 are kept. The remaining lists are removed. Afterwards, the score of each list is adjusted according to a global score function which"
C10-1012,P01-1029,0,0.0295193,"or is directly derived from the analyzer, which makes both approaches not directly comparable. 5.3 Discussion The overall performance of our SVM-based deep sentence generator ranges between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words wi"
C10-1012,W09-1201,0,0.0950376,"Missing"
C10-1012,P09-1091,0,0.394975,"ore (ULA) is the proportion of correct tokens that are assigned the correct head. To assess the quality of linearization, we use three different evaluation metrics. The first metric is the per-phrase/per-clause accuracy (acc snt.), which facilitates the automatic evaluation of results: acc = correct constituents all constituents As second evaluation metric, we use a metric related to the edit distance: di = 1 − m total number of words (with m as the minimum number of deletions combined with insertions to obtain the correct order (Ringger et al., 2004)). To be able to compare our results with (He et al., 2009) and (Ringger et al., 2004), we use the BLEU score as a third metric. For the asessment of the quality of the word form generation, we use the accuracy score. The accuracy is the ratio between correctly generated word forms and the entire set of generated word forms. For the evaluation of the sentence realizer as a whole, we use the BLEU metric. 5.2 Experimental Results Table 4 displays the results obtained for the isolated stages of sentence realization and of the realization as a whole, with reference to a baseline and to some state-of-the-art works. The baseline is the deep sentence realiza"
C10-1012,P95-1034,0,0.032851,"syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increa"
C10-1012,P98-1116,0,0.894255,"suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically annotated, or, even better, multilevel annotated corpora. Only then can they deal with such crucial generation issues as sentence planning, linearization and morphologization. Multilevel annotated corpora are increasingly available for multiple"
C10-1012,W02-2103,0,0.227142,"t it requires at least syntactically annotated corpora of significant size (Bangalore et al., 2001). Given the aspiration of NLG to start from numeric time series or conceptual or semantic structures, syntactic annotation even does not suffice: the corpora must also be at least semantically annotated. Up to date, deep stochastic sentence realization was hampered by the lack of multiple-level annotated corpora. As a consequence, available stochastic sentence generators either take syntactic structures as input (and avoid thus the need for multiple-level annotation) (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or draw upon hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). Most of the known stochastic sentence generators use syntactically annotated corpora, performing the projection to the surface in one stage. However, in full-fledged text generation, sentence realization usually starts from semantic (predicate-argument) structures. To be able to deal with semantic structures, stochastic generators require semantically an"
C10-1012,A97-1039,0,0.0615238,"ion tasks such as referring expression generation, ellipsis generation, and aggregation. As a matter of fact, this generator instantiates the Reference Architecture for Generation Systems (Mellish et al., 2006) for linguistic generation. A more practical advantage of the presented deep stochastic sentence generator (as, in principle, of all stochastic generators) is that, if trained on a representative corpus, it is domainindependent. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al., 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources. Deep stochastic sentence realizers have the potential to become real off-the-shelf modules. Our realizer is freely available for download at http://www.recerca.upf.edu/taln. 3 We are currently working on a generation-oriented multilevel annotation of corpora for a number of languages. The corpora will be made available to the community. 105 Acknowledgments Many thanks to the three anonymous reviewers for thei"
C10-1012,W00-0306,0,0.0191148,"Missing"
C10-1012,J05-1004,0,0.0546622,"0’ for “A0 realized as a relative clause”, and ‘AM-MNR’ for “manner modifier”. As can be seen, 6 out of the total of 14 edges in the complete representation of this example have been added by Algorithm 1. We still did not finish the formal evaluation of the principal changes necessary to adapt the PropBank annotation for generation, nor the quality of our completion algorithm. However, the need of an annotation with generation in mind is obvious. Completing the Semantic Annotation The semantic annotation of sentences in CoNLL ’09 shared task corpora follows the PropBank annotation guidelines (Palmer et al., 2005). Prob99 a Algorithm 1: Complete semantic graph //si is a semantic graph and yi a dependency tree // si = hNsi , Lsi , Esi i, where Nsi is the set of nodes // Lsi the set of edge labels // Esi ⊆ Ns × Ns × Ls is the set of edges for i ← 1 to |I |// iteration over the training examples let ry ∈ yi be the root node of the dependency tree // initialization of the queue nodeQueue ← children(ry ) while nodeQueue 6= ∅ do ny ← removeFirst(nodeQueue) // breath first: add nodes at the end of the queue nodeQueue ← nodeQueue ∪ children(ny ) nys ← sem(ny ); pys ← sem(parent(ny )) //get the semantic equival"
C10-1012,C04-1097,0,0.73441,"n The morphological realization algorithm selects the edit script in accordance with the highest score for each lemma of a sentence obtained during training (see Algorithm 2 above) and applies then the scripts to obtain the word forms; cf. Algorithm 5. Table 2 lists the feature schemas used for morphological realization. 5 Experiments To evaluate the performance of our realizer, we carried out experiments on deep generation of Chinese, English, German and Spanish, starting from CoNLL ’09 shared task corpora. The size of the test sets is listed in Table 3.2 2 As in (Langkilde-Geary, 2002) and (Ringger et al., 2004), we used Section 23 of the WSJ corpus as test set for English. 102 Algorithm 3: Semantic generation Algorithm 4: Dependency tree linearization //si , y semantic graph and its dependency tree for i ← 1 to |I |// iteration over the training examples // build an initial tree for all n1 ∈ si do trees ← {} // initialize the constructed trees list for all n2 ∈ si do if n1 6= n2 then for all l ∈ dependency-labels do trees = trees ∪ {(synt(n1 ),synt(n2 ),l)} trees ← sort-trees-descending-to-score(trees) trees ← look-forward(1000,sublist(trees,20)) //assess at most 1000 edges of the 20 best trees tree"
C10-1012,E09-1097,0,0.189557,"s between 0.611 (for German) and 0.688 (for Chinese) of the BLEU score. HALogen’s (Langkilde-Geary, 2002) scores range between 0.514 and 0.924, depending on the completeness of the input. The figures are not directly comparable since HALogen takes as input syntactic structures. However, it gives us an idea where our generator is situated. Traditional linearization approaches are rulebased; cf., e.g., (Br¨oker, 1998; Gerdes and Kahane, 2001; Duchier and Debusmann, 2001), and (Bohnet, 2004). More recently, statistic language models have been used to derive word order, cf. (Ringger et al., 2004; Wan et al., 2009) and (Filippova and Strube, 2009). Because of its partially free order, which is more difficult to handle than fixed word order, German has often been worked with in the context of linearization. Filippova and Strube (2009) adapted their linearization model originally developed for German to English. They use two classifiers to determine the word order in a sentence. The first classifier uses a trigram LM to order words within constituents, and the second (which is a maximum entropy classifier) determines the order of constituents that depend on a finite verb. For English, we achieve with our"
C10-1012,C98-1112,0,\N,Missing
C10-1012,C98-1026,0,\N,Missing
C10-1012,W01-0520,0,\N,Missing
C12-2082,W09-1210,0,0.0321107,"tion of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parsers. Instead, we used for each parser its own default configuration, which does not guarantee an"
C12-2082,bosco-etal-2000-building,0,0.131616,"rs and show that the precision and recall of hard-to-parse relations can be quite different, depending on the tag granularity in the annotation, that is, if the annotation contains or not morphological and/or semantic information. In contrast, our goal is to provide evidence that the creation of annotations that capture significant fine-grained distinctive features of the grammar (and only the grammar) of a language does not need to harm significantly the performance of the parsers. Consider as two 3 Some other works present a hierarchical organization of grammatical relations (in particular (Bosco et al., 2000), (Briscoe et al., 2002), and (Marneffe et al., 2006)), but those hierarchies are not used to test the impact of the tagset granularity on the results of a parser. 843 such fine-grained distinctive features the relations modal and direct-object in the following two sentences. As indicated, only the direct object can be pronominalized by a clitic pronoun and moved before the governing verb, without that a pro-verb is needed: Juan puede-modal→ venir mañana, lit. ‘John might come tomorrow’ (Juan lo puede *(hacer)), and Juan puede-dobj→ venir mañana, lit. ‘John is able to come tomorrow’ (Juan lo p"
C12-2082,bosco-etal-2010-comparing,0,0.0506274,"Missing"
C12-2082,W06-2920,0,0.135168,"toolkit provided with the parser. For the other parsers, we used the official CoNLL’06 evaluation toolkit. The LAS figures for each parser and for each version of the annotation are 4 One can always imagine some statistical “disambiguation” based on the context in which the construction is used, but the amount of data needed could be prohibitive—at least for Spanish—and eventually, the only way would probably be to imply human experts for the revision of the annotation. 5 Bohnet’s parser uses CoNLL’09 14-column format, while the other three need to be trained on the CoNLL’06 10-column format (Buchholz and Marsi, 2006), but the available information is exactly the same, whatever the format: word positions, word forms, PoS, lemmas, (all of which kept the same in our experiments), and dependencies. 844 shown in Table 2. The graphic on the right of Table 2 shows how each parser reacts to and how its performance varies with the increasing number of relations in the tagset. We can observe that all four parsers behave similarly: their accuracy is very constant from 15 to 44 SyntRels, and decreases with 60 SyntRels. We also notice that there is a significant difference between Bohnet, Merlo and Malt’s LAS progress"
C12-2082,W09-1207,0,0.0292797,"of the experiments In our experiments, we used the four tagsets introduced in Section 2. The annotation of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parse"
C12-2082,W09-1205,0,0.0323256,"sed the four tagsets introduced in Section 2. The annotation of the corpus with the most detailed tagset of 60 SyntRels has been obtained from the original annotation in AnCora (Taulé et al., 2008), which has been adapted, revised and enriched manually. Starting from the most fine-grained annotation, we derived automatically the other three, ending up with four different treebanks for the same corpus. Four reference parsers have been used. Three of them are the top three parsers for Spanish in the CoNLL Shared Task 2009 (Hajiˇc et al., 2009): Che’s (Che et al., 2009), henceforth Che, Merlo’s (Gesmundo et al., 2009), henceforth Merlo, and Bohnet’s (Bohnet, 2009), henceforth Bohnet. The fourth, the Malt Parser (Nivre et al., 2007), henceforth Malt, has been chosen because it is a very broadly used syntactic dependency parser. Malt and Merlo are transition based, while Bohnet and Che are graph based. In our experiments, all of them processed non-projective dependency trees. Each parser contains its own configuration options, which depend on the parsing approach, the learning techniques, etc. Therefore, it was not possible to apply the same setup to all parsers. Instead, we used for each parser its own defa"
C12-2082,W07-2416,0,0.0815438,"s to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency annotation scheme of the Penn Treebank has served as blueprint for annotation schemes of a series of treebanks in different languages and is thus a de facto standard. See (Marcus et al., 1993) for the original consituency annotation, and (Johansson and Nugues, 2007) for the conversion to one-word-per-line dependency representations. 2 “Minimal” refers here not only to the number of tags, but also to the level of precision of the syntactic tags. Indeed, many corpora mix several levels of representation (e.g., syntax, semantics, lexicon, etc.) such that the number of syntactic relations does not necessarily reflect the level of idiosyncracy of the annotation. 842 or demoted, etc. For any kind of dependent, we need to capture the canonical order with respect to its governor, the part-of-speech of the governor, the part-of-speech of the prototypical element"
C12-2082,P03-1054,0,0.00546165,"obl-obj1/2/3 and noun-compl. In the third column (31 SyntRels), obl-obj and agent are fused into one relation obl-obj, defined as “prepositional object which cannot be pronominalized”. Finally, in the last column (15 SyntRels), one tag OOBJ gathers any object which cannot be pronominalized, as opposed to IOBJ and DOBJ, which can be replaced by a dative and an accusative pronoun, respectively. 3 Experiments 3.1 Background A number of experiments on different granularities of annotation and their impact on the performance of probabilistic parsers are known from the literature; see in particular Klein and Manning (2003) and Petrov et al. (2006), who show the benefits of splitting generic part-of-speech tags (e.g., NP, VP, etc.) into more precise subcategories for the derivation of accurate probabilistic context-free grammars (PCFG). Our proposal differs from these works in that they focus on constituency parsing and part-of-speech tags, whereas we tackle dependency parsing and edge labels.3 But more importantly, the goals are different. Thus, they target the improvement of parsing accuracy, and for that they infer, with simple rules, from the training data (categorial) information which is more specific than"
C12-2082,J93-2004,0,0.0388651,"into the scheme. Using more or less fine-grained criteria allows us to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency annotation scheme of the Penn Treebank has served as blueprint for annotation schemes of a series of treebanks in different languages and is thus a de facto standard. See (Marcus et al., 1993) for the original consituency annotation, and (Johansson and Nugues, 2007) for the conversion to one-word-per-line dependency representations. 2 “Minimal” refers here not only to the number of tags, but also to the level of precision of the syntactic tags. Indeed, many corpora mix several levels of representation (e.g., syntax, semantics, lexicon, etc.) such that the number of syntactic relations does not necessarily reflect the level of idiosyncracy of the annotation. 842 or demoted, etc. For any kind of dependent, we need to capture the canonical order with respect to its governor, the part-"
C12-2082,de-marneffe-etal-2006-generating,0,0.0641374,"to-parse relations can be quite different, depending on the tag granularity in the annotation, that is, if the annotation contains or not morphological and/or semantic information. In contrast, our goal is to provide evidence that the creation of annotations that capture significant fine-grained distinctive features of the grammar (and only the grammar) of a language does not need to harm significantly the performance of the parsers. Consider as two 3 Some other works present a hierarchical organization of grammatical relations (in particular (Bosco et al., 2000), (Briscoe et al., 2002), and (Marneffe et al., 2006)), but those hierarchies are not used to test the impact of the tagset granularity on the results of a parser. 843 such fine-grained distinctive features the relations modal and direct-object in the following two sentences. As indicated, only the direct object can be pronominalized by a clitic pronoun and moved before the governing verb, without that a pro-verb is needed: Juan puede-modal→ venir mañana, lit. ‘John might come tomorrow’ (Juan lo puede *(hacer)), and Juan puede-dobj→ venir mañana, lit. ‘John is able to come tomorrow’ (Juan lo puede (hacer)). If the annotation of the relations doe"
C12-2082,P06-1055,0,0.0293544,"In the third column (31 SyntRels), obl-obj and agent are fused into one relation obl-obj, defined as “prepositional object which cannot be pronominalized”. Finally, in the last column (15 SyntRels), one tag OOBJ gathers any object which cannot be pronominalized, as opposed to IOBJ and DOBJ, which can be replaced by a dative and an accusative pronoun, respectively. 3 Experiments 3.1 Background A number of experiments on different granularities of annotation and their impact on the performance of probabilistic parsers are known from the literature; see in particular Klein and Manning (2003) and Petrov et al. (2006), who show the benefits of splitting generic part-of-speech tags (e.g., NP, VP, etc.) into more precise subcategories for the derivation of accurate probabilistic context-free grammars (PCFG). Our proposal differs from these works in that they focus on constituency parsing and part-of-speech tags, whereas we tackle dependency parsing and edge labels.3 But more importantly, the goals are different. Thus, they target the improvement of parsing accuracy, and for that they infer, with simple rules, from the training data (categorial) information which is more specific than what is directly availab"
C12-2082,D07-1066,0,0.022075,"Missing"
C12-2082,taule-etal-2008-ancora,0,0.460298,"t it is possible to reach a good balance between the accuracy of a parser and the richness of the linguistic annotation. They also show that the principles that we applied when designing the hierarchical annotation schema are valid and may be used for the design of other annotation schemes in the future. 2 Hierarchical syntactic annotation scheme The hierarchical annotation scheme in Table 1 has been developed for Spanish on a small corpus of 3513 sentences (100892 words, see (Mille et al., 2009); corpus available at UPF–TALN webpage), which constitutes a section of the Spanish corpus AnCora (Taulé et al., 2008). The general idea underlying this scheme is to apply only syntactic (rather than also semantic) criteria in order to identify each grammatical tag that is to be introduced into the scheme. Using more or less fine-grained criteria allows us to control the level of granularity of the tagset. We do not orientate our scheme towards any particular linguistic theory; the selected criteria are dictated by syntactic behaviour observed in the language in question (in our case, Spanish). For instance, for dependents of verbs, we need to capture whether they can be cliticized, promoted 1 The dependency"
C12-2082,W09-1201,0,\N,Missing
C14-1133,W13-2322,0,0.0603372,"Missing"
C14-1133,D12-1133,1,0.846199,"tically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set. POS LEMMA LAS UAS 96.14 91.10 78.64 86.49 Table 1: Results of Bohnet and Nivre’s surface-syntactic parser on the development set In what follows, we first present the realization of the SSyntS–DSyntS transducer and then the realization of the baseline. 3.1 SSyntS–DSyntS transducer As outlined in Section 2.2, the SSyntS–DSyntS transducer is composed of three s"
C14-1133,W06-2920,0,0.0316683,"c parsing pipeline 5 Related Work To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated synta"
C14-1133,J08-1003,0,0.0551117,"Missing"
C14-1133,W09-1207,0,0.0198429,"also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules i"
C14-1133,J07-4004,0,0.0802815,"Missing"
C14-1133,fillmore-etal-2002-framenet,0,0.0465585,"matical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attributive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank and Semantic Frame structures are not always connected, may contain either individual lexical items or phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or sentential). In other words, they"
C14-1133,W09-1205,0,0.0190178,"int parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997). However, this is not to say that the idea of the surface→surface syntax→deep syntax pipeline is"
C14-1133,P03-1046,0,0.0820135,"Missing"
C14-1133,W12-3602,0,0.0984375,", publish-COORD→or-II→perish, and so on. APPEND subsumes all parentheticals, interjections, direct addresses, etc., as, e.g., in Listen, John!: listen-APPEND→John. DSyntSs thus show a strong similarity with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated; (ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv) they are connected.4 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows: Definition 2 (DSyntS) An DSyntS of a language L is a quintuple TDS = hN, A, λls →n , ρrs →a , γn→g i defined over the full lexical items Ld of L, the set of semantic grammemes Gsem , and the set of deepsyntactic relations Rdsynt , where • the set N of nodes and the set A of directed arcs form a connected tree, • λls →n assigns to each n ∈ N an ls ∈ Ld , • ρrs →a assigns to each a ∈ A an r ∈ Rdsynt , and • γn→g assigns to each λls →n (n) a set of grammemes Gt ∈ Gsem . Consider in Figure 1 an example for an"
C14-1133,W07-2416,0,0.483157,"(forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output. 1 Introduction Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from l"
C14-1133,W08-2123,0,0.0217204,"rsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but"
C14-1133,P86-1038,0,0.257327,"proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt , i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for verbs. The value of lexssynt can be any (either full or functional) lexical item; in graphical representations of SSyntSs, usually only the value of lexssynt is shown. The edge labels of a SSyntS are grammatical functions ‘subj’, ‘dobj’, ‘det’, ‘modif’, etc. In other words, SSyntSs are syntactic structures of the kind as encountered"
C14-1133,C12-2082,1,0.889085,"ucer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf."
C14-1133,W13-3724,1,0.686421,"less than a dozen grammemes, etc. 3 Experiments In order to validate the outlined SSyntS–DSyntS transduction and to assess its performance in combination with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of 1405 experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentenc"
C14-1133,C69-0101,0,0.518574,"r outcome. Section 5 summarizes the related work, before in Section 6 some conclusions and plans for future work are presented. 2 Fundamentals of SSyntS–DSyntS transduction Before we set out to discuss the principles of the SSyntS–DSynt transduction, we must specify the DSyntSs and SSyntSs as used in our experiments. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt ,"
C14-1133,W08-2121,0,0.228819,"Missing"
C14-1133,taule-etal-2008-ancora,0,0.0989872,"Missing"
C14-1133,P08-1101,0,0.0611179,"Missing"
C14-1133,W09-1201,0,\N,Missing
C14-1133,P01-1033,0,\N,Missing
C14-1133,J05-1004,0,\N,Missing
C14-1133,Q13-1018,0,\N,Missing
C14-1133,D07-1096,0,\N,Missing
C14-1133,P13-2017,0,\N,Missing
C14-1133,ballesteros-nivre-2012-maltoptimizer-system,1,\N,Missing
C16-1037,christodoulides-2014-praaline,0,0.145802,"w, no action can be scripted based upon smaller units than tiers. Notwithstanding, Praat is a powerful tool, user-friendly, programmable, freely available, running on many platforms, and actively maintained (Mertens, 2004). Due to all these characteristics, a number of Praat-based tools have appeared over the last decade, among them, e.g., ProsodyPro (Xu, 2013). However, many of these tools create a set of parallel tiers, assigning different labels to these tiers, and then, output extracted acoustic features in a text format for further processing using other platforms. For example, Praaline (Christodoulides, 2014) process the txt file externally using the R statistical package. 3 Methodology The main goal of the present work is the development and implementation of a methodology that serves as a scaffolding upon which further improvements and empirical studies can be built upon. One of the requirements, as introduced before, is that this methodology is versatile in the sense that it is languageindependent and is able to describe prosodic cues in natural language using a parametric approach. A second goal of our work is to implement a prosody tagger embedded into a Praat-based platform that allows featu"
C16-1323,J75-4040,0,0.7351,"Missing"
C16-1323,S15-2151,0,0.0388232,"database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g."
C16-1323,P10-2020,0,0.0292908,"ore valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Compu"
C16-1323,D14-1110,0,0.0460532,"m a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word2Vec,7 so that words and synsets can be learned jointly in a single training. The output is a vector space of word and synset embeddings that we use as collocates model. 3 Methodology In this section, we provide a detailed description of the algorithm behind the construction of CWN. The system takes as input the WordNet lexical database and a set of collocation lists pertaining to predefined semantic categories, and outputs CWN. First, we collect training data and perform automatic disambiguation (Section 3.1). Then, we use this disambiguated data for"
C16-1323,P89-1010,0,0.824988,"erally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422"
C16-1323,D15-1084,1,0.857019,"the idea of ‘intense’ and ‘perform’ than ‘begin to perform’), the number of instances per category in our training data also varies significantly (see Table 1). Our training dataset consists at this stage of pairs of plain words, with the inherent ambiguity this gives raise to. We surmount this challenge by applying a disambiguation strategy based on the notion that, from all the available senses for a collocation’s base and collocate, their correct senses are those which are most similar. This is a strategy that has been proved effective in previous concept-level disambiguation tasks (Delli Bovi et al., 2015). Formally, let us denote the S ENS E MBED vector space as S, and our original text-based training data as T. For each training collocation hb, ci ∈ T we consider all the available lexicalizations (i.e., senses) for both the base b and the collocate c in S, namely Lb = {lb1 ...lbn }, and Lc = {lc1 ...lcm }, and their corresponding set of sense embeddings Vb = {~v 1b , ..., ~v nb } and Vc = 4 We downloaded the pre-trained sense embeddings at http://lcl.uniroma1.it/sensembed/. ebiquity.umbc.edu/blogger/2013/05/01/umbc-webbase-corpus-of-3b-english-words/ 6 As explained above, a synset is a set co"
C16-1323,D16-1041,1,0.848512,".g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied language such as blog posts or news items. Then, we cons"
C16-1323,esuli-sebastiani-2006-sentiwordnet,0,0.0248628,"var, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information ha"
C16-1323,P08-1017,0,0.0329205,") applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and"
C16-1323,N15-1184,0,0.145399,"et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations ha"
C16-1323,P14-1113,0,0.0269,"eves the highest MRR score, which we claim to be the most relevant measure, as it rewards cases where the first ranked returned collocation is correct without measuring in the retrieved collocates at other positions. Moreover, let us highlight the importance of two main factors. First, the need for a well-defined semantic relation between bases and collocates. It has been shown in other tasks that exploit linear transformations between embeddings models that even for one single relation there may be clusters that require certain specificity in the domain or semantic of the data (see Fu et al. Fu et al. (2014) for a discussion of this phenomenon in the task of taxonomy learning). Second, the importance of having a reasonable amount of training pairs so that the model can learn the idiosyncrasies of the semantic relation that is being encoded (e.g., Mikolov et al. (2013b) report a major increase in performance as training data increases in several orders of magnitude). This is reinforced in our experiments, where we obtain the highest MAP score for ‘intense’, the semantic category for which we have the largest training data available. 13 See Bian et al. (2008) for an in-depth analysis of these metri"
C16-1323,Y13-2006,0,0.0206674,"resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Computational Lin"
C16-1323,S13-1005,0,0.0203441,"al word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this model we exploit distributional information from a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word"
C16-1323,P12-1092,0,0.364618,"ry, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, h"
C16-1323,P15-1010,0,0.0362667,"In its 3.6 release version, BabelNet is composed of 6.1M concepts and 7.7M named entities. 3 For example, the concept defined as principal activity in your life that you do to earn money is represented by the synset {occupation, business, job, line of work, line}, where occupation, business, job, line of work, and line are senses/lexicalizations of the given synset. 2 3423 its corresponding synsetnwn , provided there exists one. In what follows, we briefly describe two different vector space models that are used in this paper for the task of synset-level collocation discovery. S ENS E MBED4 (Iacobacci et al., 2015) is a knowledge-based approach for obtaining latent continuous representations of individual word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this"
C16-1323,N15-1169,0,0.0589463,"grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community. 1 Introduction The embedding of cues about how we perceive concepts and how these concepts relate and generalize across different domains gives knowledge resources the capacity of generalization, which lies at the core of human cognition (Yu et al., 2015) and is also central to many Natural Language Processing (NLP) applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and"
C16-1323,N13-1090,0,0.716301,"rence on Computational Linguistics: Technical Papers, pages 3422–3432, Osaka, Japan, December 11-17 2016. the manual inclusion of lexical functions from Explanatory Combinatorial Lexicology (ECL) (Mel’ˇcuk, 1996) into the Spanish EuroWordNet (Wanner et al., 2004). Given the importance of collocations for a series of NLP applications (e.g. machine translation, text generation or paraphrasing), we propose to fill this gap by putting forward a new methodology which exploits intrinsic properties of state-of-the-art semantic vector space models and leverages the transformation matrix introduced by Mikolov et al. (2013b) in a word-level machine translation task. As a result, we release an extension of WordNet with detailed collocational information, named ColWordNet (CWN). This extension is carried out by means of the inclusion of novel edges, where each edge encodes a collocates-with relation, as well as the semantics of the collocation itself. For example, given the pair col:intense of synsets desire.n.01 and ardent.a.01, a novel relation −−−−−−−→ is introduced, where ‘inx tense’ is the semantic category denoting intensification, and x is the confidence score assigned by our algorithm. The remainder of th"
C16-1323,N16-1018,0,0.0583307,"Missing"
C16-1323,P16-2074,0,0.0357694,"ocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their collocationality into the model. 4.2.1 Collocational"
C16-1323,P15-2004,0,0.0213646,"ings model.14 To this end, we extract collocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their colloca"
C16-1323,P13-1132,0,0.0216348,"onceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idio"
C16-1323,L16-1367,1,0.874671,"Missing"
C16-1323,P15-2108,0,0.102421,"been explored so far in the literature (e.g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied lang"
C16-1323,wanner-etal-2004-enriching,1,0.827897,"Missing"
C16-1323,S16-1169,0,\N,Missing
C16-2046,christodoulides-2014-praaline,0,0.0279865,"ges 218–222, Osaka, Japan, December 11-17 2016. Figure 1: Standard Praat visualization:Annotation using tiers. In order to remedy these limitations, advanced users have found workarounds. Thus, the first limitation is remedied by either extracting information to an external file, as ProsodyPro (Xu, 2013) does, or by annotating in parallel tiers with cloned time segments and different labels, as shown in Figure 1. To circumvent the second limitation, experienced users tend to program in external platforms and call Praat for performing specific speech processing routines. For example, Praaline (Christodoulides, 2014) extracts acoustic information from Praat for analysis in the R statistic package (R Core Team, 2013) and visualization in the Sonic visualizer (Cannam et al., 2010). However, these workarounds make the use of Praat cumbersome. The Praat on the Web tool presented in this paper aims to address the aforementioned Praat limitations. More precisely, it upgrades Praat along the lines observed in state-of-the-art natural language processing (NLP) annotation interfaces as encountered for SEMAFOR1 (Tsatsaronis et al., 2012), Brat2 (Stenetorp et al., 2012), or GATE3 (Cunningham et al., 2011). Such an u"
C16-2046,C16-1037,1,0.827521,"Missing"
C16-2046,E12-2021,0,0.0781929,"Missing"
C94-1060,C92-3138,0,0.0547294,"Missing"
C94-1060,J87-3006,0,\N,Missing
C94-1060,W94-0316,1,\N,Missing
C94-1060,C92-2114,0,\N,Missing
C94-1060,C92-2096,0,\N,Missing
C94-1060,E87-1001,0,\N,Missing
C94-1060,J86-3001,0,\N,Missing
D16-1111,D15-1041,1,0.72849,"ed in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introduction (or restoration) of punctuation in speech transcripts. Most often, classifier models are used that are trained on n-gram models (Gr"
D16-1111,candito-etal-2010-statistical,0,0.0279997,"Missing"
D16-1111,P15-1033,1,0.696496,"al., 1972), which makes it reflect the syntactic structure of a sentence. The different functions of punctuation are also reflected in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introdu"
D16-1111,N16-1024,1,0.925026,"ntence generation, trained on a variety of syntactic features from LFG f-structures, preceding punctuation bigrams and cue words. Our proposal is most similar to Tilk and Alum¨ae (2015), but our task is more complex since we generate the full range of punctuation marks. Furthermore, we do not use any acoustic features. Compared to Guo et al. (2010), we do not use any syntactic features either since our input is just raw text material. 3 Model Our model is inspired by a number of recent works on neural architectures for structure prediction: Dyer et al. (2015)’s transition-based parsing model, Dyer et al. (2016)’s generative language model and phrase-structure parser, Ballesteros et al. (2015)’s character-based word representation for parsing, and Ling et al. (2015b)’s part-of-speech tagging . 3.1 Transition SHIFT GENERATE(“,”) SHIFT SHIFT SHIFT GENERATE(“.”) Output [] [No] [No ,] [No , it] [No , it was ] [No , it was not] [No, it was not .] Figure 1: Transition sequence for the input sequence No it was not – with the output No, it was not. put and input buffers, is encoded in terms of a vector st ; see Section 3.3 for different alternatives of state representation. As Dyer et al. (2015), we use st t"
D16-1111,N15-1142,0,0.298788,"ion are also reflected in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introduction (or restoration) of punctuation in speech transcripts. Most often, classifier models are used that are"
D16-1111,D15-1176,0,0.0277625,"Missing"
D16-1111,W14-1701,0,0.0373703,"Missing"
D16-1111,W04-0308,0,0.0236883,"s the actions (either SHIFT or GENERATE(p)).1 st encodes information about previous actions (since it may include the history with the actions taken and the generated punctuation symbols are introduced in the output buffer, see Section 3.3), thus the probability of a sequence of actions z given the input sequence is: Algorithm We define a transition-based algorithm that introduces punctuation marks into sentences that do not contain any punctuation. In the context of NLG, the input sentence would be the result of the surface realization task (Belz et al., 2011). As in transitionbased parsing (Nivre, 2004), we use two data structures: Nivre’s queue is in our case the input buffer and his stack is in our case the output buffer. The algorithm starts with an input buffer full of words and an empty output buffer. The two basic actions of the algorithm are SHIFT, which moves the first word from the input buffer to the output buffer, and GEN ERATE , which introduces a punctuation mark after the first word in the output buffer. Figure 1 shows an example of the application of the two actions. At each stage t of the application of the algorithm, the state, which is defined by the contents of the outInpu"
D16-1111,W08-1703,0,0.119176,"c features (Baron et al., 2002; Kol´aˇr and Lamel, 2012). Tilk and Alum¨ae (2015) use a lexical and acoustic (pause duration) feature-based LSTM model for the restoration of periods and commas in Estonian speech transcripts. The grammatical and syntactic functions of punctuation have been addressed in the context of written 1048 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1048–1053, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics language. Some of the proposals focus on the grammatical function (Doran, 1998; White and Rajkumar, 2008), while others bring the grammatical and syntactic functions together and design rule-based grammatical resources for parsing (Briscoe, 1994) and surface realization (White, 1995; Guo et al., 2010). Guo et al. (2010) is one of the few works that is based on a statistical model for the generation of punctuation in the context of Chinese sentence generation, trained on a variety of syntactic features from LFG f-structures, preceding punctuation bigrams and cue words. Our proposal is most similar to Tilk and Alum¨ae (2015), but our task is more complex since we generate the full range of punctuat"
D19-6301,W13-3520,0,0.0329649,"e, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of gra"
D19-6301,P11-2040,1,0.829216,"availability of evaluators: four Shallow Track in-domain datasets (Chinese-GSD, English-EWT, RussianSynTagRus, Spanish-AnCora), one Shallow Track dataset coming from parsed data (SpanishAnCoraHIT ) and one (in-domain) Deep Track dataset (English-EWT). As in SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), we assessed two quality criteria in the human evaluations, in separate evaluation experiments, Readability and Meaning Similarity, and used continuous sliders as rating tools, the evidence being that raters tend to prefer them 14 Thank you to Yevgeniy Puzikov for pointing this out. (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were first given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the grey text’), was as follows: The meaning of the grey text is adequately"
D19-6301,W11-2832,1,0.65823,"ncies.org/ Bernd Bohnet Google Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from whic"
D19-6301,K17-3005,0,0.0303018,"bed by syntactic structure or agreement (such as verbal finiteness or verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the tra"
D19-6301,D19-6302,0,0.0736613,"tion is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions app"
D19-6301,D19-6303,0,0.0332354,"ighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised sta"
D19-6301,W18-3606,0,0.194017,"andard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator’s mean and standard deviation. For both raw and standard scores, we compute the mean of sentence-level scores. Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.15 4 Overview of Submitted Systems ADAPT is a sequence to sequence model with dependency features attached to word embeddings. A BERT sentence classifier was used as a reranker to choose between different hypotheses. The implementation is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ord"
D19-6301,W18-3604,0,0.0728895,"nto a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. Baseline: In order to set a lower boundary for the automatic and human evaluations, a simple English baseline consisting of 7 lines of python code was implemented16 . It generates from a UD file with an in-order traversal of the tree read by pyconll and outputting the form of each node. 5 Evaluation results There were 14 submissions to the task, of which two were withdrawn; 9 teams participa"
D19-6301,D19-6310,0,0.021759,"tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. B"
D19-6301,D19-6304,0,0.339584,"Missing"
D19-6301,K18-2014,0,0.041076,"verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the training data, the alignments with the tokens of the Shallow Track struct"
D19-6301,D19-6311,0,0.0501754,"ter RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projec"
D19-6301,W04-2705,0,0.268698,"Missing"
D19-6301,W18-3601,1,0.502146,"Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from which functional words (in particul"
D19-6301,W15-4719,0,0.125325,"The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, whic"
D19-6301,J05-1004,0,0.0811396,"n the English-gum dataset);8 3. The lines corresponding to combined lexical units (e.g. Spanish “del” &lt;de+el&gt; lit. ’of.the’) and the contents of columns [9] and [10] were removed; 4. Information about the relative order of components of named entities, multiple coordinations and punctuation signs was added in the FEATS column (dependency relations compound, compound:prt, compound:svc, flat, flat:foreign, flat:name, fixed, conj, punct); For the Deep Track, the following steps were additionally carried out: 5. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the 8 Thank you to Guy Lapalme for spotting this. syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation. See also the inventory of relations in Table 2. 6. Functional prepositions"
D19-6301,P02-1040,0,0.108135,"a que los nuevos miembros del CNE deben tener experiencia para “dirigir procesos complejos”. In the original UD files, the reference sentences are by default detokenised. In order to carry out the evaluations of the tokenised outputs, we built a tokenised version of the reference sentences by concatenating the words of the second column of the UD structures (see Figure 1) separated by a whitespace. 3 Evaluation Methods 3.1 Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST13 is a related n-gram similarity metric 13 http://www.itl.nist.gov/iad/mig/ tests/mt/doc/ngram-study.pdf; http:// www.itl.nist.gov/iad/mig/tests/mt/2009/ weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn t"
D19-6301,N18-1202,0,0.019743,"however, permissible. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track"
D19-6301,D19-6312,0,0.0504468,"els use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to group the words to be contracted, and then generate the contracted word form of each group with a seq2seq model. The LORIA submission (Shimorina and Gardent, 2019) presents a modular approach to surface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. Th"
D19-6301,K18-2011,1,0.820368,"Missing"
D19-6301,D19-6309,0,0.0269112,"rface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. The OSU-FB pipeline for generation (Upasani et al., 2019) starts by generating inflected word forms in the tree using character seq2seq models. These inflected syntactic trees are then linearised as constituent trees by converting the relations to non-terminals. The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity)."
D19-6301,D19-6306,0,0.0857639,"weighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to g"
E17-2108,D10-1021,0,0.0379088,"tc.), the ratio of modal verbs with respect to the total number of verbs, and the percentage of verbs that appear in complex tenses referred to as “verb chains” (VCs). 3. Tree features measure the tree width, the tree depth and the ramification factor. Tree depth is defined as the maximum number of nodes between the root and a leaf node; the width is the maximum number of siblings at any of the levels of the tree; and the ramification factor is the mean numand stop word frequencies (Mosteller and Wallace, 1963; Aljumily, 2015; Gamon, 2004; Argamon et al., 2009), PoS tags (Koppel et al., 2002; Mukherjee and Liu, 2010), or patterns captured by context-free-grammar-derived linguistic patterns; see e.g. (Raghavan et al., 2010; Sarawgi et al., 2011; Gamon, 2004). When syntactic features are mentioned, often function words and punctuation marks are meant; see e.g. (Amuchi et al., 2012; Abbasi and Chen, 2005; Cheng et al., 2009). However, it is well-known from linguistics and philology that deeper syntactic features, such as sentence structure, the frequency of specific phrasal, and syntactic dependency patterns, and discourse structure are relevant characteristics of the writing style of an author (Crystal and"
E17-2108,D11-1120,0,0.25214,"ds and punctuation marks are meant; see e.g. (Amuchi et al., 2012; Abbasi and Chen, 2005; Cheng et al., 2009). However, it is well-known from linguistics and philology that deeper syntactic features, such as sentence structure, the frequency of specific phrasal, and syntactic dependency patterns, and discourse structure are relevant characteristics of the writing style of an author (Crystal and Davy, 1969; DiMarco and Hirst, 1993; Burstein et al., 2003). 3 Experimental Setup State-of-the-art techniques for author profiling / identification usually draw upon large quantities of features; e.g., Burger et al. (2011) use more than 15 million features and Argamon et al. (2009) and Mukherjee and Liu (2010) more than 1,000. This limits their application in practice. Our goal is to demonstrate that the use of syntactic dependency and discourse features allows us to minimize the total number of features to less than 200 and still achieve competitive performance with a standard classification technique. For this purpose, we use Support Vector Machines (SVMs) with a linear kernel in four different experiments. Let us introduce now these features and the data on which the trained models have been tested. 3.1 Feat"
E17-2108,P10-2008,0,0.0312086,"ear in complex tenses referred to as “verb chains” (VCs). 3. Tree features measure the tree width, the tree depth and the ramification factor. Tree depth is defined as the maximum number of nodes between the root and a leaf node; the width is the maximum number of siblings at any of the levels of the tree; and the ramification factor is the mean numand stop word frequencies (Mosteller and Wallace, 1963; Aljumily, 2015; Gamon, 2004; Argamon et al., 2009), PoS tags (Koppel et al., 2002; Mukherjee and Liu, 2010), or patterns captured by context-free-grammar-derived linguistic patterns; see e.g. (Raghavan et al., 2010; Sarawgi et al., 2011; Gamon, 2004). When syntactic features are mentioned, often function words and punctuation marks are meant; see e.g. (Amuchi et al., 2012; Abbasi and Chen, 2005; Cheng et al., 2009). However, it is well-known from linguistics and philology that deeper syntactic features, such as sentence structure, the frequency of specific phrasal, and syntactic dependency patterns, and discourse structure are relevant characteristics of the writing style of an author (Crystal and Davy, 1969; DiMarco and Hirst, 1993; Burstein et al., 2003). 3 Experimental Setup State-of-the-art techniqu"
E17-2108,W11-0310,0,0.499127,"word frequency, word n-grams and character n-grams. Another example of this type of work is (Gamon, 2004), where the author classifies the writings of the Bront¨e sisters using as features the sentence length, number of nominal/adjectival/adverbial phrases, function word frequencies, part-of-speech (PoS) trigrams, constituency patterns, semantic information and ngram frequencies. In the field of author profiling, several works addressed specifically gender identification. Schler et al. (2006), Koppel et al. (2002) extract function words, PoS and the 1000 words that have more information gain. Sarawgi et al. (2011) use long-distance syntactic patterns based on probabilistic context-free grammars, token-level language models and characterlevel language models. In what follows, we focus on the identification of the author profiling trait ‘gender’ and on author identification as such. For both, feature engineering is crucial and for both the tendency is to use word/character n-grams and/or function Introduction Author profiling and author identification are two tasks in the context of the automatic derivation of author-related information from textual material. In the case of author profiling, demographic"
E17-2108,J93-3002,0,0.202627,"s captured by context-free-grammar-derived linguistic patterns; see e.g. (Raghavan et al., 2010; Sarawgi et al., 2011; Gamon, 2004). When syntactic features are mentioned, often function words and punctuation marks are meant; see e.g. (Amuchi et al., 2012; Abbasi and Chen, 2005; Cheng et al., 2009). However, it is well-known from linguistics and philology that deeper syntactic features, such as sentence structure, the frequency of specific phrasal, and syntactic dependency patterns, and discourse structure are relevant characteristics of the writing style of an author (Crystal and Davy, 1969; DiMarco and Hirst, 1993; Burstein et al., 2003). 3 Experimental Setup State-of-the-art techniques for author profiling / identification usually draw upon large quantities of features; e.g., Burger et al. (2011) use more than 15 million features and Argamon et al. (2009) and Mukherjee and Liu (2010) more than 1,000. This limits their application in practice. Our goal is to demonstrate that the use of syntactic dependency and discourse features allows us to minimize the total number of features to less than 200 and still achieve competitive performance with a standard classification technique. For this purpose, we use"
E17-2108,C04-1088,0,0.264648,"rary corpus while keeping the feature set small: the used feature set is composed of only 188 features and still outperforms the winner of the PAN 2014 shared task on author verification in the literary genre. 1 2 Related Work Author identification in the context of the literary genre attracted attention beyond the NLP research circles, e.g., due to the work by Aljumily (2015), who addressed the allegations that Shakespeare did not write some of his best plays using clustering techniques with function word frequency, word n-grams and character n-grams. Another example of this type of work is (Gamon, 2004), where the author classifies the writings of the Bront¨e sisters using as features the sentence length, number of nominal/adjectival/adverbial phrases, function word frequencies, part-of-speech (PoS) trigrams, constituency patterns, semantic information and ngram frequencies. In the field of author profiling, several works addressed specifically gender identification. Schler et al. (2006), Koppel et al. (2002) extract function words, PoS and the 1000 words that have more information gain. Sarawgi et al. (2011) use long-distance syntactic patterns based on probabilistic context-free grammars,"
E17-2108,W08-2121,0,0.0832359,"Missing"
E17-2108,N15-3001,0,0.0289567,"Ms) with a linear kernel in four different experiments. Let us introduce now these features and the data on which the trained models have been tested. 3.1 Feature Set We extracted 188 surface-oriented, syntactic dependency, and discourse structure features for our experiments. The surface-oriented features are few since syntactic and discourse structure features are assumed to reflect better than surfaceoriented features the unconscious stylistic choices of the authors. For feature extraction, Python and its natural language toolkit, a dependency parser (Bohnet, 2010), and a discourse parser (Surdeanu et al., 2015) are used. The feature set is composed of six subgroups of features: Character-based features are composed of the ratios between upper case characters, peri1 We use the Penn Treebank tagset http: //www.ling.upenn.edu/courses/Fall_2003/ ling001/penn_treebank_pos.html 682 Used Features Complete Set Char (C) Word (W) Sent (S) Dict (Dt) Discourse (Dc) Syntactic (Sy) C+W+S+Dt+Dc C+W+S+Dt+Sy Sy+Dc C+W+S+Dt MajClassBaseline 2GramBaseline 3GramBaseline 4GramBaseline 5GramBaseline ber of children per level. In other words, the tree features characterize the complexity of the dependency structure of the"
I13-1178,W05-0307,0,0.0554544,"Missing"
I13-1178,W02-1001,0,0.166923,"Missing"
I13-1178,P04-1015,0,0.0242056,"the root node n is contained in the stack Σ: s = ([0], [], Vc , Z, E, δ, x). Figure 2 shows the possible transitions. As features of the transition-based system, we use a rich feature set based on the dependency structure drawn from (Zhang and Nivre, 2011) (since we use as input a dependency structure these features are available). In addition, we use the path from the top stack element to the word of the last open bracket (as sequence of pos tags). For the training of the transition-based system, we use the perception algorithm with averaging, a beamsearch with 10 elements and early update (Collins and Roark, 2004). The oracle for training of the system follows the bottom-up parsing strategy. As soon as the communicative part is completed, we remove (reduce) the nodes that belong to it from the stack. Figure 3 shows a sequence of transitions that the analyser performs to create the Tc of the example sentence in Figure 1. 4 Experiments Following the criteria in Section 2, four annotators in teams of two manually annotated a fragment of 435 sentences of the PTB with the thematicity structure, in a series of blocks of about 40–50 sentences. To ensure high mutual agreement, the annotation procedure went as"
I13-1178,W04-2407,0,0.0139458,"cative tree Tc of a sentence x = w1 ...wn is a quintuple Tc = (V, E, L, δ, 00 ), such that V = Vt ∪ Vc is a set of nodes, with Vt = 0, ..., n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of states and transitio"
I13-1178,W04-0308,0,0.02419,", n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of states and transitions. As set S of states, we use the tuple s = (Σ, B, Vc , Z, E, δ, o), where the stack Σ and the input buffer B are disjoint sublist"
I13-1178,J05-1004,0,0.0166827,"tion, TFA (Sgall, 1967) in the Prague School and Communicative Structure, CommStr (Mel’ˇcuk, 2001) in the Meaning-Text Theory) determines the “communicative” segmentation of the meaning of an utterance. This makes it central to the semantics–syntax–intonation interface (Lambrecht, 1994; Hajiˇcov´a et al., 1998; Steedman, 2000; Mel’ˇcuk, 2001; Erteschik-Shir, 2007) and therefore also to NLP. However, despite its prominence, IS has been largely ignored so far in the context of the reference treebanks for data-driven NLP: Penn Treebank (Marcus et al., 1993) and its semantic counterpart PropBank (Palmer et al., 2005) for English, Tiger (Thielen et al., 1999) for German, Ancora (Taul´e et al., 2008) for Spanish, etc. To the best of our knowledge, only the Prague Leo Wanner ICREA and DTIC Pompeu Fabra University Barcelona, Spain leo.wanner@upf.edu Dependency Treebank (PDT) (Hajiˇc et al., 2006) is annotated with IS in terms of TFA. This is not to say that no proposals have been made for the annotation of IS in general; see, e.g., (Calhoun et al., 2005) for English, (Dipper et al., 2004) for German, (Paggio, 2006) for Danish, etc. However, in the light of the above mentioned interface, it is crucial to have"
I13-1178,paggio-2006-annotating,0,0.060546,"Missing"
I13-1178,H05-1002,0,0.0480223,"Missing"
I13-1178,taule-etal-2008-ancora,0,0.0364344,"Missing"
I13-1178,W03-3023,0,0.0455217,"given sentence. The communicative tree Tc of a sentence x = w1 ...wn is a quintuple Tc = (V, E, L, δ, 00 ), such that V = Vt ∪ Vc is a set of nodes, with Vt = 0, ..., n as a set of terminal nodes and Vc = o0 , 10 , ..., m0 as a set of non-terminal communicative (label) nodes; E ⊆ V × V is a set of edges; L is the set of communicative labels (in the case of thematicity: SP, T, R and P); δ : E → L is a labeling function for nodes; 00 is the root node. That is, we interprete the Tc as a kind of constituency tree. For the implementation of the parser, we use the idea of transition-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), which uses a classifier to predict the shift/reduce actions. We draw upon the transition set of the arc-eager parser Nivre (2004), but with a slightly different semantics in that we define a transition system for the derivation of the Tc as a quadruple C = (S, Y, c0 , Sy ), where S is a set of parsing states; Y is a set of transitions, each of which is a (partial) function t: S → S; s0 is an initialization function that maps a sentence x to a configuration s ∈ S; and Sy ⊆ S is a set of terminal states. A transition sequence for a sentence x in C is a sequence of pairs of"
I13-1178,J93-2004,0,0.0421149,"Missing"
I13-1178,P11-2033,0,0.026146,"communicative (label) nodes, Z is the stack of communicative nodes, E is the set of edges, δ is a labeling function for communicative label nodes n ∈ Vc , and o is a counter for the number of pairs of delimitation brackets. The initial state for a sentence x is s0 = ([0], [1, ..., n], {00 }, [00 ], {}, δ, 0). Terminal configurations have an empty buffer and only the root node n is contained in the stack Σ: s = ([0], [], Vc , Z, E, δ, x). Figure 2 shows the possible transitions. As features of the transition-based system, we use a rich feature set based on the dependency structure drawn from (Zhang and Nivre, 2011) (since we use as input a dependency structure these features are available). In addition, we use the path from the top stack element to the word of the last open bracket (as sequence of pos tags). For the training of the transition-based system, we use the perception algorithm with averaging, a beamsearch with 10 elements and early update (Collins and Roark, 2004). The oracle for training of the system follows the bottom-up parsing strategy. As soon as the communicative part is completed, we remove (reduce) the nodes that belong to it from the stack. Figure 3 shows a sequence of transitions t"
I13-1178,W09-1201,0,\N,Missing
L16-1204,D11-1120,0,0.0315378,"eriments. Section 6, finally, draws some conclusions and outlines the lines of our future work in the area of author profiling. 2. Related Work In the vast majority of the existing works, author profiling and author gender identification are defined as supervised machine learning problems. Different kinds of data as input have been used; see, among others, (Argamon et al., 2009; Argamon and Shimoni, 2003; Schler et al., 2006; Zhang and Zhang, 2010), where gender, age, native language and personality detection are performed using blog posts and the international corpus of English learners. In (Burger et al., 2011), gender identification is performed for Tweets, and in (Estival et al., 2007) and (Cheng et al., 2009) author profiling is performed on email data. Chat messages have also been worked on. For instance, Kucukyilmaz et al. (2006) and Kose et al. (2008) attempt to extract the gender of the users of chat blogs. Both predict the gender of the members of conversations (in Turkish) in different chat services. Some of the specific features that they use for this purpose are the occurrence of the “smiley” symbol, abbreviations, slang words, and different function words. Gupta et al. (2012) apply this"
L16-1204,P05-1049,0,0.0443802,"uthor profiling and similar applications such as plagiarism detection and author obfuscation is held; cf. (Stamatatos et al., 2015a; Stamatatos et al., 2015b; Stamatatos et al., 2015c; Hagen et al., 2015) for more information on the shared tasks. However, hardly any work has been done on approaching author profiling as a semi-supervised machine learning problem—although semi-supervised learning has been widely used in a number of areas of Computational Linguistics; see e.g, (Zhu, 2005) for a literature survey, (Wong et al., 2008) for the use of semi-supervised learning in text summarization, (Niu et al., 2005) for its application to word sense disambiguation, (Koo et al., 2008) for the use in parsing, (Zhang and Ostendorf, 2012) for classification of movie reviews and newsgroup articles, etc. 3. Enriched KNN algorithm Our semi-supervised learning algorithm for gender identification is a modified version of the classic k nearest neighbors (kNN) classifier. Given a test instance, this algorithm, identifies the k instances that are closest (in accordance with a vector distance metric such as cosine or Euclidean distance) to the test instance. The test instance is labeled with the most common label amo"
L16-1204,soler-company-wanner-2014-use,1,0.834109,"and/or pairs of frequently co-occurring words, part of speech (POS) n-grams, punctuation marks, etc. Syntactic features are less often used; cf., e.g., (Cheng et al., 2009). However, from linguistics and philology we know that syntactic idiosyncrasies are also distinctive features of the writing style of individuals and groups of individuals who share demographic, social, cultural or gender characteristics; see, e.g., (Crystal and Davy, 1969; Biber, 1989; Strunk and White, 1999; Tufte, 2006). Therefore, syntactic features play in our setup an important role. Similar feature sets were used in (Soler and Wanner, 2014; Soler and Wanner, 2015), where the authors performed gender identification for blog posts as well as gender and language identification in a multilingual scenario. The features have been extracted using the programming language Python and its Natural Language Toolkit1 . For the extraction of the syntactic dependency features (see below), 1 http://nltk.org the dependency parser from the MATE-tools has been used (Bohnet, 2010). Each post in the dataset is represented as a multidimensional vector in which each dimension captures the value of a specific feature. In total, five types of features"
L16-1204,P14-2070,0,0.0507522,"Missing"
L16-1204,C08-1124,0,0.0142457,"or the progress of the state-of-the-art in the field is the yearly shared task on author profiling and similar applications such as plagiarism detection and author obfuscation is held; cf. (Stamatatos et al., 2015a; Stamatatos et al., 2015b; Stamatatos et al., 2015c; Hagen et al., 2015) for more information on the shared tasks. However, hardly any work has been done on approaching author profiling as a semi-supervised machine learning problem—although semi-supervised learning has been widely used in a number of areas of Computational Linguistics; see e.g, (Zhu, 2005) for a literature survey, (Wong et al., 2008) for the use of semi-supervised learning in text summarization, (Niu et al., 2005) for its application to word sense disambiguation, (Koo et al., 2008) for the use in parsing, (Zhang and Ostendorf, 2012) for classification of movie reviews and newsgroup articles, etc. 3. Enriched KNN algorithm Our semi-supervised learning algorithm for gender identification is a modified version of the classic k nearest neighbors (kNN) classifier. Given a test instance, this algorithm, identifies the k instances that are closest (in accordance with a vector distance metric such as cosine or Euclidean distance)"
L16-1325,P95-1017,0,0.281497,"89; Hornstein, 1999), as illustrated in the examples (2) and (3), has been largely neglected. (2) [John]i convinced [Mary]j to move to Barcelona. Now, [both]i+j enjoy living close to the sea. (3) [John]i , who adores [Mary]j from the first day, thinks that [they]i+j make a great couple. Some computational proposals that deal with coreference mention the phenomenon, but do not treat it (and do not justify in a satisfactory way the decision to ignore it either); see, e.g., (Mart´ınez-Barco and Palomar, 2011; Baldwin, 1997). Others treat it, but do not present the achieved performance in detail (Aone and Bennett, 1995; Kennedy and Boguraev, 1996). But, as already pointed out, most of the existing proposals (among them, e.g., (Ge et al., 1998; Refoufi, 2007)) do not even mention the phenomenon. The exclusion of MAC from coreference resolution is also reflected in the annotation tools that are frequently used: either they are not implemented to directly capture this phenomenon, as, e.g., GATE (Cunningham et al., 2011), or Clinka (Orasan, 2000), or they are partially implemented to handle it, but presumably require an advanced user configuration for this purpose, as, e.g., MMAX (M¨uller and Strube, 2006), or"
L16-1325,Q13-1034,0,0.0256591,"Missing"
L16-1325,bouayad-agha-etal-2014-exercise,1,0.851613,"element. Thus, for instance, if there are two NPs that follow the pattern detailed in (12) and (13), such as first alkaline battery and second alkaline battery, the system groups both NPs into a single plural NP3 , getting the phrase first and second alkaline batteries, which has the following structure: (14) NP3 : Adj1 + Adj2 + (Adja ) + Headipl 8 The parentheses mark optional elements. The parser has been optimized for parsing patent material (Pekar et al., 2014). 10 For single antecedent coreference resolution in patents that uses an adapted version of Stanford Coreference Resolution, see (Bouayad-Agha et al., 2014). 9 (i) Non-contrastive pre-nominal modifiers are optional in the referential phrase; if such modifiers are present, they must match the modifiers in the antecedent NPs (e.g., the top battery charger [. . . ] the bottom battery charger can corefer either with the chargers, the battery chargers, or with the top and bottom (battery) chargers). (ii) Contrastive adjectives are also optional in the referential phrase; if they are present, all of them must equally appear in the antecedent NPs. Thus, there is a multiple antecedent coreference between the first battery [. . . ] the second battery [. ."
L16-1325,P08-2012,0,0.0286537,"Missing"
L16-1325,W98-1119,0,0.126265,"o Barcelona. Now, [both]i+j enjoy living close to the sea. (3) [John]i , who adores [Mary]j from the first day, thinks that [they]i+j make a great couple. Some computational proposals that deal with coreference mention the phenomenon, but do not treat it (and do not justify in a satisfactory way the decision to ignore it either); see, e.g., (Mart´ınez-Barco and Palomar, 2011; Baldwin, 1997). Others treat it, but do not present the achieved performance in detail (Aone and Bennett, 1995; Kennedy and Boguraev, 1996). But, as already pointed out, most of the existing proposals (among them, e.g., (Ge et al., 1998; Refoufi, 2007)) do not even mention the phenomenon. The exclusion of MAC from coreference resolution is also reflected in the annotation tools that are frequently used: either they are not implemented to directly capture this phenomenon, as, e.g., GATE (Cunningham et al., 2011), or Clinka (Orasan, 2000), or they are partially implemented to handle it, but presumably require an advanced user configuration for this purpose, as, e.g., MMAX (M¨uller and Strube, 2006), or their specification does not mention the phenomenon at all, as, e.g., in the case of AncoraPipe (Bertr´an et al., 2008). The m"
L16-1325,C96-1021,0,0.392768,"illustrated in the examples (2) and (3), has been largely neglected. (2) [John]i convinced [Mary]j to move to Barcelona. Now, [both]i+j enjoy living close to the sea. (3) [John]i , who adores [Mary]j from the first day, thinks that [they]i+j make a great couple. Some computational proposals that deal with coreference mention the phenomenon, but do not treat it (and do not justify in a satisfactory way the decision to ignore it either); see, e.g., (Mart´ınez-Barco and Palomar, 2011; Baldwin, 1997). Others treat it, but do not present the achieved performance in detail (Aone and Bennett, 1995; Kennedy and Boguraev, 1996). But, as already pointed out, most of the existing proposals (among them, e.g., (Ge et al., 1998; Refoufi, 2007)) do not even mention the phenomenon. The exclusion of MAC from coreference resolution is also reflected in the annotation tools that are frequently used: either they are not implemented to directly capture this phenomenon, as, e.g., GATE (Cunningham et al., 2011), or Clinka (Orasan, 2000), or they are partially implemented to handle it, but presumably require an advanced user configuration for this purpose, as, e.g., MMAX (M¨uller and Strube, 2006), or their specification does not"
L16-1325,H05-1004,0,0.057778,"Missing"
L16-1325,orasan-2000-clinka,0,0.0594231,"n to ignore it either); see, e.g., (Mart´ınez-Barco and Palomar, 2011; Baldwin, 1997). Others treat it, but do not present the achieved performance in detail (Aone and Bennett, 1995; Kennedy and Boguraev, 1996). But, as already pointed out, most of the existing proposals (among them, e.g., (Ge et al., 1998; Refoufi, 2007)) do not even mention the phenomenon. The exclusion of MAC from coreference resolution is also reflected in the annotation tools that are frequently used: either they are not implemented to directly capture this phenomenon, as, e.g., GATE (Cunningham et al., 2011), or Clinka (Orasan, 2000), or they are partially implemented to handle it, but presumably require an advanced user configuration for this purpose, as, e.g., MMAX (M¨uller and Strube, 2006), or their specification does not mention the phenomenon at all, as, e.g., in the case of AncoraPipe (Bertr´an et al., 2008). The most obvious explanation for this gap is the implicit assumption that MAC is scarce in general discourse. And indeed, a quick examination of two small newspaper articles (in total 1354 tokens)1 seems to buttress this assumption. Even if plural coreferential elements appear in these articles with a certain"
L16-1325,W14-6105,0,0.134055,"t appear in the same sentence and share number, head and pre-nominal modifiers (except those that are ordinal or contrastive) are grouped as a single element. Thus, for instance, if there are two NPs that follow the pattern detailed in (12) and (13), such as first alkaline battery and second alkaline battery, the system groups both NPs into a single plural NP3 , getting the phrase first and second alkaline batteries, which has the following structure: (14) NP3 : Adj1 + Adj2 + (Adja ) + Headipl 8 The parentheses mark optional elements. The parser has been optimized for parsing patent material (Pekar et al., 2014). 10 For single antecedent coreference resolution in patents that uses an adapted version of Stanford Coreference Resolution, see (Bouayad-Agha et al., 2014). 9 (i) Non-contrastive pre-nominal modifiers are optional in the referential phrase; if such modifiers are present, they must match the modifiers in the antecedent NPs (e.g., the top battery charger [. . . ] the bottom battery charger can corefer either with the chargers, the battery chargers, or with the top and bottom (battery) chargers). (ii) Contrastive adjectives are also optional in the referential phrase; if they are present, all o"
L16-1325,popescu-belis-etal-2004-online,0,0.0804178,"Missing"
L16-1325,D10-1048,0,0.0498047,"Missing"
L16-1325,P10-1144,0,0.0140394,") nominal MAC, (ii) MAC with personal / relative pronouns, and MAC with reflexive / reciprocal pronouns. The evaluation shows that our strategy performs well in terms of precision and recall. Keywords: Multiple antecedent coreference, split antecedent coreference, coreference evaluation, patent processing 1. Introduction Coreference resolution has been a popular research topic for a considerable time now both in theoretical and in computational studies; see, among others, (Lasnik, 1989; L.Eguren and Fern´andez Soriano, 2007; Kayne, 2005) for the first and (Mitkov, 1999; Recasens et al., 2010; Recasens and Hovy, 2010) for the latter. It is an obligatory task for any language understanding application that goes beyond surface-oriented parsing. However, the overwhelming majority of the state-of-the-art coreference resolution works focused so far exclusively on one kind of coreference, namely the single antecedent coreference illustrated in (1): (1) [John]i met Mary in New York. At that time, [he]i studied Computer Science. Multiple antecedent coreference (henceforth MAC), or split antecedent coreference (Lasnik, 1989; Hornstein, 1999), as illustrated in the examples (2) and (3), has been largely neglected. ("
L16-1325,recasens-etal-2010-typology,0,0.0418753,"Missing"
L16-1325,M95-1005,0,0.269005,"Missing"
L16-1325,W97-1306,0,\N,Missing
L16-1367,ramos-etal-2010-towards,1,0.827952,"Missing"
L16-1367,atserias-etal-2006-freeling,0,0.0493353,"Missing"
L16-1367,P10-2020,0,0.412155,"d a language-independent approach to automatic acquisition and fine-grained semantic classification of collocation resources with respect to Lexical Functions. Such resources are crucial for second language learning as well as for computational applications related to language production, e.g., natural language generation (Smadja and McKeown, 1990; Wanner, 1992) or machine translation (Mel’ˇcuk and Wanner, 2001). Although there has been a large body of work on automatic retrieval of collocations (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010), and also some works on the semantic classification of collocations (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., in print), to the best of our knowledge, this is the first proposal to retrieve and classify collocations simultaneously in an unsupervised manner. In our future work, we will aim to improve the precision of the classification procedure with respect to the “difficult” LFs such as AntiMagn,CausPredMinus, LiquFunc0 , etc. 7. Acknowledgements The present work has been partially funded by the Spanish Ministry of Economy and Competitiveness ("
L16-1367,W14-3501,1,0.915639,"Missing"
L16-1367,O09-5002,0,0.0312063,"ped together share similar semantic features. Still, the use of semantic categories that reveal a sufficient level of detail for the presentation of collocations in dictionaries is meaningful. In computational lexicography, categories of different granularity have been used for automatic classification of collocations from given lists; cf., e.g., Wanner et al. (in print), who use 16 categories for the classification of verb+noun collocations and 5 categories for the classification of adj+noun collocations; Moreno et al. (2013), who work with 5 broader categories for verb+noun collocations, or Chung-Chi et al. (2009), who also use very coarsegrained semantic categories of the type ‘goodness’, ‘heaviness’, ‘measures’, etc. But all of these categories have the disadvantage to be ad hoc. Therefore, we follow a different approach. As already Wanner et al. (2006), Gelbukh and Kolesnikova. (2012) and also Moreno et al. (2013) in their second run of experiments, we use the semantic typology of Lexical Functions (LFs) to classify collocations– assuming that once we obtained LF instances, they can be grouped by lexicographers into more generic coherent semantic categories. However, in contrast to these works, we a"
L16-1367,P89-1010,0,0.283581,"mentar [el] valor ‘to increase [a] value’ were obtained. 6. Conclusions We presented a language-independent approach to automatic acquisition and fine-grained semantic classification of collocation resources with respect to Lexical Functions. Such resources are crucial for second language learning as well as for computational applications related to language production, e.g., natural language generation (Smadja and McKeown, 1990; Wanner, 1992) or machine translation (Mel’ˇcuk and Wanner, 2001). Although there has been a large body of work on automatic retrieval of collocations (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010), and also some works on the semantic classification of collocations (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., in print), to the best of our knowledge, this is the first proposal to retrieve and classify collocations simultaneously in an unsupervised manner. In our future work, we will aim to improve the precision of the classification procedure with respect to the “difficult” LFs such as AntiMagn,CausPredMinus, LiquFunc0 , etc. 7. Acknowledgements The present work has been"
L16-1367,N15-1184,0,0.0653661,"in the training data. Among these tasks are: Machine Translation (Mikolov et al., 2013b), where a transition matrix is learned from word pairs of two different languages and then applied to unseen cases in order to provide word-level translation; Knowledge Base (KB) Embedding (transformation of structured information in KBs such as Freebase or DBpedia into continuous vectors in a shared space) (Bordes et al., 2011); Knowledge Base Completion (introduction of novel relationships in existing KBs) (Lin et al., 2015); Word Similarity, Syntactic Relations, Synonym Selection and Sentiment Analysis (Faruqui et al., 2015); Word Similarity and Relatedness (Iacobacci et al., 2015); and taxonomy learning (Fu et al., 2014; Espinosa-Anke et al., 2016). From these examples, we can deduce that word embeddings provide an efficient semantic representation of words and concepts, and, therefore, may also be leveraged for the acquisition of collocational resources. Hence, we examine this hypothesis by putting forward an unsupervised algorithm for collocation acquisition which strongly relies in relational properties of word embeddings for discovering semantic relations. 3.1. Exploiting the Analogy Property In what follows"
L16-1367,P14-1113,0,0.133854,"tion matrix is learned from word pairs of two different languages and then applied to unseen cases in order to provide word-level translation; Knowledge Base (KB) Embedding (transformation of structured information in KBs such as Freebase or DBpedia into continuous vectors in a shared space) (Bordes et al., 2011); Knowledge Base Completion (introduction of novel relationships in existing KBs) (Lin et al., 2015); Word Similarity, Syntactic Relations, Synonym Selection and Sentiment Analysis (Faruqui et al., 2015); Word Similarity and Relatedness (Iacobacci et al., 2015); and taxonomy learning (Fu et al., 2014; Espinosa-Anke et al., 2016). From these examples, we can deduce that word embeddings provide an efficient semantic representation of words and concepts, and, therefore, may also be leveraged for the acquisition of collocational resources. Hence, we examine this hypothesis by putting forward an unsupervised algorithm for collocation acquisition which strongly relies in relational properties of word embeddings for discovering semantic relations. 3.1. Exploiting the Analogy Property In what follows, we describe our unsupervised approach to the acquisition of (base, collocate) pairs for each ind"
L16-1367,P15-1010,0,0.0320198,"nslation (Mikolov et al., 2013b), where a transition matrix is learned from word pairs of two different languages and then applied to unseen cases in order to provide word-level translation; Knowledge Base (KB) Embedding (transformation of structured information in KBs such as Freebase or DBpedia into continuous vectors in a shared space) (Bordes et al., 2011); Knowledge Base Completion (introduction of novel relationships in existing KBs) (Lin et al., 2015); Word Similarity, Syntactic Relations, Synonym Selection and Sentiment Analysis (Faruqui et al., 2015); Word Similarity and Relatedness (Iacobacci et al., 2015); and taxonomy learning (Fu et al., 2014; Espinosa-Anke et al., 2016). From these examples, we can deduce that word embeddings provide an efficient semantic representation of words and concepts, and, therefore, may also be leveraged for the acquisition of collocational resources. Hence, we examine this hypothesis by putting forward an unsupervised algorithm for collocation acquisition which strongly relies in relational properties of word embeddings for discovering semantic relations. 3.1. Exploiting the Analogy Property In what follows, we describe our unsupervised approach to the acquisition"
L16-1367,N13-1090,0,0.0546438,"here in further details; see (Mel’ˇcuk, 1996) for a detailed description. Oper1 (decision) = {make} Oper1 (idea) = {have} Real1 (‘realize/ do what is expected with B’)3 Real1 (temptation) = {succumb [to ∼], yield [to ∼]} Real1 (exam) = {pass} Real1 (piano) = {play} IncepOper1 (‘begin to do, begin to have B’) IncepOper1 (fireN ) = {open} IncepOper1 (debt) = {run up, incur} CausOper1 (‘do something so that B is performed/done’) CausOper1 (opinion) = lead [to ∼] 3. Methodology for the Acquisition of Collocation Resources Taking as inspiration the Neural Probabilistic Model (Bengio et al., 2006), Mikolov et al. (2013c) proposed an approach for computing continuous vector representations of words from large corpora by predicting words given their context, while at the same time predicting context, given an input word. The vectors computed following the approaches described in (Mikolov et al., 2013a; Mikolov et al., 2013c) have been extensively used for semantically intensive tasks, mainly because of the properties that word embeddings have to capture relationships among words which are not explicitly encoded in the training data. Among these tasks are: Machine Translation (Mikolov et al., 2013b), where a t"
L16-1367,P90-1032,0,0.56242,"un] edificio ‘to erect a building’ and encender [un] fuego ‘to light [a] fire’ were found for LiquFunc0 ; and for CausPredMinus cases such as incrementar [un] salario ‘to increase wages’ or aumentar [el] valor ‘to increase [a] value’ were obtained. 6. Conclusions We presented a language-independent approach to automatic acquisition and fine-grained semantic classification of collocation resources with respect to Lexical Functions. Such resources are crucial for second language learning as well as for computational applications related to language production, e.g., natural language generation (Smadja and McKeown, 1990; Wanner, 1992) or machine translation (Mel’ˇcuk and Wanner, 2001). Although there has been a large body of work on automatic retrieval of collocations (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010), and also some works on the semantic classification of collocations (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., in print), to the best of our knowledge, this is the first proposal to retrieve and classify collocations simultaneously in an unsupervised manner. In our future work, we will ai"
L16-1367,J93-1007,0,0.841204,"crease [a] value’ were obtained. 6. Conclusions We presented a language-independent approach to automatic acquisition and fine-grained semantic classification of collocation resources with respect to Lexical Functions. Such resources are crucial for second language learning as well as for computational applications related to language production, e.g., natural language generation (Smadja and McKeown, 1990; Wanner, 1992) or machine translation (Mel’ˇcuk and Wanner, 2001). Although there has been a large body of work on automatic retrieval of collocations (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010), and also some works on the semantic classification of collocations (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., in print), to the best of our knowledge, this is the first proposal to retrieve and classify collocations simultaneously in an unsupervised manner. In our future work, we will aim to improve the precision of the classification procedure with respect to the “difficult” LFs such as AntiMagn,CausPredMinus, LiquFunc0 , etc. 7. Acknowledgements The present work has been partially fund"
L16-1367,N13-1120,0,0.0128248,"Table 4. Experiments In what follows, we outline first the setup of our experiments and present then their outcome. 4.1. Meaning Magn Table 1: Seed examples for each LF Algorithm 1 outlines the two stages. The first stage (lines 4 − 9) consists, first, in retrieving a candidate set by means of the function relSim, which computes the similarity of the relation between bφι and ςιφ to the relation between bι and a hidden word x. relSim can be thus interpreted as satisfying the well-known analogy “a is to b what c is to x”, exploiting the vector space representation4 of a, b, and c to discover x (Zhila et al., 2013). Specifically, we compute vb −va +vc in order to obtain the set of vectors closest to vx by cosine distance. To obtain the best collocate candidate set, we retrieve the ten most similar vectors to x, where x is the unknown collocate we aim to find. This is done over a model trained with word2vec5 on a 2014 dump of the Spanish Wikipedia, preprocessed and lemmatized with Freeling (Atserias et al., 2006). The second stage (lines 10−14) implements a filtering procedure by applying N P M IC , an association measure that is based on pointwise mutual information, but takes into account the asymmetry"
L16-1367,J90-1003,0,\N,Missing
L18-1400,ramos-etal-2010-towards,1,0.853783,"interpretation of the meaning of surmount depends on obstacle. In order to be able to offer such advanced collocation checkers, sufficiently large collocation resources, and, in particular, learner corpora annotated with collocation error information, which could be used for training machine learning techniques, are needed. Unfortunately, in second language learning, corpora are usually too small. To remedy this bottleneck, artificial corpora have often been compiled in the context of automatic grammar error detection and correction; cf., e.g., Foster and Andersen (2009), Rozovskaya and Roth (2010), or Yuan and Felice (2013), among others. In our work, we adopt the same idea for automatic collocation error detection and correction. In what follows, we present an algorithm for the conversion of the Spanish GigaWord corpus into a collocation error corpus of American English learners of Spanish. As the blueprint of the error type occurrence and distribution, we use the Spanish learner corpus CEDEL2 (Lozano, 2009), which was annotated according to Alonso Ramos et al. (2010)’s three-dimensional fine-grained collocation error typology (see Section 2.). Section 3. presents the algorithm for th"
L18-1400,W14-3501,1,0.827378,"2. Algorithm for Creation of the Error Corpus The algorithm for the generation of the collocation error corpus passes through three main stages: (1) collocation extraction, (2) collocation classification, and (3) error generation and injection. Firstly, all the N–V, N–Adj and V– Adj dependencies that occur in the corpus where the errors are to be inserted, are retrieved and classified, according to their POS pattern, into three groups: N–V, N– Adj and V–Adj. A statistical check is performed to reject non-collocations: we choose the asymmetrical normalized Pointwise Mutual Information (PMI) by Carlini et al. (2014) and consider as collocations only those dependencies whose PMI is higher than 0. Collocations are stored with their prepositions, determiners and pronouns, along with relevant information that will be used at later stages, such as their position in the sentence, lemmas, POS-tags, morphological information, and their sentential context. Secondly, collocations are classified according to the types of errors that they can contain. For instance, N–Adj collo6 Recall that the incorrect choice of determiner and pronoun are not seen as collocation errors 4 In Spanish, when the preposition a ‘to’ is f"
L18-1400,N16-1077,0,0.0310034,"vely for substitution collocation errors), where the base is translated into 7 In the CEDEL2 corpus the frequency of GoBI errors was rather small, so we opted for disregarding this type of error 8 As in ‘Gender’ errors, the existence of the replacement words is checked in the RC. Resources • Google Translate. Google Translate is used as bidirectional translation engine, both to translate from Spanish to English, and from English to Spanish. Access to it is provided by the TextBlob Python library. • Morphological inflection tool. Finally, the algorithm uses the morphological inflection tool by Faruqui et al. (2016). This tool allows for the generation of morphologically inflected forms of a word according to given morphological attributes. In our case, we use it for the generation of lexical errors, to inflect the words that are automatically created by the algorithm as replacement for bases and collocates. 4. The Artificially Generated Corpus In order to check to what extent our artificial corpus simulates our learner corpus, we carried out an analysis of both of them. For this purpose, we took a sample of 50 sentences from each corpus and paid attention to three main aspects: (1) collocation errors, ("
L18-1400,W09-2112,0,0.0389182,"obstacle, obstacle keeps its meaning, while the interpretation of the meaning of surmount depends on obstacle. In order to be able to offer such advanced collocation checkers, sufficiently large collocation resources, and, in particular, learner corpora annotated with collocation error information, which could be used for training machine learning techniques, are needed. Unfortunately, in second language learning, corpora are usually too small. To remedy this bottleneck, artificial corpora have often been compiled in the context of automatic grammar error detection and correction; cf., e.g., Foster and Andersen (2009), Rozovskaya and Roth (2010), or Yuan and Felice (2013), among others. In our work, we adopt the same idea for automatic collocation error detection and correction. In what follows, we present an algorithm for the conversion of the Spanish GigaWord corpus into a collocation error corpus of American English learners of Spanish. As the blueprint of the error type occurrence and distribution, we use the Spanish learner corpus CEDEL2 (Lozano, 2009), which was annotated according to Alonso Ramos et al. (2010)’s three-dimensional fine-grained collocation error typology (see Section 2.). Section 3. p"
L18-1400,gonzalez-agirre-etal-2012-multilingual,0,0.0603791,"Missing"
L18-1400,W09-2107,0,0.0760449,"Missing"
L18-1400,P10-2021,0,0.0571139,"Missing"
L18-1400,W13-3607,0,0.0190327,"tion of the meaning of surmount depends on obstacle. In order to be able to offer such advanced collocation checkers, sufficiently large collocation resources, and, in particular, learner corpora annotated with collocation error information, which could be used for training machine learning techniques, are needed. Unfortunately, in second language learning, corpora are usually too small. To remedy this bottleneck, artificial corpora have often been compiled in the context of automatic grammar error detection and correction; cf., e.g., Foster and Andersen (2009), Rozovskaya and Roth (2010), or Yuan and Felice (2013), among others. In our work, we adopt the same idea for automatic collocation error detection and correction. In what follows, we present an algorithm for the conversion of the Spanish GigaWord corpus into a collocation error corpus of American English learners of Spanish. As the blueprint of the error type occurrence and distribution, we use the Spanish learner corpus CEDEL2 (Lozano, 2009), which was annotated according to Alonso Ramos et al. (2010)’s three-dimensional fine-grained collocation error typology (see Section 2.). Section 3. presents the algorithm for the creation of the artificia"
L18-1635,I13-1178,1,0.809053,"aticity corresponds to a wider range of intonation patterns, and is, therefore, a more adequate representation than binary approaches, especially for long syntactically complex sentences; see, e.g., (Dom´ınguez et al., 2016a). In this paper we present a methodology for the compilation of a corpus for research on the Information Structure–prosody interface from an empirical perspective. This methodology is based on the formal description of information (or communicative) structure by Mel’ˇcuk (2001), which has been already used for the annotation of hierarchical thematicity of written text in (Bohnet et al., 2013); and an automatic annotation of prosody based on a modular pipeline for extraction of acoustic parameters (Dom´ınguez et al., 2016c). An example application is introduced and demonstrated in the online platform Praat on the Web (Dom´ınguez et al., 2016b). Classification experiments on a corpus of read speech in English are carried out to validate our approach. The rest of the paper is structured as follows. The next section presents the motivation and background of this work. The methodology proposed for the compilation of a corpus to study the Information Structure correspondence is describe"
L18-1635,C16-1037,1,0.899668,"Missing"
L18-1635,C16-2046,1,0.891591,"Missing"
L18-1635,W11-2401,0,0.0334066,"to study the Information Structure correspondence is described in Section 3. A sample application of a small corpus of read speech in American English is introduced in Section 4. Then, the validation of our approach is presented in Section 5. Finally, conclusions are drawn in Section 6. 2. Motivation and Background The role of Information Structure (IS) in comprehension of read and spoken speech has been reported for a long time in linguistic and cognitive sciences (Clark and Haviland, 1977; Bock et al., 1983; Fowler and Housum, 1987; van Donselaar and Lentz, 1994). Recent studies in German (Meurers et al., 2011) and Catalan (Vanrell et al., 2013), 4030 for example, show that characteristic intonation patterns that make a distinction between theme and rheme spans contribute to a better understanding of the message. The relationship between Information Structure and intonation had been discussed even before the Tones and Breaks Index (ToBI) (Silverman et al., 1992) was agreed upon as a convention to represent intonation cues. (Beckman and Pierrehumbert, 1986) suggest that the characteristic bitonals for theme and rheme are rising (L*+H) and falling (H+L*), respectively. (Steedman, 2000) proposes a ques"
mille-wanner-2008-making,C88-2088,0,\N,Missing
mille-wanner-2008-making,W07-1414,0,\N,Missing
mille-wanner-2008-making,N03-1003,0,\N,Missing
mille-wanner-2008-making,W00-1436,1,\N,Missing
mille-wanner-2008-making,P06-1048,0,\N,Missing
mille-wanner-2008-making,P79-1016,0,\N,Missing
mille-wanner-2010-syntactic,megyesi-etal-2008-swedish,0,\N,Missing
mille-wanner-2010-syntactic,de-marneffe-etal-2006-generating,0,\N,Missing
mille-wanner-2010-syntactic,nivre-etal-2006-talbanken05,0,\N,Missing
mille-wanner-2010-syntactic,bohnet-wanner-2010-open,1,\N,Missing
mille-wanner-2010-syntactic,J93-2004,0,\N,Missing
mille-wanner-2010-syntactic,W09-1210,0,\N,Missing
mille-wanner-2010-syntactic,W03-1712,0,\N,Missing
mille-wanner-2010-syntactic,W00-1436,1,\N,Missing
mille-wanner-2010-syntactic,W07-2441,0,\N,Missing
mille-wanner-2010-syntactic,apresjan-etal-2006-syntactically,0,\N,Missing
N15-1042,P13-2009,0,0.00851502,"ic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and"
N15-1042,2004.tmi-1.14,0,0.0251845,"y avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filipp"
N15-1042,C14-1133,1,0.883628,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,W14-4416,1,0.917886,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,C00-1007,0,0.43582,"eep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a t"
N15-1042,W11-2832,0,0.444251,"SyntSs). While SSyntSs and linearized structures are isomorphic, the difference in the linguistic abstraction of the DSyntSs and SSyntSs leads to divergences that impede the isomorphy between the two and make the first mapping a challenge for statistical generation. Therefore, we focus in this section on 388 the presentation of the DSyntSs and SSyntSs and the mapping between them. 2.1 2.1.1 DSyntSs and SSyntSs Input DSyntSs DSyntSs are very similar to the PropBank (Babko-Malaya, 2005) structures and the structures as used for the deep track of the First Surface Realization Shared Task (SRST, (Belz et al., 2011)) annotations. DSyntSs are connected trees that contain only meaning-bearing lexical items and both predicate-argument (indicated by Roman numbers: I, II, III, IV, . . . ) and lexico-structural, or deepsyntactic, (ATTR(ibutive), APPEND(itive) and COORD(inative)) relations. In other words, they do not contain any punctuation and functional nodes, i.e., governed elements, auxiliaries and determiners. Governed elements such governed prepositions and subordinating conjunctions are dropped because they are imposed by sub-categorization restrictions of the predicative head and void of own meaning— a"
N15-1042,W05-1601,0,0.0234085,"focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach."
N15-1042,C10-1012,1,0.761097,"neration still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates t"
N15-1042,W11-2835,1,0.945822,"projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et"
N15-1042,de-marneffe-etal-2006-generating,0,0.0170839,"Missing"
N15-1042,P07-1041,0,0.125277,"sentations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portabi"
N15-1042,D08-1019,0,0.28662,", 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module rai"
N15-1042,W11-2833,0,0.0446441,"Missing"
N15-1042,W13-2131,0,0.0280973,"th rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use it for generation of spoken discourse in Arabic, Polish and Turkish. We believe"
N15-1042,P09-1091,0,0.108813,"their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this pap"
N15-1042,W07-2416,0,0.0989154,"starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summarize related work, before in Section 6 some conclusions are drawn and future work is outlined. 2 The Fu"
N15-1042,C12-1083,0,0.0809232,"ent a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008"
N15-1042,P95-1034,0,0.355379,"ans that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov e"
N15-1042,P98-1116,0,0.439226,"to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine"
N15-1042,W02-2103,0,0.194135,"see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and"
N15-1042,P10-1157,0,0.0362809,"Missing"
N15-1042,C12-2082,1,0.839143,"he experiments Spanish Treebank For the validation of the performance of our generator on Spanish, we use the AnCora-UPF treebank, which contains only about 100,000 tokens, but which has been manually annotated and validated on the SSyntS- and DSyntS-layers, such that its quality is rather high. The deep annotation does not contain any functional prepositions since they have been removed for all predicates of the corpus, and the DSyntS-relations have been edited following annotation guidelines. AnCora-UPF SSyntSs are annotated with fine-grained dependencies organized in a hierarchical scheme (Mille et al., 2012), in a similar fashion as the dependencies of the Stanford Scheme (de Marneffe et al., 2006).7 Thus, it is possible to use the full set of labels or to reduce it according to our needs. We performed preliminary experiments in order to assess which tag granularity is better suited for generation and came up with the 31-label tagset. 7 The main difference with the Stanford scheme is that in AnCora-UPF no distinction is explicitly made between argumental and non-argumental dependencies. 2.3.2 English Treebank For the validation of the generator on English, we use the dependency Penn TreeBank (abo"
N15-1042,W13-3724,1,0.879189,"o, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summari"
N15-1042,W11-2836,0,0.029485,"ybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use i"
N15-1042,C04-1097,0,0.0373783,"state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learni"
N15-1042,P04-1011,0,0.0319245,"on.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline t"
N15-1042,E09-1097,0,0.0213842,"nd number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a f"
N15-1042,N07-1022,0,0.00899511,"lassifiers for data-driven generators. As already mentioned in Section 1, most of the state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between mean"
N15-1042,C08-1038,0,\N,Missing
N15-1042,C98-1112,0,\N,Missing
N15-1042,W09-1201,0,\N,Missing
N15-3012,I13-2007,1,0.859674,"d simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it"
N15-3012,C14-1133,1,0.913287,"structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As s"
N15-3012,N15-1042,1,0.837746,"million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joi"
N15-3012,D12-1133,1,0.928036,"are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguis"
N15-3012,bohnet-wanner-2010-open,1,0.822011,"3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of"
N15-3012,E14-2003,0,0.0630282,"Missing"
N15-3012,S10-1059,0,0.0176587,"best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it allows for the display of any additional structural information provided by an extended parsing pipeline. For instance, if the obtained deep-syntactic structure is projected onto a frame-like structure (Chen et al., 2010) with semantic roles as arc labels, this frame structure can be displayed as well. We are currently working on such an extension. Furthermore, we aim to expand our visualization interface to facilitate active exploration of linguistic structures with Brat and thus add to the static display of structures the dimension of Visual Analytics (Keim et al., 2008). Acknowledgments This work has been partially funded by the European Union’s Seventh Framework and Horizon 2020 Research and Innovation Programmes under the Grant Agreement numbers FP7-ICT-610411, FP7-SME606163, and H2020-RIA-645012. Referen"
N15-3012,P08-5006,0,0.0225905,"plicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that"
N15-3012,N10-1011,0,0.0251457,"ructure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntacti"
N15-3012,D08-1019,0,0.0297568,"ions between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output"
N15-3012,W07-2416,0,0.13618,"ing of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics adv adv quant quant subj analyt perf (a) almost 1.2 million jobs have analyt pass prepos det prepos det been created by the state in that time ATTR ATTR agent ATTR ATTR II I II ATT"
N15-3012,C12-1083,0,0.016108,"ucture of a sentence. More precisely, a deep-syntactic structure (DSyntS) is a dependency tree that captures the argumentative, attributive and coordinative relations between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-synta"
N15-3012,nilsson-nivre-2008-malteval,0,0.0344817,"structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an opera"
N15-3012,J05-1004,0,0.0170667,", 2012). Brat takes an annotation file, which is produced by transforming the CoNLL files that the parsers output into Brat’s native format, and generates the graphical interface for the dependency trees. Figure 2 shows three sample surface syntactic structures in Brat. In Figure 3, their equivalent deepsyntactic structures are displayed. As already Figure 1, the figures illustrate the difference of both types of structures with respect to the abstraction of linguistic phenomena. The DSyntSs are clearly much closer to semantics. As a matter of fact, they are equivalent to PropBank structures (Palmer et al., 2005). However, this does not mean that they must per se be “simpler” than their corresponding surface-syntactic structures—compare, for instance, the structures (3a) and (3b) in Figures 2 and 3, where both SSyntS and DSyntS contain the same number of nodes, i.e., are isomorphic. The structures (2a) and (2b) illustrate the capacity of the deep parser to correctly identify the arguments of a lexical item without that explicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-sy"
N15-3012,E12-2021,0,0.106271,"Missing"
N15-3012,P13-4010,0,0.0552154,"Missing"
P16-2081,ramos-etal-2010-towards,1,0.856387,"Missing"
P16-2081,P14-1113,0,0.0644411,"nd the collocate matrix Cτ = [ct1 , ct2 . . . ctn ] are given by their corresponding vector representations. Together, they constitute a set of training examples Φτ , composed by vector pairs {bti , cti }ni=1 . Φτ is used to learn a linear transformation matrix Ψτ ∈ RB×C . Following the notation in (Tan et al., 2015), this transformation can be depicted as: vector representations are related by linear transformation (Mikolov et al., 2013b). This property has been exploited for word-based translation Mikolov et al. (2013b), learning semantic hierarchies (hyponym-hypernym relations) in Chinese (Fu et al., 2014), and modeling linguistic similarities between standard (Wikipedia) and nonstandard language (Twitter) (Tan et al., 2015). In our task, we learn a transition matrix over a small number of collocation examples, where collocates share the same semantic gloss, to apply then this matrix to discover new collocates for any previously unseen collocation base. We discuss the outcome of the experiments with ten different collocate glosses (including ‘do’ / ‘perform’, ‘increase’, ‘decrease’, etc.), and show that for most glosses, an approach that combines a stage of the application of a gloss-specific t"
P16-2081,P10-2020,0,0.55611,"ilgarriff, 2006). The central role of collocations for second language (henceforth, L2) learning has been discussed in a series of theoretical and empirical studies (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010) and is widely reflected in (especially English) learner dictionaries. In computational lexicography, several statistical measures have been used to retrieve collocations from corpora, among them, mutual information (Church and Hanks, 1989; Lin, 1999), entropy (Kilgarriff, 2006), pointwise mutual information (Bouma, 2010), and weighted pointwise 1 See (Pecina, 2008) for a detailed survey of such measures. 499 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 499–505, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics guage L2 , such that the found relations are between translation equivalents. In our case, we define a base space B and a collocate space C in order to relate bases with their collocates that have the same meaning, in the same language. To obtain the word vector representations in B and C, we use Mikolov et al. (2013c)’"
P16-2081,Y13-2006,0,0.0161267,"e, a state-of-the-art approach on correction of collocation errors by suggesting alternative cooccurrences, such, as, e.g., (Dahlmeier and Ng, 2011; Park et al., 2008; Futagi et al., 2008), might appear as a suitable baseline. We discarded this option given that none of them uses explicit fine-grained semantics. 502 Semantic gloss ‘intense’ ‘weak’ ‘perform’ ‘begin to perform’ ‘stop performing’ ‘increase’ ‘decrease’ ‘create’, ‘cause’ ‘put an end’ ‘show’ S6 0.82 0.45 0.40 0.42 0.22 0.55 0.37 0.59 0.43 0.85 Lin, 1999; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Futagi et al., 2008; Gao, 2013). Most of this work is based on statistical measures that indicate how likely the elements of a possible collocation are to co-occur, while ignoring the semantics of the collocations. Semantic classification of collocations has been addressed, for instance, in (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, our work is the first to automatically retrieve and typify collocations simultaneously. We have illustrated our approach with 10 semantic collocation glosses. We believe that this approach is also valid"
P16-2081,N15-1059,0,0.0260852,"However, to the best of our knowledge, our work is the first to automatically retrieve and typify collocations simultaneously. We have illustrated our approach with 10 semantic collocation glosses. We believe that this approach is also valid for the coverage of the remaining glosses (Mel’ˇcuk (1996) lists in his typology 64 glosses in total). Distributed vector representations (or word embeddings) (Mikolov et al., 2013c; Mikolov et al., 2013a), which we use, have proven useful in a plethora of NLP tasks, including semantic similarity and relatedness (Huang et al., 2012; Faruqui et al., 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015), dependency parsing (Duong et al., 2015), and Named Entity Recognition (Tang et al., 2014). We show that they also work for semantic retrieval of collocations. Only a small amount of collocations and big unannotated corpora have been necessary to perform the experiments. This makes our approach highly scalable and portable. Given the lack of semantically tagged collocation resources for most languages, our work has the potential to become influential in the context of second language learning. The datasets on which we performed the experiments as well as the details c"
P16-2081,W14-3501,1,0.852501,"analogies, such as, e.g., x ∼ applause ≡ heavy ∼ rain (implying x=thunderous) (Rodr´ıguez-Fern´andez et al., 2016). 2 Bτ Ψτ = Cτ We follow Mikolov et al.’s original approach and compute Ψτ as follows: min Ψτ Theoretical model |Φτ | X kΨτ bti − cti k2 i=1 Hence, for any given novel base bjτ , we obtain a novel list of ranked collocates by applying Ψτ bjτ and filtering the resulting candidates by part of speech and N P M I, an association measure that is based on the pointwise mutual information, but takes into account the asymmetry of the lexical dependencies between a base and its collocate (Carlini et al., 2014): The semantic glosses of collocates across collocations can be generalized into a generic semantic typology modeled, e.g., by Mel’ˇcuk (1996)’s Lexical Functions. For instance, absolute, deep, strong, heavy in absolute certainty, deep thought, strong wind, and heavy storm can all be glossed as ‘intense’; make, take, give, carry out in make [a] proposal, take [a] step, give [a] hint, carry out [an] operation can be glossed as ‘do’/‘perform’; etc. Our goal is to capture the relation that holds between the training bases and the collocates with the same gloss, such that given a new base and a gl"
P16-2081,P89-1010,0,0.0836696,"ea, etc., are restricted lexical co-occurrences of two syntactically bound lexical elements (Kilgarriff, 2006). The central role of collocations for second language (henceforth, L2) learning has been discussed in a series of theoretical and empirical studies (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010) and is widely reflected in (especially English) learner dictionaries. In computational lexicography, several statistical measures have been used to retrieve collocations from corpora, among them, mutual information (Church and Hanks, 1989; Lin, 1999), entropy (Kilgarriff, 2006), pointwise mutual information (Bouma, 2010), and weighted pointwise 1 See (Pecina, 2008) for a detailed survey of such measures. 499 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 499–505, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics guage L2 , such that the found relations are between translation equivalents. In our case, we define a base space B and a collocate space C in order to relate bases with their collocates that have the same meaning, in the same language."
P16-2081,D11-1010,0,0.0249509,"stop performing’). A closer look at the output reveals that in these The results of the experiments are displayed in Table 2. In general, the configurations S3 – S6 largely outperform the baselines, with the exception of the gloss ‘increase’, for which S2 equals S6 as far as p is concerned. However, in this case too MRR is considerably higher for S6, which achieves the highest MMR scores for 6 and the highest precision scores for 7 out of 10 glosses 5 At the first glan ce, a state-of-the-art approach on correction of collocation errors by suggesting alternative cooccurrences, such, as, e.g., (Dahlmeier and Ng, 2011; Park et al., 2008; Futagi et al., 2008), might appear as a suitable baseline. We discarded this option given that none of them uses explicit fine-grained semantics. 502 Semantic gloss ‘intense’ ‘weak’ ‘perform’ ‘begin to perform’ ‘stop performing’ ‘increase’ ‘decrease’ ‘create’, ‘cause’ ‘put an end’ ‘show’ S6 0.82 0.45 0.40 0.42 0.22 0.55 0.37 0.59 0.43 0.85 Lin, 1999; Kilgarriff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Futagi et al., 2008; Gao, 2013). Most of this work is based on statistical measures that indicate how likely the elements of a possible collocation are to co-occur, whi"
P16-2081,P12-1092,0,0.0513475,"Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, our work is the first to automatically retrieve and typify collocations simultaneously. We have illustrated our approach with 10 semantic collocation glosses. We believe that this approach is also valid for the coverage of the remaining glosses (Mel’ˇcuk (1996) lists in his typology 64 glosses in total). Distributed vector representations (or word embeddings) (Mikolov et al., 2013c; Mikolov et al., 2013a), which we use, have proven useful in a plethora of NLP tasks, including semantic similarity and relatedness (Huang et al., 2012; Faruqui et al., 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015), dependency parsing (Duong et al., 2015), and Named Entity Recognition (Tang et al., 2014). We show that they also work for semantic retrieval of collocations. Only a small amount of collocations and big unannotated corpora have been necessary to perform the experiments. This makes our approach highly scalable and portable. Given the lack of semantically tagged collocation resources for most languages, our work has the potential to become influential in the context of second language learning. The datasets on which"
P16-2081,D15-1040,0,0.0161957,"ly retrieve and typify collocations simultaneously. We have illustrated our approach with 10 semantic collocation glosses. We believe that this approach is also valid for the coverage of the remaining glosses (Mel’ˇcuk (1996) lists in his typology 64 glosses in total). Distributed vector representations (or word embeddings) (Mikolov et al., 2013c; Mikolov et al., 2013a), which we use, have proven useful in a plethora of NLP tasks, including semantic similarity and relatedness (Huang et al., 2012; Faruqui et al., 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015), dependency parsing (Duong et al., 2015), and Named Entity Recognition (Tang et al., 2014). We show that they also work for semantic retrieval of collocations. Only a small amount of collocations and big unannotated corpora have been necessary to perform the experiments. This makes our approach highly scalable and portable. Given the lack of semantically tagged collocation resources for most languages, our work has the potential to become influential in the context of second language learning. The datasets on which we performed the experiments as well as the details concerning the code and its use can be found at http://www.taln.upf"
P16-2081,N15-1184,0,0.0589229,"Missing"
P16-2081,P99-1041,0,0.194842,"lexical co-occurrences of two syntactically bound lexical elements (Kilgarriff, 2006). The central role of collocations for second language (henceforth, L2) learning has been discussed in a series of theoretical and empirical studies (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010) and is widely reflected in (especially English) learner dictionaries. In computational lexicography, several statistical measures have been used to retrieve collocations from corpora, among them, mutual information (Church and Hanks, 1989; Lin, 1999), entropy (Kilgarriff, 2006), pointwise mutual information (Bouma, 2010), and weighted pointwise 1 See (Pecina, 2008) for a detailed survey of such measures. 499 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 499–505, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics guage L2 , such that the found relations are between translation equivalents. In our case, we define a base space B and a collocate space C in order to relate bases with their collocates that have the same meaning, in the same language. To obtain th"
P16-2081,P10-2021,0,0.08028,"me of each experiment, we computed both precision (p) as the ratio of retrieved collocates that match the targeted glosses to the overall number of obtained collocates for each base, and Mean Reciprocal Rank (MRR), which rewards the position of the first correct result in a ranked list of outcomes: |Q| MRR = 1 X 1 |Q| ranki i=1 where Q is a sample of experiment runs and ranki refers to the rank position of the first relevant outcome for the ith run. MRR is commonly used in Information Retrieval and Question Answering, but has also shown to be well suited for collocation discovery; see, e.g., (Wu et al., 2010). We evaluated four different configurations of our technique against two baselines. The first baseline (S1) is based on the regularities in word embeddings, with the vec(king) − vec(man) + vec(woman) = vec(queen) example as paramount case. In this context, we manually selected one representative example for each semantic gloss to discover collocates for novel bases following the same schema; cf., e.g., for the gloss ‘perform’ vec(take) − vec(walk) + vec(suggestion) = vec(make) (where make is the collocate to be discovered); see (Rodr´ıguezFern´andez et al., 2016) for details. The second basel"
P16-2081,N13-1090,0,0.676336,"-specific. Thus, in English, you take [a] walk, in Spanish you ‘give’ it (dar [un] paseo), and in German and French you ‘make’ it ([einen] Spaziergang machen, faire [une] promenade); in English, rain is heavy, while in Spanish and German it is ‘strong’ (fuerte lluvia/starker Regen). In order to effectively support L2 learners, techniques are thus needed that are able not only to retrieve collocations, but also provide for a given base (or headword) and a given semantic gloss of a collocate meaning, the actual collocate lexeme. In what follows, we present such a technique, which is grounded in Mikolov et al. (2013c)’s word embeddings, and which leverages the fact that semantically related words in two different Abstract L2 learners often produce “ungrammatical” word combinations such as, e.g., *give a suggestion or *make a walk. This is because of the “collocationality” of one of their items (the base) that limits the acceptance of collocates to express a specific meaning (‘perform’ above). We propose an algorithm that delivers, for a given base and the intended meaning of a collocate, the actual collocate lexeme(s) (make / take above). The algorithm exploits the linear mapping between bases and colloc"
P16-2081,L16-1367,1,0.766085,"Missing"
P16-2081,J93-1007,0,0.923259,"Missing"
P16-2081,P15-2108,0,0.0226246,"Mikolov et al. (2013c)’s word2vec.2 The linear transformation model is constructed as follows. Let T be a set of collocations whose collocates share the semantic gloss τ , and let bti and cti be the collocate respectively base of the collocation ti ∈ T. The base matrix Bτ = [bt1 , bt2 . . . btn ] and the collocate matrix Cτ = [ct1 , ct2 . . . ctn ] are given by their corresponding vector representations. Together, they constitute a set of training examples Φτ , composed by vector pairs {bti , cti }ni=1 . Φτ is used to learn a linear transformation matrix Ψτ ∈ RB×C . Following the notation in (Tan et al., 2015), this transformation can be depicted as: vector representations are related by linear transformation (Mikolov et al., 2013b). This property has been exploited for word-based translation Mikolov et al. (2013b), learning semantic hierarchies (hyponym-hypernym relations) in Chinese (Fu et al., 2014), and modeling linguistic similarities between standard (Wikipedia) and nonstandard language (Twitter) (Tan et al., 2015). In our task, we learn a transition matrix over a small number of collocation examples, where collocates share the same semantic gloss, to apply then this matrix to discover new co"
P16-2081,J90-1003,0,\N,Missing
P19-1576,ramos-etal-2010-towards,1,0.891521,"Missing"
P19-1576,D16-1039,0,0.016371,"., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap regarding collocation modeling. Collocations, which are semi-compositional in their nature in that they are situated between fixed multiword expressions (MWEs) and free (semantic) word combinations, are of relevance to second language (henceforth, L2"
P19-1576,E12-1004,0,0.0348494,"ised relational similarity, as it has been shown that verb conjugation or nominalization patterns are relatively well preserved in vector spaces (Mikolov et al., 2013; Pennington et al., 2014a). Semantic relations pose a greater challenge (Vylomova et al., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap"
P19-1576,W11-2501,0,0.0368327,"to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap regarding collocation modeling. Collocations, which are semi-compositional in their nature in that they are situated between fixed multiword expressions (MWEs) and free (semantic) word combinations, are of relevance to second language (henceforth, L2) learners and NLP applications alike. In what follows, we investigate whether collocations can be modeled along the same lines as semantic relations between pairs of words. For this purpose, we introduc"
P19-1576,P10-2020,0,0.038226,"usly growing project. At the time of publication, the full set (available at https://www.upf.edu/web/taln/resources) contains around 10,000 collocations collected and manually categorized in terms of lexical functions by I. Mel’ˇcuk. 2 In our interpretation of the notion of collocation, we thus follow the lexicographic tradition Benson (1989); Cowie (1994); Mel’ˇcuk (1995); Binon and Verlinde (2013), which differs from a purely statistical interpretation based exclusively on relative co-occurrence frequency measures. text corpora; cf., e.g., (Evert and Kermes, 2013; Evert, 2007; Pecina, 2008; Bouma, 2010; Garcia et al., 2017), as well as the Shared Task of the PARSEME European Cost Action on automatic recognition of verbal MWEs.3 However, mere lists of collocations are often insufficient for both L2 acquisition and NLP. Thus, a language learner may not know the difference between, e.g., come to fruition and bring to fruition or between have [an] approach and take [an] approach, etc. Semantic labeling is required. The failure to identify the semantics of collocations also led, e.g., in earlier machine translation systems, to the necessity of the definition of collocation-specific crosslanguage"
P19-1576,J94-4004,0,0.311544,"2017), as well as the Shared Task of the PARSEME European Cost Action on automatic recognition of verbal MWEs.3 However, mere lists of collocations are often insufficient for both L2 acquisition and NLP. Thus, a language learner may not know the difference between, e.g., come to fruition and bring to fruition or between have [an] approach and take [an] approach, etc. Semantic labeling is required. The failure to identify the semantics of collocations also led, e.g., in earlier machine translation systems, to the necessity of the definition of collocation-specific crosslanguage transfer rules (Dorr, 1994; Orliac and Dillinger, 2003). The above motivates us to consider in this paper collocations and their classification in terms of LFs (Mel’ˇcuk, 1996), their most fine-grained semantic typology (see Section 2.2). Especially because, so far, this is only discussed in a reduced number of works, and typically on a smaller scale (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012). 2.2 LFs and the LexFunc dataset An LF can be viewed as a function f (·) that associates, with a given base L (which is the argument or keyword of f ), a set of (more or less) “synonymous collocates that are selected co"
P19-1576,D16-1041,0,0.0162083,"t al., 2013; Pennington et al., 2014a). Semantic relations pose a greater challenge (Vylomova et al., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap regarding collocation modeling. Collocations, which are semi-compositional in their nature in that they are situated between fixed multiword expressions (MWEs) a"
P19-1576,C18-1225,1,0.651931,"Missing"
P19-1576,P14-1113,0,0.0346028,"spaces (Mikolov et al., 2013; Pennington et al., 2014a). Semantic relations pose a greater challenge (Vylomova et al., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap regarding collocation modeling. Collocations, which are semi-compositional in their nature in that they are situated between fixed m"
P19-1576,W17-1703,0,0.217491,"Missing"
P19-1576,P89-1010,0,0.824757,"ctor). Specifically, they are formed by a freely chosen word (the base), which restricts the selection of its collocate (e.g., rain restricts us to use heavy in English to express intensity).2 Recovery of collocations from corpora plays a major role in improving L2 resources, in addition to obvious advantages in NLP applications such as natural language analysis and generation, text paraphrasing / simplification, or machine translation (Hausmann, 1984; Bahns and Eldaw, 1993; Granger, 1998; Lewis and Conzett, 2000; Nesselhauf, 2005; Alonso Ramos et al., 2010). Starting with the seminal work by Church and Hanks (1989), an extensive body of work has been produced on the detection of collocations in 1 Data and code are available at bitbucket.org/ luisespinosa/lexfunc. LexFunC is a continuously growing project. At the time of publication, the full set (available at https://www.upf.edu/web/taln/resources) contains around 10,000 collocations collected and manually categorized in terms of lexical functions by I. Mel’ˇcuk. 2 In our interpretation of the notion of collocation, we thus follow the lexicographic tradition Benson (1989); Cowie (1994); Mel’ˇcuk (1995); Binon and Verlinde (2013), which differs from a pu"
P19-1576,S12-1047,0,0.0111816,"orks address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et al., 2016). Furthermore, several high quality semantic relation datasets are available, ranging from wellknown resources such as WordNet (Miller, 1995), Yago (Suchanek et al., 2007), BLESS (Baroni and Lenci, 2011), several SemEval datasets (Jurgens et al., 2012; Camacho-Collados et al., 2018) or DiffVec (Vylomova et al., 2016). But there is a surprising gap regarding collocation modeling. Collocations, which are semi-compositional in their nature in that they are situated between fixed multiword expressions (MWEs) and free (semantic) word combinations, are of relevance to second language (henceforth, L2) learners and NLP applications alike. In what follows, we investigate whether collocations can be modeled along the same lines as semantic relations between pairs of words. For this purpose, we introduce LexFunC, a newly created dataset, in which col"
P19-1576,W14-1618,0,0.100217,"Missing"
P19-1576,N15-1098,0,0.0685489,"Missing"
P19-1576,2003.mtsummit-papers.39,0,0.0553777,"ll as the Shared Task of the PARSEME European Cost Action on automatic recognition of verbal MWEs.3 However, mere lists of collocations are often insufficient for both L2 acquisition and NLP. Thus, a language learner may not know the difference between, e.g., come to fruition and bring to fruition or between have [an] approach and take [an] approach, etc. Semantic labeling is required. The failure to identify the semantics of collocations also led, e.g., in earlier machine translation systems, to the necessity of the definition of collocation-specific crosslanguage transfer rules (Dorr, 1994; Orliac and Dillinger, 2003). The above motivates us to consider in this paper collocations and their classification in terms of LFs (Mel’ˇcuk, 1996), their most fine-grained semantic typology (see Section 2.2). Especially because, so far, this is only discussed in a reduced number of works, and typically on a smaller scale (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012). 2.2 LFs and the LexFunc dataset An LF can be viewed as a function f (·) that associates, with a given base L (which is the argument or keyword of f ), a set of (more or less) “synonymous collocates that are selected contingent on L to manifest the"
P19-1576,D14-1162,0,0.108341,"andard approach to relation classification is to combine the embeddings corresponding to the arguments of a given relation into a meaningful representation, which is then passed to a classifier. As for which relations have been targeted so far, the landscape is considerably more varied, although we may safely group them into morphosyntactic and semantic relations. Morphosyntactic relations have been the focus of work on unsupervised relational similarity, as it has been shown that verb conjugation or nominalization patterns are relatively well preserved in vector spaces (Mikolov et al., 2013; Pennington et al., 2014a). Semantic relations pose a greater challenge (Vylomova et al., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. als"
P19-1576,P16-2081,1,0.765438,"Missing"
P19-1576,C14-1097,0,0.151989,"Missing"
P19-1576,P16-1226,0,0.0337872,"Missing"
P19-1576,D12-1110,0,0.0577086,"sed relation vectors as a complementary input. While these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature. 1 Introduction Relation classification is the task of predicting whether between a given pair of words or phrases, a certain lexical, semantic or morphosyntactic relation holds. This task has direct impact in downstream NLP tasks such as machine translation, paraphrase identification (Etzioni et al., 2005), named entity recognition (Socher et al., 2012), or knowledge base completion (Socher et al., 2013). The currently standard approach to relation classification is to combine the embeddings corresponding to the arguments of a given relation into a meaningful representation, which is then passed to a classifier. As for which relations have been targeted so far, the landscape is considerably more varied, although we may safely group them into morphosyntactic and semantic relations. Morphosyntactic relations have been the focus of work on unsupervised relational similarity, as it has been shown that verb conjugation or nominalization patterns"
P19-1576,P16-1158,0,0.0822021,"corresponding to the arguments of a given relation into a meaningful representation, which is then passed to a classifier. As for which relations have been targeted so far, the landscape is considerably more varied, although we may safely group them into morphosyntactic and semantic relations. Morphosyntactic relations have been the focus of work on unsupervised relational similarity, as it has been shown that verb conjugation or nominalization patterns are relatively well preserved in vector spaces (Mikolov et al., 2013; Pennington et al., 2014a). Semantic relations pose a greater challenge (Vylomova et al., 2016), however. In fact, as of today, it is unclear which operation performs best (and why) for the recognition of individual lexico-semantic relations (e.g., hyperonymy or meronymy, as opposed to cause, location or action). Still, a number of works address this challenge. For instance, hypernymy has been modeled using vector concatenation (Baroni et al., 2012), vector difference and component-wise squared difference (Roller et al., 2014) as input to linear regression models (Fu et al., 2014; Espinosa-Anke et al., 2016); cf. also a sizable number of neural approaches (Shwartz et al., 2016; Anh et a"
P19-1576,D18-1058,0,0.25365,"Missing"
R15-1069,ramos-etal-2010-towards,1,0.711749,"Missing"
R15-1069,P10-2020,0,0.318327,"gles, we first clarify our usage of the term. Then, we outline the miscollocation typology that underlies our classification. 2.1  On the Nature of Collocations The term “collocation” as introduced by Firth (1957) and cast into a definition by Halliday (1961) encompasses the statistical distribution of lexical items in context: lexical items that form high probability associations are considered collocations. It is this interpretation that underlies most works on automatic identification of collocations in corpora; see, e.g., (Choueka, 1988; Church and Hanks, 1989; Pecina, 2008; Evert, 2007; Bouma, 2010). However, in contemporary lexicography and lexicology, an interpretation that stresses the idiosyncratic nature of collocations prevails. According to Hausmann (1984), Cowie (1994), Mel’ˇcuk (1995) and others, a collocation is a binary idiosyncratic co-occurrence of lexical items between which a direct syntactic dependency holds and where the occurrence of one of the items (the base) is subject of the free choice of the speaker, while the occurrence of the other item (the collocate) is restricted by the base. Thus, in the case of take [a] walk, walk is the base and take the collocate, in the"
R15-1069,W14-3501,1,0.825992,"sitive P M I, i.e., they are found together more often that this would happen if they would be independent variables. P M I has been a standard collocation measure throughout the literature since Church and Hank’s proposal in 1989. However, a mere use of P M I or any similar measure neglects that the lexical dependencies between the base and the collocate are not symmetric (recall that P M I is commutative, i.e., P M I(a, b) = P M I(b, a)). Only a few studies take into consideration the asymmetry of collocations; see, e.g., Gries (2013), who proposes an asymmetric association measure, ∆P, and Carlini et al. (2014), who propose an assymmetric normalization of P M I; see Eq. (2). In our work, we use Carlini et al. (2014)’s asymmetric N P M IC . Background on Collocations and Collocation Errors Given that the notion of collocation has been discussed and interpreted in lexicology from different angles, we first clarify our usage of the term. Then, we outline the miscollocation typology that underlies our classification. 2.1  On the Nature of Collocations The term “collocation” as introduced by Firth (1957) and cast into a definition by Halliday (1961) encompasses the statistical distribution of lexical it"
R15-1069,P89-1010,0,0.791559,"sed and interpreted in lexicology from different angles, we first clarify our usage of the term. Then, we outline the miscollocation typology that underlies our classification. 2.1  On the Nature of Collocations The term “collocation” as introduced by Firth (1957) and cast into a definition by Halliday (1961) encompasses the statistical distribution of lexical items in context: lexical items that form high probability associations are considered collocations. It is this interpretation that underlies most works on automatic identification of collocations in corpora; see, e.g., (Choueka, 1988; Church and Hanks, 1989; Pecina, 2008; Evert, 2007; Bouma, 2010). However, in contemporary lexicography and lexicology, an interpretation that stresses the idiosyncratic nature of collocations prevails. According to Hausmann (1984), Cowie (1994), Mel’ˇcuk (1995) and others, a collocation is a binary idiosyncratic co-occurrence of lexical items between which a direct syntactic dependency holds and where the occurrence of one of the items (the base) is subject of the free choice of the speaker, while the occurrence of the other item (the collocate) is restricted by the base. Thus, in the case of take [a] walk, walk is"
R15-1069,R15-1069,1,0.0513221,"Missing"
ramos-etal-2008-using,Y01-1001,0,\N,Missing
S17-2158,N15-1042,1,0.845391,"of the input graphs and the resolution of morphological agreements, and (ii) an off-theshelf statistical linearization component. 1 1 2 3 4 5 6 7 8 Setup of the system Layermtt ConS #rul. N/A SemS 190 SemSpos 96 DSyntS 267 SSyntS 294 DMorphS SMorphS Text Textf inal 85 N/A 1 4 Table 1: Overview of the AMR-to-text pipeline. The generator we presented for Task 9.2 of SemEval is a pipeline of graph transducers called Fabra Open Rule-based Generator (FORGe).1 It is built upon work presented, e.g., in (Bohnet, 2006; Wanner et al., 2010). It can be also considered an extended rule-based version of (Ballesteros et al., 2015). The current generator has been mainly developed on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicate-argument structures, and adapted to the AMR inputs using SemEval’s training and evaluation sets. 1.1 Step Conversion of AMRs format into CoNLL’09 format Mapping of AMRs onto predicate-argument graphs Assignment of parts of speech Derivation of deep syntactic structure Introduction of function words Resolution of agreements Linearization Retrieval of surface forms Post-processing 1.2 Input format conversion Since our generator cannot read the provid"
S17-2158,W11-2832,0,0.0456219,"tion of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NMOD IOBJ PMOD NMOD SBJ he peek at dog the black bark that AMRs are underspecified in terms of tense, aspect, number, and definiteness. For the task, a past progressive is equally correct as a simple present. Our generator is able to introduce all types of auxiliaries and/or mo"
S17-2158,kingsbury-palmer-2002-treebank,0,0.216658,", and the root of this tree is the main node (the root) of the sentence, this step can seem redundant. But since during Step 1 some 921 2.4 data used for the linearizer, and question and exclamation marks are introduced. Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NM"
S17-2158,W04-2705,0,0.127656,"ode (the root) of the sentence, this step can seem redundant. But since during Step 1 some 921 2.4 data used for the linearizer, and question and exclamation marks are introduced. Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004); see (Mille and Wanner, 2015). For instance, the entry corresponding to “peek” would contain the following information: 2.6 The surface-syntactic structures are linearized with an off-the-shelf tool used in the first SRST (Belz et al., 2011), a statistical tree linearizer that orders bottom-up each head and its children (Bohnet et al., 2011). 2.7 It indicates that, according to PropBank, the second argument of “peek” needs the preposition “at”. Hence, this preposition is introduced in the surface-syntactic structure, as shown in the followNMOD ing example: NMOD IOBJ PMOD NMOD SBJ he peek at d"
soler-company-wanner-2014-use,D10-1021,0,\N,Missing
soler-company-wanner-2014-use,D11-1120,0,\N,Missing
W00-1436,A97-1039,0,0.109442,"Missing"
W00-1436,C92-3158,0,\N,Missing
W01-0807,W96-0404,0,\N,Missing
W01-0807,W98-1406,1,\N,Missing
W01-0807,A00-1009,0,\N,Missing
W01-0807,C00-2149,0,\N,Missing
W01-0807,A97-1039,0,\N,Missing
W01-0807,P98-1060,0,\N,Missing
W01-0807,C98-1058,0,\N,Missing
W01-0807,W00-1436,1,\N,Missing
W11-2810,H05-1042,0,0.357791,"lassification as the event of a specific gameweek, comparing it to the events of the previous gameweek—that is, to the 20 classifications 8 of the previous gameweek and to the events of the same gameweek (also 20 classifications), such as the delta of category, points and team between classifications. In this way, we obtained a total of 760 feature types. In order to classify the data, we used Boostexter (Schapire and Singer, 2000), a boosting algorithm that uses decision stumps over several iterations and that has already been used in previous works on training content selection classifiers (Barzilay and Lapata, 2005; Kelly et al., 2009).9 For each of the three categories (goal, red card, classification), we experimented with 15 different classifiers by considering a section dimension 8 The Spanish League competition involves 20 teams. After a number of experiments, the number of iterations was set to 300. 9 77 Our evaluation of the content selection consisted of three stages: (1) evaluation of the automatic dataarticle alignment procedure, (2) evaluation of the performance of the classifiers for the empirical relevance determination, and (3) evaluation of the content selection as a whole. The evaluation"
W11-2810,W10-4202,0,0.213877,"a step towards more flexible content selection, (O’Donnell et al., 2001) put forward a proposal to select content by navigating a text potential. Also, in the recent past, determination of the relevant episodes in large time-series gained prominence (Yu et al., 2007; Portet et al., 2009). Although some of the data of a football league competition can also be expressed in terms of a time-series, in general, it goes beyond a numeric attribute-value pair sequence. Statistical techniques on numerical data have also been investigated—among them (Duboue and McKeown, 2003; Barzilay and Lapata, 2005; Demir et al., 2010). Some of these techniques use classifiers trained with supervised learning methods to decide on the selection of individual units of data (e.g., a row of a table in a relational database, or entities in an RDF graph). Others construct a graph-based representation of the content and apply an optimisation algorithm for network analysis (i.e. a flow or a centrality algorithm) to find out the most relevant subset of content. Ontologies have a long standing tradition in NLG, the most notable of which is the Upper Model (Bateman et al., 1990; Henschel, 1992, 1993; Bateman et al, 1995) which is a a"
W11-2810,W03-1016,0,0.748784,"rse structuring is presented in (Bouayad-Agha et al., 2011). The framework has been implemented with a KB that models the First Spanish Football League competitions for the generation (in Spanish) of short user perspective-tailored summaries of the individual matches. The user model is a simple model that contains the preference of the user for one of the teams. The content bounding parameters include the time, location and protagonists of the match of interest. The heuristics are based on weights determined empirically by supervised learning on a corpus of summaries aligned with data, as in (Duboue and McKeown, 2003). The following is an example generated summary:1 “Victoria del F.C. Barcelona. El Barcelona gan´o contra el Almer´ıa por 2-1 gracias a un gol de Ronaldinho en el minuto 34 y otro de Eto’o en el minuto 56. El Barcelona gano´ aunque acab´o el partido con 10 jugadores a causa de la expulsi´on de Eto’o. Gracias a esta victoria, permanece en la zona de champions. En la vig´esimo quinta jornada, se enfrentar´a al Villarreal.” The first and the last sentences of the text are template-based. The content selection strategy is responsible for dynamically selecting the contents used to generate the text"
W11-2810,W09-0623,0,0.290401,"of a specific gameweek, comparing it to the events of the previous gameweek—that is, to the 20 classifications 8 of the previous gameweek and to the events of the same gameweek (also 20 classifications), such as the delta of category, points and team between classifications. In this way, we obtained a total of 760 feature types. In order to classify the data, we used Boostexter (Schapire and Singer, 2000), a boosting algorithm that uses decision stumps over several iterations and that has already been used in previous works on training content selection classifiers (Barzilay and Lapata, 2005; Kelly et al., 2009).9 For each of the three categories (goal, red card, classification), we experimented with 15 different classifiers by considering a section dimension 8 The Spanish League competition involves 20 teams. After a number of experiments, the number of iterations was set to 300. 9 77 Our evaluation of the content selection consisted of three stages: (1) evaluation of the automatic dataarticle alignment procedure, (2) evaluation of the performance of the classifiers for the empirical relevance determination, and (3) evaluation of the content selection as a whole. The evaluation of the automatic alig"
W11-2810,C10-2116,0,0.0284778,"rs construct a graph-based representation of the content and apply an optimisation algorithm for network analysis (i.e. a flow or a centrality algorithm) to find out the most relevant subset of content. Ontologies have a long standing tradition in NLG, the most notable of which is the Upper Model (Bateman et al., 1990; Henschel, 1992, 1993; Bateman et al, 1995) which is a a linguistically motivated ontology. More directly related to our approach are ontology-oriented proposals in NLG whether to leverage linguistic generation (Bontcheva and Wilks, 2004), to verbalize ontologies (Wilcock, 2003; Power and Third, 2010) or to select content for the purpose of ontology verbalization (Mellish and Pan, 2008). 6 Conclusions and future work We have presented an NLG content selection approach performed on a task-independent ontologybased knowledge base. The lack of domain communication knowledge (Kittredge et al., 1991) in the ontology was remedied by adding to the basic ontology a second layer populated using inference rules that includes the modelling of semantic relations between individuals. Ontological information, that is knowledge of classes and properties, was exploited at all stages of content selection,"
W11-2810,W07-2315,0,0.0212631,"ng; see, e.g., (Hovy, 1993; Moore and Paris, 1993). In an attempt to systematize the structure of the used KBs and to build an intermediate knowledge-oriented layer between them and linguistic structures, language-oriented ontologies such as To identify and abstract regular patterns and trends and introduce semantic relations between the individuals of a generic domain ontology, which are critical for high quality generation, but absent from any general purpose ontology, prior to content selection a stage akin to signal analysis and data assessment used for the generation from numerical data (Reiter, 2007; Wanner et al., 2010) is performed. This new information is modeled as an additional layer on top of the domain ontology, which is populated via rule-based inferences. Content selection proper then takes place at a number of levels of increasing granularity. First, a content bounding task is in charge of selecting, based on the user query, a subset of the KB that includes the maximal set of information that might be communicated to the user. Next the main topics to be included in the content plan are selected, taking into account: 1) a user model, 2) a set of heuristics, and 3) the seman72 Pr"
W11-2810,J93-4004,0,\N,Missing
W11-2835,C10-1012,1,0.800448,"atical function relation labels (as SSyntR).1 The system thus realizes the following steps: 1. Semantic graph → Deep-syntactic tree 2. Deep-syntactic tree → Surface-syntactic tree 3. Surface-syntactic tree → Linearized structure 4. Linearized structure → Surface In addition, two auxiliary steps are carried out. The first one is part-of-speech tagging; it is carried out after step 3. The second one is introduction of commata; it is done after step 4. Each step is implemented as a decoder that uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x)"
W11-2835,P04-1015,0,0.0509894,"the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further processing is obsolete since the correct solution cannot be reached extending any elements of the beam. When we reach a final state, i.e. a tree spanning over all words and the correct solution is in the beam but not ranked first, we perform an update as well, since the correct element should have ranked first in the beam. Algorithm 1 displays the algorithm for the generation of the DSyntR from the SemR."
W11-2835,C96-1058,0,0.0241739,"uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further proces"
W12-1506,C00-1007,0,0.0623427,"ose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoN"
W12-1506,W11-2832,0,0.222768,"for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that th"
W12-1506,C10-3009,1,0.871223,"Missing"
W12-1506,C10-1012,1,0.93562,"erators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a ce"
W12-1506,W11-2835,1,0.926256,"thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order pr"
W12-1506,P98-1116,0,0.715111,"oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion o"
W12-1506,W02-2103,0,0.0370789,"standard annotation, with the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank al"
W12-1506,P10-1157,0,0.0356542,"Missing"
W12-1506,W00-0306,0,0.0630775,"t is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. 1 Introduction With the increasing interest in data-driven surface realization, the question on the adequate annotation of corpora for generation also becomes increasingly important. While in the early days of stochastic generation, annotations produced for other applications were used (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; LangkildeGeary, 2002), the poor results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdea"
W12-1506,J05-1004,0,0.0754015,"results obtained, e.g., by Bernd Bohnet Universit¨at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de (Bohnet et al., 2010) with the original CoNLL 2009 corpora, show that annotations that serve well for other applications, may not do so for generation and thus need at least to be adjusted.1 This has also been acknowledged in the run-up to the surface realization challenge 2011 (Belz et al., 2011), where a considerable amount of work has been invested into the conversion of the annotations of the CoNLL 2008 corpora (Surdeanu et al., 2008), i.e., PropBank (Palmer et al., 2005), which served as the reference treebank, into a more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Lang"
W12-1506,C04-1097,0,0.22939,"th the goal to obtain an increasingly more semantic annotation, can only be accepted if the quality of (deep) stochastic generation does not unacceptably decrease. To assess this aspect, we converted automatically the PropBank annotation of the WSJ journal as used in the CoNLL shared task 2009 into an annotation that complies with all of the principles sketched above 28 for deep statistical generation and trained (Bohnet et al., 2010)’s generator on this new annotation.11 For our experiments, we used the usual training, development and test data split of the WSJ corpus (Langkilde-Geary, 2002; Ringger et al., 2004; Bohnet et al., 2010); Table 1 provides an overview of the used data. set training development test section 2 - 21 24 23 # sentences 39218 1334 2400 Table 1: Data split of the used data in the WSJ Corpus The resulting BLEU score of our experiment was 0.64, which is comparable with the accuracy reported in (Bohnet et al., 2010) (namely, 0.659), who used an annotation that still contained all functional nodes (such that their generation task was considerably more syntactic and thus more straightforward). To assess furthermore whether the automatically converted PropBank already offers some adva"
W12-1506,P04-1011,0,0.0192458,"more “generation friendly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improv"
W12-1506,W08-2121,0,0.0817605,"Missing"
W12-1506,N07-1022,0,0.0136454,"dly” annotation. However, all of the available annotations are to a certain extent still syntactic. Even PropBank and its generation-oriented variants contain a significant number of syntactic features (Bohnet et al., 2011b). Some previous approaches to data-driven generation avoid the problem related to the lack of semantic resources in that they use hybrid models that imply a symbolic submodule which derives the syntactic representation that is then used by the stochastic submodule (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998). (Walker et al., 2002), (Stent et al., 2004), (Wong and Mooney, 2007), and (Mairesse et al., 2010) start from deeper structures: Walker et al. and Stent et al. from deep-syntactic structures (Mel’ˇcuk, 1988), and Wong and Mooney and Mairesse et al. from higher order predicate logic structures. However, to the best of our knowledge, 1 Trained on the original ConLL 2009 corpora, (Bohnet et al., 2010)’s SVM-based generator reached a BLEU score of 0.12 for Chinese, 0.18 for English, 0.11 for German and 0.14 for Spanish. Joining the unconnected parts of the sentence annotations to connected trees (as required by a stochastic realizer) improved the performance to a B"
W12-1506,P95-1034,0,\N,Missing
W12-1506,C98-1112,0,\N,Missing
W12-1525,W04-2705,0,\N,Missing
W12-1525,C10-1012,1,\N,Missing
W12-1525,W08-2121,0,\N,Missing
W12-1525,W11-2832,1,\N,Missing
W12-1525,W07-2416,0,\N,Missing
W12-1525,J05-1004,0,\N,Missing
W12-1525,W12-1528,1,\N,Missing
W12-1525,W04-3250,0,\N,Missing
W12-1525,kow-belz-2012-lg,1,\N,Missing
W12-1525,W12-1527,1,\N,Missing
W12-1527,H05-1042,0,0.176602,"user profile, discourse history, query, etc) needed for this task depend on the application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012),"
W12-1527,W11-2810,1,0.799661,"aches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good attempt at realisation, the problem of content determination from Semantic Web data is relatively untouched. For these reasons, we believe that"
W12-1527,W03-1016,0,0.767573,"tory, query, etc) needed for this task depend on the application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However"
W12-1527,W09-0623,0,0.0155452,"e application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good att"
W12-1527,C10-2116,0,0.028387,"ontent selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good attempt at realisation, the problem of content determination from Semantic Web data is relatively untouched. For these reasons, we believe that the time has come to bring together researchers working on (or interested in working on) content selection to participate in a challenge for this task using standard freely available web data as input. The availability of open modular multi-domain multi-billion triple data and of open ontological resources (Bizer et al., 2009) prese"
W12-1527,W10-4202,0,\N,Missing
W12-1527,W07-2322,0,\N,Missing
W13-2112,W13-2133,1,0.692485,"Missing"
W13-2112,W13-2134,0,0.0651598,"Missing"
W13-2112,W12-1527,1,0.850892,"iversity of Illinois at Chicago (USA). Before the presentation of the baseline evaluation of the submitted systems and the discussion of the results (Section 4), we outline the two data preparation subtasks (Sections 2 and 3). In Section 5, we then sketch some conclusions with regard to the achievements and future of the content selection task challenge. More details about the data, annotation and resources described in this overview, as well as links for downloading the data and other materials (e.g., evaluation results, code, etc.) are available on the challenge’s website.2 Introduction In (Bouayad-Agha et al., 2012), we presented the NLG challenge of content selection from semantic web data. The task to perform was described as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity). The task first required a data preparation stage that involved the following two subtasks: 1) data gathering and preparation, that is, deciding which data and texts to use, then downloading and pairing them, and 2) working dataset selection and annotation, that is, defining the criteria/guidelines for det"
W13-2112,W03-1016,0,0.0390448,"submitted systems was limited. Both of the presented systems were data-intensive in that they usedeither a pool of textual knowledge or the corpus of triple data provided by the challenge in order to select the most relevant data. Unlike several previous challenges that involve more traditional NLG tasks (e.g., surface realization, referring expression generation), content selection from large input semantic data is a relatively new research endeavour in the NLG community that coincides with the rising interest in statistical approaches to NLG and dates back, to the best of our knowledge, to (Duboue and McKeown, 2003). Furthermore, although we had initially planned to produce a training set for the task, the cost of manual annotation turned out to be prohibitive and the resulting corpus was only fit for development and baseline evaluation. Despite these setbacks, we believe that open semantic web data is a promising test-bed and application field for NLG-oriented content selection (Bouayad-Agha et al., 2013) and trust that this first challenge has prepared the ground for follow up challenges with a larger participation. We would also like to encourage researchers from NLG and Semantic Web research fields t"
W13-3724,apresjan-etal-2006-syntactically,0,0.0811369,"ic, but also with semantic information. This need implies that dependency treebanks must be annotated with both syntactic and semantic information, as, e.g., the Prague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sent"
W13-3724,W13-3703,1,0.884674,"Missing"
W13-3724,W11-2832,0,0.0721964,"transparent semantic frames, in the sense that no difference is made between external or internal arguments. 221 (d) SemS Figure 1: The four levels of annotation for the sentence El documento propone que este contrato afecte a las personas que engrosen las listas del paro ‘The document suggests that this contract affect the persons who make the unemployment lists swell’ sented in a single standard 14-column CoNLL file. The deep-syntactic layer is also provided in a separate CoNLL file, while the semantic layer is presented in the HFG format used in the SurfaceRealization Shared Task in 2011 (Belz et al., 2011). The different layers are connected thanks to the IDs of the nodes. 3 Multilayered annotation in practice Annotating such a corpus manually can seem too costly at the first sight. In this section, we show that a solid theoretical framework and the use of adequate tools can allow for significant reduction of the manual workload. 3.1 The advantages of our theoretical framework As already mentioned, our annotation model is strongly influenced by the Meaning-Text Theory (Mel’ˇcuk, 1988). Its rich stratification facilitates a clear separation of different types of linguistic phenomena and thus a s"
W13-3724,W00-1436,1,0.628518,"a given level to the corresponding representations at the adjacent levels. This has an interesting consequence as far as corpus annotation is concerned: starting from a given stratum and a manually created mapping grammar (the coverage does not need to be broad at first), the annotations at the adjacent strata can be easily obtained, and they can on their turn be used to derive the annotations at the next strata, and so on. In other words, with a corpus of SSyntSs, it is straightforward to derive parallel corpora of DSyntSs and SemSs using an adequate tool, such as the graph transducer MATE (Bohnet et al., 2000). The process of annotation can be reduced to a minimal manual revision of automatically created structures. For the surface-syntactic annotation, we use our detailed annotation schema that allows for relatively easy dependency relation identification, based on easy-to-use criteria. The annotation schema has been defined taking into account that (a) the schema should cover only criteria that are 222 related to the syntactic behaviour of the nodes; (b) the granularity of the schema should be balanced in the sense that it should be fine-grained enough to capture language-specific syntactic idios"
W13-3724,C10-1012,1,0.912806,"Missing"
W13-3724,W09-1210,0,0.0260248,"Missing"
W13-3724,W07-2416,0,0.0691859,"e labeling or semantic analysis, sentence generation, abstractive summarization, etc.) to deal not only with syntactic, but also with semantic information. This need implies that dependency treebanks must be annotated with both syntactic and semantic information, as, e.g., the Prague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the lin"
W13-3724,J93-2004,0,0.0427188,"layer is a simple chain of surface lexical units bearing morpho-syntactic information. Surface lexical units are all the items of the vocabulary, that is, words as they appear in any monolingual dictionary, and their inflected variants. In Table 1, all possible values of all morphosyntactic features used in our annotation are detailed. In addition to features such as gender and number, we use three different tagsets for Part-ofSpeech: a coarse-grained one, dpos, and two finegrained ones: pos and spos. The difference between pos, which is a subset of the PoS tagset from the Penn TreeBank set (Marcus et al., 1993), and spos is minor, although, for instance, in parsing 1 It includes the 3,510 sentences that AnCora comprised at the time we launched this project back in early 2008, and three additional sentences we used for early tests. For downloads, see http://www.taln.upf. edu/content/resources/495. Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 217–226, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. Features dpos Possible values A, Adv, N, V adjective, adverb, auxiliary, conjunction, copula, deter"
W13-3724,W04-2705,0,0.0932115,"2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical units bearing morpho-syntactic"
W13-3724,C12-2082,1,0.823944,"dence with the nodes of the morphological level. The 47 language-specific surface-syntactic relations used for the annotation of this layer are given and briefly explained in Table 4.3 In the corpus, 14 of these relations occur more than a thousand times; these are, from the most frequent to the less frequent: prepos, det, punc, adv, modif, subj, obl_obj, dobj, conj, coord, aux_phras, attr, copul, and relat. Depending on the application, one can need more or less tags in the annotation. In order to allow for tuning the granularity of the tagset, we organized the relations in a hierarchy (see (Mille et al., 2012) for illustration). 2.3 Deep-syntactic (DSynt) layer The structures at this layer are dependency trees in which labelled dependencies link pairs of deep lexical units. To the lexical units, deep-syntactic grammemes are assigned. The deep-syntactic dependency relations (cf. Table 5) are languageindependent and thus also more abstract than the surface-syntactic ones. In our corpus, the deepsyntactic layer contains only 66,980 nodes since all punctuation signs and functional nodes have been removed. In the following, the four particular cases of node-removal are listed.4 (a) Governed elements The"
W13-3724,J05-1004,0,0.0981899,"ague Dependency Treebank (PDT) 2.0 for Czech (Hajiˇc, 2004; J.Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical u"
W13-3724,N06-2015,0,0.0881324,"Missing"
W13-3724,taule-etal-2008-ancora,0,0.425113,"Hajiˇc et al., 2006) and the Italian Syntactic-Semantic Treebank (S.Montemagni et al., 2003). However, most of the widely-known treebanks contain only one layer of annotation, namely the syntactic one; see, e.g., the dependency version of the Penn TreeBank (Johansson and Nugues, 2007) for English, Talbanken05 for Swedish (Nilsson et al., 2005), and SynTagRus for Russian (Apresjan et al., 2006). To also offer semantic annotation, some corpora have been enriched a posteriori by semantic information; cf., e.g., Penn Treebank/PropBank (Palmer et al., 2005)/NomBank (Meyers et al., 2004) or Ancora (Taulé et al., 2008). The disadvantage of such 217 Our annotation intends to ensure that (i) a level of representation does not percolate into another one, and (ii) the annotation is complete in order to allow for easy automatic processing at each layer. Following the levels of the linguistic model in the Meaning-Text Theory (Mel’ˇcuk, 1988), we annotate four different layers on top of the sentence level: morphological, surface-syntactic, deep-syntactic, and semantic. 2.1 Morphological layer The morphological layer is a simple chain of surface lexical units bearing morpho-syntactic information. Surface lexical un"
W14-3501,ramos-etal-2010-towards,1,0.40546,"Missing"
W14-3501,P10-2020,0,0.475268,"ese experiments. In Section 6, a short summary of the related work is presented, before Section 7 draws some conclusions. 2 The Linguistic nature of collocations The term “collocation"" as introduced by Firth (1957) and cast into a definition by Halliday (1961) encompasses the statistical distribution of lexical items in context: lexical items that form high probability associations are considered collocations. It is this interpretation that underlies most works on automatic identification of collocations in corpora; see, e.g., (Choueka, 1988; Church and Hanks, 1989; Pecina, 2008; Evert, 2007; Bouma, 2010). However, in contemporary lexicography and lexicology an interpretation that stresses the idiosyncratic nature of collocations prevails. Thus, Benson (1989) states that “collocations should be defined not just as ‘recurrent 2 word combinations’, but as ‘ARBITRARY recurrent word combinations’ ”. “Arbitrary” as opposed to “regular” means that collocations are unpredictable and language-specific. For instance, in English, one takes a walk, while in French, German and Italian one ‘makes’ it: faire une promenade, einen Spaziergang machen, fare una passeggiata, and in Spanish one ‘gives’ it: dar un"
W14-3501,P89-1010,0,0.663666,"proposal, and Section 5 discusses the outcome of these experiments. In Section 6, a short summary of the related work is presented, before Section 7 draws some conclusions. 2 The Linguistic nature of collocations The term “collocation"" as introduced by Firth (1957) and cast into a definition by Halliday (1961) encompasses the statistical distribution of lexical items in context: lexical items that form high probability associations are considered collocations. It is this interpretation that underlies most works on automatic identification of collocations in corpora; see, e.g., (Choueka, 1988; Church and Hanks, 1989; Pecina, 2008; Evert, 2007; Bouma, 2010). However, in contemporary lexicography and lexicology an interpretation that stresses the idiosyncratic nature of collocations prevails. Thus, Benson (1989) states that “collocations should be defined not just as ‘recurrent 2 word combinations’, but as ‘ARBITRARY recurrent word combinations’ ”. “Arbitrary” as opposed to “regular” means that collocations are unpredictable and language-specific. For instance, in English, one takes a walk, while in French, German and Italian one ‘makes’ it: faire une promenade, einen Spaziergang machen, fare una passeggia"
W14-3501,D11-1010,0,0.149669,"Missing"
W14-3501,W09-2107,0,0.114639,"Missing"
W14-3501,P10-2021,0,0.228523,"Missing"
W14-3501,J90-1003,0,\N,Missing
W14-4416,W02-2103,0,\N,Missing
W14-4416,C10-1012,1,\N,Missing
W14-4416,N01-1001,0,\N,Missing
W14-4416,D08-1019,0,\N,Missing
W14-4416,W08-2102,0,\N,Missing
W14-4416,C00-1007,0,\N,Missing
W14-4416,C10-3009,0,\N,Missing
W14-4416,W11-2832,0,\N,Missing
W14-4416,D08-1008,0,\N,Missing
W14-4416,W13-3724,1,\N,Missing
W14-4416,W02-2105,0,\N,Missing
W15-2107,de-marneffe-etal-2014-universal,0,0.0487752,"Missing"
W15-2107,P84-1058,0,0.665272,"d not individual nodes. The degree of “semanticity” of DSyntSs can be directly compared to Prague’s tectogrammatical structures (Hajiˇc et al., 2006), which contain autosemantic words only, leaving out synsemantic elements such as determiners, auxiliaries, (all) prepositions and conjunctions. Collapsed SDs (de Marneffe et al., 2006) differ from the DSyntSs in that they collapse only (but all) prepositions, conjunctions and possessive clitics, they do not involve any removal of (syntactic) information, and they do not add semantic information compared to the surface annotation. 7 As, e.g., in (Gross, 1984), and the Explanatory Combinatorial Dictionary (Mel’ˇcuk, 1988). 53 nen et al. (2013) point out. If the syntactic behavior is not different when a dependent is an adverb or a noun, only one syntactic relation should be needed. uses the SD scheme adapted to Finnish. The second layer inserts additional dependencies over the first layer. This second layer tries, on the one hand, to cover more semantic phenomena (conjunct propagation for coordinations, and external subjects), but, on the other hand, it aims at covering some syntactic phenomena–gaps resulting from the first layer annotation–such as"
W15-2107,C14-1133,1,0.897425,"Missing"
W15-2107,N15-1042,1,0.887809,"Missing"
W15-2107,W12-3602,0,0.0218237,"kii . . . ‘Metla predicts that the birch will be in bloom . . . ’ for the latter. Thanks to this lexicon, rules can check in the input SSyntS if a word has a dependent of the type described in its entry, and perform the adequate mapping. For instance, if a dependent of ennustaa is a noun in the nominative case with the depen8 The lexicon furthermore contains additional information about the entries which is not related to subcategorization, such as morphological invariability, as well as the values for some lexical functions. 9 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. In particular, DSyntSs show some resemblance, but also some important differences, with PropBank structures, mainly due to the fact that the latter concern phrasal chunks and not individual nodes. The degree of “semanticity” of DSyntSs can be directly compared to Prague’s tectogrammatical structures (Hajiˇc et al., 2006), which contain autosemantic words only, leaving out synsemantic elements such as determiners, auxiliaries, (all) prepositions and conjunctions. Collapsed SDs (de Marneffe et al., 2006) differ from the DSyntSs in that they collaps"
W15-2107,W00-1436,1,0.475204,"ent); for instance, subj and dobj in Figure 1 map to argumental relations in Figure 2 (respectively I and II), while relat and adv are mapped to the non-argumental relation ATTR. 6 Even if it is possible to find sentences with the two nominal elements at the same side of the copula, they are not interpreted as neutral copulative sentences, but are communicatively marked. In other words, during the mapping between surface- and deep-syntax, functional elements and 52 predicate-argument relations have to be identified. Thanks to the existence of dedicated tools such as the graph-transducer MATE (Bohnet et al., 2000), the mapping of the SSynt-annotation onto the DSynt-annotation is facilitated. For instance, Mille et al. (2013) describe how they obtain the DSynt annotation of a Spanish treebank. To make the mapping straightforward, predicate-argument information is included in the tags of surfacesyntactic annotation, enriching surface-syntactic relations with semantic information. Thus, for instance, instead of simply annotating the relation obl obj when this relation is identified, specifying the argument number in the label is also required: obl obj0 corresponds to the first argument, obl obj1 to the se"
W15-2107,W13-3724,1,0.836354,"tation of the Stanford Dependency (SD) schema for 2 A surface-syntactic annotation of Finnish Our annotation schema for Finnish follows the methodology adopted for the elaboration of the 1 According to KORP -https://korp.csc.fi- the FTB with all its versions joined contains 4,386,152 sentences (76,532,636 tokens). However, the limited number of relations makes an in-depth analysis and/or comparison difficult. 48 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 48–57, Uppsala, Sweden, August 24–26 2015. schema of the Spanish AnCora-UPF treebank (Mille et al., 2013). Taking into account a series of clearly cut syntactically-motivated criteria, a tagset of Finnish syntactic dependencies has been established. In what follows, we first present the SSynt relation tagset, and then discuss some of the main criteria applied for the identification of selected tags. 2.1 DepRel adjunct adv appos attr aux aux phras bin junct Distinctive properties mobile sentential adverbial mobile verbal adverbial right-sided apposed element genitive complement of nouns non finite V governed by auxiliary verbs multi-word marker relates binary constructions non-independent adjacent"
W15-2107,W08-1301,0,0.159535,"Missing"
W15-2107,voutilainen-etal-2012-specifying,0,0.0643061,"Missing"
W15-2107,de-marneffe-etal-2006-generating,0,0.143549,"Missing"
W17-2506,itamar-itai-2008-using,0,0.0883031,"Missing"
W17-2506,L16-1147,0,0.0355873,"dic aspects in the interpreted speech or do not carry the traits of natural speech. Or they simply do not align well the source and the target language sides. To account for this deficit, we propose to exploit dubbed movies where expressive speech is readily available in multiple languages and their corresponding aligned scripts are easily accessible through subtitles. Movies and TV shows have been a good resource for collecting parallel bilingual data because of the availability and open access of subtitles in different languages. With 1850 bitexts of 65 languages, the OpenSubtitles project (Lison and Tiedemann, 2016) is the largest re31 Proceedings of the 10th Workshop on Building and Using Comparable Corpora, pages 31–35, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics In Section 5, finally, some conclusions are drawn and some aspects of our future work in the context of parallel speech corpora are mentioned. 2 information: (i) start time, (ii) end time, and (iii) text of the speech spoken at that time in the movie. The subtitle entries do not necessarily correspond to sentences: a subtitle entry may include more than one sentence, and a sentence can spread over many s"
W17-2506,melvin-etal-2004-creation,0,0.128831,"Missing"
W17-3517,W11-2832,1,0.948241,"Missing"
W17-3517,kow-belz-2012-lg,1,0.830242,"em, synonym and paraphrase matches. We will apply text normalization before scoring. For n-best ranked system outputs, we will compute a single score for all outputs by computing the weighted sum of their individual scores, with a weight assigned to an output in inverse proportion to its rank. For a subset of the test data we may obtain additional alternative realizations via Mechanical Turk for use in the automatic evaluations. 8 http://universaldependencies.org/ format.html For the human-assessed evaluation, we are planning to use a type of evaluation that is based on preference judgements (Kow and Belz, 2012, p.4035), using the existing evaluation interface described in Kow and Belz’s paper. As in SR’11, we plan to use students in the third year of an undergraduate degree, from Cambridge, Oxford and Edinburgh. Two candidate outputs9 will be presented to the evaluators, who will assess them for Clarity, Fluency and Meaning Similarity. For each criterion, they will be asked not only to state which system output they prefer, but also how strong is their preference. We plan to organize a workshop collocated with ACL ’18, COLING ’18, or EMNLP ’18 at which the results of the SR’18 will be presented. To"
W17-3517,S17-2090,0,0.0229781,"rained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand. 1 Introduction In 2017, three shared tasks on Natural Language Generation (NLG) take place: Task 9 of SemEval (May and Priyadarshi, 2017), WebNLG1 and E2E2 . The first starts from Abstract Meaning Representations (AMRs), the second from RDF triples, and the third from dialog act-based Meaning Representations (MRs) respectively. With these efforts, the focus is put on “real-life” generation, since the respective inputs come from existing analyzers (for AMRs) or existing databases (for RDF triples and 1 http://talc1.loria.fr/webnlg/stories/ challenge.html 2 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ MRs). This shows that the research on NLG is on the right track and that there is an interest in large scale “deep” NLG. However,"
W17-3517,W04-2705,0,0.187846,"Missing"
W17-3517,L16-1262,0,0.0611377,"Missing"
W17-3517,J05-1004,0,0.0504225,"their lemmas or stems, depending on the availability of lemmatization and stemming tools, respectively. For the Deep Track, additionally: 3. functional prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure will be removed, as e.g., “by” and “of” in Figure 2; 4. determiners and auxiliaries will be replaced (when needed) by attribute/value pairs, as, e.g., “Definiteness” and “Aspect” in Figure 3; 5. edge labels will be generalized into predicate argument labels, following the PropBank/NomBank edge label nomenclature (Meyers and et al., 2004; Palmer et al., 2005), with three main differences: (i) there will be no special label for external arguments (i.e., no “A0”), which means that all first arguments of a predicate will be mapped to A1, and the rest of the arguments will be labeled starting from A2; (ii) all modifier edges “AM-...” will be generalized to “AM”; (iii) there will be a coordinative relation; and (iv) any relation that does not fall into the first three cases will be assigned an underspecified edge label. 6. morphological information coming from the syntactic structure or from agreements will be removed; in other words, only “semantic” i"
W17-3539,N15-1042,1,0.84383,"Some other rules order governordependent pairs and siblings with one another. We then match the triple &lt;lemma&gt;&lt;POS&gt;&lt;morphosyntactic features&gt; with an entry of a morphological dictionary and simply replace the triple by the surface form. The final sentence corresponding to the running example would be He peeks at the black dog that barks. 3 Acknowledgments The work described in this paper has been partially funded by the European Commission under the contract numbers FP7-ICT-610411, H2020-645012RIA, H2020-700024-RIA, and H2020-700475-RIA. References SBJ he peek at dog the black bark that 2.4 (Ballesteros et al., 2015) or the linearization step (Bohnet et al., 2011), in order to overcome a possible lack of coverage of the rules. During the demo session, participants will be encouraged to play with the generator through a graphical interface, in order to see all the details of a generation process (in English, with some examples in German and Polish). A flexible multilingual generation pipeline The presented pipeline is flexible from several perspectives. First, it is quite easily adaptable to different types of inputs; for instance, it took only one week to adapt it to the AMRs of SemEval’17. Second, many r"
W17-3539,W11-2835,1,0.828323,"iblings with one another. We then match the triple &lt;lemma&gt;&lt;POS&gt;&lt;morphosyntactic features&gt; with an entry of a morphological dictionary and simply replace the triple by the surface form. The final sentence corresponding to the running example would be He peeks at the black dog that barks. 3 Acknowledgments The work described in this paper has been partially funded by the European Commission under the contract numbers FP7-ICT-610411, H2020-645012RIA, H2020-700024-RIA, and H2020-700475-RIA. References SBJ he peek at dog the black bark that 2.4 (Ballesteros et al., 2015) or the linearization step (Bohnet et al., 2011), in order to overcome a possible lack of coverage of the rules. During the demo session, participants will be encouraged to play with the generator through a graphical interface, in order to see all the details of a generation process (in English, with some examples in German and Polish). A flexible multilingual generation pipeline The presented pipeline is flexible from several perspectives. First, it is quite easily adaptable to different types of inputs; for instance, it took only one week to adapt it to the AMRs of SemEval’17. Second, many rules are language-independent, and others can be"
W17-3539,W07-2416,0,0.027111,"alana de Recerca i Estudis Avanc¸ats (ICREA), Lluis Companys 23, 08010 Barcelona, Spain firstname.lastname@upf.edu Abstract 2.1 This demo paper presents the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graphtransducers. 1 Introduction FORGe (Mille et al., 2017)1 is a pipeline of graph transducers which, coupled with lexical resources, allows for generating texts, starting from a variety of abstract input structures. The current generator has been mainly developed for English on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicateargument structures, and on Abstract Meaning Representations, using the SemEval’17 data (May and Priyadarshi, 2017). It is currently being adapted to languages such as Spanish, German French, and Polish, in the context of ontology-to-text generation as part of a dialogue system. Our generator follows the theoretical model of the Meaning-Text Theory (Mel’ˇcuk, 1988), and performs the following actions: (i) syntacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the"
W17-3539,kingsbury-palmer-2002-treebank,0,0.105846,"tacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the system In this section, we briefly describe the input to the system and the successive transductions . 1 See this paper for an evaluation of the system in the context of the SemEval AMR-to-text generation challenge. Inputs The input structures can be trees or acyclic graphs that contain linguistic information only, which includes meaning bearing units and predicateargument relations such as ARG0 (if licensing external arguments, as in PropBank (Kingsbury and Palmer, 2002)), ARG1, ARG2, . . . , ARGn). In order to allow for more compact representations, the generator can also handle “non-core” predicates as edges, be it with a generic label nonCore, or with a typed label such as purpose; see, for example two alternative representations of a purpose meaning between two nodes N1 and N2 : ARG1 ARG2 purpose N1 Npurpose N2 N1 N2 2.2 Generation of the deep syntactic structure First of all, parts of speech are assigned to each node of the structure. Then, during this transduction, a top-down recursive syntacticization of the semantic graph is performed. It looks for th"
W17-3539,S17-2090,0,0.0142368,"the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graphtransducers. 1 Introduction FORGe (Mille et al., 2017)1 is a pipeline of graph transducers which, coupled with lexical resources, allows for generating texts, starting from a variety of abstract input structures. The current generator has been mainly developed for English on the dependency Penn Treebank (Johansson and Nugues, 2007) automatically converted to predicateargument structures, and on Abstract Meaning Representations, using the SemEval’17 data (May and Priyadarshi, 2017). It is currently being adapted to languages such as Spanish, German French, and Polish, in the context of ontology-to-text generation as part of a dialogue system. Our generator follows the theoretical model of the Meaning-Text Theory (Mel’ˇcuk, 1988), and performs the following actions: (i) syntacticization of predicate-argument graphs; (ii) introduction of function words; (iii) linearization and retrieval of surface forms. 2 Overview of the system In this section, we briefly describe the input to the system and the successive transductions . 1 See this paper for an evaluation of the system"
W17-3539,W04-2705,0,0.0722544,"al Natural Language Generation conference, pages 245–246, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics 2.3 Introduction of function words The next step towards the realization of the sentence is the introduction of all idiosyncratic words (prepositions, auxiliaries, determiners, etc.) and of a fine-grained (surface-)syntactic structure that gives enough information for linearizing and resolving agreements between the different words. For this task, we use a valency (subcategorization) lexicon built automatically from PropBank and NomBank (Meyers et al., 2004). During this transduction, anaphora are resolved, and personal pronouns are introduced in the tree (this includes possessive, relative and personal pronouns). See, e.g., how the preposition “at” is introduced in the following surface-syntactic structure: SBJ NMOD NMOD IOBJ PMODNMOD Resolution of morpho-syntactic agreements, linearization, and retrieval of surface forms In order to resolve agreements, the rules for this transduction check the governor/dependent pairs, together with the syntactic relation that links them together. Some other rules order governordependent pairs and siblings with"
W17-3539,S17-2158,1,0.79536,"Missing"
W17-6506,J08-3003,0,0.0766905,"Missing"
W17-6506,W11-3806,0,0.0581726,"Missing"
W17-6506,W13-3724,1,0.818031,"h-words can take case annotation schema followed in the MS treebank, suffixes (governed by the head) and question with the MS’ tagset as basis. The design of our particles cannot, and the first ones can only take annotation schema follows the principles of the verbal inflectional suffixes in copulative sentences MTT framework (Mel’ˇcuk, 1988) and the method(as any other noun), whereas particles take them ology adopted for the elaboration of the annotain any yes–no question, if the questioned element tion schema of the Spanish AnCora-UPF treebank is the verb; (ii) the mapping to deeper levels (Mille et al., 2013) and the Finnish weather corpus becomes truncated, given that the real syntac(Burga et al., 2015). tic function of the wh-words (e.g., OBJECT, The mapping of the original MS annotation into SUBJECT, etc.) is not annotated at the SSynt our annotation was carried out in two stages; its layer. result is henceforth referred to as “UPF-METU SSynt”. In the first stage, general transformations (12) Wh-word treated as question particle: have been made. These transformations targeted, QUESTION.PARTICLE first of all, the removal of the relation DERIV (enNe kaybedersiniz? coding the corresponding morphol"
W17-6506,W03-2405,0,0.0856972,"morphological phenomena only, we can define a grammeme as “an element of an inflectional category” and a derivateme “as an element that is formally expressed by the same linguistic means as a grammeme, but that is not obligatory and not necessarily regular” (Melˇcuk and Wanner, 2008). Agglutination: SSynt vs. DMorph Agglutinative languages are synthetic languages in which words consist of a base and a set of agglu2 Most of the reported work has been done prior to the release of the IMST corpus. Note also that in the meantime some modifications of the original MS treebank have been made; cf. (Atalay et al., 2003). However, we use the original version. 33 2.2.2 of them stored as a feature-value pair assigned to the lexeme in question (e.g., table [number = PL]). In the next subsection, we analyze the MS treebank annotation schema from the perspective of this phenomenon separation as well as from the perspective of the coverage of the individual syntactic phenomena. 2.2 Apart from the problem resulting from the merge of DMorph and SSynt layers of annotation, the MS annotation reveals some issues that originate mainly from the underlying annotation guidelines and that affect directly the syntactic annota"
W17-6506,bohnet-wanner-2010-open,1,0.908749,"er from the theoretical point of view to separate the different levels of linguistic representation. Since the relation DERIV, included in the MS vardı. (it-was)-existing ‘They had a single aim.’ 36 SSynt tagset, relates the inflectional groups between each other and thus encodes a phenomenon that belongs to the DMorph layer (see Section 2.2), it needs to be removed from the SSynt tagset. For this purpose, the nodes related through DERIV are merged into one and the information of each node is stored in terms of feature-value pairs of the resulting node using a MATE graph transduction grammar (Bohnet and Wanner, 2010). As a consequence, an MS subtree such as shown in (15) is converted into a single node with many morphological features (as in (16)). (15) Addressing the vagueness in syntactic relation delimitation. According to the MTT principles, it is crucial to distinguish between adjuncts and objects in SSynt, given that each of them maps to different relations in deeper layers. Therefore, in order to distinguish between the relations ADJUNCT and OBJECT in the case of a verbal head, we consult the case suffix added to the dependent and the analysis of the meaning of the verb. In MTT, the case of objects"
W17-6506,W15-2107,1,0.932479,"ad) and question with the MS’ tagset as basis. The design of our particles cannot, and the first ones can only take annotation schema follows the principles of the verbal inflectional suffixes in copulative sentences MTT framework (Mel’ˇcuk, 1988) and the method(as any other noun), whereas particles take them ology adopted for the elaboration of the annotain any yes–no question, if the questioned element tion schema of the Spanish AnCora-UPF treebank is the verb; (ii) the mapping to deeper levels (Mille et al., 2013) and the Finnish weather corpus becomes truncated, given that the real syntac(Burga et al., 2015). tic function of the wh-words (e.g., OBJECT, The mapping of the original MS annotation into SUBJECT, etc.) is not annotated at the SSynt our annotation was carried out in two stages; its layer. result is henceforth referred to as “UPF-METU SSynt”. In the first stage, general transformations (12) Wh-word treated as question particle: have been made. These transformations targeted, QUESTION.PARTICLE first of all, the removal of the relation DERIV (enNe kaybedersiniz? coding the corresponding morphological informaWhat (you)-will-lose tion in terms of morphological feature-values as‘What will you"
W17-6506,W13-3704,0,0.0439153,"Missing"
W17-6506,coltekin-2010-freely,0,0.0337441,"Missing"
W18-3601,P11-2040,1,0.888439,"Missing"
W18-3601,W11-2832,1,0.629164,"e languages may also work for others. The SR’18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation. SR’18 also addresses questions about just how suitable and useful the notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc"
W18-3601,W17-4755,1,0.838761,"gy) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a ‘damaged’ version and (iii) some are replaced by their corresponding reference texts. In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high. For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a). Below we report QA figures for the MTurk evaluations (Section 3.2.1). Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.10 3.2.2 Google Data Compute Evaluation In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google’s internal ‘Data Compute’ system evaluation service, where experienced evaluators carefully assess each system output. We used an interface that matches the WMT’17 interface above, as closely as was possible within the constraints of th"
W18-3601,P17-1017,0,0.0630018,"he notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD str"
W18-3601,D14-1020,1,0.831205,"Missing"
W18-3601,S17-2090,0,0.0582517,"h is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD structures in which word order information has b"
W18-3601,W04-2705,0,0.458005,"Missing"
W18-3601,L16-1262,0,0.0728681,"Missing"
W18-3601,W17-5525,0,0.0981398,"Missing"
W18-3601,J05-1004,0,0.102132,"in CoNLL-U format, with no meta-information.7 Figures 1, 2 and 3 show 6 universaldependencies.org 7 http://universaldependencies.org/ a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it. To create inputs to the Shallow Track, the UD structures were processed as follows: 1. Word order information was removed by randomised scrambling; 2. Words were replaced by their lemmas. For the Deep Track, the following steps were additionally carried out: 3. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1. 4. Functional prepositions and conjunctions in argument position (i.e. prepos"
W18-3601,P02-1040,0,0.102984,"nt Example fall→ the ball the ball→ fall fall→ last night fall→ [and] bounce Tower→ Eiffel N/A Table 1: Deep labels. train dev test ar 6,016 897 676 cs 66,485 9,016 9,876 en 12,375 1,978 2,061 es 14,289 1,651 1,719 fi 12,030 1,336 1,525 fr 14,529 1,473 416 it 12,796 562 480 nl 12,318 720 685 pt 8,325 559 476 ru 48,119 6,441 6,366 Table 2: SR’18 dataset sizes for training, development and test sets. 3 3.1 Evaluation Methods Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative. Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single)"
W18-6527,W10-4226,1,0.705033,"atically parsed data mentioned above shows. It is conceivable that a future shared task in NLG will involve paired (structured) data and text, plus an automatically created intermediate level of representation comprising underspecified UD (UUD) structures enriched with additional information obtained from the structured data level. This would correspond to three linked tracks (data-to-text, data-to-UUD, and UUD-totext) where one track is the end-to-end task, and the other two tracks are subtasks that can be combined to solve the end-to-end task, similar to the GREC’10 shared task competition (Belz and Kow, 2010). Or it could be argued, perhaps controversially still, that the days of structured linguistic representations in NLG are numbered anyway. The rapid development and spread of highly successful neural approaches to diverse NLG tasks, and the limited success so far of attempts to inject linguistic knowledge directly into neural networks, certainly lends some strength to this point of view. In the meantime, the above tripartite shared-task structure has the potential to accommodate both systems that map directly from data to text without structured representations, and two-component systems with"
W18-6527,W11-2832,1,0.853605,"arious ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependency structures offer a more flexible input structure and statistical systems, in principle, offer more robustness, the uptake of such systems as comp"
W18-6527,W04-2705,0,0.339346,"Missing"
W18-6527,W18-3601,1,0.855834,"as Inputs for Multilingual Surface Realisation Simon Mille Universitat Pompeu Fabra Barcelona, Spain simon.mille@upf.edu Anja Belz University of Brighton Brighton, UK a.s.belz@brighton.ac.uk Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Leo Wanner ICREA and Universitat Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract resolved. The success of SimpleNLG (Gatt and Reiter, 2009) which had much reduced grammatical coverage, but accepted radically simpler inputs demonstrated the importance of this issue. The recently completed first Multilingual Surface Realisation Task (SR’18) (Mille et al., 2018) used for the first time inputs derived from the Universal Dependencies (UDs) (de Marneffe et al., 2014), a framework which was devised with the aim of facilitating cross-linguistically consistent grammatical annotation, and which has grown into a large-scale community effort involving more than 200 contributors, who have created over 100 treebanks in over 70 languages between them.1 UDs provide a more general and potentially flexible input representation for surface realisation (SR). However, their use for NLG has not so far been demonstrated. In this paper, we present the UD datasets used in"
W18-6527,bohnet-wanner-2010-open,1,0.85998,"Missing"
W18-6527,P06-1130,0,0.111814,"Missing"
W18-6527,W96-0501,0,0.474787,"In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process. 1 Introduction There has long been an assumption in Natural Language Generation (NLG) that surface realisation can be treated as an independent subtask for which stand-alone, plug-and-play tools can, and should, be created. Early surface realisers such as KPML (Bateman, 1997) and FUF/Surge (Elhadad and Robin, 1996) were ambitious, independent surface realisation tools for English with wide grammatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG"
W18-6527,P17-1017,0,0.0215344,"mmatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG more generally. Some conclusions are presented in Section 7. • WebNLG dataset (Gardent et al., 2017): DBpedia triples covering properties of 15 DBpedia categories; 2 • E2E dataset (Novikova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journa"
W18-6527,W05-1510,0,0.0204745,"ibute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn Tree"
W18-6527,W09-0613,0,0.204092,"Missing"
W18-6527,W17-5525,0,0.0700189,"Missing"
W18-6527,W02-2103,0,0.0384065,"ova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the"
W18-6527,J05-1004,0,0.700717,"0 2 3 4 A2 ROOT A1 A2 AM Figure 3: Deep input (Track 2) derived from UD structure in Figure 1. (left: CoNLL-U, righ: graphical) ures 1 and 2 show an original UD structure and a SR’18 Shallow input, respectively. • the in the head can be seen as a marker for nominal definiteness; 3.2 • the conjunction (complementiser) that in, e.g., I demand that you apologise, appears because it connects a finite verb apologise as an argument of another verb demand. Deep inputs The Deep Track input structures are trees that contain only content words linked by predicateargument edges, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. The Deep inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information. At the same time, we used only information found in the UD structures to create the Deep inputs, and tried to keep their structure simple. In Deep inputs, words are not disambiguated, full (semantically loaded) prepositions may be missing, and some argument relations may be underspecified or missing. The next two subsections provide more details a"
W18-6527,P09-1011,0,0.0151157,"ncies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 2017): abstract predicate-argument graphs that cover several genres; 200 1 2 3 4 5 6 7 8 9 10 11 12 13 The third was being run by the head of an investment firm . the third be be run by the head of a investment firm . DET ADJ AUX AUX VERB ADP DET NOUN ADP DET NOUN NOUN PUNCT DT JJ VBD VBG VBN IN DT NN IN DT NN NN . Definite=Def|PronType=Art Degree=Pos|NumType=Ord Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin VerbForm=Ger Tense=Past|VerbForm=Part|Voice=Pass Definite=Def"
W18-6527,W08-2121,0,0.136007,"Missing"
W18-6527,de-marneffe-etal-2014-universal,0,0.0679075,"Missing"
W18-6527,D09-1043,0,0.027139,"ground With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependen"
W18-6527,K17-3001,0,0.0322515,"a Shallow Track, starting from syntactic structures in which word order information has been removed and tokens have been lemmatised, and a Deep Track, which starts from more abstract structures from which, additionally, functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological information have been removed. Taking advantage of the growing availability of multilingual treebanks annotated with Universal Dependencies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 201"
W18-6542,N16-1087,0,0.0197341,"eebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional generation task distribution, sen350 Proceedin"
W18-6542,W13-2111,0,0.013224,"se subgraphs. We interpret the problem of sentence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, th"
W18-6542,R11-1012,0,0.028271,"mon Mille DTIC, UPF simon.mille@upf.edu Abstract tence packaging is largely avoided. It is assumed that the text planning module creates a text plan from selected elementary statements (elementary discourse units), establishing discourse relations between them. The sentence planning module then either aggregates the elementary statements contained in the text plan into more complex statements or keeps them as separate simple statements, depending on the language, style, preferences of the targeted reader, etc. (Shaw, 1998; Dalianis, 1999; Stone et al., 2003). Even if datadriven, as, e.g., in (Bayyarapu, 2011), this strategy may suggest itself mainly for input representations with a limited number of elementary elements and simple sentential structures as target. In the context of scalable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statem"
W18-6542,W11-2832,0,0.0506235,"Missing"
W18-6542,J17-1001,0,0.0150892,"., AMRs; cf., e.g., (May and Priyadarshi, 2017; Song et al., 2018). For these generators, the problem of sentence packaging or aggregation is obviously obsolete. As already mentioned in the Introduction, in setups that start from input that is not yet cast into sentence structures, traditional NLTG foresees the task of (content) aggregation, which is dealt with as part of sentence planning (or microplanning): the elementary content elements, as assumed to be present in the text plan, are aggregated into more complex elements; see, among others, (Shaw, 1998; Dalianis, 1999; Stone et al., 2003; Gardent and Perez-Beltrachini, 2017). Our work is more in line with Konstas and LaFor illustration, consider in Figure 5 a subgraph obtained from a larger initial graph, which is shown in Figure 6 (the obtained subgraph is circled). The subgraph corresponds to the ground truth subgraph with a precision of 0.938 and a recall of 0.882. It might be seen that the obtained subgraph contains enough information to generate a sentence with a similar meaning as the original one. The original sentence that corresponds to the subgraph in Figure 5 is He said the company is experimenting with the technique on alfalfa, and plans to include co"
W18-6542,P17-1017,0,0.0228317,"ommunity detection algorithms (and focus only on the problem of sentence packaging), they view the entire problem of the verbalization of a hypergraph as a graph traversal problem. The difference in the size of the input data (and thus the number of the resulting sentences) is also a distinctive feature of our proposal when we compare it to other works that deal with sentence packaging. For instance, Narayan et al. (2017) split in their experiments on text simplification complex sentences into 2 to 3 more simple sentences. As content representation, they use the WebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the corefere"
W18-6542,C10-1012,1,0.790405,"ure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional generation task distri"
W18-6542,W09-0613,0,0.0471738,"Missing"
W18-6542,W16-6626,0,0.0164395,"entence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into"
W18-6542,W02-2101,0,0.0313778,"VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arises. In the traditional g"
W18-6542,N12-1093,0,0.0227533,"calable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the ch"
W18-6542,W02-2105,0,0.119288,"eriments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to as “sentence packaging”, arise"
W18-6542,J14-4003,0,0.016096,"-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the challenge of text generation from abstract ontological or semantic structures, which are in their very nature po"
W18-6542,P14-5010,0,0.00680384,"xt of numerous applications, including biomedicine (e.g., for protein interaction network (Bader and Hogue, 2003) or brain connectivity analysis (Hagmann et al., 2008)), web mining (Sarıyuece et al., 2015), influence analysis (Ugander et al., 2012), community detection (Asim et al., 2017), etc. Our model is inspired by the work on community detection. The model has been validated in experiments on the VerbNet/FrameNet annotated version of the Penn TreeBank (Mille et al., 2017), in which coreferences in the individual texts of the corpus have been identified using the Stanford CoreNLP toolkit (Manning et al., 2014) and fused to obtain a graph representation. The experiments show that we achieve an F1 -score of 0.738 (with a precision of 0.792 and a recall of 0.73), which means that our model is able to cope with the problem of sentence packaging in NLTG. The remainder of the paper is structured as follows. In Section 2, we introduce the semantic graphs that are assumed to be decomposed and analyze them. Section 3 outlines the experiments we carried out, and Section 4 discusses the outcome of these experiments. In Section 5, we briefly review the work that is related to ours. In Section 6, finally, we dr"
W18-6542,S17-2090,0,0.0604259,"Missing"
W18-6542,N01-1001,0,0.113406,"ith post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we will refer henceforth to a"
W18-6542,W18-3601,1,0.818738,"ebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final version of the paper. Re"
W18-6542,D15-1199,0,0.0215971,"Missing"
W18-6542,W18-6527,1,0.823291,"ebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final version of the paper. Re"
W18-6542,D17-1064,0,0.0926301,"ch can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse structure is, as a rule, defined over sentential structures rather than elementary statements, sentence packaging on semantic representations appears as an alternative that is worth to be explored. More recent data-driven concept-to-text approaches to NLTG, e.g., (Konstas and Lapata, 2012), text simplification, e.g., (Narayan et al., 2017), dialogue act realization, e.g., (Mairesse and Young, 2014; Wen et al., 2015), deal with sentence packaging, but, as a rule, all of them concern inputs of limited size, with at most 3 to 5 resulting sentence packages, while realistic large input semantic graphs may give rise to dozens. In what follows, we present a model for sentence packaging of large semantic graphs, which contain up to 75 sentences. In general, the problem of sentence packaging consists in the optimal decomposition of a given An increasing amount of research tackles the challenge of text generation from abstract ontologica"
W18-6542,W17-5525,0,0.0177525,"ntation, they use the WebNLG dataset of RDF-triples (Gardent et al., 2017). To split a given set of RDF-triples into several subsets, they learn a probabilistic model. Wen et al. (2015) use LSTM-models to generate utterances from a given sequence of tokens in the context of a dialogue application. Since for our experiments we apply coreference resolution to create from the VerbNet/Framenet annotated sentences of the Penn Treebank large connected graphs, our work could be also considered to be related to the recent efforts on the creation of datasets for NLTG; cf., e.g., (Gardent et al., 2017; Novikova et al., 2017; Mille et al., 2018b). However, so far, the coreference resolution has been entirely automatic, with no subsequent thorough validation and manual correction. Both would be needed to ensure high quality of the resulting dataset. 6 Acknowledgments The presented work was supported by the European Commission under the contract numbers H2020-645012-RIA, H2020-7000024-RIA, H2020-700475-IA, and H2020-779962-RIA and by the Russian Foundation for Basic Research under the contract number 18-37-00198. Many thanks to the three anonymous reviewers, whose insighful comments helped to improve the final vers"
W18-6542,A00-2026,0,0.0106883,"detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1 -score of 0.738. 1 Leo Wanner ICREA and DTIC, UPF leo.wanner@upf.edu Introduction An increasing amount of research in Natural Language Text Generation (NLTG) tackles the challenge of generation from abstract ontological (Bontcheva and Wilks, 2004; Sun and Mellish, 2006; Bouayad-Agha et al., 2012; Banik et al., 2013; Franconi et al., 2014; Colin et al., 2016) or semantic (Ratnaparkhi, 2000; Varges and Mellish, 2001; Corston-Oliver et al., 2002; Kan and McKeown, 2002; Bohnet et al., 2010; Flanigan et al., 2016) structures. Unlike input structures to surface generation, which are syntactic trees, ontological and genuine semantic representations are predominantly connected graphs or collections of elementary statements (as, e.g., RDF-triples or minimal predicate-argument structures) in which re-occurring elements are duplicated (but which can be, again, considered to be a connected graph). In both cases, the problem of the division of the graph into sentential subgraphs, which we"
W18-6542,W98-1415,0,0.536168,"Community Detection Problem Alexander Shvets DTIC, UPF alexander.shvets@upf.edu Simon Mille DTIC, UPF simon.mille@upf.edu Abstract tence packaging is largely avoided. It is assumed that the text planning module creates a text plan from selected elementary statements (elementary discourse units), establishing discourse relations between them. The sentence planning module then either aggregates the elementary statements contained in the text plan into more complex statements or keeps them as separate simple statements, depending on the language, style, preferences of the targeted reader, etc. (Shaw, 1998; Dalianis, 1999; Stone et al., 2003). Even if datadriven, as, e.g., in (Bayyarapu, 2011), this strategy may suggest itself mainly for input representations with a limited number of elementary elements and simple sentential structures as target. In the context of scalable report (or any other narration) generation, which can be assumed to start, for instance, from large RDF-graphs (i.e., RDFtriples with cross-referenced elements), or from large semantic graphs, the aggregation challenge is incomparably more complex. In the light of this challenge and the fact that in a narration the discourse"
W18-6542,P18-1150,0,0.0587581,"Missing"
W19-3510,W17-3013,0,0.0438216,"Missing"
W19-3510,W17-6615,0,0.0397333,"Missing"
W19-3510,W18-4401,0,0.116503,"Missing"
W19-3510,W16-3638,0,0.0472051,"Missing"
W19-3510,L18-1443,0,0.387562,"2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sanguinetti et al., 2018), or German (Ross et al., 2016). In this work, we aim to contribute to the field of hate speech detection. Our contribution is twofold: (i) diversification of the research on hate speech by provision of a new dataset of hate speech in another language than English, namely Portuguese; (ii) introduction of a novel fine-grained hate speech typology that improves on the common state-of-the-art used typologies, which tend to disregard the existence of subtypes of hate speech and either consider hate speech recognition as a binary classification task, or take into account only a few classes, such as"
W19-3510,W17-1101,0,0.0328408,"self-image and social exclusion of the targeted individuals, groups or populations, and incites violence against them. A clear example of the extreme harm that can be caused by hate speech is the 1994 Rwandan genocide; see Schabas (2000) for a detailed analysis. The detection of online hate speech is thus a pressing problem that calls for solutions. Over the last decade, a considerable number of supervised machine learning-based works tackled the problem. Most of them focused on English (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sanguinetti et al., 2018), or German (Ross et al., 2016). In this work, we aim to contribute to the field of hate sp"
W19-3510,W12-2103,0,0.297983,"nd 30 messages while others had more than 5 We use the term “search instance” to refer to profiles, keywords or hashtags used for the Twitter search. 97 4.2 Hierarchical annotation following properties: When studying hate speech, it is possible to distinguish between different categories of it, like ‘racism’, ‘sexism’, or ‘homophobia’. A more finegrained view can be useful in hate speech classification because each category has a specific vocabulary and ways to be expressed, such that creating a language model for each category may be helpful to improve the automatic detection of hate speech (Warner and Hirschberg, 2012). Another phenomenon we can observe when analyzing different categories of hate speech is their intersectionality. This concept appeared as an answer to the historical exclusion of black women from early women’s rights movements often concerned with the struggles of white women alone. Intersectionality brings attention to the experiences of people who are subjected to multiple forms of discrimination within a society (e.g., being woman and black) (Collins, 2015). Waseem (2016) introduce a hate speech labeling scheme that follows an intersectional approach. In addition to ‘racism’, ‘sexism’, an"
W19-3510,W17-3006,0,0.0437384,"Missing"
W19-3510,W16-5618,0,0.112572,"ng a language model for each category may be helpful to improve the automatic detection of hate speech (Warner and Hirschberg, 2012). Another phenomenon we can observe when analyzing different categories of hate speech is their intersectionality. This concept appeared as an answer to the historical exclusion of black women from early women’s rights movements often concerned with the struggles of white women alone. Intersectionality brings attention to the experiences of people who are subjected to multiple forms of discrimination within a society (e.g., being woman and black) (Collins, 2015). Waseem (2016) introduce a hate speech labeling scheme that follows an intersectional approach. In addition to ‘racism’, ‘sexism’, and ‘neither’, they use the label “both” arguing that the intersection of multiple oppression categories can differ from the forms of oppression it consists of (Crenshaw, 2018). To better take into account different hate speech categories from an intersectional perspective, we approach the definition of the hate speech annotation schema in terms of a hierarchical structure of classes. • The ‘hate speech’ class corresponds to the root of the graph. • If hate speech can be divided"
W19-3510,N16-2013,0,0.435001,"ne of these observed negative phenomena is the propagation of hate speech. Hate speech leads to a negative self-image and social exclusion of the targeted individuals, groups or populations, and incites violence against them. A clear example of the extreme harm that can be caused by hate speech is the 1994 Rwandan genocide; see Schabas (2000) for a detailed analysis. The detection of online hate speech is thus a pressing problem that calls for solutions. Over the last decade, a considerable number of supervised machine learning-based works tackled the problem. Most of them focused on English (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Jigsaw, 2018), see also the overview by Schmidt and Wiegand (2017). As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g., Waseem and Hovy (2016); Davidson et al. (2017); Nobata et al. (2016); Jigsaw (2018)) than for other languages. However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g., Spanish (Fersini et al., 2018), Italian (Poletto et al., 2017; Sang"
W19-8659,P17-1017,0,0.139318,"a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Statistical generators increasingly dominate the researc"
W19-8659,W17-3518,0,0.159541,"a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Statistical generators increasingly dominate the researc"
W19-8659,W09-0613,0,0.288225,"appeared in the training data (‘Astronaut’, ‘Building’, ‘University’, etc.), i.e., were “seen”, and five categories were “unseen”, i.e., they did not appear in the training data (‘Athlete’, ‘Artist’, etc.). At the time of the challenge, the WebNLG dataset contained about 10K distinct inputs and 25K data-text pairs; a sample data-text pair is shown in Figure 1. The neural generator ADAPT (Elder et al., 2018) performed best on seen data, and FORGe on unseen data and overall. In what follows, we aim to improve the performance of FORGe on seen data for English and furthermore port it to Spanish. (Gatt and Reiter, 2009) demonstrated that a welldefined generation infrastructure, along with a transparent, easy to handle rule and structure format, is a key for its take up and use for creation of generation modules for multiple languages. In what follows, we aim to demonstrate that the FORGe generator can also well serve as a multilingual portable text generator for verbalization of structured data and that its lexical and grammatical resources can be easily extended to reach a higher coverage of linguistic constructions. For this, we extend its publicly available resources for English, so as to improve the qual"
W19-8659,W11-2832,0,0.0259419,"lability of large scale syntactically annotated corpora and the lack of publicly available knowledge repositories, the focus had shifted to statistical surface generation. However, thanks to Semantic Web (SW) initiatives such as the W3C Linking Open Data Project,1 1 https://www.w3.org/wiki/SweoIG/ TaskForces/CommunityProjects/ LinkingOpenData 473 Proceedings of The 12th International Conference on Natural Language Generation, pages 473–483, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ing statistically the most appropriate output (Gardent et al., 2017b; Belz et al., 2011). Templatebased systems are very robust, but also limited in terms of portability since new templates need to be defined for every new domain, style, language, etc. Statistical systems have the best coverage, but the relevance and the quality of the produced texts cannot be ensured. Furthermore, they are fully dependent on the available (still scarce and mostly monolingual) training data. The development of grammar-based systems is time-consuming and they usually have coverage issues. However, they do not require training material, allow for a greater control over the outputs (e.g. for mitigat"
W19-8659,kingsbury-palmer-2002-treebank,0,0.511563,"nality and number information labels are also assigned. Last, in the case of multiple triple inputs, the triples are ordered (as a preliminary step for the subsequent aggregation) based on the number of appearances of their subjects and on whether a subject of a triple serves also as an object in another triple. For the population of the templates of Figure 2, the subject and object placeholders are simply replaced by the corresponding subjects and objects of Figure 1, without cleaning or further modification. Mapping properties to PredArg templates Predicate-argument templates in a PropBank (Kingsbury and Palmer, 2002; Babko-Malaya, 2005) fashion were defined taking into account the property as well as the type of the subject and object values.2 Thus, each of the properties found in the evaluation triples was associated to one of these templates. Parts of speech (e.g., NP –proper noun), grammatical features (e.g., verbal tense or nominal definiteness), or information from DBpedia (e.g., classes), for instance, can be specified in the template.3 Figure 2 shows sample PredArg templates for the DBpedia properties leader and language respectively;4 318 templates were used for the 373 properties of WebNLG. 3.2"
W19-8659,bohnet-wanner-2010-open,1,0.704242,"organization of the linguistic resources can be an adequate choice for NLG applications. 1 Introduction One of the rule-based generators presented at WebNLG was FORGe (Mille and Dasiopoulou, 2017), which ranked first with respect to overall quality in the human evaluation. FORGe is grounded in the linguistic model of the MeaningText Theory (Mel’ˇcuk, 1988). The multistratal nature of this model allows for a modular organization of blocks of graph-transduction rules, from blocks that are universal, i.e., multilingual, to blocks that are language-specific. The graphtransduction framework MATE (Bohnet and Wanner, 2010) furthermore facilitates a systematic hierarchical rule writing and testing. SimpleNLG The origins of Natural Language Generation (NLG) are in rule-based sentence/text generation from numerical data or deep semantic structures. With the availability of large scale syntactically annotated corpora and the lack of publicly available knowledge repositories, the focus had shifted to statistical surface generation. However, thanks to Semantic Web (SW) initiatives such as the W3C Linking Open Data Project,1 1 https://www.w3.org/wiki/SweoIG/ TaskForces/CommunityProjects/ LinkingOpenData 473 Proceeding"
W19-8659,L18-1478,0,0.0150211,"twerp and Belgium in Figure 3, which are merged at the end of the process, c.f. Figure 4. During linguistic generation, this results in the introduction of postnominal modifiers such as relative and participial clauses or appositions (see next section). In order to avoid the formation of heavy nominal groups, at most one aggregation is allowed per argument. Figure 5: Deep-syntactic structures gorisation lexicon. For this purpose, lexical resources derived from PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) or VerbNet (Schuler, 2005) are used; see (Mille and Wanner, 2015; Lareau et al., 2018). Personal and relative pronouns are introduced using the coreference relations (dotted arrows) and the class feature, which allows for distinguishing between human and non-human antecedents. Finally, morpho-syntactic agreements are resolved, the syntactic tree is linearized through the ordering of (i) governor/dependent and (ii) dependents with each other, and the surface forms are retrieved. Post-processing rules are then applied: upper casing, replacement of underscores by spaces, etc. Figure 4: Aggregated PredArg structures 3.4 Linguistic generation The next and last step is the rendering"
W19-8659,W14-4412,0,0.0557466,"Missing"
W19-8659,W16-6630,0,0.0154321,"lts of the automatic evaluation of the extended system, and Section 6 a qualitative evaluation of the outputs in both languages. Section 7, finally, draws some conclusions and presents the future work. 2 Related work The most prominent recent illustration of the portability of a generation framework is SimpleNLG. Originally developed for generation of English in practical applications (Gatt and Reiter, 2009), in the meantime it has been ported to generate, among others, in Brasilian Portuguese (De Oliveira and Sripada, 2014), Dutch (de Jong and Theune, 2018), German (Bollmann, 2011), Italian (Mazzei et al., 2016), and Spanish (Soto et al., 2017). However, while SimpleNLG is a framework for surface generation, usually with a limited coverage, we are interested in a portable multilingual framework for large scale text generation from structured data, more precisely, from DBpedia properties (Lehmann et al., 2015). Although most existing NLG generators combine different techniques, there are three main approaches to generating texts from an input sequence of structured data (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018): (i) filling slot values in predefined sentence templates (Androutsopoulos et al."
W19-8659,W18-6556,0,0.0202,"systems of types (ii) and (iii) have been presented. The task consisted in generating texts from up to 7 DBpedia triples from 15 categories, covering in total 373 distinct DBpedia properties. Nine categories appeared in the training data (‘Astronaut’, ‘Building’, ‘University’, etc.), i.e., were “seen”, and five categories were “unseen”, i.e., they did not appear in the training data (‘Athlete’, ‘Artist’, etc.). At the time of the challenge, the WebNLG dataset contained about 10K distinct inputs and 25K data-text pairs; a sample data-text pair is shown in Figure 1. The neural generator ADAPT (Elder et al., 2018) performed best on seen data, and FORGe on unseen data and overall. In what follows, we aim to improve the performance of FORGe on seen data for English and furthermore port it to Spanish. (Gatt and Reiter, 2009) demonstrated that a welldefined generation infrastructure, along with a transparent, easy to handle rule and structure format, is a key for its take up and use for creation of generation modules for multiple languages. In what follows, we aim to demonstrate that the FORGe generator can also well serve as a multilingual portable text generator for verbalization of structured data and t"
W19-8659,W04-2705,0,0.053654,"dentified, the PredArg structures are merged by fusing the common argument; see e.g. Antwerp and Belgium in Figure 3, which are merged at the end of the process, c.f. Figure 4. During linguistic generation, this results in the introduction of postnominal modifiers such as relative and participial clauses or appositions (see next section). In order to avoid the formation of heavy nominal groups, at most one aggregation is allowed per argument. Figure 5: Deep-syntactic structures gorisation lexicon. For this purpose, lexical resources derived from PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) or VerbNet (Schuler, 2005) are used; see (Mille and Wanner, 2015; Lareau et al., 2018). Personal and relative pronouns are introduced using the coreference relations (dotted arrows) and the class feature, which allows for distinguishing between human and non-human antecedents. Finally, morpho-syntactic agreements are resolved, the syntactic tree is linearized through the ordering of (i) governor/dependent and (ii) dependents with each other, and the surface forms are retrieved. Post-processing rules are then applied: upper casing, replacement of underscores by spaces, etc. Figure 4: Aggregate"
W19-8659,W16-3500,0,0.138673,"Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract a tremendous amount of structured knowledge has been made publicly available as languageindependent triples; the Linked Open Data (LOD) cloud currently contains over one thousand interlinked datasets (e.g., DBpedia, Wikidata), which cover a large range of domains and amount to billions of different triples. The verbalization of LOD triples, i.e., their mapping onto sentences in natural languages, has been attracting a growing interest in the past years, as shown by the organization of dedicated events such as the WebNLG 2016 workshop (Gardent and Gangemi, 2016) and the 2017 WebNLG challenge (Gardent et al., 2017b). As a result, a variety of new NLG systems designed specifically for handling structured data have emerged, most of them statistical, as seen in the 2017 WebNLG challenge, although a number of rule-based generators have also been presented. All systems focus on English, mainly because no training data other than for English are available as yet. Given the high cost for the creation of training data, this state of affairs is likely to persist for some time. Therefore, the question on the competitiveness of rule-based generators arises. Stat"
W19-8659,W13-3724,1,0.848275,"noun and get morphological agreement features from it (third person singular), while NMOD towards a preposi5 Note that the node brought together during the previous step are not necessarily split up at this level. 476 tion causes the opposite order and no agreement, etc.: Charles Michel3sg &gt; is3sg &gt; the &gt; leader &gt;of &gt;Belgium. The final sentence generated for the four triples is The Antwerp International Airport serves Antwerp, which is in Belgium. Charles Michel is the leader of Belgium, in which the German language is spoken. 4 For designing the rules, we followed the approach of AnCora-UPF (Mille et al., 2013), a Spanish dataset in which each dependency relation is associated with a set of syntactic properties. For instance, a subject is characterized by being linearized to the left of its governing verb (by default), by being removable, by triggering the number and person agreements on the verb, etc. During the linguistic generation stage, 27 out of the 47 relations proposed in AnCora-UPF 8 are currently supported. In order to generalize the ordering rules across languages, the dependencies were introduced in the lexicon with details about how they are linearized with respect to their governor (ve"
W19-8659,P02-1040,0,0.104053,"structions containing all and only the entities and relations in the triples. The reference texts were written by one of the authors, a native Spanish speaker, having at hand the English references from the WebNLG challenge to serve as a potential model. Evaluation 5.3 In this section, we detail how we built a new dataset for evaluating the outputs, and describe the results of the automatic evaluations. 5.1 Reference sentences Automatic evaluation The predicted outputs in English and Spanish were compared to the reference sentences in the corresponding language; three metrics were used: BLEU (Papineni et al., 2002), which matches exact words, METEOR (Banerjee and Lavie, 2005), which matches also synonyms, and TER (Snover et al., 2006), which reflects the amount of edits needed to transform the predicted output into the reference output. Table 1 shows the results of the automatic evaluation on the English and Spanish Selection of triples for evaluation For evaluation purposes, we compiled a benchmark dataset of 200 inputs, i.e., sets of DBpe9 Note that we exclude from the count all rules than simply transfer individual attributes at each level, which amount to about 250. There are more English-specific r"
W19-8659,C16-1141,0,0.0239632,"Deep-Syntactic graphs, apply for both languages. When getting closer to the surface, the rules are less languageindependent, representing about half of the DSyntSSynt rules (108/239) and of the linearization and agreement resolution rules (66/129). 5 dia triples, with sizes ranging from 1 to 7 triples, using as reference pool the WebNLG challenge test set. The reason for using as reference basis the WebNLG challenge dataset is that it is the most recent and comprehensive dataset with respect to text generation from RDF data that has been specifically designed to promote data and text variety (Perez-Beltrachini et al., 2016). Moreover, it allows the direct comparison with the generators that participated in the challenge. In order to ensure future comparisons with machine learning-based systems in terms of their best obtained performance, only the seen categories subset of the original test set has been considered, i.e., only inputs with entities that belonged to DBpedia categories that were contained in the training data. The compilation methodology for our benchmark dataset implements a twofold goal. On one hand, we want to ensure that all properties appearing in the seen categories subset are included. On the"
W19-8659,2006.amta-papers.25,0,0.0889691,"authors, a native Spanish speaker, having at hand the English references from the WebNLG challenge to serve as a potential model. Evaluation 5.3 In this section, we detail how we built a new dataset for evaluating the outputs, and describe the results of the automatic evaluations. 5.1 Reference sentences Automatic evaluation The predicted outputs in English and Spanish were compared to the reference sentences in the corresponding language; three metrics were used: BLEU (Papineni et al., 2002), which matches exact words, METEOR (Banerjee and Lavie, 2005), which matches also synonyms, and TER (Snover et al., 2006), which reflects the amount of edits needed to transform the predicted output into the reference output. Table 1 shows the results of the automatic evaluation on the English and Spanish Selection of triples for evaluation For evaluation purposes, we compiled a benchmark dataset of 200 inputs, i.e., sets of DBpe9 Note that we exclude from the count all rules than simply transfer individual attributes at each level, which amount to about 250. There are more English-specific rules simply because the coverage of the English generator is higher. 479 extensions proposed in this paper using for each"
W19-8659,W17-3521,0,0.0221256,"the extended system, and Section 6 a qualitative evaluation of the outputs in both languages. Section 7, finally, draws some conclusions and presents the future work. 2 Related work The most prominent recent illustration of the portability of a generation framework is SimpleNLG. Originally developed for generation of English in practical applications (Gatt and Reiter, 2009), in the meantime it has been ported to generate, among others, in Brasilian Portuguese (De Oliveira and Sripada, 2014), Dutch (de Jong and Theune, 2018), German (Bollmann, 2011), Italian (Mazzei et al., 2016), and Spanish (Soto et al., 2017). However, while SimpleNLG is a framework for surface generation, usually with a limited coverage, we are interested in a portable multilingual framework for large scale text generation from structured data, more precisely, from DBpedia properties (Lehmann et al., 2015). Although most existing NLG generators combine different techniques, there are three main approaches to generating texts from an input sequence of structured data (Bouayad-Agha et al., 2014; Gatt and Krahmer, 2018): (i) filling slot values in predefined sentence templates (Androutsopoulos et al., 2013), (ii) applying grammars ("
W90-0105,C88-2100,0,0.130093,"Missing"
W90-0105,J87-3007,0,0.039809,"Missing"
W90-0105,C88-1063,0,0.0607267,"Missing"
W90-0105,H89-1022,0,0.0219914,"Missing"
W90-0105,W89-0201,0,0.025161,"Missing"
W90-0105,E83-1027,0,0.0419759,"Missing"
W90-0105,J87-3006,0,0.0425847,"Missing"
W94-0316,J87-3006,0,0.341931,"Missing"
W94-0316,E87-1001,0,0.438922,"Missing"
W94-0316,C92-2096,0,0.195097,"Missing"
W94-0316,C94-1060,1,0.785768,"Missing"
W96-0401,W90-0108,0,0.0476712,"Missing"
W96-0401,W96-0508,1,0.869369,"Missing"
W96-0401,A92-1006,0,0.0776024,"Missing"
W96-0401,T78-1009,0,0.345926,"Missing"
W96-0401,W94-0315,0,0.165643,"Missing"
W96-0401,W96-0415,0,0.0597035,"Missing"
W96-0401,C92-2114,0,0.0534274,"Missing"
W96-0401,J95-1002,0,0.206089,"Missing"
W96-0401,W90-0105,1,0.854162,"Missing"
W96-0401,W94-0316,1,0.877546,"Missing"
W96-0401,J93-3002,0,\N,Missing
W96-0401,J78-3015,0,\N,Missing
W98-1406,J97-2001,0,0.19888,"Missing"
W98-1406,W96-0403,0,0.019266,"Missing"
W98-1406,1996.amta-1.10,1,0.774247,"Missing"
W98-1406,W94-0315,0,0.0593217,"Missing"
W98-1406,P96-1005,1,0.871835,"Missing"
W98-1406,W96-0401,1,0.844947,"Missing"
W98-1406,A92-1006,0,\N,Missing
wanner-etal-2004-enriching,magnini-cavaglia-2000-integrating,0,\N,Missing
