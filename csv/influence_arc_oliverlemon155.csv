2005.sigdial-1.11,P05-1030,1,0.734055,"Missing"
2005.sigdial-1.11,C00-1073,0,0.0435766,"Missing"
2005.sigdial-1.11,P01-1066,0,\N,Missing
2005.sigdial-1.11,2005.sigdial-1.6,0,\N,Missing
2007.sigdial-1.11,P01-1066,0,0.0451117,"an robustness of the learned dialogue policies. We will focus on one type of stochastic user simulation but different types of 55 Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 55–58, c Antwerp, September 2007. 2007 Association for Computational Linguistics users and on different environmental conditions. (Frampton and Lemon, 2006) train a policy for 4-gram stochastic user simulation and test it on a 5gram simulation, and vice-versa, showing that the learned policy works well for the 2 different simulations. However, these simulations are trained on the same dataset (Walker et al., 2001) and thus do not simulate different types of user or noise conditions. Similarly (Henderson et al., 2005) test and train on different segments of the COMMUNICATOR data, so the results presented there do not deal with the issue of policy transfer. (Lemon et al., 2006) show that a single policy trained on a human-machine dialogue corpus also performs well with real users of a dialogue system. 2 The experimental set-up We experiment with a 3-slot information-seeking system, resulting in 8 binary state variables (1 for whether each slot is filled, 1 for whether each slot is confirmed, 2 for whethe"
2007.sigdial-1.11,P06-1024,1,\N,Missing
2020.acl-main.682,D16-1203,0,0.0262625,"e that is mapped to a scene graph composed of objects represented in terms of abstract and situated attributes. Introduction Several grounded language learning tasks have been proposed to capture perceptual aspects of language (Shekhar et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019; Agrawal et al., 2018). However, the advances in this field have been primarily driven by the final performance measures and less on the grounding capability of the models. In fact, in some cases, high-performance models exploit dataset biases to achieve high scores on the final task (Zhang et al., 2016; Agrawal et al., 2016). In the literature, several methods have been proposed to analyse what kind of information is captured by neural network representations (K´ad´ar et al., 2017; Belinkov and Glass, 2019). Most of these works examine the hidden state representations learned by models trained on only textual data. However, many aspects of human semantic representations are grounded in perceptual experience (Andrews et al., 2009; Riordan and Jones, 2011). This paper explores the idea that visually grounded representations ought to be a result of systematic composition of grounded representations (Harnad, 1990). F"
2020.acl-main.682,J17-4003,0,0.0687773,"Missing"
2020.acl-main.682,W18-5446,0,0.0639334,"Missing"
2020.coling-main.95,I17-1014,0,0.0561196,"Missing"
2020.coling-main.95,D15-1293,0,0.0180715,"the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning to ground pixels of the scene into object representations that are relevant for the object ca"
2020.coling-main.95,P15-2038,0,0.0179683,"at” (e.g., what a boat looks like, the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning to ground pixels of the scene into object representation"
2020.coling-main.95,N18-1038,0,0.103248,"their memory and are associated with the concept of “boat” (e.g., what a boat looks like, the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning"
2020.coling-main.95,P18-1085,0,0.0146813,"zi ))), (2) where η is the minimum margin between two components: i) the distance between the perceptual embedding vi and its reconstruction Dθ (zi ), and ii) the distance between the perceptual embedding vj of a randomly sampled object oj ∈ O¬ci and the reconstruction Dθ (zi ). By doing so, we enforce each object representation to be representative of its category given a specific context by locally contrasting it to another object of a different category in the same scene. Note that this is strikingly different from previous approaches employing a max-margin loss (Elliott and K´ad´ar, 2017; Kiros et al., 2018) where “negative” objects are arbitrarily sampled from other scenes in the same batch. Imagining at inference time. Differently from the category embeddings c employed by all previous work, our imagination embeddings z do not depend on gold category labels at inference time, while still being context-aware and category-aware. In fact, once parameters φ have been learned, the encoder Eφ contains all the information needed to distill embeddings z independently of LIMG , which is necessary only at training time. We consider imagination the ability of the model of generating latent representations"
2020.coling-main.95,N15-1016,0,0.0447796,"Missing"
2020.coling-main.95,N19-1265,0,0.155831,"Missing"
2020.coling-main.95,P19-1646,0,0.133912,"ich the game is played (contextaware). As the scene S is an image, it is natural to associate each object oi ∈ O with a perceptual embedding, i.e., a vector vi ∈ RdO extracted from the penultimate layer of a pretrained vision model (e.g. ResNet-152 (Shekhar et al., 2019)) based on their bounding box.1 However, these representations are not sufficient as they are neither context-aware nor category-aware, i.e., they ignore other objects in the scene and do not leverage their category information. GDSE and other recent approaches (De Vries et al., 2017; Shekhar et al., 2019; Zhuang et al., 2018; Shukla et al., 2019) coped with the second issue by introducing category embeddings as dC -dimensional continuous representations ck ∈ RdC for k = 1, . . . , K. Once learned, a category embedding c is then concatenated to an 8-dimensional feature vector si derived from the object bounding box (cf. De Vries et al. (2017)). While these embeddings partially solve category-awareness, they are not object-aware. For instance, the embedding for the object category “apple” will be the same regardless of a particular object to be a red or green apple, i.e., most likely a centroid representation of the objects seen only du"
2020.coling-main.95,2020.acl-main.682,1,0.869596,"e Bastianelli1 , Andrea Vanzo1 , and Oliver Lemon1 1 Heriot-Watt University, Edinburgh, UK 2 University of California, Los Angeles, USA 3 Carnegie Mellon University, Pittsburgh, USA 1 {as247,i.konstas,a.vanzo,e.bastianelli,o.lemon}@hw.ac.uk 2 aver@cs.ucla.edu, 3 ybisk@cs.cmu.edu Abstract In visual guessing games, a Guesser has to identify a target object in a scene by asking questions to an Oracle. An effective strategy for the players is to learn conceptual representations of objects that are both discriminative and expressive enough to ask questions and guess correctly. However, as shown by Suglia et al. (2020), existing models fail to learn truly multi-modal representations, relying instead on gold category labels for objects in the scene both at training and inference time. This provides an unnatural performance advantage when categories at inference time match those at training time, and it causes models to fail in more realistic “zeroshot” scenarios where out-of-domain object categories are involved. To overcome this issue, we introduce a novel “imagination” module based on Regularized Auto-Encoders, that learns context-aware and category-aware latent embeddings without relying on category label"
2020.sigdial-1.5,W18-5701,1,0.890513,"Missing"
2021.eacl-main.183,2020.emnlp-main.703,1,0.834536,"d 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL. 1 Background & Related Work Learning a language requires interacting with both the environment and other agents (Bisk et al., 2020). Language games represent one common example of this (Wittgenstein et al., 1953), as seen by the important role of play in L1 child language acquisition (Hainey et al., 2016) as well as L2 learners (Godwin-Jones, 2014). Among the language games defined in the literature (Steels, 2015), guessing games represent the first step in a curriculum for language learning. For example, in GuessWhat?! (de Vries et al., 2017), two agents interact with each other: a Questioner generates questions aimed at finding a hidden object in the scene and an Oracle, aware of the target object, answers the questions"
2021.eacl-main.183,P18-1238,0,0.0243165,"Missing"
2021.eacl-main.183,C18-1104,0,0.0518742,"Missing"
2021.eacl-main.183,N19-1265,0,0.276696,"Missing"
2021.reinact-1.5,W17-5522,0,0.0280882,"al., 2020). The joint modelling of the entity extraction and intent classification sub-tasks has been shown to improve performance, indicating that intent and entities closely interact with each other. Beyond improved accuracy, this model is faster to train than fine-tuning BERT for NLU. Aye-saac relies on Rasa’s implementation of the DIET classifier to achieve state-of-the-art NLU results. Rasa is a set of open-source libraries that can be used to create conversational agents (Bocklisch et al., 2017), and perform on par with paid NLU system, such as Microsoft’s Language Understanding (LUIS) (Braun et al., 2017). Rasa offers the typical advantages of self-hosted open-source software such as adaptability and data control. This privacy is particularly important when interacting with visually impaired people in their homes. 3 Aye-saac Aye-saac is a modular and extensible conversational VQA framework that is implemented as a collection of independent microservices. Isolating each service allows Aye-saac to utilise concurrency when analysing image data; ensuring more intensive operations do not hinder the system from responding to other requests. Figure 2 shows the flow of data between all the individual"
2021.reinact-1.5,D18-1164,0,0.0276097,"ut object positioning by returning more descriptive and usable positioning information. 2 2.1 Related Work Visual Question Answering A VQA system takes a natural language question and an image as input, aiming to reason over the contents and respond with a natural language utterance (Antol et al., 2015). Recently, VQA systems that achieve state-of-the-art results have been trained E2E, an example is the Pythia system that won the 2018 VQA competition (Jiang et al., 2018). While these systems provide improved performance, they lack the ability to explain why they generated a specific response (Li et al., 2018) and have to be trained on large datasets. A crowd-sourcing approach can provide VQA systems with the required training data to cover many subjects and handle common questions (Gurari et al., 2018), but the objects or properties in an image must be within this data to be mapped correctly between the query and image (Antol et al., 2015). As an example, consider Figure 1: if the model was not trained to recognise bananas, the system is unable to correctly handle queries related to bananas. This is a challenge when working within a very specific domain. The VizWiz Social (Brady et al., 2013) app"
2021.reinact-1.5,2020.splu-1.5,0,0.0847026,"Missing"
2021.reinact-1.5,2020.acl-main.682,1,0.716327,"elationships Current object detection systems perform well at detecting entities, but are not robust at inferring the spatial relationships between them (Krishna et al., 2017). This weakness stems from the available datasets, as they lack relative spatial positioning information and must implicitly infer them. To address this limitation, Krishna et al. (2017) introduced the Visual Genome (VG) dataset, converting natural language descriptions of images to dense scene graphs that include spatial relationships and common descriptive attributes of entities. Datasets such as CompGuessWhat?! (CGW) (Suglia et al., 2020) and GQA (Hudson and Manning, 2019) extend object detection datasets by including dense scene graphs that contain additional situational and abstract attributes, and further include binary question-answer pairs grounded on the context of the scene (Suglia et al., 2020; Hudson and Manning, 2019). However, models trained on these datasets are not as robust with zero-shot evaluation — where models attempt to reason about visual scenes with previously unseen entities (Suglia et al., 2020), which can be dangerous in practice for visually impaired people who need to rely on the response. 33 2.3 Exis"
C08-3005,E06-2009,1,0.807393,"rmine what taskbased steps to take next, such as asking for a cinema name. General aspects of dialogue, such as confirmation and clarification strategies, are handled by the domain-general DM. Values for constraints on transitions and branching in the BPM, for example “present insurance option if the user is business-class”, are compiled into domain-specific parts of the DM’s update rules. XML format is used for BPMs, and they are compiled into finite state machines consulted by the spoken dialogue system through the BPM module. The domaingeneral DM was mostly abstracted from the TALK system (Lemon et al., 2006). 2.2 Compiling Grammars for Business User Resources and Databases Figure 2: Part of an example Business Process Model for searching for Hotels The resulting spoken dialogue system deploys the following main modules: • Speech Recogniser module, e.g. ATK/HTK (Young, 2007; Young, 1995) or Nuance (Nuance, 2002) For Spoken Language Understanding, ADT currently uses Grammatical Framework (GF) (Ranta, 2004) which is a language for writing multilingual grammars, on top of which various applications such as machine translation and human-machine interaction have been built. A GF grammar not only define"
D12-1008,W10-4342,0,0.0390158,"Missing"
D12-1008,W11-2814,1,0.887242,"Missing"
D12-1008,W12-1509,1,0.78501,"here Qij (s, a) specifies the expected cumulative reward for executing action a in state s and then following π ∗ . We use HSMQ-Learning to induce dialogue policies, see (Cuay´ahuitl, 2009), p. 92. 5 Experimental Setting 5.1 Hierarchy of Learning Agents The HRL agent in Figure 3 shows how the tasks of (1) dealing with incrementally changing input hypotheses, (2) choosing a suitable IP strategy and (3) presenting information, are connected. Note that 1 we focus on a detailed description of models M0...3 here, which deal with barge-ins and backchannels and are the core of this paper. Please see Dethlefs et al. (2012) for details of an RL model that deals with the remaining decisions. Briefly, model M00 deals with dynamic input hypotheses. It chooses when to listen to an incoming user utterance (M31 ) and when and how to present 1 ) by calling and passing control information (M0...2 to a child subtask. The variable ‘incrementalStatus’ characterises situations in which a particular (incremental) action is triggered, such as a floor holder ‘let me see’, a correction or self-correction. The variable ‘presStrategy’ indicates whether a strategy for IP has been chosen or not, and the variable ‘userReaction’ show"
D12-1008,W09-3902,0,0.0268795,"crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context"
D12-1008,P02-1026,0,0.0136262,"that for an utterance u consisting of the word sequence w1 . . . wi−1 , we can compute the ID at each point during the utterance as: Information Theory n Information Theory as introduced by Shannon (1948) is based on two main concepts: a communication channel through which information is transferred in bits and the information gain, i.e. the information load that each bit carries. For natural language, the assumption is that people aim to com84 log X 1 1 log = P (u) P (wi |w1 . . . wi−1 ) (1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel and Charniak (2002) in restricting our computation to tri-grams for computability reasons. Given a I want Italian food in the city centre. Yes, I need a moderately priced restaurant in the New Chesterton area. I need the address of a Thai restaurant. 20 Information Density language model of the domain, we can therefore optimise ID in system-generated discourse, where we treat ID as “an optimal solution to the problem of rapid yet error-free communication in a noisy environment” (Levy and Jaeger (2007), p.2). We will now transfer the notion of ID to IP and investigate the distribution of information over user res"
D12-1008,J08-4002,1,0.70664,"ser utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incrementa"
D12-1008,P10-1008,1,0.72985,"d accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output plan"
D12-1008,W03-2311,0,0.103331,"oretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93"
D12-1008,W11-2706,0,0.0354197,"ins based on a partially data-driven reward function. Generating backchannels can be beneficial for grounding in interaction. Similarly, barge-ins can lead to more efficient interactions, e.g. when a system can clarify a bad recognition result immediately before acting based on a misrecognition. A central concept to our approach is Information Density (ID) (Jaeger, 2010), a psycholinguistic hypothesis that human utterance production is sensitive to a uniform distribution of information across the utterance. This hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example domain of application is the Information Presentation phase in an interactive system for restaurant recommendations, extending previous work by Rieser et al. (2010). This previous work incrementally constructs IP strategies according to the predicted user"
D12-1008,N09-1071,0,0.0304254,"architecture offers inherently incremental mechanisms to update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation"
D12-1008,P10-1103,1,0.93045,"ech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output planning that is based on"
D12-1008,W11-2014,0,0.0143645,"they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et"
D12-1008,W10-4301,0,0.322476,"Missing"
D12-1008,W11-2813,1,\N,Missing
D17-1236,W15-0130,1,0.935431,"rcement Learning (RL) of word-level actions for system output (i.e. a combined incremental DM and NLG module for the resulting dialogue system). 3.1 Dynamic Syntax and Type Theory with Records (DS-TTR) Dynamic Syntax (DS) is an action-based, wordby-word incremental and semantic grammar formalism (Kempson et al., 2001; Cann et al., 2005), especially suited to the highly fragmentary and context-dependent nature of dialogue. In DS, words are conditional actions - semantic updates; and dialogue is modelled as the interactive and incremental construction of contextual and semantic representations (Eshghi et al., 2015) - see Fig. 2. The contextual representations aﬀorded by DS are of the fine-grained semantic content that is jointly negotiated/agreed upon by the interlocutors, as a result of processing questions and answers, clarification interaction, acceptances, self/other-corrections, restarts, and other characteristic incremental phenomena in dialogue - see 3 for a sketch of how self-corrections and restarts are processed via a backtrack and search mechanism over the parse search graph (see Hough (2011); Hough and Purver (2014); Eshghi et al. (2015) for details of the model, and how this parse search gr"
D17-1236,W13-2611,1,0.8844,"ation is the key mechanism used in our system below for feature checking. jointly built and GROUNDED by the interlocutors (loosely following the DGB model of (Ginzburg, 2012)). 3. Define the MDP action set as the DS lexicon L (i.e. actions are words); 3.2 Overall Method: babble In this section we describe our method for combining incremental dialogue parsing with Reinforcement Learning for Dialogue Management (DM) and Natural Language Generation (NLG) where these are treated as a joint decision/optimisation problem. We start with two resources: a) a DS-TTR parser DS (either learned from data (Eshghi et al., 2013a), or constructed by hand), for incremental language processing, but also, more generally, for tracking the context of the dialogue using Eshghi et al.’s model of feedback (Eshghi et al., 2015; Eshghi, 2015; Eshghi et al., 2011); b) a set D of transcribed successful dialogues in the target domain. We perform the following steps overall to induce a fully incremental dialogue system from D: 1. Automatically induce the MDP state space, S , and the dialogue goal, G D , from D; 2. Automatically define the state encoding function F : C → S ; where s ∈ S is a (binary) state vector, designed to extra"
D17-1236,W13-0110,1,0.911246,"ation is the key mechanism used in our system below for feature checking. jointly built and GROUNDED by the interlocutors (loosely following the DGB model of (Ginzburg, 2012)). 3. Define the MDP action set as the DS lexicon L (i.e. actions are words); 3.2 Overall Method: babble In this section we describe our method for combining incremental dialogue parsing with Reinforcement Learning for Dialogue Management (DM) and Natural Language Generation (NLG) where these are treated as a joint decision/optimisation problem. We start with two resources: a) a DS-TTR parser DS (either learned from data (Eshghi et al., 2013a), or constructed by hand), for incremental language processing, but also, more generally, for tracking the context of the dialogue using Eshghi et al.’s model of feedback (Eshghi et al., 2015; Eshghi, 2015; Eshghi et al., 2011); b) a set D of transcribed successful dialogues in the target domain. We perform the following steps overall to induce a fully incremental dialogue system from D: 1. Automatically induce the MDP state space, S , and the dialogue goal, G D , from D; 2. Automatically define the state encoding function F : C → S ; where s ∈ S is a (binary) state vector, designed to extra"
D17-1236,R11-2012,0,0.0186392,"d as the interactive and incremental construction of contextual and semantic representations (Eshghi et al., 2015) - see Fig. 2. The contextual representations aﬀorded by DS are of the fine-grained semantic content that is jointly negotiated/agreed upon by the interlocutors, as a result of processing questions and answers, clarification interaction, acceptances, self/other-corrections, restarts, and other characteristic incremental phenomena in dialogue - see 3 for a sketch of how self-corrections and restarts are processed via a backtrack and search mechanism over the parse search graph (see Hough (2011); Hough and Purver (2014); Eshghi et al. (2015) for details of the model, and how this parse search graph is eﬀectively the context of the conversation). Generation/linearisation in DS is defined using trialand-error parsing (see Section 3.2, with the provision of a generation goal, viz. the semantics of the utterance to be generated. Generation thus proceeds, just as with parsing, on a word-by-word basis (see Purver et al. (2014); Hough (2015) for details). The upshot of this is that using DS, we can not only track the semantic content of some current turn as it is being constructed (parsed o"
D17-1236,W14-1410,0,0.127138,"active and incremental construction of contextual and semantic representations (Eshghi et al., 2015) - see Fig. 2. The contextual representations aﬀorded by DS are of the fine-grained semantic content that is jointly negotiated/agreed upon by the interlocutors, as a result of processing questions and answers, clarification interaction, acceptances, self/other-corrections, restarts, and other characteristic incremental phenomena in dialogue - see 3 for a sketch of how self-corrections and restarts are processed via a backtrack and search mechanism over the parse search graph (see Hough (2011); Hough and Purver (2014); Eshghi et al. (2015) for details of the model, and how this parse search graph is eﬀectively the context of the conversation). Generation/linearisation in DS is defined using trialand-error parsing (see Section 3.2, with the provision of a generation goal, viz. the semantics of the utterance to be generated. Generation thus proceeds, just as with parsing, on a word-by-word basis (see Purver et al. (2014); Hough (2015) for details). The upshot of this is that using DS, we can not only track the semantic content of some current turn as it is being constructed (parsed or generated) word-by-word"
D17-1236,D13-1161,0,0.0296377,"domain. Arguably, a good computational model of dialogue processing, and interactional dynamics should be able to capture this synonymy. Lexical synonymy relations, on the other hand, hold among utterances, or dialogues, when diﬀerent words (or sequences of words) express meanings that are suﬃciently similar in a particular domain. What is striking about lexical synonymy relations is that unlike syntactic/interactional ones, they can often break down when one moves to another domain: lexical synonymy relations are domain specific. Eshghi & Lemon (2014) developed a method similar in spirit to Kwiatkowski et al. (2013) for capturing lexical synonymy relations by creating clusters of semantic representations based on observations that they give rise to similar or identical extra-linguistic actions observed within a domain (e.g. a data-base query, a flight booking, or any API call). Distributional methods could also be used for this purpose (see e.g. Lewis & Steedman (2013)). In general, this kind of clustering is achieved when the domain-general semantics resulting from semantic parsing is grounded in a particular domain. We note that while interactional synonymy relations in dialogue should be accounted for"
D17-1236,W11-0144,1,0.876417,"Missing"
D17-1236,W10-4301,0,0.217405,"Missing"
D17-1236,N15-1020,0,0.0337655,"nd of clustering is achieved when the domain-general semantics resulting from semantic parsing is grounded in a particular domain. We note that while interactional synonymy relations in dialogue should be accounted for by semantic grammars or formal models of dialogue structure (such as DS-TTR (Eshghi et al., 2012), or KoS (Ginzburg, 2012)), lexical synonymy relations have to be learned from data. 2 Why a grammar-based approach? Recent end-to-end data-driven machine learning approaches treat dialogue as a sequence-tosequence generation problem, and train their models from large datasets e.g. (Sordoni et al., 2015; Wen et al., 2016b,a; Vinyals and Le, 2015). The systems resulting from these types of approach are in principle able to handle variations/patterns that they have encountered (suﬃciently often) in the training data, but not beyond. This large-data constraint is problematic for developers but is also strange when we consider the structural knowledge that we have about language and dialogue that can be encoded in grammars and computational models of interaction. Indeed, it is often stated that for humans to learn how to perform adequately in a domain, one example is enough from which to learn ("
D17-1236,N16-1015,0,0.0631523,"Missing"
D17-1236,Q13-1015,0,\N,Missing
D19-1183,K16-1002,0,0.0560151,"Missing"
D19-1183,chang-manning-2012-sutime,0,0.0218889,"Missing"
D19-1183,D14-1179,0,0.00856343,"Missing"
D19-1183,W17-5506,0,0.139803,"dialogue context, and F e and F d are respectively hierarchical encoder and decoder. We work with goal-oriented dialogues, so it’s natural in our setting to take into account an underlying Knowledge Base (or API) providing results on the user’s queries. Given that such KB information may contain unseen token sequences for the most part, especially in the target domain, we use a copy mechanism in order to be able to use this information in the system’s responses. More specifically, we represent KB info as token sequences and concatenate it to the dialogue context similarly to CopyNet setup of Eric et al. (2017). Our copy mechanism’s implementation is the PointerSentinel Mixture Model (Merity et al., 2017; Zhao and Esk´enazi, 2018): p(wt |st ) = gpvocab (wt |st ) + (1 − g)pptr (wt |st ) (2) In the formula above, wt and st are respectively the output word and the decoder state at step t; pptr is the probability of attention-based copying of the word wt , and g is the mixture weight: pptr (wt |st ) = � αkj ,t kj ∈I(w,x) g = Softmax(uT tanh (Wα si )) (3) (4) where αkj ,t is the attention weight for kth token in ﬂattened dialogue context at the decoding step t and u is the sentinel vector — for more deta"
D19-1183,P05-1045,0,0.0548543,"Missing"
D19-1183,N16-1162,0,0.0396492,"Missing"
D19-1183,P82-1020,0,0.702804,"Missing"
D19-1183,D17-1238,0,0.025818,"in common across domains. Therefore, in order to increase Entity F1, KB information should be directly copied to the output more efﬁciently — and increasing the robustness of the copy-augmented decoder is one of our future research directions. 1748 8.4 Discussion of the evaluation metrics We use BLEU as one of the main evaluation metrics in this paper — we do it in order to fully conform with the setup of Zhao and Esk´enazi (2018) which we base our work on. But while being widely adopted as a general-purpose language generation metric, BLEU might not be sufﬁcient in the dialogue settings (see Novikova et al. (2017) for a review). Speciﬁcally, we have observed several cases where the model would produce an overall grammatical response with the correct dialogue intent (e.g. “You are welcome! Anything else?”), but BLEU would output a lower score for it due to word mismatch (e.g. “You’re welcome!”; see more examples in Table 4). This is a general issue in dialogue model evaluation since the variability of possible responses equivalent in meaning is very high in dialogue. In future work, we will put more emphasis on the meaning of utterances, for example by incorporating external dialogue act tagging resourc"
D19-1183,D14-1162,0,0.0824172,"in, they presented this approach as “zero-shot” learning. In our approach, we do use complete in-domain dialogues, but with signiﬁcantly less data with respect to the number of in-domain utterances. Moreover, our method requires no annotation whatsoever. Recent research in Natural Language Processing has shown that the transfer of text representation learned on larger data sources beneﬁts target models’ performance, just as was the case with ImageNet-based computer vision models (Deng et al., 2009). For text, the main means for transfer was Word2Vec and GloVe embeddings (Mikolov et al., 2013; Pennington et al., 2014) recently extended with context-aware models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). Trained on large and diverse textual corpora, they were shown to improve target models’ performance on a number of Natural Language Processing tasks. Although highly beneﬁcial, those models’ use may not be sufﬁcient for the case of dialogue as response generation for goal-oriented dialogue from extremely limited data requires specialized tools. General-purpose embeddings lack speciﬁcity for close dialogue domains since they have been learned from very heterogeneously distributed data: i"
D19-1183,N18-1202,0,0.0693849,"use complete in-domain dialogues, but with signiﬁcantly less data with respect to the number of in-domain utterances. Moreover, our method requires no annotation whatsoever. Recent research in Natural Language Processing has shown that the transfer of text representation learned on larger data sources beneﬁts target models’ performance, just as was the case with ImageNet-based computer vision models (Deng et al., 2009). For text, the main means for transfer was Word2Vec and GloVe embeddings (Mikolov et al., 2013; Pennington et al., 2014) recently extended with context-aware models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). Trained on large and diverse textual corpora, they were shown to improve target models’ performance on a number of Natural Language Processing tasks. Although highly beneﬁcial, those models’ use may not be sufﬁcient for the case of dialogue as response generation for goal-oriented dialogue from extremely limited data requires specialized tools. General-purpose embeddings lack speciﬁcity for close dialogue domains since they have been learned from very heterogeneously distributed data: in dialogue, the distribution of word sequences is highly speciﬁc to a given"
D19-1183,P19-1253,0,0.0200263,"rical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1741–1751, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics respect to in-domain utterances) while requiring no annotations whatsoever. form of a dialogue grammar, this method inherited the limitations of it as well. Speciﬁcally, it’s limited to a single domain until a wide-coverage grammar is available. Meta-learning has also gained a lot of attention as a way to train models for maximally efﬁcient adaptation to new data. As such, Qian and Yu (2019) presented such approach for fast adaptation of a dialogue model to a new domain. While highly promising, its main result was achieved on a synthetic dataset and would ideally need more testing on real data. Figure 1: DI-VAE and DI-VST (DiKTNet Stage 1) 2 Related Work The problem of data efﬁciency of dialogue systems has been extensively researched in the past. Starting with domain adaptation of a dialogue state tracker (Henderson et al., 2014) approached using Bayesian Processes (Gasic et al., 2017) and Recurrent Neural Networks (Mrksic et al., 2015), there has been signiﬁcant work on trainin"
D19-1183,E17-1042,0,0.276076,"and Apple Siri provide a uniﬁed conversational user interface (CUI) for third-party applications and services. Furthermore, products like Google Dialogﬂow, Wit.ai, Microsoft LUIS, and Rasa offer means for rapid development of a dialogue system’s core modules. In addition, with the recently adopted technique of training dialogue systems end-to-end data-efﬁciency of such systems becomes the key question in their adoption in practical applications. Currently, while being extremely ﬂexible and requiring little to no programming of in-domain business logic (see e.g. Ultes et al. (2018); Wen et al. (2017); Rojas-Barahona et al. (2017)), such systems have too high data consumption — including both collection and annotation effort — in order for them to be used in rapidly paced industrial product cycles. Therefore, approaches to training such systems with extremely limited data (i.e. zero-, one- and few-shot training) are a priority research direction in the dialogue systems area. In this paper, we present the Dialogue Knowledge Transfer Network (or DiKTNet), a generative goal-oriented dialogue model designed for fewshot learning, i.e. training only using a small number of complete in-domain dia"
D19-1183,P15-2130,0,0.0601638,"Missing"
D19-1183,N19-1178,0,0.0270013,"Missing"
D19-1183,W18-5032,0,0.0198963,"Google Assistant, Amazon Alexa, and Apple Siri provide a uniﬁed conversational user interface (CUI) for third-party applications and services. Furthermore, products like Google Dialogﬂow, Wit.ai, Microsoft LUIS, and Rasa offer means for rapid development of a dialogue system’s core modules. In addition, with the recently adopted technique of training dialogue systems end-to-end data-efﬁciency of such systems becomes the key question in their adoption in practical applications. Currently, while being extremely ﬂexible and requiring little to no programming of in-domain business logic (see e.g. Ultes et al. (2018); Wen et al. (2017); Rojas-Barahona et al. (2017)), such systems have too high data consumption — including both collection and annotation effort — in order for them to be used in rapidly paced industrial product cycles. Therefore, approaches to training such systems with extremely limited data (i.e. zero-, one- and few-shot training) are a priority research direction in the dialogue systems area. In this paper, we present the Dialogue Knowledge Transfer Network (or DiKTNet), a generative goal-oriented dialogue model designed for fewshot learning, i.e. training only using a small number of com"
D19-1183,P17-1062,0,0.0255007,"While highly promising, its main result was achieved on a synthetic dataset and would ideally need more testing on real data. Figure 1: DI-VAE and DI-VST (DiKTNet Stage 1) 2 Related Work The problem of data efﬁciency of dialogue systems has been extensively researched in the past. Starting with domain adaptation of a dialogue state tracker (Henderson et al., 2014) approached using Bayesian Processes (Gasic et al., 2017) and Recurrent Neural Networks (Mrksic et al., 2015), there has been signiﬁcant work on training different dialogue system components using as little data as possible. As such, Williams et al. (2017) introduced a dialogue management model designed for bootstrapping from limited training data and further ﬁne-tuning. A recent paper by Vlasov et al. (2018) introduced a dialogue management model which uses a uniﬁed embedding space for user and system turns allowing efﬁcient cross-domain knowledge transfer. There also exist approaches to end-to-end dialogue generation. Eshghi et al. (2017) proposed a linguistically informed model based on an incremental semantic parser (Eshghi et al., 2011) combined with a reinforcement learning-based agent. The parser was used for both maintaining the agent’s"
D19-1183,W18-5001,0,0.0501089,"Missing"
D19-1183,P18-1101,0,0.027166,"Missing"
E03-1075,P93-1008,1,0.823149,"a good testbed for Targeted Help. 2.2 The Targeted Help Module 2 System Description 2.1 The WITAS Dialogue System Targeted Help was deployed and tested as part of the WITAS dialogue system l , a command and control and mixed-initiative dialogue system for interacting with a simulated robotic helicopter or UAV (Unmanned Aerial Vehicle) (Lemon et al., 2001). The dialogue system is implemented as a suite of agents communicating though the SRI Open Agent Architecture (OAA) (Martin et al., 1998). The agents include: Nuance Communications Recognizer (Nuance, 2002); the Gemini parser and generator (Dowding et al., 1993) (both 'See http://www.ida.liu.se/ext/witas and http://www-csli.stanford.edu/semlab/ witas 148 The Targeted Help Module is a separate component that can be added to an existing dialogue system with minimal changes to accomodate the specifics of the domain. This modular design makes it quite portable, and a version of this agent is in fact being used in a second command and control dialogue system (Hockey et al., 2002a; Hockey et al., 2002b). It is argued in (Lemon and Cavedon, 2003) that &quot;low-level&quot; processing components such as the Targeted Help module are an important focus for future dialog"
E03-1075,W02-0216,1,0.780999,"antage of the strengths of both types of language models by using the grammar based model for in-coverage utterances and the SLM as part of the Targeted Help system for outof-coverage utterances. In this paper we report on controlled experiments, testing the effectiveness of an implementation of Targeted Help in a mixed initiative dialogue system to control a simulated robotic helicopter. using a grammar designed for the UAV application); Festival text-to-speech synthesizer (Systems, 2001); a GUI which displays a map of the area of operation and shows the UAV's location; the Dialogue Manager (Lemon et al., 2002); the Robot Control and Report component, which translates commands and queries bi-directionally between the dialogue interface and the UAV. The Dialogue Manager interleaves multiple planning and execution dialogue threads (Lemon et al., 2002). While the helicopter is airborne, an on-board active vision system will interpret the scene below to interpret ongoing events, which may be reported (via NL generation) to the operator. The robot can carry out various activities such as flying to a location, fighting fires, following a vehicle, and landing. Interaction in WITAS thus involves joint-activ"
E06-2004,W03-2123,1,\N,Missing
E06-2004,A00-2028,0,\N,Missing
E06-2004,E06-2009,1,\N,Missing
E06-2009,W03-2123,1,0.88171,"Missing"
E06-2009,C00-1073,0,0.023374,"trics combined with a fixed penalty for dialogue length (see (Henderson et al., 2005)). This agent can be called whenever the system has to decide on the next dialogue move. In the original hand-coded system this decision is made by way of a dialogue plan (using the “deliberate” solvable). The RL agent can be used to drive the entire dialogue policy, or can be called only in certain circumstances. This makes it usable for whole dialogue strategies, but also, if desired, it can be targetted only on specific dialogue management decisions (e.g. implicit vs. explicit confirmation, as was done by (Litman et al., 2000)). One important research issue is that of tranferring learnt strategies between domains. We learnt a strategy for the C OMMUNICATOR flight booking dialogues (Lemon et al., 2005; Henderson et al., 2005), but this is generated by rather different scenarios than the in-car dialogues. However, both are “slot-filling” or information-seeking applications. We defined a mapping (described below) between the states and actions of both systems, in order to construct an interface between the learnt policies for C OMMUNICATOR and the in-car baseline system. 5.2 Mapping between COMMUNICATOR and the In-car"
E09-1058,W03-2111,0,0.426616,"Missing"
E09-1058,P04-1044,1,0.736088,"e European Chapter of the ACL, pages 505–513, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 505 alogue acts in the dialogue history, that the US is sensitive to (see section 3). The paper is organized as follows. After a short relation to previous work, we describe the data (Section 5) and derive baseline results (Section 6). Section 3 describes the User Simulations that we use for re-ranking hypotheses. Section 7 describes our learning experiments for classifying and selecting from n-best recognition hypotheses and Section 9 reports our results. 2 (Gabsdil and Lemon, 2004) similarly perform reordering of n-best lists by combining acoustic and pragmatic features. Their study shows that dialogue features such as the previous system question and whether a hypothesis is the correct answer to a particular question contributed more to classification accuracy than the other attributes. (Jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection. These labels were learned in a similar way to (Gabsdil and Lemon, 2004) and correspond to varying levels of confidence, being essentially potential directives to"
E09-1058,P03-2004,0,0.0465568,"Missing"
E09-1058,A00-2029,0,0.0222918,"ate Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems. Relation to Previous Work In psycholinguistics, the idea that human dialogue participants simulate each other to some extent is gaining currency. (Pickering and Garrod, 2007) write: “if B overtly imitates A, then A’s comprehension of B’s utterance is facilitated by A’s memory for A’s previous utterance.” We explore aspects of this idea in a computational manner. Similar work in the area of spoken dialogue systems is described below. (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of acoustic-prosodic features (e.g. amplitude in the speech signal) for hypothesis classification, but we also use User Simulation predictions. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse"
E09-1058,N07-2038,0,0.321225,"ctions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P (o|h) (approximated by the ASR confidence score) and the User Simulation probability P (h|us, C) of the hypothesis: We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS). Our novel approach is to use a simple statistical User Simulation (US) for this tas"
E09-1078,P08-1055,0,0.291305,"ing (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Associ"
E09-1078,P08-1073,1,0.836856,"general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one. Each such action ha"
E09-1078,E06-1009,0,0.477751,"ehaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Li"
E09-1078,P04-1011,0,0.096083,"e user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG acti"
E09-1078,P07-1043,0,0.0327774,"3 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al.,"
E09-1078,P06-1140,0,0.0352404,"oal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adop"
E09-1078,whittaker-etal-2002-fish,0,0.134408,"relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an upper limit on how many attributes users like to hear. We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007). We therefore include “cognitive load” as a variable when training the policy (see Section 6). In addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOM MEND one specific item/restaurant. For example “The 4 restaurants are all French, but differ in 1 For comparison: (Walker et al., 2000) report on R2 between .4 and .5 on a slightly lager data set. 685 Figure 1: Possible NLG policies (X=stop generation) describe a worked through example of the overall framework. food quality, and cost. Le Madeleine has the best overall value among the selected restaurants. Le Madeleine’s price is 40 dollars and It has very good food qua"
E09-1078,J08-4002,1,\N,Missing
E09-1078,W08-0119,0,\N,Missing
E09-1078,P09-1010,0,\N,Missing
E09-1078,P08-2019,1,\N,Missing
E12-2010,W10-4336,1,0.831059,"ecent research in statistical SDSs has successfully addressed aspects of these problems through the application of Partially Observable Markov Decision Process (POMDP) approaches (Thomson and Young, 2010; Young et al., 2010). However POMDP SDSs are currently limited by the representation of user goals adopted to make systems computationally tractable. Work in dialogue system evaluation, e.g. Walker et al. (2004) and Lemon et al. (2006), shows that real user goals are generally sets of items, rather than a single item. People like to explore possible trade offs between the attributes of items. Crook and Lemon (2010) identified this as a central challenge for the field of spoken dialogue systems, proposing the use of automatic compression techniques to allow for extended accurate representations of user goals. This paper presents a proof of concept of these ideas in the form of a complete, working spoken dialogue system. The POMDP dialogue manager (DM) of this demonstration system uses a compressed belief space that was generated using a modified version of the Value Directed Compression (VDC) algorithm as originally proposed by Poupart (2005). This demonstration system extends work presented by Crook and"
E14-1074,W09-0609,0,0.0155285,"lected by Mairesse et al. (2010),3 using Amazon Mechanical Turk. Turkers typed in recommendations for various specified semantics; e.g. “I recommend the restaurant Beluga near the cathedral.” • CLASSIC is a dataset of transcribed spoken user utterances from the CLASSiC project.4 The utterances consist of user queries for restaurants, such as “I need an Italian restaurant with a moderate price range.” User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that ratings from different u"
E14-1074,D12-1008,1,0.847662,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,W12-1509,1,0.848115,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,P13-1123,1,0.900886,"Missing"
E14-1074,W06-1405,0,0.0346695,"t personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 LIST MAI CLASSIC 4 Estimation of Style Prediction Models Corpora and Style Dimensions Our domain of interest is the automatic generation of restaurant recommendations that differ with respect to their colloquialism and politeness and are as natural as possible. All three stylistic dimension were identified from a qualitative analysis of human domain data. To estimate the strength of each of them in a single utterance, we c"
E14-1074,J14-4006,1,0.602156,"ly by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utterances, so that their ratings can be estimated more accurately than 702 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702–711, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics based on an average population of users. This is similar to Janarthanam and Lemon (2014), who show that clustering users and adapting to their level of domain expertise can significantly improve task success and user ratings. Our resulting model is evaluated with realisers not originally built to deal with stylistic variation, and produces natural variation recognisable by humans. User Clusters Ranking + Evaluation 2 Architecture and Domain Regressor We aim to with generating restaurant recommendations as part of an interactive system. To do this, we assume that a generator input is provided by a preceding module, e.g. the interaction manager, and that the task of the surface rea"
E14-1074,J11-3002,0,0.336021,"es, keeping traces of each generator decision, and obtaining style scores for each output based on the estimated factor model. The result is a dataset of <generator decision, style score&gt; pairs which can be used in a correlation analysis to identify the predictors of particular output styles. During generation, the correlation equations inform the generator at each choice point so as to best express the desired style. Unfortunately, no human evaluation of the model is presented so that it remains unclear to what extent the generated styles are perceivable by humans. Closely related is work by Mairesse and Walker (2011) who present the PERSONAGE system, which aims to generate language reflecting particular personalities. Instead of choosing generator decisions by considering their predicted style scores, however, Mairesse and Walker (2011) directly predict generator decisions based on target personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of sev"
E14-1074,P10-1157,0,0.0952924,"Missing"
E14-1074,P05-1008,0,0.223977,"ormation inform the resulting stylistic regression model. For surface realisation (topright box, blue), a semantic input from a preceding model is given as input to a surface realiser. Any realiser is suitable that returns a ranked list of output candidates. The resulting list is re-ranked according to stylistic scores estimated by the regressor, so that the utterance which most closely reflects the target score is ranked highest. The reranking process is shown in the lower box (red). 3 Related Work 3.1 Stylistic Variation in Surface Realisation Our approach is most closely related to work by Paiva and Evans (2005) and Mairesse and Walker 1 http://parlance-project.eu Surface Realisation Figure 1: Architecture of stylistic realisation model. Top left: user clusters are estimated from corpus utterances described by linguistic features and ratings. Top right: surface realisation ranks a list of output candidates based on a semantic input. These are ranked stylistically given a trained regressor. (2011), discussed in turn here. Paiva and Evans (2005) present an approach that uses multivariate linear regression to map individual linguistic features to distinguishable styles of text. The approach works in thr"
E14-1074,P05-1015,0,0.0770249,"so as to reflect stylistic variation that is as natural as possible. On the other hand, we aim to minimise the amount of annotation and human engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p<0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utte"
E14-1074,W10-4301,0,0.0314935,"ions per dimension between actual and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s us"
E14-1074,J97-1007,0,\N,Missing
E14-1074,W13-4026,1,\N,Missing
E14-1074,W11-2813,1,\N,Missing
E14-4041,W13-2115,1,0.807008,"Summarisation of time-series data refers to the task of automatically generating summaries from attributes whose values change over time. Content selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance 210 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics then decides whether to talk about it or not. The states and actions are similar for all systems. Section 5, directions for future work are discussed. 2 Methodology Lecturer-adapted reward function The reward function is derived from"
E14-4041,W10-4324,1,0.857758,"t are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to"
E14-4041,P02-1040,0,0.1008,"esented in Table 2. 3.1 Evaluation in Simulation 26 summaries were produced by each system. The output of each system was evaluated with the three reward functions. Table 3 shows the results. As expected, all systems score highly when evaluated with the reward function for which they were trained, with the second highest reward scored from the MO function. Table 2 illustrates this with the MO Policy clearly between the other two policies. Moreover, the MO function reduces the variability between summaries as is also reflected in the standard deviation given in Table 3. We used BLEU (4-grams) (Papineni et al., 2002) to measure the similarities between the feedback summaries generated by the three systems. BLEU score is between 0-1 with values closer to 1 indicating texts are more similar. Our results demonstrate that the summaries generated by the three systems are quite different (BLEU score between 0.33 and 0.36). This shows that the framework presented here is capable of producing quite different summaries based on the various reward functions. 4 Discussion It is interesting to examine the weights derived from the multiple-linear regression to determine the preferences of the different user groups. Fo"
E14-4041,P10-1103,1,0.795582,"ere is a direct mapping between the values of factor and reference type and the surface text. The timeseries attributes are listed in Table 1 (bottom left). Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RM O = 0.5 ∗ RLECT + 0.5 ∗ RST U DEN T To reduce the confounding variables, we kept the ordering of content in all systems the same. 2.2 Time-series summarisation systems Actions and states: The state consists of the timeseries data and the selected templates. In order to explore the state space the agent selects a timeseries attribute (e.g. marks, deadlines etc.) and 3 Evaluation The output of the above-mentioned three systems were evaluated both in s"
E14-4041,P97-1035,0,0.440656,"s of time-series data. There is a direct mapping between the values of factor and reference type and the surface text. The timeseries attributes are listed in Table 1 (bottom left). Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RM O = 0.5 ∗ RLECT + 0.5 ∗ RST U DEN T To reduce the confounding variables, we kept the ordering of content in all systems the same. 2.2 Time-series summarisation systems Actions and states: The state consists of the timeseries data and the selected templates. In order to explore the state space the agent selects a timeseries attribute (e.g. marks, deadlines etc.) and 3 Evaluation The output of the above-mentioned three systems"
E17-2077,W14-4308,1,\N,Missing
J08-4002,W03-2123,1,0.704684,"2005; Georgila et al., submitted) to assign Speech Acts and Tasks to the user utterances, and to compute state representations for each point in the dialogue (i.e., after every utterance). Although we annotated the whole 2000 and 2001 corpora, because we need the results of user questionnaires (as discussed subsequently), we only make use of the 2001 data for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and 125,388 total states, two thirds of which result from system actions and one third from user actions. The annotation system is implemented using DIPPER (Bos et al. 2003) and OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and Henderson, 2005, and Georgila et al., submitted, for more details). Following the ISU approach, we represented states using Information States, which are feature structures intended to record all the information about the preceding portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in an Information State is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Given this corpus, we need to lea"
J08-4002,P06-1024,1,0.840119,"Missing"
J08-4002,P04-1044,1,0.798229,"Missing"
J08-4002,P06-2085,1,0.919667,"y of the states of information slots (e.g., destination city ﬁlled with high conﬁdence) in the application (Goddeau and Pineau 2000; Levin, Pieraccini, and Eckert 2000; Singh et al. 2000a, 2000b, 2002; Young 2000; Schefﬂer and Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005; Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such as acoustic features [Pietquin 2004]). Only recently have researchers experimented with using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon et al. 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this article. From this work it is known that adding context features leads to better dialogue strategies, compared to, for example, simply using the status of ﬁlled or conﬁrmed information slots as has been studied in all prior work (Frampton and Lemon 2006). In this article we explore methods for scalable, tractable learning when using all the available context features. Reinforcement Learning requires estimating how good different actions will be in different dialogue contexts. Because most previous work has only differentiated between a small number of possible dialogue con"
J08-4002,2005.sigdial-1.6,1,0.727657,"learning approaches is the extent to which they train on data from simulated users of different kinds, rather than train on data gathered from real user interactions (as is done in this article). Simulated users are generally preferred due to the much smaller development effort involved, and the fact that trialand-error training with humans is tedious for the users. However, the issues of how to construct and then evaluate simulated users are open problems. Clearly there is a dependency between the accuracy of the simulation used for training and the eventual dialogue policy that is learned (Schatzmann et al. 2005). Current research attempts to develop metrics for user simulation that are predictive of the overall quality of the ﬁnal learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann et al. 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser and Lemon 2006a; Schatzmann et al. 2006; Williams 2007). Furthermore, several approaches use simple probabilistic simulations encoded by hand, using intuitions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005; Pietquin and Dutoit 2006a), whereas other work (e.g., Schefﬂer and Youn"
J08-4002,H01-1015,0,0.0687517,"portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in our Information States is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Previous work has raised the question of whether dialogue management policies can be learned (Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue context, for example, not including prior speech act history (see the following). One prominent representation of the set of possible system actions is the DATE scheme (Walker and Passonneau 2001). In particular, this representation is used in the C OMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed herein. The DATE scheme classiﬁes system actions in terms of their Conversational Domain, Speech Act, and Task. For example, one possible system action is about task, Figure 1 Example ﬁelds from an Information State annotation. User-provided information is in square brackets. 489 Computational Linguistics Volume 34, Number 4 request info, dest city, which corresponds to a system utterance such as What is your destination city? The speciﬁc instantiation of this"
J08-4002,P98-2219,0,0.0208242,"Missing"
J08-4002,P01-1066,0,0.149979,"olicy that is different from that used to generate the data (called “off-policy” learning), but these methods have been found not to work well with linear function approximation (Sutton and Barto 1998). They also do not solve the problem of straying from the region of state space that has been observed in the data, discussed subsequently. 491 Computational Linguistics Volume 34, Number 4 1.2 The COMMUNICATOR Domain and Data Annotation To empirically evaluate our proposed learning method, we apply it to the C OMMU NICATOR domain using the C OMMUNICATOR corpora. The C OMMUNICATOR corpora (2000 [Walker et al. 2001] and 2001 [Walker et al. 2002b]) consist of human–machine dialogues (approximately 2,300 dialogues in total). The users always try to book a ﬂight, but they may also try to select a hotel or car rental. The dialogues are primarily “slotﬁlling” dialogues, with some information being presented to the user after the system thinks it has ﬁlled (or conﬁrmed) the relevant slots. These corpora have been previously annotated using the DATE scheme, for the Conversational Domain, Speech Act, and Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and Boland 2001). In addition"
J08-4002,2005.sigdial-1.4,0,0.152387,"Missing"
J08-4002,W03-2111,0,0.256834,"Missing"
J08-4002,P00-1013,0,\N,Missing
J08-4002,C98-2214,0,\N,Missing
J08-4002,E06-2009,1,\N,Missing
J11-1006,N07-2001,0,0.0577333,"Missing"
J11-1006,J97-1002,0,0.248661,"Missing"
J11-1006,E06-1045,0,0.0126602,"licy. The results from a Wilcoxon Signed Ranks Test on the user questionnaire data (see Table 9) show signiﬁcantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal presentation, and that more users would use the RL-based system in the future (Future Use). All the observed differences have a medium effects size (r ≥ |.3|). We also observe that female participants clearly favor the RL-based strategy, whereas the ratings by male participants are more indifferent. Similar gender effects are also reported by other studies on multimodal output presentation (e.g., Foster and Oberlander 2006; Jokinen and Hurtig 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are signiﬁcantly shorter (p &lt; .005), while fewer items are displayed (p &lt; .001), and the help function is used signiﬁcantly less (p &lt; .003). The mean 9 The WOZ study was performed in German, whereas the user tests are performed in English. Therefore, a different database had to be used and task sets and user questionnaires had to be translated. 173 Computational Linguistics Volume 37, Number 1 performance measures for testing with real users are shown in Table 10. Also"
J11-1006,P04-1044,1,0.755758,"list verbally (presentInfoverbal) or multi-modally (presentInfo-multimodal). All the feature selection techniques consistently choose the feature DB (number of retrieved items from the database). This result is maybe not very surprising, but it supports the claim that using feature selection on WOZ data delivers valid results. Relevant features for other domains may be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies. For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and Lemon and Konstas (2009) for automatic ASR re-ranking. We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3. 161 Computational Linguistics Volume 37, Number 1 Figure 2 State-action space for hierarchical Reinforcement Learning. 3.2 MDP and Problem Representation The structure of an information-seeking dialogue system consists of an information acquisition phase, and an information presentation phase. For information acquisition the ta"
J11-1006,N07-1034,0,0.176134,"nts from the WOZ corpus using datadriven methods. Although this requires quite a large effort, the exercise is important as a case study for exploring the proposed methodology. The employed database contains 438 items and is similar in retrieval ambiguity and structure to the one used in the WOZ experiment. The dialogue system used for learning implements a multimodal information presentation strategy which is untrained, but comprises some obvious constraints reﬂecting the system logic (e.g., that only ﬁlled slots can be conﬁrmed), implemented as Information State Update (ISU) rules (see also Heeman 2007; Henderson, Lemon, and Georgila 2008). Other behavior which is hand-coded in the system is to greet the user in the beginning of a dialogue and to provide help if the user requests help. The help function provides the user with some examples of what to say next (see system prompt s6 in the Example Dialogue in Table 1 in Appendix D). All other actions are left for optimization. 3.1 Feature Space A state or context in our system is a dialogue “information state” as deﬁned in (Lemon et al., 2005). We divide the types of information represented in the dialogue information state into local feature"
J11-1006,N01-1027,0,0.091206,"Missing"
J11-1006,P10-1008,1,0.704514,"imodal output generation are two closely interrelated problems for information seeking dialogues: the decision of when to present information depends on how many pieces of information to present and the available options for how to present them, and vice versa. We therefore formulate the problem as a hierarchy of joint learning decisions which are optimized together. We see this as a ﬁrst step towards an integrated statistical model of Dialogue Management and more advanced output planning/Natural Language Generation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon 2010). In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how to collect appropriate data when no initial data or system exists), Section 3 explains the construction of the simulated learning environment (including how to determine a data-driven reward function), Section 4 presents training and evaluation of the learned policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents the results of the tests with real users, and Section 6 presents a meta-evaluation of the framework, including transfer results. 2. Wizard-of-Oz Data Collection T"
J11-1006,J00-4006,0,0.00543448,"on and form. The EM algorithm generates three state clusters: The system acts askAQuestion and implConfirm are summarized into cluster 1; explConf and reject are in cluster 2; and presentListVerbal and presentListMM are in cluster 3. For every cluster we assign the observed frequencies of user actions (i.e., all the user actions which occur with one of the states belonging to that cluster), as shown in Figure 5. 3.5.2 Smoothed Bigram User Simulation. For our second user simulation model we apply smoothing to a bigram model. We implement a simple smoothing technique called “add-one smoothing” (Jurafsky and Martin 2000). This technique discounts some nonzero counts in order to obtain probability mass that will be assigned to the zero counts. We apply this technique to the original frequency-based bigram model. The resulting model is shown in Figure 6. In general, the smoothed model is closer to the original data than the cluster-based one (thus being more realistic at the expense of allowing less exploratory behavior). In the next section we introduce an evaluation metric which allows us to assess the level of exploratory versus realistic user behavior as exhibited by the different user simulations. Table 6"
J11-1006,kruijff-korbayova-etal-2006-sammie,1,0.872121,"Missing"
J11-1006,E09-1058,1,0.848023,"bal) or multi-modally (presentInfo-multimodal). All the feature selection techniques consistently choose the feature DB (number of retrieved items from the database). This result is maybe not very surprising, but it supports the claim that using feature selection on WOZ data delivers valid results. Relevant features for other domains may be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies. For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and Lemon and Konstas (2009) for automatic ASR re-ranking. We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3. 161 Computational Linguistics Volume 37, Number 1 Figure 2 State-action space for hierarchical Reinforcement Learning. 3.2 MDP and Problem Representation The structure of an information-seeking dialogue system consists of an information acquisition phase, and an information presentation phase. For information acquisition the task of the dialogue manager is"
J11-1006,P99-1024,0,0.0790434,"Missing"
J11-1006,W07-0306,0,0.0238041,"ve functions has received little attention so far. In fact, as noted in Section 3.6, the reward function is one of the most hand-coded aspects of RL (Paek 2006). Here, we bring together two strands of research for evaluating the reward function: One strand uses Reinforcement Learning to automatically optimize dialogue strategies (e.g., Singh et al. 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a, 2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR ADISE framework [Walker et al. 1997]), and meta-evaluation of dialogue metrics (e.g., ¨ 2007; Paek 2007). Clearly, automatic optimization and evaluation Engelbrecht and Moler of dialogue policies, as well as quality control of the objective function, are closely interrelated problems: How can we make sure that we optimize a system according to real users’ preferences? In Section 3.6 we constructed a data-driven objective function using the PARADISE framework, and used it for automatic dialogue strategy optimization, following work by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such a predictive model is, that is, whether it indeed estimates real user preferences."
J11-1006,2005.sigdial-1.11,1,0.870335,"Missing"
J11-1006,P06-2085,1,0.958538,"ciﬁed the number of slots, and information about the “grounded-ness” of the slots, needed to learn conﬁrmation strategies.3 We also added the features which were automatically discovered by the feature selection techniques deﬁned in Section 3.1.1. The state-space comprises eight binary features representing the task for a fourslot problem: filledSlot indicates whether a slot is ﬁlled, confirmedSlot indicates whether a slot is conﬁrmed. We also add the number of retrieved items (DB). We found that human wizards especially pay attention to this feature, using the feature selection techniques of Rieser and Lemon (2006b). The feature DB takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states for the state space. In total there are 4112,128 theoretically possible policies for information acquisition.4 For 3 Note that we simpliﬁed the notion of a slot being grounded as a binary feature, following Henderson, Lemon, and Georgila (2008). More recent work uses more ﬁne-grained notions of conﬁdence in user-provided information (e.g., Roque and Traum 2008), or the notion of “belief states” in Partially Observable Markov Decision Processes (e.g., Williams and Young 2007). Th"
J11-1006,P08-1073,1,0.67083,"but also in the reward function, resulting in a direct mapping between quantized retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision of when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with non-linear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more ﬁne-grained local trade-offs between the parameters, as demonstrated by Rieser and Lemon (2008a). 4. Training and Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALLDUDE toolkit of Lemon et al. (2006). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-a"
J11-1006,E09-1078,1,0.796013,"for an in-car digital music player. Dialogue Management and multimodal output generation are two closely interrelated problems for information seeking dialogues: the decision of when to present information depends on how many pieces of information to present and the available options for how to present them, and vice versa. We therefore formulate the problem as a hierarchy of joint learning decisions which are optimized together. We see this as a ﬁrst step towards an integrated statistical model of Dialogue Management and more advanced output planning/Natural Language Generation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon 2010). In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how to collect appropriate data when no initial data or system exists), Section 3 explains the construction of the simulated learning environment (including how to determine a data-driven reward function), Section 4 presents training and evaluation of the learned policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents the results of the tests with real users, and Section 6 presents a meta-evaluation of the f"
J11-1006,P10-1103,1,0.220792,"Missing"
J11-1006,P05-1030,1,0.179777,"is applied in order to identify more general situations than the previously annotated system speech acts by grouping them according to their similarity. For building such clusters we apply the Expectation-Maximization (EM) algorithm. The EM algorithm is an incremental approach to clustering (Dempster, Laird, and Rubin 1977), which ﬁts parameters of Gaussian density distributions to the data. In order to deﬁne similarity between system actions, we need to describe their (semantic) properties. We therefore annotate the system acts using a ﬁne-grained scheme by Rodriguez and Schlangen (2004) and Rieser and Moore (2005), which allows classiﬁcation of dialogue acts in terms of different forms and functions. We use a slightly modiﬁed version of the scheme, where we only use a subset of the suggested annotation tags, while adding another level describing the output modality, as summarized in Figure 4. In particular, the annotation scheme describes wizard actions in terms of their communication level, which describes the linguistic target after Clark (1996). We distinguish between utterances which aim to elicit acoustic 166 Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications Figu"
J11-1006,W08-0107,0,0.0128932,"(DB). We found that human wizards especially pay attention to this feature, using the feature selection techniques of Rieser and Lemon (2006b). The feature DB takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states for the state space. In total there are 4112,128 theoretically possible policies for information acquisition.4 For 3 Note that we simpliﬁed the notion of a slot being grounded as a binary feature, following Henderson, Lemon, and Georgila (2008). More recent work uses more ﬁne-grained notions of conﬁdence in user-provided information (e.g., Roque and Traum 2008), or the notion of “belief states” in Partially Observable Markov Decision Processes (e.g., Williams and Young 2007). This does lead to new policies in information acquisition, but is not the focus of this article. 4 In practice, the policy space is smaller, as some combinations are not possible (e.g., a slot cannot be conﬁrmed before being ﬁlled). Furthermore, some incoherent action choices are excluded by the basic system logic. 162 Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications the presentation phase the DB feature is discretized, as we will further dis"
J11-1006,2005.sigdial-1.6,0,0.0550597,"Missing"
J11-1006,N07-2038,0,0.0249903,"uation of Dialogue Strategies for New Applications The use of WOZ data has earlier been proposed in the context of RL. Williams and Young (2004) use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ data to build a simulated user and noise model for simulation-based RL. Although both studies show promising ﬁrst results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. Schatzmann et al. (2007) propose to “bootstrap” with a simulated user which is entirely hand-crafted. In the following we propose what is currently the most strongly data-driven approach to these problems. We also show that the resulting policy performs well for real users. In particular we propose a ﬁve-step procedure (see Figure 1): 1. We start by collecting data in a WOZ experiment, as described in Section 2. 2. From these data we train and test different components of our simulated environment using Supervised Learning techniques (Section 3). In Figure 1 Data-driven methodology for simulation-based dialogue strat"
J11-1006,2007.sigdial-1.48,0,0.0569895,"uation of Dialogue Strategies for New Applications The use of WOZ data has earlier been proposed in the context of RL. Williams and Young (2004) use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ data to build a simulated user and noise model for simulation-based RL. Although both studies show promising ﬁrst results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. Schatzmann et al. (2007) propose to “bootstrap” with a simulated user which is entirely hand-crafted. In the following we propose what is currently the most strongly data-driven approach to these problems. We also show that the resulting policy performs well for real users. In particular we propose a ﬁve-step procedure (see Figure 1): 1. We start by collecting data in a WOZ experiment, as described in Section 2. 2. From these data we train and test different components of our simulated environment using Supervised Learning techniques (Section 3). In Figure 1 Data-driven methodology for simulation-based dialogue strat"
J11-1006,P98-2219,0,0.0274315,"Missing"
J11-1006,P97-1035,0,0.247552,"alker 2005). Despite its central aspect for RL, quality assurance for objective functions has received little attention so far. In fact, as noted in Section 3.6, the reward function is one of the most hand-coded aspects of RL (Paek 2006). Here, we bring together two strands of research for evaluating the reward function: One strand uses Reinforcement Learning to automatically optimize dialogue strategies (e.g., Singh et al. 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a, 2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR ADISE framework [Walker et al. 1997]), and meta-evaluation of dialogue metrics (e.g., ¨ 2007; Paek 2007). Clearly, automatic optimization and evaluation Engelbrecht and Moler of dialogue policies, as well as quality control of the objective function, are closely interrelated problems: How can we make sure that we optimize a system according to real users’ preferences? In Section 3.6 we constructed a data-driven objective function using the PARADISE framework, and used it for automatic dialogue strategy optimization, following work by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such a predictive m"
J11-1006,W03-2111,0,\N,Missing
J11-1006,J08-4002,1,\N,Missing
J11-1006,J06-2004,0,\N,Missing
J11-1006,C98-2214,0,\N,Missing
J14-4006,C94-2197,0,0.326302,"knowledge of one concept when his/her knowledge of other concepts is established (Kass 1991; Cawsey 1993). Hand-coded policies can also be designed by dialogue system designers to inform the system about when to seek information in order to partially populate the user model (Cawsey 1993). However, hand-coding such adaptation policies can be difficult for large and complex tasks that contain a large number of domain objects. Similarly, supervised learning approaches like Bayesian networks can be used to specify the relationship between different domain concepts and can be used for prediction (Akiba and Tanaka 1994; Nguyen and Do 2009). However, they require many annotated adaptive dialogues to train on. In gathering such a corpus, the expert should have exhibited adaptive behavior with users of all types. In addition, annotating a large number of dialogues to learn user modeling and adaptive strategies could be very expensive. Such an annotated corpus of expert– layperson interactions is a scarce resource. Another issue is that domain experts suffer from what psychologists call the curse of expertise (Hinds 1999). This means that experts have difficulties communicating with non-experts because their ow"
J14-4006,W07-2302,0,0.012955,"In order to adapt to the user, it is necessary for the system to have a model of the user’s domain knowledge. This is currently taken into account by state-of-the-art REG algorithms by using an internal user model (UM). The UM determines whether the user would be able to relate the referring expression made by the system to the intended referent. To be more specific, it is used to estimate whether the user knows or would be able to determine whether an attribute-value pair applies to an object (Dale 1988; Reiter and Dale 1992, 1995; Krahmer and Theune 2002; Krahmer, van Erk, and Verleg 2003; Belz and Varges 2007; Gatt and Belz 2008; Gatt and van Deemter 2009). So, if the user model believes that the user cannot associate an attribute-value pair (e.g., < category, recliner &gt;) to the target entity x, then it would return false. On the other hand, if he can instead associate the pair (e.g., < category, chair &gt;) to x, the user model would return true. This would inform the algorithm to choose the category “chair” in order to refer to x. Therefore, using an accurate user model, an appropriate choice can be made to suit the user. However, these models are static and are predefined before run-time. How can"
J14-4006,2007.sigdial-1.45,0,0.0290275,"conversations between users and the system. Some systems will simply aim to adapt to the user as much as possible and do not need to attend to users’ learning, which is the approach we have taken in this article. For instance, a city 909 Computational Linguistics Volume 40, Number 4 navigation system that interacts with locals and tourists (such as Rogers, Fiechter, and Thompson 2000; Janarthanam et al. 2013) should use proper names and descriptions of landmarks appropriately to different users to guide them around the city. A technical support system helping expert and novice users (such as Boye 2007) should use referring expressions and instructions appropriate to the user’s expertise. An Ambient Intelligence Environment in a public space (e.g., museum) interacting with visitors (such as Lopez-Cozar et al. 2005) can guide visitors and describe the exhibits in a language that the user would appreciate and understand. 8.2 Comparison with More Intelligent Hand-Coded Policies Although some of our hand-coded policies adapted to users, most of them did not use internal user models (except the Stereotype policy). We therefore also compared the performance of our learned policy with a more intell"
J14-4006,J11-3003,0,0.0607548,"Missing"
J14-4006,W10-4204,0,0.0462305,"Missing"
J14-4006,N07-4003,0,0.031784,"ecially in long conversations involving many domain entities. There are several applications of this approach to user modeling. For instance, an assistive health care system that interacts with patients to educate and assist them in taking care of themselves (Bickmore and Giorgino 2004) should be able to adapt to patients’ initial levels of knowledge and in subsequent dialogues change its language according to the improvement in the patient’s understanding and improving knowledge of the domain. Similarly, a tutorial dialogue system that tutors students or trains personnel in industry (such as Dzikovska et al. 2007) should adapt to the needs of the learner in terms of their levels of understanding and expertise. Such systems pay attention to learning gain, but aim to keep the 911 Computational Linguistics Volume 40, Number 4 user in the zone of proximal development (Vygotsky 1978) and therefore not use too many jargon expressions or complex instructions. We leave these issues for future research. 8.4 Extending Our Approach to Content Selection and Dialogue Management In this section, we discuss how our approach to user modeling (that we have demonstrated in choosing referring expressions) can be extended"
J14-4006,W08-1108,0,0.0165031,"the user, it is necessary for the system to have a model of the user’s domain knowledge. This is currently taken into account by state-of-the-art REG algorithms by using an internal user model (UM). The UM determines whether the user would be able to relate the referring expression made by the system to the intended referent. To be more specific, it is used to estimate whether the user knows or would be able to determine whether an attribute-value pair applies to an object (Dale 1988; Reiter and Dale 1992, 1995; Krahmer and Theune 2002; Krahmer, van Erk, and Verleg 2003; Belz and Varges 2007; Gatt and Belz 2008; Gatt and van Deemter 2009). So, if the user model believes that the user cannot associate an attribute-value pair (e.g., < category, recliner &gt;) to the target entity x, then it would return false. On the other hand, if he can instead associate the pair (e.g., < category, chair &gt;) to x, the user model would return true. This would inform the algorithm to choose the category “chair” in order to refer to x. Therefore, using an accurate user model, an appropriate choice can be made to suit the user. However, these models are static and are predefined before run-time. How can a system adapt when"
J14-4006,W09-0614,1,0.843793,"abled the wizard to annotate the content of the participant’s utterances. Participant responses ranging from answers to questions, to acknowledging instructions, to requesting clarifications can be annotated. The annotated dialogue act is sent to the dialogue system for response. Table 2 shows the set of dialogue acts that can be annotated using this panel. In addition to these, other behaviors, like remaining silent or saying irrelevant things, were also accommodated. The WIT sent the generated dialogue act to the dialogue manager. For a more detailed description of the tool, please refer to Janarthanam and Lemon (2009). 3.2 Dialogue Manager The dialogue manager identifies the next dialogue act (As,t where t denotes turn number, s denotes system) to give to the user based on the dialogue management policy πdm . The dialogue management policy is coded in the form of a finite state machine. It represents a series of instructions to be given to the user in order to set up a home broadband connection. In this dialogue task, the system provides instructions to either observe or manipulate the environment. The user’s environment consists of several domain entities such as broadband and Ethernet cables, a broadband"
J14-4006,P13-1163,1,0.882546,"Missing"
J14-4006,W10-4323,0,0.0160256,"Missing"
J14-4006,J03-1003,0,0.0216518,"Missing"
J14-4006,P93-1031,0,0.341613,"Missing"
J14-4006,W90-0112,0,0.596304,"ns, as the name suggests, are more descriptive and identify the referent using attributes like shape, size and color, and so on (e.g., small white box). Although the choice between jargon and descriptive expressions may be motivated by many factors (learning gain, lexical alignment/entrainment, etc.), we focus on enabling users with different domain knowledge levels to identify the target entity efficiently. By domain knowledge, we mean the user’s capability to identify domain objects when the system uses jargon expressions to refer to them. This is also called domain communication knowledge (Rambow 1990; Kittredge, Korelsky, and Rambow 1991). Therefore, this means that an expert user as defined in this article will not necessarily be able to reason about domain entities in terms of their functionality and how they relate with each other. It simply means that she/he will be able to identify the domain entities using jargon expressions. 3. The Dialogue System In order to explore the problem of dynamic user modeling, we built a “wizarded” technical support dialogue system that helps users to set up a home broadband connection. The dialogue system consists of a dialogue manager, a user modeling"
J14-4006,C92-1038,0,0.142445,"gement and adapting to dynamic knowledge profiles in Section 8. 2. Dynamic User Modeling In order to adapt to the user, it is necessary for the system to have a model of the user’s domain knowledge. This is currently taken into account by state-of-the-art REG algorithms by using an internal user model (UM). The UM determines whether the user would be able to relate the referring expression made by the system to the intended referent. To be more specific, it is used to estimate whether the user knows or would be able to determine whether an attribute-value pair applies to an object (Dale 1988; Reiter and Dale 1992, 1995; Krahmer and Theune 2002; Krahmer, van Erk, and Verleg 2003; Belz and Varges 2007; Gatt and Belz 2008; Gatt and van Deemter 2009). So, if the user model believes that the user cannot associate an attribute-value pair (e.g., < category, recliner &gt;) to the target entity x, then it would return false. On the other hand, if he can instead associate the pair (e.g., < category, chair &gt;) to x, the user model would return true. This would inform the algorithm to choose the category “chair” in order to refer to x. Therefore, using an accurate user model, an appropriate choice can be made to suit"
J14-4006,E09-1078,1,0.493641,"transitions. The agent learns to solve the problem by learning a policy π : s → a that optimally maps all the states to actions that lead to a high expected cumulative reward. The state of the agent represents the environment as observed by the agent. Reinforcement learning has been widely used to learn dialogue management policies that decide what dialogue action the system should take in a given dialogue state (Eckert, Levin, and Pieraccini 1997; Levin, Pieraccini, and Eckert 1997; Williams and Young 2003; Cuayahuitl et al. 2005; Henderson, Lemon, and Georgila 2008). Recently, Lemon (2008), Rieser and Lemon (2009), and Dethlefs and Cuayahuitl (2010) have extended this approach to NLG to learn NLG policies to choose the appropriate attributes and strategies in information presentation tasks. However, to our knowledge, the application of RL for dynamically modeling users’ domain knowledge and generation of referring expressions based on user’s domain knowledge is novel. Figure 5 shows the interaction between the dialogue system and the user simulation (along with environment simulation). The user modeling component (as discussed in Section 3.2) is the learning agent. The user modeling module was trained"
J14-4006,J11-1006,1,0.843535,"used to collect dialogues between real users and dialogue systems before actually implementing the dialogue system (Fraser and Gilbert 1991) . In this framework, participants interact with an expert human operator (known as a “wizard”), who is disguised as an automated dialogue system. These dialogue systems are called wizarded dialogue systems (Forbes-Riley and Litman 2010). WOZ systems have been used extensively to collect data to learn and test dialogue management policies (Whittaker, Walker, and Moore 2002; Hajdinjak and Miheli 2003; Cheng et al. 2004; Strauss, Hoffmann, and Scherer 2007; Rieser and Lemon 2011) and information presentation strategies (Demberg, Winterboer, and Moore 2011). 887 Computational Linguistics Volume 40, Number 4 Figure 1 Wizarded spoken dialogue system. In our system, the wizards played the role of intercepting, recognizing, and interpreting user speech into dialogue acts. Like Demberg, Winterboer, and Moore (2011), wizards in our set-up did not make dialogue management decisions. These were computed by the dialogue manager module based on the user dialogue act and the current dialogue state. Usually, in fully automated dialogue systems, automatic speech recognition (ASR) a"
J14-4006,N07-2038,0,0.0832223,"Missing"
J14-4006,whittaker-etal-2002-fish,0,0.0822852,"Missing"
J14-4006,W03-2111,0,0.0443944,"sition probabilities T from one state to another (when an action is taken), and rewards R associated with such transitions. The agent learns to solve the problem by learning a policy π : s → a that optimally maps all the states to actions that lead to a high expected cumulative reward. The state of the agent represents the environment as observed by the agent. Reinforcement learning has been widely used to learn dialogue management policies that decide what dialogue action the system should take in a given dialogue state (Eckert, Levin, and Pieraccini 1997; Levin, Pieraccini, and Eckert 1997; Williams and Young 2003; Cuayahuitl et al. 2005; Henderson, Lemon, and Georgila 2008). Recently, Lemon (2008), Rieser and Lemon (2009), and Dethlefs and Cuayahuitl (2010) have extended this approach to NLG to learn NLG policies to choose the appropriate attributes and strategies in information presentation tasks. However, to our knowledge, the application of RL for dynamically modeling users’ domain knowledge and generation of referring expressions based on user’s domain knowledge is novel. Figure 5 shows the interaction between the dialogue system and the user simulation (along with environment simulation). The use"
J14-4006,J08-4002,1,\N,Missing
lemon-gruenstein-2002-language,C00-1073,0,\N,Missing
lemon-gruenstein-2002-language,H94-1037,0,\N,Missing
lemon-gruenstein-2002-language,P01-1022,0,\N,Missing
lemon-gruenstein-2002-language,W01-1609,0,\N,Missing
lemon-gruenstein-2002-language,P93-1008,0,\N,Missing
lemon-gruenstein-2002-language,H93-1008,0,\N,Missing
lemon-gruenstein-2002-language,P96-1009,0,\N,Missing
lemon-gruenstein-2002-language,P99-1024,0,\N,Missing
lemon-gruenstein-2002-language,C98-2126,0,\N,Missing
N06-2044,N01-1028,0,0.0184465,"ystem response. Recent research has focused on generating dialogue strategies automatically. This work is based on modelling dialogue as a markov decision process, formalised by a finite state space S, a finite action Progress has been made with this approach but some important challenges remain. For instance, very little success has been achieved with the large state spaces that are typical of real-life systems. Similarly, work on summarising learned strategies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly represented (Lecœuche, 2001). This tabular representation severely limits the size of the state space. We propose an alternative approach to finding optimal dialogue policies. We make use of XCS, an evolutionary reinforcement learning algorithm that seeks to represent a policy as a compact set of stateaction rules (Wilson, 1995). We suggest that this algorithm could overcome both the challenge of large state spaces and the desire for strategy inspectability. In this paper, we focus on the issue of inspectability. We present a series of experiments that illustrate how XCS can be used to evolve dialogue strategies that are"
P04-1044,daelemans-hoste-2002-evaluation,0,0.0469976,"Missing"
P04-1044,P93-1008,0,0.0494408,"human operator to interact with a simulated “unmanned aerial vehicle” (UAV): a small robotic helicopter. The human operator is provided with a GUI – an interactive (i.e. mouse clickable) map – and specifies mission goals using natural language commands spoken into a headset, or by using combinations of GUI actions and spoken commands. The simulated UAV can carry out different activities such as flying to locations, following vehicles, and delivering objects. The dialogue system uses the Nuance 8.0 speech recognizer with language models compiled from a grammar (written using the Gemini system (Dowding et al., 1993)), which is also used for parsing and generation. 3.1 WITAS Information States The WITAS dialogue system is part of a larger family of systems that implement the Information State Update (ISU) approach to dialogue management (Traum et al., 1999). The ISU approach has been used to formalize different theories of dialogue and forms the basis of several dialogue system implementations in domains such as route planning, home automation, and tutorial dialogue. The ISU approach is a particularly useful testbed for our technique because it collects information relevant to dialogue context in a centra"
P04-1044,P03-2004,1,0.864026,"Missing"
P04-1044,W02-0216,1,0.911175,"ore, our approach is more generally applicable than preceding research, since we frame our methodology in the Information State Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems. 3 cate conversational contributions that are still in some sense open, and to which new utterances can attach. • Activity Tree (AT): a tree-structure representing the current, past, and planned activities that the back-end system (in this case a UAV) performs. The WITAS Dialogue System The WITAS dialogue system (Lemon et al., 2002) is a multimodal command and control dialogue system that allows a human operator to interact with a simulated “unmanned aerial vehicle” (UAV): a small robotic helicopter. The human operator is provided with a GUI – an interactive (i.e. mouse clickable) map – and specifies mission goals using natural language commands spoken into a headset, or by using combinations of GUI actions and spoken commands. The simulated UAV can carry out different activities such as flying to locations, following vehicles, and delivering objects. The dialogue system uses the Nuance 8.0 speech recognizer with languag"
P04-1044,A00-2029,0,0.577814,"bines fixed recognition confidence Oliver Lemon School of Informatics Edinburgh University Scotland olemon@inf.ed.ac.uk rejection thresholds with dialogue-state dependent recognition grammars (Lemon, 2004). The paper is organized as follows. After a short relation to previous work, Section 3 introduces the WITAS multimodal dialogue system, which we use to collect data (Section 4) and to derive baseline results (Section 5). Section 6 describes our learning experiments for classifying and selecting from nbest recognition hypotheses and Section 7 reports our results. 2 Relation to Previous Work (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of acousticprosodic features (e.g. amplitude in the speech signal) for hypothesis classification. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse history to classify hypotheses as correct, par"
P04-1044,P03-1062,0,0.0309183,"Missing"
P04-1044,H93-1008,0,\N,Missing
P06-1024,P06-2085,1,0.403251,"ACL, pages 185–192, c Sydney, July 2006. 2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 1.2 Outline Section 2 contains a description of our basic experimental framework, and a detailed description of the reinforcement learning component and user simulations. Sections 3 and 4 describe the experiments and analyse our results, and in section 5 we conclude and suggest future work. 2 The Experimental Framework Each experiment is executed using the DIPPER Information State Update dialogue manager (Bos et al., 2"
P06-1024,W03-2123,1,0.219369,"Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 1.2 Outline Section 2 contains a description of our basic experimental framework, and a detailed description of the reinforcement learning component and user simulations. Sections 3 and 4 describe the experiments and analyse our results, and in section 5 we conclude and suggest future work. 2 The Experimental Framework Each experiment is executed using the DIPPER Information State Update dialogue manager (Bos et al., 2003) (which here is used to track and update dialogue context rather than deciding which actions to take), a Reinforcement Learning program (which determines the next dialogue action to take), and various user simulations. In sections 2.3 and 2.4 we give more details about the reinforcement learner and user simulations. Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Bar"
P06-1024,P01-1066,0,0.549876,"tractable, and at the very least means that data is more sparse and state approximation methods may need to be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). Abstract We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of differe"
P06-2085,W05-1624,1,0.862136,"Missing"
P06-2085,2005.sigdial-1.5,0,0.017767,"ference across wizards (H(5)=10.94, p > .05). 3 Mean performance ratings for the wizards’ multimodal behaviour ranged from 1.67 to 3.5 on a fivepoint Likert scale. Observing significantly different strategies which are not significantly different in terms of user satisfaction, we conjecture that the wizards converged on strategies which were appropriate in certain contexts. To strengthen this 1 Translated from German. Where a new “turn” begins at the start of each new user utterance after a wizard utterance, taking the user utterance as a most basic unit of dialogue progression as defined in (Paek and Chickering, 2005). 3 The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA. Since the users indicated their satisfaction on a 5-point likert scale, an ANOVA which assumes normality would be invalid. 2 660 id Context (turns) hypothesis we split the data by wizard and and performed a Kruskal-Wallis test on multimodal behaviour per session. Only the two wizards with the lowest performance score showed no significant variation across session, whereas the wizards with the highest scores showed the most varying behaviour. These results again indicate a context dependent strategy. In the followi"
P06-2085,P05-1030,1,0.699766,"Missing"
P06-2085,2005.sigdial-1.11,1,0.648465,"Missing"
P08-1073,E06-1045,0,0.0653763,"than the SL-based policy. The results from a paired t-test on the user questionnaire 644 data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal presentation, and that more users would use the RL-based system in the future (Future Use). All the observed differences have a medium effects size (r ≥ |.3|). We also observe that female participants clearly favour the RL-based strategy, whereas the ratings by male participants are more indifferent. Similar gender effects are also reported by other studies on multimodal output presentation, e.g. (Foster and Oberlander, 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are significantly shorter (p < .005), while fewer items are displayed (p < .001), and the help function is used significantly less (p < .003). The mean performance measures for testing with real users are shown in Table 2 and Figure 3. However, there is no significant difference for the performance of the secondary driving task. 5 Comparison of Results We finally test whether the results obtained in simulation transfer to tests with real users, following (Lemon et al., 2006a). We evaluate the qu"
P08-1073,J06-2004,0,0.0515612,"* 5.07(±2.9)*** 1.1(±.3) 1.2(±.4) 11.2(±2.4)*** 8.73(±4.4)*** 44.06(±51.5)*** 37.62(±60.7)*** Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strategies; *** denotes significant difference between SL and RL at p < .001 Figure 3: Graph comparison of objective measures: SLs = SL policy in simulation; SLr = SL policy with real users; RLs = RL policy in simulation; RLr = RL policy with real users. allows us to target the experiments to the dialogue management decisions, and block ASR quality from interfering with the experimental results (Hajdinjak and Mihelic, 2006). 17 subjects (8 female, 9 male) are given a set of 6×2 predefined tasks, which they solve by interaction with the RL-based and the SLbased system in controlled order. As a secondary task users are asked to count certain objects in a driving simulation. In total, 204 dialogues with 1,115 turns are gathered in this setup. 4.2 Results In general, the users rate the RL-based significantly higher (p < .001) than the SL-based policy. The results from a paired t-test on the user questionnaire 644 data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multim"
P08-1073,N07-1034,0,0.0710866,"of information.”, “The information presented verbally was easy to remember.” 70 of non-linear reward functions (as introduced in the previous section). We therefore quantise the state space for information presentation. We partition the database feature into 3 bins, taking the first intersection point between verbal and multimodal reward and the turning point of the multimodal function as discretisation boundaries. Previous work on learning with large databases commonly quantises the database feature in order to learn with large state spaces using manual heuristics, e.g. (Levin et al., 2000; Heeman, 2007). Our quantisation technique is more principled as it reflects user preferences for multi-modal output. Furthermore, in previous work database items were not only quantised in the state-space, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewar"
P08-1073,P06-2085,1,0.952442,"Decision Process (MDP), relating states to actions in a hierarchical manner (see Figure 1): 4 actions are available for 640 the information acquisition phase; once the action presentInfo is chosen, the information presentation phase is entered, where 2 different actions for output realisation are available. The state-space comprises 8 binary features representing the task for a 4 slot problem: filledSlot indicates whether a slots is filled, confirmedSlot indicates whether a slot is confirmed. We also add features that human wizards pay attention to, using the feature selection techniques of (Rieser and Lemon, 2006b). Our results indicate that wizards only pay attention to the number of retrieved items (DB). We therefore add the feature DB to the state space, which takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presenta3 tion phase there are 22 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Super"
P08-1073,rieser-lemon-2008-automatic,1,0.924062,"n, curve fitting does not assume a linear inductive bias, but it selects the most likely model (given the data points) by function interpolation. The resulting models are shown in Figure 3.5. The reward for multimodal presentation is a quadratic function that assigns a maximal score to a strategy displaying 14.8 items (curve inflection point). The reward for verbal presentation is a linear function assigning negative scores to all presented items ≤ 4. The reward functions for information presentation intersect at no. items=3. A comprehensive evaluation of this reward function can be found in (Rieser and Lemon, 2008a). reward function for information presentation 10 turning point:14.8 -10 intersection point -20 -30 -40 -50 -60 -70 -80 0 10 20 30 40 50 60 no. items Figure 2: Evaluation functions relating number of items presented in different modalities to multimodal score 3.6 T askEase = − 20.2 ∗ dialogueLength + multimodal presentation: MM(x) verbal presentation: Speech(x) 0 user score and Lemon, 2006a). For testing, we apply smoothing to the bi-gram model. The simulations are evaluated using the SUPER metric proposed earlier (Rieser and Lemon, 2006a), which measures variance and consistency of the simu"
P08-1073,2005.sigdial-1.11,1,0.82658,"Missing"
P08-1073,N07-2038,0,0.0151524,"to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. (Schatzmann et al., 2007) propose to ‘bootstrap’ with a simulated user which is entirely hand-crafted. In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data. We also show that the resulting policy performs well for real users. 2 Wizard-of-Oz data collection Our domains of interest are information-seeking dialogues, for example a multimodal in-car interface to a large database of music (MP3) files. The corpus we use for learning was collected in a multimodal study of German task-oriented dialogues for an incar music player appl"
P08-1073,P98-2219,0,0.0448707,"data is sparse. For training, we therefore use a cluster-based user simulation method, see (Rieser 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build a predictive model of subjective user ratings (from questionnaires) from objective dialogue performance measures (such as dialogue length). We use PARADISE to predict Task Ease (a variable obtained by taking the average of two questions in the questionnaire) 2 from various input variables, via stepwise regression. The chosen model comprises dialogue length in turns, task completion (as manually annotated in the WOZ data), and the multimodal user score from the user questionnaire, as shown in Equation 2. For the information presentation"
P08-1073,W03-2111,0,\N,Missing
P08-1073,C98-2214,0,\N,Missing
P08-2019,W03-2123,1,0.734069,"owever, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Decision Making with MM POMDPs Q(at , bt ) ≈ 4 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state j i ,a updates (for f (rt−1 t−1 , ht )), statistical ASR-SLU j j (for P (ht |ut )/P (ht )), and a statistical user model i )). The state list is pruned as (for P (hjt |at−1 , rt−1 described in section 2, where the “core features” are the filled information slot values and whether they have been confirmed. For example, the system will merge two states which agree that the user only wants a cheap hotel, even if they disagree on the sequence of dialogue acts which lead to this information. It also never pr"
P08-2019,P03-1031,0,0.0137565,"s over POMDP states is similar to the approach in (Young et al., 2007), where POMDP belief states are represented using a set of partitions of POMDP states. For any set of partitions, the mixture model approach could express the same model by defining one MDP state per partition and giving it a uniform distribution inside its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Decision Making with MM POMDPs Q(at , bt ) ≈ 4 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state j i ,a updates (for f (rt−1 t−1 , ht )), statistical ASR-SLU j j (for P (ht |ut )/P (ht )), and a statistical user model i )). The state list is pr"
P08-2019,2007.sigdial-1.11,1,0.813542,"bed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager. Thus the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning. For the user model, the system uses an ngram user model, as described in (Georgila et al., 2005), trained on the annotated TownInfo corpus.2 The system’s dialogue management policy is a Mixture Model Q-MDP (MM Q-MDP) policy. As with the MDP states, the MDP Q function is from (Lemon and Liu, 2007). It was trained in an MDP system using reinforcement learning with simulated users (Lemon and Liu, 2007), and was not modified for use in our MM Q-MDP policy. We tested this system with 10 different users, each attempting 9 tasks in the TownInfo domain (searching for hotels and restaurants in a fictitious town), resulting in 90 test dialogues. The users each attempted 3 tasks with the MDP system of (Lemon and Liu, 2007), 3 tasks with a state-of-the-art handcoded system (see (Lemon et al., 2006)), and 3 tasks with the MM Q-MDP system. Ordering of systems and tasks was controlled, and 3 of the"
P10-1008,P93-1031,0,0.168504,"2 Related work There are several ways in which natural language generation (NLG) systems adapt to users. Some of them adapt to a user’s goals, preferences, environment and so on. Our focus in this study is restricted to the user’s lexical domain expertise. Several NLG systems adapt to the user’s domain expertise at different levels of generation text planning (Paris, 1987), complexity of instructions (Dale, 1989), referring expressions (Reiter, 1991), and so on. Some dialogue systems, such as COMET, have also incorporated NLG modules that present appropriate levels of instruction to the user (McKeown et al., 1993). However, in all the above systems, the user’s knowledge is assumed to be accurately represented in an initial user model using which the system adapts its language. In contrast to all these systems, our adaptive REG policy knows nothing about the user when the conversation starts. Rule-based and supervised learning approaches have been proposed to learn and adapt during the conversation dynamically. Such systems learned from the user at the start and later adapted to the domain knowledge of the users. However, they either require expensive expert knowledge resources to hand-code the inferenc"
P10-1008,C94-2197,0,0.749816,"dialogue partners learn about each other and adapt their language to suit their domain expertise (Issacs and Clark, 1987). This kind of adaptation is called Alignment through Audience Design (Clark and Murphy, 1982; Bell, 1984). We assume that users are mostly unknown to the system and therefore that a spoken dialogue system (SDS) must be capable of observing the user’s dialogue behaviour, modelling his/her domain knowledge, and adapting accordingly, just like human interlocutors. Rule-based and supervised learning approaches to user adaptation in SDS have been proposed earlier (Cawsey, 1993; Akiba and Tanaka, 1994). However, such methods require expensive resources such as domain experts to hand-code the rules, or a corpus of expertlayperson interactions to train on. In contrast, we present a corpus-driven framework using which a user-adaptive REG policy can be learned using RL from a small corpus of non-adaptive humanmachine interaction. We show that these learned policies perform better than simple hand-coded adaptive policies in terms of accuracy of adaptation and dialogue Introduction We present a reinforcement learning (Sutton and Barto, 1998) framework to learn user-adaptive referring expression g"
P10-1008,E09-1078,1,0.521533,"Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 69–78, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics cessfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Rieser and Lemon, 2009; Hernandez et al., 2003; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In contrast, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Earlier, we reported a proof-of-concept work using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c). time. We also compared the performance of policies learned using a hand-coded rule-based simulation and a data-driven statistical simulation and show that data-driven simulations produce be"
P10-1008,P10-1103,1,0.480187,"sociation for Computational Linguistics, pages 69–78, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics cessfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Rieser and Lemon, 2009; Hernandez et al., 2003; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In contrast, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Earlier, we reported a proof-of-concept work using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c). time. We also compared the performance of policies learned using a hand-coded rule-based simulation and a data-driven statistical simulation and show that data-driven simulations produce better policies than rule-based ones. In section 2,"
P10-1008,P89-1009,0,0.0248878,"and section 5 describe the dialogue system framework and the user simulation models. In section 6, we present the training and in section 7, we present the evaluation for different REG policies. 2 Related work There are several ways in which natural language generation (NLG) systems adapt to users. Some of them adapt to a user’s goals, preferences, environment and so on. Our focus in this study is restricted to the user’s lexical domain expertise. Several NLG systems adapt to the user’s domain expertise at different levels of generation text planning (Paris, 1987), complexity of instructions (Dale, 1989), referring expressions (Reiter, 1991), and so on. Some dialogue systems, such as COMET, have also incorporated NLG modules that present appropriate levels of instruction to the user (McKeown et al., 1993). However, in all the above systems, the user’s knowledge is assumed to be accurately represented in an initial user model using which the system adapts its language. In contrast to all these systems, our adaptive REG policy knows nothing about the user when the conversation starts. Rule-based and supervised learning approaches have been proposed to learn and adapt during the conversation dyn"
P10-1008,N07-2038,0,0.0426963,"statistical distribution of in-built knowledge profiles that determines the dialogue behaviour of the user being simulated. If the user does not know a referring expression, then he is more likely to request clarification. If the user is able to interpret the referring expressions and identify the references then he is more likely to follow the system’s instruction. This behaviour is simulated by the action selection models described below. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to handle data sparsity problem. The user simulation (US) receives the system action As,t and its referrin"
P10-1008,W09-3916,1,0.949189,"uests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Rieser and Lemon, 2009; Hernandez et al., 2003; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In contrast, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Earlier, we reported a proof-of-concept work using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c). time. We also compared the performance of policies learned using a hand-coded rule-based simulation and a data-driven statistical simulation and show that data-driven simulations produce better policies than rule-based ones. In section 2, we present some of the related work. Section 3 presents the dialogue data that we used to train the user simulation. Section 4 and section 5 describe the dialogue system framework and the user simulation models. In section 6, we present the training and in section 7, we present the evaluation for different REG policies. 2 Related work There are several wa"
P10-1008,W09-0614,1,0.947158,"uests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Rieser and Lemon, 2009; Hernandez et al., 2003; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In contrast, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Earlier, we reported a proof-of-concept work using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c). time. We also compared the performance of policies learned using a hand-coded rule-based simulation and a data-driven statistical simulation and show that data-driven simulations produce better policies than rule-based ones. In section 2, we present some of the related work. Section 3 presents the dialogue data that we used to train the user simulation. Section 4 and section 5 describe the dialogue system framework and the user simulation models. In section 6, we present the training and in section 7, we present the evaluation for different REG policies. 2 Related work There are several wa"
P10-1008,W09-0611,1,\N,Missing
P10-1103,P04-1009,0,0.0150203,"most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a user modelling approach, where attributes are ranked according to user preferences (Demberg and Moore, 2006; Winterboer et al., 2007). Our data collection (see Section 3) a"
P10-1103,E06-1009,0,0.474188,"ation to the user (for example helping them to feel confident that they have a Previous work on Information Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has"
P10-1103,P08-1055,0,0.393306,"n presenting “enough” information to the user (for example helping them to feel confident that they have a Previous work on Information Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content o"
P10-1103,P08-1073,1,0.495387,"Missing"
P10-1103,P08-2019,1,0.764383,"eser and Lemon, 2008). 4.1 User Simulations User Simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. 4 However, this NLG predicted user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, 4 Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008; Gasic et al., 2008) for estimation of user act probabilities. 1012 the NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. We are most interested in the following user reactions: 1. select: the user chooses one of the presented items, e.g. “Yes, I’ll take that one.”. This reply type indicates that the Information Presentation was sufficient for the user to make a choice. 2. addInfo: The user provides more attributes, e.g. “I want something cheap.”. This reply type indicates that the user has more specific requests, which s/he wants to spec"
P10-1103,W09-3916,1,0.726511,")), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P (au,t |IPs,t , attributess,t ). 5 Where au,t is the predicted next user action at time t, IPs,t was the system’s Information Presentation action at t, and attributess,t is the attributes selected by the system at t. We evaluate the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by, e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compare the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context, see table 2. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies. The absolute discounting method for the bigram model is most dissimilar to the data, as is the WittenBell method for the tri-gram model, i.e. the models using these discounting methods have the highest KL score. The best performing meth"
P10-1103,P10-1008,1,0.546478,"Missing"
P10-1103,W08-0111,0,0.0593541,"ormation Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker,"
P10-1103,polifroni-walker-2006-learning,0,0.0266139,"2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a user modelling approach, where attributes are ranked according to user preferences (Demberg and Moore, 2006; Winterboer et al., 2007). Our data collection (se"
P10-1103,E09-1078,1,0.920697,"ent or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in re"
P10-1103,P04-1011,0,0.0146641,"ces in total: 1465 wizard utterances and 771 user utterances. We automatically extracted 81 features (e.g #sentences, #DBhits, #turns, #ellipsis)1 from the XML logfiles after each dialogue. Please see (Rieser et al., 2009) 1 The full corpus and list of features is available at https://www.classic-project.org/corpora/ for more details. 3.2 NLG Realiser In the Wizard-of-Oz environment we implemented a NLG realiser for the chosen IP structures and attribute choices, in order to realise the wizards’ choices in real time. This generator is based on data from the stochastic sentence planner SPaRKy (Stent et al., 2004). We replicated the variation observed in SPaRKy by analysing high-ranking example outputs (given the highest possible score by the SPaRKy judges) and implemented the variance using dynamic sentence generation. The realisations vary in sentence aggregation, aggregation operators (e.g. ‘and’, period, or ellipsis), contrasts 1011 (e.g. ‘however’, ‘on the other hand’) and referring expressions (e.g. ‘it’, ‘this restaurant’) used. The length of an utterance also depends on the number of attributes chosen, i.e. the more attributes the longer the utterance. All of these variations were logged. In pa"
P10-1103,W09-0626,0,0.139346,"Missing"
P10-1103,P01-1066,0,0.0730721,"Missing"
P10-1103,whittaker-etal-2002-fish,0,0.0168536,"m the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a"
P10-1103,W08-0119,0,\N,Missing
P12-3009,W11-2830,1,0.822091,"Missing"
P12-3009,W09-0611,1,\N,Missing
P13-1123,W11-2814,1,0.903062,"Missing"
P13-1123,P11-2115,1,0.328589,"Missing"
P13-1123,D12-1008,1,0.878448,"iming and Ordering Interaction Manager The Beluga is a great Italian restaurant y0 y1 y2 Micro-turn dialogue act, inform(food=Thai) semantics of user utterance intervening modules Semantic tree Lexical Syntactic Semantic features features features speech User String of words (synthesised) (b) Surface Realisation root root inform( name= Beluga) Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example A DDRESS A REA F OOD NAME P HONE P OSTCODE Q UALITY P RICE S IGNATURE V ENUE The venue’s address is . . . It is located in . . . The restaurant serves . . . cuisine. The restaurant’s name is . . . The venue’s phone number is . . . The postcode is . . . This is a . . . venue. It is located in the . . . price range. The venue specialises in . . . This venue is a . . . Table 1: Semantic slots required for our domain along with example realisations. Attributes can be"
P13-1123,W12-1509,1,0.93412,"iming and Ordering Interaction Manager The Beluga is a great Italian restaurant y0 y1 y2 Micro-turn dialogue act, inform(food=Thai) semantics of user utterance intervening modules Semantic tree Lexical Syntactic Semantic features features features speech User String of words (synthesised) (b) Surface Realisation root root inform( name= Beluga) Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example A DDRESS A REA F OOD NAME P HONE P OSTCODE Q UALITY P RICE S IGNATURE V ENUE The venue’s address is . . . It is located in . . . The restaurant serves . . . cuisine. The restaurant’s name is . . . The venue’s phone number is . . . The postcode is . . . This is a . . . venue. It is located in the . . . price range. The venue specialises in . . . This venue is a . . . Table 1: Semantic slots required for our domain along with example realisations. Attributes can be"
P13-1123,P12-1039,0,0.0291171,"lar importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In our case, they are produced by a reinforcement learning"
P13-1123,D09-1042,0,0.149375,"Missing"
P13-1123,P10-1157,0,0.230168,"ased generation which takes long-range dependencies into account outperforms several baselines. However, Lu et al.’s generator does not take context beyond the current utterance into account and is thus restricted to local features. Furthermore, their model is not able to modify generation results on the fly due to new or updated inputs. In terms of surface realisation from graphical models (and within the context of SDSs), our approach is also related to work by Georgila et al. (2002) and Dethlefs and Cuay´ahuitl (2011b), who use HMMs, Dethlefs and Cuay´ahuitl (2011a) who use Bayes Nets, and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the natural directionality between the input variables is unknown. In terms of surface realisation for SDSs, Oh and Rudnicky (2000) prese"
P13-1123,de-marneffe-etal-2006-generating,0,0.00319151,"Missing"
P13-1123,P06-1140,0,0.0206843,"ime SDS. This could present a problem in incremental settings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface r"
P13-1123,W00-0306,0,0.136525,", and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the natural directionality between the input variables is unknown. In terms of surface realisation for SDSs, Oh and Rudnicky (2000) present foundational work in using an n-gram-based system. They train a surface realiser based on a domain-dependent language model and use an overgeneration and ranking approach. Candidate utterances are ranked according to a penalty function which penalises too long or short utterances, repetitious utterances and utterances which either contain more or less information than required by the dialogue act. While their approach is fast to execute, it has the disadvantage of not being able to model long-range dependencies. They show that humans rank their output equivalently to template-based ge"
P13-1123,E09-1081,0,0.0386098,"sometimes be difficult to model, because they require rich contextawareness that keeps track of all (or much) of what was generated before, i.e. a growing generation history. In text generation, cohesion can span over the entire text. In interactive settings such as generation within a spoken dialogue system (SDS), a challenge is often to keep track of cohesion over several utterances. In addition, since interactions are dynamic, generator inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. This has been addressed by work on incremental processing (Schlangen and Skantze, 2009). Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). In this paper, we propose to formulate surface realisation as a sequence labelling task. We use conditional random fields (Lafferty et al., 2001; Sutton and McCallum, 2006), which are suitable for modelling rich contexts, in combination with semantic trees for rich linguistic information. This combination is able to keep track of"
P13-1123,W10-4301,0,0.00679094,"e seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utte"
P13-1123,E09-1085,0,0.00902622,"[barges in] No, I’m looking for a restaurant with good quality food. inform(quality=good [0.6], name=Beluga [0.6]) Oh sorry, so a nice restaurant located . . . [barges in] . . . in the city centre. inform(area=centre [0.8]) Table 4: Example dialogue where the dialogue manager needs to send incremental updates to the NLG. Incremental surface realisation from semantic trees for this dialogue is shown in Figure 3. because the authors tested only for Phrasing and Natural, respectively. 5 Incremental Surface Realisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and"
P13-1123,W06-1404,0,0.0319321,"ettings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In"
P13-1123,W03-2311,0,0.0285797,"lisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local co"
P13-1123,W12-1641,0,\N,Missing
P13-1123,W11-2813,1,\N,Missing
P13-1163,P09-2082,1,0.859812,"Missing"
P13-1163,D07-1074,0,0.0152147,") trained over 2013 annotated utterances. Once the question has been typed, QA proceeds to focus detection also using machine learning techniques (Mikhailsian et al., 2009). Detected foci include possibly anaphoric expressions (“Who was he?”, “Tell me more about the castle”). These expressions are resolved against the dialogue history and geographical context. QA then proceeds to a textual search on texts from the Gazetteer of Scotland (Gittings, 2012) and Wikipedia, and definitions from WordNet glosses. The task is similar to TAC KBP 2013 Entity Linking Track and named entity disambiguation (Cucerzan, 2007). Candidate answers are reranked using a trained confidence score with the top candidate used as the final answer. These are usually long, descriptive answers and are provided as a flow of sentence chunks that the user can interrupt (see table 2). The Interaction Manager queries the QA model and pushes information when a salient PoI is in the vicinity of the user. “Edinburgh’s most famous and historic thoroughfare, which has formed the heart of the Old Town since mediaeval times. The Royal Mile includes Castlehill, the Lawnmarket, the Canongate and the Abbey Strand, but, is officially known si"
P13-1163,P11-2115,0,0.0276641,"Missing"
P13-1163,W11-2830,1,0.844185,"c, mix navigation with exploration. But such apps present information primarily visually on the screen for the user to read. Some of these are available for download at the Google Play Android app store1 . Several dialogue and natural language systems have addressed the issue 1 of pedestrian navigation (Malaka and Zipf, 2000; Raubal and Winter, 2002; Dale et al., 2003; Bartie and Mackaness, 2006; Shroder et al., 2011; Dethlefs and Cuay´ahuitl, 2011). There has also been recent interest in shared tasks for generating navigation instructions in indoor and urban environments (Byron et al., 2007; Janarthanam and Lemon, 2011). Some dialogue systems deal with presenting information concerning points of interest (Ko et al., 2005; Kashioka et al., 2011) and interactive question answering (Webb and Webber, 2009). In contrast, Spacebook has the objective of keeping the user’s cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, it allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1). 3 Architecture The architecture of the Spacebook system is shown"
P13-1163,W12-1619,1,0.816848,"wering (Webb and Webber, 2009). In contrast, Spacebook has the objective of keeping the user’s cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, it allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1). 3 Architecture The architecture of the Spacebook system is shown in figure 1. Our architecture brings together Spoken Dialogue Systems (SDS), Geographic Information Systems (GIS) and QuestionAnswering (QA) technologies (Janarthanam et al., 2012). Its essentially a spoken dialogue system (SDS) consisting of an automatic speech recogniser (ASR), a semantic parser, an Interaction Manager, an utterance generator and a text-tospeech synthesizer (TTS). The GIS modules in this architecture are the City Model, the Visibility Engine, and the Pedestrian tracker. Users communicate with the system using a smartphone-based client app (an Android app) that sends users’ position, pace rate, and spoken utterances to the system, and delivers synthesised system utterances to the user. Figure 1: System Architecture https://play.google.com/store 1661 3."
P14-1116,D10-1049,0,0.0269502,"s. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include"
P14-1116,N04-1015,0,0.0190901,"evious methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. In cont"
P14-1116,W10-4217,0,0.0265119,"tween the two methods. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for cont"
P14-1116,E06-1040,0,0.0463478,"Missing"
P14-1116,W10-1301,0,0.0150193,"e carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras a"
P14-1116,W03-1016,0,0.0505137,"entation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boos"
P14-1116,W08-1113,0,0.0606181,"Missing"
P14-1116,P04-1044,1,0.73951,"e results of the production phase, i.e. the ensemble of LPs with the corresponding klabelsets, the set of labels L, and the new instance x and it outputs the result vector of predicted labels for instance x. During run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset. We used the standard parameter values of t, k and m (t = 0.5, k = 3 and m equals to 58 (2*29 templates)). In future, we could perform parameter optimisation by using a technique similar to (Gabsdil and Lemon, 2004). 5 Evaluation Firstly, we performed a preliminary evaluation on classification methods, comparing our proposed ML classification with multiple iterated classification approaches. The summaries generated by the ML classification system are then compared with the output of a RL system and two baseline systems in simulation and with real students. 5.1 Comparison with Simple Classification We compared the RAkEL algorithm with singlelabel (SL) classification. Different SL classifiers were trained using WEKA: JRip, Decision Trees, Naive Bayes, k-nearest neighbour, logistic regression, multi-layer p"
P14-1116,W13-2115,1,0.859104,"describe these factors in four different ways: 1. <trend>: referring to the trend of a factor over the semester (e.g. “Your performance was increasing...”), 2. <weeks>: explicitly describing the factor value at specific weeks (e.g. “In weeks 2, 3 and 9...”), 3. <average>: considering the average of a factor value (e.g. “You dedicated 1.5 hours studying on average...”), and 4. <other>: mentioning other relevant information (e.g. “Revising material will improve your performance”). For the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data (Gkatzia et al., 2013). As a result, for the same student there are various summaries provided by the different experts. This characteristic of the dataset, that each instance is associated with more than one solution, additionally motivates the use of multi-label classification, which is concerned with learning from examples, where each example is associated with multiple labels. Our analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student’s understanding of the material (Und), see Table 2. As we will d"
P14-1116,E14-4041,1,0.751077,"2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classifica"
P14-1116,P13-1138,0,0.0126066,"), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on"
P14-1116,P13-2100,0,0.0128051,"al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the t"
P14-1116,P10-1103,1,0.868322,"report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is simi"
P14-1116,H05-1042,0,\N,Missing
P16-2043,D10-1049,0,0.124444,"Missing"
P16-2043,J12-1004,0,0.0394575,"Missing"
P16-2043,W10-4217,0,0.239849,"Missing"
P16-2043,P08-1073,1,0.873259,"Missing"
P16-2043,W05-1615,0,0.0802978,"Missing"
P16-2043,E06-1045,0,0.0614969,"Missing"
P16-2043,W09-0613,0,0.0729274,"Missing"
P16-2043,W15-4720,1,0.905349,"human decision-making. We consider the exemplar task of weather forecast generation. We initially present two NLG strategies which present the uncertainty in the input data. The two strategies are based on (1) the World Meteorological Organisation (WMO) (Kootval, 2008) guidelines and (2) commercial forecast presentations (e.g. from BBC presenters). We then evaluate the strategies against a state-of-the-art graphical system (Stephens et al., 2011), which presents the uncertain data in a graphical way. Figure 1 shows an example of this baseline graphical presentation. We use a gamebased setup (Gkatzia et al., 2015) to perform taskbased evaluation, to investigate the effect that the different information presentation strategies have on human decision-making. Weather forecast generation is a common topic within the NLG community, e.g. (Konstas and Lapata, 2012; Angeli et al., 2010; Belz and Kow, 2010; Sripada et al., 2005). Previous approaches have not focused on how to communicate uncertain information or the best ways of referring to Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores or probabilities. We present a comparison of different information present"
P16-2043,N12-1093,0,\N,Missing
rieser-lemon-2008-automatic,2005.sigdial-1.11,1,\N,Missing
rieser-lemon-2008-automatic,J08-4002,1,\N,Missing
rieser-lemon-2008-automatic,P98-2219,0,\N,Missing
rieser-lemon-2008-automatic,C98-2214,0,\N,Missing
rieser-lemon-2008-automatic,P08-1073,1,\N,Missing
W02-0216,C98-2126,0,0.0540525,"Missing"
W02-0216,W01-1609,1,0.766997,"Missing"
W02-0216,H91-1070,0,0.110603,"Missing"
W02-0217,W02-0216,1,0.916163,"ility distributions provide a natural model of the ambiguities that thus arise. For these reasons it is natural to explore probabilistic representations and algorithms in dialogue management, rather than purely deterministic models. We have experience building deterministic dialogue managers (see e.g. (Lemon et al., 1 This research was (partially) funded under the Wallenberg laboratory for research on Information Technology and Autonomous Systems (WITAS) Project, Link¨oping University, by the Wallenberg Foundation, Sweden. Stanley Peters CSLI Stanford University peters@csli.stanford.edu 2001; Lemon et al., 2002)) which use deterministic context update rules. This paper briefly describes our construction of a Bayes Net modelling dialogue context. We will consider a series of examples of increasing complexity involving anaphoric resolution in Section 3.1. We will point out how they are to be resolved intuitively, and then discuss how our Bayesian net fares. We will see that many of the best insights of deterministic approaches (e.g. in the axiomatic BDI tradition and in the planning literature) can be preserved, often in less brittle forms, in a probabilistic setting. 1.1 Probabilistic modelling ideas"
W03-2114,P96-1009,0,0.0609557,"Missing"
W03-2114,W01-1609,0,0.0285283,"e.g. the user asked a question), or if a problem is detected (e.g. an ambiguity must be resolved). Since system communication is planned here, this layer is also the one that interacts with the rest of the agent architecture: any goals, state-changes, or observations that the agent may wish to communicate are added as communicative goals, typically via the Activity Model. For command-and-control applications (e.g. guiding a robot or UAV), system-initiated utterances tend to be fairly short and simple and conversation-planning is minimal; however, for our dialogue-enabled tutorial application (Clark et al., 2001), conversation-planning is quite complex and the system may generate multiple, relatively long utterances on its own initiative. 5 Low-level Conversation Management: Maintaining the Communication Channel We currently employ a range of shallow processing techniques to maintain a smooth interaction with the human dialogue participant. By “shallow processing” we mean processing that does not necessarily result in or concern itself with the semantic representation or pragmatic interpretation of the utterance in the context of the dialogue. In particular, information at this level is not processed"
W03-2114,W02-0216,1,0.865707,"uts are respected without interruption (except when necessary); turn passes to the appropriate participant, based on the highest priority Agenda item and the dialogue move that generated it; generated outputs are natural and timely; recognized user inputs are acknowledged quickly using simple feedback utterances. The upper level is responsible for modeling other aspects of the conversational context, as well as communicative goals and intentions. The content (i.e. logical forms) of user utterances are processed using the dialogue model (e.g. updates and adding nodes to the Dialogue Move Tree (Lemon et al., 2002b)), and system utterances are constructed which are in line with the system’s communicative Content layer: - utterance planning - communicative intentions - grounding - content management - interaction with agent arch Context Dialogue Mgr Move Tree Conversation Planner Agent Activity Model - intentions - goals - plans - observations ack Interaction layer - timing - form - engagement - acknowledgement Backup Shallow Processor (Helper) Speech recogition and Parsing Attention Monitor Speech channel Generation Module Output Agenda Turn Mgr Generation: - anaphora - pronouns - aggregation - echoing"
W03-2114,J86-3001,0,0.203097,"mented with. In the rest of the paper, we discuss our dialogue management architecture and, in particular, the techniques employed so far at each of the two levels described here to enhance user experience and improve overall system performance. The current implementation based on the above architecture is still being refined; we focus on the features that have already been implemented. 4 Top-Level Context Management The approach to dialogue modeling we have implemented is based on the theory of dialogue games (Carlson, 1983; Power, 1979), and, for task-oriented dialogues, discourse segments (Grosz and Sidner, 1986). These accounts rely on the observation that answers generally follow questions, commands are usually acknowledged, and so on, so that dialogues can be partially described as consisting of adjacency pairs of such dialogue moves. The notion of “attachment” of dialogue moves on a Dialogue Move Tree (DMT) (Lemon et al., 2002b) embodies this idea. An Activity Tree represents hierarchical and temporal information about the task-state of the dialogue. Activities are the joint tasks managed by the dialogue: e.g. booking a flight or moving a robot— again, see (Lemon et al., 2002b) for details. Nodes"
W03-2123,P93-1008,0,0.0306061,"apply effects(+Effects); (2) the DME agent can call other agents directly, in particular if it is not interested in the results of those requests; (3) the DME agent can use the DME server as a mediating agent, normally when the results are needed for updating the information state of the DME. The advantage of this architecture is the flexibility imposed by it, while at the same time allowing asynchronous interaction of the input/output and supporting agents with the dialogue move engine. 2.4 Supporting Agents OAA itself comes with agents for parsing and generating based on the Gemini system (Dowding et al., 1993). DIPPER provides a further set of agents to deal with natural language understanding, based on Discourse Representation Theory (Kamp and Reyle, 1993). There is an ambiguity resolution agent that resolves underspecified DRSs into fully resolved DRSs, and there is an inference agent that checks consistency of DRSs, using standard firstorder theorem proving techniques, including the theorem prover SPASS (Weidenbach et al., 1999) and the model builder MACE (McCune, 1998). DIPPER also includes a high-level dialogue planning component using O-Plan (Currie and Tate, 1991) which can be used to build"
W03-2123,E03-3005,0,0.0265549,"Missing"
W03-2123,C02-1067,1,\N,Missing
W09-0611,W09-0614,1,0.309206,"nular fashion. In this model, the user’s domain knowledge profile is factored into lexical (LKu,t ), factual (F Ku,t ) and procedural knowledge (P Ku,t ) components. 75 Lexical knowledge LKu,t vocab([modem, router], dobj1) vocab([wireless, WiFi], dobj3) vocab([modem power light], dobj7) Factual knowledge F Ku,t location(dobj1) location(dobj7) Procedural knowledge P Ku,t procedure(replace filter) procedure(refresh page) current conditional probabilities were set by hand based on intuition. In future work, these values will be populated based on simple knowledge surveys performed on real users (Janarthanam and Lemon, 2009). This method creates a spectrum of users from ones who have no knowledge of technical terms to ones who know all the technical jargon, though every profile will have a different frequency of occurrence. This difference in frequency reflects that expert users are less common than novice users. The user’s domain knowledge can be dynamically updated. The new REs, both technical and descriptive, presented by the system through clarification moves are stored in the user’s short term memory. Exactly how long (in terms of dialogue turns) to retain the newly acquired knowledge is given by a retention"
W09-0611,P03-1033,0,0.0263482,"ies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that simulates a variety of users with different domain knowledge profiles. Although this model incorporated clarification acts at the conceptual level, these users ignore the issues concerning the user’s understanding of the REs used by the system. In this work, in contrast to the above, we present a User Simulation model which explicitly encodes the user’s lexical knowledge of the"
W09-0611,E09-1078,1,0.756017,"statistical user model which incorporates the lexical knowledge of different users. We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are significantly better than adaptive hand-coded policies for this problem. The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies. 1 In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” instead of “ADSL filter” and “monitor symbol” instead of “network icon”, because it learnt that the user is a"
W09-0611,N07-2038,0,0.0858188,"then present a reinforcement learning model of lexical alignment due to audience design (in sections 4 & 5). We then evaluate the User Simulation (section 6), testing whether a simulation that is sensitive to a system’s RE choices can be used to learn good lexical alignment policies. Finally, we compare policies learned in interaction with the User Simulation with hand-coded policies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that"
W09-0611,W04-2325,0,0.0139902,"description is issued when the SDS uses technical terms that the simulated user does not know, e.g. “What is a router?”. Request verification is issued when the SDS uses descriptive lexical items for domain objects that the user knows more technical terms for, e.g. System: “Is the black box plugged in?” User: “Do you mean the router?”. Request disambiguation is issued when the user faces an underspecified and ambiguous descriptive expression, e.g.“User: I have two black boxes here - one with lights and one without. Which one is it?”. These clarification strategies have been modeled based on (Schlangen, 2004). The user simulation also issues request location and request procedure dialogue acts, when it does not know the location of domain objects or how to manipulate them, respectively. 3.3 Environment simulation The environment simulation includes both physical objects, such as the computer, modem, ADSL filter, etc and virtual objects, such as the browser, control panel, etc in the user’s environment. Physical and virtual connections between these objects Figure 2: Bayes Net for User Lexical Knowledge Using this Bayesian model, we instantiate different knowledge profiles for different users. The"
W09-0611,W07-0301,0,0.0172318,"s from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” instead of “ADSL filter” and “monitor symbol” instead of “network icon”, because it learnt that the user is a novice based on their clarification requests. However, it switches to using technical terms like “browser”, when it learns that the user is not a complete novice (as he verifies the description for the network icon in Usr 4). Introduction In current “troubleshooting” spoken dialogue systems (SDS)(Williams, 2007), the major part of the conversation is directed by the system, while the user follows the system’s instructions. Once the system decides what instruction to give the user (at the dialogue management level), it faces several decisions to be made at the natural language generation (NLG) level. These include, deciding which concepts to include in the utterance, deciding the referring expressions (RE) to use in the utterance and so on. A little-studied problem is to what extent a system could automatically align to the user’s lexical knowledge by adapting its RE choices, in particular based on hi"
W09-0614,W09-0611,1,0.311046,"essions (RE) in a situated dialogue context. We also study the effect of the system’s lexical alignment due to priming (Pickering and Garrod, 2004) by the user’s choice of REs. The users follow instructions from an implemented dialogue manager and realiser to perform a technical but realistic task – setting up a home Internet connection. The dialogue system’s utterances are manipulated to contain different types of REs - descriptive, technical, tutorial or lexically aligned REs, to refer to various domain objects in the task. The users’ responses to different REs are then logged and studied. (Janarthanam and Lemon, 2009) presented a framework for reinforcement learning of optimal natural language generation strategies to choose appropriate REs to users with different domain knowledge expertise. For this, we need user simulations with different domain knowledge profiles that are sensitive to the system’s choice of REs. A WoZ environment is an ideal tool for data collection to build data-driven user simulations. However, our study requires a novel WoZ environment. In section 2, we present prior related work. Section 3 describes the task performed by partici3 The Domain Task In this experiment, the task for each"
W09-0614,W06-1420,0,0.070379,"Missing"
W09-0614,whittaker-etal-2002-fish,0,\N,Missing
W09-3916,W09-0614,1,0.909523,"spoken dialogue systems using reinforcement learning methods. An adaptive REG policy equips a dialogue system to dynamically modify its utterances in order to adapt to user’s domain knowledge level. For instance, to refer to domain objects, the system might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level dialogue acts. Also, they do not simulate different user groups like experts, novices, etc. In order to learn adaptive REG policies, user simulations need to respond to the system’s choice of referring expressio"
W09-3916,W09-0611,1,0.920294,"spoken dialogue systems using reinforcement learning methods. An adaptive REG policy equips a dialogue system to dynamically modify its utterances in order to adapt to user’s domain knowledge level. For instance, to refer to domain objects, the system might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level dialogue acts. Also, they do not simulate different user groups like experts, novices, etc. In order to learn adaptive REG policies, user simulations need to respond to the system’s choice of referring expressio"
W09-3916,2005.sigdial-1.6,0,0.0647818,"and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P ||Q) = 1 N PN DKL (P ||Q)+DKL (Q||P ) 2 PM pi i=1 pi ∗ log( qi ) i=1 DKL (P ||Q) = The metric measures the divergence between distributions P and Q in N different contexts with M responses per context. Ideally, the dialogue similarity between two similar distributions is close to zero. if (C(ei ) &gt; 0) t (N +T )(V −T ) T (N +T )V While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 4.5 Equal Probability model baseline C(ei ) N +T + 5 Metrics for evaluation of simulations P (EAu,t |As,t , As,t−1 ) P ∗ (ei ) = C(ei ) N +T if (C(ei ) = 0) On analysis, we found that the Witten-Bell discounting assigns greater probability to unobserved responses than to observed responses, i"
W09-3916,N07-2038,0,0.0852981,"might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level dialogue acts. Also, they do not simulate different user groups like experts, novices, etc. In order to learn adaptive REG policies, user simulations need to respond to the system’s choice of referring expressions and simulate user groups with different knowledge levels. We propose a two-tier simulation which simulates users with different knowledge levels and is sensitive to the system’s choice of referring expressions. Proceedings of SIGDIAL 2009: the 1"
W09-3916,whittaker-etal-2002-fish,0,\N,Missing
W09-3922,E06-2009,1,0.749865,"take next, such as asking for price range after establishing preferred cuisine type. General aspects of dialogue, such as confirmation and clarification strategies, are handled by the domain-general DM. Values for constraints on transitions and branching in the BPM, for example “present insurance offer if the user is business-class”, are compiled into domainspecific parts of the Information State. XML format is used for BPMs, and they are compiled into finite state machines consulted by the spoken dialogue system. The domain-general dialogue manager was mostly abstracted from the TALK system (Lemon et al., 2006). 2 The DUDE Development Environment Figure 1 shows the DUDE Development Environment architecture whereby the main algorithm takes the business-user resources and databases as input and uses these to automatically generate the spoken dialogue system which includes a Voice XML generator. Advantages of using businessuser resources such as Business Process Models (BPM) (Williams, 1967) include the fact that graphical interfaces and authoring environments are widely available (e.g. Eclipse). In addition, business-user resources can contain a lot of additional information as well as call flow inclu"
W09-3922,W03-2123,1,\N,Missing
W10-4235,P08-2019,1,0.913578,"ion, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decision making according to a cost function (van Deemter, 2009). It would be very interesting to explore how different approaches perform in NLG problems where different types of uncertainty"
W10-4235,W05-1606,0,0.0129602,"dule), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain."
W10-4235,W09-3916,1,0.828479,"e Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P (au,t |IPs,t )), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P (au,t |IPs,t , attributess,t ). We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies (Rieser et al., 2010). 2.2 Other Simulated Components In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations f"
W10-4235,P10-1008,1,0.922406,"ces....), • the likely user reaction to the generated output, • the behaviour of related components (e.g. a surface realiser, or TTS module), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applic"
W10-4235,E09-1078,1,0.875208,"s of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010),"
W10-4235,P10-1103,1,0.890218,"count appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decis"
W10-4235,P04-1011,0,0.0326812,"omponents In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations for these components to capture their inherent uncertainty, again, is an interesting challenge. For example, one might want to adapt the generated output according to the predicted TTS quality. Therefore, one needs a model of the expected/ predicted TTS quality for a TTS engine (Boidin et al., 2009). Furthermore, NLG decisions might be inputs to a stochastic sentence realiser, such as SPaRKy (Stent et al., 2004). However, one might not have a fully trained stochastic sentence realiser for this domain (yet). In (Rieser et al., 2010) we therefore modelled the variance as observed in the top ranking SPaRKy examples. 2.3 Generating Referring Expressions under uncertainty In this section, we present an example user simulation (US) model, that simulates the dialogue behaviour of users who react to referring expressions depending on their domain knowledge. These external simulation models are different from internal user models used by dialogue systems. In 2 Where au,t is the predicted next user action at t"
W10-4235,W09-0626,0,0.0512972,"Missing"
W10-4235,W08-0119,0,\N,Missing
W10-4324,W09-3916,1,0.894162,"mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Figure 1: System User Interaction (learning) 3.1 Dialogue Manager The dialogue manager identifies the next dialogue act (As,t where t denotes turn, s denotes system) to give to the user based on the dialogue management policy πdm . The dialogue management is coded in the form of a finite state machine. In this dialogue task, the system provides instructions to either obser"
W10-4324,W09-0614,1,0.934841,"mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Figure 1: System User Interaction (learning) 3.1 Dialogue Manager The dialogue manager identifies the next dialogue act (As,t where t denotes turn, s denotes system) to give to the user based on the dialogue management policy πdm . The dialogue management is coded in the form of a finite state machine. In this dialogue task, the system provides instructions to either obser"
W10-4324,W09-0611,1,0.899396,"mation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Figure 1: System User Interaction (learning) 3.1 Dialogue Manager The dialogue manager identifies the next dialogue act (As,t where t denotes turn, s denotes system) to give to the user based on the dialogue management policy πdm . The dialogue management is coded in the form of a finite state machine. In this dialogue task, the system provides instructions to either obser"
W10-4324,P10-1008,1,0.20341,"al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Figure 1: System User Interaction (learning) 3.1 Dialogue Manager The dialogue manager identifies the next dialogue act (As,t where t denotes turn, s denotes system) to give to the user based on the dialogue management policy πdm . The dialogue management is coded in the form of a finite state machine. In this dialogue task, the system provides instructions to either observe or manipulate the environment. When users ask for clarifications on referring expressions, the system clarifies (provide clar) by giving information to enable the user to associate the expression with the intended refe"
W10-4324,C94-2197,0,0.236744,"ent the training and in section 6, we present the evaluation for different REG policies with real users. 2 Related work Rule-based and supervised learning approaches have been proposed to learn and adapt during conversations dynamically. Such systems learn from a user at the start and later adapt to the domain knowledge of the user. However, they either require expensive expert knowledge resources to hand-code the inference rules (Cawsey, 1993) or a large corpus of expert-layperson interaction from which adaptive strategies can be learned and modelled, using methods such as Bayesian networks (Akiba and Tanaka, 1994). In contrast, we present an approach that learns in the absence of these expensive resources. It is also not clear how supervised approaches choose between when to seek more information and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise"
W10-4324,E09-1078,1,0.811902,"tion and when to adapt. In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Fi"
W10-4324,P10-1103,1,0.728153,"In this study, we show that using reinforcement learning this decision is learned automatically. Reinforcement Learning (RL) has been successfully used for learning dialogue management policies since (Levin et al., 1997). The learned policies allow the dialogue manager to optimally choose appropriate dialogue acts such as instructions, confirmation requests, and so on, under uncertain noise or other environment conditions. There have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning (Hernandez et al., 2003; Rieser and Lemon, 2009; Rieser and Lemon, 2010), and joint optimisation of Dialogue Management and NLG using hierarchical RL has been proposed by (Lemon, 2010). In addition, we present a framework to learn to choose appropriate referring expressions based on a user’s domain knowledge. Following a proof-of-concept study using a hand-coded rule-based user simulation (Janarthanam and Lemon, 2009c), we previously showed that adaptive REG policies can be learned using an RL framework with datadriven user simulations and that such policies perform better than simple hand-coded policies (Janarthanam and Lemon, 2010). 3 Figure 1: System User Inter"
W10-4324,W10-4204,0,0.0273673,"Missing"
W10-4324,N07-2038,0,0.0304823,"initial knowledge helps to predict the user’s knowledge of the rest of the referring expressions. RECs,t = {(R1 , T1 ), ..., (Rn , Tn )} In the evaluation mode, a trained REG policy interacts with unknown users. It consults the learned policy πreg to choose the referring expressions based on the current user model. 4 User Simulations In this section, we present user simulation models that simulate the dialogue behaviour of a real human user. Several user simulation models have been proposed for use in reinforcement learning of dialogue policies (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007; Ai and Litman, 2007). However, they are suited only for learning dialogue management policies, and not natural language generation policies. In particular, our model is the first to be sensitive to a system’s choices of referring expressions. Earlier, we presented a two-tier simulation trained on data precisely for REG policy learning (Janarthanam and Lemon, 2009a). However, it is not suited for training on small corpus like the one we have at our disposal. In contrast to the earlier model, we now condition the clarification requests on the referent class rather than the referent itself to h"
W11-2002,2007.sigdial-1.23,1,\N,Missing
W11-2017,N07-1053,0,0.0734426,"Missing"
W11-2017,N06-1018,0,0.0202095,"ns are linguistic expressions that are used to refer to a date and are often a source of confusion in human-human, human-computer and text interactions such as emails and instant messaging. For example, “Let’s meet next Sunday”– “do you mean Sunday this week or a week on Sunday?”. (Mccoy and Strube, 1999) state that changes in temporal structure in text are often indicated by either Much work in the field of Natural Language Processing concerns understanding and resolving these temporal expressions in text (Gerber et al., 2002; Pustejovsky et al., 2003; Ahn et al., 2007; Mazur and Dale, 2007; Han et al., 2006), however, little work has looked at how best to plan and realise temporal expressions in order to minimize ambiguity and confusion in a Spoken Dialogue System (SDS). (Reiter et al., 2005) presented a data driven approach to generating TEs to refer to time in weather forecast information where appropriate expressions were identified using contextual features using supervised learning. We adopt an adaptive, data-driven reinforcement learning approach instead. Similar data-driven approaches have been applied to information presentation (Rieser et al., 2010; Walker et al., 2007) where each Natura"
W11-2017,P10-1008,1,0.720143,"appropriate expressions were identified using contextual features using supervised learning. We adopt an adaptive, data-driven reinforcement learning approach instead. Similar data-driven approaches have been applied to information presentation (Rieser et al., 2010; Walker et al., 2007) where each Natural Language Generation (NLG) action is a sequential decision point, based on the current dialogue context and expected long-term reward of that action. A data-driven approach has also been applied to the problem of referring expression generation in dialogue for expert and noviceusers of a SDS (Janarthanam and Lemon, 2010). However, to date, there has been no previous work on adaptive data-driven approaches for temporal referring expression generation, where uncertainty in 142 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142–151, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics the stochastic environment is explicitly modelled. The data-driven approach to temporal expression generation presented here is in the context of appointment scheduling dialogues. The fact that there are multiple ways that a tim"
W11-2017,J88-2003,0,0.0258364,"Missing"
W11-2017,P10-1103,1,0.822432,"Missing"
W11-2801,W10-4235,1,\N,Missing
W11-2801,W10-4324,1,\N,Missing
W11-2801,E09-1078,1,\N,Missing
W11-2801,W11-2017,1,\N,Missing
W11-2801,P10-1103,1,\N,Missing
W11-2801,P10-1008,1,\N,Missing
W11-2813,W11-2002,1,0.774392,"s baseline system has been made accessible by phone using VoIP technology, enabling outof-lab evaluation with large numbers of users. Apart from practical advantages in managing evaluation campaigns, this development effort was also intended as a step towards evaluating spoken dialogue systems under more realistic conditions. Please note, however, that the users in this evaluation were still recruited and asked to complete predefined tasks (see Section 4), and therefore the evaluation might not be as realistic as an evaluation of a final deployed application with real users having real goals (Black et al., 2011). The speech recogniser (ASR), semantic parser (SLU) and dialogue manager (DM) have all been developed at Cambridge University. For speech synthesis (TTS), the Baratinoo synthesiser, developed at France Telecom, was used. The DM uses a POMDP (Partially Observable Markov Decision Process) framework, allowing it to process N-Best lists of ASR hypotheses and keep track of multiple dialogue state hypotheses. The DM policy is trained to select system dialogue acts given a probability distribution over possible dialogue states. It has been shown that such dialogue managers can exploit the informatio"
W11-2813,W09-0626,0,0.0608888,"Missing"
W11-2813,E06-1009,0,0.0254445,"ommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked according to a pre-defined u"
W11-2813,P11-2115,0,0.0999679,"Missing"
W11-2813,W11-2011,0,0.0422671,"Missing"
W11-2813,W10-4324,1,0.802997,"hile keeping the utterances short and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to automatically optimise the IP policy with respect to a data-driven objective function. In previous simulation-based work, we demonstrated that this IP model “locally”"
W11-2813,W11-2017,1,0.752589,"hort and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to automatically optimise the IP policy with respect to a data-driven objective function. In previous simulation-based work, we demonstrated that this IP model “locally” outperforms other IP strat"
W11-2813,W08-0111,0,0.0290224,"ochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked according to a pre-defined user model (i.e. cuisine, price range, location, food quality, and service quality). We formulate the problem as a Markov Decision Process (MDP), where states are dialogu"
W11-2813,P08-1055,0,0.0216807,"retrieved items and then recommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked ac"
W11-2813,E09-1078,1,0.933763,"ocal” NLG task is to present “enough” information to the user (for example helping them to feel confident that they have a good overview of the search results) while keeping the utterances short and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to"
W11-2813,J11-1006,1,0.88254,"Missing"
W11-2813,P10-1103,1,0.901663,"Missing"
W11-2813,P04-1011,0,0.0355313,"g (RL) (Sutton and Barto, 1998), where the example task is to present a set of search results (e.g. restaurants) to users. In particular, we consider 7 possible policies for structuring the content (see Figure 1): Recommending one single item, comparing two items, summarising all items, or ordered combinations of those actions, e.g. first summarise all the retrieved items and then recommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOM"
W11-2830,P11-2115,0,0.134956,"Missing"
W11-2830,W11-2011,0,0.0261834,"Missing"
W11-2830,gargett-etal-2010-give,0,0.0924004,"’s location and viewshed (i.e. what objects in the world are in the user’s view). In the first version of the challenge, the users moved “block by block” in a grid-based virtual environment. Therefore it was possible to give instructions such as “move 3 steps forward”. Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 208–211, c Nancy, France, September 2011. 2011 Association for Computational Linguistics Figure 1: An outdoor virtual environment from SecondLife However in the latest version of this challenge, the users move continuously and not discretely (Gargett et al., 2010). This challenge examined the issues concerning generating instructions and referring expressions in situated contexts. Our proposed challenge is similar to the GIVE challenge in the sense that it involves systems generating instructions for navigation, and generating referring expressions to refer to entities in the world. But in contrast, in this challenge, we propose to use an outdoor virtual environment where route instruction giving and referring to outdoor entities would be for pedestrian navigation in city-like environments, involving issues such as uncertainty in user’s location and vi"
W11-2830,W10-4235,1,0.739703,"o verbalize them. Research has shown that inclusion of landmarks in route instructions is highly effective (May et al., 2003; Schroder et al., 2011). In order to include landmarks in instructions, decisions such as which landmarks to include, how best to refer to them, and so on, must be taken. Another interesting issue for real-world route-giving is that it is not always possible to know where the user is, or where they are looking. Even when using tools like GPS trackers, there is an element of uncertainty in the pedestrian user’s location, so generation under uncertainty becomes important (Lemon et al., 2010). Finally, instructions and referring expressions should also take into account the pedestrian’s 208 field of view or “viewshed”, which is not directly observable but may be inferred from uncertain information about location and orientation. Virtual environments provide an important development and test infrastructure for real-world systems. They avoid the need for costly and time-consuming real-world experiments and datacollections, while allowing manipulation of the spatial environment to investigate specific issues and contexts for NLG systems. There is therefore an interesting and practica"
W12-1509,D10-1049,0,0.0275246,"ave also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left"
W12-1509,W11-2015,0,0.0714349,"rved in human-human conversation. Doing this in a deterministic fashion through hand-written rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010;"
W12-1509,W10-4342,0,0.253369,"n rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has esta"
W12-1509,W11-2814,1,0.901339,"Missing"
W12-1509,W11-2011,1,0.88201,"Missing"
W12-1509,W09-3902,0,0.0484885,"ontent selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type and, at the same time, produce arbitrarily complex output which is monitored and can be modified at any time. On the other hand, current models are based on deterministic decision making and thus shar"
W12-1509,P10-1008,1,0.261639,"In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data,"
W12-1509,P10-1157,0,0.111541,"Missing"
W12-1509,W03-2311,0,0.489373,"remental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-Adjoining Grammar, and Purver and Otsuka (2003) who define an incremental generator based on Dynamic Syntax. Both of these generators can monitor their own output and initiate corrections if necessary. Over recent years, adaptive and data-driven ap49 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49–58, c Utica, May 2012. 2012 Association for Computational Linguistics Self-correction (the system made a mistake) USR I want Italian food in the centre of town . . . SYS OK. I found 35 Indian restaurants . . . USR No, I want Italian. SYS oh sorry . . . SYS I have 24 Italian restaurants in the city c"
W12-1509,N09-1071,0,0.0613976,"c. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division of NLG levels into utterance (IU1), content selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type"
W12-1509,P10-1103,1,0.862448,"and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contex"
W12-1509,E09-1081,0,0.20724,"explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left buffer of a module, such as the NLG module, receives IUs from one or more other processing modules, such as the dialogue manager. These input IUs are then passed on to the processor, where they are mapped to corresponding (higher-level) IUs. For an NLG module, this could be a mapping from the dialogue act present(cuisine=Indian) to the realisation ‘they serve Indian food’. The resulting IUs are passed on to the right buffer which co-incides with the left buffer of another module (for example the speech synthesis module in our example)."
W12-1509,W10-4301,0,0.366113,"ypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence. 1 Introduction Traditionally, the smallest unit of speech processing for interactive systems has been a full utterance with strict, rigid turn-taking. Components of these interactive systems, including NLG systems, have so far treated the utterance as the smallest processing unit that triggers a module into action. More recently, work on incremental systems has shown that processing smaller ‘chunks’ of user input can improve Skantze and Hjalmarsson (2010) present a model of incremental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-"
W12-1509,E09-1085,0,0.198264,"se, purge and commit. Whenever new IUs enter the module’s left buffer, the module’s knowledge base is updated to reflect the new information. Such information typically corresponds to the current best hypothesis of a preceding processing module. As a property of incremental systems, however, such hypotheses can be revised by the respective preceding module and, as a result, the knowledge bases of all subsequent modules need to be purged and updated to the newest hypothesis. Once a hypothesis is certain to not be revised anymore, it is committed. For concrete implementations of this model, see Skantze and Schlangen (2009), Skantze and Hjalmarsson (2010), Baumann and Schlangen (2011). An implementation of an incremental dialogue manager is based on the Information State Update (ISU) model (Buss et al., 2010; Buss and Schlangen, 2011). The model is related in spirit to the bufferbased architecture, but all of its input processing and output planning is realised by ISU rules. This is true for the incremental ‘house-keeping’ actions update, revise, etc. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division"
W12-1509,W09-0626,0,0.028565,"Missing"
W12-1509,P04-1011,0,\N,Missing
W12-1619,P12-3009,1,0.857468,"Missing"
W12-1619,P09-2082,1,0.641259,"s the castle”, “Can you see the tower in front of you?”, “Turn left after the large building on your left after the junction” and so on. 3.5 The research has received funding from the European Community’s 7th Framework Programme (FP7/2007-2013) under grant agreement no. 270019 (SPACEBOOK project http://www.spacebookproject.eu/). References Question-Answering server The QA server currently answers a range of definition questions. E.g., “Tell me more about the Scottish Parliament”, “Who was David Hume?”, etc. QA identifies the entity focused on in the question using machine-learning techniques (Mikhailian et al., 2009), and then proceeds to a textual search on texts from the Gazetteer of Scotland and Wikipedia, and definitions from WordNet glosses. Candidates are reranked using a trained confidence score with the top candidate used as the final answer. This answer is provided as a flow of sentence chunks that the user can interrupt. This information can also be pushed by the system when a salient entity appears in the user’s viewshed. 4 Acknowledgments Web-based User interface For the purposes of this (necessarily non-mobile) demonstration, we present a web-based interface that simulates users walking in a"
W12-1619,2007.mtsummit-ucnlg.4,0,0.0434356,"ces) which can be used to calculate minimal cost routes, such as the shortest path. 3.4 Visibility Engine Figure 1: System Architecture 3.1 navigational instructions and interesting PoI information. It receives the user’s input in the form of a dialogue act and the user’s location in the form of latitude and longitude information. Based on these inputs and the dialogue context, it responds with system output dialogue act (DA), based on a dialogue policy. The utterance generator is a natural language generation module that translates the system DA into surface text, using the Open CCG toolkit (White et al., 2007). Dialogue interface The dialogue interface consists of an utterance parser, an interaction manager and an utterance generator. The interaction manager is the central component of this architecture, which provides the user 135 This module identifies the entities that are in the user’s vista space (Montello, 1993). To do this it accesses a digital surface model, sourced from LiDAR, which is a 2.5D representation of the city including buildings, vegetation, and land surface elevation. The visibility engine uses this dataset to offer a number of services, such as determining the line of sight fro"
W12-1808,W10-4342,0,0.0138484,"n is in fact incremental (Tanenhaus and Brown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a recognition error or"
W12-1808,W12-1509,1,0.837166,"). Figure 1: Incremental phenomena observed in humanhuman dialogue that systems should be able to model. of-Oz experiments can be used to collect data from the system side, but user-initiated phenomena, such as the user changing his/her mind are more difficult to instigate. Therefore, data collections of naturally occurring incremental phenomena in human-human settings will be essential for further development of incremental systems. Such data can inform user simulations which provide means of training stochastic SDS with less initial data and can compensate for data sparsity. For example, in Dethlefs et al. (2012) the user simulation can change its mind and react to different NLG strategies such as giving information with partial input or waiting for complete input from the user. Both the academic community and industry would benefit from open access data, such as will be collected in the Parlance project and made available to the dialogue community2 . There would also need to be a clear path from academic research on ISDS to industry standards such as VoiceXML to facilitate adoption. Various components and techniques of ISDS are needed to handle ‘micro-turns’. Challenges here include recognizing and u"
W12-1808,W11-2014,0,0.0122953,"rown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a recognition error or a change in user’s preferences. Some examples o"
W12-1808,E09-1085,0,0.013845,"s that human-human interaction is in fact incremental (Tanenhaus and Brown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a re"
W13-2115,D10-1049,0,0.25429,"entire semester. As a case study, we took a module in Artificial Intelligence and asked students to fill out a very short diarytype questionnaire on a weekly basis. Questions included, for example, number of deadlines, number of classes attended, severity of personal issues. These data were then combined with the marks from the weekly lab reflecting the students’ performance. As data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to"
W13-2115,W11-2017,1,0.905463,"Missing"
W13-2115,N04-1015,0,0.0917522,"et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to say it (Surface Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods"
W13-2115,W10-4324,1,0.830817,"to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generating instructions (Dethlefs and Cuayahuitl, 2011). 3 Figure 1 shows graphically our approach to the development of a generation system. Firstly, we collected data from students including marks, demographic details and weekly study habits. Next, we created templates for surface realisation with the help of a Teaching and Learning expert. These templates were used to generate summaries that were rated by lecturers. We used these ratings to train the learning agent. The output of the"
W13-2115,W10-4217,0,0.248767,"students across the entire semester. As a case study, we took a module in Artificial Intelligence and asked students to fill out a very short diarytype questionnaire on a weekly basis. Questions included, for example, number of deadlines, number of classes attended, severity of personal issues. These data were then combined with the marks from the weekly lab reflecting the students’ performance. As data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content S"
W13-2115,W10-1301,0,0.112214,"s data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to say it (Surface Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven syste"
W13-2115,N12-1093,0,0.0127296,"del approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generating instructions (Dethlefs and Cuayahuitl, 2011). 3 Figure 1 shows graphically our approach to the development of a generation system. Firstly, we collected data from students including marks, demographic details and weekly study habits."
W13-2115,P10-1103,1,0.747106,"face Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generatin"
W13-2115,W11-2814,0,\N,Missing
W13-2115,H05-1042,0,\N,Missing
W13-4026,W12-1509,1,0.682243,"Missing"
W13-4026,P13-1123,1,0.0875028,"Missing"
W13-4026,W11-2014,0,0.0111653,"d phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive search through a Spoken Dialogue System (SDS). Th"
W13-4026,E09-1085,0,0.210008,"uthentic Afghan cuisine. USR What is the address and phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive se"
W13-4036,W09-3935,0,0.0121398,"ration have proven to be feasible for effective and robust interactive systems (Rieser and Lemon, 2011; Lemon and Pietquin, 2012; Young et al., 2010; Young et al., 2013). Although such methods have recently also been applied to (multi-modal) human-robot interaction (Stiefelhagen et al., 2007; Cuay´ahuitl et al., 2012), work on multi-user humanrobot interaction has been limited to non-statistical, hand-coded models (Klotz et al., 2011). On the other hand, substantial work has been done in the field of situated multi-party interaction in general, including data-driven approaches. In particular, Bohus & Horvitz (2009) have addressed the task of recognising engagement intentions using online learning in the setting of a screen-based embodied virtual receptionist, and have also worked on multi-party turn-taking in this context (Bohus and Horvitz, 2011). Introduction As the use of robot technology in the home as well as in public spaces is increasingly gaining attention, the need for effective and robust models for natural and social human robot interaction becomes more important. Whether it involves robot companions (Vardoulakis et al., 2012), game-playing robots (Klotz et al., 2011; Brooks et al., 2012; Cua"
W13-4036,W11-2013,0,0.0284669,"modal) human-robot interaction (Stiefelhagen et al., 2007; Cuay´ahuitl et al., 2012), work on multi-user humanrobot interaction has been limited to non-statistical, hand-coded models (Klotz et al., 2011). On the other hand, substantial work has been done in the field of situated multi-party interaction in general, including data-driven approaches. In particular, Bohus & Horvitz (2009) have addressed the task of recognising engagement intentions using online learning in the setting of a screen-based embodied virtual receptionist, and have also worked on multi-party turn-taking in this context (Bohus and Horvitz, 2011). Introduction As the use of robot technology in the home as well as in public spaces is increasingly gaining attention, the need for effective and robust models for natural and social human robot interaction becomes more important. Whether it involves robot companions (Vardoulakis et al., 2012), game-playing robots (Klotz et al., 2011; Brooks et al., 2012; Cuay´ahuitl and Kruijff-Korbayov´a, 2012), or robots that help people with exercising (Fasola and Mataric, 2013), human users should be able to interact with such service robots in an effective and natural way, using speech as well as other"
W13-4036,bunt-etal-2010-towards,0,0.00466375,"in response (e.g., a customer appears, begins seeking attention, or makes a drink order). 4 Social Skills Executor The Social Skills Executor (SSE) controls the behaviour of the robot system, based on the social state updates it receives from the SSR. The output of the SSE consists of a combination of noncommunicative robot actions and/or communicative actions with descriptions of their multi-modal realisations. In the bartender domain, the noncommunicative actions typically involve serving a specific drink to a specific user, whereas the communicative actions have the form of dialogue acts (Bunt et al., 2010), directed at a specific user, e.g. setQuestion(drink) (“What would you like to drink?”) or initialGreeting() (“Hello”). In our design of the SSE, the decision making process resulting in such outputs (including the ‘no action’ output) consists of three stages: 1) social multi-user coordination: managing the system’s engagement with the users present in the scene (e.g., accept a user’s bid for attention, or proceed with an engaged user), 2) single-user interaction: if proceeding with an engaged user, generating a highlevel response to that user, in the form of a communicative act or physical a"
W13-4036,N12-3005,0,0.0310961,"Missing"
W13-4036,C12-3012,0,0.0361276,"Missing"
W13-4036,W11-2042,0,0.062483,"Missing"
W13-4036,W11-2033,0,0.017087,"s introduced to permit switching from an incomplete SMDP to another SMDP at the same level, making interactions more flexible. In our approach, control always starts at the top level MDP and lower level MDPs are triggered depending on the action taken by their parent MDP. For social interaction with multiple users, flexible switching between interactions with different users is important, so an arguably more sophisticated HRL approach to multi-user interaction will rely heavily on the transition model. Another approach to modularising the task domain through multiple policies is described in (Lison, 2011), where ‘meta-control’ of the policies relies on an activation vector. As in the HRL SMDP approach, this approach has not been applied in the context of multi-user interaction. In any case, a more thorough and possibly experimental analysis comparing our approach with these other approaches would be worth investigating. In the future, we plan to extend our MDP model to a POMDP (Partially Observable MDP) model, taking uncertainty about both speech and visual input into account in the optimisation of SSE policies by incorporating alternative hypotheses and confidence scores provided by the input"
W13-4047,W11-2814,1,0.884276,"Missing"
W13-4047,W12-1625,0,0.0224747,"Missing"
W13-4047,W10-4306,0,0.0606422,"Missing"
W13-4047,J00-3003,0,0.245043,"Missing"
W13-4067,P08-2019,1,0.263383,"c.uk Abstract methods are usually developed under a partially observable Markov decision process (POMDP) framework (Young et al., 2010; Thomson and Young, 2010; Williams, 2010), where the distribution over dialogue states is called a ‘belief’ and is modelled as a posterior updated every turn given an observation. Furthermore, instead of simply taking the most probable (or highest confidence score) hypothesis of the user act as in ‘traditional’ handcrafted systems, the observation here may consist of an n-best list of the SLU hypotheses (dialogue acts) with (normalised) confidence scores. See (Henderson and Lemon, 2008; Williams and Young, 2007b; Thomson et al., 2010; Young et al., 2013) for more details of POMDP-based SDS. It is understandable that beliefs more accurately estimating the true dialogue states will ease the tuning of dialogue policies, and hence can result in better overall system performance. The accuracy of belief tracking has been studied in depth by Williams (2012) based on two SDS in public use. Here the effects of several mechanisms are analysed, which can alter the ‘most-believed’ dialogue state hypothesis (computed using a generative POMDP model) from the one derived directly from an"
W13-4067,W10-4306,0,0.0132382,"roduces the proposed belief tracker. Section 4 briefly describes the DSTC task. The evaluation results and detailed analysis are illustrated in Section 5. Finally, we further discuss in Section 6 and conclude in Section 7. dialogue states (or user goals) based on the previous belief, a current observation (normally an SLU n-best list), and some prior knowledge. The prior knowledge can be observation probabilities given a hidden state, the previous system action and/or dialogue histories (Young et al., 2010; Thomson and Young, 2010; Williams, 2010), or probabilistic domain-specific ontologies (Mehta et al., 2010), where the probabilities can be either trained on a collection of dialogue examples or manually assigned by human experts. In such models, a common strategy is to use the confidence scores in the observed n-best list as immediate information substituted into the model for belief computation, which implies that the performance of such belief tracking methods to a large extent depends on the reliability of the confidence scores. On the other hand, since the confidence scores may reflect the probabilities of the occurrences of corresponding user acts (SLU hypotheses), a belief can also be mainta"
W13-4067,P00-1013,0,0.0116872,"al are investigated. 1 Introduction Spoken dialogue system (SDS) can be modelled as a decision process, in which one of the main problems researchers try to overcome is the uncertainty in tracking dialogue states due to errorprone outputs from automatic speech recognition (ASR) and spoken language understanding (SLU) components (Williams, 2012). Recent advances in SDS have demonstrated that maintaining a distribution over a set of possible (hidden) dialogue states and optimising dialogue policies with respect to long term expected rewards can significantly improve the interaction performance (Roy et al., 2000; Williams and Young, 2007a). Such 423 Proceedings of the SIGDIAL 2013 Conference, pages 423–432, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics plementation, suggests that another practical use of the proposed method could be as a module in an initial system installation to collect training data for machine learning techniques, in addition to functioning as a baseline for further analysing them. The remainder of this paper is organised as follows. Section 2 reviews some basic mathematical background, based on which Section 3 introduces the proposed belief tr"
W13-4067,W13-4065,0,\N,Missing
W14-0208,P13-1163,1,0.925451,"tem. Subsequent user references to these entities using expressions such as “the museum”, “the cafe”, and so on, are resolved by searching for the latest entity of the given type. In cases where the IM cannot resolve the referent, it asks the user to clarify. phrases to reintroduce a previous topic. We discuss some examples later in section 4.3. 4 SPACEBOOK Interaction Manager As a part of the SpaceBook EU FP7 project, we implemented the above design for a multithreaded interaction manager that presents the user with navigational instructions, pushes PoI information, and manages QA questions (Janarthanam et al., 2013). It receives the user’s input in the form of a dialogue act (DA) from the ASR module and the user’s location (latitude and longitude), orientation, and speed from the Pedestrian Tracker module. Based on these inputs and the dialogue context, the IM responds with a system output dialogue act. It should be noted that the location coordinates of the user are sent to the IM every 2 seconds. This allows the IM to generate location aware information at a high frequency. In addition, the IM has to deal with incoming requests and responses from the user’s spoken inputs. With the possibility of system"
W14-0208,W08-0114,0,0.0271724,"ypoint, search for a truck), which could be active simultaneously. The situation for our pedestrian navigation and information system is similar - concurrent tasks need to be managed coherently via conversation. The approach adopted in this paper is similar to (Lemon and Gruenstein, 2004). However, in this work we separate out a domain-general thread called ‘dialogue control’ which handles generic issues like clarification of reference across all tasks. This increasing modularisation of the dialogue threads makes it possible to learn individual dialogue policies for each one, in future work. (Nakano et al., 2008) presented an approach where one of the several expert modules handling different tasks is activated based on the user input, but only one verbal expert is active at any one time. In contrast to this, we present an approach where several thread managers each handling a different task can be activated in parallel and their outputs stored and retrieved based on priority. We present a multi-threaded Interaction Manager (IM) that is used to track different dimensions of user-system conversations that are required to interleave with each other in a coherent and timely manner. This is explained in t"
W14-0208,C08-1129,0,0.0314947,"they are generated and time that they are pushed. Therefore dialogue actions in the queues are revised periodically to reflect changes in context. Obsolete dialogue actions will have to removed for two reasons. Firstly, pushing them to the user may make the conversation incoherent because the system may be speaking about an entity that is no longer relevant and secondly, these obsolete dialogue actions may delay other other important dialogue actions from being pushed on time. In addition, it may also be useful to edit the dialogue actions to include discourse markers to signify topic change (Yang et al., 2008) and bridge Multi-threading and queuing In order to manage complex interactions involving several conversational tasks/topics, we propose that the each task be handled by a thread manager within the interaction management framework. Each such manager will handle a conversational thread using a dialogue policy. Each thread manager will be fed with the input from the user and the dialogue actions generated will be stored in separate queues. This approach allows the interaction manager to produce several dialogue actions at the same time although for different 49 The IM resolves anaphoric referen"
W14-4308,N13-1127,0,0.153565,"e tickets (Walker et al., 2001) or finding a place to eat (Young et al., 2010). Gricean cooperative principles have been shown to emerge from multi-agent decision theory, in a language 60 Proceedings of the SIGDIAL 2014 Conference, pages 60–68, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics that the reward will not be given for performing non-cooperative moves themselves, but only for winning trading games. We therefore explore whether the agent can learn the advantages of being non-cooperative in dialogue, in a variety of settings. This is similar to (Vogel et al., 2013a) who show how cooperativity emerges from multiagent decision making, though in our case we show the emergence of non-cooperative dialogue behaviours. We begin with the case of a simple but challenging 2-player trading game, which is stochastic and involves hidden information. In section 2 we describe and motivate the trading game used in this work, and in section 3 we describe the Learning Agent. In section 4 we explain the different adversaries for experimentation, in section 5 we provide results, and in section 6 we conclude and discuss areas for future work. and Lemon, 2011). This researc"
W14-4308,P13-2014,0,0.06027,"e tickets (Walker et al., 2001) or finding a place to eat (Young et al., 2010). Gricean cooperative principles have been shown to emerge from multi-agent decision theory, in a language 60 Proceedings of the SIGDIAL 2014 Conference, pages 60–68, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics that the reward will not be given for performing non-cooperative moves themselves, but only for winning trading games. We therefore explore whether the agent can learn the advantages of being non-cooperative in dialogue, in a variety of settings. This is similar to (Vogel et al., 2013a) who show how cooperativity emerges from multiagent decision making, though in our case we show the emergence of non-cooperative dialogue behaviours. We begin with the case of a simple but challenging 2-player trading game, which is stochastic and involves hidden information. In section 2 we describe and motivate the trading game used in this work, and in section 3 we describe the Learning Agent. In section 4 we explain the different adversaries for experimentation, in section 5 we provide results, and in section 6 we conclude and discuss areas for future work. and Lemon, 2011). This researc"
W14-4308,P01-1066,0,0.0619625,"Missing"
W14-4336,P13-1123,1,0.850652,"Missing"
W14-4336,W13-4026,1,0.906157,"LG) components. We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions. 1 Introduction With the advent of evaluations “in the wild”, emphasis is being put on converting research prototypes into mobile applications that can be used for evaluation and data collection by real users downloading the application from the market place. This is the motivation behind the work demonstrated here where we present a modular framework whereby research components from the Parlance project (Hastie et al., 2013) can be plugged in, tested and evaluated in a mobile environment. The goal of Parlance is to perform interactive search through speech in multiple languages. The domain for the demonstration system is interactive search for restaurants in Cambridge, UK for Mandarin and San Francisco, USA for English. The scenario is that Mandarin speaking tourists would be able to download the application and use it to learn about restaurants in English speaking towns and cities. 2 Figure 1: Overview of the Parlance Mandarin mobile application system architecture Figure 2: Overview of the Parlance English mobi"
W14-4422,P10-1103,1,0.810776,"ture describes both (1) the trend of a factor (e.g. marks increasing, see also Table 1) and (2) the way that this factor could be conveyed in the summary (e.g. one possible way is referring to average, another possible way is referring to increasing/decreasing trend). If both conditions are met, the value of the feature is 1, otherwise 0. The 91 binary features describe all the different possible combinations. For both the Lecturer-adapted and Studentadapted systems, the reward function is derived from a linear regression analysis of the provided dataset, similarly to Walker et al. (1997) and Rieser et al. (2010). factor trend (1) marks stable (2) hours studied decreasing (3) health issues decreasing (4) lectures attended stable (5) personal issues increasing way it is mentioned average trend weeks average trend Table 2: The top 5 features out of the 18 selected through PCR analysis. 4 Evaluation FeedbackGen is evaluated with real users against two alternative systems: one that adapts to lecturers’ preferences and one that adapts to students’ preferences. The output of the three systems is ranked by 30 computer science students from a variety of years of study. Time-series data of three students are p"
W14-4422,W13-2115,1,0.906156,"d Oliver Lemon School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh {dg106, h.hastie, o.lemon}@hw.ac.uk Abstract In addition, different stakeholders often have conflicting goals, needs and preferences, for example managers with employees, or doctors with patients and relatives, or novice and expert users. In our data, for instance, lecturers tend to comment on the hours that the student studied, whereas the students disprefer this content. In our previous work, we showed that lecturers and students have different perceptions regarding what constitutes good feedback (Gkatzia et al., 2013). Here, we present a novel approach to generation by adapting its content to two user groups simultaneously. Producing the same summary for two groups is important as it allows for shared context and meaningful further discussion and reduces development time. We present FeedbackGen, a system that uses a multi-adaptive approach to Natural Language Generation. With the term ‘multi-adaptive’, we refer to a system that is able to adapt its content to different user groups simultaneously, in our case adapting to both lecturers and students. We present a novel approach to student feedback generation"
W14-4422,P14-1116,1,0.843129,"le on the top left shows an example of the time-series data. The table on the bottom left shows an example of described trends. The box on the right presents a target summary. respond to a sequence of actions (see Section 3.2). Temporal Difference (TD) learning (Sutton and Barto, 1990) is used for training three agents in a simulated environment to learn to make optimal content selection decisions: of time-series data. Our previous work showed that when comparing RL and supervised learning in the context of student feedback generation, students preferred the output generated by the RL system (Gkatzia et al., 2014a). Therefore, here, we used RL rather than a supervised learning method. The work described here builds on work reported in (Gkatzia et al., 2014b), which uses as a reward function the average of the Lecturer-adapted and Student-adapted reward functions. However, that method seems to cancel out the preferences of the two groups whereas PCR is able to identify relevant content for both groups. In the next section, we describe the data used, and the methodology for the multi-adaptive NLG, as well as two alternative systems. In Section 4, we describe the comparison of these three systems in a su"
W14-4422,E14-4041,1,0.531209,"le on the top left shows an example of the time-series data. The table on the bottom left shows an example of described trends. The box on the right presents a target summary. respond to a sequence of actions (see Section 3.2). Temporal Difference (TD) learning (Sutton and Barto, 1990) is used for training three agents in a simulated environment to learn to make optimal content selection decisions: of time-series data. Our previous work showed that when comparing RL and supervised learning in the context of student feedback generation, students preferred the output generated by the RL system (Gkatzia et al., 2014a). Therefore, here, we used RL rather than a supervised learning method. The work described here builds on work reported in (Gkatzia et al., 2014b), which uses as a reward function the average of the Lecturer-adapted and Student-adapted reward functions. However, that method seems to cancel out the preferences of the two groups whereas PCR is able to identify relevant content for both groups. In the next section, we describe the data used, and the methodology for the multi-adaptive NLG, as well as two alternative systems. In Section 4, we describe the comparison of these three systems in a su"
W14-4422,P97-1035,0,0.173961,"binary features. Each feature describes both (1) the trend of a factor (e.g. marks increasing, see also Table 1) and (2) the way that this factor could be conveyed in the summary (e.g. one possible way is referring to average, another possible way is referring to increasing/decreasing trend). If both conditions are met, the value of the feature is 1, otherwise 0. The 91 binary features describe all the different possible combinations. For both the Lecturer-adapted and Studentadapted systems, the reward function is derived from a linear regression analysis of the provided dataset, similarly to Walker et al. (1997) and Rieser et al. (2010). factor trend (1) marks stable (2) hours studied decreasing (3) health issues decreasing (4) lectures attended stable (5) personal issues increasing way it is mentioned average trend weeks average trend Table 2: The top 5 features out of the 18 selected through PCR analysis. 4 Evaluation FeedbackGen is evaluated with real users against two alternative systems: one that adapts to lecturers’ preferences and one that adapts to students’ preferences. The output of the three systems is ranked by 30 computer science students from a variety of years of study. Time-series dat"
W14-4422,W10-4324,1,0.821445,"by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the students’ perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach. 2 Related Work Previous work on NLG systems that address more than one user group employs different versions of a system for each different user group (Gatt et al., 2009; Hunter et al., 2011; Mahamood and Reiter, 2011), makes use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001) or personalises the output to individual users using rules (Reiter et al., 1999). Our proposed system adapts the output to the preferences of more than one user type1 , lecturers and students, but instead of developing many different systems or using User Models that describe different users, it attempts to model the middle ground between the preferences. In order to identify the users’ preferences, we apply Principal Components Regression (PCR (Jolliffe, 1982)) analysis to two datasets that contain lecturers’ and students’ ratings and identi"
W14-4422,W11-2803,0,0.0184742,"e knowledge derived from ratings on feedback summaries by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the students’ perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach. 2 Related Work Previous work on NLG systems that address more than one user group employs different versions of a system for each different user group (Gatt et al., 2009; Hunter et al., 2011; Mahamood and Reiter, 2011), makes use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001) or personalises the output to individual users using rules (Reiter et al., 1999). Our proposed system adapts the output to the preferences of more than one user type1 , lecturers and students, but instead of developing many different systems or using User Models that describe different users, it attempts to model the middle ground between the preferences. In order to identify the users’ preferences, we apply Principal Components Regression (PCR (Jolliffe, 1982)) analysis to two datasets t"
W15-2811,W15-0130,1,0.644987,"-by-word incremental semantic parser/generator, based around the Dynamic Syntax (DS) grammar framework (Cann et al., 2005) especially suited to the fragmentary and highly contextual nature of dialogue. In DS, dialogue is modelled as the interactive and incremental construction of contextual and semantic representations (Purver et al., 2011). The contextual representations afforded by DS are of the fine-grained semantic content that is jointly negotiated/agreed upon by the interlocutors, as a result of processing questions and answers, clarification requests, corrections, acceptances, etc (see Eshghi et al (2015) for an account of how this can be achieved grammar-internally as a low-level semantic update process). Recent versions of DS incorporate Type Theory with Records (TTR) as the logical formalism in which meaning representations are couched (Purver et al., 2011; Eshghi et al., 2012), due to its useful properties. Here we do not introduce DS due to space limitations but proceed to introducing TTR. 3.3 &quot; 3.4 Integration Fig. 2 shows how the various parts of the system interact. At any point in time, the system has access to an ontology of (object) types and attributes encoded as a set of TTR Recor"
W15-2811,P14-1068,0,0.280882,"tember 2015. 2015 Association for Computational Linguistics. Dialogue Image S: Is this a green mug? T: No it’s red S: Thanks. Final semantics   x=o1 : e  p2 : red(x)  p3 : mug(x)   x1=o2  p   p1 p2 T: What can you see? S: something red. What is it? T: A book. S: Thanks. : : : :     e book(x1) red(x1) see(sys, x1)      Figure 1: Example dialogues & resulting semantic representations Some approaches attempt to ground meaning of words/phrases/sentences in images/objects by mapping these modalities into the same vector space (Karpathy and Fei-Fei, 2014; Silberer and Lapata, 2014; Kiros et al., 2014), or using distributional semantic models that build distributional representations with the conjunction of textual and visual information (Bruni et al., 2014). Other approaches, such as (Socher et al., 2014), propose Neural Network models based on Dependency Trees (DT), which project all words in a sentence into a DT structured representation to explore parents of each node and correlations between nodes. In contrast to these approaches, which do not support NL dialogues, some approaches are designed based on logical semantic representations and some of them are incorpora"
W15-2811,W15-0124,0,0.0320259,"nces are performing far worse. Since our ultimate goal here is to create a full dialogue system that can learn concepts (word 66 Figure 6: Macro-F1 on each attribute for each method (MLkNN, DAP and Linear SVM) Figure 7: Area Under ROC curve for each attribute for each method (MLkNN, DAP and Linear SVM) meanings) from human tutors, these results would lead us to pick, at least in an initial proof-ofconcept system, attributes that show rapid learning rates. Presumably this is why prior work on this problem has often used ‘toy’ images where real image processing is not required (e.g. (Roy, 2002; Kennington et al., 2015)). Finally we note that none of these algorithms are incremental. Incremental learning methods (Kankuekul et al., 2012; Tsai et al., 2014; Furao et al., 2007; Zheng et al., 2013) have been developed to train object classification networks without abandoning previously learned knowledge or destroying the old trained prototypes. These methods (such as (Kankuekul et al., 2012)) could enable systems to label known/unknown attributes gradually through NL interaction with human tutors. Incremental learning approaches can also speed up the object learning/prediction process and the system responses,"
W15-2811,Q14-1017,0,0.060149,"ction Identifying, classifying and talking about objects or events in the surrounding environment are key capabilities for intelligent, goal-driven systems that interact with other agents and the external world (e.g. smart phones, robots, and other automated systems), as well as for image search/retrieval systems. To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions (Karpathy and Fei-Fei, 2014; Bruni et al., 2014; Socher et al., 2014). Another strand of work has focused on learning to generate 60 Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 60–69, c Lisbon, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. Dialogue Image S: Is this a green mug? T: No it’s red S: Thanks. Final semantics   x=o1 : e  p2 : red(x)  p3 : mug(x)   x1=o2  p   p1 p2 T: What can you see? S: something red. What is it? T: A book. S: Thanks. : : : :     e book(x1) red(x1) see(sys, x1)      Figure 1: Example dialogues & resulting semantic representations Som"
W15-2811,Q13-1016,0,0.0232837,"ured representation to explore parents of each node and correlations between nodes. In contrast to these approaches, which do not support NL dialogues, some approaches are designed based on logical semantic representations and some of them are incorporated with spoken dialogue systems (Skocaj et al., 2011; Matuszek et al., 2012; Kollar et al., 2013). A well-known logical semantic parser is the Combinatory Categorial Grammar (CCG) parser, which represents natural language sentences from human tutors in the logical forms. The “Logical Semantics with Perception” (LSP) framework by Kollar et al. (Krishnamurthy and Kollar, 2013) and the joint language/perception model by Matuszek et al. (Matuszek et al., 2012) are based on a CCG parser or using a CCG lexicon respectively. Although a CCG parser could generate similar logical representations to the DS-TTR parser/generator we use here, we believe that DS-TTR would show better performance than CCG in terms of handling the inherent incremental, fragmentary and highly context-dependent nature of dialogue. The “Describer” system (Roy, 2002) learns to generate image descriptions, but it works at the level of word sequences rather than logical semanwith the tutor. This paper"
W15-2811,W11-0144,1,0.91949,"ral existing state-of-the-art classifiers with regard to their suitability for interactive language grounding tasks. We compare the performance of MLKNN (Zhang and Zhou, 2007), DAP (zero-shot learning (Lampert et al., 2014)), and SVMs (Farhadi et al., 2010) on the image datasets aPascal (for training) and aYahoo (testing) – see section 4. To our knowledge, this paper is the first to compare these attribute classifiers in terms of their suitability for interactive language grounding. Our other contribution is to integrate an incremental semantic grammar suited to dialogue processing – DS-TTR1 (Purver et al., 2011; Eshghi et al., 2012), see section 3 – with visual classification algorithms that provide perceptual grounding for the basic semantic atoms in the representations produced by the parser through the course of a dialogue (see Fig. 1). In effect, the dialogue with the tutor continuously provides semantic information about objects in the scene which is then fed to an online classifier in the form of training instances. Conversely, the system can utilise the grammar and its existing knowledge about the world, encoded in its classifiers, to make reference to and formulate questions about the differ"
W15-4720,W09-0613,0,0.214526,"Missing"
W15-4720,W13-0215,0,\N,Missing
W16-3206,W15-0130,1,0.814612,"ngen (2015) who learn a mapping between individual words - rather than logical atoms - and low-level visual features (e.g. colour-values) directly. The system is compositional, yet does not use a grammar (the compositions are defined by hand). Further, the groundings are learned from pairings of object references in NL and images rather than from dialogue. What sets our approach apart from others is: a) that we use a domain-general, incremental se1 Download from http://dylan.sourceforge.net 49 mantic grammar with principled mechanisms for parsing and generation; b) Given DS model of dialogue (Eshghi et al., 2015), representations are constructed jointly and interactively by the tutor and system over the course of several turns (see Fig. 1); c) perception and NL-semantics are modelled in a single logical formalism (TTR); d) we effectively induce an ontology of atomic types in TTR, which can be combined in arbitrarily complex ways for generation of complex descriptions of arbitrarily complex visual scenes (see e.g. (Dobnik et al., 2012) and compare this with (Kennington and Schlangen, 2015), who do not use a grammar and therefore do not have logical structure over grounded meanings). 3 Experimental Setu"
W16-3206,P15-1029,0,0.373534,"dylan.sourceforge.net 49 mantic grammar with principled mechanisms for parsing and generation; b) Given DS model of dialogue (Eshghi et al., 2015), representations are constructed jointly and interactively by the tutor and system over the course of several turns (see Fig. 1); c) perception and NL-semantics are modelled in a single logical formalism (TTR); d) we effectively induce an ontology of atomic types in TTR, which can be combined in arbitrarily complex ways for generation of complex descriptions of arbitrarily complex visual scenes (see e.g. (Dobnik et al., 2012) and compare this with (Kennington and Schlangen, 2015), who do not use a grammar and therefore do not have logical structure over grounded meanings). 3 Experimental Setup Our goal in this paper is an experimental study of the effect of different dialogue policies and capabilities on the overall performance of the learning agent, which, as we describe below is a measure that combines accuracy of learned meanings with the cost of tutoring over time. Design. We use the dialogue system outlined above to carry out our main experiment with a 2 × 2 × 2 factorial design, i.e. with three factors each with two levels. Together, these factors determine the"
W16-3206,P14-1068,0,0.104125,"obots, smart spaces, and other automated systems). To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions e.g. (Bruni et al., 2014; Socher et al., 2014). Another strand of work has focused on learning to generate object descriptions and object classification based on low level concepts/features (such as colour, shape and material), enabling systems to identify and describe novel, unseen images (Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects – this is different from previous work such as e.g. (Roy, 2002), that learn groundings from descriptions without any interaction, and more recent work using Deep Learning methods (e.g. (Socher et al., 2014)). Most of these systems using machine learning rely on training data of high quantity with no possibility of online error correction. Furthermore, they are unsuitable for robots and multimodal systems that need to continuously, and increment"
W16-3206,Q14-1017,0,0.163933,"lly constructed dialogue turns. 1 Figure 1: Example dialogues Introduction Identifying, classifying, and talking about objects or events in the surrounding environment are key capabilities for intelligent, goal-driven systems that interact with other agents and the external world (e.g. robots, smart spaces, and other automated systems). To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions e.g. (Bruni et al., 2014; Socher et al., 2014). Another strand of work has focused on learning to generate object descriptions and object classification based on low level concepts/features (such as colour, shape and material), enabling systems to identify and describe novel, unseen images (Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects – this is different from previous work such as e.g. (Roy, 2002), that learn groundings from descriptions without any interaction, and more recent work usin"
W16-3206,W16-3643,1,0.776484,"onal Linguistics tion interactions with the tutor. This setting means that the system must be trainable from little data, compositional, adaptive, and able to handle natural human dialogue with all its glorious context-sensitivity and messiness – for instance so that it can learn visual concepts suitable for specific tasks/domains, or even those specific to a particular user. Interactive systems that learn continuously, and over the long run from humans need to do so incrementally, quickly, and with minimal effort/cost to human tutors. In this paper, we use an implemented dialogue system (see Yu et al. (2016b) and architecture in figure 2) that integrates an incremental, semantic grammar framework, especially suited to dialogue processing – Dynamic Syntax and Type Theory with Records (DS-TTR1 (Kempson et al., 2001; Eshghi et al., 2012)) with visual classifiers which are learned during the interaction, and which provide perceptual grounding for the basic semantic atoms in the semantic representations (Record Types in TTR) produced by the parser (see Fig. 1). We use this system in interaction with a simulated human tutor, to test hypotheses about how the accuracy of learned meanings, learning rates"
W16-3643,P15-1029,0,0.146228,"neral, incremental semantic grammar with principled mechanisms for parsing and generation; b) Given the DS model of dialogue (Eshghi et al., 2015), representations are constructed jointly and interactively by the tutor and system over the course of several turns (see Fig. 1); c) perception and NL-semantics are modelled in a single logical formalism (TTR); d) we effectively induce an ontology of atomic types in TTR, which can be combined in arbitrarily complex ways for generation of complex descriptions of arbitrarily complex visual scenes (see e.g. (Dobnik et al., 2012) and compare this with (Kennington and Schlangen, 2015), who do not use a grammar and therefore do not have logical structure over grounded meanings). 3 3.1.1 In contrast to previous work (Yu et al., 2015a; Yu et al., 2015b), to reduce feature noise through the learning process, we simplify the method of feature extraction consisting of two base feature categories, i.e. the colour space for colour attributes, and a ‘bag of visual words’ for the object shapes/class. Colour descriptors, consisting of HSV colour space values, are extracted for each pixel and then are quantized to a 16×4×4 HSV matrix. These descriptors inside the bounding box are binn"
W16-3643,W15-0130,1,0.312784,"work is probably that of Kennington & Schlangen (2015) who learn a mapping between individual words - rather than logical atoms - and low-level visual features (e.g. colour-values) directly. The system is compositional, yet does not use a grammar (the compositions are defined by hand). Further, the groundings are learned from pairings of object references in NL and images rather than from dialogue. What sets our approach apart from others is: a) that we use a domain-general, incremental semantic grammar with principled mechanisms for parsing and generation; b) Given the DS model of dialogue (Eshghi et al., 2015), representations are constructed jointly and interactively by the tutor and system over the course of several turns (see Fig. 1); c) perception and NL-semantics are modelled in a single logical formalism (TTR); d) we effectively induce an ontology of atomic types in TTR, which can be combined in arbitrarily complex ways for generation of complex descriptions of arbitrarily complex visual scenes (see e.g. (Dobnik et al., 2012) and compare this with (Kennington and Schlangen, 2015), who do not use a grammar and therefore do not have logical structure over grounded meanings). 3 3.1.1 In contrast"
W16-3643,P14-1068,0,0.242855,"rned meanings, learning rates, and efforts/costs to the tutor. We show that the overall performance of the learning agent is affected by (1) who takes initiative in the dialogues; (2) the ability to express/use their confidence level about visual attributes; and (3) the ability to process elliptical and incrementally constructed dialogue turns. Ultimately, we train an adaptive dialogue policy which optimises the trade-off between classifier accuracy and tutoring costs. 1 Figure 1: Example dialogues & interactively agreed semantic contents. al., 2014; Socher et al., 2014; Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects – this is different from previous work such as e.g. (Roy, 2002), that learn groundings from descriptions without any interaction, and more recent work using Deep Learning methods (e.g. (Socher et al., 2014)). Most of these systems rely on training data of high quantity with no possibility of online error correction. Furthermore, they are unsuitable for robots and multimodal systems that need to continuously, and incrementally learn from the env"
W16-3643,W14-1410,0,0.0762131,"s ‘red’ or ‘square’ are grounded in the set of classifiers trained so far. Given a set of individuated objects in a scene, encoded as a TTR Record, the system can utilise its existing ontology to output a Record Type which maximally characterises the scene (see e.g. Fig. 1). Dynamic Syntax operates over the same representations, they provide a direct interface between perceptual classification and semantic processing in dialogue: this representation acts not only as (1) the non-linguistic (here, visual) context of the dialogue for the resolution of e.g. definite reference and indexicals (see (Hough and Purver, 2014)); but also as (2) the logical database from which the system can generate utterances (descriptions), ask, or answer questions about the objects Fig. 4 illustrates how the semantics of the answer to a question is retrieved from the visual context through unification (this uses the standard subtype checking operation within TTR). Conversely, for concept learning, the DS-TTR parser incrementally produces Record Types (RT), representing the meaning jointly established by the tutor and the system so far. In this domain, this is ultimately one or more type judgements, i.e. that some scene/image/obj"
W16-3643,Q14-1017,0,0.181277,"olicies and capabilities on accuracy of learned meanings, learning rates, and efforts/costs to the tutor. We show that the overall performance of the learning agent is affected by (1) who takes initiative in the dialogues; (2) the ability to express/use their confidence level about visual attributes; and (3) the ability to process elliptical and incrementally constructed dialogue turns. Ultimately, we train an adaptive dialogue policy which optimises the trade-off between classifier accuracy and tutoring costs. 1 Figure 1: Example dialogues & interactively agreed semantic contents. al., 2014; Socher et al., 2014; Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Our goal is to build interactive systems that can learn grounded word meanings relating to their perceptions of real-world objects – this is different from previous work such as e.g. (Roy, 2002), that learn groundings from descriptions without any interaction, and more recent work using Deep Learning methods (e.g. (Socher et al., 2014)). Most of these systems rely on training data of high quantity with no possibility of online error correction. Furthermore, they are unsuitable for robots and multimodal systems that need to c"
W16-3643,W15-2811,1,0.84614,"re constructed jointly and interactively by the tutor and system over the course of several turns (see Fig. 1); c) perception and NL-semantics are modelled in a single logical formalism (TTR); d) we effectively induce an ontology of atomic types in TTR, which can be combined in arbitrarily complex ways for generation of complex descriptions of arbitrarily complex visual scenes (see e.g. (Dobnik et al., 2012) and compare this with (Kennington and Schlangen, 2015), who do not use a grammar and therefore do not have logical structure over grounded meanings). 3 3.1.1 In contrast to previous work (Yu et al., 2015a; Yu et al., 2015b), to reduce feature noise through the learning process, we simplify the method of feature extraction consisting of two base feature categories, i.e. the colour space for colour attributes, and a ‘bag of visual words’ for the object shapes/class. Colour descriptors, consisting of HSV colour space values, are extracted for each pixel and then are quantized to a 16×4×4 HSV matrix. These descriptors inside the bounding box are binned into individual histograms. Meanwhile, a bag of visual words is built in PHOW descriptors using a visual dictionary (that is pre-defined with a ha"
W16-6619,W15-0130,1,0.914409,"isual classifiers that are learned throughout the interaction and which ground the semantic/contextual representations that it produces (c.f. Kennington & Schlangen (2015) where words, rather than semantic atoms, are grounded in visual classifiers). Our approach extends Dobnik et al. (2012) in integrating perception (vision in this case) and language within a single formal system: Type Theory with Records (TTR (Cooper, 2005)). The combination of deep semantic representations in TTR with an incremental grammar (Dynamic Syntax) allows for complex multi-turn dialogues to be parsed and generated (Eshghi et al., 2015). These include clarification interaction, corrections, ellipsis and utterance continuations (see e.g. the dialogue in Fig. 1). Architecture: the system is made up of two key components – a Vision system and the DS-TTR parser/generator. The Vision system classifies a (visual) situation, i.e. deems it to be of a particular type, expressed as a TTR Record Type (RT) (see Fig. 1). This is done by deploying a set of binary attribute classifiers (Logistic Regression SVMs with Stochastic Gradient Descent, see Yu et al. (2015)) which ground the simple types (atoms) in the system (e.g. ‘red’, ‘square’)"
W16-6619,W14-1410,0,0.0313482,"one by deploying a set of binary attribute classifiers (Logistic Regression SVMs with Stochastic Gradient Descent, see Yu et al. (2015)) which ground the simple types (atoms) in the system (e.g. ‘red’, ‘square’), and composing their output to 1 Downloadable from: projects/dylan/ http://sourceforge.net/ Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk construct the more complex, total type of the visual scene. This representation then acts not only as (1) the non-linguistic context of the dialogue for DSTTR, for the resolution of e.g. definite references and indexicals (see Hough & Purver (2014)); but also (2) the logical database from which answers to questions about the objects’ attributes are generated. Questions are parsed and their logical representation acts directly as a query on the non-linguistic/visual context to retrieve an answer (via type checking in TTR, itself done via unification, see Fig. 1 for a simple example). Conversely, the system can generate questions to the tutor about the attributes of objects based on the entropy of the classifiers that ground the semantic concepts, e.g. those for colour and shape. The tutor’s answer then acts as a training instance for the"
W16-6619,P15-1029,0,0.0916459,"action Lab Heriot-Watt University a.eshghi@hw.ac.uk Yanchao Yu Interaction Lab Heriot-Watt University y.yu@hw.ac.uk We present a multi-modal dialogue system for interactive learning of perceptually grounded word meanings from a human tutor (Yu et al., ). The system integrates an incremental, semantic, and bidirectional grammar framework – Dynamic Syntax and Type Theory with Records (DS-TTR1 , (Eshghi et al., 2012; Kempson et al., 2001)) – with a set of visual classifiers that are learned throughout the interaction and which ground the semantic/contextual representations that it produces (c.f. Kennington & Schlangen (2015) where words, rather than semantic atoms, are grounded in visual classifiers). Our approach extends Dobnik et al. (2012) in integrating perception (vision in this case) and language within a single formal system: Type Theory with Records (TTR (Cooper, 2005)). The combination of deep semantic representations in TTR with an incremental grammar (Dynamic Syntax) allows for complex multi-turn dialogues to be parsed and generated (Eshghi et al., 2015). These include clarification interaction, corrections, ellipsis and utterance continuations (see e.g. the dialogue in Fig. 1). Architecture: the syste"
W16-6619,W16-3643,1,0.788409,"Missing"
W16-6619,W15-2811,1,0.849133,"ax) allows for complex multi-turn dialogues to be parsed and generated (Eshghi et al., 2015). These include clarification interaction, corrections, ellipsis and utterance continuations (see e.g. the dialogue in Fig. 1). Architecture: the system is made up of two key components – a Vision system and the DS-TTR parser/generator. The Vision system classifies a (visual) situation, i.e. deems it to be of a particular type, expressed as a TTR Record Type (RT) (see Fig. 1). This is done by deploying a set of binary attribute classifiers (Logistic Regression SVMs with Stochastic Gradient Descent, see Yu et al. (2015)) which ground the simple types (atoms) in the system (e.g. ‘red’, ‘square’), and composing their output to 1 Downloadable from: projects/dylan/ http://sourceforge.net/ Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk construct the more complex, total type of the visual scene. This representation then acts not only as (1) the non-linguistic context of the dialogue for DSTTR, for the resolution of e.g. definite references and indexicals (see Hough & Purver (2014)); but also (2) the logical database from which answers to questions about the objects’ attributes are generated."
W16-6644,D10-1049,0,0.0180983,"e complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning repres"
W16-6644,W11-2002,1,0.809218,"eous, natural and varied data from crowdworkers? To address (1), we first filter the crowdsourced data using automatic and manual validation procedures. We evaluate the quality of crowdsourced NLG data using automatic measures, e.g. measuring the semantic similarity of a collected NL utterance. To address (2), we conduct a principled study regarding the trade-off between semantic expressiveness of the MR and the quality of crowd-sourced utterances elicited for the different semantic representations. In particular, we investigate translating MRs into pictorial representations as used in, e.g. (Black et al., 2011; Williams and Young, 2007) for evaluating spoken dialogue systems. We compare these pictorial MRs to text-based MRs used by previous crowd-sourcing work (Mairesse et al., 2010; Wang et al., 2012). These text-based MRs take the form of Dialogue Acts, such as inform(type[hotel],pricerange[expensive]). However, there is a limit in the semantic complexity that crowd workers can handle (Mairesse et al., 2010). Also, (Wang et al., 2012) observed that the semantic formalism unfortunately influences the collected language, i.e. crowd-workers are “primed” by the words/tokens and ordering used in the M"
W16-6644,W10-0701,0,0.0423033,"as the format of the example MR provided (logical or pictorial). In terms of financial compensation, crowd workers were paid the standard pay on CrowdFlower, which is $0.02 per page (each containing 1 MR). Workers were expected to spend about 20 seconds per page. Participants were allowed to complete up to 20 pages, i.e. create utterances for up to 20 MRs. Mason and Watts (2010) in their study of financial incentives on Mechanical Turk, found (counterintuitively) that increasing the amount of compensation for a particular task does not tend to improve the quality of the results. Furthermore, Callison-Burch and Dredze (2010) observed that there can be an inverse relationship between the amount of payment 267 and the quality of work, because it may be more tempting for crowd workers to cheat on high-paying tasks if they do not have the skills to complete them. Following these findings, we did not increase the payment for our task over the standard level. In order to check for random inputs/“gibberish” and to control quality of the data, we introduced a validation procedure, which consisted of two main parts (see sections 3.3 and 3.4 for details): (1) Automatic pre-validation. The purpose of the automatic validatio"
W16-6644,D12-1008,1,0.671908,"of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using this framework. The data will be released as part of this submission. 2 Background Apart from (Mairesse et al., 2010), this research is the first to investigate crowdsourcing for collecting NLG data. So far, crowdsourcing is mainly used for evaluation in the NLG community, e.g. (Rieser et al., 2014; Dethlefs et al., 2012). Recent efforts in corpus creation via crowdsourcing have proven to be successful in related tasks. For example, (Zaidan and Callison-Burch, 2011) showed that crowdsourcing can result in datasets of comparable quality to those created by professional translators given appropriate quality control methods. (Mairesse et al., 2010) demonstrate that crowd workers can produce NL descriptions from abstract MRs, a method which also has shown success in related NLP tasks, such as Spoken Dialogue Systems (Wang et al., 2012) or Semantic Parsing (Wang et al., 2015). However, when collecting corpora for t"
W16-6644,P15-1044,0,0.050418,"Missing"
W16-6644,S13-1005,0,0.025928,"Missing"
W16-6644,N12-1093,0,0.0313111,"ts of pictorial stimuli increase. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we coll"
W16-6644,J14-4003,0,0.054676,"crease. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using"
W16-6644,P10-1157,0,0.0853364,"Missing"
W16-6644,P15-1129,0,0.0441058,"Missing"
W16-6644,D15-1199,0,0.0688315,"Missing"
W16-6644,P11-1122,0,0.0135992,"quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using this framework. The data will be released as part of this submission. 2 Background Apart from (Mairesse et al., 2010), this research is the first to investigate crowdsourcing for collecting NLG data. So far, crowdsourcing is mainly used for evaluation in the NLG community, e.g. (Rieser et al., 2014; Dethlefs et al., 2012). Recent efforts in corpus creation via crowdsourcing have proven to be successful in related tasks. For example, (Zaidan and Callison-Burch, 2011) showed that crowdsourcing can result in datasets of comparable quality to those created by professional translators given appropriate quality control methods. (Mairesse et al., 2010) demonstrate that crowd workers can produce NL descriptions from abstract MRs, a method which also has shown success in related NLP tasks, such as Spoken Dialogue Systems (Wang et al., 2012) or Semantic Parsing (Wang et al., 2015). However, when collecting corpora for training NLG systems, new challenges arise: (1) How to ensure the required high quality of the collected data? Proceedings of The 9th International"
W17-2001,W08-0126,0,0.0267341,"tools for creating more abstract (e.g. turn-based) representations of conversation. “it is a ...”, Learner: 2.2 Training a dialogue strategy is one of the fundamental tasks of the user simulation. Approaches to user simulation can be categorised based on the level of abstraction at which the dialogue is modeled: 1) the intention-level has become the most popular user model that predicts the next possible user dialogue action according to the dialogue history and the user/task goal (Eckert et al., 1997; Asri et al., 2016; Cuay´ahuitl et al., 2005; Chandramohan et al., 2012; Eshky et al., 2012; Ai and Weng, 2008; Georgila et al., 2005); 2) on the word/utterance-level, instead of dialogue action, the user simulation can also be built for predicting the full user utterances or a sequence of words given specific information (Chung, 2004; Schatzmann et al., 2007b); and 3) on the semanticlevel, the whole dialogue can be modeled as a sequence of user behaviors in the semantic representation (Schatzmann et al., 2007a; Schatzmann et al., 2007c; Kalatzis et al., 2016). (b) Task Panel for Tutor (Learner only sees the object) Figure 2: Snapshot of the DiET Chat tool, the Tutor’s Interface 2 Related Work In this"
W17-2001,D12-1007,0,0.0461647,"Missing"
W17-2001,P04-1009,0,0.0671825,"e categorised based on the level of abstraction at which the dialogue is modeled: 1) the intention-level has become the most popular user model that predicts the next possible user dialogue action according to the dialogue history and the user/task goal (Eckert et al., 1997; Asri et al., 2016; Cuay´ahuitl et al., 2005; Chandramohan et al., 2012; Eshky et al., 2012; Ai and Weng, 2008; Georgila et al., 2005); 2) on the word/utterance-level, instead of dialogue action, the user simulation can also be built for predicting the full user utterances or a sequence of words given specific information (Chung, 2004; Schatzmann et al., 2007b); and 3) on the semanticlevel, the whole dialogue can be modeled as a sequence of user behaviors in the semantic representation (Schatzmann et al., 2007a; Schatzmann et al., 2007c; Kalatzis et al., 2016). (b) Task Panel for Tutor (Learner only sees the object) Figure 2: Snapshot of the DiET Chat tool, the Tutor’s Interface 2 Related Work In this section, we will present an overview of relevant data-sets and techniques for Human-Human dialogue collection, as well as approaches to user simulation based on realistic data. 2.1 User Simulation Human-Human Data Collection"
W17-2001,Q14-1017,0,0.0585182,"m built previously. 1 (a) Dialogue Example from the corpus (b) The Chat Tool Window during dialogue in (a) above Figure 1: Example of turn overlap + subsequent correction in the BURCHAK corpus (‘sako’ is the invented word for red, ‘suzuli’ for green and ‘burchak’ for square) nal world (e.g. robots, smart spaces, and other automated systems). To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions (Bruni et al., 2014; Socher et al., 2014; Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Another strand of work has focused on incremental reference resolution in a model where word meaning is modeled as classifiers (the so-called Words-As-Classifiers model (Kennington and Schlangen, 2015)). However, none of this prior work focuses on how concepts/word meanings are learned and adapted in interactive dialogue with a human, the most common setting in which robots, home automation devices, smart spaces etc. operate, and, indeed the richest resource that such devices could exploit for adaptation over time to the idi"
W17-2001,P15-1029,0,0.0623166,"hak’ for square) nal world (e.g. robots, smart spaces, and other automated systems). To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions (Bruni et al., 2014; Socher et al., 2014; Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Another strand of work has focused on incremental reference resolution in a model where word meaning is modeled as classifiers (the so-called Words-As-Classifiers model (Kennington and Schlangen, 2015)). However, none of this prior work focuses on how concepts/word meanings are learned and adapted in interactive dialogue with a human, the most common setting in which robots, home automation devices, smart spaces etc. operate, and, indeed the richest resource that such devices could exploit for adaptation over time to the idiosyncrasies of the language used by their users. Though recent prior work has focused on the problem of learning visual groundings in interaction with a tutor (see e.g. (Yu et al., 2016b; Yu et Introduction Identifying, classifying, and talking about objects and events i"
W17-2001,W09-3937,0,0.0339532,"Missing"
W17-2001,H93-1005,0,0.580974,"User Simulation Human-Human Data Collection There are several existing corpora of humanhuman spontaneous spoken dialogue, such as SWITCHBOARD (Godfrey et al., 1992), and the British National Corpus, which consist of open, unrestricted telephone conversations between people, where there are no specific tasks to be achieved. These datasets contain many of the incremental dialogue phenomena that we are interested in, but there is no shared visual scene between participants, meaning we cannot use such data to explore learning of perceptually grounded language. More relevant is the MAPTASK corpus (Thompson et al., 1993), where dialogue participants both have maps which are not shared. This dataset allows investigation of negotiation dialogue, where object names can be agreed, and so does support some work on language grounding. However, in the MAPTASK, grounded word meanings are not taught by ostensive definition as is the case in our new dataset. There are also some user simulations built on multiple levels. For instance, Jung et al. (2009) integrated different data-driven approaches on intention and word levels to build a novel user simulation. The user intent simulation is for generating user intention pa"
W17-2001,W16-6619,1,0.848301,"ise and are used by listeners to guide linguistic processing (Clark and Fox Tree, 2002); similarly, while simultaneous speech is the bane of dialogue system designers, interruptions and subsequent continuations (see examples in table 1.c and 1.d) are performed deliberately by speakers to demonstrate strong levels of understanding (Clark, 1996). Despite this importance, these phenomena are excluded in many dialogue corpora, and glossed over/removed by state of the art speech recognisers (e.g. Sphinx-4 (Walker et al., 2004) and Google’s web-based ASR (Schalkwyk et al., 2010); see Baumann et al. (2016) for a comparison). One reason for this is that naturalistic spoken interaction is excessively expensive and timeconsuming to transcribe and annotate on a level of granularity fine-grained enough to reflect the strict time-linear nature of these phenomena. In this paper, we present a new dialogue data set - the BURCHAK corpus - collected using a new incremental variant of the DiET chat-tool (Healey et al., 2003; Mills and Healey, submitted)1 , which enables character-by-character, text-based interaction between pairs of participants, and which circumvents all transcription effort as all this d"
W17-2001,N07-2038,0,0.0531041,"based on the level of abstraction at which the dialogue is modeled: 1) the intention-level has become the most popular user model that predicts the next possible user dialogue action according to the dialogue history and the user/task goal (Eckert et al., 1997; Asri et al., 2016; Cuay´ahuitl et al., 2005; Chandramohan et al., 2012; Eshky et al., 2012; Ai and Weng, 2008; Georgila et al., 2005); 2) on the word/utterance-level, instead of dialogue action, the user simulation can also be built for predicting the full user utterances or a sequence of words given specific information (Chung, 2004; Schatzmann et al., 2007b); and 3) on the semanticlevel, the whole dialogue can be modeled as a sequence of user behaviors in the semantic representation (Schatzmann et al., 2007a; Schatzmann et al., 2007c; Kalatzis et al., 2016). (b) Task Panel for Tutor (Learner only sees the object) Figure 2: Snapshot of the DiET Chat tool, the Tutor’s Interface 2 Related Work In this section, we will present an overview of relevant data-sets and techniques for Human-Human dialogue collection, as well as approaches to user simulation based on realistic data. 2.1 User Simulation Human-Human Data Collection There are several existin"
W17-2001,W16-3643,1,0.713062,"ise and are used by listeners to guide linguistic processing (Clark and Fox Tree, 2002); similarly, while simultaneous speech is the bane of dialogue system designers, interruptions and subsequent continuations (see examples in table 1.c and 1.d) are performed deliberately by speakers to demonstrate strong levels of understanding (Clark, 1996). Despite this importance, these phenomena are excluded in many dialogue corpora, and glossed over/removed by state of the art speech recognisers (e.g. Sphinx-4 (Walker et al., 2004) and Google’s web-based ASR (Schalkwyk et al., 2010); see Baumann et al. (2016) for a comparison). One reason for this is that naturalistic spoken interaction is excessively expensive and timeconsuming to transcribe and annotate on a level of granularity fine-grained enough to reflect the strict time-linear nature of these phenomena. In this paper, we present a new dialogue data set - the BURCHAK corpus - collected using a new incremental variant of the DiET chat-tool (Healey et al., 2003; Mills and Healey, submitted)1 , which enables character-by-character, text-based interaction between pairs of participants, and which circumvents all transcription effort as all this d"
W17-2001,2007.sigdial-1.48,0,0.0230151,"based on the level of abstraction at which the dialogue is modeled: 1) the intention-level has become the most popular user model that predicts the next possible user dialogue action according to the dialogue history and the user/task goal (Eckert et al., 1997; Asri et al., 2016; Cuay´ahuitl et al., 2005; Chandramohan et al., 2012; Eshky et al., 2012; Ai and Weng, 2008; Georgila et al., 2005); 2) on the word/utterance-level, instead of dialogue action, the user simulation can also be built for predicting the full user utterances or a sequence of words given specific information (Chung, 2004; Schatzmann et al., 2007b); and 3) on the semanticlevel, the whole dialogue can be modeled as a sequence of user behaviors in the semantic representation (Schatzmann et al., 2007a; Schatzmann et al., 2007c; Kalatzis et al., 2016). (b) Task Panel for Tutor (Learner only sees the object) Figure 2: Snapshot of the DiET Chat tool, the Tutor’s Interface 2 Related Work In this section, we will present an overview of relevant data-sets and techniques for Human-Human dialogue collection, as well as approaches to user simulation based on realistic data. 2.1 User Simulation Human-Human Data Collection There are several existin"
W17-2001,P14-1068,0,0.12256,"from the corpus (b) The Chat Tool Window during dialogue in (a) above Figure 1: Example of turn overlap + subsequent correction in the BURCHAK corpus (‘sako’ is the invented word for red, ‘suzuli’ for green and ‘burchak’ for square) nal world (e.g. robots, smart spaces, and other automated systems). To this end, there has recently been a surge of interest and significant progress made on a variety of related tasks, including generation of Natural Language (NL) descriptions of images, or identifying images based on NL descriptions (Bruni et al., 2014; Socher et al., 2014; Farhadi et al., 2009; Silberer and Lapata, 2014; Sun et al., 2013). Another strand of work has focused on incremental reference resolution in a model where word meaning is modeled as classifiers (the so-called Words-As-Classifiers model (Kennington and Schlangen, 2015)). However, none of this prior work focuses on how concepts/word meanings are learned and adapted in interactive dialogue with a human, the most common setting in which robots, home automation devices, smart spaces etc. operate, and, indeed the richest resource that such devices could exploit for adaptation over time to the idiosyncrasies of the language used by their users."
W17-2001,W16-3206,1,\N,Missing
W17-2802,N15-1017,0,0.128243,"out any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these (Skocaj et al., 2016; Yu et al., 2016c). Here, we concentrate on the latter scenario, where a system learns to identify and describe visual attributes (colour and shape in this case) through interaction with human tutors, incrementally, over time. Previous work has approached the grounding problem using a variety of resources and approaches, for instance, either using annotated visual datasets (Silberer and Lapata, 2014; Socher et al., 2014; Naim et al., 2015; Al-Omari et al., 2016; Tellex et al., 2014; Matuszek et al., 2012, 2014), or through interactions with other agents or real humans (Kollar et al., 2013; Tellex et al., 2013; Thomason et al., 2015, 2016; Skocaj et al., 2016; Yu et al., 2016c), where feedback from other Introduction As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings. Among other competencies, this involves the ability to learn and adapt mappings between words, phrases, and"
W17-2802,W09-3944,0,0.0638391,"Missing"
W17-2802,P14-1068,0,0.156394,"the color is right, but the shape is not. L: oh, okay, so? T: a burchak, burchak, sako burchak. L: cool, got it. L: what is this? T: en ... a aylana suzili. L: is aylana for color? T: no, it’s a shape. L: so it is an suzili aylana, right? T: yes. Figure 1: Human-Human Example Dialogues in the BURCHAK Corpus (Yu et al., 2017) (‘sako’ for ‘red’, ‘burchak’ for ‘square’, ‘suzuli’ for ‘green’, ‘aylana’ for ‘circle’, ‘wakaki’ for ‘triangle’) agent needs to learn to ground (map) NL symbols onto their existing perceptual and lexical knowledge (e.g. a dictionary of pre-trained classifiers) as in e.g. Silberer and Lapata (2014); Thomason et al. (2016); Kollar et al. (2013); Matuszek et al. (2014); and 2) the agent as a child: without any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these (Skocaj et al., 2016; Yu et al., 2016c). Here, we concentrate on the latter scenario, where a system learns to identify and describe visual attributes (colour and shape in this case) through interaction with human tutors, incrementally, over time. Previous work has approached the grounding problem using a variety of resources and approache"
W17-2802,W15-2811,1,0.888715,"Missing"
W17-2802,W16-3206,1,0.749142,"Missing"
W17-2802,W16-3643,1,0.892903,": oh, okay, so? T: a burchak, burchak, sako burchak. L: cool, got it. L: what is this? T: en ... a aylana suzili. L: is aylana for color? T: no, it’s a shape. L: so it is an suzili aylana, right? T: yes. Figure 1: Human-Human Example Dialogues in the BURCHAK Corpus (Yu et al., 2017) (‘sako’ for ‘red’, ‘burchak’ for ‘square’, ‘suzuli’ for ‘green’, ‘aylana’ for ‘circle’, ‘wakaki’ for ‘triangle’) agent needs to learn to ground (map) NL symbols onto their existing perceptual and lexical knowledge (e.g. a dictionary of pre-trained classifiers) as in e.g. Silberer and Lapata (2014); Thomason et al. (2016); Kollar et al. (2013); Matuszek et al. (2014); and 2) the agent as a child: without any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these (Skocaj et al., 2016; Yu et al., 2016c). Here, we concentrate on the latter scenario, where a system learns to identify and describe visual attributes (colour and shape in this case) through interaction with human tutors, incrementally, over time. Previous work has approached the grounding problem using a variety of resources and approaches, for instance, either"
W17-2802,Q14-1017,0,0.51999,"gent as a child: without any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these (Skocaj et al., 2016; Yu et al., 2016c). Here, we concentrate on the latter scenario, where a system learns to identify and describe visual attributes (colour and shape in this case) through interaction with human tutors, incrementally, over time. Previous work has approached the grounding problem using a variety of resources and approaches, for instance, either using annotated visual datasets (Silberer and Lapata, 2014; Socher et al., 2014; Naim et al., 2015; Al-Omari et al., 2016; Tellex et al., 2014; Matuszek et al., 2012, 2014), or through interactions with other agents or real humans (Kollar et al., 2013; Tellex et al., 2013; Thomason et al., 2015, 2016; Skocaj et al., 2016; Yu et al., 2016c), where feedback from other Introduction As intelligent systems/robots are brought out of the laboratory and into the physical world, they must become capable of natural everyday conversation with their human users about their physical surroundings. Among other competencies, this involves the ability to learn and adapt mappings between"
W17-2802,W17-2001,1,0.934889,"sifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies. 1 Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk Human-Human Dialogue T(utor): do you know this object? L(earner): a suzuli ... wait no ... sako wakaki? T: the color is right, but the shape is not. L: oh, okay, so? T: a burchak, burchak, sako burchak. L: cool, got it. L: what is this? T: en ... a aylana suzili. L: is aylana for color? T: no, it’s a shape. L: so it is an suzili aylana, right? T: yes. Figure 1: Human-Human Example Dialogues in the BURCHAK Corpus (Yu et al., 2017) (‘sako’ for ‘red’, ‘burchak’ for ‘square’, ‘suzuli’ for ‘green’, ‘aylana’ for ‘circle’, ‘wakaki’ for ‘triangle’) agent needs to learn to ground (map) NL symbols onto their existing perceptual and lexical knowledge (e.g. a dictionary of pre-trained classifiers) as in e.g. Silberer and Lapata (2014); Thomason et al. (2016); Kollar et al. (2013); Matuszek et al. (2014); and 2) the agent as a child: without any prior knowledge of perceptual categories, the agent must learn both the perceptual categories themselves and also how NL expressions map to these (Skocaj et al., 2016; Yu et al., 2016c). H"
W17-2802,P15-1029,0,\N,Missing
W17-2811,P17-1017,0,0.0222597,"Missing"
W17-2811,W11-2020,0,0.0241354,"eedings of the First Workshop on Language Grounding for Robotics, pages 86–94, c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics in adult users (Tapus and Mataric, 2007). In the area of spoken dialogue systems, signals recognised from linguistic cues and prosody have been used to detect problematic dialogues (Herm et al., 2008) and to assess dialogue quality as a whole (Schmitt and Ultes, 2015). This type of dialogue-related signals has also been used to automatically detect miscommunication (Meena et al., 2015), or to predict the user satisfaction (Schmitt et al., 2011). However, there is very little research combining the areas of detecting multi-modal signals during spoken HRI and evaluation of human-robot conversation, and using them to create an adaptive social dialogue. In this paper, we make a first step towards building a multi-modally-rich, conversational, and human-like robotic agent, potentially able to react to the changes in human behaviour during face-toface dialogue and able to adjust the dialogue strategy in order to improve an interlocutor’s impression. We present a setup that targets the development of a dialogue system to explore verbal and"
W17-2811,W15-4647,0,0.0124362,"positioned face to face. The scene was recorded by cameras (triangles C) from the robot’s perspective focusing on the face of the participant and from the side, showing the whole scene. The experimenter (red) was seated behind a divider. 1 Introduction Social signals, such as emotional expressions, play an important role in human-human interaction, thus they are increasingly recognised as an important factor to be considered both in human-robot interaction research (Cid et al., 2013; Novikova et al., 2015; Devillers et al., 2015) and in the area of spoken dialogue systems (Herm et al., 2008; Meena et al., 2015). Recognition of human social signals has become a popular topic in Human-Robot Interaction (HRI) in recent years. Social signals are recognized well from human facial expressions or prosodic features of speech (Ekman, 2004; Zeng et al., 2009), and have become the most popular methods for recognising human affective signals in human-robot interaction (R´azuri et al., 2015; Devillers et al., 2015; Cid et al., 2013). In human-robot interaction, recognized human emotions are mostly used for mimicking human behaviour and enhancing the empathy towards a robot both in children (Tielman et al., 2014)"
W17-5524,P14-1068,0,0.0736662,"Missing"
W17-5524,J00-3003,0,0.532794,"Missing"
W17-5524,W16-3643,1,0.896227,"Missing"
W17-5524,W17-2001,1,0.911199,"– this is widely known as the grounding problem. Our work is similar in spirit to e.g. (Roy, 2002; Skocaj et al., 2011) but advances it in several aspects (Yu et al., 2016). In this demo paper, we present a dialogue agent that learns visually grounded word meanings interactively from a human tutor, which we call: VOILA (Visually Optimised Interactive Learning Agent). Our goal is to enable this agent to learn to identify and describe objects/attributes (colour 1 Oliver Lemon Interaction Lab Heriot-Watt University o.lemon@hw.ac.uk • VOILA is trained on a corpus of real HumanHuman conversations (Yu et al., 2017), and is thus able to process natural human dialogue, which contains phenomena such as self- corrections, repetitions and restarts, pauses, fillers, and continuations VOILA is deployed onto Furhat, a humanlike robot head with a custom back-projected face, built-in stereo microphones, and a Microsoft http://www.furhatrobotics.com/ 197 Proceedings of the SIGDIAL 2017 Conference, pages 197–200, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Interactive Visual Concept Learning in the VOILA Agent (Screenshot) processes user utterances by extract"
W18-5701,D16-1127,0,0.383285,"et al., 2017), and were therefore of sufficient quality for a production system receiving thousands of calls per day. We also compare our model to a recently published dualencoder response selection model by Lu et al. (2017) based on an approach principally close to ours. 4.1 Handcrafted ranker In the handcrafted approach, several turn-level and dialogue-level features are calculated, and a linear combination of those feature values with manually adjusted coefficients is used to predict the final ranking. The list of features includes: • coherence, information flow, and dullness as defined by Li et al. (2016); • overlap between the context and the response with regards to named entities and noun phrases; • topic divergence between the context turns and the response – topics are represented using the Latent Dirichlet Allocation (LDA) model (Hoffman et al., 2010); • sentiment polarity, as computed by the NLTK Vader sentiment analyser (Gilbert and Hutto, 2014).2 P red(C, r) = σ(L(Sem(C, r) ⊕ f (C, r))) (2) where: L(x) = ReLU(M x + b) is the layer used in the Predictor (the number of such layers is a model parameter), Sem = L(Enc(C, r)) is the vector of semantic context-response features, and f (C, r)"
W18-5701,D14-1179,0,0.0166892,"Missing"
W18-5701,W15-4640,0,0.018873,"ork Work on response ranking for conversational systems has been been growing rapidly in recent years. Some authors employ ranking based on heuristically defined measures: Yu et al. (2015, 2016) use a heuristic based on keyword matching, part-of-speech filters, and Word2Vec similarity. (Krause et al., 2017) apply standard information retrieval metrics (TF-IDF) with importance weighting for named entities. However, most of the recent research attempts to train the ranking function from large amounts of conversational data, as we do. Some authors use task-based conversations, such as IT forums (Lowe et al., 2015) or customer services (Lu et al., 2017; Kumar et al., 2018), while others focus on online conversations on social media (e.g. Wu et al., 2016; Al-Rfou et al., 2016). The basic approach to learning the ranking function in most recent work is the same (e.g. Lowe et al., 2015; Al-Rfou et al., 2016; Wu et al., 2016): the predictor is taught to rank positive responses taken from real dialogue data higher than randomly sampled negative examples. Some of the approaches do not even include rich dialogue Acknowledgements This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1). The"
W18-5701,P05-1045,0,0.0185211,"e the 3 most recent system and user turns). They are encoded into a latent representation using a single shared RNN encoder based on GRU cells (Cho et al., 2014). The context embedding vectors are then summed up and concatenated with the response embedding (Eq. 1): Enc(C, r) = X RNN(Ci ) ⊕ RNN(r) (1) i 4 where C is the dialogue context and r is a response candidate. The context and the response are represented using combined word-agent tokens (where agent is either a specific bot from the ensemble or the user) and are concatenated with the lists of named entities extracted using Stanford NER (Finkel et al., 2005). All the word-agent tokens and named entities share the same unified vocabulary. Encoder outputs, along with additional dialogue features such as context and response sentiment, timestamp, and bot names in the context and the response, go into the Predictor, a feed-forward neural network (MLP) whose output is the resulting rating (Eq. 2): We compare our neural ranker to two other models also developed during the competition: handcrafted and linear rankers — all three were deployed live in the Alana Alexa Prize 2017 finalist system (Papaioannou et al., 2017), and were therefore of sufficient q"
W18-5701,C16-1063,0,0.0876872,"istically defined measures: Yu et al. (2015, 2016) use a heuristic based on keyword matching, part-of-speech filters, and Word2Vec similarity. (Krause et al., 2017) apply standard information retrieval metrics (TF-IDF) with importance weighting for named entities. However, most of the recent research attempts to train the ranking function from large amounts of conversational data, as we do. Some authors use task-based conversations, such as IT forums (Lowe et al., 2015) or customer services (Lu et al., 2017; Kumar et al., 2018), while others focus on online conversations on social media (e.g. Wu et al., 2016; Al-Rfou et al., 2016). The basic approach to learning the ranking function in most recent work is the same (e.g. Lowe et al., 2015; Al-Rfou et al., 2016; Wu et al., 2016): the predictor is taught to rank positive responses taken from real dialogue data higher than randomly sampled negative examples. Some of the approaches do not even include rich dialogue Acknowledgements This research received funding from the EPSRC project MaDrIgAL (EP/N017536/1). The Titan Xp used for this research was donated by the NVIDIA Corporation. 6 References Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2017. B"
W18-5701,W16-3649,0,0.0293196,"o two parts accordingly: • sample efficiency – the number of data points needed for the model to train. As such, it is useful to specify an order of magnitude of the training set size for different types of machine learning models; • annotation efficiency – the amount of annotation Introduction Chatbots, or socialbots, are dialogue systems aimed at maintaining an open-domain conversation with the user spanning a wide range of topics, with the main objective of being engaging, entertaining, and natural. Under one of the current approaches to such systems, the bot ensemble (Serban et al., 2017; Yu et al., 2016; Song et al., 2016), a collection, or ensemble, of different bots is used, each of which proposes a candidate response to the user’s input, and a response ranker selects the best 1 Code and trained models are available at https://github.com/WattSocialBot/alana_ learning_to_rank 1 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 1–8 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 Variables rating/length rating/positive feedback rating/negative feedback length/positive fee"
W19-5904,chang-manning-2012-sutime,0,0.0285496,"Missing"
W19-5904,W17-5506,0,0.0220341,"9 6.0 ± 1.7 26.5 ± 5.4 8.3 ± 1 20.7 ± 4.8 5.5 ± 0.8 25.6 ± 8.2 8.2 ± 1.1 34.8 ± 4.4 9.7 ± 1.4 37.6 ± 8.0 12.0 ± 1.0 38.2 ± 4.2 4.6 ± 1.6 28.9 ± 7.3 6.6 ± 1.7 34.8 ± 7.7 8.2 ± 1.7 34.2 ± 8.7 11.6 ± 1.5 39.9 ± 6.9 7.4 ± 1.2 29.1 ± 6.6 9.2 ± 1.2∗ 32.7 ± 6.1 11.8 ± 1.9 37.6 ± 6.1∗ 15.2 ± 1.6 38.7 ± 8.4 Table 1: Evaluation results. Marked with asterisks are individual results higher than the ZSDG baseline which are achieved with the minimum amount of training data, and in bold is the model consistently outperforming ZSDG in all domains and metrics with minimum data. then plugged in to the Eq. 1): (Eric et al., 2017) in 3 domains: appointment scheduling, city navigation, and weather information. Each dialogue comes with knowledge base snippet from the underlying domain-specific API. For LAED training, we use MetaLWOz (Lee et al., 2019), a human-human goal-oriented dialogue corpus specifically designed for various meta-learning and pre-training purposes. It contains conversations in 51 domains with several tasks in each of those. The dialogues are collected using the Wizard-of-Oz method where human participants were given a problem domain and a specific task. No domain-specific APIs or knowledge bases were"
W19-5904,D17-1238,0,0.0228925,"s from the target domain2 . For evaluation, we follow the approach of (Zhao and Esk´enazi, 2018) and report BLEU and Entity F1 scores — means/variances over 10 runs. 6 which in turn results in a more informative, generalizable representation. Finally, we discuss the evaluation metrics. Since we base this paper on the work of (Zhao and Esk´enazi, 2018), we have had to fully conform to the metrics they used to enable direct comparison. However, BLEU as the primary evaluation metric, does not necessarily reflect NLG quality in dialogue settings — see examples in Table 2 of the Appendix (see also Novikova et al. (2017)). This is a general issue in dialogue model evaluation since the variability of possible responses equivalent in meaning is very high in dialogue. In future work, instead of using BLEU, we will put more emphasis on the meaning of utterances, for example by using external dialogue act tagging resources, using quality metrics of language generation – e.g. perplexity – as well as more taskoriented metrics like Entity F1. We expect these to make for more meaningful evaluation criteria. Results and discussion Our results are shown in Table 1. Our objective here is maximum accuracy with minimum tra"
W19-5904,N18-1202,0,0.0200172,"achieved. Transfer learning for Natural Language Processing is strongly motivated by recent advances in vision. When training a convolutional neural network (CNN) on a small dataset for a specific problem domain, it often helps to learn low-level convolutional features from a greater, more diverse dataset. For numerous applications in vision, ImageNet (Deng et al., 2009) became the source dataset for pre-training convolutional models. For NLP, the main means for transfer were Word2Vec word embeddings (Mikolov et al., 2013) which have recently been updated to models capturing contexts as well (Peters et al., 2018; Devlin et al., 2018). While these tools are widely known to improve performance in various tasks, more specialized models could as well be created for specific research areas, e.g. dialogue generation in our case. The models above are some of the approaches to one of the central issues of efficient knowledge transfer — learning a unified data representation generalizable across datasets, dubbed ‘representation learning’. In our approach, we will use one such technique based on variational autoencoding with discrete latent variables (Zhao et al., 2018). In this paper we present an approach to"
W19-5904,D17-1236,1,0.801839,"utterances, slot-value annotations, and domain names respectively for source and target domains. ZSDG is essentially a hierarchical encoderdecoder model which is trained in a multi-task fashion by receiving two types of data: (1) dialogue batches drawn from all the available sourcedomain data, and (2) seed data batches, a limited number of which are drawn from domain description data for all of the source and target domains. ZSDG model optimizes for 2 objectives. With dialogue batches, the model maximizes the probability of generating a response given the context: guistically informed model: (Eshghi et al., 2017) used an incremental semantic parser – DyLan (Eshghi et al., 2011; Eshghi, 2015) – to obtain contextual meaning representations, and based the dialogue state on this (Kalatzis et al., 2016). Incremental response generation was learned using Reinforcement Learning, again using the parser to incrementally process the agent’s output and thus prune ungrammatical paths for the learner. Compared to a neural model — End-to-End Memory Network (Sukhbaatar et al., 2015), this linguistically informed model was superior in a 1-shot setting (Shalyminov et al., 2017). At the same time, its main linguistic r"
W19-5904,W18-5001,0,0.188051,"Missing"
W19-5904,P18-1101,0,0.130488,"Missing"
W19-5904,W18-5032,0,\N,Missing
W19-5904,N19-1423,0,\N,Missing
W19-5904,P05-1045,0,\N,Missing
W19-5931,D17-1206,0,0.194733,"ion-giving for executing Multi-task NLU Deriving such a multi-layered meaning representation can be approached through a multi-task learning approach. Multitask learning has found success in several NLP 1 https://gitlab.com/hwu-ilab/ hermit-nlu 254 Proceedings of the SIGDial 2019 Conference, pages 254–263 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Robotics-Oriented MUltitask Language UnderStanding (ROMULUS) corpus, annotated with Dialogue Acts and Frame Semantics. HERMIT produces promising results for the application in a real scenario. problems (Hashimoto et al., 2017; Strubell et al., 2018), especially with the recent rise of Deep Learning. Thanks to the possibility of building complex networks, handling more tasks at once has been proven to be a successful solution, provided that some degree of dependence holds between the tasks. Moreover, multi-task learning allows the use of different datasets to train subparts of the network (Sanh et al., 2018). Following the same trend, HERMIT is a hierarchical multitask neural architecture which is able to deal with the three tasks of tagging dialogue acts, frame-like structures, and their arguments in parallel. The"
W19-5931,P98-1013,0,0.241778,"Rasa, DialogueFlow, LUIS, and Watson). The contribution of the different network components is then highlighted through an ablation study. We also test HERMIT on the smaller 255 to the next dialogue state. General frame structures (FRs) provide a reference framework to capture user intents, in terms of required or desired actions that a conversational agent has to perform. Depending on the level of abstraction required by an application, these can be interpreted as more domain-dependent paradigms like intent, or to shallower representations, such as semantic frames, as conceived in FrameNet (Baker et al., 1998). From this perspective, semantic frames represent a versatile abstraction that can be mapped over an agent’s capabilities, allowing also the system to be easily extended with new functionalities without requiring the definition of new ad-hoc structures. Similarly, frame arguments (ARs) act as slots in a traditional intent-slots scheme, or to frame elements for semantic frames. In our work, the whole process of extracting a complete semantic interpretation as required by the system is tackled with a multi-task learning approach across DAs, FRs, and ARs. Each of these tasks is modelled as a seq"
W19-5931,W17-5514,0,0.0567887,"first attempts to jointly detect domains in addition to intent-slot tagging is the work of (Guo et al., 2014). An utterance syntax is encoded through a Recursive NN, and it is used to predict the joined domain-intent classes. Syntactic features extracted from the same network are used in the per-word slot classifier. The work of (Hakkani-Tur et al., 2016) applies the same idea of (Zhang and Wang, 2016), this time using a context-augmented BiLSTM, and performing domain-intent classification as a single joint task. As in (Chen et al., 2016), the history of user utterances is also considered in (Bapna et al., 2017), in combination with a dialogue context encoder. A two-layer hierarchical structure made of a combination of BiLSTM and BiGRU is used Multi-dialogue act and -intent NLU Another degree of complexity in NLU is represented by the granularity of knowledge that can be extracted from an utterance. Utterance semantics is often rich and expressive: approximating meaning to a single user intent is often not enough to convey the required information. As opposed to the traditional single-dialogue act and single-intent view in previous work (Guo et al., 2014; Liu and Lane, 2016; Hakkani-Tur et al., 2016)"
W19-5931,D18-1417,0,0.0205798,"of joint classification models of intents and slots. Bidirectional GRUs are applied in (Zhang and Wang, 2016), where the hidden state of each time step is used for slot tagging in a seq2seq fashion, while the final state of the GRU is used for intent classification. The application of attention mechanisms in a BiLSTM architecture is investigated in (Liu and Lane, 2016), while the work of (Chen et al., 2016) explores the use of memory networks (Sukhbaatar et al., 2015) to exploit encoding of historical user utterances to improve the slot-filling task. Seq2seq with self-attention is applied in (Li et al., 2018), where the classified intent is also used to guide a special gated unit that contributes to the slot classification of each token. One of the first attempts to jointly detect domains in addition to intent-slot tagging is the work of (Guo et al., 2014). An utterance syntax is encoded through a Recursive NN, and it is used to predict the joined domain-intent classes. Syntactic features extracted from the same network are used in the per-word slot classifier. The work of (Hakkani-Tur et al., 2016) applies the same idea of (Zhang and Wang, 2016), this time using a context-augmented BiLSTM, and pe"
W19-5931,N18-1202,0,0.0301018,"c frames (F R layer), and frame elements (AR layer). The layers are arranged in a hierarchical structure that allows the information produced by earlier layers to be fed to downstream tasks. AR BiLSTM Encoder + + + + B-Locating I-Locating I-Locating … FR CRF Self-attention FR BiLSTM Encoder + + + + B-REQ_INFO I-REQ_INFO I-REQ_INFO … Self-attention DA CRF DA BiLSTM Encoder Embeddings Where The network is mainly composed of three BiLSTM (Schuster and Paliwal, 1997) encoding layers. A sequence of input words is initially converted into an embedded representation through an ELMo embeddings layer (Peters et al., 2018), and is fed to the DA layer. The embedded representation is also passed over through shortcut connections (Hashimoto et al., 2017), and concatenated with both the outputs of the DA and F R layers. Self-attention layers (Zheng et al., 2018) are placed after the DA and F R BiLSTM encoders. Where wt is the input word at time step t of the sentence w = (w1 , ..., wT ), the architecture can be formalised by: can I … Figure 2: HERMIT Network topology bedding, so that: DA y = CRF DA (aDA ), yF R = CRF F R (aF R ) yAR = CRF AR (sAR ), where aDA , aF R are attended embedded sequences. Due to shortcut"
W19-5931,W18-5045,0,0.0209765,"where a task-specific label is assigned to each token of the sentence according to the IOB2 notation (Sang and Veenstra, 1999), with “B-” marking the Beginning of the chunk, “I-” the tokens Inside the chunk while “O-” is assigned to any token that does not belong to any chunk. Task labels are drawn from the set of classes defined for DAs, FRs, and ARs. Figure 1 shows an example of the tagging layers over the sentence Where can I find Starbucks?, where Frame Semantics has been selected as underlying reference theory. for joint classification of domains and intents, together with slot tagging. (Rastogi et al., 2018) apply multi-task learning to the dialogue domain. Dialogue state tracking, dialogue act and intent classification, and slot tagging are jointly learned. Dialogue states and user utterances are encoded to provide hidden representations, which jointly affect all the other tasks. Many previous systems are trained and compared over the ATIS (Airline Travel Information Systems) dataset (Price, 1990), which covers only the flight-booking domain. Some of them also use bigger, not publicly available datasets, which appear to be similar to the NLU-BM in terms of number of intents and slots, but they c"
W19-5931,E99-1023,0,0.0743949,"lities, allowing also the system to be easily extended with new functionalities without requiring the definition of new ad-hoc structures. Similarly, frame arguments (ARs) act as slots in a traditional intent-slots scheme, or to frame elements for semantic frames. In our work, the whole process of extracting a complete semantic interpretation as required by the system is tackled with a multi-task learning approach across DAs, FRs, and ARs. Each of these tasks is modelled as a seq2seq problem, where a task-specific label is assigned to each token of the sentence according to the IOB2 notation (Sang and Veenstra, 1999), with “B-” marking the Beginning of the chunk, “I-” the tokens Inside the chunk while “O-” is assigned to any token that does not belong to any chunk. Task labels are drawn from the set of classes defined for DAs, FRs, and ARs. Figure 1 shows an example of the tagging layers over the sentence Where can I find Starbucks?, where Frame Semantics has been selected as underlying reference theory. for joint classification of domains and intents, together with slot tagging. (Rastogi et al., 2018) apply multi-task learning to the dialogue domain. Dialogue state tracking, dialogue act and intent class"
W19-5931,D18-1548,0,0.0470345,"Missing"
W19-5931,H90-1020,0,\N,Missing
W19-5931,C98-1013,0,\N,Missing
