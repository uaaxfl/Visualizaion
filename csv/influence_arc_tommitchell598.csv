2020.acl-demos.25,D13-1160,0,0.0323067,"aph query on the UI snapshot graph, executes it on the graph, and verifies if the result matches the correct entity that the user originally demonstrated. The goal of this process is to generate a query that uniquely matches the target UI element and also reflects the user’s underlying intent. Our semantic parser uses a Floating Parser arFigure 4: S UGILITE’s instruction parsing and grounding process for intent clarifications illustrated on an example UI snapshot graph constructed from a simplified GUI snippet. chitecture (Pasupat and Liang, 2015) and is implemented with the SEMPRE framework (Berant et al., 2013). We represent UI snapshot graph queries in a simple but flexible LISP-like query language (Sexpressions) that can represent joins, conjunctions, superlatives and their compositions, constructed by the following 7 grammar rules: E → e; E → S; S → (join r E); S → (and S S) T → (ARG MAX r S); T → (ARG MIN r S); Q → S |T where Q is the root non-terminal of the query expression, e is a terminal that represents a UI object entity, r is a terminal that represents a relation, and the rest of the non-terminals are used for intermediate derivations. S UGILITE’s language forms a subset of a more general"
2020.acl-demos.25,D18-2025,1,0.890326,"on enabling task automation agents to learn new tasks and their corresponding relevant concepts through natural interaction with human users (Laird et al., 2017). This topic is also known as end user development (EUD) for task automation (Ko et al., 2011; Myers et al., 2017). Work in this domain includes both physical agents (e.g., robots) that learn tasks that might involve sensing and manipulating objects in the real world (Chai et al., 2018; Argall et al., 2009), as well as software agents that learn how to perform tasks through software interfaces (Azaria et al., 2016; Allen et al., 2007; Labutov et al., 2018; Leshed et al., 2008). This paper focuses on the latter category. A particularly useful application of ITL is for conversational virtual assistants (e.g., Apple Siri, Google Assistant, Amazon Alexa). These systems have been widely adopted by end users to perform tasks in a variety of domains through natural language conversation. However, a key limitation of these systems is that their task fulfillment and language understanding capabilities are limited to a small set of pre-programmed tasks (Li et al., 2018b; Labutov et al., 2018). This limited support is not adequate for the diverse “long-t"
2020.acl-demos.25,J13-2005,0,0.0134839,"P-like query language (Sexpressions) that can represent joins, conjunctions, superlatives and their compositions, constructed by the following 7 grammar rules: E → e; E → S; S → (join r E); S → (and S S) T → (ARG MAX r S); T → (ARG MIN r S); Q → S |T where Q is the root non-terminal of the query expression, e is a terminal that represents a UI object entity, r is a terminal that represents a relation, and the rest of the non-terminals are used for intermediate derivations. S UGILITE’s language forms a subset of a more general formalism known as Lambda Dependency-based Compositional Semantics (Liang et al., 2013), which is a notationally simpler alternative to lambda calculus which is particularly well-suited for expressing queries over knowledge graphs. More technical details and the user evaluation are discussed in Li et al. (2018a). 3.3 Task Parameterization through GUI Grounding Another way S UGILITE leverages GUI groundings in the natural language instructions is to infer task parameters and their possible values. This allows the agent to learn generalized procedures (e.g., to order any kind of beverage from Starbucks) from a demonstration of a specific instance of the task (e.g., ordering an ice"
2020.acl-demos.25,D18-1540,0,0.0164074,"references without dealing with low-level mundane details (e.g., “buy 2 burgers” means setting the value of the textbox below the text “quantity” and next to the text “Burger” to be “2”). Some works have made early progress in this domain (Liu et al., 2018b; Deka et al., 2016; Chen et al., 2020) thanks to the availability of large datasets of GUIs like RICO (Deka et al., 2017). Recent reinforcement learning-based approaches and semantic parsing techniques have also shown promising results in 220 learning models for navigating through GUIs for user-specified task objectives (Liu et al., 2018a; Pasupat et al., 2018). For ITL, an interesting future challenge is to combine these user-independent domain-agnostic machine-learned models with the user’s personalized instructions for a specific task. This will likely require a new kind of mixedinitiative instruction (Horvitz, 1999) where the agent is more proactive in guiding the user and takes more initiative in the dialog. This could be supported by improved background knowledge and task models, and more flexible dialog frameworks that can handle the continuous refinement and uncertainty inherent in natural language interaction, and the variations in user goa"
2020.acl-demos.25,P15-1142,0,0.0249261,"igure 4). A semantic parser translates the user’s explanation into a graph query on the UI snapshot graph, executes it on the graph, and verifies if the result matches the correct entity that the user originally demonstrated. The goal of this process is to generate a query that uniquely matches the target UI element and also reflects the user’s underlying intent. Our semantic parser uses a Floating Parser arFigure 4: S UGILITE’s instruction parsing and grounding process for intent clarifications illustrated on an example UI snapshot graph constructed from a simplified GUI snippet. chitecture (Pasupat and Liang, 2015) and is implemented with the SEMPRE framework (Berant et al., 2013). We represent UI snapshot graph queries in a simple but flexible LISP-like query language (Sexpressions) that can represent joins, conjunctions, superlatives and their compositions, constructed by the following 7 grammar rules: E → e; E → S; S → (join r E); S → (and S S) T → (ARG MAX r S); T → (ARG MIN r S); Q → S |T where Q is the root non-terminal of the query expression, e is a terminal that represents a UI object entity, r is a terminal that represents a relation, and the rest of the non-terminals are used for intermediate"
2020.acl-demos.25,D17-1161,1,0.84932,"ents provide APIs to enable thirdparty developers to develop new “skills” for them, this requires significant programming expertise and relevant APIs, and therefore is not usable by the vast majority of end users. Natural language instructions play a key role in some ITL systems for virtual assistants, because this modality represents an natural way for humans to teach new tasks (often to other humans), and has a low learning barrier compared to existing textual or visual programming languages for task automation. Some prior systems (Azaria et al., 2016; Labutov et al., 2018; Le et al., 2013; Srivastava et al., 2017, 2018) relied solely natural language instruction, while others (Allen et al., 2007; Kirk and Laird, 2019; Sereshkeh et al., 2020) also used demonstrations of direct manipulations to supplement the natural language instructions. We surveyed the prior work, and identified the following five key design challenges: 1. Usability: The system should be usable for users without significant programming expertise. It should be easy and intuitive to use with a low learning barrier. This requires careful design of the dialog flow to best match the user’s natural model of task instruction. 2. Applicabili"
2020.acl-demos.25,P18-1029,1,0.898216,"Missing"
2020.acl-demos.25,2020.acl-main.538,0,0.0146527,"approach comes with many benefits, such as only requiring a small amount of training data and not relying on any domain knowledge, it has rigid patterns and therefore sometimes encounters problems with the flexible structures and varied expressions in the user utterances. We are looking at alternative approaches for parsing natural language instructions into our domainspecific language (DSL) for representing data description queries and task execution procedures. A promising strategy is to take advantage of the abstract syntax tree (AST) structure in our DSL for constructing a neural parser (Xu et al., 2020; Yin and Neubig, 2017), which reduces the amount of training data needed and enforces the wellformedness of the output code. The current model also only uses the semantic information from the local user instructions and their corresponding app GUIs. Another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., BERT (Devlin et al., 2018)) to encode the user instructions and the information extracted from app GUIs. 5.3 Extracting Task Semantics from GUIs An interesting future direction is to better extract se"
2020.acl-demos.25,P17-1041,0,0.0149618,"ith many benefits, such as only requiring a small amount of training data and not relying on any domain knowledge, it has rigid patterns and therefore sometimes encounters problems with the flexible structures and varied expressions in the user utterances. We are looking at alternative approaches for parsing natural language instructions into our domainspecific language (DSL) for representing data description queries and task execution procedures. A promising strategy is to take advantage of the abstract syntax tree (AST) structure in our DSL for constructing a neural parser (Xu et al., 2020; Yin and Neubig, 2017), which reduces the amount of training data needed and enforces the wellformedness of the output code. The current model also only uses the semantic information from the local user instructions and their corresponding app GUIs. Another promising approach to enable more robust natural language understanding is to leverage the pre-trained generalpurpose language models (e.g., BERT (Devlin et al., 2018)) to encode the user instructions and the information extracted from app GUIs. 5.3 Extracting Task Semantics from GUIs An interesting future direction is to better extract semantics from app GUIs s"
2021.emnlp-main.588,N19-1423,0,0.0127518,"are pre-conditions. Semantic Closeness: In order to measure semantic closeness of the object and subject phrases, we embed the sequence of tokens that make up the object of the previous tuple (oi−1 ) and the sequence of tokens that make up the subject of the next tuple (si ) in the proof trace. Semantic closeness, used for ranking the returned proofs, is defined as a vector cosine similarity of larger than a threshold, τ. We investigate several embedding methods to find one that best suites our multi-hop prover. We used GloVe embeddings (Pennington et al., 2014), BERT pre-trained embeddings (Devlin et al., 2019) and fine-tuned embeddings in COMET, which we call commonsense embeddings. For GloVe and BERT embeddings, we compute the phrase embedding by averaging the embeddings of the tokens. For commonsense embeddings, we use the phrase embeddings returned by COMET. In the results section, we compare the outcome of these choices. Let us now formally define a reasoning chain or proof extracted from our neural knowledge base. A proof consists of a chain of knowledge tuples {si , ri , oi }, where i ∈ [1, N] and N indicates the number of hops in the proof chain such that oi−1 is semantically close to si . ("
2021.emnlp-main.588,D18-2025,1,0.832702,"lexa, Siri, Google Home and others have goal when the state holds. The symbolic logic very recently entered our daily lives. However, they cannot currently engage in natural sounding conver- rules help significantly reduce the multi-hop reasoning search space and improve the quality of the sations with their human users mainly due to lack of generated commonsense reasoning chains. We evalcommonsense reasoning. Moreover, they operate uate  with a user study with human users. mostly on a pre-programmed set of tasks. On the other hand, instructable agents (Azaria et al., 2016; Acknowledgments Labutov et al., 2018; Li et al., 2018, 2017b,a; Guo Tom Mitchell is supported in part by AFOSR unet al., 2018; Mohan and Laird, 2014; Mininger and der grant FA95501710218. Antoine Bosselut and Laird, 2018; Mohan et al., 2012), can be taught new tasks through natural language instructions/demon- Yejin Choi gratefully acknowledge the support of DARPA under No. N660011924033 (MCS), strations. One of the challenges these bots face is correctly grounding their natural language instruc- JD.com, and the Allen Institute for AI. tions into executable commands. Our approach addresses a new reasoning task References propose"
2021.emnlp-main.588,D16-1127,0,0.039014,"tes blue, orange and green (Figure 3). The red template contains commands for which there is no unifying logic template. Therefore, we cannot use it. Evaluation: We do not use automated evaluation metrics and instead use human evaluations for two reasons. First, there are currently no metrics in the literature that assess whether the returned reasoning chains are “correct” from a commonsense perspective. Second, evaluating dialog systems is challenging. It is debated that metrics such as BLEU (Papineni et al., 2002) and perplexity often fail to measure true response quality (Liu et al., 2016; Li et al., 2016). Humans as Knowledge Evaluators The dialog generator confirms the returned COMET proofs with humans before adding it as background knowledge to K. In order to do this, it chooses the top 5 proofs with the highest similarity scores and presents them as candidates to the human user to choose from. Our study shows that these multiple choices not only help confirm COMET results but they also provide guidance to users as to what Experiments: In the first experiment, we evaluinformation the system is looking for. ate our multi-hop prover in isolation and without 7409 conversational interactions. Th"
C12-1118,N09-1003,0,0.0202617,"oesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the Rubenstein 1938 Figure 2: Behavioral evaluation of SVD models with range of dimensionality (see Section 3.1.1 for more details). and Goodenough (1965) set of 65 concrete word pairs, and the strict-similarity subset of 203 pairs (Agirre et al., 2009) selected from the WordSim353 test-set (Finkelstein et al., 2002). Performance was evaluated with the Spearman correlation between the aggregate human judgements and pairwise cosine distances between word vectors in the model in question. Finally the TOEFL benchmark (Landauer and Dumais, 1997) consists of aggregate records from an examination task for learners of English, who have to identify a synonym among a set of distractors. Performance was measured as the percentage correct over 80 questions, if the cosine-distance of the target-synonym pair was smaller than the distance between the targ"
C12-1118,W04-3221,0,0.056996,"nts in this section will be available at http://www.cs.cmu.edu/~bmurphy/NNSE/. 3.1 Evaluating Model Performance 3.1.1 Behavioral Experiments The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as emulating elicited judgements of pairwise similarity or of category membership (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Here we used five such tests. The two categorization tests were the Battig (Battig and Montague, 1969) test-set consisting of 82 nouns, each assigned to one of 10 concrete classes; and the AAMP (Almuhareb and Poesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the Rubenstein 1938 Figure 2: Behavioral evaluation of SVD models with range of dimensionality (see Section 3.1.1 for more details). and Goodenough (1965) set of 65 concrete word pairs, and the strict-similarity subset of 203 pairs (Agirre et al"
C12-1118,J10-4006,0,0.0139912,"models of semantics, also termed vector-space models or word embeddings, derive word-representations in an unsupervised fashion from large corpora. They are primarily based on observed co-occurrence patterns, but are typically subsequently reduced in dimensionality using techniques such as Clustering, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), or Singular Value Decomposition (SVD). They have proven effective as components of a wide range of NLP applications, and in the modelling of cognitive operations such as judgements of word similarity (Sahlgren, 2006; Turney and Pantel, 2010; Baroni and Lenci, 2010), and the brain activity elicited by particular concepts (Mitchell et al., 2008). However, with few exceptions (e.g. Baroni et al., 2010), the representations they derive from corpora are lacking in cognitive plausibility. For instance, one of the SVD-based models described in this paper models similarity very successfully, revealing the set of words mango, plum, cranberry, blueberry, melon as the cosine-distance nearest neighbours of pear. However, the latent SVD dimension for which pear has its largest weighting is hard to interpret – its most strongly positively associated tokens are action"
C12-1118,D07-1097,0,0.0841593,"Missing"
C12-1118,P12-1092,0,0.0502649,"by NNSE are considerably more interpretable compared to those estimated by SVD. We find that interpretability for both of them peak at k = 300. For qualitative comparison, top 5 words from 5 randomly selected dimensions each of SVD300 and NNSE1000 are presented in Table 2. From this, we get further anecdotal evidence about the higher interpretability of NNSE compared to SVD. 4 Related Work Corpus-derived models of semantics have been extensively studied in the NLP and machine learning communities (Collobert and Weston, 2008; Ratinov et al., 2010; Turney and Pantel, 2010; Socher et al., 2011; Huang et al., 2012). Additionally, dimensionality reduction techniques such as SVD, and topic distributions learned by probabilistic topic models such LDA (Blei et al., 2003) can also be used to induce word embeddings. Although the embeddings learned by these methods have many overlapping properties, to the best of our knowledge, none of these previous proposals satisfy the three desirable properties: effective in practice, sparse, and interpretable. We find that Non-Negative Sparse Encoding (NNSE), a variation on a matrix factorization technique previously studied in the machine learning community, can result i"
C12-1118,P98-2127,0,0.0415079,"Missing"
C12-1118,S12-1019,1,0.718734,"Missing"
C12-1118,J07-2002,0,0.0207571,"Missing"
C12-1118,2003.mtsummit-papers.42,0,0.0312786,"arious extrinsic evaluation tasks (Section 3.1)? How sparse are the SVD and NNSE representations (Section 3.2)? And how interpretable are these representations (Section 3.3)? All the NNSE representations used in the experiments in this section will be available at http://www.cs.cmu.edu/~bmurphy/NNSE/. 3.1 Evaluating Model Performance 3.1.1 Behavioral Experiments The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as emulating elicited judgements of pairwise similarity or of category membership (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Here we used five such tests. The two categorization tests were the Battig (Battig and Montague, 1969) test-set consisting of 82 nouns, each assigned to one of 10 concrete classes; and the AAMP (Almuhareb and Poesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the"
C12-1118,P10-1040,0,0.39138,"given by the i th row of matrix X . We note that overcomplete decomposition, i.e., k &gt; n, is possible in case of NNSE. For all experiments in this paper, we set λ = 0.05 and implement NNSE using the SPAMS package1 . 1 SPAMS Package: http://spams-devel.gforge.inria.fr/ 1936 3 Experiments In this section we try to answer two main questions: what broad categories of word embedding methods are most effective in modelling cognition; and which of the well-performing models are more cognitively plausible. Several of the models evaluated were already available, and were ˇ uˇrek and Sojka adopted from Ratinov et al. (2010) (Collobert & Weston, HLBL), and from Reh˚ (2010) (LDA topic model based on the English Wikipedia). The SVD and sparse non-negative models on which we concentrate, were constructed from scratch, based on both LSA-style word-document co-occurrence counts (i.e., word-region features) and HAL-style word-dependency co-occurrence counts (i.e., word-collocate features). Counts were computed from a large English web-corpus, Clueweb (Callan and Hoy, 2009), over a fixed 40,000 word vocabulary. These were the most frequent word-forms found in the American National Corpus (Nancy Ide and Keith Suderman, 2"
C12-1118,C98-2122,0,\N,Missing
C12-1118,ide-suderman-2004-american,0,\N,Missing
D11-1049,C10-1057,0,0.00921353,"is extremely efficient at link prediction or retrieval tasks, in which we are interested in identifying top links from a large number of candidates, instead of focusing on a particular node pair or joint inferences. 1.4 Related Work The TextRunner system (Cafarella et al., 2006) answers list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type 532 information, which is important for achieving high accuracy predictions. The HOLMES system (Schoenmackers et al., 2008) derives new assertions using a few manually written inference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules autom"
D11-1049,D08-1095,1,0.514992,"eraged over 96 tasks. `=3 `=4 all paths up to length L 15, 376 1, 906, 624 +query support≥ α = 0.01 522 5016 +ever reach a target entity 136 792 +L1 regularization 63 271 enumeration; however, for domains with a large number of edge types (e.g., a knowledge base), it is impractical to enumerate all possible relation paths even for small `. For instance, if the number of edge types related to each node type is 100, even the number of length three paths types easily reaches millions. For other domains like parsed natural language sentences, useful relation paths can be as long as ten relations (Minkov and Cohen, 2008). In this case, even with smaller number of possible edge types, the total number of relation paths is still too large for systematic enumeration. In order to apply PRA to these domains, we modify the path generation procedure in PRA to produce only relation paths which are potentially useful for the task. Define a query s to be supporting a path P if hs,P (e) 6= 0 for any entity e. We require that any path node created during path finding needs to be supported by at least a fraction α of the training queries si , as well as being of length no more than ` (In the experiments, we set α = 0.01)"
D11-1049,P06-1015,0,0.0330095,"Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical 529 inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a"
D11-1049,D08-1009,0,0.0392401,"Missing"
D11-1049,P06-1101,0,0.00851504,"Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical 529 inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect kn"
D11-1049,N07-4013,0,\N,Missing
D11-1134,P08-1004,0,0.125937,"Missing"
D11-1134,W09-2201,1,0.145935,"Missing"
D11-1134,P04-1053,0,0.0657912,"Missing"
D11-1134,C92-2082,0,\N,Missing
D11-1134,I05-1034,0,\N,Missing
D12-1069,J07-4004,0,0.0180492,"ences represent assertions about the world. In addition to these categories, the grammar includes type-changing rules from N to N |N . These rules capture noun compounds by allowing nouns to become functions from nouns to nouns. There are several such type-changing rules since the resulting category includes a hidden relation r between the noun and its modifier (see Table 1). As with lexical categories, the set of type changing rules included in the grammar is determined by matching dependency parse patterns to the training data. Similar rules for noun compounds are used in other CCG parsers (Clark and Curran, 2007). 4.2 The instantiated lexicon represents the semantics of words and phrases as conjunctions of predicates from the knowledge base, possibly including existentially quantified variables and λ expressions. The syntactic types N and P P are semantically represented as functions from entities to truth values (e.g., λx.CITY(x)), while sentences S are statements with no λ terms, such as ∃x, y.x = C ALIFORNIA ∧ CITY (y) ∧ LOCATED I N (x, y). Variables in the semantic representation (x, y) range over entities from the knowledge base. Intuitively, the N and P P cate759 Extensions to CCG The semantic p"
D12-1069,W10-2903,0,0.829863,"m a knowledge base. The expressivity and utility of semantic parsing is derived from this meaning representation, which is essentially a program that is directly executable by a computer. In this sense, broad coverage semantic parsing is the goal of natural language understanding. Unfortunately, due to data annotation constraints, modern semantic parsers only operate in narrow domains. The best performing semantic parsers are trained using extensive manual annotation: typically, a number of sentences must be annotated with their desired logical form. Although other forms of supervision exist (Clarke et al., 2010; Liang et al., 2011), these methods similarly require annotations for individual sentences. More automated training methods are required to produce semantic parsers with richer meaning representations. This paper presents an algorithm for training a semantic parser without per-sentence annotations. Instead, our approach exploits two easily-obtainable sources of supervision: a large knowledge base and (automatically) dependency-parsed sentences. The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree w"
D12-1069,W02-1001,0,0.0382111,"r(e1 , e2 ) ∈ ∆, and 0 otherwise. This setting trains the semantic parser to extract every true relation instance between (e1 , e2 ) from some sentence in S(e1 ,e2 ) , while simultaneously avoiding incorrect instances. Finally, z = 1, to encourage agreement between the semantic and dependency parses. The training data for the model is therefore a collection, {(sj , yj , zj )}nj=1 , where j indexes entity tuples (e1 , e2 ). Training optimizes the semantic parser parameters θ to predict Y = yj , Z = zj given S = sj . The parameters θ are estimated by running the structured perceptron algorithm (Collins, 2002) on the training data defined above. The structured perceptron algorithm iteratively applies a simple update rule for each example (sj , yj , zj ) in the training data: `predicted ← arg max max p(`, y, z|sj ; θt ) ` ` actual θt+1 y,z ← arg max p(`|yj , zj , sj ; θt ) ` X t ← θ + f (`actual , si ) i i − X f (`predicted , si ) i i Each iteration of training requires solving two maximization problems. The first maximization, max`,y,z p(`, y, z|s; θt ), is straightforward because y and z are deterministic functions of `. Therefore, it is solved by finding the maximum probability assignment `, then"
D12-1069,W05-0602,0,0.0275556,"trates the benefit of including syntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic pa"
D12-1069,P11-1149,0,0.0328634,"ervision required to train a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly s"
D12-1069,P11-1055,0,0.735402,"hing). Output: 1. Parameters θ for the CCG that produce correct semantic parses ` for sentences s ∈ S. This problem is ill-posed without additional assumptions: since the correct logical form for a sentence is never observed, there is no a priori reason to prefer one semantic parse to another. Our training algorithm makes two assumptions about correct semantic parses, which are encoded as weak supervision constraints. These constraints make learning possible by adding an inductive bias: 1. Every relation instance r(e1 , e2 ) ∈ ∆ is expressed by at least one sentence in S (Riedel et al., 2010; Hoffmann et al., 2011). 2. The correct semantic parse of a sentence s contains a subset of the syntactic dependencies contained in a dependency parse of s. Our weakly supervised training uses these constraints as a proxy for labeled semantic parses. The training algorithm has two steps. First, the algorithm constructs a graphical model that contains both the semantic parser and constant factors encoding the above two constraints. This graphical model is then used to estimate parameters θ for the semantic parser, essentially optimizing θ to produce parses that satisfy the weak supervision constraints. If our assumpt"
D12-1069,D10-1119,0,0.660419,"ficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, bu"
D12-1069,P11-1060,0,0.829998,"he expressivity and utility of semantic parsing is derived from this meaning representation, which is essentially a program that is directly executable by a computer. In this sense, broad coverage semantic parsing is the goal of natural language understanding. Unfortunately, due to data annotation constraints, modern semantic parsers only operate in narrow domains. The best performing semantic parsers are trained using extensive manual annotation: typically, a number of sentences must be annotated with their desired logical form. Although other forms of supervision exist (Clarke et al., 2010; Liang et al., 2011), these methods similarly require annotations for individual sentences. More automated training methods are required to produce semantic parsers with richer meaning representations. This paper presents an algorithm for training a semantic parser without per-sentence annotations. Instead, our approach exploits two easily-obtainable sources of supervision: a large knowledge base and (automatically) dependency-parsed sentences. The semantic parser is trained to identify relation instances from the knowledge base while simultaneously producing parses that syntactically agree with the dependency pa"
D12-1069,D08-1082,0,0.259718,"e system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work eliminates the need for an annotated logi"
D12-1069,P09-1113,0,0.568057,"requent relations in the test set. 5.2 Relation Extraction The first experiment measures the semantic parser’s ability to extract relations from sentences in our web corpus. We compare our semantic parser to M UL TI R (Hoffmann et al., 2011), which is a state-ofthe-art weakly supervised relation extractor. This method uses the same weak supervision constraint and parameter estimation procedure, but replaces the semantic parser by a linear classifier. The features for this classifier include the dependency path between the entity mentions, the type of each mention, and the intervening context (Mintz et al., 2009). Both the semantic parser and M ULTI R were trained by running 5 iterations of the structured per4 Note that the positive/negative ratio was much lower without the length filter or entity disambiguation, which is partly why filtering was performed. 1.0 1.0 M ULTI R PARSE + DEP 0.8 PARSE - DEP 0.4 0.2 0.2 0 0.2 0.4 0.6 0.8 0 1.0 Figure 3: Aggregate precision as a function of recall, for M ULTI R (Hoffman et al., 2011) and our three semantic parser variants. 1.0 M ULTI R PARSE + DEP 0.8 PARSE PARSE - DEP 0.6 0.4 0.2 0 0 600 1200 1800 2400 PARSE 0.6 0.4 0 PARSE + DEP 0.8 PARSE 0.6 M ULTI R 3000"
D12-1069,nivre-etal-2006-maltparser,0,0.0842929,"Missing"
D12-1069,D09-1001,0,0.0595585,"with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain than full logical forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example,"
D12-1069,N06-1056,0,0.40432,"including syntactic supervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work"
D12-1069,P07-1121,0,0.796459,"pervision. Examining the system output, we find two major sources of error. The first is missing lexical categories for uncommon words (e.g., “ex-guitarist”), which negatively impact recall by making some queries unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work eliminates the need for"
D12-1069,P10-1013,0,0.0221215,"l forms, but still requires individually annotated sentences. Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). It is also possible to self-train a semantic parser without any labeled data (Goldwasser et al., 2011). However, this approach does not perform as well as more supervised approaches, since the parser’s self-training predictions are not constrained by the correct logical form. Recent research has produced several weakly supervised relation extractors (Craven and Kumlien, 1999; Mintz et al., 2009; Wu and Weld, 2010; Riedel et al., 2010; Hoffmann et al., 2011). These systems scale up to hundreds of predicates, but have much shallower semantic representations than semantic parsers. For example, these systems cannot be directly used to respond to natural language queries. This work extends weakly supervised relation extraction to produce richer semantic structure, using only slightly more supervision in the form of dependency parses. 7 Discussion This paper presents a method for training a semantic parser using only a knowledge base and a corpus of unlabeled sentences. Our key observation is that multiple"
D12-1069,D07-1071,0,0.797611,"s unparsable. The second is difficulty distinguishing between relations with similar type signatures, such as CITY L OCATED I N C OUNTRY and CITYC APITAL O F C OUNTRY. 6 Related Work There are many approaches to supervised semantic parsing, including inductive logic programming (Zelle and Mooney, 1996), probabilistic and synchronous grammars (Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008), and automatically learned transformation rules (Kate et al., 2005). This work most closely follows the work on semantic parsing using CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). These supervised systems are all trained with annotated sentence/logical form pairs; hence these approaches are labor intensive and do not scale to broad domains with large numbers of predicates. Several recent papers have attempted to reduce the amount of human supervision required to train a semantic parser. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al., 2011) or even a binary correct/incorrect signal (Clarke et al., 2010). This type of feedback may be easier to obtain"
D13-1080,C92-2082,0,0.0751417,"na¨ıve addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to lear"
D13-1080,P03-1009,0,0.0130843,"els are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach"
D13-1080,D11-1049,1,0.790658,"need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augm"
D13-1080,D12-1093,0,0.818427,"represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen833 Proceedings of the 2013 Conference on"
D13-1080,P02-1006,0,0.0359393,"ificant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et"
D13-1080,N13-1008,0,0.0279984,"ns the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 3.1 Method Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012)"
D13-1080,P06-1101,0,0.0241815,"LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to"
D13-1080,D11-1132,0,0.0318212,"eature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labe"
D14-1030,D13-1140,1,0.840612,"ler than the entire vocabulary V . The probability of an output word l at position t + 1 given that its class is m is defined as: P (yl (t + 1) |cm (t), s(t)) = exp (s(t)D0 vl ) , PV 0 0 v )) (exp (s(t)D k k=1 where D0 is a matrix of output word embeddings and vl is a one hot vector representing the word with index l. The probability of the word w(t + 1) given its class ci can now be computed as: P (w(t + 1) |s(t)) =P (w(t + 1) |ci , s(t)) P (ci |s(t)). s(t) = φ (Dw(t) + Ws(t − 1)) , Neural Probabilistic Language Model We use the feedforward neural probabilistic language model architecture of Vaswani et al. (2013), as shown in Figure 4. Each context u comprises a sequence of words uj (1 ≤ j ≤ n − 1) represented as one-hot vectors, which are fed as input to the neural network. At the output layer, the neural network computes the probability P (w |u) for each word w, as follows. The output of the first hidden layer h1 is   n−1 X h1 = φ  Cj Duj + b1  , where D is the matrix of input word embeddings, W is a matrix that transforms the activations from the hidden layer in position t − 1, and φ is a 1 sigmoid function, defined as φ(x) = 1+exp(−x) , that is applied elementwise. We need to compute the proba"
D14-1030,P13-2152,0,0.0401615,"s the word with inversely proportional eftheir representations in terms of how well they fort to how predictable the word is (Frank et al., can be used to decode the word being read from 2013). There is a well studied response known as MEG data. We obtain correspondences between the N400 that is an increase of the activity in the the models and the brain data that are consistent temporal cortex that has been recently shown to be with a model of language processing in which graded by the amount of surprisal of the incoming brain activity encodes story context, and where word given the context (Frank et al., 2013). This is 3.2. Pre-lexical analysis each new word generates additional brain activity, 3.3. Lexical-semantic analysis analogous to the output probability of the incomIn order to tease apart early pre-lexical processes in flowing generally from visual processing areas to To identify cortical dynamics of reading comprehension, reading, Tarkiainen and colleagues (Tarkiainen et al., ing word from the neural network. Helenius and colleagues et al., 1998) employed a 1999) used words, syllables, and single letters,areas, imbeddedculminating more high level in an (Helenius updated Fig. 2 shows a hypot"
D14-1044,D13-1080,1,0.723034,"a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment"
D14-1044,D12-1093,0,0.080969,"ed as features in a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such la"
D14-1044,D12-1048,0,0.00909139,"rity between the two edge types. This lets us combine notions of distributional similarity with symbolic logical inference, with the result of decreasing the sparsity of the feature space considered by PRA. We show with experiments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surfac"
D14-1044,mendes-etal-2012-dbpedia,0,0.0233272,"Missing"
D14-1044,W14-1609,0,0.00593772,"rge graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our experiments. steps of PRA, and spikiness and restart pa"
D14-1044,N13-1008,0,0.145809,"ments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surface relation triples. So far these two subgraphs we have created are entirely disconnected, with the KB graph containing nodes representing entities, and the surface relation graph containing nodes representing noun phrases, wit"
D14-1044,P13-1045,0,0.00644971,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D14-1044,D13-1170,0,0.0021682,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D14-1207,D11-1142,0,0.343683,"Missing"
D14-1207,P13-1146,1,0.888196,"Missing"
D14-1207,P14-1095,1,0.881404,"Missing"
D14-1207,S07-1108,0,0.0310236,"mention ’nixon’ and ’president’ over time. To improve accuracy, CoTS combined this frequency signal with manually supplied constraints such as the functionality of the US presidency relation to scope the beginning and end of Nixon presidency. In contrast, the proposed system does not require constraints as input. There have also been tools and competitions developed to facilitate temporal scope extraction. TARSQI (Verhagen 2005) is a tool for automatically annotating time expressions in text. The TempEval (Verhagen 2007) challenge has led to a number of works on temporal relation extraction (Puscasu 2007; Yoshikawa 2009; Bethard 2007). 3 Method Given an entity and its Contextual Temporal Profile (CTP), we can learn when such an entity undergoes a specific state change. We can then directly infer the begin or end time of the fact associated with the state change. The CTP of an entity at a given time point t contains the context within which the entity is mentioned at that time. Our method is based on two related insights: i) the context of the entity at time t reflects the events happening to the entity and the state of the entity at time t. ii) the difference in context before, at time t − 1,"
D14-1207,P05-3021,0,0.0991344,"Missing"
D14-1207,S07-1014,0,0.104661,"Missing"
D14-1207,P09-1046,0,0.0371183,"Missing"
D14-1207,S07-1025,0,\N,Missing
D15-1059,N13-1055,0,0.126037,"Missing"
D15-1059,P13-1162,0,0.0284244,"Missing"
D15-1059,D14-1207,1,0.929355,"ellon University 5000 Forbes Avenue Pittsburgh, PA, 15213 dwijaya@cs.cmu.edu ndapa@cs.cmu.edu tom.mitchell@cs.cmu.edu Abstract by verbs acting on entities in text. This is different from simply applying the same text extraction pipeline, that created the original KB, to dynamic Web content. In particular, our approach has the following advantages: (1) Consider for example the SPOUSE relation, both marry and divorce are good patterns for extracting this relation. In our work, we wish to learn that they cause different state changes. Thus, we can update the entity’s fact and its temporal scope (Wijaya et al., 2014a). (2) Learning state changing verbs can pave the way for learning the ordering of verbs in terms of their pre- and post-conditions. Our approach learns state changing verbs from Wikipedia revision history. In particular, we seek to establish a correspondence between infobox edits and verbs edits in the same article. The infobox of a Wikipedia article is a structured box that summarizes an entity as a set of facts (attribute-value pairs) . Our assumption is that when a statechanging event happens to an entity e.g., a marriage, its Wikipedia infobox is updated by adding a new SPOUSE value. At"
D15-1059,W04-3205,0,0.0549925,"replace (arg2), +(arg1) join cabinet as (arg2), +(arg1) join as (arg2) +(arg1) lose seat to (arg2), +(arg1) resign on (arg2), +(arg1) resign from post on (arg2) +(arg1) be appointed on (arg2), +(arg1) serve from (arg2), +(arg1) be elected on (arg2) +(arg1) marry on (arg2), +(arg1) marry (arg2), +(arg1) be married on (arg2), -(arg1) be engaged to (arg2) +(arg1) file for divorce in (arg2), +(arg1) die on (arg2), +(arg1) divorce in (arg2) +(arg1) start career with (arg2), +(arg1) begin career with (arg2), +(arg1) start with (arg2) The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pantel, 2004; Hosseini et al., 2014) do not make a connection to KB relations such as Wikipedia infoboxes. In a vision paper, (Wijaya et al., 2014b) give high level descriptions of a number of possible methods for learning state changing methods. They did"
D15-1059,D13-1055,0,0.0758081,"ints that begin-spouse is mutex with end-spouse, these verbs (whose base form is “marry”) are filtered out from the features of end-spouse. We show some of the learned verb features (after feature selection) for some of the labels in (Table 1). In average, we have about 18 verbs per infobox state change in our state changing verb resource that we make available for future research. 4 Conclusion Related Work Learning from Wikipedia Revision History. Wikipedia edit history has been exploited in a number of problems. A popular task in this regard is that of Wikipedia edit history categorization (Daxenberger and Gurevych, 2013). This task involves characterizing a given edit instance as one of many possible categories such as spelling error correction, paraphrasing, vandalism, and textual entailment (Nelken and Yamangil, 2008; Cahill et al., 2013; Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013). Prior methods target various tasks different from ours. Learning State Changing Verbs. Very few works have studied the problem of learning state changing verbs. (Hosseini et al., 2014) learned state changing verbs in the context of solving arithmetic word problems. They learned the effect of words such as add, subtr"
D15-1059,D11-1142,0,0.234343,"Missing"
D15-1059,D14-1058,0,0.0269812,"arg2), +(arg1) die on (arg2), +(arg1) divorce in (arg2) +(arg1) start career with (arg2), +(arg1) begin career with (arg2), +(arg1) start with (arg2) The VerbOcean resource was automatically generated from the Web (Chklovski and Pantel, 2004). The authors studied the problem of fine-grained semantic relationships between verbs. They learn relations such as if someone has bought an item, they may sell it at a later time. This then involves capturing empirical regularities such as “X buys Y” happens before “X sells Y”. Unlike the work we present here, the methods of (Chklovski and Pantel, 2004; Hosseini et al., 2014) do not make a connection to KB relations such as Wikipedia infoboxes. In a vision paper, (Wijaya et al., 2014b) give high level descriptions of a number of possible methods for learning state changing methods. They did not implement any of them. Table 1: Comparison of verb phrases learned before and after feature selection for various labels (infobox types). The texts in bold are (preposition+) noun that occur most frequently with the hverb phrase, labeli pair in the training data. 5 In this paper we presented a method that learns state changing verb phrases from Wikipedia revision history. W"
D15-1059,P14-5010,0,0.00481342,"t that we release can be seen in Figure 2. We show only labels that we evaluate in our task. For our task of learning state changing verbs from this revision history dataset, for each labeled dp,t , we extract as features, verbs (or verbs+prepositions) v ∈ ∆Cp,t of which its subject (or object) matches the Wikipedia entity p and its object (or subject resp.) matches an infobox value, start or end time: (vsubject , vobject ) = (arg1, arg2) or (vsubject , vobject ) = (arg2, arg1), where arg1= p and hsatt ,arg2, ∗, ∗i or hsatt , ∗,arg2, ∗i or hsatt , ∗, ∗,arg2i ∈ ∆Sp,t . We use Stanford CoreNLP (Manning et al., 2014) to dependency parse sentences and extract the subjects and objects of verbs. We find that 27,044 out of the 41,139 labeled documents contain verb edits, but only 4,735 contain verb edits with two arguments, where one argument matches the entity and another matches the value of the infobox change. We use the latter for our task, to improve the chance that the verb edits used as features are related to the infobox change. labels in our training data. The classifier can then be applied to classify an unlabeled document du using: exp(wy · vdu ) y 0 exp(wy 0 · vdu ) p(y|vdu ) = P 2.3 (1) Feature S"
D15-1059,W12-3008,1,0.876841,"Missing"
D15-1059,W10-3504,0,\N,Missing
D15-1127,C08-1007,0,0.0669954,"Missing"
D15-1127,P14-5004,0,0.0226708,"Missing"
D15-1127,E14-1049,0,0.171658,"Missing"
D15-1127,N13-1092,0,0.0184137,"Missing"
D15-1127,P15-1119,0,0.0481052,"Missing"
D15-1173,D13-1160,0,0.0100185,"nique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link p"
D15-1173,D14-1165,0,0.0251102,"e KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al., 2014; Wang et al., 2014). These methods perform well when there is structural redundancy in the knowledge base tensor, but when the tensor (or individual relations in the tensor) has high rank, learning good embeddings can be challenging. The ARE model (Nickel et al., 2014) attempted to address this by only making the embeddings capture the residual of the tensor that cannot be readily predicted from the graph-based techniques mentioned below. 1490 Dataset Freebase NELL Method Probabilities Binarized Probabilities Binarized MAP .337 .344 .303 .319 Table 1: Using binary"
D15-1173,P15-1127,0,0.0235711,"on Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the f"
D15-1173,D13-1080,1,0.739227,"one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, or whether we might mor"
D15-1173,P11-1055,0,0.196105,"res than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm ("
D15-1173,D12-1069,1,0.625098,"Missing"
D15-1173,D11-1049,1,0.848931,"hnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity. In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between n"
D15-1173,D12-1093,0,0.0644912,"is work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, o"
D15-1173,mendes-etal-2012-dbpedia,0,0.018079,"ll subgraph feature extraction (SFE). In addition to being conceptually simpler than PRA, SFE is much more efficient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 1 Introduction Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world. These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013). While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015). The task of knowledge base completion—filling in missing facts by examining the facts already in the KB, or by l"
D15-1173,P09-1113,0,0.0627354,"knowledge base completion have the same goal: to predict new instances of relations in a formal knowledge base such as Freebase or NELL. The difference is that relation extraction focuses on determining what relationship is expressed by a particular sentence, while knowledge base completion tries to predict which relationships hold between which entities. A relation extraction system can be used for knowledge base completion, but typical KB completion methods do not make predictions on single sentences. This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riede"
D15-1173,D14-1044,1,0.641963,"ming knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014). PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph. The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph. While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014). PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix). This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix. In this paper we consider whether this computational effort is wellspent, or whether we might more profitably spend comp"
D15-1173,P15-1016,0,0.445107,"nto this category, as does ProPPR (Wang et al., 2013) and many other logic-based systems. PRA, the main subject of this paper, also fits in this line of work. Work specifically with PRA has ranged from incorporating a parsed corpus as additional evidence when doing random walk inference (Lao et al., 2012), to introducing better representations of the text corpus (Gardner et al., 2013; Gardner et al., 2014), and using PRA in a broader context as part of Google’s Knowledge Vault (Dong et al., 2014). An interesting piece of work that combines embedding methods with graph-based methods is that of Neelakantan et al. (2015), which uses a recursive neural network to create embedded representations of PRA-style paths. 4 Motivation We motivate our modifications to PRA with three observations. First, it appears that binarizing the feature matrix produced by PRA, removing most of the information gained in PRA’s second step, has no significant impact on prediction performance in knowledge base completion tasks. We show this on the NELL KB and the Freebase KB in Table 1.3 The fact that random walk probabilities carry no additional information for this task over binary features is surprising, and it shows that the secon"
D15-1173,N13-1008,0,0.0603883,"2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al., 2014; Wang et al., 2"
D15-1173,D12-1042,0,0.0113149,"l: to predict new instances of relations in a formal knowledge base such as Freebase or NELL. The difference is that relation extraction focuses on determining what relationship is expressed by a particular sentence, while knowledge base completion tries to predict which relationships hold between which entities. A relation extraction system can be used for knowledge base completion, but typical KB completion methods do not make predictions on single sentences. This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB compl"
D15-1173,D13-1136,0,0.0858641,"action (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB. The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013). Embedding methods for KB completion. There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships. Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011). These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al."
D17-1128,P98-1013,0,0.457235,"rapyD ECISION ]. [WeH EALER ] decided to treat [the patientPATIENT ] [with combination chemotherapyT REATMENT ]. Introduction One way to represent meaning is through organization of semantic structures. Consider the following sentences “John sells Marry a car.” and “Mary buys a car from John.”. While having different syntactic structures, they express the same type of event that involves a buyer, a seller, and goods. Such meaning can be represented using semantic frames – structured representations that characterize events, scenarios, and the participants. Researchers have developed FrameNet (Baker et al., 1998; Fillmore et al., 2003), a large lexical database of English that comes with sentences annotated with semantic frames. It has been considered a valuable resource for Natural Language Processing and useful for studying tasks such as We address frame identification and semantic role labeling in this work.1 Frame identification can be addressed as a word sense disambiguation problem, while semantic role labeling can be formulated as a structured prediction problem. We train different neural network models for these two problems, and interpret their outputs as factors in a graphical model for per"
D17-1128,C10-3009,0,0.0871002,"Missing"
D17-1128,W05-0620,0,0.0700603,"Missing"
D17-1128,W14-3007,0,0.0126905,"a and Jurafsky (2002). This work focuses on extracting semantic frames defined in FrameNet (Baker et al., 1998), which includes predicting frame types and frame-specific semantic roles. Our model can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent work has exploited neural networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract"
D17-1128,J14-1002,0,0.314461,"ion and semantic role labeling in this work.1 Frame identification can be addressed as a word sense disambiguation problem, while semantic role labeling can be formulated as a structured prediction problem. We train different neural network models for these two problems, and interpret their outputs as factors in a graphical model for performing joint inference over the distribution of frames and semantic roles. Specifically, our frame identification model is a simple multi-layer neural network that learns ap1 We do not consider target identification due to the lack of consistent labeled data (Das et al., 2014). 1247 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1247–1256 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics propriate feature representations for frame disambiguation. Our SRL model is an integrated model of an LSTM-based network that learns to predict semantic roles on a word-by-word basis and a multi-layer network that learns to directly predict semantic roles for individual text spans in relation to a given predicate. The sequential neural network is powerful for modeling sentencelevel information"
D17-1128,P15-1033,0,0.00496127,"dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) employs stack LSTMs (Dyer et al., 2015) for joint syntacticsemantic dependency parsing. He et al. (2017) recently proposed further improvements to the DBLSTM architecture which significantly improve the state of the art results on PropBank SRL. In order to enforce structural consistency, most existing work applies different types of structural constraints during inference. The inference problem are typically solved via Integer Linear Programming (ILP) (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) improves the inference efficiency with a dynamic programming algorithm that encodes tractable global constraints. Recently, Belang"
D17-1128,D15-1112,0,0.20665,"Missing"
D17-1128,J02-3001,0,0.626218,". We evaluate our models for frame identification, SRL, and full structure extraction on the FrameNet 1.5 data. Our full model achieves 76.6 F1, a 5.7 absolute gain over the prior state of the art. We also evaluate our SRL model on CoNLL 2005. It demonstrates strong performance that is close to the best published results. Error analysis further confirms the benefits of integrating sequential and relational models and performing joint inference over frames and semantic roles. 2 Related Work Research on automatic semantic structure extraction has been widely studied since the pioneering work of Gildea and Jurafsky (2002). This work focuses on extracting semantic frames defined in FrameNet (Baker et al., 1998), which includes predicting frame types and frame-specific semantic roles. Our model can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent wo"
D17-1128,P17-1044,0,0.0345803,"mbeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) employs stack LSTMs (Dyer et al., 2015) for joint syntacticsemantic dependency parsing. He et al. (2017) recently proposed further improvements to the DBLSTM architecture which significantly improve the state of the art results on PropBank SRL. In order to enforce structural consistency, most existing work applies different types of structural constraints during inference. The inference problem are typically solved via Integer Linear Programming (ILP) (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) improves the inference efficiency with a dynamic programming algorithm that encodes tractable global constraints. Recently, Belanger et al. (2017) models SRL using end-to-end structured predictio"
D17-1128,P14-1136,0,0.870357,"tification, including results on all predicates and ambiguous predicates in the FrameNet lexicon. For all our experimental results, we perform statistical significance tests using the paired bootstrap test (Efron and Tibshirani, 1994) with 1000 bootstrap samples of the evaluated examples, and use ∗ to indicate statistical significance (p &lt; 0.05) of the differences between our best model and our second best model. 7.4 FrameNet Results Frame Identification. We first evaluate our frame identification model in § 5. For baselines, we consider the prior state-of-the-art approach W SABIE E MBEDDING (Hermann et al., 2014), which learns feature representations based on word embeddings and dependency path embeddings using the W SA BIE algorithm (Weston et al., 2011). We also include two strong baselines implemented in Hermann et al. (2014): L OG -L INEAR W ORDS and L OG -L INEAR E MBEDDINGS, which are both loglinear models, one with standard linguistic features and one with embedding features. Table 1 shows the results.8 We can see that our model in general gives competitive performance and it outperforms all the baselines on predicting frames for ambiguous predicates (i.e., seen with more than one possible fram"
D17-1128,S07-1048,0,0.034338,"ely studied since the pioneering work of Gildea and Jurafsky (2002). This work focuses on extracting semantic frames defined in FrameNet (Baker et al., 1998), which includes predicting frame types and frame-specific semantic roles. Our model can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent work has exploited neural networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutio"
D17-1128,P15-2036,0,0.258505,"Missing"
D17-1128,C10-1081,0,0.13446,"Missing"
D17-1128,P14-5010,0,0.00188594,"the exemplar sentences generally introduces a 3-4 F1 gain for FrameNet SRL. 1251 structure extraction precision, recall and F16 . For PropBank-style SRL, we use the CoNLL2005 data set (Carreras and M`arquez, 2005) with the official scripts7 for evaluation. It contains section 2-21 of WallStreet Journal (WSJ) data as training set, section 24 as development set and section 23 of WSJ concatenated with 3 sections from Brown corpus as the test set. For data pre-processing, we parse all the sentences with the part-of-speech tagger and the dependency parser provided in the Stanford CoreNLP toolkit (Manning et al., 2014). 7.2 Argument candidate extraction Existing work relied on either constituency syntax (Xue and Palmer, 2004) or dependency syntax (T¨ackstr¨om et al., 2015) to derive heuristic rules for extracting candidate arguments. Instead, we extract candidate arguments using a pretrained sequential SRL model (described in § 4.1). Specifically, we extract the argument spans from the K-best semantic role label sequences output by the sequential model. We choose K from {5,10,20,50}. Increasing K will increase the recall of unlabeled arguments but lower the precision. We tune K based on the argument extract"
D17-1128,J05-1004,0,0.878683,"g performance that is close to the best published results. Error analysis further confirms the benefits of integrating sequential and relational models and performing joint inference over frames and semantic roles. 2 Related Work Research on automatic semantic structure extraction has been widely studied since the pioneering work of Gildea and Jurafsky (2002). This work focuses on extracting semantic frames defined in FrameNet (Baker et al., 1998), which includes predicting frame types and frame-specific semantic roles. Our model can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent work has exploited neural networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGer"
D17-1128,P03-1002,0,0.0677798,"Missing"
D17-1128,K16-1019,0,0.0221658,"atures. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) employs stack LSTMs (Dyer et al., 2015) for joint syntacticsemantic dependency parsing. He et al. (2017) recently proposed further improvements to the DBLSTM architecture which significantly improve the state of the art results on PropBank SRL. In order to enforce structural consistency, most existing work applies different types of structural constraints during inference. The inference problem are typically solved via Integer Linear Programming (ILP) (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) improves the inference efficiency with a dynamic programming algorithm that encodes tracta"
D17-1128,Q15-1003,0,0.21288,"Missing"
D17-1128,J08-2002,0,0.0614498,"80.6 80.3 79.4 82.8 79.9 79.4 80.3 70.1 68.8 67.8 69.4 71.3 71.2 72.2 Ours (Seq) Ours (Rel) Ours (Seq+Rel) 78.5 79.2 80.3 80.5 81.4 81.9 70.8 71.3 72.0∗ Table 5: Semantic role labeling results on CoNLL 2005. Figure 3: Full structure F1 on the FrameNet test set by the sentence length. joint inference of both frames and semantic roles, our model performs even better, achieving a 5.7 absolute F1 gain over the prior state of the art. 7.5 CoNLL Results Table 5 shows the results of our SRL models on the CoNLL 2005 data. Our baselines include the best feature-based systems of Surdeanu et al. (2007), Toutanova et al. (2008), and Punyakanok et al. (2008), the recurrent neural network model (DB-LSTM) (Zhou and Xu, 2015), the graphical model with global factors (T¨ackstr¨om et al., 2015) and the improved versions that use neural network factors (FitzGerald et al., 2015). Note that our sequential model in this setting is essentially the same as the DB-LSTM model (Zhou and Xu, 2015) since all the frame-specific constraints are removed, except that we use simpler input features.10 We observe a similar performance trend among our models. However, the performance gain introduced by our integrated model is relatively sma"
D17-1128,W04-3212,0,0.0393653,"Missing"
D17-1128,P15-1109,0,0.338931,"l networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) employs stack LSTMs (Dyer et al., 2015) for joint syntacticsemantic dependency parsing. He et al. (2017) recently proposed further improvements to the DBLSTM architecture which significantly improve the state of the art results on PropBank SRL. In order to enforce structural consistency, most existing work applies different types of structural constraints during inference. The inference problem are typically solved via Integer Linear Programming"
D17-1128,J08-2005,0,0.337679,"s a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) employs stack LSTMs (Dyer et al., 2015) for joint syntacticsemantic dependency parsing. He et al. (2017) recently proposed further improvements to the DBLSTM architecture which significantly improve the state of the art results on PropBank SRL. In order to enforce structural consistency, most existing work applies different types of structural constraints during inference. The inference problem are typically solved via Integer Linear Programming (ILP) (Punyakanok et al., 2008). T¨ackstr¨om et al. (2015) improves the inference efficiency with a dynamic programming algorithm that encodes tractable global constraints. Recently, Belanger et al. (2017) models SRL using end-to-end structured prediction energy networks and demonstrates benefits of accounting for complex structural dependencies during training. In this work, we explicitly encode structural constraints as factors in a graphical model, and adopt the Alternating Directions Dual Decomposition (AD3 ) algorithm (Martins et al., 2011) for efficient inference. 3 Overview We aim to extract frame-semantic structures"
D17-1128,Q15-1032,0,0.0949876,"amat Framat+context 69.2 71.1 71.1 65.1 63.7 64.8 67.1 67.2 67.8 Hermann T¨ackstr¨om (Struct.) FitzGerald (Struct.) FitzGerald (Struct., PoE) FitzGerald (Local, PoE, Joint) 74.3 75.4 74.8 74.6 75.0 66.0 65.8 65.5 66.3 67.3 69.9 70.3 69.9 70.2 70.9 Ours (Seq) Ours (Rel) Ours (Seq+Rel) Ours (JointAll) 69.6 77.1 77.3 78.8 70.9 68.7 71.2 74.5 70.2 72.7 74.1 76.6∗ Table 4: Full structure extraction results on the FrameNet test set in comparison to the previously published results. Table 3: Full structure extraction results on the FrameNet test set (with gold frames) in comparison to the results in Roth and Lapata (2015). 9 Model role label or ∅ to each candidate argument span. We compare with previous work using four model variants: three are pipeline models that combine our frame identification model with each of our SRL models and JointAll is the joint model that simultaneously predicts frames and roles as described in § 6. Table 4 compares our models with previously published results. The first block shows results from Roth and Lapata (2015) and the second block shows results from FitzGerald et al. (2015). All these previous methods implements a pipeline of frame identification and semantic role labeling."
D17-1128,P16-1113,0,0.00632084,"el can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent work has exploited neural networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and achieves stateof-the-art results on PropBank-style SRL. Recently, Swayamdipta et al. (2016) empl"
D17-1128,D14-1045,0,0.0133453,"er et al., 1998), which includes predicting frame types and frame-specific semantic roles. Our model can be easily adapted to predict PropBank-style semantic roles (Palmer et al., 2005), where role labels are generic instead of frame-specific. The core problem in semantic frame extraction is semantic role labeling (SRL). Earlier SRL systems employ linear classifiers which rely heavily on hand-engineered feature templates to represent argument structures (Johansson and Nugues, 2007; Das et al., 2010; Das, 2014). Recent work has exploited neural networks to learn better feature representations. Roth and Woodsend (2014) improves the feature-based system by adding word embeddings as features. Roth and Lapata (2016) further includes dependency path embeddings as features. FitzGerald et al. (2015) embeds the standard SRL features into a low-dimensional vector space using a feed-forward neural network and demonstrates state-of-the-art results on FrameNet. Different neural network architectures have also been explored for SRL. Collobert et al. (2011) first applies a convolutional neural network to extract features from a window of words. Zhou and Xu (2015) employs a deep bi-directional LSTM (DB-LSTM) network and"
D17-1128,D07-1002,0,0.698052,"Missing"
D17-1128,C98-1013,0,\N,Missing
D17-1161,D15-1198,0,0.0211352,"Missing"
D17-1161,D13-1160,0,0.0610942,"g from few examples, and presents encouraging results for one-shot learning by learning representations over Bayesian programs. However, none of these address the issue of learning from natural language. Semantic interpretation of language has been explored in diverse domains. While semantic parsers have traditionally relied on labeled datasets of statements paired with labeled logical forms (Zettlemoyer and Collins, 2005), recent approaches have focused on training semantic parsers from 1528 denotations of logical forms, rather than logical forms themselves (Krishnamurthy and Mitchell, 2012; Berant et al., 2013). Our work extends this paradigm by attempting to learn from still weaker signal, where denotations (evaluations) of logical forms too are not directly observed. Similar to our work, previous approaches have used different kinds of external-world signals to guide semantic interpretation (Liang et al., 2009; Branavan et al., 2009). Natural instructions have been studied in game playing frameworks (Branavan et al., 2012; Eisenstein et al., 2009). Our work is also closely related to work by Goldwasser and Roth (2014); Clarke et al. (2010), who also train semantic parsers in weakly supervised cont"
D17-1161,P09-1010,0,0.0122146,"labeled datasets of statements paired with labeled logical forms (Zettlemoyer and Collins, 2005), recent approaches have focused on training semantic parsers from 1528 denotations of logical forms, rather than logical forms themselves (Krishnamurthy and Mitchell, 2012; Berant et al., 2013). Our work extends this paradigm by attempting to learn from still weaker signal, where denotations (evaluations) of logical forms too are not directly observed. Similar to our work, previous approaches have used different kinds of external-world signals to guide semantic interpretation (Liang et al., 2009; Branavan et al., 2009). Natural instructions have been studied in game playing frameworks (Branavan et al., 2012; Eisenstein et al., 2009). Our work is also closely related to work by Goldwasser and Roth (2014); Clarke et al. (2010), who also train semantic parsers in weakly supervised contexts, where language interpretation is integrated in real-world tasks. The general idea of learning through human interactions has previously been explored in settings such as behavioral programming (Harel et al., 2012), natural language programming (Biermann, 1983), learning by instruction (Azaria et al., 2016), etc. To the best"
D17-1161,W10-2903,0,0.027855,"ogical forms themselves (Krishnamurthy and Mitchell, 2012; Berant et al., 2013). Our work extends this paradigm by attempting to learn from still weaker signal, where denotations (evaluations) of logical forms too are not directly observed. Similar to our work, previous approaches have used different kinds of external-world signals to guide semantic interpretation (Liang et al., 2009; Branavan et al., 2009). Natural instructions have been studied in game playing frameworks (Branavan et al., 2012; Eisenstein et al., 2009). Our work is also closely related to work by Goldwasser and Roth (2014); Clarke et al. (2010), who also train semantic parsers in weakly supervised contexts, where language interpretation is integrated in real-world tasks. The general idea of learning through human interactions has previously been explored in settings such as behavioral programming (Harel et al., 2012), natural language programming (Biermann, 1983), learning by instruction (Azaria et al., 2016), etc. To the best of our knowledge, this work is the first to use semantic interpretation to guide concept learning. 3 Method We consider concept learning problems in which the goal is to approximate an unknown classification f"
D17-1161,D09-1100,0,0.22025,"es have focused on training semantic parsers from 1528 denotations of logical forms, rather than logical forms themselves (Krishnamurthy and Mitchell, 2012; Berant et al., 2013). Our work extends this paradigm by attempting to learn from still weaker signal, where denotations (evaluations) of logical forms too are not directly observed. Similar to our work, previous approaches have used different kinds of external-world signals to guide semantic interpretation (Liang et al., 2009; Branavan et al., 2009). Natural instructions have been studied in game playing frameworks (Branavan et al., 2012; Eisenstein et al., 2009). Our work is also closely related to work by Goldwasser and Roth (2014); Clarke et al. (2010), who also train semantic parsers in weakly supervised contexts, where language interpretation is integrated in real-world tasks. The general idea of learning through human interactions has previously been explored in settings such as behavioral programming (Harel et al., 2012), natural language programming (Biermann, 1983), learning by instruction (Azaria et al., 2016), etc. To the best of our knowledge, this work is the first to use semantic interpretation to guide concept learning. 3 Method We cons"
D17-1161,N16-1074,0,0.0246618,"Missing"
D17-1161,D12-1069,1,0.448681,"Missing"
D17-1161,P15-1024,0,0.021674,"Missing"
D17-1161,P12-2018,0,0.00851212,"ional supervised learning methods on the task of learning email-based concepts described in the previous section. Our baselines include the following models: Text-only models: • BoW: A logistic regression (LR) classifier over bag-of-words representation of emails • BoW tf-idf: LR classifier over bag-of-words representation, with tf-idf weighting • Para2Vec: LR classifier over a distributed representation of documents, using deep neural network approach by Le and Mikolov (2014). • Bigram: LR model also incorporating bigram features, known to be competitive on several text classification tasks (Wang and Manning, 2012). • ESA: LR model over ESA (Explicit Semantic Analysis) representations of emails (Gabrilovich and Markovitch, 2007), which describe a text in terms of its Wikipedia topics. Models incorporating Statements: • RTE: This uses a Textual Entailment model (based on features from Sachan et al. (2015)) that computes a score for aligning of each statement to the text of each email. A logistic regression is trained over this representation of the data. • Keyword filtering: Filters based on keywords are common in email systems. We add this as a baseline by manually filtering statements referring to occu"
D17-1161,D07-1071,0,0.0311992,"Missing"
D17-1161,P09-1011,0,0.00951875,"ditionally relied on labeled datasets of statements paired with labeled logical forms (Zettlemoyer and Collins, 2005), recent approaches have focused on training semantic parsers from 1528 denotations of logical forms, rather than logical forms themselves (Krishnamurthy and Mitchell, 2012; Berant et al., 2013). Our work extends this paradigm by attempting to learn from still weaker signal, where denotations (evaluations) of logical forms too are not directly observed. Similar to our work, previous approaches have used different kinds of external-world signals to guide semantic interpretation (Liang et al., 2009; Branavan et al., 2009). Natural instructions have been studied in game playing frameworks (Branavan et al., 2012; Eisenstein et al., 2009). Our work is also closely related to work by Goldwasser and Roth (2014); Clarke et al. (2010), who also train semantic parsers in weakly supervised contexts, where language interpretation is integrated in real-world tasks. The general idea of learning through human interactions has previously been explored in settings such as behavioral programming (Harel et al., 2012), natural language programming (Biermann, 1983), learning by instruction (Azaria et al.,"
D17-1161,P11-1060,0,0.211492,"loglinear model (with associated parameters θc ). We provide more details in Section 3.3. On the other hand, the probability of the latent statement evaluation values z can be parametrized using a probabilistic semantic parsing model (with associated parameters θp ). The second term decouples over evaluations P of individual statements (log pθp (z |x, S) = j log pθp (zj |x, sj )). In 1529 turn, since we never observe the correct interpretation l for any statement, but only model its evaluation zj , we marginalize over all interpretations whose evaluations in a context x matches zj (similar to Liang et al. (2011)). log pθp (zj |x, sj ) = log X pθp (l |sj ) (2) l:JlKx =zj Following recent work in semantic parsing (Liang and Potts, 2015; Krishnamurthy and Mitchell, 2012), we use a log-linear model over logical forms: pθp (l |s) ∝ exp(θp T φ(s, l)) (3) where φ(s, l) ∈ Rd is a feature vector over statements s and logical interpretations l. 3.2 Learning: In Equation 1, q(z) denotes a distribution over evaluation values of statements; whereas θc and θp denote the model parameters for the classifier and semantic parser. The learning algorithm consists of an iterative generalized EM procedure, which can be in"
D17-1161,P11-1028,0,\N,Missing
D18-1039,P15-1166,0,0.642249,"the mapping of a source language to a target language without any need for training or tuning any component of the system separately. This has led to a rapid progress in NMT and its successful adoption in many large-scale settings (Wu et al., 2016; Crego et al., 2016). The encoder-decoder abstraction makes it conceptually feasible to build a system that maps any source sentence in any language to a vector representation, and then decodes this representation into any target language. Thus, various approaches have been proposed to extend this abstraction for multilingual MT (Luong et al., 2016; Dong et al., 2015; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016a). Prior work in multilingual NMT can be broadly categorized into two paradigms. The first, univer1 In fact, it could likely be applied in other scenarios, such as domain adaptation, as well. 425 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 425–435 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics be applied to most existing NMT systems with some minor modification, and it is able to accommodate attention layers seamlessly. 2. Multilingua"
D18-1039,D18-1549,0,0.0321517,"a model on a many-to-one translation task. Closest to our work are more recent approaches, already described in Section 2 (Firat et al., 2016a; Johnson et al., 2017; Ha et al., 2016), that attempt to enforce different kinds of parameter sharing across languages. Parameter sharing in multilingual NMT naturally enables semi-supervised and zero-shot learning. Unsupervised learning has been previously explored with key ideas such as back-translation (Sennrich et al., 2016a), dual learning (He et al., 2016), common latent space learning (Lample et al., 2018), etc. In the vein of multilingual NMT, Artetxe et al. (2018) proposed a model that uses a shared encoder and multiple decoders with a focus on unsupervised translation. The entire system uses cross-lingual embeddings and is trained to reconstruct its input using only monolingual data. Zero-shot translation was first attempted in (Firat et al., 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was a"
D18-1039,N16-1101,0,0.611934,"single model for all languages. Universal NMT lacks any language-specific parameterization, which is an oversimplification and detrimental when we have very different languages and limited training data. As verified by our experiments, the method of Johnson et al. (2017) suffers from high sample complexity and thus underperforms in limited data settings. The universal model proposed by Ha et al. (2016) requires a new coding scheme for the input sentences, which results in large vocabulary sizes that are difficult to scale. The second paradigm, per-language encoder-decoder (Luong et al., 2016; Firat et al., 2016a), uses separate encoders and decoders for each language. This does not allow for sharing of information across languages, which can result in overparameterization and can be detrimental when the languages are similar. In this paper, we strike a balance between these two approaches, proposing a model that has the ability to learn parameters separately for each language, but also share information between similar languages. We propose using a new contextual parameter generator (CPG) which (a) generalizes all of these methods, and (b) mitigates the aforementioned issues of universal and per-lan"
D18-1039,D16-1026,0,0.128128,"Missing"
D18-1039,W16-2358,0,0.0355132,"Missing"
D18-1039,N18-1032,0,0.0438699,", 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was attempted in (Johnson et al., 2017) using their shared encoderdecoder network. An iterative training procedure that leverages the duality of translations directly generated by the system for zero-shot learning was proposed by Lakew et al. (2017). For extremely low resource languages, Gu et al. (2018) proposed sharing lexical and sentence-level representations across multiple source languages with a single target language. Closely related is the work of Cheng et al. (2016) who proposed the joint training of source-to-pivot and pivot-to-target NMT models. Ha et al. (2018) are probably the first to introduce a similar idea to that of having one network (called a hypernetwork) generate the parameters of another. However, in that work, the input to the hypernetwork are structural features of the original network (e.g., layer size and index). Al-Shedivat et al. (2017) also propose a related met"
D18-1039,2004.iwslt-evaluation.1,0,0.0530795,"Missing"
D18-1039,P17-1176,0,0.0574597,"(Lample et al., 2018), etc. In the vein of multilingual NMT, Artetxe et al. (2018) proposed a model that uses a shared encoder and multiple decoders with a focus on unsupervised translation. The entire system uses cross-lingual embeddings and is trained to reconstruct its input using only monolingual data. Zero-shot translation was first attempted in (Firat et al., 2016b) who performed zero-zhot translation using their pre-trained multi-way multilingual model, fine-tuning it with pseudo-parallel data generated by the model itself. This was recently extended using a teacher-student framework (Chen et al., 2017). Later, zero-shot translation without any additional steps was attempted in (Johnson et al., 2017) using their shared encoderdecoder network. An iterative training procedure that leverages the duality of translations directly generated by the system for zero-shot learning was proposed by Lakew et al. (2017). For extremely low resource languages, Gu et al. (2018) proposed sharing lexical and sentence-level representations across multiple source languages with a single target language. Closely related is the work of Cheng et al. (2016) who proposed the joint training of source-to-pivot and pivo"
D18-1039,Q17-1024,0,0.161978,"Missing"
D18-1039,D15-1166,0,0.0540913,"ate representation that can later be used by a decoder to generate sentences in a target language. Generally, we can think of the encoder as a function, f (enc) , parameterized by θ(enc) . Similarly, we can think of the decoder as another function, f (dec) , parameterized by θ(dec) . The goal of learning to translate can then be defined as finding the values for θ(enc) and θ(dec) that result in the best translations. A large amount of previous work proposes novel designs for the encoder/decoder module. For example, using attention over the input sequence while decoding (Bahdanau et al., 2015; Luong et al., 2015) provides significant gains in translation performance.2 Parameter Generator. All modules defined so far have previously been used when describing NMT systems and are thus easy to conceptualize. However, in previous work, most models are trained for a given language pair, and it is not trivial to extend them to work for multiple pairs of languages. We introduce here the concept of the parameter generator, which makes it easy to define and describe multilingual NMT systems. This module is responsible for generating θ(enc) and θ(dec) for any given source and target language. Different parameter"
D18-1039,P18-2050,1,0.726171,"= {θj }j=1 and (enc) (enc) Pj θj ∈R , where G denotes the number of groups. Then, we define: Parameter Generator Network (enc) We refer to the functions g (enc) and g (dec) as parameter generator networks. Even though our proposed NMT framework does not rely on a specific choice for g (enc) and g (dec) , here we describe the functional form we used for our experiments. Our goal is to provide a simple form that works, and for which we can reason about. For this reason, we decided to define the parameter generator networks as simple linear transforms, similar to the factored adaptation model of Michel and Neubig (2018), which was only applied to the bias terms of the output softmax: g (enc) (ls ) , W(enc) ls , g (dec) (lt ) , W (dec) lt , θj (enc) (enc) , Wj (enc) (enc) Pj 0 ls , (3) (enc) where Wj ∈ RPj ×M and Pj ∈ 0 ×M M 0 R , with M < M (and similarly for the de(enc) coder parameters). We can see now that Pj is used to extract the relevant information (size M 0 ) for parameter group j, from the larger language embedding (size M ). This allows us to control the parameter sharing across languages in the following way: if we want to increase the number of per-language parameters (i.e., the language embeddin"
D18-1039,N16-1004,0,0.047795,"language pairs. 7 Note that, our results for IWSLT-17 are not comparable to those of the official challenge report (Cettolo et al., 2017), as we use less training data, a smaller baseline model, and our evaluation pipeline potentially differs. However, the numbers presented for all methods in this paper are comparable, as they were all obtained using the same baseline model and evaluation pipeline. 8 https://github.com/eaplatanios/symphony-mt https://github.com/tensorflow/nmt 10 https://github.com/rsennrich/subword-nmt 9 432 (Caglayan et al., 2016) for translation across multiple modalities. Zoph and Knight (2016) flipped this idea with a many-to-one translation model, however requiring the presence of a multi-way parallel corpus between all the languages, which is difficult to obtain. Lee et al. (2017) used a single character-level encoder across multiple languages by training a model on a many-to-one translation task. Closest to our work are more recent approaches, already described in Section 2 (Firat et al., 2016a; Johnson et al., 2017; Ha et al., 2016), that attempt to enforce different kinds of parameter sharing across languages. Parameter sharing in multilingual NMT naturally enables semi-superv"
D18-1039,P16-1009,0,0.513938,"art performance. We first introduce a modular framework that can be used to define and describe most existing NMT systems. Then, in Section 3, we introduce our main contribution, the contextual parameter generator (CPG), in terms of that framework. We also argue that the proposed approach takes us a step closer to a common universal interlingua. 2 two-way mapping from preprocessed sentences to sequences of word indices that will be used for the translation. A commonly used proposal for defining the vocabulary is the byte-pair encoding (BPE) algorithm which generates subword unit vocabularies (Sennrich et al., 2016b). This eliminates the notion of out-of-vocabulary words, often resulting in increased translation quality. Encoder/Decoder. The encoder takes in indexed source language sentences, and produces an intermediate representation that can later be used by a decoder to generate sentences in a target language. Generally, we can think of the encoder as a function, f (enc) , parameterized by θ(enc) . Similarly, we can think of the decoder as another function, f (dec) , parameterized by θ(dec) . The goal of learning to translate can then be defined as finding the values for θ(enc) and θ(dec) that resul"
D18-1195,P09-1010,0,0.0459524,"uracy on unlabeled questions, partitioned by feedback complexity (i.e., number of corrections expressed in a single feedback utterance). Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017); learning from indirect supervision from a large-scale knowledge base (Reddy et al., 2014; Krishnamurthy and Mitchell, 2012); learning from conversations of systems asking for and confirming information (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Padmakumar et al., 2017); and learning from interactions with a simulated world environment (Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Goldwasser and Roth, 2014; Misra et al., 2015). The supervision used in these methods is mostly in the form of binary feedback, partial logical forms (e.g., slots) or execution results. In this paper, we explore a new form of supervision – natural language feedback. We demonstrate that such feedback not only provides rich and expressive supervisory signals for learning but also can be easily collected via crowd-sourcing. Recent work (Iyer et al., 2017) trains an online language-to-SQL parser from user feedback. Unlike our work, their collected feedback is structu"
D18-1195,W10-2903,0,0.140089,"s further exacerbated in semantic parsing by the fact that these annotations can only be done by people familiar with the underlying logical language, making it challenging to construct large scale datasets by non-experts. Over the years, this practical observation has spurred many creative solutions to training semantic parsers that are capable of leveraging weaker forms of supervision, amenable to non-experts. One such weaker form of supervision relies on logical form denotations (i.e, the results of a logical form’s execution) – rather than the logical form itself, as “supervisory” signal (Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017). In Question Answeing (QA), for example, this means the annotator needs only to know the answer to a question, rather than the full SQL query needed to obtain that answer. Paraphrasing of utterances already annotated with logical forms is another practical approach to scale up annotation without requiring experts with a knowledge of the underlying logical formalism (Berant and Liang, 2014; Wang et al., 2015). Although these and similar methods do reduce the difficulty of the anno"
D18-1195,D09-1100,0,0.266185,"cing. Recent work (Iyer et al., 2017) trains an online language-to-SQL parser from user feedback. Unlike our work, their collected feedback is structured and is used for acquiring more labeled data during training. Our model jointly learns from questions and feedback and can be trained with limited labeled data. There has been a growing interest on machine learning from natural language instructions. Much work has been done in the setting where an autonomous agent learns to complete a task in an environment, for example, learning to play games by utilizing text manuals (Branavan et al., 2012; Eisenstein et al., 2009; Narasimhan et al., 2015) and guiding policy learning using high-level human advice (Kuhlmann et al., 2004; Squire et al., 2015; Harrison et al., 2017). Recently, natural language explanations have been used to augment laConclusion and Future Work In this work, we proposed a novel task of learning a semantic parser directly from end-users’ openended natural language feedback during a conversation. The key advantage of being able to learn from natural language feedback is that it opens the door to learning continuously through natural interactions with the user, but it also presents a challeng"
D18-1195,P17-1089,0,0.106227,"Missing"
D18-1195,P16-1002,0,0.0171479,"s in an utterance (tokenized) u and computes a context-sensitive embedding hi for each token ui using a bidirectional Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), where hi is computed as the concatenation of the hidden states at position i output by the forward LSTM and the backward LSTM. The decoder generates the output logical form y one token at a time using another LSTM. At each time step j, it generates yj based on the current LSTM hidden state sj , a summary of the input context cj , and attention scores aji , which are used for attention-based copying as in (Jia and Liang, 2016). Specifically, p(yj = w, w ∈ Vout |u, y1:j−1 ) ∝ exp (Wo [sj ; cj ]) 1679 p(yj = ui |u, y1:j−1 ) ∝ exp (aji ) where yj = w denotes that yj is chosen from the output vocabulary Vout ; yj = ui denotes that yj is a copy of ui ; aji = sTj Wa hi P is an attention score on the input word ui ; cj = i αi hi , αi ∝ exp (aji ) is a context vector that summarizes the encoder states; and Wo and Wa are matrix parameters to be learned. After generating yj , the decoder LSTM updates its hidden state sj+1 by taking as input the concatenation of the embedding vector for yj and the context vector cj . An impor"
D18-1195,D17-1160,0,0.0443775,"iliar with the underlying logical language, making it challenging to construct large scale datasets by non-experts. Over the years, this practical observation has spurred many creative solutions to training semantic parsers that are capable of leveraging weaker forms of supervision, amenable to non-experts. One such weaker form of supervision relies on logical form denotations (i.e, the results of a logical form’s execution) – rather than the logical form itself, as “supervisory” signal (Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017). In Question Answeing (QA), for example, this means the annotator needs only to know the answer to a question, rather than the full SQL query needed to obtain that answer. Paraphrasing of utterances already annotated with logical forms is another practical approach to scale up annotation without requiring experts with a knowledge of the underlying logical formalism (Berant and Liang, 2014; Wang et al., 2015). Although these and similar methods do reduce the difficulty of the annotation task, collecting even these weaker forms of supervision (e.g., denotations and paraphrases) still requires a"
D18-1195,D12-1069,1,0.874259,"Missing"
D18-1195,D10-1119,0,0.217565,"al language utterance into a formal meaning representation, e.g., an executable logical form (Zelle and Mooney, 1996). Because the space of all logical forms is large but constrained by an underlying structure (i.e., all trees), the problem of learning a semantic parser is commonly formulated as an instance of structured prediction. Historically, approaches based on supervised learning of structured prediction models have emerged as some of the first and still remain common in the semantic parsing community (Zettle∗ Work done while at Carnegie Mellon University. moyer and Collins, 2005, 2009; Kwiatkowski et al., 2010). A well recognized practical challenge in supervised learning of structured models is that fully annotated structures (e.g., logical forms) that are needed for training are often highly labor-intensive to collect. This problem is further exacerbated in semantic parsing by the fact that these annotations can only be done by people familiar with the underlying logical language, making it challenging to construct large scale datasets by non-experts. Over the years, this practical observation has spurred many creative solutions to training semantic parsers that are capable of leveraging weaker fo"
D18-1195,J13-2005,0,0.107902,"in semantic parsing by the fact that these annotations can only be done by people familiar with the underlying logical language, making it challenging to construct large scale datasets by non-experts. Over the years, this practical observation has spurred many creative solutions to training semantic parsers that are capable of leveraging weaker forms of supervision, amenable to non-experts. One such weaker form of supervision relies on logical form denotations (i.e, the results of a logical form’s execution) – rather than the logical form itself, as “supervisory” signal (Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017). In Question Answeing (QA), for example, this means the annotator needs only to know the answer to a question, rather than the full SQL query needed to obtain that answer. Paraphrasing of utterances already annotated with logical forms is another practical approach to scale up annotation without requiring experts with a knowledge of the underlying logical formalism (Berant and Liang, 2014; Wang et al., 2015). Although these and similar methods do reduce the difficulty of the annotation task, collect"
D18-1195,P17-1015,0,0.0210841,"airs of sentences and logical forms (Zettlemoyer and Collins, 2005, 2009; Kwiatkowski et al., 2010). As hand-labeled logical forms are very costly to obtain, different forms of weak supervision have been explored. Example works include learning from pairs of sentences and answers by querying a database (Clarke et al., 2010; 1683 8.4 80.0 8.2 77.5 8.0 75.0 7.8 72.5 7.6 70.0 MH (full model) 7.4 MH (no feedback + reject yˆ) 67.5 MH (no feedback) Self-training 65.0 1 7.2 2 beled examples for concept learning (Srivastava et al., 2017) and to help induce programs that solve algebraic word problems (Ling et al., 2017). Our work is similar in that natural language is used as additional supervision during learning, however, our natural language annotations consist of user feedback on system predictions instead of explanations of the training data. Average number of predicates in logical form Unlabeled accuracy Avg. number of predicates in logical form 82.5 3 Number of corrections expressed in feedback utterance 8 Figure 4: Parsing accuracy on unlabeled questions, partitioned by feedback complexity (i.e., number of corrections expressed in a single feedback utterance). Liang et al., 2013; Berant et al., 2013;"
D18-1195,D15-1166,0,0.0213616,"θt ) ; ∇θf ← ∇ log P (ˆ yif |ui , fi , yˆi ; θf ) ; θt ← SGD (θt , ∇θt ) ; θf ← SGD (θf , ∇θf ) ; end end We have experimented with a sampling based approximation of the true gradients with contrastive divergence (Hinton, 2002), however, found that our approximation works sufficiently well empirically. See Algorithm 1 for more details of the complete algorithm. P (ˆ yf |u,θt ) P (ˆ ycurr |u,θt ) Sample accept ∼ Bernoulli(r) if accept then pt ← P (ˆ yf |u, θt ) pf ← P (ˆ yf |u, f, yˆ, θf ) samples[ˆ yf ] ← pt · pf yˆcurr ← yˆf end end yˆf ← argmax samples return yˆf els (Bahdanau et al., 2014; Luong et al., 2015). The encoder takes in an utterance (tokenized) u and computes a context-sensitive embedding hi for each token ui using a bidirectional Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), where hi is computed as the concatenation of the hidden states at position i output by the forward LSTM and the backward LSTM. The decoder generates the output logical form y one token at a time using another LSTM. At each time step j, it generates yj based on the current LSTM hidden state sj , a summary of the input context cj , and attention scores aji , which are used for attention-bas"
D18-1195,P15-1096,0,0.0170506,"of corrections expressed in a single feedback utterance). Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017); learning from indirect supervision from a large-scale knowledge base (Reddy et al., 2014; Krishnamurthy and Mitchell, 2012); learning from conversations of systems asking for and confirming information (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Padmakumar et al., 2017); and learning from interactions with a simulated world environment (Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Goldwasser and Roth, 2014; Misra et al., 2015). The supervision used in these methods is mostly in the form of binary feedback, partial logical forms (e.g., slots) or execution results. In this paper, we explore a new form of supervision – natural language feedback. We demonstrate that such feedback not only provides rich and expressive supervisory signals for learning but also can be easily collected via crowd-sourcing. Recent work (Iyer et al., 2017) trains an online language-to-SQL parser from user feedback. Unlike our work, their collected feedback is structured and is used for acquiring more labeled data during training. Our model jo"
D18-1195,D15-1001,0,0.0337314,"t al., 2017) trains an online language-to-SQL parser from user feedback. Unlike our work, their collected feedback is structured and is used for acquiring more labeled data during training. Our model jointly learns from questions and feedback and can be trained with limited labeled data. There has been a growing interest on machine learning from natural language instructions. Much work has been done in the setting where an autonomous agent learns to complete a task in an environment, for example, learning to play games by utilizing text manuals (Branavan et al., 2012; Eisenstein et al., 2009; Narasimhan et al., 2015) and guiding policy learning using high-level human advice (Kuhlmann et al., 2004; Squire et al., 2015; Harrison et al., 2017). Recently, natural language explanations have been used to augment laConclusion and Future Work In this work, we proposed a novel task of learning a semantic parser directly from end-users’ openended natural language feedback during a conversation. The key advantage of being able to learn from natural language feedback is that it opens the door to learning continuously through natural interactions with the user, but it also presents a challenge of how to interpret such"
D18-1195,P09-1110,0,0.106049,"Missing"
D18-1195,E17-1052,0,0.099246,"cal form 82.5 3 Number of corrections expressed in feedback utterance 8 Figure 4: Parsing accuracy on unlabeled questions, partitioned by feedback complexity (i.e., number of corrections expressed in a single feedback utterance). Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017); learning from indirect supervision from a large-scale knowledge base (Reddy et al., 2014; Krishnamurthy and Mitchell, 2012); learning from conversations of systems asking for and confirming information (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Padmakumar et al., 2017); and learning from interactions with a simulated world environment (Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Goldwasser and Roth, 2014; Misra et al., 2015). The supervision used in these methods is mostly in the form of binary feedback, partial logical forms (e.g., slots) or execution results. In this paper, we explore a new form of supervision – natural language feedback. We demonstrate that such feedback not only provides rich and expressive supervisory signals for learning but also can be easily collected via crowd-sourcing. Recent work (Iyer et al., 2017) trains an online langu"
D18-1195,P15-1142,0,0.051043,"se annotations can only be done by people familiar with the underlying logical language, making it challenging to construct large scale datasets by non-experts. Over the years, this practical observation has spurred many creative solutions to training semantic parsers that are capable of leveraging weaker forms of supervision, amenable to non-experts. One such weaker form of supervision relies on logical form denotations (i.e, the results of a logical form’s execution) – rather than the logical form itself, as “supervisory” signal (Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017). In Question Answeing (QA), for example, this means the annotator needs only to know the answer to a question, rather than the full SQL query needed to obtain that answer. Paraphrasing of utterances already annotated with logical forms is another practical approach to scale up annotation without requiring experts with a knowledge of the underlying logical formalism (Berant and Liang, 2014; Wang et al., 2015). Although these and similar methods do reduce the difficulty of the annotation task, collecting even these weaker forms of supervision (e."
D18-1195,Q14-1030,0,0.0214152,"st of user feedback on system predictions instead of explanations of the training data. Average number of predicates in logical form Unlabeled accuracy Avg. number of predicates in logical form 82.5 3 Number of corrections expressed in feedback utterance 8 Figure 4: Parsing accuracy on unlabeled questions, partitioned by feedback complexity (i.e., number of corrections expressed in a single feedback utterance). Liang et al., 2013; Berant et al., 2013; Pasupat and Liang, 2015; Liang et al., 2016; Krishnamurthy et al., 2017); learning from indirect supervision from a large-scale knowledge base (Reddy et al., 2014; Krishnamurthy and Mitchell, 2012); learning from conversations of systems asking for and confirming information (Artzi and Zettlemoyer, 2011; Thomason et al., 2015; Padmakumar et al., 2017); and learning from interactions with a simulated world environment (Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Goldwasser and Roth, 2014; Misra et al., 2015). The supervision used in these methods is mostly in the form of binary feedback, partial logical forms (e.g., slots) or execution results. In this paper, we explore a new form of supervision – natural language feedback. We demonstrate that s"
D18-1195,D17-1161,1,0.848373,"ramming (Zelle and Mooney, 1996). Modern systems apply statistical models to learn from pairs of sentences and logical forms (Zettlemoyer and Collins, 2005, 2009; Kwiatkowski et al., 2010). As hand-labeled logical forms are very costly to obtain, different forms of weak supervision have been explored. Example works include learning from pairs of sentences and answers by querying a database (Clarke et al., 2010; 1683 8.4 80.0 8.2 77.5 8.0 75.0 7.8 72.5 7.6 70.0 MH (full model) 7.4 MH (no feedback + reject yˆ) 67.5 MH (no feedback) Self-training 65.0 1 7.2 2 beled examples for concept learning (Srivastava et al., 2017) and to help induce programs that solve algebraic word problems (Ling et al., 2017). Our work is similar in that natural language is used as additional supervision during learning, however, our natural language annotations consist of user feedback on system predictions instead of explanations of the training data. Average number of predicates in logical form Unlabeled accuracy Avg. number of predicates in logical form 82.5 3 Number of corrections expressed in feedback utterance 8 Figure 4: Parsing accuracy on unlabeled questions, partitioned by feedback complexity (i.e., number of corrections"
D18-1195,P15-1129,0,0.0922147,"Missing"
D18-2025,D13-1160,0,0.0701056,"nt, and is difficult to solve using rule-based heuristics. The problem of semantic parsing and variable resolution is addressed by LIA using a machine learning based approach. For example, if the user mentions “Tom’s email”, and there are multiple contacts named Tom, the agent can use its conversational context (e.g., most recently mentioned entities) and world knowledge to help and resolve the reference – both are naturally incorporated as features; weights for these are learned continuously through interactions with the user. LIA uses a synchronous CFG-based parser implemented using SEMPRE (Berant et al., 2013), and an underlying frame-based meaning representation (i.e., a frame consists of an intent such as CREATE NEW CONCEPT and any arguments such as the name of the concept), allowing nested frames for certain intents (currently the IF THEN intent). The parser has an underlying log-linear parameterization of the frame/utterance pairs, with weights that can be learned offline and updated online during interactions with the user. LIA uses the following two classes of features to represent utterance/frame pairs: • Lexical/logical form features: these include indicator features for derivation rules us"
D18-2025,D17-1161,1,0.796301,"Figure 3: Knowledge View in user interface, which displays procedures taught by a user, along with utilized sensors and effectors Grounding New Knowledge in Sensors and Effectors An important component of an intelligent assistant is the ability to ground language and abstract concepts in observable perception, through sensors and effectors. We envision enabling the agent to learn concepts (such as important emails) from a combination of explanations, and examples of the concept. This is motivated by our recent research on using natural language to define feature functions for learning tasks (Srivastava et al., 2017), and also work on using declarative knowledge in natural language explanations to supervise training of classifiers (Srivastava et al., 2018). Using semantic parsing, we can map natural language statements to predicates in a logical language, which are grounded in sensor-effector capabilities of the personal agent. These may enable the user to: 3.5 Mixed-Initiative Interactions In a conventional programming language, the programmer anticipates all possible outcomes of various API calls made in a program, wrapping these calls with control-flow statements (if/then/else blocks) to account for di"
D19-1104,P16-1004,0,0.195896,"start by introducing the notation we will use in the following sections. Bold capital letters represent matrices, and bold lower case letters represent vectors. X denotes a distributed representation of an utterance, where each column of X is the repre3.1 Look-up In this section, we describe our look-up strategy and discuss its sub-modules in detail. Encoder Bidirectional RNNs have been successfully used to represent sentences in many areas of natural language processing such as question answering, neural machine translation, and semantic parsing (Bengio and LeCun, 2015; Hermann et al., 2015; Dong and Lapata, 2016). We use Bidirectional LSTMs where the forward LSTM reads the input in the utterance word order (x1 , x2 , ..., xT ) and the backward LSTM reads the input in reversed order (xT , xT 1 , ..., x1 ). The output for each word xi is the concatenation of the ith hidden state hfi wd of the forward LSTM and the T ith hidden state hbck T i of the backward LSTM. As shown in Figure 1, we use two Bidirectional LSTMs, one to encode the utterance, and one to encode the logical form. The utterance is treated as a sequence of words, where each word is represented as a pretrained GloVe (Pennington et al., 2014"
D19-1104,Q18-1031,0,0.051279,"neural semantic parsing have been explored in (Zhong et al., 2017). Retrieve-and-edit style semantic parsing is gaining popularity. Hashimoto et al. (2018) proposed a retrieve and edit framework that can efficiently learn to embed utterances in a taskdependent way for easy editing. Our work differs in that we perform hierarchical retrievals and edits, and that we evaluate on cross-domain data and focus on one-shot semantic parsing. It is worth noting that retrieve-and-edit as a general framework is not limited to semantic parsing and is applicable to other areas such as sentence generation. (Guu et al., 2018) and machine translation 1136 Sequence-To-Sequence LOOKUPADAPT full d=2 d=3 full d=2 d=3 person 20.1 25.0 13.9 45.9 56.7 27.8 restaurant 25.1 18.1 37.3 61.7 85.4 20.0 extension event course 26.1 19.9 20.0 31.8 42.4 0.0 85.5 79.2 94.5 86.7 61.7 66.7 animal 25.0 26.5 21.5 63.6 70.1 46.5 vehicle 18.8 25.9 5.8 87.6 93.6 76.5 Table 3: Test Accuracy on the extension Dataset. Sequence-To-Sequence LOOKUPADAPT ORACLE-DISCRIM PRETRAIN-ENC full d=2 d=3 full d=2 d=3 full d=2 d=3 full d=2 d=3 person 5.2 6.4 3.1 41.8 52.5 21.4 46.9 55.6 30.3 98.0 100.0 94.0 restaurant 29.2 41.7 8.4 43.8 53.4 27.8 45.8 56.7"
D19-1104,P17-2098,0,0.0168634,"thods as well as machine learning-based methods to address this problem. Grammar-based parsers work by having a set of grammar rules that are either learned or hand-written, an algorithm for generating a set of candidate logical forms by recursive application of the grammar rules, and a criterion for picking the best candidate logical form within that set (Liang and Potts, 2015; Zettlemoyer and Collins, 2005, 2007). However, they are brittle to the flexibility of language. To improve this limitation, supervised sequence-based neural semantic parsers have been proposed (Dong and Lapata, 2016). Herzig and Berant (2017) improved the performance of neural semantic parsers by training over multiple knowledge bases and providing the domain encoding at decoding time. In addition to supervised learning, reinforcement learning methods for neural semantic parsing have been explored in (Zhong et al., 2017). Retrieve-and-edit style semantic parsing is gaining popularity. Hashimoto et al. (2018) proposed a retrieve and edit framework that can efficiently learn to embed utterances in a taskdependent way for easy editing. Our work differs in that we perform hierarchical retrievals and edits, and that we evaluate on cros"
D19-1104,D18-1190,0,0.12396,"Missing"
D19-1104,J81-4005,0,0.648514,"Missing"
D19-1104,D18-2025,1,0.888282,"Missing"
D19-1104,D14-1162,0,0.092034,"Dong and Lapata, 2016). We use Bidirectional LSTMs where the forward LSTM reads the input in the utterance word order (x1 , x2 , ..., xT ) and the backward LSTM reads the input in reversed order (xT , xT 1 , ..., x1 ). The output for each word xi is the concatenation of the ith hidden state hfi wd of the forward LSTM and the T ith hidden state hbck T i of the backward LSTM. As shown in Figure 1, we use two Bidirectional LSTMs, one to encode the utterance, and one to encode the logical form. The utterance is treated as a sequence of words, where each word is represented as a pretrained GloVe (Pennington et al., 2014) embedding. The logical form is treated as a sequence of predicates and parentheses, each of which is also represented as a pretrained GloVe embedding. The motivation to use GloVe embeddings for both words and predicates is to avoid the problem of unknown words/predicates encountered in the one-shot utterance’s domain. Retrieving from the memory The memory consists of a subset of the utterance-logical form pairs given as input to the algorithm. We refer to the number of pairs in the memory as its size and denote it with n. We would like to note that the algorithm’s success depends on having a"
D19-1104,D17-1127,0,0.131335,"Missing"
D19-1104,P15-1129,0,0.113977,"Missing"
D19-1104,D07-1071,0,0.188435,"Missing"
D19-1426,N18-1197,0,0.0222622,"r-learner interaction is a classification model (here, for important emails). We present a framework that (a) enables learning classifiers from a mix of such supervision; (b) learns to ask appropriate sequences of questions to accelerate this. Introduction The ability to learn new tasks and behaviors from language is characteristic of human intelligence. In recent years, the fields of machine learning and NLP have seen an renewed interest in incorporating natural language supervision in models of machine intelligence (Narasimhan et al., 2015; Elhoseiny et al., 2013; Goldwasser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai, 2017; Cakmak and Thomaz, 2012; Krishnamurthy and Kollar, 2013) and inter"
D19-1426,D09-1100,0,0.0344633,"ly on passively receiving instruction from a teacher. Rather, the interaction takes the form of a mixed-initiative dialog, where they ask questions and proactively seek clarifications to simplify learning. These questions can generalize learning to novel situations, explore hypotheses, or fill information gaps. The ability to ask questions can, thus, fundamentally facilitate learning. Second, existing approaches have focused on using language either as a standalone replacement for labeled data (Hancock et al., 2018), or to drive learning such as through specifying features for learning tasks (Eisenstein et al., 2009). In contrast, many realistic scenarios of learning from language would involve not learning from language alone, but learning from a mix of supervision, including both traditional labeled data, and natural language advice. Thus, automated learners should be capable of learning from a blend of observations, explanations and clarification. In this work, we introduce a framework for learning from language in a conversational setting (LiD, for Learning with Interactive Dialog), which is a step towards alleviating these shortcomings. Language provides a natural medium for conversational interactio"
D19-1426,D19-1107,0,0.0654472,"Missing"
D19-1426,P18-1175,0,0.0429672,"Missing"
D19-1426,Q13-1016,0,0.0876143,"sser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai, 2017; Cakmak and Thomaz, 2012; Krishnamurthy and Kollar, 2013) and interactive learning in non-linguistic settings (see Section 2), existing approaches for training machine learning models from language are largely noninteractive, i.e. the learner agent receives statically collected text-based advice from a teacher as input, but does not directly engage with the 4164 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4164–4174, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics teacher.1 In comparison, w"
D19-1426,D18-1195,1,0.862958,"Missing"
D19-1426,D15-1001,0,0.0235868,"clarifications (to overcome parsing limitations). The output from the teacher-learner interaction is a classification model (here, for important emails). We present a framework that (a) enables learning classifiers from a mix of such supervision; (b) learns to ask appropriate sequences of questions to accelerate this. Introduction The ability to learn new tasks and behaviors from language is characteristic of human intelligence. In recent years, the fields of machine learning and NLP have seen an renewed interest in incorporating natural language supervision in models of machine intelligence (Narasimhan et al., 2015; Elhoseiny et al., 2013; Goldwasser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai,"
D19-1426,P18-1255,0,0.0570139,"Missing"
D19-1426,C16-1163,0,0.0671793,"Missing"
D19-1426,P17-1150,0,0.0313641,"et al., 2015; Elhoseiny et al., 2013; Goldwasser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai, 2017; Cakmak and Thomaz, 2012; Krishnamurthy and Kollar, 2013) and interactive learning in non-linguistic settings (see Section 2), existing approaches for training machine learning models from language are largely noninteractive, i.e. the learner agent receives statically collected text-based advice from a teacher as input, but does not directly engage with the 4164 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4164–4174, c Hong Kong, China, November 3–7, 2019. 2019 Associatio"
D19-1426,D17-1161,1,0.934157,"from a mix of such supervision; (b) learns to ask appropriate sequences of questions to accelerate this. Introduction The ability to learn new tasks and behaviors from language is characteristic of human intelligence. In recent years, the fields of machine learning and NLP have seen an renewed interest in incorporating natural language supervision in models of machine intelligence (Narasimhan et al., 2015; Elhoseiny et al., 2013; Goldwasser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai, 2017; Cakmak and Thomaz, 2012; Krishnamurthy and Kollar, 2013) and interactive learning in non-linguistic settings (see Section 2), existing approaches for training machine learning models from language are largely"
D19-1426,P18-1029,1,0.908853,"eled examples to label next while training supervised machine learning models (Settles, 2012; Collins et al., 2008). This can be seen as asking a specific kind of question (as illustrated in Figure 1). Learning to ask questions generalizes active learning in multiple ways by possibly soliciting a wider range of data measurements. These include feature labels (‘Are emails with subject “urgent” usually important?’), label proportions (‘Around what fraction of emails are important?’), constraints on model expectations (‘Are you more likely to reply to important emails?’), etc. Approaches such as Srivastava et al. (2018) map such language to data measurements that computational models can reason over.2 Statistical frameworks such as Generalized Expectation (Druck et al., 2008), Posterior Regularization (Ganchev et al., 2010) and Bayesian 2 For example, a statement such as ‘Emails from my boss are usually important’ may be mapped to a data measurement of form P (y = important|sender = boss) ≈ Pusually . 4165 Measurements (Liang et al., 2009) then allow for model training from a broad range of such data measurements in conjunction with unlabeled data, rather than using labeled examples. Other recent approaches"
D19-1426,P16-1224,0,0.0267219,"model (here, for important emails). We present a framework that (a) enables learning classifiers from a mix of such supervision; (b) learns to ask appropriate sequences of questions to accelerate this. Introduction The ability to learn new tasks and behaviors from language is characteristic of human intelligence. In recent years, the fields of machine learning and NLP have seen an renewed interest in incorporating natural language supervision in models of machine intelligence (Narasimhan et al., 2015; Elhoseiny et al., 2013; Goldwasser and Roth, 2014; Andreas et al., 2018; Fried et al., 2018; Wang et al., 2016). In particular, methods such as BabbleLabble (Hancock et al., 2018) and LNL (Srivastava et al., 2017) show progress towards realistic ∗ * Work done while the first and second authors were at CMU applications of supervised learning from language on tasks such as information extraction and email categorization. However, until now, such methods have been limited in two ways. First, despite a body of work on leveraging language for tasks involving human robot interaction (She and Chai, 2017; Cakmak and Thomaz, 2012; Krishnamurthy and Kollar, 2013) and interactive learning in non-linguistic settin"
D19-1426,P18-1243,0,0.0213835,"the learning task itself. i.e., the value of a question is evaluated in utilitarian terms of how it affects performance on a downstream classification task. This follows a Wittgensteinian view of language as a cooperative game (Wittgenstein, 1953) between agents (here, the teacher and a learner) with a shared goal (here, building an effective classifier). While the space of questions that an interactive learner can ask can be vast in general, here we specifically focus on leveraging interactivity for three specific aspects (highlighted in Figure 1): 1. Seeking labels for specific examples. 1 Zhang et al. (2018) diverge from prior work in this respect, and model language games between teachers and learners. However, their learning tasks are toylike, and the method does not generalize to realistic scenarios. 2. Asking for explanations of a concept. 3. Requesting clarifications about explanations. As illustrated in Figure 1, these dimensions can facilitate multiple aspects of the learning process: including learning from labeled examples (similar to traditional supervised learning), learning from natural language explanations (similar to recent work on learning from explanations) and alleviating limita"
J19-4002,H05-1071,0,0.0754747,"Missing"
J19-4002,D07-1087,0,0.00946961,"cally extracting structured information from unstructured and/or semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is l"
J19-4002,W04-2504,0,0.561617,"; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes,"
J19-4002,P08-1007,0,0.0604381,"Missing"
J19-4002,W12-3014,0,0.0254481,"; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering const"
J19-4002,J87-1002,0,0.504203,"try problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 201"
J19-4002,C14-1206,0,0.049657,"Missing"
J19-4002,W16-1303,0,0.0164499,"04; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering constraints to improve th"
J19-4002,N10-1031,0,0.0263931,"Missing"
J19-4002,P09-1075,0,0.0225883,"component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Ch"
J19-4002,P12-1007,0,0.0226245,"c devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et"
J19-4002,P14-1048,0,0.380757,"to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse conside"
J19-4002,D14-1168,0,0.0268694,"ann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into"
J19-4002,W12-1622,0,0.0534271,"Missing"
J19-4002,J86-3001,0,0.739399,"isting solver for geometry problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and"
J19-4002,P14-1092,0,0.0603686,"Missing"
J19-4002,P14-1002,0,0.0243145,"nicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without m"
J19-4002,P13-1127,0,0.012602,"Language to Programs: After harvesting axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-s"
J19-4002,P14-1003,0,0.0224181,"The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting."
J19-4002,D15-1193,0,0.0155235,"semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our appr"
J19-4002,P11-1060,0,0.017064,"Missing"
J19-4002,W04-1013,0,0.00883788,"iscourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure Indicator matching the current (and parent) node of axiom mentions in respective JSON hierarchies; i.e., are both nodes mentioned as axioms, diagrams or bounding boxes? Equation Template Indicator feature that matches templates of equations detected in the axiom mentions. The template matcher is designed such that it identifies various rewritings of the same axiom equation, e.g., PA × PB = PT2 and PA × PB = PC2 could refer to the same axiom with point T in one axiom mention"
J19-4002,P16-1057,0,0.0602349,"Missing"
J19-4002,P17-1015,0,0.0126677,"ooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text have been rare beca"
J19-4002,W10-4327,0,0.0608886,"Missing"
J19-4002,P14-5010,0,0.00542909,"Missing"
J19-4002,J96-3006,0,0.282068,"ore explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 20"
J19-4002,P15-1121,0,0.0128899,"nd Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes, indentations, or quotations. It may refer to asso"
J19-4002,P15-1085,0,0.0213097,"Missing"
J19-4002,P15-1024,1,0.90458,"Missing"
J19-4002,D15-1171,0,0.193157,"es in a multimedia document (Hovy 1998) for the various stages of information extraction. Our experiments show the usefulness of all the various typographical features over and above the various lexical semantic and discourse level features considered for the task. We use our model to extract and parse axiomatic knowledge from a novel data set of 20 publicly available math textbooks. We use this structured axiomatic knowledge to build a new axiomatic solver that performs logical inference to solve geometry problems. Our axiomatic solver outperforms GEOS on all existing test sets introduced in Seo et al. (2015) as well as a new test set of geometry questions collected from these textbooks. We also performed user studies on a number of school students studying geometry who found that our axiomatic solver is more interpretable and useful compared with GEOS. 2. Background and Related Work Discourse Analysis: Discourse analysis is the analysis of semantics conveyed by a coherent sequence of sentences, propositions, or speech. Discourse analysis is taken up in a variety of disciplines in the humanities and social sciences and a number of discourse theories have been proposed (Mann and Thompson 1988; Kamp"
J19-4002,N03-1030,0,0.361741,"Missing"
J19-4002,N09-1064,0,0.0330417,"Missing"
J19-4002,K15-2002,0,0.0155504,"icking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is o"
J19-4002,P13-2123,0,0.0111238,"es (constants, predicates, and functions) across the two axioms. When comparing geometric entities, we include geometric entities derived from the associated diagrams when available. Longest Common Subsequence Real valued feature that computes the length of longest common subsequence of words between two axiom mentions normalized by the total number of words in the two mentions. Number of discourse elements Real valued feature that computes the absolute difference in the number of discourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure"
J19-4002,P17-1041,0,0.0605138,"ing axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text"
K17-1026,D10-1119,0,0.0802407,"n natural language understanding. Even seemingly trivial sentences may have a large number of ambiguous interpretations. Consider the sentence “Ada started the machine with the GPU,” for example. Without additional knowledge, such as the fact that “machine” can refer to computing devices that contain GPUs, or that computers generally contain devices such as Accurate and efficient semantic parsing is a long-standing goal in natural language processing. Existing approaches are quite successful in particular domains (Zettlemoyer and Collins, 2005, 2007; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010, 2011, 2013; Li et al., 2013; Zhao and Huang, 2014; Dong and Lapata, 2016). However, they are largely domain-specific, relying on additional supervision such as a lexicon that provides the semantics or the type of each token in a set (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011; Liang et al., 2011; Zhao and Huang, 2014; Dong and Lapata, 2016), or a set of initial synchronous context-free grammar rules (Wong and Mooney, 2007; Li et al., 2013). To apply the above systems to a new domain, additional supervision is necessary. When beginning to read text from a new domain,"
K17-1026,N10-1028,0,0.0280754,". Thus, we employ collapsed Gibbs sampling by integrating out θ. In this algorithm, we repeatedly sample from ti |t−i , xi , yi where t−i = t  {ti }. 1{yield(ti ) = yi } n∈t∗i (3) After sampling t∗i , we choose to accept the new sample with probability T  Q n |x, t n n p r ∗ −i n∈ti n∈ti p(r |x , t−i )  Q T , n n n p n∈ti r |x, t−i n∈t∗ p(r |x , t−i ) Induction p(ti |t−i , xi , yi ) = Y where ti , here, is the old sample, and t∗i is the newly proposed sample. In practice, this acceptance probability is very high. This approach is very similar in structure to that in Johnson et al. (2007); Blunsom and Cohn (2010); Cohn et al. (2010). If an application requires posterior samples of the grammar variables θ, we can obtain them by drawing from θ|t after the collapsed Gibbs sampler has mixed. Note that this algorithm requries no further supervision beyond the utterances y and logical forms x. However, it is able to exploit additional information such as supervised derivations/parse trees. For example, a lexicon can be provided where each entry is a terminal symbol yi with a corresponding logical form label xi . We evaluate our method with and without such a lexicon. Refer to Saparov and Mitchell (2016) for"
K17-1026,D11-1140,0,0.0562711,"Missing"
K17-1026,P16-1004,0,0.0124209,"large number of ambiguous interpretations. Consider the sentence “Ada started the machine with the GPU,” for example. Without additional knowledge, such as the fact that “machine” can refer to computing devices that contain GPUs, or that computers generally contain devices such as Accurate and efficient semantic parsing is a long-standing goal in natural language processing. Existing approaches are quite successful in particular domains (Zettlemoyer and Collins, 2005, 2007; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010, 2011, 2013; Li et al., 2013; Zhao and Huang, 2014; Dong and Lapata, 2016). However, they are largely domain-specific, relying on additional supervision such as a lexicon that provides the semantics or the type of each token in a set (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011; Liang et al., 2011; Zhao and Huang, 2014; Dong and Lapata, 2016), or a set of initial synchronous context-free grammar rules (Wong and Mooney, 2007; Li et al., 2013). To apply the above systems to a new domain, additional supervision is necessary. When beginning to read text from a new domain, humans do not need to re-learn basic English gram248 Proceedings of the 21s"
K17-1026,P11-1060,0,0.0748938,"Missing"
K17-1026,W06-1673,0,0.036968,"fter the collapsed Gibbs sampler has mixed. Note that this algorithm requries no further supervision beyond the utterances y and logical forms x. However, it is able to exploit additional information such as supervised derivations/parse trees. For example, a lexicon can be provided where each entry is a terminal symbol yi with a corresponding logical form label xi . We evaluate our method with and without such a lexicon. Refer to Saparov and Mitchell (2016) for details on HDP inference and computing p(rn |xn , t−i ). 3.1 Sampling t∗i To sample from equation (3), we use insideoutside sampling (Finkel et al., 2006; Johnson et al., 2007), a dynamic programming approach, where the inside step is implemented using an agenda-driven chart parser (Indurkhya and Damerau, 2010). The algorithm fills a chart, which has a cell for every (2) ! t−i , xi , 251 nonterminal A, sentence start position i, end position j, and logical form x. The algorithm aims to compute the inside probability of every chart cell: that is, for every cell (A, i, j, x), we compute the probability that t∗i contains a subtree rooted with the nonterminal A and logical form x, spanning the sentence positions (i, j). Let I(A,i,j,x) be the insid"
K17-1026,P09-1108,0,0.0191852,"ogical forms. Nothing in our model precludes incorporating syntactic information like f-structures into the logical form, and as such, LFG is realized in our framework. Our approach can be used to define new generative models of these grammatical formalisms. We implemented our method with a particular semantic formalism, but the grammatical model is agnostic to the choice of semantic formalism or the language. As in some previous parsers, a parallel can be drawn between our parsing problem and the problem of finding shortest paths in hypergraphs using A* search (Klein and Manning, 2001, 2003; Pauls and Klein, 2009; Pauls et al., 2010; Gallo et al., 1993). 8 Discussion In this article, we presented a generative model of sentences, where each sentence is generated recursively top-down according to a semantic grammar, where each step is conditioned on the logical form. We developed a method to learn the posterior of the grammar using a Metropolis-Hastings sampler. We also derived a Viterbi parsing algorithm that takes into account the prior probability of the logical forms. Through this semantic prior, background knowledge and other information can be easily incorporated to better guide the parser during"
K17-1026,P10-2037,0,0.0444218,"Missing"
K17-1026,N07-1018,0,0.036555,"hain can be slow to mix. Thus, we employ collapsed Gibbs sampling by integrating out θ. In this algorithm, we repeatedly sample from ti |t−i , xi , yi where t−i = t  {ti }. 1{yield(ti ) = yi } n∈t∗i (3) After sampling t∗i , we choose to accept the new sample with probability T  Q n |x, t n n p r ∗ −i n∈ti n∈ti p(r |x , t−i )  Q T , n n n p n∈ti r |x, t−i n∈t∗ p(r |x , t−i ) Induction p(ti |t−i , xi , yi ) = Y where ti , here, is the old sample, and t∗i is the newly proposed sample. In practice, this acceptance probability is very high. This approach is very similar in structure to that in Johnson et al. (2007); Blunsom and Cohn (2010); Cohn et al. (2010). If an application requires posterior samples of the grammar variables θ, we can obtain them by drawing from θ|t after the collapsed Gibbs sampler has mixed. Note that this algorithm requries no further supervision beyond the utterances y and logical forms x. However, it is able to exploit additional information such as supervised derivations/parse trees. For example, a lexicon can be provided where each entry is a terminal symbol yi with a corresponding logical form label xi . We evaluate our method with and without such a lexicon. Refer to Saparo"
K17-1026,N06-1056,0,0.0924572,"Missing"
K17-1026,P07-1121,0,0.307314,"irection. Knowledge plays a critical role in natural language understanding. Even seemingly trivial sentences may have a large number of ambiguous interpretations. Consider the sentence “Ada started the machine with the GPU,” for example. Without additional knowledge, such as the fact that “machine” can refer to computing devices that contain GPUs, or that computers generally contain devices such as Accurate and efficient semantic parsing is a long-standing goal in natural language processing. Existing approaches are quite successful in particular domains (Zettlemoyer and Collins, 2005, 2007; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010, 2011, 2013; Li et al., 2013; Zhao and Huang, 2014; Dong and Lapata, 2016). However, they are largely domain-specific, relying on additional supervision such as a lexicon that provides the semantics or the type of each token in a set (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011; Liang et al., 2011; Zhao and Huang, 2014; Dong and Lapata, 2016), or a set of initial synchronous context-free grammar rules (Wong and Mooney, 2007; Li et al., 2013). To apply the above systems to a new domain, additional supervision is necessary. W"
K17-1026,D07-1071,0,0.311812,"ned prob5 Semantic prior The modular nature of the semantic prior allows us to explore many different models of logical forms. We experiment with a fairly straightforward prior: Predicate instances are generated left-to-right, conditioned only on the last predicate instance that was sampled for each variable. When a predicate instance is sampled, its predicate, arity, and “direction”2 are simultaneously sampled from a cat2 254 size(A), size(A,B), vs size(B,A), etc. Method WASP (Wong and Mooney, 2006) λ-WASP (Wong and Mooney, 2007) Extended GHKM (Li et al., 2013) Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) UBL (Kwiatkowski et al., 2010) FUBL (Kwiatkowski et al., 2011) TISP (Zhao and Huang, 2014) GSG − lexicon − type-checking GSG + lexicon − type-checking GSG − lexicon + type-checking GSG + lexicon + type-checking GeoQuery P R F1 87.2 74.8 80.5 92.0 86.6 89.2 93.0 87.6 90.2 96.3 79.3 87.0 91.6 86.1 88.8 94.1 85.0 89.3 88.6 88.6 88.6 92.9 88.9 90.9 86.9 75.7 80.9 88.4 81.8 85.0 89.3 77.9 83.2 90.7 83.9 87.2 Additional Supervision 1,2 1,2,6 2,6 3,5,6 3,5,6 5 5 5,6 4 4,5 4,6 4,5,6 P Jobs R F1 97.3 79.3 87.4 85.0 89.5 91.4 93.2 97.4 85.0 67.1 75.7 69.3 81.4 85.0 76.7 82.8 79.5 88.7 Legend for source"
K17-1026,W01-1812,0,\N,Missing
K17-1026,N03-1016,0,\N,Missing
N15-1004,W13-3206,0,0.110838,"hrasal semantics, which we leverage to analyze performance on a behavioral task. 1 Introduction Vector Space Models (VSMs) are models of word semantics typically built with word usage statistics derived from corpora. VSMs have been shown to closely match human judgements of semantics (for an overview see Sahlgren (2006), Chapter 5), and can be used to study semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Turney, 2012). Composition has been explored with different types of composition functions (Mitchell and Lapata, 2010; Mikolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some mod"
N15-1004,D14-1162,0,0.121474,"kolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some model introspection has been attempted: Levy and Goldberg (2014) examined connections between layers, Mikolov et al. (2013) and Pennington et al. (2014) explored how shifts in VSM space encodes semantic relationships. Still, interpreting NN VSM dimensions, or factors, remains elusive. This paper introduces a new method, Compositional Non-negative Sparse Embedding (CNNSE). In contrast to many other VSMs, our method learns an interpretable VSM that is tailored to suit the semantic composition function. Such interpretability allows for deeper exploration of semantic composition than pr"
N15-1004,W13-3510,1,0.817656,". VSMs have been shown to closely match human judgements of semantics (for an overview see Sahlgren (2006), Chapter 5), and can be used to study semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Turney, 2012). Composition has been explored with different types of composition functions (Mitchell and Lapata, 2010; Mikolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some model introspection has been attempted: Levy and Goldberg (2014) examined connections between layers, Mikolov et al. (2013) and Pennington et al. (2014) explored how shifts in VSM space encodes semantic relationships. S"
N15-1004,D12-1110,0,\N,Missing
N15-1004,D14-1163,0,\N,Missing
N15-1004,C12-1118,1,\N,Missing
N16-1033,W06-0901,0,0.647478,"ntences, there is little or no benefit of joint modeling. We also find that some entity misclassification errors can be avoided if entity coreference information is available. We plan to investigate coreference resolution as an additional component to our joint model in future work. 296 Related Work Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel an"
N16-1033,D14-1159,0,0.0255841,"lum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work. There has been work on improving event extraction by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causality, inhibition, which frequently occur in biological texts. For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with ATTACK events and T RANS PORT"
N16-1033,D12-1091,0,0.0197476,"cause error propagation. J OINT E VENT E NTITY provides the best performance among all the models on all evaluation categories. It boosts both precision and recall compared to W ITHIN E VENT.10 This demonstrates the advantages of J OINT E VENT E NTITY in allowing information propagation across event mentions and entity mentions and making more context-aware and semantically coherent predictions. We also compare the results of J OINT E VENT E N TITY with the best known results on the ACE event 10 All significance tests reported in this paper were computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012) with 10,000 samples of the test documents. 295 PER 85.1 87.1 GPE 87.0 87.0 ORG 65.4 70.2 TIME 78.4 80.2 Table 6: Entity extraction results (F1 score) per entity type. extraction task in Table 4. C ROSS - DOC (Ji and Grishman, 2008) performs cross-document inference of events using document clustering information, and CNN (Nguyen and Grishman, 2015) is a convolutional neural network for extracting event triggers at the sentence level. We see that J OINT E VENT E N TITY outperforms both models and achieves new state-of-the-art results for event trigger and argument extraction in an end-to-end e"
N16-1033,W09-1402,0,0.0873134,"Missing"
N16-1033,P15-2061,0,0.0271285,"ide of the 5 We use the Stanford coreference system (Lee et al., 2013) for within-document entity coreference. Category Trigger Type Lexical resources: WordNet Nomlex FrameNet Word2Vec Syntactic resources: Stanford parser Argument Lexical resources: WordNet Syntactic resources: Stanford parser Entity Entity resources: Stanford NER NELL KB Features 1. lemmas of the words in the trigger mention 2. nominalization of the words based on Nomlex (Macleod et al., 1998) 3. context words within a window of size 2 4. similarity features between the head word and a list of trigger seeds based on WordNet (Bronstein et al., 2015) 5. semantic frames that associate with the head word and its p-o-s tag based on FrameNet (Li et al., 2014) 6. pre-trained vector for the head word (Mikolov et al., 2013) 7. dependency edges involving the head word, both lexicalized and unlexicalized 8. whether the head word is a pronoun 1. lemmas of the words in the entity mention 2. lemmas of the words in the trigger mention 3. words between the entity mention and the trigger mention 4. the relative position of the entity mention to the trigger mention (before, after, or contain) 5. whether the entity mention and the trigger mention are in t"
N16-1033,D08-1073,0,0.0416002,"xtracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work. There has been work on improving event extraction by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causality, inhibition, which frequently occur in biological texts. For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with ATTACK events and T RANS PORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and Jurafsky (2009; 2011) learn narrative schemas by linking event"
N16-1033,P09-1068,0,0.045718,"focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with ATTACK events and T RANS PORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and Jurafsky (2009; 2011) learn narrative schemas by linking event verbs that have coreferring syntactic arguments. Our model also adopts this intuition to relate event triggers across sentences. In addition, each event argument is grounded by its entity type (e.g., an entity mention of type PER can only fill roles that can be played by a person). 6 Conclusion In this paper, we introduce a new approach for automatic extraction of events and entities across a document. We first decompose the learning problem into three tractable subproblems: learning within-event structures, learning event-event relations, and l"
N16-1033,P11-1098,0,0.121068,"Missing"
N16-1033,P15-1017,0,0.38994,"ormation is available. We plan to investigate coreference resolution as an additional component to our joint model in future work. 296 Related Work Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, the"
N16-1033,S12-1029,0,0.0202469,"j ) = log pψ (aj |j, x) and pψ (aj |j, x) is the marginal probability derived from the linear-chain CRF described in Section 3.3. The optimization is subjected to agreement constraints that enforce the overlapping variables among the three components to agree on their values. The joint inference problem can be formulated as an integer linear program (ILP). To solve it efficiently, we find solutions for the relaxation of the problem using a dual decomposition algorithm AD3 (Martins et al., 2011). AD3 has been shown to be orders of magnitude faster than a general purpose ILP solver in practice (Das et al., 2012). It is also particularly suitable for our problem since it involves decompositions that have many overlapping simple factors. We observed that AD3 recovers the exact solutions for all the test documents in our experiments and the runtime for labeling each document is only three seconds in average in a 64-bit machine with two 2GHz CPUs and 8GB of RAM. 4 Experiments We conduct experiments on the ACE2005 corpus.6 It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data"
N16-1033,D12-1062,0,0.0260929,"nd entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work. There has been work on improving event extraction by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causality, inhibition, which frequently occur in biological texts. For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with ATTACK events and T RANS PORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and Jurafsky (2009; 2011) learn narrative schemas by linking event verbs that have c"
N16-1033,doddington-etal-2004-automatic,0,0.369152,"tion successfully improves recall and F1. However, since the ACE dataset is restricted to a limited set of events, a large portion of the sentences does not contain any event triggers and event arguments that are of interest. For these sentences, there is little or no benefit of joint modeling. We also find that some entity misclassification errors can be avoided if entity coreference information is available. We plan to investigate coreference resolution as an additional component to our joint model in future work. 296 Related Work Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poo"
N16-1033,P08-1030,0,0.546713,"l., 2013) observes that using previously extracted entities in event extraction results in 289 Proceedings of NAACL-HLT 2016, pages 289–299, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics a substantial decrease in performance compared to using gold-standard entity information. Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers’ outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information. Both approaches only propagate the highly confident predictions from the first stage to the second stage. To the best of our knowledge, there is no unified m"
N16-1033,S15-1018,0,0.0117721,"s a person, then the event extractor will fail to extract “Baghdad” as the place where the attack happened. In fact, previous work (Li et al., 2013) observes that using previously extracted entities in event extraction results in 289 Proceedings of NAACL-HLT 2016, pages 289–299, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics a substantial decrease in performance compared to using gold-standard entity information. Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers’ outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information. Both approaches on"
N16-1033,W09-1401,0,0.0646117,"s restricted to a limited set of events, a large portion of the sentences does not contain any event triggers and event arguments that are of interest. For these sentences, there is little or no benefit of joint modeling. We also find that some entity misclassification errors can be avoided if entity coreference information is available. We plan to investigate coreference resolution as an additional component to our joint model in future work. 296 Related Work Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosk"
N16-1033,J13-4004,0,0.0212642,"ncy relation (based on dependency parsing); (2) whether they share a subject or an object (based on dependency parsing and coreference resolution); (3) whether they have the same head word lemma; (4) whether they share a semantic frame based on FrameNet. During training, we use L-BFGS to compute the maximumlikelihood estimates of φ. 3.3 Entity Extraction For entity extraction, we trained a standard linearchain Conditional Random Field (CRF) (Lafferty et al., 2001) using the BIO scheme (i.e., identifying the Beginning, the Inside and the Outside of the 5 We use the Stanford coreference system (Lee et al., 2013) for within-document entity coreference. Category Trigger Type Lexical resources: WordNet Nomlex FrameNet Word2Vec Syntactic resources: Stanford parser Argument Lexical resources: WordNet Syntactic resources: Stanford parser Entity Entity resources: Stanford NER NELL KB Features 1. lemmas of the words in the trigger mention 2. nominalization of the words based on Nomlex (Macleod et al., 1998) 3. context words within a window of size 2 4. similarity features between the head word and a list of trigger seeds based on WordNet (Bronstein et al., 2015) 5. semantic frames that associate with the hea"
N16-1033,P13-1008,0,0.546345,"n Baghdad with Tomahawk cruise missiles being the weapon. The second sentence on its own contains little event-related information, but together with the context of the previous sentence, it indicates another U.S. attack on Baghdad. State-of-the-art event extraction systems have difficulties inferring such information due to two main reasons. First, they extract events and entities in separate stages: entities such as people, organization, and locations are first extracted by a named entity tagger, and then these extracted entities are used as inputs for extracting events and their arguments (Li et al., 2013). This often causes errors to propagate. In the above example, if the entity tagger mistakenly identifies “Baghdad” as a person, then the event extractor will fail to extract “Baghdad” as the place where the attack happened. In fact, previous work (Li et al., 2013) observes that using previously extracted entities in event extraction results in 289 Proceedings of NAACL-HLT 2016, pages 289–299, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics a substantial decrease in performance compared to using gold-standard entity information. Second, most existing w"
N16-1033,D14-1198,0,0.480898,"with compatible types. For example, the P ERSON role type can only be filled with an entity of type PER. However, a N ONE role type can be filled with an entity of any type. To account for these compatibility constraints, we enforce the probabilities of all invalid configurations to be zero. Features. f1 , f2 , and f4 are unary feature functions that depend on trigger variable ti , argument variable rij , and entity variable aj respectively. We construct a set of features for each feature function (see Table 1). Many of these features overlap with those used in previous work (Li et al., 2013; Li et al., 2014), except for the word embedding features for triggers and the features for entities which are derived from multiple entity resources. f3 and f5 are pairwise feature functions that depend on trigger-argument pair (ti , rij ) and argument-entity pair (rij , aj ) respectively. We consider simple indicator functions 1t,r and 1r,a as features (1y (x) equals 1 when x = y and 0 otherwise). Training. For model training, we find the optimal parameters θ using the maximum-likelihood estimates with an L2 regularization: θ ∗ = arg max L(θ) − λ||θ||22 θ L(θ) = X log p(ti , ri· , a· |i, Ni , x) i We use L-B"
N16-1033,P10-1081,0,0.600002,"dard entity information. Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers’ outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information. Both approaches only propagate the highly confident predictions from the first stage to the second stage. To the best of our knowledge, there is no unified model that jointly extracts events from sentences across the whole document. In this paper, we propose a novel approach that simultaneously extracts events and entities within a document context.1 We first decompose the learning problem into three tractable subproblems: (1) learning the dependencie"
N16-1033,D12-1080,0,0.0400293,"s; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work. There has been work on improving event extraction by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causality, inhibition, which frequently occur in biological texts. For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with ATTACK events and T RANS PORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and Jurafsky (2009; 2011) learn narrative schemas by linking event verbs that have coreferring syntactic argument"
N16-1033,P11-1163,0,0.156656,", 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, arg"
N16-1033,P15-2060,0,0.44477,"nt extractor will fail to extract “Baghdad” as the place where the attack happened. In fact, previous work (Li et al., 2013) observes that using previously extracted entities in event extraction results in 289 Proceedings of NAACL-HLT 2016, pages 289–299, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics a substantial decrease in performance compared to using gold-standard entity information. Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers’ outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information. Both approaches only propagate the highly conf"
N16-1033,N10-1123,0,0.045916,"04) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP d"
N16-1033,W09-1119,0,0.0652951,"e trigger mention are in the same clause 6. the shortest dependency paths between the entity mention and the trigger mention 1. Gender and animacy attributes of the entity mention 2. Stanford NER type for the entity mention 3. Semantic type for the entity mention based on the NELL knowledge base (Mitchell et al., 2015) 4. Predicted entity type and confidence score for the entity mention output by the entity extractor described in Section 3.3 Table 1: Features for event triggers, event arguments, and entity mentions. text segments). We use features that are similar to those from previous work (Ratinov and Roth, 2009): (1) current words and part-of-speech tags; (2) context words in a window of size 2; (3) word type such as all-capitalized, is-capitalized, and all-digits; (4) Gazetteer-based entity type if the current word matches an entry in the gazetteers collected from Wikipedia (Ratinov and Roth, 2009). In addition, we consider pre-trained word embeddings (Mikolov et al., 2013) as dense features for each word in order to improve the generalizability of the model. 3.4 Joint Inference Our end goal is to extract coherent event mentions and entity mentions across a document. To achieve this, we propose a jo"
N16-1033,D11-1001,0,0.0703329,"Ahn, 2006; Bj¨orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work. There has been work on improving event extraction by exploiting document-level context. Berant et"
N16-1096,kingsbury-palmer-2002-treebank,0,0.144305,"tering of English data (Section 3.1) and a high degree of inflection in Portuguese verbs. The smaller size of Portuguese KB also means more of its proposed instances are new. 4 Related Work Existing verb resources are limited in their ability to map to KBs. Some existing resources classify verbs into semantic classes either manually (e.g. WordNet (Miller et al., 1990)) or automatically (e.g. DIRT (Lin and Pantel, 2001)). However, these classes are not directly mapped to KB relations. Other resources provide relations between verbs and their arguments in terms of semantic roles (e.g. PropBank (Kingsbury and Palmer, 2002), VerbNet (Kipper et al., 2000), FrameNet (Ruppenhofer et al., 2006)). However, it is not directly clear how the verbs map to relations in specific KBs. Most existing verb resources are also manually constructed and not scalable. A verb resource that maps to KBs should grow in coverage with the KBs, possibly by leveraging large corpora such as the Web for high coverage mapping. One system that leverages Web-text as an interlingua is (Wijaya et al., 2013). However, they use it to map KBs to KBs, and obtain a verb-to-relation mapping only indirectly. They also compute heuristic confidences in ve"
N16-1096,P14-5010,0,0.00487042,"Missing"
N16-1096,N13-1008,0,0.0316442,"straints in KB ontology to couple the learning to produce extractors consistent with these constraints. However, CPL uses a combination of heuristics in its learning, while we use EM. In our experiments, we use CPL patterns that contain verbs as priors and 826 show that our approach outperforms CPL in terms of effectiveness for extracting relation instances. In terms of the relation extraction, there are distantly-supervised methods that can produce verb groupings as a by product of relation extraction. One state-of-the-art uses matrix factorization and universal schemas to extract relations (Riedel et al., 2013). In this work, they populate a database of a universal schema (which involves surface form predicates and relations from pre-existing KBs such as Freebase) by using matrix factorization models that learn latent feature vectors for relations and entity tuples. One can envision obtaining a verb grouping for a particular relation by predicting verb surface forms that occur between entity tuples that are instances of the relation. However, unlike our proposed method that learns mapping between typedverbs to relations, they do not incorporate argument types in their learning, preferring to learn l"
N19-1119,N18-1118,0,0.0224075,"the sentence length that was proposed earlier (longer sentence scores are products over more terms in [0, 1] and are thus likely to be smaller). We thus propose the following difficulty heuristic: drarity (si ) , − Ni X log pˆ(wki ), (3) k=1 where we use logarithms of word probabilities to prevent numerical errors. Note that negation is used because we define less likely (i.e., more rare) sentences as more difficult. These are just two examples of difficulty metrics, and it is easy to conceive of other metrics such as the occurrence of homographs (Liu et al., 2018) or context-sensitive words (Bawden et al., 2018), the examination of which we leave for future work. 2.2 Competence Functions For this paper, we propose two simple functional forms for c(t) and justify them with some intuition. More sophisticated strategies that depend on the loss function, the loss gradient, or on the learner’s performance on held-out data, are possible, but we do not consider them in this paper. Linear: This is a simple way to define c(t). Given an initial value c0 , c(0) ≥ 0 and a slope parameter r, we define: c(t) , min (1, tr + c0 ) . (4) In this case, new training examples are constantly being introduced during the tr"
N19-1119,P10-1088,0,0.0823459,"Missing"
N19-1119,N09-1047,0,0.0622144,"Missing"
N19-1119,D13-1176,0,0.0572781,"Missing"
N19-1119,kocmi-bojar-2017-curriculum,0,0.147614,"uistics of Elman (1993) and Krueger and Dayan (2009). The main motivation is that training algorithms can perform better if training data is presented in a specific order, starting from easy examples and moving on to more difficult ones, as the learner becomes more competent. In the case of machine learning, it can also be thought of as a means to avoid getting stuck in bad local optima early on in training. An overview of the proposed framework is shown in Figure 1. Notably, we are not the first to examine curriculum learning for NMT, although other related works have met with mixed success. Kocmi and Bojar (2017) explore impact of several curriculum heuristics on training a translation system for a single epoch, presenting the training examples in an easy-to-hard order based on sentence length and vocabulary frequency. However, their strategy introduces all training samples during the first epoch, and how this affects learning in following epochs is not clear, with official evaluation results (Bojar et al., 2017b) indicating that final performance may indeed be hurt with this strategy. Contemporaneously to our work, Zhang et al. (2018) further propose to split the training samples into a predefined nu"
N19-1119,N18-1121,1,0.775598,"length, throughout training. formation about the sentence length that was proposed earlier (longer sentence scores are products over more terms in [0, 1] and are thus likely to be smaller). We thus propose the following difficulty heuristic: drarity (si ) , − Ni X log pˆ(wki ), (3) k=1 where we use logarithms of word probabilities to prevent numerical errors. Note that negation is used because we define less likely (i.e., more rare) sentences as more difficult. These are just two examples of difficulty metrics, and it is easy to conceive of other metrics such as the occurrence of homographs (Liu et al., 2018) or context-sensitive words (Bawden et al., 2018), the examination of which we leave for future work. 2.2 Competence Functions For this paper, we propose two simple functional forms for c(t) and justify them with some intuition. More sophisticated strategies that depend on the loss function, the loss gradient, or on the learner’s performance on held-out data, are possible, but we do not consider them in this paper. Linear: This is a simple way to define c(t). Given an initial value c0 , c(0) ≥ 0 and a slope parameter r, we define: c(t) , min (1, tr + c0 ) . (4) In this case, new training examp"
N19-1119,D18-1039,1,0.936632,"24 GBs of system memory. During training, we use a label smoothing factor of 0.1 (Wu et al., 2016) and the AMSGrad optimizer (Reddi et al., 2018) with its default parameters in TensorFlow, and a batch size of 5,120 tokens 1166 # Train 133k 224k 4.5m # Dev 768 1080 3003 # Test 1268 1133 2999 Plain SL Linear RNN SL Sqrt 30 30.00 SR Sqrt Transformer 27.50 28.00 Table 1: Number of parallel sentences in each dataset. “k” stands for “thousand” and “m” stands for “million”. 25 20 0 5000 Step RNN 10000 Data Preprocessing. Our experiments are performed using the machine translation library released by Platanios et al. (2018). We use the same data preprocessing approach the authors used in their experiments. While training, we consider sentences up to length 200. Similar to them, for the IWSLT-15 experiments we use a per-language vocabulary which contains the 20,000 most frequently We emphasize that we did not run experiments with other architectures or configurations, and thus our baseline architectures were not chosen because they were favorable to our method, but rather because they were frequently mentioned in existing literature. 20 25 0 10000 Step 20000 20 0 50000 Step WMT16 : En → De 30 26.50 BLEU Transform"
N19-1119,P16-1162,0,0.391724,"Missing"
N19-1119,D08-1112,0,0.072346,"l words in a sentence to obtain a single difficulty score for that sentence. Previous research has proposed various pooling operations, such as minimum, maximum, and average (Zhang et al., 2018), but they show that they do not work well in practice. We propose a different approach. Ultimately, what might be most important is the overall likelihood of a sentence as that contains information about both word frequency and, implicitly, sentence length. An approximation to this likelihood is the product of the unigram probabilities, which is related to previous work in the area of active learning (Settles and Craven, 2008). This product can be thought of as an approximate language model (assuming words are sampled independently) and also implicitly incorporates in1 NMT models typically first pick up information about producing sentences of correct length. It can be argued that presenting only short sentences first may lead to learning a strong bias for the sentence lengths. In our experiments, we did not observe this to be an issue as the models kept improving and predicting sentences of correct length, throughout training. formation about the sentence length that was proposed earlier (longer sentence scores ar"
N19-1119,P16-5005,0,0.054726,"Missing"
N19-1119,D13-1141,0,0.0190191,"Missing"
P09-1072,J90-1003,0,0.466519,"ed by an adjective. The classification analysis also helps us to identify participants whose mental representations for phrases are consistent across phrase presentations. Subsequent regression analysis on phrase activation will be based on subjects who perform the phrase task well. 4 4.1 Using vector-based models of semantic representation to account for the systematic variances in neural activity Lexical Semantic Representation Computational linguists have demonstrated that a word’s meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990). Consequently, Mitchell et al. (2008) encoded the meaning of a word as a vector of intermediate semantic features computed from the co-occurrences with stimulus words within the Google trillion-token text corpus that captures the typical use of words in English text. Motivated by existing conjectures regarding the centrality of sensory-motor features in neural representations of objects (Caramazza and Shelton, 1998), they selected a set of 25 semantic features defined by 25 verbs: see, hear, listen, taste, smell, eat, touch, rub, lift, manipulate, run, push, fill, move, ride, say, fear, open,"
P09-1072,P08-1028,0,0.556691,"ctive-noun phrases. In this study, we applied the vector-based models of semantic composition used in computational linguistics to model neural activation patterns obtained while subjects comprehended adjective-noun phrases. In an object-contemplation task, human participants were presented with 12 text labels of objects (e.g. dog) and were instructed to think of the same properties of the stimulus object consistently during multiple presentations of each item. The participants were also shown adjective-noun phrases, where adjectives were used to modify the meaning of nouns (e.g. strong dog). Mitchell and Lapata (2008) presented a framework for representing the meaning of phrases and sentences in vector space. They discussed how an additive model, a multiplicative model, a weighted additive model, a Kintsch (2001) model, and a model which combines multiplicative and additive models can be used to model human behavior in similiarity judgements when human participants were presented with a reference containing a subjectverb phrase (e.g., horse ran) and two landmarks (e.g., galloped and dissolved) and asked to choose which landmark was most similiar to the reference (in this case, galloped). They compared the"
P11-1058,S07-1002,0,0.074701,"Missing"
P11-1058,N10-1061,0,0.0389599,"dependent views that are beneficial to semi-supervised training. Concept discovery is also related to coreference resolution (Ng, 2008; Poon and Domingos, 2008). The difference between the two problems is that coreference resolution finds noun phrases that refer to the same concept within a specific document. We think the concepts produced by a system like ConceptResolver could be used to improve coreference resolution by providing prior knowledge about noun phrases that can refer to the same concept. This knowledge could be especially helpful for crossdocument coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. 3 Background: Never-Ending Language Learner ConceptResolver is designed as a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010). In this section, we provide some pertinent background information about NELL that influenced the design of ConceptResolver 1 . NELL is an information extraction system that has been running 24x7 for over a year, using coupled semi-supervised learning to populate an ontology from unstructured text found on the web. The ontology defines two types of predicates: c"
P11-1058,C02-1144,0,0.213384,"d predicates like ceoOf(x1 , x2 ) and (2) mapping noun phrases like x1 , x2 to the concepts they can refer to. The main input to ConceptResolver is a set of extracted category and relation instances over noun phrases, like person(x1 ) and ceoOf(x1 , x2 ), produced by running NELL. Here, any individual noun phrase xi can be labeled with multiple categories and relations. The output of ConceptResolver is a set of concepts, {c1 , c2 , ..., cn }, and a mapping from each noun phrase in the input to the set of concepts it can refer to. Like many other systems (Miller, 1995; Yates and Etzioni, 2007; Lin and Pantel, 2002), ConceptResolver represents each output concept ci as a set of synonymous noun phrases, i.e., ci = {xi1 , xi2 , ..., xim }. For example, Figure 2 shows several concepts output by ConceptResolver; each concept clearly reveals which noun phrases can refer to it. Each concept also has a semantic type that corresponds to a category in ConceptResolver’s ontology; for instance, the first 10 concepts in Figure 2 belong to the category company. Previous approaches to concept discovery use little prior knowledge, clustering noun phrases based on co-occurrence statistics (Pantel and Lin, 2002). In comp"
P11-1058,S10-1011,0,0.0453195,"Missing"
P11-1058,D08-1067,0,0.0247453,"002), we learn a similarity measure for clustering based on a set of must-link and cannot-link constraints. Unlike prior work, our algorithm exploits multiple views of the data to improve the similarity measure. As far as we know, ConceptResolver is the first application of semi-supervised clustering to relational data – where the items being clustered are connected by relations (Getoor and Diehl, 2005). Interestingly, the relational setting also provides us with the independent views that are beneficial to semi-supervised training. Concept discovery is also related to coreference resolution (Ng, 2008; Poon and Domingos, 2008). The difference between the two problems is that coreference resolution finds noun phrases that refer to the same concept within a specific document. We think the concepts produced by a system like ConceptResolver could be used to improve coreference resolution by providing prior knowledge about noun phrases that can refer to the same concept. This knowledge could be especially helpful for crossdocument coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. 3 Background: Never-Ending L"
P11-1058,D08-1068,0,0.0239365,"earn a similarity measure for clustering based on a set of must-link and cannot-link constraints. Unlike prior work, our algorithm exploits multiple views of the data to improve the similarity measure. As far as we know, ConceptResolver is the first application of semi-supervised clustering to relational data – where the items being clustered are connected by relations (Getoor and Diehl, 2005). Interestingly, the relational setting also provides us with the independent views that are beneficial to semi-supervised training. Concept discovery is also related to coreference resolution (Ng, 2008; Poon and Domingos, 2008). The difference between the two problems is that coreference resolution finds noun phrases that refer to the same concept within a specific document. We think the concepts produced by a system like ConceptResolver could be used to improve coreference resolution by providing prior knowledge about noun phrases that can refer to the same concept. This knowledge could be especially helpful for crossdocument coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. 3 Background: Never-Ending Language Learner ConceptRes"
P11-1058,P06-1101,0,0.292913,"n and synonym resolution. Word sense induction is typically performed using unsupervised clustering. In the SemEval word sense induction and disambiguation task (Agirre and Soroa, 2007; Manandhar et al., 2010), all of the submissions in 2007 created senses by clustering the contexts each word occurs in, and the 2010 event explicitly disallowed the use of external resources like ontologies. Other systems cluster words to find both word senses and concepts (Pantel and Lin, 2002; Lin and Pantel, 2002). ConceptResolver’s category-based approach is quite different from these clustering approaches. Snow et al. (2006) describe a system which adds new word senses to WordNet. However, Snow et al. assume the existence of an oracle which provides the senses of each word. In contrast, ConceptResolver automatically determines the number of senses for each word. Synonym resolution on relations extracted from web text has been previously studied by Resolver (Yates and Etzioni, 2007), which finds synonyms in relation triples extracted by TextRunner (Banko et al., 2007). In contrast to our system, Resolver is unsupervised and does not have a schema for the relations. Due to different inputs, ConceptResolver and Reso"
P11-1058,N07-1016,0,0.344694,"e c1 and c2 from extracted predicates like ceoOf(x1 , x2 ) and (2) mapping noun phrases like x1 , x2 to the concepts they can refer to. The main input to ConceptResolver is a set of extracted category and relation instances over noun phrases, like person(x1 ) and ceoOf(x1 , x2 ), produced by running NELL. Here, any individual noun phrase xi can be labeled with multiple categories and relations. The output of ConceptResolver is a set of concepts, {c1 , c2 , ..., cn }, and a mapping from each noun phrase in the input to the set of concepts it can refer to. Like many other systems (Miller, 1995; Yates and Etzioni, 2007; Lin and Pantel, 2002), ConceptResolver represents each output concept ci as a set of synonymous noun phrases, i.e., ci = {xi1 , xi2 , ..., xim }. For example, Figure 2 shows several concepts output by ConceptResolver; each concept clearly reveals which noun phrases can refer to it. Each concept also has a semantic type that corresponds to a category in ConceptResolver’s ontology; for instance, the first 10 concepts in Figure 2 belong to the category company. Previous approaches to concept discovery use little prior knowledge, clustering noun phrases based on co-occurrence statistics (Pantel"
P11-1058,D07-1107,0,\N,Missing
P14-1046,D13-1202,0,0.198944,"Missing"
P14-1046,W11-2503,0,0.0388722,"Missing"
P14-1046,J90-1003,0,0.357969,"Missing"
P14-1046,W13-3510,1,0.837904,"data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA seeks two linear transformations that maximally correlate two data sets in the transformed form. CCA requires that the data sources be paired (all rows in the corpus data must have a corresponding brain data), as correlation between points is integral to the objective. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3 . They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3 http://www.cs.cmu.edu/˜afyshe/papers/ conll2013/ 492 each question for each word on a scale of 1-5. At least 3 respondents answered each question and the median score was used. This gives us a semantic representation of each of the 60 words in a 218-dimensional behavioral space. Because we required answers to each of the questions for all words, we do not have the problems of sparsity that exist for f"
P14-1046,C12-1118,1,0.753644,"g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information"
P14-1046,S12-1019,1,0.86172,"g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information"
P14-1046,D12-1130,0,0.0185333,"Missing"
P14-1046,P13-1056,0,0.0240867,"Missing"
P14-1095,P12-2041,0,0.0248329,"Missing"
P14-1095,N13-1091,0,0.0264171,"(Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). computation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to i"
P14-1095,P13-1146,1,0.886202,"Missing"
P14-1095,D11-1142,0,0.187814,"inues to grow with the development of tools for easy Web authorship. Blogs, forums and social networking websites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an increasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believabil"
P14-1095,D12-1104,1,0.851173,"iguous. Therefore, we first determine the semantics of a fact candidate before computing its truthfulness. Entity Types. We first determine the expected types of the subject and object in the SVO. For example, for the SVO hEinsteini died in hPrincetoni, the expected types are person × location. We determine this by first computing the types of entities that are valid for each verb (verbal phrase) in a large SVO collection of 114m SVO triples (Talukdar et al., 2012). Typing verbal phrases is a once-off computation. Our phrase typing method is similar to prior work on typing relational phrases (Nakashole et al., 2012). Examples of typed phrases are: hpersoni died in hyeari, hpersoni died in hlocationi, and hathletei plays for hteami. Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge 1010 1 http://www.w3.org/TR/rdf-primer/ base containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inferenc"
P14-1095,nastase-etal-2010-wikinet,0,0.0500968,"Missing"
P14-1095,P04-1035,0,0.0716619,"orrelation in our believability computation model. To incorporate objectivity in FactChecker, we require for a given source document, an objectivity score ∈ [0, 1], where 0 means the source is subjective and 1 means it is objective. Next, describe our method for automatically determining objectivity of sources. 3.2 Automatic Objectivity Detection We trained a logistic regression classifier to predict the objectivity of a document. For training and testing data, we used the labeled data from the Mechanical Turk study. We additionally used labeled text from prior work on subjectivity detection (Pang and Lee, 2004). This resulted in a total of 4, 600 documents, half subjective and the other half objective. We used 4000 documents for 1012 # 1 2 3 4 5 Feature Subjectivity lexicon of strong and weak subjective words (Riloff and Wiebe, 2003). Sentiment lexicon of positive and negative words (Liu et al., 2005). Wikipedia-derived bias lexicon (Recasens et al., 2013). Part-of-speech (POS) tags Frequent bi-grams Approach Sentiment Lexicon Wikipedia bias Lexicon Subjectivity Lexicon FC-Objectivity Detector Accuracy 0.65±0.06 0.69±0.06 0.70±0.06 0.78±0.05 Table 2: Accuracy of the objectivity detector. Di has obje"
P14-1095,W03-1017,0,0.0307666,"Missing"
P14-1095,P13-1162,0,0.029339,"8; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2 . Articles violating the NPOV policy are marked 1011 2 http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text: (S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya. Text S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Literature on sentiment analysis (Turney et a"
P14-1095,P02-1053,0,0.0469803,"Missing"
P14-1095,J04-3002,0,0.0300454,"motion or personal bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2 . Articles violating the NPOV policy are marked 1011 2 http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view to indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text: (S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya. Text S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Lit"
P14-1095,W03-1014,0,\N,Missing
P14-1112,P98-1013,0,0.255643,"antic parsing: in training the parser, in resolving syntactic ambiguities when the trained parser is applied to new text, and in its output semantic representation. Using semantic information from the knowledge base at training and test time will ideally improve the parser’s ability to solve difficult syntactic parsing problems, as in the examples above. A semantic representation tied to a knowledge base allows for powerful inference operations – such as identifying the possible entity referents of a noun phrase – that cannot be performed with shallower representations (e.g., frame semantics (Baker et al., 1998) or a direct conversion of syntax to logic (Bos, 2005)). This paper presents an approach to training a joint syntactic and semantic parser using a large background knowledge base. Our parser produces a full syntactic parse of every sentence, and furthermore produces logical forms for portions of the sentence that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like λx.CITY(x) ∧ LOCATED I N(x, C ALIFORNIA) to the “town in California” portion. Additionally, the parser"
P14-1112,J99-2004,0,0.0679688,"Missing"
P14-1112,D13-1160,0,0.119029,"g and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semantically analyzing conjunctions and relative clauses based on the syntact"
P14-1112,hockenmaier-steedman-2002-acquiring,0,0.0636388,"nce that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like λx.CITY(x) ∧ LOCATED I N(x, C ALIFORNIA) to the “town in California” portion. Additionally, the parser uses predicate and entity type information during parsing to select a syntactic parse. Our parser is trained by combining a syntactic parsing task with a distantly-supervised relation extraction task. Syntactic information is provided by CCGbank, a conversion of the Penn Treebank into the CCG formalism (Hockenmaier and Steedman, 2002a). Semantics are learned by training the parser to extract knowledge base relation instances from a corpus of unlabeled sentences, in a distantly-supervised training regime. This approach uses the knowledge base to avoid expensive manual labeling of individual sentence semantics. By optimizing the parser to perform both tasks simultaneously, we train a parser that produces accurate syntactic and semantic analyses. We demonstrate our approach by training a joint syntactic and semantic parser, which we call A SP. A SP produces a full syntactic analysis of every sentence while simultaneously pro"
P14-1112,P02-1043,0,0.0208008,"nce that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like λx.CITY(x) ∧ LOCATED I N(x, C ALIFORNIA) to the “town in California” portion. Additionally, the parser uses predicate and entity type information during parsing to select a syntactic parse. Our parser is trained by combining a syntactic parsing task with a distantly-supervised relation extraction task. Syntactic information is provided by CCGbank, a conversion of the Penn Treebank into the CCG formalism (Hockenmaier and Steedman, 2002a). Semantics are learned by training the parser to extract knowledge base relation instances from a corpus of unlabeled sentences, in a distantly-supervised training regime. This approach uses the knowledge base to avoid expensive manual labeling of individual sentence semantics. By optimizing the parser to perform both tasks simultaneously, we train a parser that produces accurate syntactic and semantic analyses. We demonstrate our approach by training a joint syntactic and semantic parser, which we call A SP. A SP produces a full syntactic analysis of every sentence while simultaneously pro"
P14-1112,P03-1046,0,0.0430818,"example (S(e1 ,e2 ) , y(e1 ,e2 ) ). If a syntactic example is sampled, we apply the following parameter update: ˆ d, ˆ tˆ ← arg max Γ(`, d, t|si ; θt ) `, `,d,t ∗ ∗ θ t+1 ` ,d ← arg max Γ(`, d, ti |si ; θt ) `,d ˆ tˆ, si ) ← θ + φ(d∗ , ti , si ) − φ(d, t This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗ , ti , si ). If a semantic example is 1192 Labeled Dependencies P R F Unlabeled Dependencies P R F A SP A SP - SYN 85.58 86.06 85.31 85.84 85.44 85.95 91.75 92.13 91.46 91.89 91.60 92.01 99.63 99.63 C&C (Clark and Curran, 2007b) (Hockenmaier, 2003a) 88.34 84.3 86.96 84.6 87.64 84.4 93.74 91.8 92.28 92.2 93.00 92.0 99.63 99.83 Coverage Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies. 5.1 sampled, we instead apply the following update: ˆ`, d, ˆ ˆt ← arg max Γ(`, d, t|S(e ,e ) ; θt ) 1 2 `,d,t ∗ ∗ ∗ ` , d , t ← arg max Γ(`, d, t|S(e1 ,e2 ) ; θt ) `,d,t + Ψ(y(e1 ,e2 ) , `, d, t) θ t+1 t ← θ + φ(d∗ , t∗ , S(e1 ,e2 ) ) ˆ ˆt, S(e ,e ) ) − φ(d, 1 2 This update moves the parameters toward the features of the best"
P14-1112,P13-1042,0,0.23301,"elle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semant"
P14-1112,P11-1055,0,0.090056,"tree equals the true syntactic tree ti . In the above equation |· |+ denotes the positive part of the expression. Minimizing this objective therefore finds parameters θ that reproduce the annotated syntactic trees. 4.2 Semantic Objective The semantic objective corresponds to a distantlysupervised relation extraction task that constrains the logical forms produced by the semantic parser. Distant supervision is provided by the following constraint: every relation instance r(e1 , e2 ) ∈ K must be expressed by at least one sentence in S(e1 ,e2 ) , the set of sentences that mention both e1 and e2 (Hoffmann et al., 2011). If this constraint is empirically true and sufficiently constrains the parser’s logical forms, then optimizing the semantic objective produces an accurate semantic parser. A training example in the semantic objective consists of the set of sentences mentioning a pair of entities, S(e1 ,e2 ) = {s1 , s2 , ...}, paired with a binary vector representing the set of relations that the two entities participate in, y(e1 ,e2 ) . The distant supervision constraint Ψ forces the logical forms predicted for the sentences to entail the relations in y(e1 ,e2 ) . Ψ is a deterministic OR constraint that chec"
P14-1112,S13-1045,0,0.129662,"elle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semant"
P14-1112,C04-1041,0,0.0138333,"Missing"
P14-1112,W07-1202,0,0.315647,"n”, CITY ) ∧ M (y, “england”, COUNTRY ) ∧ 3.5 Parametrization The parser Γ is trained as a discriminative linear model of the following form: Γ(`, d, t|s; θ) = θT φ(d, t, s) Given a parameter vector θ and a sentence s, the parser produces a score for a syntactic parse tree t, a collection of dependency structures d and a logical form `. The score depends on features of the parse produced by the feature function φ. φ contains four classes of features: lexicon features, combinator features, dependency features and dependency distance features (Table 1). These features are based on those of C&C (Clark and Curran, 2007b), modified to include semantic types. The features are designed to share syntactic information about a word across its distinct semantic realizations in order to transfer syntactic information from CCGbank to semantic parsing. The parser also includes a hard type-checking constraint to ensure that logical forms are welltyped. This constraint states that dependency structures with a head semantic type only accept arguments that (1) have a semantic type, and (2) are within the domain/range of the head type. 4 Parameter Estimation This section describes the training procedure for A SP. Training"
P14-1112,J07-4004,0,0.707407,"n”, CITY ) ∧ M (y, “england”, COUNTRY ) ∧ 3.5 Parametrization The parser Γ is trained as a discriminative linear model of the following form: Γ(`, d, t|s; θ) = θT φ(d, t, s) Given a parameter vector θ and a sentence s, the parser produces a score for a syntactic parse tree t, a collection of dependency structures d and a logical form `. The score depends on features of the parse produced by the feature function φ. φ contains four classes of features: lexicon features, combinator features, dependency features and dependency distance features (Table 1). These features are based on those of C&C (Clark and Curran, 2007b), modified to include semantic types. The features are designed to share syntactic information about a word across its distinct semantic realizations in order to transfer syntactic information from CCGbank to semantic parsing. The parser also includes a hard type-checking constraint to ensure that logical forms are welltyped. This constraint states that dependency structures with a head semantic type only accept arguments that (1) have a semantic type, and (2) are within the domain/range of the head type. 4 Parameter Estimation This section describes the training procedure for A SP. Training"
P14-1112,W10-2903,0,0.0283051,"d Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this"
P14-1112,W05-0602,0,0.0414269,", 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a"
P14-1112,P06-1115,0,0.0716504,"C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a large"
P14-1112,D12-1069,1,0.734232,"Missing"
P14-1112,D11-1140,0,0.153761,"a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of the"
P14-1112,D13-1161,0,0.153413,"Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semantically analyzing conjunctions and relative clause"
P14-1112,Q13-1015,0,0.0481319,"successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 20"
P14-1112,P11-1060,0,0.120043,"wever, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed"
P14-1112,D08-1082,0,0.222308,"osely based on C&C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this"
P14-1112,N06-1056,0,0.0577621,"2003b). The parsing model in this paper is loosely based on C&C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2"
P14-1112,P07-1121,0,0.134054,"del in this paper is loosely based on C&C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 201"
P14-1112,D12-1035,0,0.0258443,"mple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for exampl"
P14-1112,C98-1013,0,\N,Missing
P15-1036,P08-1037,0,0.0811697,"is used to replace certain words in rules with their WordNet classes. Parser Correction Methods. The quadruples formulation of the PP problem can be seen as a simplified setting. This is because, with quadruples, there is no need to deal with complex sentences but only well-defined quadruples of the form {v, n1, p, n2}. Thus in the quadruples setting, there are only two possible attachment sites for the PP, the v and n1. An alternative setting is to work in the context of full sentences. In this setting the problem is cast as a dependency parser correction problem (Atterer and Sch¨utze, 2007; Agirre et al., 2008; Anguiano and Candito, 2011). That is, given a dependency parse of a sentence, with potentially incorrect PP attachments, rectify it such that the prepositional phrases attach to the correct sites. Unlike our approach, these methods do not take semantic knowledge into account. Sense Disambiguation. In addition to prior work on prepositional phrase attachment, a highly related problem is preposition sense disambiguation (Hovy et al., 2011; Srikumar and Roth, 2013). Even a syntactically correctly attached PP can still be semantically ambiguous with respect to questions of machine reading such a"
P15-1036,D11-1113,0,0.0206918,"rtain words in rules with their WordNet classes. Parser Correction Methods. The quadruples formulation of the PP problem can be seen as a simplified setting. This is because, with quadruples, there is no need to deal with complex sentences but only well-defined quadruples of the form {v, n1, p, n2}. Thus in the quadruples setting, there are only two possible attachment sites for the PP, the v and n1. An alternative setting is to work in the context of full sentences. In this setting the problem is cast as a dependency parser correction problem (Atterer and Sch¨utze, 2007; Agirre et al., 2008; Anguiano and Candito, 2011). That is, given a dependency parse of a sentence, with potentially incorrect PP attachments, rectify it such that the prepositional phrases attach to the correct sites. Unlike our approach, these methods do not take semantic knowledge into account. Sense Disambiguation. In addition to prior work on prepositional phrase attachment, a highly related problem is preposition sense disambiguation (Hovy et al., 2011; Srikumar and Roth, 2013). Even a syntactically correctly attached PP can still be semantically ambiguous with respect to questions of machine reading such as where, when, and why. There"
P15-1036,D11-1142,0,0.0758211,"Missing"
P15-1036,J07-4002,0,0.0551691,"Missing"
P15-1036,N07-4013,0,0.0926722,"Missing"
P15-1036,J93-1005,0,0.231592,"Collins and Brooks, 1995) proposed a back-off model that uses subsets of the words in the quadruple, by also keeping frequency counts of triples, pairs and single words. Another approach to overcoming sparsity has been to use WordNet (Fellbaum, 1998) classes, by replacing nouns with their WordNet classes (Stetina and Nagao, 1997; Toutanova et al., 2004) to obtain less sparse corpus statistics. Corpus-derived clusters of similar nouns and verbs have also been used (Pantel and Lin, 2000). Hindle and Rooth proposed a lexical association approach based on how words are associated with each other (Hindle and Rooth, 1993). Lexical preference is used by computing co-occurrence frequencies (lexical associations) of verbs and nouns, with prepositions. In this manner, they would discover that, for example, the verb “send” is highly associated with the preposition from, indicating that in this case, the PP is likely to be a verb attachment. Structure-based Methods. These methods are based on high-level observations that are then generalized into heuristics for PP attachment decisions. (Kimball, 1988) proposed a right association method, whose premise is that a word tends to attach to another word immediately to its"
P15-1036,C94-2195,0,0.717969,"and the 7th International Joint Conference on Natural Language Processing, pages 365–375, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Noun attachments Verb attachments 0.9 1 0.8 0.75 0.7 0.6 0.5 0.5 0.25 0.4 WITH AT FROM FOR AS IN ON 0 Figure 2: Dependency parser PP attachment accuracy for various frequent prepositions. IN FROM WITH FOR OF As AT ON Figure 3: Noun vs. verb attachment proportions for frequent prepositions in the labeled NYTC dataset. for the PP, choose the most plausible attachment site. In the literature, prior work going as far back as (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995) has focused on the language pattern that causes most PP ambiguities, which is the 4-word sequence: {v, n1, p, n2} (e.g., {caught, butterfly, with, spots}). The task is to determine if the prepositional phrase (p, n2) attaches to the verb v or to the first noun n1. Following common practice, we focus on PPs occurring as {v, n1, p, n2} quadruples — we shall refer to these as PP quads. The approach we present here differs from prior work in two main ways. First, we make extensive use of semantic knowledge about nouns, verbs, prepositions, pair"
P15-1036,P11-2056,0,0.0235325,"ing is to work in the context of full sentences. In this setting the problem is cast as a dependency parser correction problem (Atterer and Sch¨utze, 2007; Agirre et al., 2008; Anguiano and Candito, 2011). That is, given a dependency parse of a sentence, with potentially incorrect PP attachments, rectify it such that the prepositional phrases attach to the correct sites. Unlike our approach, these methods do not take semantic knowledge into account. Sense Disambiguation. In addition to prior work on prepositional phrase attachment, a highly related problem is preposition sense disambiguation (Hovy et al., 2011; Srikumar and Roth, 2013). Even a syntactically correctly attached PP can still be semantically ambiguous with respect to questions of machine reading such as where, when, and why. Therefore, when extracting information from prepositions, the problem of preposition sense disambiguation (semantics) has to be addressed in addition to prepositional phrase attachment disambiguation (syntax). In this paper, our focus is on the latter. Statistics-based Methods. Prominent prior methods learn to perform PP attachment based on corpus co-occurrence statistics, gathered either from manually annotated tr"
P15-1036,W95-0103,0,0.772175,"ural Language Processing, pages 365–375, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Noun attachments Verb attachments 0.9 1 0.8 0.75 0.7 0.6 0.5 0.5 0.25 0.4 WITH AT FROM FOR AS IN ON 0 Figure 2: Dependency parser PP attachment accuracy for various frequent prepositions. IN FROM WITH FOR OF As AT ON Figure 3: Noun vs. verb attachment proportions for frequent prepositions in the labeled NYTC dataset. for the PP, choose the most plausible attachment site. In the literature, prior work going as far back as (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995) has focused on the language pattern that causes most PP ambiguities, which is the 4-word sequence: {v, n1, p, n2} (e.g., {caught, butterfly, with, spots}). The task is to determine if the prepositional phrase (p, n2) attaches to the verb v or to the first noun n1. Following common practice, we focus on PPs occurring as {v, n1, p, n2} quadruples — we shall refer to these as PP quads. The approach we present here differs from prior work in two main ways. First, we make extensive use of semantic knowledge about nouns, verbs, prepositions, pairs of nouns, and the discourse context in which a PP q"
P15-1036,de-marneffe-etal-2006-generating,0,0.0564257,"Missing"
P15-1036,P03-1054,0,0.0406414,"Missing"
P15-1036,H94-1048,0,0.775621,"al Joint Conference on Natural Language Processing, pages 365–375, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Noun attachments Verb attachments 0.9 1 0.8 0.75 0.7 0.6 0.5 0.5 0.25 0.4 WITH AT FROM FOR AS IN ON 0 Figure 2: Dependency parser PP attachment accuracy for various frequent prepositions. IN FROM WITH FOR OF As AT ON Figure 3: Noun vs. verb attachment proportions for frequent prepositions in the labeled NYTC dataset. for the PP, choose the most plausible attachment site. In the literature, prior work going as far back as (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995) has focused on the language pattern that causes most PP ambiguities, which is the 4-word sequence: {v, n1, p, n2} (e.g., {caught, butterfly, with, spots}). The task is to determine if the prepositional phrase (p, n2) attaches to the verb v or to the first noun n1. Following common practice, we focus on PPs occurring as {v, n1, p, n2} quadruples — we shall refer to these as PP quads. The approach we present here differs from prior work in two main ways. First, we make extensive use of semantic knowledge about nouns, verbs, prepositions, pairs of nouns, and the discou"
P15-1036,D11-1049,1,0.886783,"Missing"
P15-1036,P98-2177,0,0.122188,"tions of machine reading such as where, when, and why. Therefore, when extracting information from prepositions, the problem of preposition sense disambiguation (semantics) has to be addressed in addition to prepositional phrase attachment disambiguation (syntax). In this paper, our focus is on the latter. Statistics-based Methods. Prominent prior methods learn to perform PP attachment based on corpus co-occurrence statistics, gathered either from manually annotated training data (Collins and Brooks, 1995; Brill and Resnik, 1994) or from automatically acquired training data that may be noisy (Ratnaparkhi, 1998; Pantel and Lin, 2000). These models collect statistics on how often a given quadruple, {v, n1, p, n2}, occurs in the training data as a verb attachment as opposed to a noun attachment. The issue with this approach is sparsity, that is, many quadruples occuring in the test data might not have been seen in the training data. Smoothing techniques are often employed to overcome sparsity. For example, (Collins and Brooks, 1995) proposed a back-off model that uses subsets of the words in the quadruple, by also keeping frequency counts of triples, pairs and single words. Another approach to overcom"
P15-1036,P08-1028,0,0.186653,"Missing"
P15-1036,Q13-1019,0,0.0144647,"he context of full sentences. In this setting the problem is cast as a dependency parser correction problem (Atterer and Sch¨utze, 2007; Agirre et al., 2008; Anguiano and Candito, 2011). That is, given a dependency parse of a sentence, with potentially incorrect PP attachments, rectify it such that the prepositional phrases attach to the correct sites. Unlike our approach, these methods do not take semantic knowledge into account. Sense Disambiguation. In addition to prior work on prepositional phrase attachment, a highly related problem is preposition sense disambiguation (Hovy et al., 2011; Srikumar and Roth, 2013). Even a syntactically correctly attached PP can still be semantically ambiguous with respect to questions of machine reading such as where, when, and why. Therefore, when extracting information from prepositions, the problem of preposition sense disambiguation (semantics) has to be addressed in addition to prepositional phrase attachment disambiguation (syntax). In this paper, our focus is on the latter. Statistics-based Methods. Prominent prior methods learn to perform PP attachment based on corpus co-occurrence statistics, gathered either from manually annotated training data (Collins and B"
P15-1036,P14-1095,1,0.864349,"Missing"
P15-1036,E03-1051,0,0.0859684,"Missing"
P15-1036,W12-3008,1,0.898077,"Missing"
P15-1036,P90-1004,0,0.0878637,"indicating that in this case, the PP is likely to be a verb attachment. Structure-based Methods. These methods are based on high-level observations that are then generalized into heuristics for PP attachment decisions. (Kimball, 1988) proposed a right association method, whose premise is that a word tends to attach to another word immediately to its right. (Frazier, 1978) introduced a minimal attachment method, which posits that words attach to an existing non-terminal word using the fewest additional syntactic nodes. While simple, in practice these methods have been found to perform poorly (Whittemore et al., 1990). Rule-based Methods. (Brill and Resnik, 1994) 4 Methodology Our approach consists of first generating features from background knowledge and then training a model to learn with these features. The types of features considered in our experiments are summarized in Table 2. The choice of features was motivated by our empirically driven characterization of the problem as follows: (Verb attach) −→ v hhas-slot-filleri n2 (Noun attach a.) −→ n1 hdescribed-byi n2 (Noun attach b.) −→ n2 hdescribed-byi n1 367 Feature Type Noun-Noun Binary Relations Noun Semantic Categories Verb Role Fillers Preposition"
P15-1036,D14-1207,1,0.807416,"Missing"
P15-1036,P13-1146,1,0.899116,"Missing"
P15-1036,P00-1014,0,0.0909499,"ading such as where, when, and why. Therefore, when extracting information from prepositions, the problem of preposition sense disambiguation (semantics) has to be addressed in addition to prepositional phrase attachment disambiguation (syntax). In this paper, our focus is on the latter. Statistics-based Methods. Prominent prior methods learn to perform PP attachment based on corpus co-occurrence statistics, gathered either from manually annotated training data (Collins and Brooks, 1995; Brill and Resnik, 1994) or from automatically acquired training data that may be noisy (Ratnaparkhi, 1998; Pantel and Lin, 2000). These models collect statistics on how often a given quadruple, {v, n1, p, n2}, occurs in the training data as a verb attachment as opposed to a noun attachment. The issue with this approach is sparsity, that is, many quadruples occuring in the test data might not have been seen in the training data. Smoothing techniques are often employed to overcome sparsity. For example, (Collins and Brooks, 1995) proposed a back-off model that uses subsets of the words in the quadruple, by also keeping frequency counts of triples, pairs and single words. Another approach to overcoming sparsity has been t"
P15-1036,C98-2172,0,\N,Missing
P17-1132,D14-1165,1,0.409302,"sfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture of our KBLSTM model draws on the development of attention mechanisms that are widely emp"
P17-1132,P16-1223,0,0.0144414,"er and Schmidhuber, 1997) (LSTMs) or Gated Recurrent Unit (Cho et al., 2014) (GRU) as they are able to handle longterm dependencies by adaptively memorizing values for either long or short durations. Their bidirectional variants BiLSTM (Graves et al., 2005) or BiGRU further allow the incorporation of both past and future information. Such ability has been shown to be generally helpful in various NLP tasks such as named entity recognition (Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), semantic role labeling (Zhou and Xu, 2015), and reading comprehension (Hermann et al., 2015; Chen et al., 2016). In this work, we also employ the BiLSTM architecture. In parallel to the development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 conte"
P17-1132,P15-1017,0,0.0189669,"ieves the best performance and outperforms the previous state-of-the-art methods without having access to any gold-standard entities. 4.3 Table 5: event extraction results on the ACE2005 test set. 4.2.1 Results We compare our models with the prior state-ofthe-art approaches for event extraction, including neural and non-neural ones: J OINT B EAM refers to the joint beam search approach with local and global features (Li et al., 2013); J OINT E NTI TY E VENT refers to the graphical model for joint entity and event extraction (Yang and Mitchell, 2016); DMCNN is the dynamic multi-pooling CNNs in Chen et al. (2015); and JRNN is an RNN model with memory introduced by Nguyen et al. (2016). The first block in Table 5 shows the results of the feature-based linear models (taken from Yang and Mitchell (2016)). The second block shows the previously reported results for the neural models. Note that they both make use of gold-standard entity annotations. The third block shows the results of our models. We can see that our KBLSTM models significantly outperform the Model Analysis In order to better understand our model, we visualize the learned attention weights α for KB concepts and the sentinel weight β that me"
P17-1132,Q16-1026,0,0.0372426,". 2 Related Work Essential to RNNs’ success on natural language processing is the use of Long Short-Term Memory neural networks (Hochreiter and Schmidhuber, 1997) (LSTMs) or Gated Recurrent Unit (Cho et al., 2014) (GRU) as they are able to handle longterm dependencies by adaptively memorizing values for either long or short durations. Their bidirectional variants BiLSTM (Graves et al., 2005) or BiGRU further allow the incorporation of both past and future information. Such ability has been shown to be generally helpful in various NLP tasks such as named entity recognition (Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), semantic role labeling (Zhou and Xu, 2015), and reading comprehension (Hermann et al., 2015; Chen et al., 2016). In this work, we also employ the BiLSTM architecture. In parallel to the development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers str"
P17-1132,Q14-1037,0,0.140532,"yt+1 is the score of transitioning from tag yt to yt+1 . By replacing ht in Eq. 8 and Eq. 9 with the knowledge-aware state ˆ t (Eq. 6), we can compute the objective vector h for KBLSTM and KBLSTM-CRF respectively. 4.1.1 Model Implementation Details We evaluate our models on the ACE2005 corpus (LDC, 2005) and the OntoNotes 5.0 corpus (Hovy et al., 2006) for entity extraction. Both datasets consist of text from a variety of sources such as newswire, broadcast conversations, and web text. We use the same data splits and task settings for ACE2005 as in Li et al. (2014) and for OntoNotes 5.0 as in Durrett and Klein (2014). At each time step, our models take as input a word vector and a capitalization feature (Chiu and Nichols, 2016). We initialize the word vectors using pretrained paraphrastic embeddings (Wieting et al., 2015), as we find that they significantly outperforms randomly initialized embeddings. The word embeddings are fine-tuned during training. For the KBLSTM models, we obtain the embeddings of KB concepts from NELL and WordNet as described in Section § 3.3. These embeddings are kept fix during training. We implement all the models using Theano on a single GPU. We update the model parameters on ev"
P17-1132,P15-1033,0,0.00896077,"tly processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset. 1 Introduction Recurrent neural networks (RNNs), a neural architecture that can operate over text sequentially, have shown great success in addressing a wide range of natural language processing problems, such as parsing (Dyer et al., 2015), named entity recognition (Lample et al., 2016), and semantic role labeling (Zhou and Xu, 2015)). These neural networks are typically trained end-to-end where the input is only text or a sequence of words and a lot of background knowledge is disregarded. The importance of background knowledge in natural language understanding has long been recognized (Minsky, 1988; Fillmore, 1976). Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns. With the advanced development of knowledge base construction, large amounts of sem"
P17-1132,D14-1044,1,0.161312,"ure. In parallel to the development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture o"
P17-1132,D15-1038,0,0.0172785,"Missing"
P17-1132,N06-2015,0,0.0163651,"entire sequence. It computes the probability of the correct types for a sequence of chunks: py = P exp(g(x, y)) 0 y0 exp(g(x, y )) (9) Pl Pl where g(x, y) = t=0 Ayt ,yt+1 , t=1 Pt,yt + Pt,yt = wyTt ht is the score of assigning the t-th chunk with tag yt and Ayt ,yt+1 is the score of transitioning from tag yt to yt+1 . By replacing ht in Eq. 8 and Eq. 9 with the knowledge-aware state ˆ t (Eq. 6), we can compute the objective vector h for KBLSTM and KBLSTM-CRF respectively. 4.1.1 Model Implementation Details We evaluate our models on the ACE2005 corpus (LDC, 2005) and the OntoNotes 5.0 corpus (Hovy et al., 2006) for entity extraction. Both datasets consist of text from a variety of sources such as newswire, broadcast conversations, and web text. We use the same data splits and task settings for ACE2005 as in Li et al. (2014) and for OntoNotes 5.0 as in Durrett and Klein (2014). At each time step, our models take as input a word vector and a capitalization feature (Chiu and Nichols, 2016). We initialize the word vectors using pretrained paraphrastic embeddings (Wieting et al., 2015), as we find that they significantly outperforms randomly initialized embeddings. The word embeddings are fine-tuned duri"
P17-1132,N16-1030,0,0.0103979,"tion mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset. 1 Introduction Recurrent neural networks (RNNs), a neural architecture that can operate over text sequentially, have shown great success in addressing a wide range of natural language processing problems, such as parsing (Dyer et al., 2015), named entity recognition (Lample et al., 2016), and semantic role labeling (Zhou and Xu, 2015)). These neural networks are typically trained end-to-end where the input is only text or a sequence of words and a lot of background knowledge is disregarded. The importance of background knowledge in natural language understanding has long been recognized (Minsky, 1988; Fillmore, 1976). Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns. With the advanced development of knowledge base construction, large amounts of semantic knowledge become available, ranging from m"
P17-1132,P16-1101,0,0.00559981,"al to RNNs’ success on natural language processing is the use of Long Short-Term Memory neural networks (Hochreiter and Schmidhuber, 1997) (LSTMs) or Gated Recurrent Unit (Cho et al., 2014) (GRU) as they are able to handle longterm dependencies by adaptively memorizing values for either long or short durations. Their bidirectional variants BiLSTM (Graves et al., 2005) or BiGRU further allow the incorporation of both past and future information. Such ability has been shown to be generally helpful in various NLP tasks such as named entity recognition (Huang et al., 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016), semantic role labeling (Zhou and Xu, 2015), and reading comprehension (Hermann et al., 2015; Chen et al., 2016). In this work, we also employ the BiLSTM architecture. In parallel to the development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization a"
P17-1132,D16-1147,0,0.0347915,"wledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture of our KBLSTM model draws on the development of attention mechanisms that are widely employed in tasks such as machine translation (Bahdanau et al., 2015) a"
P17-1132,P16-1105,0,0.0431009,"Missing"
P17-1132,P15-1036,1,0.833885,"been recognized (Minsky, 1988; Fillmore, 1976). Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns. With the advanced development of knowledge base construction, large amounts of semantic knowledge become available, ranging from manually annotated semantic networks like WordNet 1 to semi-automatically or automatically constructed knowledge graphs like DBPedia 2 and NELL 3 . While traditional approaches have exploited the use of these knowledge bases (KBs) in NLP tasks (Ratinov and Roth, 2009; Rahman and Ng, 2011; Nakashole and Mitchell, 2015), they require a lot of task-specific engineering to achieve good performance. One way to leverage KBs in recurrent neural networks is by augmenting the dense representations of the networks with the symbolic features derived from KBs. This is not ideal as the symbolic features have poor generalization ability. In addition, they can be highly sparse, e.g., using WordNet synsets can easily produce millions of indicator features, leading to high computational cost. Furthermore, the usefulness of knowledge features varies across contexts, as general KBs involve polysemy, e.g., “Clinton” can refer"
P17-1132,D11-1049,1,0.0619687,"e BiLSTM architecture. In parallel to the development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extracti"
P17-1132,N16-1034,0,0.00814535,"t methods without having access to any gold-standard entities. 4.3 Table 5: event extraction results on the ACE2005 test set. 4.2.1 Results We compare our models with the prior state-ofthe-art approaches for event extraction, including neural and non-neural ones: J OINT B EAM refers to the joint beam search approach with local and global features (Li et al., 2013); J OINT E NTI TY E VENT refers to the graphical model for joint entity and event extraction (Yang and Mitchell, 2016); DMCNN is the dynamic multi-pooling CNNs in Chen et al. (2015); and JRNN is an RNN model with memory introduced by Nguyen et al. (2016). The first block in Table 5 shows the results of the feature-based linear models (taken from Yang and Mitchell (2016)). The second block shows the previously reported results for the neural models. Note that they both make use of gold-standard entity annotations. The third block shows the results of our models. We can see that our KBLSTM models significantly outperform the Model Analysis In order to better understand our model, we visualize the learned attention weights α for KB concepts and the sentinel weight β that measures the tradeoff between knowledge and context. Figure 2a visualizes t"
P17-1132,P11-1082,0,0.00852313,"nderstanding has long been recognized (Minsky, 1988; Fillmore, 1976). Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns. With the advanced development of knowledge base construction, large amounts of semantic knowledge become available, ranging from manually annotated semantic networks like WordNet 1 to semi-automatically or automatically constructed knowledge graphs like DBPedia 2 and NELL 3 . While traditional approaches have exploited the use of these knowledge bases (KBs) in NLP tasks (Ratinov and Roth, 2009; Rahman and Ng, 2011; Nakashole and Mitchell, 2015), they require a lot of task-specific engineering to achieve good performance. One way to leverage KBs in recurrent neural networks is by augmenting the dense representations of the networks with the symbolic features derived from KBs. This is not ideal as the symbolic features have poor generalization ability. In addition, they can be highly sparse, e.g., using WordNet synsets can easily produce millions of indicator features, leading to high computational cost. Furthermore, the usefulness of knowledge features varies across contexts, as general KBs involve poly"
P17-1132,P14-1038,0,0.0472698,"Missing"
P17-1132,D14-1198,0,0.0742535,"ssigning the t-th chunk with tag yt and Ayt ,yt+1 is the score of transitioning from tag yt to yt+1 . By replacing ht in Eq. 8 and Eq. 9 with the knowledge-aware state ˆ t (Eq. 6), we can compute the objective vector h for KBLSTM and KBLSTM-CRF respectively. 4.1.1 Model Implementation Details We evaluate our models on the ACE2005 corpus (LDC, 2005) and the OntoNotes 5.0 corpus (Hovy et al., 2006) for entity extraction. Both datasets consist of text from a variety of sources such as newswire, broadcast conversations, and web text. We use the same data splits and task settings for ACE2005 as in Li et al. (2014) and for OntoNotes 5.0 as in Durrett and Klein (2014). At each time step, our models take as input a word vector and a capitalization feature (Chiu and Nichols, 2016). We initialize the word vectors using pretrained paraphrastic embeddings (Wieting et al., 2015), as we find that they significantly outperforms randomly initialized embeddings. The word embeddings are fine-tuned during training. For the KBLSTM models, we obtain the embeddings of KB concepts from NELL and WordNet as described in Section § 3.3. These embeddings are kept fix during training. We implement all the models using Theano"
P17-1132,P13-1008,0,0.100264,"ionally be adjectives and other content words. Therefore, the task is typically addressed as a classification problem where the goal is to label each word in a sentence with an event type or an O type if it does not express any of the defined events. It is straightforward to apply the BiLSTM architecture to event extraction. Similarly to the models for entity extraction, we can train the BiLSTM network with both the softmax objective and the CRF objective. We evaluate our models on the portion ACE2005 corpus that has event annotations. We use the same data split and experimental setting as in Li et al. (2013). The training procedure is the same as in Section 4.1.1, and we train all the models for about 5 epochs. For the KBLSTM models, we integrate the learned embeddings of WordNet synsets during training. 1442 (a) The X-axis represents relevant NELL concepts for the entity mention clinton. The Y-axis represents the concept weights and the knowledge sentinel weight. (b) The X-axis represents relevant WordNet concepts for the event trigger head. The Y-axis represents the concept weights and the knowledge sentinel weight. Figure 2: Visualization of the attention weights for KB features learned by KBL"
P17-1132,N13-1008,0,0.0310207,"distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture of our KBLSTM model draws on the development of attention mechanisms that are widely employed in tasks such as"
P17-1132,D15-1174,0,0.0349752,"e development for text processing, neural networks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture of our KBLSTM model draws"
P17-1132,D13-1136,0,0.0269662,"orks have been successfully used to learn distributed representations of structured knowledge from large KBs (Bordes et al., 2011, 2013; Socher et al., 2013; Yang et al., 2015; Guu et al., 2015). Embedding the symbolic representations into continuous space not only makes KBs more easy to use in statistical learning approaches, but also offers strong generalization ability. Many attempts have been made on connecting distributed representations of KBs with text in the 1437 context of knowledge base completion (Lao et al., 2011; Gardner et al., 2014; Toutanova et al., 2015), relation extraction (Weston et al., 2013; Chang et al., 2014; Riedel et al., 2013), and question answering (Miller et al., 2016). However, these approaches model text using shallow representations such as subject/relation/object triples or bag of words. More recently, Ahn et al. (2016) proposed a neural knowledge language model that leverages knowledge bases in RNN language models, which allows for better representations of words for language modeling. Unlike their work, we leverage knowledge bases in LSTMs and applies it to information extraction. The architecture of our KBLSTM model draws on the development of attention mechanisms"
P17-1132,N16-1033,1,0.914927,"ention 8 ∗ indicates p &lt; 0.05 when comparing to the BiLSTMbased models. 1441 Model KB P R F1 BiLSTM-Fea-CRF NELL WordNet Both 87.2 86.4 87.7 86.1 86.0 86.1 86.6 86.2 86.9 KBLSTM-CRF NELL WordNet Both 87.4 87.1 88.1 87.6 87.4 87.8 87.5 87.3 88.0 Model Table 2: Ablation results with different KBs. chunking using the same setting as described in Section 4.1.1. We then treat the predicted chunks as units for entity type labeling. Table 3 reports the full entity extraction results on the ACE2005 test set. We compare our models with the state-of-the-art feature-based linear models Li et al. (2014), Yang and Mitchell (2016), and the recently proposed sequence- and tree-structured LSTMs (Miwa and Bansal, 2016). Interestingly, we find that using BiLSTM-CRF without any KB information already gives strong performance compared to previous work. The KBLSTM-CRF model demonstrates the best performance among all the models and achieves the new state-of-theart performance on the ACE2005 dataset. We also report the entity extraction results on the OntoNotes 5.0 test set in Table 4. We compare our models with the existing feature-based models Ratinov and Roth (2009) and Durrett and Klein (2014), which both employ heavy feat"
P17-1132,P15-1109,0,0.0620094,"e whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset. 1 Introduction Recurrent neural networks (RNNs), a neural architecture that can operate over text sequentially, have shown great success in addressing a wide range of natural language processing problems, such as parsing (Dyer et al., 2015), named entity recognition (Lample et al., 2016), and semantic role labeling (Zhou and Xu, 2015)). These neural networks are typically trained end-to-end where the input is only text or a sequence of words and a lot of background knowledge is disregarded. The importance of background knowledge in natural language understanding has long been recognized (Minsky, 1988; Fillmore, 1976). Earlier NLP systems mostly exploited restricted linguistic knowledge such as manually-encoded morphological and syntactic patterns. With the advanced development of knowledge base construction, large amounts of semantic knowledge become available, ranging from manually annotated semantic networks like WordNet"
P18-1029,P07-1036,0,0.494474,"onceptually allows the framework to subsume a flexible range of grounding behaviors. The main contributions of this work are: 1. We introduce the problem of zero-shot learning of classifiers from language, and present an approach towards this. 2. We develop datasets for zero-shot classification from natural descriptions, exhibiting Related Work Many notable approaches have explored incorporation of background knowledge into the training of learning algorithms. However, none of them addresses the issue of learning from natural language. Prominent among these are the Constraint-driven learning (Chang et al., 2007a), Generalized Expectation (Mann and McCallum, 2010) and Posterior Regularization (Ganchev et al., 2010) and Bayesian Measurements (Liang et al., 2009) frameworks. All of these require domain knowledge to be manually programmed in before learning. Similarly, Probabilistic Soft Logic (Kimmig et al., 2012) allows users to specify rules in a logical language that can be used for reasoning over graphical models. More recently, multiple approaches have explored fewshot learning from perspective of term or attributebased transfer (Lampert et al., 2014), or learning representations of instances as p"
P18-1029,W17-2809,0,0.0785584,"7) has explored using language explanations for feature space construction in concept learning tasks, where the problem of learning to interpret language, and learning classifiers is treated jointly. However, this approach assumes availability of labeled data for learning classifiers. Also notable is recent work by Andreas et al. (2017), who propose using language descriptions as parameters to model structure in learning tasks in multiple settings. More generally, learning from language has also been previously explored in tasks such as playing games (Branavan et al., 2012), robot navigation (Karamcheti et al., 2017), etc. Natural language quantification has been studied from multiple perspectives in formal logic (Barwise and Cooper, 1981), linguistics (Löbner, 1987; Bach et al., 2013) and cognitive psychology (Kurtzman and MacDonald, 1993). While quantification has traditionally been defined in set-theoretic terms in linguistic theories1 , our approach joins alternative 307 1 e.g., ‘some A are B’ ⇔ A ∩ B 6= ∅ perspectives that represent quantifiers probabilistically (Moxey and Sanford, 1993; Yildirim et al., 2013). To the best of our knowledge, this is the first work to leverage the semantics of quantifi"
P18-1029,P11-1060,0,0.0333084,"a linear score 5. Features indicating presence of a linguistic S(s, lxy ) = wT ψ(s, lxy ) indicating the goodness quantifier in a det or an advmod relation with of assigning a partial logical form, lxy , to a sensyntactic head of x or y. tence s. Here, ψ(s, lxy ) ∈ Rn are features that can Since the constraint type is determined by depend on both the sentence and the partial logical syntactic and dependency parse features, this form, and w ∈ Rn is a parameter weight-vector for this component. Following recent work in semantic 3 We also identify whether a feature x is negated, through parsing (Liang et al., 2011), we assume a loglinear the existence of a neg dependency relation with the head of distribution over interpretations of a sentence. its text-span. e.g., Important emails are usually not deleted 309 Type P (y |x) P (x |y) P (y) Example description Emails that I reply to are usually important I often reply to important emails I rarely get important emails Conversion to Expectation Constraint E[Iy=important,reply(x):true ] − pusually × E[Ireply(x):true ] = 0 E[Iy=important,reply(x):true ] − pof ten × E[Iy=important ] = 0 Same as P (y|x0 ), where x0 is a constant feature Table 2: Common constrain"
P18-1029,D17-1161,1,0.829049,"before learning. Similarly, Probabilistic Soft Logic (Kimmig et al., 2012) allows users to specify rules in a logical language that can be used for reasoning over graphical models. More recently, multiple approaches have explored fewshot learning from perspective of term or attributebased transfer (Lampert et al., 2014), or learning representations of instances as probabilistic programs (Lake et al., 2015). Other work (Lei Ba et al., 2015; Elhoseiny et al., 2013) considers language terms such as colors and textures that can be directly grounded in visual meaning in images. Some previous work (Srivastava et al., 2017) has explored using language explanations for feature space construction in concept learning tasks, where the problem of learning to interpret language, and learning classifiers is treated jointly. However, this approach assumes availability of labeled data for learning classifiers. Also notable is recent work by Andreas et al. (2017), who propose using language descriptions as parameters to model structure in learning tasks in multiple settings. More generally, learning from language has also been previously explored in tasks such as playing games (Branavan et al., 2012), robot navigation (Ka"
P18-1029,D07-1071,0,0.069213,"probability of observing a feature and concept labels lxy based on the text of the sentence, (ii) probability of the type of the assertion ltype based on the identified feature, concept label and syntactic properties of the sentence s, and (iii) identifying the linguistic quantifier, lquant , in the sentence. Provided data consisting of statements labeled with logical forms, the model can be trained via maximum likelihood estimation, and be used to predict interpretations for new statements. For training this component, we use a CCG semantic parsing formalism, and follow the feature-set from Zettlemoyer and Collins (2007), consisting of simple indicator features for occurrences of keywords and lexicon entries. This is also compatible with the semantic parsing formalism in Srivastava et al. (2017), whose data (and accompanying lexicon) are also used in our evaluation. For other datasets with predefined features, this component is learned easily from simple lexicons consisting of trigger words for features and labels.3 This component is the only part of the parser that is domain-specific. We note that while this component assumes a domain-specific lexicon (and possibly statement annotated with logical forms), th"
P19-1507,D14-1162,0,0.0914539,"ccuracy results on macrocontext tests, therefore the layer aligned well with the whole brain activity. The choice of representation (deep neural network layer) to encode brain activity should be done carefully, as each representation may be good at encoding different parts of brain. A good criteria for representation selection requires further research. To demonstrate the efficacy of the synthetic dataset, we present the accuracy in predicting noun (or verb) stimuli from observed MEG activity with and without the additional synthetic MEG data. With linear ridge regression model (X2), a GloVe (Pennington et al., 2014) feature to brainactivity prediction models were trained to predict the MEG activity when a word is observed . To test the model performance, we calculate the accuracy of the predicted brain activity given the true brain activity during a word processing (Equation 1). All the experiments use 4-fold cross-validation. Figure 7 shows the increase in the noun/verb prediction accuracy with additional synthetically generated data. The statistical significance is calculated over 400 random label permutation tests. To summarize, these results show the utility of using previously trained regressor mode"
P19-1507,N18-1202,0,0.0490997,"of research by investigating the following three questions: (1) what is the relationship between sentence representations learned by deep learning networks and those encoded by the brain; (2) is there any correspondence between hidden layer activations in these deep models and brain regions; and (3) is it possible for deep recurrent models to synthesize brain data so that they can effectively be used for brain data augmentation. In order to evaluate these questions, we focus on representations of simple sentences. We employ various deep network architectures, including recently proposed ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) networks. We use MagnetoEncephaloGraphy (MEG) brain recording data of simple sentences as the target reference. We then correlate the representations learned by these various networks with the MEG recordings. Overall, we observe that BERT representations are the most predictive of MEG data. We also observe that the deep network models are effective at synthesizing brain data which are useful in overcoming data sparsity in stimuli decoding tasks involving brain data. In summary, in this paper we make the following contributions. ∗ This research was carried out du"
P19-1507,D14-1030,1,\N,Missing
P19-1507,N19-1423,0,\N,Missing
Q15-1019,S13-1002,0,0.0156804,"mposition. This approach is similar to ours, except that we use matrix factorization and Freebase entities. Finally, some work has focused on the problem of textual inference within this paradigm. Fader et al. (2013) present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of Open IE triples (Fader et al., 2011). Distributional similarity has also been used to learn weighted logical inference rules that can be used for recognizing textual entailment or identifying semantically similar text (Garrette et al., 2011; Garrette et al., 2013; Beltagy et al., 2013). This line of work focuses on performing inference between texts, whereas our work computes a text’s denotation. A significant difference between our work and most of the related work above is that our work computes denotations containing Freebase entities. Using these entities has two advantages: (1) it enables us to use entity linking to disambiguate textual mentions, and (2) it facilitates a comparison against alternative approaches that rely on a closed predicate vocabulary. Disambiguating textual mentions is a major challenge for previous approaches, so an entity-linked corpus is a much"
Q15-1019,P14-1133,0,0.133797,"owledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Republican front-runner” cannot be adequately encoded in the Freebase schema because it lacks the concept of a “front-runner.” We c"
Q15-1019,D13-1160,0,0.748468,"uction Traditional knowledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Republican front-runner” cannot be adequately encoded in the Freebase schema because it lacks the concept o"
Q15-1019,W06-3812,0,0.0064198,"both appear in the training data. All answers found in this fashion are assigned probability 1. The C LUSTERING baseline first clusters the predicates in the training corpus, then answers questions using the clustered predicates. The clustering aggregates predicates with similar denotations, ideally identifying synonyms to smooth over sparsity in the training data. Our approach is closely based on Lewis and Steedman (2013), though is also conceptually related to approaches such as DIRT (Lin and Pantel, 2001) and USP (Poon and Domingos, 2009). We use the Chinese Whispers clustering algorithm (Biemann, 2006) and calculate the similarity between predicates as the cosine similarity of their TF-IDF weighted entity count vectors. The denotation of each cluster is the union of the denotations of the clustered predicates, and each entity in the denotation is assigned probability 1. We also trained two probabilistic database models, FACTORIZATION (OP ) and FACTORIZATION (OQ ), using the two objective functions described in Sections 6.2 and 6.3, respectively. We optimized both objectives by performing 100 passes over the 265 b b + + r ur u b b + + r bu bur + + ur u b b + + r r u u b + 0.4 because they re"
Q15-1019,W08-2222,0,0.302123,"ed in worse performance, as it dramatically increased the sparsity of training instances for the combined relations. Third, entity mentions with the N /N category are analyzed using a special noun-noun relation, as in the second example in Figure 2. Our intuition is that this relation shares instances with other relations (e.g., “city in Texas” implies “Texan city”). Finally, we lowercased each word to create its predicate name, but performed no lemmatization or other normalization. 3.4 Discussion The scope of our semantic analysis system is somewhat limited relative to other similar systems (Bos, 2008; Lewis and Steedman, 2013) as it only outputs existentially-quantified conjunctions of predicates. Our goal in building this system was to analyze noun phrases and simple sentences, for which this representation generally suffices. The reason for this focus is twofold. First, this subset of language is sufficient to capture much of the language surrounding Freebase entities. Second, for various technical reasons, this restricted semantic representation is easier to use (and more informative) for training the probabilistic database (see Section 6.3). Note that this system can be straightforwar"
Q15-1019,P13-1042,0,0.330007,"r many questions that Freebase cannot. 1 Introduction Traditional knowledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Republican front-runner” cannot be adequately encoded in the"
Q15-1019,D11-1142,0,0.00671742,"clustering-based approach that incorporates com267 position using a generative model for each sentence that factors according to its parse tree. Lewis and Steedman (2013) also present a clustering-based approach that uses CCG to perform semantic composition. This approach is similar to ours, except that we use matrix factorization and Freebase entities. Finally, some work has focused on the problem of textual inference within this paradigm. Fader et al. (2013) present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of Open IE triples (Fader et al., 2011). Distributional similarity has also been used to learn weighted logical inference rules that can be used for recognizing textual entailment or identifying semantically similar text (Garrette et al., 2011; Garrette et al., 2013; Beltagy et al., 2013). This line of work focuses on performing inference between texts, whereas our work computes a text’s denotation. A significant difference between our work and most of the related work above is that our work computes denotations containing Freebase entities. Using these entities has two advantages: (1) it enables us to use entity linking to disambi"
Q15-1019,P13-1158,0,0.014606,"lem of compositional semantics with an open predicate vocabulary. Unsupervised semantic parsing (Poon and Domingos, 2009; Titov and Klementiev, 2011) is a clustering-based approach that incorporates com267 position using a generative model for each sentence that factors according to its parse tree. Lewis and Steedman (2013) also present a clustering-based approach that uses CCG to perform semantic composition. This approach is similar to ours, except that we use matrix factorization and Freebase entities. Finally, some work has focused on the problem of textual inference within this paradigm. Fader et al. (2013) present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of Open IE triples (Fader et al., 2011). Distributional similarity has also been used to learn weighted logical inference rules that can be used for recognizing textual entailment or identifying semantically similar text (Garrette et al., 2011; Garrette et al., 2013; Beltagy et al., 2013). This line of work focuses on performing inference between texts, whereas our work computes a text’s denotation. A significant difference between our work and most of the related work above is t"
Q15-1019,W11-0112,0,0.0128298,"approach that uses CCG to perform semantic composition. This approach is similar to ours, except that we use matrix factorization and Freebase entities. Finally, some work has focused on the problem of textual inference within this paradigm. Fader et al. (2013) present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of Open IE triples (Fader et al., 2011). Distributional similarity has also been used to learn weighted logical inference rules that can be used for recognizing textual entailment or identifying semantically similar text (Garrette et al., 2011; Garrette et al., 2013; Beltagy et al., 2013). This line of work focuses on performing inference between texts, whereas our work computes a text’s denotation. A significant difference between our work and most of the related work above is that our work computes denotations containing Freebase entities. Using these entities has two advantages: (1) it enables us to use entity linking to disambiguate textual mentions, and (2) it facilitates a comparison against alternative approaches that rely on a closed predicate vocabulary. Disambiguating textual mentions is a major challenge for previous app"
Q15-1019,hockenmaier-steedman-2002-acquiring,0,0.0604,"l input/output pairs for this system are shown in Figure 2. The conversion to logical form has 3 phases: 1. CCG syntactic parsing parses the text and applies several deterministic syntactic transformations to facilitate semantic analysis. 259 2. Entity linking marks known Freebase entities in the text. 3. Semantic analysis assigns a logical form to each word, then composes them to produce a logical form for the complete text. 3.1 Syntactic Parsing The first step in our analysis is to syntactically parse the text. We use the ASP-SYN parser (Krishnamurthy and Mitchell, 2014) trained on CCGBank (Hockenmaier and Steedman, 2002). We then automatically transform the resulting syntactic parse to make the syntactic structure more amenable to semantic analysis. This step marks NP s in conjunctions by replacing their syntactic category with NP [conj]. This transformation allows semantic analysis to distinguish between appositives and comma-separated lists. It also transforms all verb arguments to core arguments, i.e., using the category PP /NP as opposed to ((S NP )(S NP ))/NP . This step simplifies the semantic analysis of verbs with prepositional phrase arguments. The final transformation adds a word feature to each"
Q15-1019,D12-1069,1,0.60776,"Missing"
Q15-1019,P14-1112,1,0.699507,"ng to generate training data (see Section 6.1). Several input/output pairs for this system are shown in Figure 2. The conversion to logical form has 3 phases: 1. CCG syntactic parsing parses the text and applies several deterministic syntactic transformations to facilitate semantic analysis. 259 2. Entity linking marks known Freebase entities in the text. 3. Semantic analysis assigns a logical form to each word, then composes them to produce a logical form for the complete text. 3.1 Syntactic Parsing The first step in our analysis is to syntactically parse the text. We use the ASP-SYN parser (Krishnamurthy and Mitchell, 2014) trained on CCGBank (Hockenmaier and Steedman, 2002). We then automatically transform the resulting syntactic parse to make the syntactic structure more amenable to semantic analysis. This step marks NP s in conjunctions by replacing their syntactic category with NP [conj]. This transformation allows semantic analysis to distinguish between appositives and comma-separated lists. It also transforms all verb arguments to core arguments, i.e., using the category PP /NP as opposed to ((S NP )(S NP ))/NP . This step simplifies the semantic analysis of verbs with prepositional phrase arguments. T"
Q15-1019,D13-1161,0,0.454643,"Freebase cannot. 1 Introduction Traditional knowledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Republican front-runner” cannot be adequately encoded in the Freebase schema because i"
Q15-1019,Q13-1015,0,0.349237,"performance, as it dramatically increased the sparsity of training instances for the combined relations. Third, entity mentions with the N /N category are analyzed using a special noun-noun relation, as in the second example in Figure 2. Our intuition is that this relation shares instances with other relations (e.g., “city in Texas” implies “Texan city”). Finally, we lowercased each word to create its predicate name, but performed no lemmatization or other normalization. 3.4 Discussion The scope of our semantic analysis system is somewhat limited relative to other similar systems (Bos, 2008; Lewis and Steedman, 2013) as it only outputs existentially-quantified conjunctions of predicates. Our goal in building this system was to analyze noun phrases and simple sentences, for which this representation generally suffices. The reason for this focus is twofold. First, this subset of language is sufficient to capture much of the language surrounding Freebase entities. Second, for various technical reasons, this restricted semantic representation is easier to use (and more informative) for training the probabilistic database (see Section 6.3). Note that this system can be straightforwardly extended to model addit"
Q15-1019,D09-1001,0,0.345155,"will return the set of entities e such that CEO(e) and OF(e, / EN / SPRINT) both appear in the training data. All answers found in this fashion are assigned probability 1. The C LUSTERING baseline first clusters the predicates in the training corpus, then answers questions using the clustered predicates. The clustering aggregates predicates with similar denotations, ideally identifying synonyms to smooth over sparsity in the training data. Our approach is closely based on Lewis and Steedman (2013), though is also conceptually related to approaches such as DIRT (Lin and Pantel, 2001) and USP (Poon and Domingos, 2009). We use the Chinese Whispers clustering algorithm (Biemann, 2006) and calculate the similarity between predicates as the cosine similarity of their TF-IDF weighted entity count vectors. The denotation of each cluster is the union of the denotations of the clustered predicates, and each entity in the denotation is assigned probability 1. We also trained two probabilistic database models, FACTORIZATION (OP ) and FACTORIZATION (OQ ), using the two objective functions described in Sections 6.2 and 6.3, respectively. We optimized both objectives by performing 100 passes over the 265 b b + + r ur u"
Q15-1019,P11-1138,0,0.00442299,"and comma-separated lists. It also transforms all verb arguments to core arguments, i.e., using the category PP /NP as opposed to ((S NP )(S NP ))/NP . This step simplifies the semantic analysis of verbs with prepositional phrase arguments. The final transformation adds a word feature to each PP category, e.g., mapping PP to PP [by]. These features are used to generate verb-preposition relation predicates, such as DIRECTED BY. 3.2 Entity Linking The second step is to identify mentions of Freebase entities in the text. This step could be performed by an off-the-shelf entity linking system (Ratinov et al., 2011; Milne and Witten, 2008) or string matching. However, our training and test data is derived from Clueweb 2009, so we rely on the entity linking for this corpus provided by Gabrilovich et. al (2013). Our system incorporates the provided entity links into the syntactic parse provided that they are consistent with the parse structure. Specifically, we require that each mention is either (1) a constituent in the parse tree with syntactic category N or N P or (2) a collection of N/N or N P/N P modifiers with a single head word. The first case covers noun and noun phrase mentions, while the second"
Q15-1019,Q14-1030,0,0.0912016,"ssumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Republican front-runner” cannot be adequately encoded in the Freebase schema because it lacks the concept of a “front-runner.” We could choose to encode"
Q15-1019,N13-1008,0,0.218125,"semantics of “Republican front-runner” cannot be adequately encoded in the Freebase schema because it lacks the concept of a “front-runner.” We could choose to encode this concept as “politician” at the cost of abstracting away the distinction between the two. As this example illustrates, these two problems are prevalent in even the largest knowledge bases. An alternative paradigm is an open predicate vocabulary, where each natural language word or phrase is given its own formal predicate. This paradigm is embodied in both open information extraction (Banko et al., 2007) and universal schema (Riedel et al., 2013). Open predicate vocabularies have the potential to capture subtle semantic distinctions and achieve high coverage. However, we have yet to develop compelling approaches to compositional semantics within this paradigm. This paper takes a step toward compositional se257 Transactions of the Association for Computational Linguistics, vol. 3, pp. 257–270, 2015. Action Editor: Katrin Erk. Submission batch: 12/2014; Revision batch 3/2015; Published 5/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. ... (G . BUSH, TEXAS) (G . BUSH, REPUB .) (REPUB ."
Q15-1019,P11-1080,0,0.01628,"ated work above is that our work computes denotations containing Freebase entities. Using these entities has two advantages: (1) it enables us to use entity linking to disambiguate textual mentions, and (2) it facilitates a comparison against alternative approaches that rely on a closed predicate vocabulary. Disambiguating textual mentions is a major challenge for previous approaches, so an entity-linked corpus is a much cleaner source of data. However, our approach could also work with automatically constructed entities, for example, created by clustering mentions in an unsupervised fashion (Singh et al., 2011). Semantic Parsing Several semantic parsers have been developed for Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014). Our approach is most similar to that of Reddy et al. (2014), which uses fixed syntactic parses of unlabeled text to train a Freebase semantic parser. Like our approach, this system automatically-generates query/answer pairs for training. However, this system, like all Freebase semantic parsers, uses a closed predicate vocabulary consisting of only Freebase predicates. In contrast, our approach uses an open predicate vocabular"
Q15-1019,P11-1145,0,0.0100924,"ons in a corpus of relation triples. Matrix factorization is an alternative approach to clustering that has been used for relation extraction (Riedel et al., 2013; Yao et al., 2013) and finding analogies (Turney, 2008; Speer et al., 2008). All of this work is closely related to distributional semantics, which uses distributional information to identify semantically similar words and phrases (Turney and Pantel, 2010; Griffiths et al., 2007). Some work has considered the problem of compositional semantics with an open predicate vocabulary. Unsupervised semantic parsing (Poon and Domingos, 2009; Titov and Klementiev, 2011) is a clustering-based approach that incorporates com267 position using a generative model for each sentence that factors according to its parse tree. Lewis and Steedman (2013) also present a clustering-based approach that uses CCG to perform semantic composition. This approach is similar to ours, except that we use matrix factorization and Freebase entities. Finally, some work has focused on the problem of textual inference within this paradigm. Fader et al. (2013) present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of Open IE tri"
Q15-1019,D12-1035,0,0.00778326,"that our open predicate vocabulary enables us to answer many questions that Freebase cannot. 1 Introduction Traditional knowledge representation assumes that world knowledge can be encoded using a closed vocabulary of formal predicates. In recent years, semantic parsing has enabled us to build compositional models of natural language semantics using such a closed predicate vocabulary (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These semantic parsers map natural language statements to database queries, enabling applications such as answering questions using a large knowledge base (Yahya et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). Furthermore, the modeltheoretic semantics provided by such parsers have the potential to improve performance on other tasks, such as information extraction and coreference resolution. However, a closed predicate vocabulary has inherent limitations. First, its coverage will be limited, as such vocabularies are typically manually constructed. Second, it may abstract away potentially relevant semantic differences. For example, the semantics of “Repub"
Q15-1019,P12-1075,0,0.00920649,"estions can be answered correctly using an open vocabulary approach like ours. This evaluation also suggests that recall is a limiting factor of our approach; in the future, recall can be improved by using a larger corpus or including Freebase instances during training. 8 Related Work Open Predicate Vocabularies There has been considerable work on generating semantic representations with an open predicate vocabulary. Much of the work is non-compositional, focusing on identifying similar predicates and entities. DIRT (Lin and Pantel, 2001), Resolver (Yates and Etzioni, 2007) and other systems (Yao et al., 2012) cluster synonymous expressions in a corpus of relation triples. Matrix factorization is an alternative approach to clustering that has been used for relation extraction (Riedel et al., 2013; Yao et al., 2013) and finding analogies (Turney, 2008; Speer et al., 2008). All of this work is closely related to distributional semantics, which uses distributional information to identify semantically similar words and phrases (Turney and Pantel, 2010; Griffiths et al., 2007). Some work has considered the problem of compositional semantics with an open predicate vocabulary. Unsupervised semantic parsin"
Q15-1019,N07-1016,0,0.0134725,"question. However, many of the remaining questions can be answered correctly using an open vocabulary approach like ours. This evaluation also suggests that recall is a limiting factor of our approach; in the future, recall can be improved by using a larger corpus or including Freebase instances during training. 8 Related Work Open Predicate Vocabularies There has been considerable work on generating semantic representations with an open predicate vocabulary. Much of the work is non-compositional, focusing on identifying similar predicates and entities. DIRT (Lin and Pantel, 2001), Resolver (Yates and Etzioni, 2007) and other systems (Yao et al., 2012) cluster synonymous expressions in a corpus of relation triples. Matrix factorization is an alternative approach to clustering that has been used for relation extraction (Riedel et al., 2013; Yao et al., 2013) and finding analogies (Turney, 2008; Speer et al., 2008). All of this work is closely related to distributional semantics, which uses distributional information to identify semantically similar words and phrases (Turney and Pantel, 2010; Griffiths et al., 2007). Some work has considered the problem of compositional semantics with an open predicate voc"
S12-1019,W04-3221,0,0.381645,"Missing"
S12-1019,J10-4006,0,0.0716838,"Missing"
S12-1019,W10-0609,0,0.117011,"with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make"
S12-1019,D07-1097,0,0.0220474,"Missing"
S12-1019,W10-0603,0,0.122196,"hat correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabulary. The Mitchell et al. (2008) 25-verb model wou"
S12-1019,W03-0208,0,0.0260834,"Missing"
S12-1019,P98-2127,0,0.0806289,"word- and documentlevel statistics, in raw and dimensionality reduced forms (Bullinaria and Levy, 2007; Turney and Pantel, 2010).2 ( PMIwf if PMIwf &gt; 0 PPMIwf = (1) 0 otherwise   p(w, f ) PMIwf = log (2) p(w)p(f ) A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions a cut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting dimension2 Preliminary analyses confirmed that PPMI performed as well or better than alternat"
S12-1019,W02-0109,0,0.0201132,"Missing"
S12-1019,D09-1065,1,0.837138,"antics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly v"
S12-1019,J07-2002,0,0.340177,"Missing"
S12-1019,2003.mtsummit-papers.42,0,0.473495,"ocument co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f ) for the varying frequency of the target word p(w) and its features p(f ). PPMI up-weights cooccurrences between rare words, yielding positive values for collocations that are more common than would be expected by chance (i.e. if word distributions were independent), and discards negative values that represent patterns of co-occurrences that are rarer than one would expect by chance. It has been shown to perform well generally, with both wo"
S12-1019,N03-1036,0,0.0336465,"Missing"
S12-1019,W02-0908,0,\N,Missing
S12-1019,P06-4018,0,\N,Missing
S12-1019,C98-2122,0,\N,Missing
S12-1019,ide-suderman-2004-american,0,\N,Missing
W04-3240,J96-2004,0,0.0371147,"rocessed by removing quoted material, attachments, and non-subject header information. This preprocessing was performed manually, but was limited to operations which can be reliably automated. The most difficult step is removal of quoted material, which we address elsewhere (Carvalho & Cohen, 2004). 4.2 Inter-Annotator Agreement Each message may be annotated with several labels, as it may contain several speech acts. To evaluate inter-annotator agreement, we doublelabeled N03F2 for the verbs Deliver, Commit, Request, Amend, and Propose, and the noun, Meeting, and computed the kappa statistic (Carletta, 1996) for each of these, defined as κ= A −R 1− R where A is the empirical probability of agreement on a category, and R is the probability of agreement for two annotators that label documents at random (with the empirically observed frequency of each label). Hence kappa ranges from -1 to +1. The results in Table 1 show that agreement is good, but not perfect. Email Act Kappa Meeting 0.82 Deliver 0.75 Commit 0.72 Request 0.81 Amend 0.83 Propose 0.72 Table 1 - Inter-Annotator Agreement on N03F2. We also took doubly-annotated messages which had only a single verb label and constructed the 5-class conf"
W04-3240,W02-1011,0,0.0295964,"Missing"
W04-3240,C96-2125,0,0.0226534,"Missing"
W04-3240,W01-1626,0,0.00873616,"Missing"
W09-2201,P07-1036,0,0.00680981,"ern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models. Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of those constraints. While similar in spirit, our work differs in that we consider learning many models, rather than one structured model, and that we are consider a much larger scale application in a different domain. 4 4.1 Approach Coupling of Predicates As me"
W09-2201,W99-0613,0,0.248716,"at one implies the other). In this paper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to h"
W09-2201,C92-2082,0,0.19285,"r, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than"
W09-2201,N06-1020,0,0.0150239,"aper, we focus on a ‘bootstrapping’ method for semi-supervised learning. Bootstrapping approaches start with a small number of labeled ‘seed’ examples, use those seed examples to train an initial model, then use this model to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and"
W09-2201,P06-1102,0,0.0352017,"Missing"
W09-2201,P02-1006,0,0.0200999,"el to label some of the unlabeled data. The model is then retrained, using the original seed examples plus the self-labeled examples. This process iterates, gradually expanding the amount of labeled data. Such approaches have shown promise in applications such as web page classification (Blum and Mitchell, 1998), named entity classification (Collins and Singer, 1999), parsing (McClosky et al., 2006), and machine translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work"
W09-2201,P07-1076,0,0.0155944,"vano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). Other research explores the task of ‘open information extraction’, where the predicates to be learned are n"
W09-2201,N06-1039,0,0.0101674,"e ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst, 1992). This approach was later scaled up to the web by Etzioni et al. (2005). Other research explores the task of ‘open information extraction’, where the predicates to be learned are not specified in advance (Shinyama and Sekine, 2006; Banko et al., 2007), but emerge instead from analysis of the data. In contrast, our approach relies strongly on knowledge in the ontology about the predicates to be learned, and relationships among them, in order to achieve high accuracy. Chang et al. (2007) present a framework for learning that optimizes the data likelihood plus constraint-based penalty terms than capture prior knowledge, and demonstrate it with semi-supervised learning of segmentation models. Constraints that capture domain knowledge guide bootstrap learning of a structured model by penalizing or disallowing violations of"
W09-2201,P03-1044,0,0.0482584,"translation (Ueffing, 2006). Bootstrapping approaches to information extraction can yield impressive results with little initial human effort (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Pasca et al., 2006). However, after many iterations, they usually suffer from semantic drift, where errors in labeling accumulate and the learned concept ‘drifts’ from what was intended (Curran et al., 2007). Coupling the learning of predicates by using positive examples of one predicate as negative examples for others has been shown to help limit this drift (Riloff and Jones, 1999; Yangarber, 2003). Additionally, ensuring that relation arguments are of certain, expected types can help mitigate the promotion of incorrect instances (Pas¸ca et al., 2006; Rosenfeld and Feldman, 2007). Our work builds on these ideas to couple the simultaneous bootstrapped training of multiple categories and multiple relations. Our approach to information extraction is based on using high precision contextual patterns (e.g., ‘is mayor of arg1’ suggests that arg1 is a city). An early pattern-based approach to information extraction acquired ‘is a’ relations from text using generic contextual patterns (Hearst,"
W13-3201,P06-1115,0,0.0281236,"and phrases have uniform semantic representations (Socher et al., 2011; Socher et al., 2012). Notably, Hermann and Blunsom (2013) instantiate such a framework using CCG. VSSP generalizes these approaches, as they can be implemented within VSSP by choosing appropriate logical forms. Furthermore, our experiments demonstrate that VSSP can learn composition operations that cannot be learned by these approaches. The VSSP framework uses semantic parsing to define a compositional vector space model. Semantic parsers typically map sentences to logical semantic representations (Zelle and Mooney, 1996; Kate and Mooney, 2006), with many systems using CCG as the parsing formalism (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012). Although previous work has focused on logical semantics, it has demonstrated that semantic parsing is an elegant technique for specifying models of compositional semantics. In this paper, we show how to use semantic parsing to produce compositional models of vector space semantics. 6 Discussion and Future Work We present vector space semantic parsing (VSSP), a general framework for building compositional models of vector space semantics. Our framew"
W13-3201,J07-4004,0,0.08161,"te the set of all parameters; for example, θ = {Ared , vball }. We further assume a loss function L defined over pairs of label vectors. The training problem is therefore to minimize the objective: n X λ O(θ) = L(y i , g(`i (θ)) + ||θ||2 2 Producing the Parametrized Lexicon We create a lexicon using a set of manuallyconstructed templates that associate each syntactic category with a parametrized logical form. Each template contains variables that are instantiated to define per-word parameters. The output of this step is a CCG lexicon which can be used in a broad coverage syntactic CCG parser (Clark and Curran, 2007) to produce logical forms for input language.4 Table 2 shows some templates used to create logical forms for syntactic categories. To reduce annotation effort, we define one template per semantic type, covering all syntactic categories with that type. These templates are instantiated by replacing the variable w in each logical form with the current word. For example, instantiating the second template for “red” produces the logical form λx.σ(Ared x), where Ared is a matrix of parameters. i=1 Above, g represents a global postprocessing function which is applied to the output of VSSP to make a ta"
W13-3201,D12-1069,1,0.555527,"Missing"
W13-3201,D11-1140,0,0.0195794,"n and Blunsom (2013) instantiate such a framework using CCG. VSSP generalizes these approaches, as they can be implemented within VSSP by choosing appropriate logical forms. Furthermore, our experiments demonstrate that VSSP can learn composition operations that cannot be learned by these approaches. The VSSP framework uses semantic parsing to define a compositional vector space model. Semantic parsers typically map sentences to logical semantic representations (Zelle and Mooney, 1996; Kate and Mooney, 2006), with many systems using CCG as the parsing formalism (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012). Although previous work has focused on logical semantics, it has demonstrated that semantic parsing is an elegant technique for specifying models of compositional semantics. In this paper, we show how to use semantic parsing to produce compositional models of vector space semantics. 6 Discussion and Future Work We present vector space semantic parsing (VSSP), a general framework for building compositional models of vector space semantics. Our framework is based on Combinatory Categorial Grammar (CCG), which defines a correspondence between syntactic categori"
W13-3201,P08-1028,0,0.198466,"ombination forces adverbs to have a consistent direction of effect on the size of the noun, which is incompatible with the desired enhancing and attenuating behavior. Examining VSSP’s learned parameters clearly demonstrates its ability to learn enhancing and 5 Related Work Several models for compositionality in vector spaces have been proposed in recent years. Much work has focused on evaluating composition operations for word pairs (Mitchell and Lapata, 2010; Widdows, 2008). Many operations have been proposed, including various combinations of addition, multiplication, and linear operations (Mitchell and Lapata, 2008), holographic reduced representations (Plate, 1991) and others (Kintsch, 2001). Other work has used regression to train models for adjectives in adjective-noun phrases (Baroni and Zamparelli, 2010; Guevara, 2010). All of this work is complementary to ours, as these composition operations can be used within VSSP by appropriately choosing the logical forms in the lexicon. A few comprehensive frameworks for composition have also been proposed. One approach is to take tensor outer products of word vectors, following syntactic structure (Clark and Pulman, 2007). However, this approach results in di"
W13-3201,D11-1129,0,0.292481,"ts. We have made some progress applying VSSP to SemEval Task 8, learning to extract relations between nominals (Hendrickx et al., 2010). Although our work thus far is preliminary, we have found that the generality of VSSP makes it easy to experiment with different models of composition. To swap between models, we simply modify the CCG lexicon templates – all of the remaining infrastructure is unchanged. Such preliminary results suggest the power of VSSP as a general framework for learning vector space models. typed objects lie in the same vector space (Clark et al., 2008; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). VSSP generalizes this work by allowing nonlinear composition operations and considering supervised parameter estimation. Several recent neural network models implicitly use a framework which assumes that composition factorizes according to a binarized constituency parse, and that words and phrases have uniform semantic representations (Socher et al., 2011; Socher et al., 2012). Notably, Hermann and Blunsom (2013) instantiate such a framework using CCG. VSSP generalizes these approaches, as they can be implemented within VSSP by choosing appropriate logical forms. Furthermore, our experiments"
W13-3201,2003.mtsummit-papers.42,0,0.0233609,"ific fashion. We present experiments using noun-verbnoun and adverb-adjective-noun phrases which demonstrate that VSSP can learn composition operations that RNN (Socher et al., 2011) and MV-RNN (Socher et al., 2012) cannot. 1 Introduction Vector space models represent the semantics of natural language using vectors and operations on vectors (Turney and Pantel, 2010). These models are most commonly used for individual words and short phrases, where vectors are created using distributional information from a corpus. Such models achieve impressive performance on standardized tests (Turney, 2006; Rapp, 2003), correlate 1 It is not necessary to assume that sentences are vectors. However, this assumption simplifies presentation and seems like a reasonable first step. CCG can be used similarly to explore alternative representations. 1 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 1–10, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics Input: Log. Form: „« semantic “red ball” → → Ared vball → evaluation → ◦ parsing ◦ ↑ ↑ Lexicon: red:= λx.Ared x Params.: Ared = ball:= vball vball = „ ◦◦ ◦◦ First, we demonstrate how CCG p"
W13-3201,D11-1014,0,0.0562935,"Missing"
W13-3201,W10-2805,0,0.0305079,"its ability to learn enhancing and 5 Related Work Several models for compositionality in vector spaces have been proposed in recent years. Much work has focused on evaluating composition operations for word pairs (Mitchell and Lapata, 2010; Widdows, 2008). Many operations have been proposed, including various combinations of addition, multiplication, and linear operations (Mitchell and Lapata, 2008), holographic reduced representations (Plate, 1991) and others (Kintsch, 2001). Other work has used regression to train models for adjectives in adjective-noun phrases (Baroni and Zamparelli, 2010; Guevara, 2010). All of this work is complementary to ours, as these composition operations can be used within VSSP by appropriately choosing the logical forms in the lexicon. A few comprehensive frameworks for composition have also been proposed. One approach is to take tensor outer products of word vectors, following syntactic structure (Clark and Pulman, 2007). However, this approach results in differently-shaped tensors for different grammatical structures. An improvement of this framework uses a categorial grammar to ensure that similarly7 work. An interesting aspect of VSSP is that it highlights cases"
W13-3201,D12-1110,0,0.680087,"entations for phrases out of their component words. Recent work in this area raises many important theoretical questions. For example, should all syntactic categories of words be represented as vectors, or are some categories, such as adjectives, different? Using distinct semantic representations for distinct syntactic categories has the advantage of representing the operational nature of modifier words, but the disadvantage of more complex parameter estimation (Baroni and Zamparelli, 2010). Also, does semantic composition factorize according to a constituency parse tree (Socher et al., 2011; Socher et al., 2012)? A binarized constituency parse cannot directly represent many intuitive intra-sentence dependencies, such as the dependence between a verb’s subject and its object. What is needed to resolve these questions is a comprehensive theoretical framework for compositional vector space models. In this paper, we observe that we already have such a framework: Combinatory Categorial Grammar (CCG) (Steedman, 1996). CCG provides a tight mapping between syntactic categories and semantic types. If we assume that nouns, sentences, and other basic syntactic categories are represented by vectors, this mapping"
W13-3201,S10-1006,0,0.0197124,"Missing"
W13-3201,P13-1088,0,0.0127902,"esults suggest the power of VSSP as a general framework for learning vector space models. typed objects lie in the same vector space (Clark et al., 2008; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). VSSP generalizes this work by allowing nonlinear composition operations and considering supervised parameter estimation. Several recent neural network models implicitly use a framework which assumes that composition factorizes according to a binarized constituency parse, and that words and phrases have uniform semantic representations (Socher et al., 2011; Socher et al., 2012). Notably, Hermann and Blunsom (2013) instantiate such a framework using CCG. VSSP generalizes these approaches, as they can be implemented within VSSP by choosing appropriate logical forms. Furthermore, our experiments demonstrate that VSSP can learn composition operations that cannot be learned by these approaches. The VSSP framework uses semantic parsing to define a compositional vector space model. Semantic parsers typically map sentences to logical semantic representations (Zelle and Mooney, 1996; Kate and Mooney, 2006), with many systems using CCG as the parsing formalism (Zettlemoyer and Collins, 2005; Kwiatkowski et al.,"
W13-3201,J06-3003,0,0.0259474,"in a task-specific fashion. We present experiments using noun-verbnoun and adverb-adjective-noun phrases which demonstrate that VSSP can learn composition operations that RNN (Socher et al., 2011) and MV-RNN (Socher et al., 2012) cannot. 1 Introduction Vector space models represent the semantics of natural language using vectors and operations on vectors (Turney and Pantel, 2010). These models are most commonly used for individual words and short phrases, where vectors are created using distributional information from a corpus. Such models achieve impressive performance on standardized tests (Turney, 2006; Rapp, 2003), correlate 1 It is not necessary to assume that sentences are vectors. However, this assumption simplifies presentation and seems like a reasonable first step. CCG can be used similarly to explore alternative representations. 1 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 1–10, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics Input: Log. Form: „« semantic “red ball” → → Ared vball → evaluation → ◦ parsing ◦ ↑ ↑ Lexicon: red:= λx.Ared x Params.: Ared = ball:= vball vball = „ ◦◦ ◦◦ First, we demonstr"
W13-3201,D10-1115,0,\N,Missing
W13-3201,P08-1000,0,\N,Missing
W13-3510,N09-1003,0,0.0161568,"beautiful). The two feature sets can work together to up-weight the most suitable NNs (as in beautiful), or help to drown out noise (as in the NNs for bad publicity in the Document VSM). The remaining three tests use group judgements of similarity: the “Concrete Similarity” set of 65 concrete word pairs (Rubenstein and Goodenough, 1965); and two variations on the WordSim353 test-set (Finkelstein et al., 2002), partitioned into subsets corresponding to strict attributional similarity (“Mixed Similarity”, 203 noun pairs), and broader topical “relatedness” (“Mixed Relatedness”, 252 noun pairs) (Agirre et al., 2009). Performance on these benchmarks is Spearman correlation between the aggregate human judgements and pairwise cosine distances of word vectors in a VSM. The results in Figure 1 show that the Dependency VSM substantially outperforms the Document VSM when predicting human judgements of strict attributional (categorial) similarity (“Similarity” as opposed to “Relatedness”) for concrete nouns. Conversely the Document VSM is compet4.1 Judgements of Word Similarity As an initial test of the informativeness of Document and Dependency features, we evaluate the representation of single words. Behaviora"
W13-3510,D07-1097,0,0.0130624,"and use more expressive dependency-based features in our typebased VSM. A comparison of vector-space representations was recently published (Blacoe and Lapata, 2012), in which the authors compared several methods of combining single words vectors to create phrase vectors. They found that the best performance for adjective-noun composition used point-wise multiplication and a model based on type-based word co-occurrence patterns. 3 Creating a Vector-Space To create the Dependency vectors, a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) was dependency parsed using the Malt parser (Hall et al., 2007). Dependency statistics were then collected for a predetermined list of target words and adjective-noun phrases, and for arbitrary adjective-noun phrases observed in the corpus. The list was composed of the 40 thousand most frequent single tokens in the American National Corpus (Ide and Suderman, 2006), and a small number of words and phrases used as stimuli in our brain imaging experiments. Additionally, we included any phrase found in the corpus whose maximal token span matched the PoS pattern J+N+, where J and N denote adjective and noun PoS tags respectively. For each unit (i.e., word or p"
W13-3510,W04-3221,0,0.0314299,"Missing"
W13-3510,J10-4006,0,0.0223793,"e a more type-based meaning (e.g. the noun judge might be modified by the adjective harsh, or be the subject of decide, as would related and substitutable words such as referee or conductor). Global patterns have been used in Latent Semantic Analysis (Landauer and Dumais, 1997) and LDA Topic models (Blei et al., 2003). Local patterns based on word co-occurrence in a fixed width window were used in Hyperspace Analogue to Language (Lund and Burgess, 1996). Subsequent models added increasing linguistic sophistication, up to full syntactic and dependency parses (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). In most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both. In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases. Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effective for semantic composition, it is often outperfo"
W13-3510,D10-1115,0,0.0326886,"fective for semantic composition, compared to a VSM built from Document and Dependency features alone. • We introduce a novel task: to predict the vector representation of a composed phrase from the brain activity of human subjects reading that phrase. • We explore two composition methods, addition and dilation, and find that while addition performs well on corpus-only tasks, dilation performs best on the brain activity task. • We build our VSMs, for both phrases and words, from a large syntactically parsed text corpus of 16 billion tokens. We also make the resulting VSM publicly available. 2 Baroni and Zamparelli (2010) extended the typical vector representation of words. Their model used matrices to represent adjectives, while nouns were represented with column vectors. The vectors for nouns and adjective-noun phrases were derived from local word co-occurrence statistics. The matrix to represent the adjective was estimated with partial least squares regression where the product of the learned adjective matrix and the observed noun vector should equal the observed adjective-noun vector. Socher et al. (2012) also extended word representations beyond simple vectors. Their model assigns each word a vector and a"
W13-3510,P09-1116,0,0.0352792,"omposition. Turney (2012) published an exploration of the impact of domain- and function-specific vector space models, analogous to the topic and type meanings encoded by our Document and Dependency models respectively. In Turney’s work, domain-specific information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA"
W13-3510,D12-1050,0,0.130686,"ts both negative target-feature associations, and those that were not observed or fell below the frequency cut-off. To combine Document and Dependency information, we concatenate vectors. In contrast to the composite model in (Griffiths et al., 2005), in this paper we explore the complementarity of semantics captured by topical information and syntactic/semantic types. We focus on learning VSMs (involving both words and phrases) for semantic composition, and use more expressive dependency-based features in our typebased VSM. A comparison of vector-space representations was recently published (Blacoe and Lapata, 2012), in which the authors compared several methods of combining single words vectors to create phrase vectors. They found that the best performance for adjective-noun composition used point-wise multiplication and a model based on type-based word co-occurrence patterns. 3 Creating a Vector-Space To create the Dependency vectors, a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) was dependency parsed using the Malt parser (Hall et al., 2007). Dependency statistics were then collected for a predetermined list of target words and adjective-noun phrases, and for arbitrary adjective-noun ph"
W13-3510,P98-2127,0,0.0221569,"erdict). Certain local patterns give a more type-based meaning (e.g. the noun judge might be modified by the adjective harsh, or be the subject of decide, as would related and substitutable words such as referee or conductor). Global patterns have been used in Latent Semantic Analysis (Landauer and Dumais, 1997) and LDA Topic models (Blei et al., 2003). Local patterns based on word co-occurrence in a fixed width window were used in Hyperspace Analogue to Language (Lund and Burgess, 1996). Subsequent models added increasing linguistic sophistication, up to full syntactic and dependency parses (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). In most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both. In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases. Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effecti"
W13-3510,P09-1072,1,0.797446,"mber of Dependency Dimensions Figure 3: The percentile rank of observed phrase vectors compared to vectors created using the addition composition function. and our predicted composed VSM for that phrase. We collected brain activity data using Magnetoencephalography (MEG). MEG is a brain imaging method with much higher temporal resolution (1 ms) than fMRI (∼2 sec). Since words are naturally read at a rate of about 2 per second, MEG is a better candidate for capturing the fast dynamics of semantic composition in the brain. Some previous work has explored adjective-noun composition in the brain (Chang et al., 2009), but used fMRI and corpus statistics based only on co-occurrence with 5 hand-selected verbs. (a) Addition composition function results. Our MEG data was collected while 9 participants viewed 38 phrases, each repeated 20 times (randomly interleaved). The stimulus nouns were chosen because previous research had shown them to be decodable from MEG recordings, and the adjectives were selected to modulate their most decodable semantic properties (e.g. edibility, manipulability) (Sudre et al., 2012). The 8 adjectives selected are (“big”, “small”, “ferocious”, “gentle”, “light”, “heavy”, “rotten”, “"
W13-3510,S12-1019,1,0.853656,"c information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA) topic models. For the type-based models, multiplication performed the best, followed 85 the number of target word/phrase types they contained and choosing the top 10 million. A series of three additional filtering steps selected target words/phrases, a"
W13-3510,C12-1118,1,0.819117,"c information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA) topic models. For the type-based models, multiplication performed the best, followed 85 the number of target word/phrase types they contained and choosing the top 10 million. A series of three additional filtering steps selected target words/phrases, a"
W13-3510,J07-2002,0,0.0918622,"Missing"
W13-3510,2003.mtsummit-papers.42,0,0.0710214,"Missing"
W13-3510,D12-1110,0,0.0660457,"parsed text corpus of 16 billion tokens. We also make the resulting VSM publicly available. 2 Baroni and Zamparelli (2010) extended the typical vector representation of words. Their model used matrices to represent adjectives, while nouns were represented with column vectors. The vectors for nouns and adjective-noun phrases were derived from local word co-occurrence statistics. The matrix to represent the adjective was estimated with partial least squares regression where the product of the learned adjective matrix and the observed noun vector should equal the observed adjective-noun vector. Socher et al. (2012) also extended word representations beyond simple vectors. Their model assigns each word a vector and a matrix, which are composed via an nonlinear function (e.g. tanh) to create phrase representations consisting of another vector/matrix pair. This process can proceed recursively, following a parse tree to produce a composite sentence meaning. Other general semantic composition frameworks have been suggested, e.g. (Sadrzadeh and Grefenstette, 2011) who focus on the operational nature of composition, rather than the representations that are supplied to the framework. Here we focus on creating w"
W13-3510,C98-2122,0,\N,Missing
W13-3510,ide-suderman-2004-american,0,\N,Missing
W17-2403,N16-1096,1,0.831296,"inferred relationships before pruning. The last row sums up all the categories. Experiments A complete set of experiments has been designed to test the performance of both approaches in the task of identifying equivalent concepts across languages. A baseline based on dictionary translations is used to put these results in context. 4.1 Portuguese G RAPH 1 its mean out-degree). An enhanced connectivity among entities is achieved considering a SVO corpus. A SVO consists of statistics about the presence of a triplet subject-verb-object in a text corpus usually crawled from the Web. In this study, Wijaya and Mitchell (2016) method to map verbs found in a corpus to relationships of a given structured KB has been used. It explores a SVO corpus looking for verbs which can be used to represent the different relation types r ∈ R of an ontology O. Given the returned set of representative verbs for a specific relation type r, pairs of literal strings s1 , s2 ∈ S which appear linked by means of one or more representative verbs in the SVO corpus can be considered as evidence of a r relationship. In practice, all the entities which can be referred to by s1 and s2 are connected by means of an edge of type r. As can be obse"
W17-2403,D15-1084,0,0.0677709,"Missing"
