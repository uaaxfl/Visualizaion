2008.eamt-1.16,bojar-prokopova-2006-czech,0,0.0283705,"Missing"
2008.eamt-1.16,J03-1002,0,0.011532,"ntent words). We show that the inter-annotator agreement on such t-layer links is SEnglishT set date yet get_back #Cor no table bargaining SCzechT stanovit datum žádný dosud n vrat st l vyjednávac  Fig. 2. Example of alignment on the t-layer (t-trees are simplified). 105 12th EAMT conference, 22-23 September 2008, Hamburg, Germany higher compared to that on word-layer. The main result achieved in the presented work is following: we show that the t-alignment produced by our feature-based structural t-aligner outperforms the t-alignment derived from the word-layer alignment provided by GIZA++ [3] in terms of their f-measure. This is probably caused by the fact that tectogrammatical representations of Czech and English sentences are much closer compared to the distance of their surface shapes. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not"
2008.eamt-1.16,P96-1018,0,0.0295322,"rom the word-layer alignment provided by GIZA++ [3] in terms of their f-measure. This is probably caused by the fact that tectogrammatical representations of Czech and English sentences are much closer compared to the distance of their surface shapes. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical represent"
2008.eamt-1.16,W01-1406,0,0.66403,"es. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical representation is based on the Functional Generative Description, developed by Petr Sgall and his collaborators since 1960s (see [8]). We use its implementation specified in Prague Dependency Treebank 2.0 as described in [1]. It consists of three interlinke"
2008.eamt-1.16,W04-3228,0,0.0244207,"aged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical representation is based on the Functional Generative Description, developed by Petr Sgall and his collaborators since 1960s (see [8]). We use its implementation specified in Prague Dependency Treebank 2.0 as described in [1]. It consists of three interlinked annotation layers: the morphological layer, the analytical layer (a-layer for short, describing the surface syntax) and the tectogrammatical layer (t-layer, describing the deep syntax – transition between syntax and semantics). On the t-layer, every sentence is represented as a roote"
2008.eamt-1.16,W08-0325,1,0.778306,"dinating conjunctions and prepositions are represented in the respective nodes in the form of their attributes. For example, there is no node representing auxiliary will at the t-layer, but its meaning is captured by attribute tense. Other attributes describe several cognitive, syntactic and morphological categories. The presence of an attribute in a node is determined by the node type. In this paper we will use the following attributes: – t-lemma – tectogrammatical lemma, – formeme – simplified description of the morphosyntactic form (how the tnode is expressed on the surface), introduced in [9] – deepord – describes the organization of words in a sentence according to their increasing communicative dynamism. It also determines the linear position of t-node in the tree. As for the alignment on t-layer we currently do not distinguish different arrow types. Every t-node is aligned with no, one or more t-nodes in the opposite 106 12th EAMT conference, 22-23 September 2008, Hamburg, Germany language. Sometimes it is necessary to align more Czech t-nodes with more English t-nodes. Then the arrows will lead from each such Czech t-node to all corresponding English t-nodes. This occurs infre"
2008.eamt-1.16,H05-1066,0,0.0871239,"Missing"
2008.eamt-1.16,A00-1031,0,0.00967893,"Missing"
2008.eamt-1.16,W02-1001,0,0.0175887,"Missing"
2008.eamt-1.16,2001.mtsummit-ebmt.4,0,\N,Missing
2008.eamt-1.16,J03-4003,0,\N,Missing
2020.cl-3.3,abeille-barrier-2004-enriching,0,0.0372292,"Missing"
2020.cl-3.3,C16-1327,0,0.0608024,"Missing"
2020.cl-3.3,L18-1719,0,0.0272427,"Missing"
2020.cl-3.3,apresjan-etal-2006-syntactically,0,0.0289852,"of lexical strings in the deep-syntactic representation of the sentence. They are substituted for lexemes at the next, lower level (surface-syntactic representation). Some features that are captured within the deep-syntactic representation in other approaches (e.g., topic-focus articulation in Functional Generative Description, see Section 3.3) are described at a separate, more abstract level, so-called semantic representation in MTT. The multileveled scheme proposed by the MTT is applied in a corpus for Russian (SynTagRus) and in a treebank for Spanish (AnCora-UPF Treebank): • In SynTagRus (Apresjan et al. 2006), Russian sentences were assigned a morphological annotation and a surface-syntactic dependency tree (these annotations are available at http://www.ruscorpora.ru/). In addition, a lexical semantic annotation and lexical-functional annotation were announced by Boguslavsky (2014). While lexical semantic annotation consisted in disambiguating ambiguous words that have different lemmas and/or different part-of-speech tags, the aim of the lexical functional annotation was to identify LFs and their arguments and values in the texts. Semantic annotation, as described by Apresjan et al. (2006), does n"
2020.cl-3.3,W04-2704,0,0.0734599,"tic layer is not available. 3.4 Proposition Bank and Closely Related Resources The Proposition Bank (PropBank) project aimed at “adding a layer of predicate– argument information, or semantic role labels, to the syntactic structures of the Penn Treebank” (Palmer, Gildea, and Kingsbury 2005, page 71). The project started by marking clause nuclei composed of verbal predicates and their arguments (predicate– argument structure); PropBank annotation pointed to constituents in the original Penn Treebank annotation (Kingsbury and Palmer 2002). Later, “modifiers of event variables” were added (e.g., Babko-Malaya et al. 2004), broadening the predicate–argument 621 Computational Linguistics Volume 46, Number 3 structures with adjuncts. The main features of PropBank annotation as presented by Palmer, Gildea, and Kingsbury (2005) are: • Structure: directed acyclic graph, typically consisting of multiple unconnected components. The nodes can be ordered following the surface word order. • Nodes are constituents of the Penn Treebank surface tree (but in PropBanks of other languages, the surface structure may be a dependency tree). Predicates are represented by terminal nodes, arguments are represented by their highest-s"
2020.cl-3.3,P98-1013,0,0.728722,"Missing"
2020.cl-3.3,C14-1133,0,0.048993,"Missing"
2020.cl-3.3,W13-2322,0,0.766328,"ˇcuk 1965) SynTagRus, AnCora-UPF Functional Generative Description (FGD; Sgall 1967) Associated lexical resource Used in NLP apps Languages MT hi, ur, bn, te ECD MT ru, en, es, fr PDT, PCEDT PDT-VALLEX MT cs, en PropBank (Kingsbury and Palmer 2002) PropBank + NomBank + PDTB PropBank lex. many en, ar, zh, fi, hi, ur, fa, pt, tr, de, fr FrameNet-based approaches such as SALSA (Erk and Pado 2004) e.g. TIGER Treebank (for SALSA) FrameNet Enju (Yakushiji et al. 2005) Enju Treebank DELPH-IN (Oepen and Lønning 2006) DeepBank Sequoia (Candito et al. 2014) Sequoia Abstract Meaning Representation (AMR; Banarescu et al. 2013) AMR Bank Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013) English Wiki, parallel fiction, etc. Enhanced Universal Dependencies (Schuster and Manning 2016) Universal Dependencies ERG en, de, fr, ko IE en, zh many en, de, es, ja fr PropBank lex. many en, zh, pt, ko, vi, es, fr, de en, de, fr relation extraction ar, bg, cs, en, et, fi, it, lt, lv, nl, pl ru, sk, sv, ta, uk We ended up with a selection of frameworks, listed in Table 1 (and then described in detail in Section 3) in roughly chronological order of their introduction. Associated corpora and major lexicograph"
2020.cl-3.3,I08-2099,0,0.0593094,"an treebanks. Figure 1 shows an example sentence from the Hindi treebank with four karakas. ´ 8 As Przepiorkowski (2016) says: “Probably all modern linguistic theories assume some form of the Argument-Adjunct dichotomy, which may be traced back to Lucien Tesni`ere’s 1959 distinction between actants and circumstants.” 614 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Table 2 The six karaka relations of the Paninian syntax. Note that there is no karaka labeled k6, at least not in modern annotation schemes referring to the Paninian grammar, such as Begum et al. (2008). Relation number 6 denotes possession but it does not have the karaka status and is labeled r6. karta karma karana sampradaana apaadaana adhikarana k1 k2 k3 k4 k5 k7 doer / agent / subject patient / object instrument recipient / beneficiary source location in space or time Figure 1 A Hindi sentence with the first four karaka relations. The relations prefixed lwg are chunk-internal, i.e., they are not part of the main structure. • Structure: rooted tree.9 The nodes can be ordered following the surface word order; the order is partial if the tree contains an empty node. • Nodes generally corres"
2020.cl-3.3,P19-2012,0,0.0183343,"n and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in a specific way in order to facilitate parsing, for instance, by reducing the length of long-distance dependencies (Bommasani 2019). 26 A dependency tree is projective if and only if an edge from node x to node y implies that x is an ancestor of all nodes located linearly between x and y. 645 Computational Linguistics Volume 46, Number 3 the node order in deep-syntactic structures is not paid much attention to, but the structures are presented as ordered and it is assumed that the linear order can be induced from the linear order of corresponding surface strings. In fact, most deep-syntactic nodes are somehow anchored in the totally ordered sequence of sentence tokens, implicitly or explicitly. However, depending on a cho"
2020.cl-3.3,burchardt-etal-2006-salsa,0,0.0613297,"Missing"
2020.cl-3.3,F12-2024,0,0.0706851,"reference, the same lexical unit (node) serves as argument of multiple predicates. • Semantically ambiguous predicates and their valency frames are disambiguated. • Coordination: Conjunction is treated as the head, that is, like a predicate, in EDS; empty nodes are used where an overt conjunction is not available. In DM, coordination is transformed to a left-to-right chain (see the “Mel’ˇcuk/Moscow style” in Section 4.4). 3.8 Sequoia French Treebank In the Sequoia corpus, the deep-syntactic representation (Candito et al. 2014) was built on top of the existing surface-syntactic representation (Candito and Seddah 2012), which followed the annotation scheme used in the French Treebank (Abeill´e and Barrier 2004). The surface-syntactic annotation in the Sequoia corpus, originally based on constituent trees, was converted into dependencies and used for specification of the dependency-oriented deep-syntactic representation. The main features of the deep-syntactic representation can be summarized as follows (Candito and Perrier 2016): • Structure: directed graph; may contain cycles (Figure 10). The nodes can be ordered following the surface word order. • Nodes of the graph correspond to content words. Function w"
2020.cl-3.3,J14-1002,0,0.0606323,"Missing"
2020.cl-3.3,W08-1301,0,0.265466,"Missing"
2020.cl-3.3,W18-4912,0,0.0399968,"Missing"
2020.cl-3.3,A97-1021,0,0.126295,", 2006) have a narrower (and more theoretical) focus, dealing with dependency and valency issues. They also include review chapters on individual theories dealing with these concepts. We can also list many published attempts at comparing various features of deepsyntactic frameworks; however, to our knowledge each of them handles only a very limited number of existing frameworks and/or narrow scope of features compared. Hajiˇcov´a and Kuˇcerov´a (2002) compare three frameworks, namely, PropBank (Kingsbury and Palmer 2002), the LCS Database containing Lexical Conceptual Structures introduced by Dorr (1997), and the (pilot) annotation of the Prague Dependency Treebank (Hajiˇc 1998); a possible mapping among these three representations is sketched, with a focus on mapping semantic roles. A mapping from PropBank argument labels to 20 thematic roles used in VerbNet (Kipper, Dang, and Palmer 2000) is designed by Rambow et al. (2003). Ellsworth et al. (2004) compare PropBank, SALSA (Erk and Pado 2004), and FrameNet (Johnson et al. 2002), with a focus on several selected phenomena such as metaphor, support constructions, words with multiple meaning aspects, phrases realizing more than one semantic rol"
2020.cl-3.3,W17-0406,1,0.838651,"Missing"
2020.cl-3.3,W19-7717,1,0.92148,"Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation of the Prague Czech-English Dependency Treebank) in relation to the task of broad-coverage semantic dependency parsing in SemEval 2015. Kuhlmann and Oepen (2016) follow up on the SemEval paper and describe graph properties of the three frameworks from the SemEval task, plus CCG Dependencies and Abstract Meaning Representation. Zhu, Li, and Chiticariu (2019) take two approaches, namely, the semantic role labeling approach of the PropBank project and Abstract Meaning Representation, as a point of departure to propose and discuss which issues are to be covered by the universal semantic representation (with the focus on temporal features and modality). To the best of our knowledge, so far the most comprehensive comparison of existing semantic representation accounts (Abend and Rappoport 2017) puts together a list of central semantic phenomena (predicates, argument structure, semantic roles, coreference, anaphora, temporal and spatial relations, disc"
2020.cl-3.3,duran-aluisio-2012-propbank,0,0.0654259,"Missing"
2020.cl-3.3,erk-pado-2004-powerful,0,0.207601,"Missing"
2020.cl-3.3,W08-1105,0,0.0473235,"by Menezes and Richardson (2003) makes use of Logical Forms similar to those introduced by Jensen (1993); the expected advantage of using Logical Forms for such a purpose is that “additional generality obtained by normalizing both the lexical and syntactic form of examples, they may then be matched and applied more broadly when new sentences are translated.” A Logical Form is an unordered graph representing the relations among the most meaningful elements of a sentence. Nodes are identified by the lemma of a content word and directed, labeled arcs indicate the underlying semantic relations. • Filippova and Strube (2008) present an unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees. A tree of an original sentence is pruned, that is, edges are removed in an optimized way so that retained edges form a valid tree and their total edge weight is maximized. Finally, a shortened sentence is synthetized from the pruned compression tree. The trees to be pruned result from a transformation of surface dependency trees. During this transformation, function words like determiners, auxiliary verbs, and negative particles are removed from"
2020.cl-3.3,J11-3004,0,0.0788417,"Missing"
2020.cl-3.3,L18-1013,0,0.0594348,"Missing"
2020.cl-3.3,W07-2413,0,0.0172022,"Missing"
2020.cl-3.3,D14-1163,0,0.0273627,"1 are not well characterized. The herpesvirus encodes a functional Entity1 that activates Entity2. Entity1 can functionally cooperate to synergetically activate Entity2. The Entity1 play key roles by activating Entity2. Figure 7 Enju Predicate–Argument Structures of the sentence A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. (Adapted from Oepen et al. 2015). Although the direction and the labels of the edges are deep-syntactic, all surface words including function words are included as graph nodes. resulting graph is connected and acyclic (Hashimoto et al. 2014). Due to the direct correspondence between nodes and words, the nodes are totally ordered. • Words are converted to their base forms and augmented with their POS tags. Every word in a sentence is treated as a predicate, an argument, or both. • A predicate has a certain category and governs zero or more arguments. • Predicate categories are relatively fine-grained and rather syntactically oriented, such as verb arg123 relation for a verb that takes two NP objects or a verb that takes one NP object and one sentential complement, conj arg12 relation for subordinating conjunctions that take two ar"
2020.cl-3.3,P07-1077,0,0.0308269,"majority25 of approaches the linear precedence of tokens in the original sentence is simply preserved (be the writing system oriented left-to-right as in Latin-based scripts, or right-to-left or top-down as in Arabic and Japanese, respectively). There is a rich body of literature dealing with word order in surface-syntactic formalisms, for example, from the following perspectives: 1. What empirical evidence on word-order phenomena can be found in syntactically annotated data, such as in the studies on non-projectivity26 structures occurring in dependency treebanks (Kuhlmann and Nivre 2006), (Havelka 2007), or a typological view of Alzetta et al. (2018). Word-order phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based mode"
2020.cl-3.3,W12-3602,0,0.409593,"than one semantic role, and non-local semantic roles. ˇ Zabokrtsk y´ (2005) points out several parallels between Meaning-Text theory ˇ (Zolkovskij and Mel‘ˇcuk 1965) and Functional Generative Description (Sgall 1967) in general, and more specifically between the deep-syntactic level of the former one and tectogrammatical level of the latter one. 1 There are also entirely different approaches to representing sentence meaning, such as vector space models; they are outside of the focus of our study. 606 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Ivanova et al. (2012) contrast seven annotation schemes for syntactic-semantic dependencies: CoNLL Syntactic Dependencies (Nivre et al. 2007), CoNLL PropBank Semantics (Surdeanu et al. 2008), Stanford Basic and Collapsed Dependencies (De Marneffe and Manning 2008), Enju Predicate–Argument Structures (Yakushiji et al. 2005), as well as Syntactic Derivation Trees and Minimal Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation"
2020.cl-3.3,kingsbury-palmer-2002-treebank,0,0.339136,"le future developments of the particular resources are outlined, usually by the authors of the resources themselves. ´ The volumes by Agel et al. (2003, 2006) have a narrower (and more theoretical) focus, dealing with dependency and valency issues. They also include review chapters on individual theories dealing with these concepts. We can also list many published attempts at comparing various features of deepsyntactic frameworks; however, to our knowledge each of them handles only a very limited number of existing frameworks and/or narrow scope of features compared. Hajiˇcov´a and Kuˇcerov´a (2002) compare three frameworks, namely, PropBank (Kingsbury and Palmer 2002), the LCS Database containing Lexical Conceptual Structures introduced by Dorr (1997), and the (pilot) annotation of the Prague Dependency Treebank (Hajiˇc 1998); a possible mapping among these three representations is sketched, with a focus on mapping semantic roles. A mapping from PropBank argument labels to 20 thematic roles used in VerbNet (Kipper, Dang, and Palmer 2000) is designed by Rambow et al. (2003). Ellsworth et al. (2004) compare PropBank, SALSA (Erk and Pado 2004), and FrameNet (Johnson et al. 2002), with a fo"
2020.cl-3.3,Q15-1040,0,0.0202955,"ar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based models are able to produce non-projective dependencies natively (McDonald and Satta 2007), while special techniques had to be developed to adapt transition-based models for non-projective parsing (Kuhlmann and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in a specific way in order to facilitate parsing, for instance, by reducing the length of long-distance dependencies (Bommasani 2019). 26 A dependency tree is projective if and only if an edge from node x to node y implies that x is an ancestor of all nodes located linearly between x and y. 645 Computational Linguistics Volume 46, Number 3 the"
2020.cl-3.3,P06-2066,0,0.0346833,"cy-oriented, then in a vast majority25 of approaches the linear precedence of tokens in the original sentence is simply preserved (be the writing system oriented left-to-right as in Latin-based scripts, or right-to-left or top-down as in Arabic and Japanese, respectively). There is a rich body of literature dealing with word order in surface-syntactic formalisms, for example, from the following perspectives: 1. What empirical evidence on word-order phenomena can be found in syntactically annotated data, such as in the studies on non-projectivity26 structures occurring in dependency treebanks (Kuhlmann and Nivre 2006), (Havelka 2007), or a typological view of Alzetta et al. (2018). Word-order phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some"
2020.cl-3.3,J16-4009,0,0.196255,"Semantics (Surdeanu et al. 2008), Stanford Basic and Collapsed Dependencies (De Marneffe and Manning 2008), Enju Predicate–Argument Structures (Yakushiji et al. 2005), as well as Syntactic Derivation Trees and Minimal Recursion Semantics from the DEPLH-IN project,2 and show a few similarities across the frameworks. Oepen et al. (2015) compare three approaches (DELPH-IN semantic annotation, Enju Predicate– Argument Structures, and the deep-syntactic annotation of the Prague Czech-English Dependency Treebank) in relation to the task of broad-coverage semantic dependency parsing in SemEval 2015. Kuhlmann and Oepen (2016) follow up on the SemEval paper and describe graph properties of the three frameworks from the SemEval task, plus CCG Dependencies and Abstract Meaning Representation. Zhu, Li, and Chiticariu (2019) take two approaches, namely, the semantic role labeling approach of the PropBank project and Abstract Meaning Representation, as a point of departure to propose and discuss which issues are to be covered by the universal semantic representation (with the focus on temporal features and modality). To the best of our knowledge, so far the most comprehensive comparison of existing semantic representati"
2020.cl-3.3,W19-3317,0,0.0319945,"nology, implicit terminals are empty leaf nodes. • A unit may participate in more than one relation; that is why the graph is not necessarily tree (Figure 14). • Relations are labeled with coarse-grained categories; the inventory contains 12 values such as P – Process, A – Participant, D – Adverbial, E – Elaborator, and N – Connector. 16 See https://nert-nlp.github.io/AMR-Bibliography/. In some languages this required language-specific modifications of the annotation scheme because of phenomena that have no analogy in English, viz. coreference of noun classifiers in Vietnamese as discussed by Linh and Nguyen (2019), or third-person clitic pronouns in Spanish as discussed by Migueles-Abraira, Agerri, and de Ilarraza (2018). 17 Unlike some other frameworks, in UCCA the term relation does not mean an edge. It is one of two types of concepts of which an utterance is constructed, the other being an entity. 633 Computational Linguistics Volume 46, Number 3 Figure 13 UCCA graph of the sentence John kicked his ball (Adapted from Abend and Rappoport 2013). The non-scene unit his ball is represented as a subgraph with one non-terminal and two terminal nodes; the C edge marks ball as the “center,” while E means “e"
2020.cl-3.3,L16-1147,0,0.0463032,"Missing"
2020.cl-3.3,W07-2216,0,0.0195375,"phenomena that are non-trivial to handle and require, for example, using traces in constituency formalisms or allowing non-projectivities in dependency formalisms, sometimes also lead to introducing finer-grained categories such as mildly context-sensitive grammar (Joshi, Shanker, and Weir 1990) or mildly non-projective ´ dependency grammar (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). 2. What the impact is of various word-order-related requirements on parsing, in terms of complexity and efficiency. For instance, some graph-based models are able to produce non-projective dependencies natively (McDonald and Satta 2007), while special techniques had to be developed to adapt transition-based models for non-projective parsing (Kuhlmann and Nivre 2010). More recently, extensions from parsing into trees to parsing into more general graphs (which is supposed to be beneficial for downstream semantic processing) have been studied, too (Kuhlmann and Jonsson 2015). When it comes to the order of nodes in deep-syntactic representations, the discussion in the literature seems to be much less structured. The dominating approach is that 25 There are exceptions, such as systems in which tokens are artificially permuted in"
2020.cl-3.3,W04-2705,0,0.406758,"complete the transaction by year-end. The ARGM-TMP edge between expects and by year-end seems disputable but it appears in the annotated data, so we include it, too. Traces and their antecedents are connected to chains identifying grammatical coreference within sentence boundaries. Note, however, that the textual coreference between it and the thrift holding company is not annotated. Pustejovsky et al. (2005) announced a project of merging the English PropBank with four other resources that focused on other parts considered as belonging to sentence meaning in English, namely with: • NomBank (Meyers et al. 2004), in which argument structure was assigned with eventive nouns occurring in PropBank (data of the Wall Street Journal Corpus of the Penn Treebank). First, “markable” noun instances were identified among common nouns, that is, eventive nouns that are accompanied by a PropBank-defined argument or adjunct. In each noun phrase with such a noun, the head was identified and its arguments and adjuncts were marked and assigned a semantic role label from the PropBank label set (ARG0 to ARG5 and different ARGM labels; see https://nlp.cs.nyu.edu/meyers/NomBank.html for detailed annotation instructions)."
2020.cl-3.3,L18-1486,0,0.0378796,"Missing"
2020.cl-3.3,W13-3724,0,0.0651479,"Missing"
2020.cl-3.3,W04-2703,0,0.241655,"Missing"
2020.cl-3.3,miltsakaki-etal-2004-penn,0,0.424125,"ach noun phrase with such a noun, the head was identified and its arguments and adjuncts were marked and assigned a semantic role label from the PropBank label set (ARG0 to ARG5 and different ARGM labels; see https://nlp.cs.nyu.edu/meyers/NomBank.html for detailed annotation instructions). • Penn Discourse Treebank (PDTB), in which the relations between propositions (i.e., meanings of individual clauses made up of a predicate–argument structure and related adjuncts) are annotated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and thei"
2020.cl-3.3,L16-1606,0,0.0297999,"Missing"
2020.cl-3.3,S14-2056,1,0.857807,"Missing"
2020.cl-3.3,2020.lrec-1.497,1,0.868476,"Missing"
2020.cl-3.3,L16-1377,0,0.0331719,"Missing"
2020.cl-3.3,K19-2001,0,0.212083,"Missing"
2020.cl-3.3,S15-2153,1,0.913576,"Missing"
2020.cl-3.3,W04-2327,0,0.138285,"ated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and their arguments, as captured in PropBank, were broadened with the argument structures for instances of common nouns (NomBank) and, finally, the isolated islands were connected with discourse relations (PDTB). By also having an explicit temporal and coreference annotation, the initially limited focus of PropBank was substantially extended, providing a more complex semantic annotation than available in the particular resources. A more general goal of the merging project w"
2020.cl-3.3,J98-2001,0,0.0194006,"ment structure and related adjuncts) are annotated. Propositions are marked as arguments with regard to discourse connectives, which are either explicit or implicit (Miltsakaki et al. 2004a, 2004b). • TimeBank (Pustejovsky et al. 2003), in which temporal features of propositions (expressed by temporal adjuncts, temporal prepositions and connectives, tensed verbs, etc.) and temporal relations between propositions are annotated. 623 Computational Linguistics • Volume 46, Number 3 Coreference Annotation created at the University of Essex, which contained texts from a subset of the Penn Treebank (Poesio and Vieira 1998) and the Gnome Corpus (Poesio 2004) annotated with coreference relations. Merging these resources meant that the clause nuclei composed of verbal predicates and their arguments, as captured in PropBank, were broadened with the argument structures for instances of common nouns (NomBank) and, finally, the isolated islands were connected with discourse relations (PDTB). By also having an explicit temporal and coreference annotation, the initially limited focus of PropBank was substantially extended, providing a more complex semantic annotation than available in the particular resources. A more ge"
2020.cl-3.3,P13-1051,1,0.926379,"athesis. For instance, a noun with the preposition by with a passive verb is assigned the final grammatical function of an object in the surface structure. When displayed in a linear sentence, all words of the sentence are parts of the surface-syntactic representation and are connected with final grammatical functions. Those nodes that correspond to content words in the sentence enter the deep-syntactic representation and are assigned also a canonical grammatical function; see Figure 10. 3.9 Abstract Meaning Representation Abstract Meaning Representations (AMRs) introduced by Banarescu et al. (2013) represent sentences as graphs in which non-leaf nodes stand for variables and only leaf nodes capture lexical content (i.e., only leaves are labeled with concepts). An example of such structure is depicted in Figure 12. Compared with most other approaches under our survey, the correspondence between AMR structures and surface-syntactic structures such as surface dependency trees is relatively limited, as the origins of AMR go back rather to a knowledge representation tradition. • Structure: A directed graph, typically acyclic, although cycles are not completely excluded (Kuhlmann and Oepen 20"
2020.cl-3.3,W19-3319,0,0.207272,"Missing"
2020.cl-3.3,prasad-etal-2008-penn,0,0.0567831,"dicate (see Section 4.2.2), coordinating conjunctions are often part of the sentence meaning representation (similarly to content words) and different accounts of coordination are documented in Section 4.4. 652 ˇ c´ıkov´a ˇ Zabokrtsk y, ´ Zeman, and Sevˇ Sentence Meaning Representations Across Languages Unlike subordination and coordination as intrasentential relations, relations between propositions that are separated into different sentences (inter-sentential relations) are omitted in most approaches or, if considered, they are annotated at a separate layer. • PDTB (Miltsakaki et al. 2004b; Prasad et al. 2008) is a project related to Penn Treebank and PropBank (cf. Section 3.4). It annotates the Wall Street Journal Section of the Penn Treebank with discourse relations. If an explicit discourse connective is found in the sentence or between two sentences, it is assigned a sense tag. If no discourse connective is present, a connective expression is added into the structure (being encoded as a lexical item or with a special label). Discourse relations are assigned between clauses in a sentence and between each successive pair of sentences within paragraphs. • In Prague Dependency Treebank (Section 3.3"
2020.cl-3.3,W04-1908,0,0.172299,"Missing"
2020.cl-3.3,W05-0302,0,0.0368945,"tence Meaning Representations Across Languages Figure 5 PropBank annotation over the constituents of the Penn Treebank for the sentence The thrift holding company said it expects to obtain regulatory approval and complete the transaction by year-end. The ARGM-TMP edge between expects and by year-end seems disputable but it appears in the annotated data, so we include it, too. Traces and their antecedents are connected to chains identifying grammatical coreference within sentence boundaries. Note, however, that the textual coreference between it and the thrift holding company is not annotated. Pustejovsky et al. (2005) announced a project of merging the English PropBank with four other resources that focused on other parts considered as belonging to sentence meaning in English, namely with: • NomBank (Meyers et al. 2004), in which argument structure was assigned with eventive nouns occurring in PropBank (data of the Wall Street Journal Corpus of the Penn Treebank). First, “markable” noun instances were identified among common nouns, that is, eventive nouns that are accompanied by a PropBank-defined argument or adjunct. In each noun phrase with such a noun, the head was identified and its arguments and adjun"
2020.cl-3.3,L16-1376,0,0.418969,"ructured sentence meaning representations (deep-syntactic representations),1 in order to demonstrate that there are basic principles shared by most (if not all) of them, on the one hand, and specific decisions, on the other. The shared principles, being considered the core elements of deep-syntactic representations, will be reformulated into a handful of humble suggestions for a discussion on a unifying approach to sentence meaning. This perspective justifies the inclusion of the Universal Dependencies project, though currently not containing a proper sentence meaning annotation (Schuster and Manning 2016), since the project sets trends in carrying out a unified annotation at the surface-syntactic level. 1.2 Existing Surveys These days, one can find comprehensive handbooks collecting a number of descriptions of various linguistic issues and language data resources. The recent Handbook of Linguistic Annotation (Ide and Pustejovsky 2017) provides an overview of annotation approaches applied in several tens of data resources capturing a wide range of language phenomena. Design decisions on the annotation schemes, evolution, and possible future developments of the particular resources are outlined,"
2020.cl-3.3,L16-1680,0,0.0564827,"Missing"
2020.cl-3.3,K17-3009,0,0.0599691,"Missing"
2020.cl-3.3,W08-2121,0,0.182274,"Missing"
2020.cl-3.3,taule-etal-2008-ancora,0,0.0776985,"Missing"
2020.cl-3.3,W11-0403,0,0.0431887,"Missing"
2020.cl-3.3,W08-0325,1,0.782768,"Missing"
2020.cl-3.3,W12-2511,0,0.0307876,"Missing"
2020.cl-3.3,K18-2001,1,0.890212,"Missing"
2020.cl-3.3,W17-6944,0,0.064815,"Missing"
2020.cl-3.3,W19-3320,0,0.0264804,"Missing"
2021.findings-emnlp.303,P15-1136,0,0.0504828,"Missing"
2021.findings-emnlp.303,N09-1037,0,0.136783,"Missing"
2021.findings-emnlp.303,guillou-etal-2014-parcor,0,0.0314253,"henomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for Engli"
2021.findings-emnlp.303,2020.lrec-1.641,0,0.0768802,"Missing"
2021.findings-emnlp.303,hendrickx-etal-2008-coreference,0,0.126363,"Missing"
2021.findings-emnlp.303,N19-1419,0,0.0170907,"ctic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, whi"
2021.findings-emnlp.303,W05-0303,0,0.12226,"cuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generative syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but i"
2021.findings-emnlp.303,D19-1588,0,0.0125746,"ount, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phe"
2021.findings-emnlp.303,J94-4002,0,0.242889,"the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limis"
2021.findings-emnlp.303,L18-1065,0,0.0239068,"syntax (Chomsky, 1993). However, with the advent of large-scale annotated corpora, coreference and syntax have somewhat diverged. The syntax-aware annotation of coreference demands for manual syntactic annotation, which is very expensive and not always feasible. As a result, coreference relations in most existing large-scale annotated resources are marked on raw texts, textual spans being defined as coreferring mentions, see, e.g. Hinrichs et al. (2005); Uryupina et al. (2020); Hendrickx et al. (2008); Désoyer et al. (2016); Landragin (2016); Bourgonje and Stede (2020); Guillou et al. (2014); Lapshinova-Koltunski et al. (2018); Žitkus and 3 Data selection Butkien˙e (2018); Toldova et al. (2014). Some of these datasets (Hendrickx et al., 2008; Toldova We draw our empirical observations about correet al., 2014) label syntactic heads of the mentions. spondences between manually annotated mention For some other datasets, syntactic annotation ex- spans and manually or automatically produced dependency trees from CorefUD 0.1 (Nedoluzhko ists but it was created independently of coreference et al., 2021), the biggest collection of coreference annotation. This is the case of GUM for English datasets converted to a harmonize"
2021.findings-emnlp.303,N18-2108,0,0.0459646,"Missing"
2021.findings-emnlp.303,2020.findings-emnlp.245,0,0.031795,"Missing"
2021.findings-emnlp.303,L16-1026,1,0.753884,"ructures correspond by design. In the latter case, To the best of our knowledge, there are only two coreference annotations made use either of conlarge-scale coreference-annotated datasets where syntax is closely linked to coreference relations. stituency trees – an English dataset from OntoNotes In AnCora-CO (Recasens and Martí, 2010), co- (Weischedel et al., 2011), and Spanish and Catalan referring mentions are nodes in constituency trees, datasets from the AnCora project (Recasens and and in the Prague Dependency corpora (Hajiˇc et al., Martí, 2010)), or of dependency trees – a Czech 2020; Nedoluzhko et al., 2016; Mikulová et al., dataset from the Prague Dependency Treebank (Hajiˇc et al., 2020), and English and Czech datasets 2017), coreference relations are annotated directly from the Prague Czech-English Dependency Treebetween syntactic heads in dependency trees and bank (Nedoluzhko et al., 2016). mention spans are implicitly defined as subtrees of the heads. The selection resulted in 9 datasets, for which Finkel and Manning (2009) deal with issues simi- we use their CorefUD labels: (1) English-GUM: lar to our work and have developed a model that per- Georgetown Multilayer Corpus (Zeldes, 2017) (th"
2021.findings-emnlp.303,P02-1014,0,0.105273,"utational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite p"
2021.findings-emnlp.303,2020.lrec-1.497,1,0.883163,"Missing"
2021.findings-emnlp.303,P13-1051,1,0.859969,"Missing"
2021.findings-emnlp.303,L18-1061,0,0.0383519,"Missing"
2021.findings-emnlp.303,2020.acl-main.622,0,0.0142617,"s naive approaches to pronoun resolution (Hobbs, 1978), Carter’s shallow processing approach (Carter, 1986) or fully symbolic Lappin and Leass’ algorithms for resolving third person pronouns and traversing syntactic trees (Lappin and Leass, 1994). Morpho-syntactic features were later largely used in statistical approaches (e.g., Ng and Cardie, 2002; Bergsma and Lin, 2006; Clark and Manning, 2015), especially for morphologically rich languages (e.g, Novák, 2017). With the advent of neural networks and contextual embeddings for coreference resolution (e.g., Lee et al., 2018; Joshi et al., 2019; Wu et al., 2020), the explicit treatment of morpho-syntax has practically vanished, even for the related task of mention detection. Such models are able to encode syntactic aspects implicitly, as shown by e.g., Hewitt and Manning (2019) and Limisiewicz et al. (2020). The idea of considering coreference and syntactic information together was quite popular in the last two decades of the 20th century, generally accepted in the Meaning-Text theory (Mel’ˇcuk, 1981) or in the Functional-Generative Description (Sgall et al., 1986). Coreference is also one of the main concepts underlying binding phenomena in generati"
2021.findings-emnlp.303,K17-3009,0,0.0297901,"is a collection of coreference datasets unidependency trees, would be beneficial in the long fied under a common scheme. Mention spans in term from various linguistic and computational per- all 9 datasets result from manual annotation. Despectives, especially if we hypothesize that: pendency trees available in the collection follow the Universal Dependencies (UD) scheme (Nivre 1. mentions are not just unconstrained subseet al., 2020) and result from manual annotation in quences of tokens, but mostly correspond to one case and from automatic parsing with UDPipe syntactically meaningful units, (Straka and Straková, 2017) in the 8 remaining 2. certain types of coreference relations are man- cases. In all cases, the dependency trees came into ifested primarily by syntactic means (such as existence independently of the coreference annotareflexive and relative constructions), tion. 3570 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3570–3576 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related work the parse tree. As for coreference resolution systems, some earlier algorithms took syntactic information into account, see e.g. Hobb’s naive approaches to prono"
bojar-etal-2008-czeng,steinberger-etal-2006-jrc,0,\N,Missing
bojar-etal-2008-czeng,P02-1040,0,\N,Missing
bojar-etal-2008-czeng,P07-2045,1,\N,Missing
bojar-etal-2008-czeng,W04-3250,0,\N,Missing
bojar-etal-2010-evaluating,steinberger-etal-2006-jrc,0,\N,Missing
bojar-etal-2010-evaluating,J03-1002,0,\N,Missing
bojar-etal-2012-joy,bojar-etal-2010-evaluating,1,\N,Missing
bojar-etal-2012-joy,C00-2163,0,\N,Missing
bojar-etal-2012-joy,W07-1709,0,\N,Missing
bojar-etal-2012-joy,P02-1040,0,\N,Missing
bojar-etal-2012-joy,H05-1066,0,\N,Missing
bojar-etal-2012-joy,P07-2045,1,\N,Missing
bojar-etal-2012-joy,W10-1703,0,\N,Missing
bojar-etal-2012-joy,W09-3939,1,\N,Missing
C10-3003,T75-2034,0,0.847112,"Missing"
C10-3003,C08-1085,0,0.128131,"ntation of sen tences during the annotation, a possibili ty to define, display and connect arbi trary groups of nodes, a clausebased compact depiction of trees, etc. For studying differences among parallel an notations, the tool offers a simultaneous depiction of parallel annotations of the data. 1 2 Tree Editor TrEd and the Annota tion Extension The primary format of PDT 2.0 is called PML. It is an abstract XMLbased format designed for annotation of linguistic corpora, and especially treebanks. Data in the PML format can be browsed and edited in TrEd, a fully customizable tree editor (Pajas and Štěpánek, 2008). TrEd is completely written in Perl and can be easily customized to a desired purpose by exten sions that are included into the system as mod ules. In this paper, we describe the main fea tures of an extension that has been implemented for our purposes. The data scheme used in PDT 2.0 has been enriched too, to support the annotation of the discourse relations. Introduction The Prague Dependency Treebank 2.0 (PDT 2.0; Hajič et al., 2006) is a manually annotated corpus of Czech. It belongs to the most complex end elaborate linguistically annotated treebanks in the world. The texts are annota"
C10-3003,P09-4009,0,0.318238,"Missing"
C10-3003,zikanova-etal-2010-typical,1,\N,Missing
C10-3003,mirovsky-etal-2010-annotation,1,\N,Missing
C10-3003,prasad-etal-2008-penn,0,\N,Missing
C12-1015,W03-1812,0,0.174103,"Missing"
C12-1015,W09-3004,0,0.0277464,"erlying phrase structure into account. Therefore, units that enter the coreference relations sometimes do not correspond to a continuous subtree of a tree. As a result, it is substantially non-trivial to search for phenomena that involve several such layers (e.g. “list all the verbs at which a given named entity or a pronoun corefering to it can appear as Arg0”). The PML format (see Section 2.1), on the other hand, results in unambiguous interconnection of the annotation layers. There are other projects aiming at standardisation of the solutions and conversion of old formats to new ones, cf. (Ide and Suderman, 2009). The solution used in the PDT is comparable to these efforts and standards (such as the LAF or TEI and its variants), but it has the added advantage of being supported by a complete suite of tools for annotation, search and processing mentioned earlier. Finally, not all treebanks are freely available. Various license restrictions (and usage fees) exist. PDT 2.5 is now being distributed under the standard Creative Commons license (3.0-BY-NC-SA) allowing free access and distribution of additions and modifications. 4 Multiword expressions Multiword expressions (MWEs) such as idioms, phrasemes, a"
C12-1015,J93-2004,0,0.0413198,"ical categories. Another advantage of the PML format is the availability of the framework surrounding it. The tools provided include the tree editor TrEd (Pajas and Štěpánek, 2008), the query language and engine PML-TQ (Pajas and Štěpánek, 2009, see also Figure 2) and a highly modular NLP system Treex (Popel and Žabokrtský, 2010). 3 Related work During the past decade, plenty of treebanks have been published. New treebanks keep appearing at least bi-monthly.3 There are some features, though, that set PDT 2.5 apart from most of them. The most popular treebank of all times is the Penn Treebank (Marcus et al., 1993). It has been since extended by several projects: PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), and a few more. 2 There are several exceptions of a technical nature. For instance, counterparts of coordinating conjunctions are included in the tree because they are used for the representation of coordinating constructions. 3 LDC has published 5 new treebanks so far in 2012: http://www.ldc.upenn.edu/Catalog/ByYear.jsp 233 The pitfall of this process can be demonstrated on the Chinese Treebank (Xue et"
C12-1015,J87-3006,0,0.0470924,"Missing"
C12-1015,W04-2705,0,0.0258332,"ools provided include the tree editor TrEd (Pajas and Štěpánek, 2008), the query language and engine PML-TQ (Pajas and Štěpánek, 2009, see also Figure 2) and a highly modular NLP system Treex (Popel and Žabokrtský, 2010). 3 Related work During the past decade, plenty of treebanks have been published. New treebanks keep appearing at least bi-monthly.3 There are some features, though, that set PDT 2.5 apart from most of them. The most popular treebank of all times is the Penn Treebank (Marcus et al., 1993). It has been since extended by several projects: PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), and a few more. 2 There are several exceptions of a technical nature. For instance, counterparts of coordinating conjunctions are included in the tree because they are used for the representation of coordinating constructions. 3 LDC has published 5 new treebanks so far in 2012: http://www.ldc.upenn.edu/Catalog/ByYear.jsp 233 The pitfall of this process can be demonstrated on the Chinese Treebank (Xue et al., 2010), whose development followed the Penn Treebank pattern. The additional layers of annotation follow t"
C12-1015,C08-1085,1,0.866217,"eceded the annotation stage. 2.1 Prague Markup Language PDT uses the PML format (Pajas and Štěpánek, 2006) based on XML. Each token and node has been assigned a unique identifier; any layer built atop of another uses the identifiers from the lower layer as reference targets, effectively creating inter-layer links (of various types). Each node can be assigned an attribute-value structure, an attribute in short, that represents various grammatical categories. Another advantage of the PML format is the availability of the framework surrounding it. The tools provided include the tree editor TrEd (Pajas and Štěpánek, 2008), the query language and engine PML-TQ (Pajas and Štěpánek, 2009, see also Figure 2) and a highly modular NLP system Treex (Popel and Žabokrtský, 2010). 3 Related work During the past decade, plenty of treebanks have been published. New treebanks keep appearing at least bi-monthly.3 There are some features, though, that set PDT 2.5 apart from most of them. The most popular treebank of all times is the Penn Treebank (Marcus et al., 1993). It has been since extended by several projects: PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), BBN Pronoun Coreference and Entity Type Corpus"
C12-1015,P09-4009,1,0.857799,"the PML format (Pajas and Štěpánek, 2006) based on XML. Each token and node has been assigned a unique identifier; any layer built atop of another uses the identifiers from the lower layer as reference targets, effectively creating inter-layer links (of various types). Each node can be assigned an attribute-value structure, an attribute in short, that represents various grammatical categories. Another advantage of the PML format is the availability of the framework surrounding it. The tools provided include the tree editor TrEd (Pajas and Štěpánek, 2008), the query language and engine PML-TQ (Pajas and Štěpánek, 2009, see also Figure 2) and a highly modular NLP system Treex (Popel and Žabokrtský, 2010). 3 Related work During the past decade, plenty of treebanks have been published. New treebanks keep appearing at least bi-monthly.3 There are some features, though, that set PDT 2.5 apart from most of them. The most popular treebank of all times is the Penn Treebank (Marcus et al., 1993). It has been since extended by several projects: PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), and a few more. 2 There are se"
C12-1015,J05-1004,0,0.0706786,"framework surrounding it. The tools provided include the tree editor TrEd (Pajas and Štěpánek, 2008), the query language and engine PML-TQ (Pajas and Štěpánek, 2009, see also Figure 2) and a highly modular NLP system Treex (Popel and Žabokrtský, 2010). 3 Related work During the past decade, plenty of treebanks have been published. New treebanks keep appearing at least bi-monthly.3 There are some features, though, that set PDT 2.5 apart from most of them. The most popular treebank of all times is the Penn Treebank (Marcus et al., 1993). It has been since extended by several projects: PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), and a few more. 2 There are several exceptions of a technical nature. For instance, counterparts of coordinating conjunctions are included in the tree because they are used for the representation of coordinating constructions. 3 LDC has published 5 new treebanks so far in 2012: http://www.ldc.upenn.edu/Catalog/ByYear.jsp 233 The pitfall of this process can be demonstrated on the Chinese Treebank (Xue et al., 2010), whose development followed the Penn Treebank pattern. The additiona"
C14-1003,D08-1092,0,0.0238042,"n of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.” (Quirk et al., 1985, p.314) 2 Quirk (1985) uses these terms instead of terms masculine and feminine related to grammatical gender. 15 bilingually-informed parsing by Haulrich (2012), in which English trees are used to enrich the feature set for a Danish parser and vice-versa. Rosa et al. (2012) explored the same approach on the CzechEnglish language pair. Moreover, they adapted this technique to pars"
C14-1003,N10-1015,0,0.0132625,"CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.” (Quirk et al., 1985, p.314) 2 Quirk (1985) uses these terms instead of terms masculine and feminine related to grammatical gender. 15 bilingually-informed parsing by Haulrich (2012), in which English trees are used to enrich the feature set for a Danish parser and vice-versa. Rosa et al. (2012) explored the same approach on the CzechEnglish lan"
C14-1003,E09-1018,0,0.0665801,"Missing"
C14-1003,P11-1061,0,0.0155451,"e the paper in Section 7. 2 Related work The task of coreference resolution has been studied for a few decades, with supervised systems dominating the field. The most popular approaches have been thoroughly summarized by Ng (2010). The system for English CR we use has been built for automatic coreference annotation in the CzechEnglish parallel treebank CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pr"
C14-1003,W08-0509,0,0.0139477,"and Zabokrtsk´ y, 2010). 3 4 http://hdl.handle.net/11858/00-097C-0000-0015-8DAF-4 Its antecedent is imposed by the grammar of the language, e.g. coreference of relative pronouns. 16 Treex is a multi-purpose open-source framework for NLP applications development, which integrates a wide range of modules, such as tools for sentence splitting, tokenization, morphological analysis, part-of-speech tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, among others. Moreover, we performed an unsupervised word alignment on the complete PCEDT using the MGIZA++ tool (Gao and Vogel, 2008), which is a multi-threaded version of the popular GIZA++ (Och and Ney, 2000) that supports applying a saved model on a new sentence pair. We used a model trained on CzEng 1.0, which is about 300 times bigger in terms of the number of sentence pairs. The resulting alignment of the intersection and grow-diag-final-and types was subsequently projected onto the tectogrammatical layer. Furthermore, a simple heuristic was applied to find the English counterparts for reconstructed Czech personal pronouns. We denote this alignment as the original in the following. 4 Supervised alignment The alignment"
C14-1003,guillou-etal-2014-parcor,0,0.108982,"Missing"
C14-1003,hajic-etal-2012-announcing,1,0.896241,"Missing"
C14-1003,A00-1020,0,0.145407,"Missing"
C14-1003,N06-2015,0,0.060739,"erformed both a monolingual CR and a cross-lingual projection system. 1 Introduction Coreference resolution (CR) is a well-established task in the field of Natural Language Processing (NLP). The majority of papers published so far has focused on the monolingual CR, mostly experimenting on the English data. An important step towards multilingual CR was the CoNLL-2012 Shared Task in Modeling Multilingual Unrestricted Coreference in OntoNotes, where the participants were asked to build a CR system that could be applied on three typologically different languages contained in the OntoNotes corpus (Hovy et al., 2006): English, Chinese, and Arabic. Same just as in other NLP tasks such as part-of-speech tagging or parsing, recent years have witnessed a rising interest in cross-lingual projection techniques, mostly aiming at under-resourced languages. However, little attention is paid to leveraging cross-lingual information for CR in two resource-rich languages. This is probably due to lack of bilingual resources annotated with coreference since such techniques would require rich linguistic annotation on both sides of the bitext. Moreover, to solve this issue using a supervised learner, one needs the gold st"
C14-1003,P12-1073,0,0.0329748,"of coreference resolution has been studied for a few decades, with supervised systems dominating the field. The most popular approaches have been thoroughly summarized by Ng (2010). The system for English CR we use has been built for automatic coreference annotation in the CzechEnglish parallel treebank CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.” (Quirk et al., 1985, p.314) 2 Quirk"
C14-1003,P10-1142,0,0.139404,": After introducing related work in Section 2 and describing the data used in experiments in Section 3, we present the design of a supervised approach to improve English pronoun alignment in Section 4. Section 5 describes the cross-lingual CR system and the experiments conducted with it. Finally, we discuss the main observations made in the experiments in Section 6 and conclude the paper in Section 7. 2 Related work The task of coreference resolution has been studied for a few decades, with supervised systems dominating the field. The most popular approaches have been thoroughly summarized by Ng (2010). The system for English CR we use has been built for automatic coreference annotation in the CzechEnglish parallel treebank CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both"
C14-1003,W09-3939,1,0.900651,"Missing"
C14-1003,W13-3307,1,0.883261,"Missing"
C14-1003,P00-1056,0,0.306537,"F-4 Its antecedent is imposed by the grammar of the language, e.g. coreference of relative pronouns. 16 Treex is a multi-purpose open-source framework for NLP applications development, which integrates a wide range of modules, such as tools for sentence splitting, tokenization, morphological analysis, part-of-speech tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, among others. Moreover, we performed an unsupervised word alignment on the complete PCEDT using the MGIZA++ tool (Gao and Vogel, 2008), which is a multi-threaded version of the popular GIZA++ (Och and Ney, 2000) that supports applying a saved model on a new sentence pair. We used a model trained on CzEng 1.0, which is about 300 times bigger in terms of the number of sentence pairs. The resulting alignment of the intersection and grow-diag-final-and types was subsequently projected onto the tectogrammatical layer. Furthermore, a simple heuristic was applied to find the English counterparts for reconstructed Czech personal pronouns. We denote this alignment as the original in the following. 4 Supervised alignment The alignment described in the previous section is sufficiently accurate for content words"
C14-1003,postolache-etal-2006-transferring,0,0.699569,"Missing"
C14-1003,D09-1101,0,0.0437377,"respectively. Finally, several variants of the CR system are evaluated and compared in Section 5.5. 5.1 Coreference model Our resolver employs a supervised model denoted as mention ranker by Ng (2010). Its advantage lies in judging all antecedent candidates simultaneously, and then picking the candidate with the highest score as the predicted antecedent. However, it is unable to exploit features that describe already formed clusters of mentions belonging to the same entity. A typical issue related to ranking models is how to deal with non-anaphoric mentions. We use the approach introduced by Rahman and Ng (2009) – adding a special candidate that indicates no anaphor. Since this work focuses only on the so-called pronoun resolution, all the anaphor candidates are English 3rd person central pronouns, i.e. personal, possessive and reflexive pronouns. For every anaphor, we collect in the set of its antecedent candidates all semantic nouns6 from the previous sentence and the part of the current sentence prior to the anaphor. CR can be treated as a ranking task, so we represent it in the same way as we handled alignment in Section 3.1 – as a discriminative log-linear model trained in the csoaa-ldf strategy"
C14-1003,N12-1090,0,0.0868353,"Missing"
C14-1003,W12-4205,0,0.147754,"Missing"
C14-1003,W04-3207,0,0.020446,"It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.” (Quirk et al., 1985, p.314) 2 Quirk (1985) uses these terms instead of terms masculine and feminine related to grammatical gender. 15 bilingually-informed parsing by Haulrich (2012), in which English trees are used to enrich the feature set for a Danish parser and vice-versa. Rosa et al. (2012) explored the same approach on the CzechEnglish language pair. Moreover, they adap"
C14-1003,D10-1030,0,0.025588,"systems dominating the field. The most popular approaches have been thoroughly summarized by Ng (2010). The system for English CR we use has been built for automatic coreference annotation in the CzechEnglish parallel treebank CzEng 1.0 (Bojar et al., 2012). It is an implementation of the so-called mention ranking model, first introduced by Denis and Baldridge (2007). Parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging (Das and Petrov, 2011), named entity recognition (Kim et al., 2012), name tagging (Li et al., 2012), and semantic role labeling (Zhuang and Zong, 2010). Undoubtedly, this approach is most popular with parsing. Joint parsing of both the source and the target text along with searching for the best alignment between the trees has been approached in a more (Burkett et al., 2010) or less (Smith and Smith, 2004; Burkett and Klein, 2008) integrated approach. However, much closer to our work is the research on 1 “Nouns are classified semantically according to their coreferential relations with personal, reflexive and wh-pronouns.” (Quirk et al., 1985, p.314) 2 Quirk (1985) uses these terms instead of terms masculine and feminine related to grammatic"
C14-1003,D08-1067,0,\N,Missing
C14-1003,bojar-etal-2012-joy,1,\N,Missing
D12-1028,D10-1117,0,0.24525,"Missing"
D12-1028,A00-1031,0,0.170556,"arts of the treebanks (the files test.conll) for the dependency tree induction. As a source of the part-of-speech tags, we use the fine-grained gold PoS tags, which are in the fifth column in the CoNLL format. For obtaining reducibility scores, we used the W2C corpus9 of Wikipedia articles, which was ˇ downloaded by Majliˇs and Zabokrtsk´ y (2012). Their statistics across languages are shown in Table 4. To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective CoNLL training data (the files train.conll). The quality of such tagging is not very high, since we do not use any lexicons11 or pretrained models. However, it is sufficient for obtaining good reducibility scores. 8 We do not have appropriate Chinese segmenter that would segment Chinese texts in the same way as in CoNLL. 9 http://ufal.mff.cuni.cz/˜majlis/w2c/ 10 The segmentation to sentences and tokenization was perˇ formed using the TectoMT framework (Popel and Zabokrtsk´ y, 2010). 11 Using lexicons or another pretrained models for tagging means using oth"
D12-1028,W06-2920,0,0.178839,"o the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manual"
D12-1028,C96-1058,0,0.324093,"r location in the corpus. These counts are collected over the whole corpus with the collection-rate 0.01.7 When the samling is finished, we build final dependency trees based on the edge counts obtained during the sampling. We employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them; the weights of edges for computing MST correspond to the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking"
D12-1028,P07-1077,0,0.0248163,"e employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them; the weights of edges for computing MST correspond to the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situ"
D12-1028,N09-1012,0,0.237092,"Missing"
D12-1028,P04-1061,0,0.905252,"briefly outlines the state of the art in unsupervised dependency parsing. Our measure of reducibility based on a large monolingual corpus is presented in Section 3. Section 4 shows our models which serve for generating probability estimates for edge sampling described in Section 5. Experimental parsing results for languages included in CoNLL shared task treebanks are summarized in Section 6. Section 7 concludes this article. 2 Related Work The most popular approach in unsupervised dependency parsing of the recent years is to employ Dependency Model with Valence (DMV), which was introduced by Klein and Manning (2004). The inference algorithm was further improved by Smith (2007) and Cohen et al. (2008). Headden, Johnson and McClosky (2009) introduced the Extended Valence Grammar (EVG) and added lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning larger dependency fragments. Unfortunately, many of these works show results only for English.1 However, the main feature of unsupervised methods should be their applicability across a wide range of languages. Such experiments were done by Spitkovsky (2011b; 2011c), where the parsing algorithm was evaluated on"
D12-1028,majlis-zabokrtsky-2012-language,1,0.869845,"Missing"
D12-1028,W11-3901,1,0.901282,"Missing"
D12-1028,D11-1006,0,0.0748374,"Missing"
D12-1028,D11-1118,0,0.375042,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
D12-1028,D11-1117,0,0.337233,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
D12-1028,W11-0303,0,0.394274,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
hajic-etal-2012-announcing,W04-2705,0,\N,Missing
hajic-etal-2012-announcing,C00-2163,0,\N,Missing
hajic-etal-2012-announcing,P07-1031,0,\N,Missing
I13-1142,E12-3001,0,0.0639359,"Czech syntax-based MT framework. Several works have previously focused on translating pronouns. The linguistic study of Morin (2009) investigated the translation of pronouns, proper names and kinship terms from Indonesian into English. Onderkov´a (2010) has conducted a corpus-based research on possesive pronouns in Czech and English, focusing especially on their use with parts of the human body. From the perspective of MT, translating personal pronouns from English to morphologically richer languages, such as French (Le Nagard and Koehn, 2010), German (Hardmeier and Federico, 2010) and Czech (Guillou, 2012) has recently aroused higher interest. In these languages, one usually has to ensure agreement in gender and number between the pronoun and its direct antecedent, which requires a coreference resolver to be involved. In this work, we make use of the English-toCzech translation implemented within the Tecˇ toMT system (Zabokrtsk´ y et al., 2008). In contrast to the phrase-based approach (Koehn et al., 2003), TectoMT performs a tree-to-tree machine translation. An input English sentence is first analyzed into its deep-syntactic representation, which is subsequently transferred into Czech. The pip"
I13-1142,W08-0325,1,0.874151,"especially on their use with parts of the human body. From the perspective of MT, translating personal pronouns from English to morphologically richer languages, such as French (Le Nagard and Koehn, 2010), German (Hardmeier and Federico, 2010) and Czech (Guillou, 2012) has recently aroused higher interest. In these languages, one usually has to ensure agreement in gender and number between the pronoun and its direct antecedent, which requires a coreference resolver to be involved. In this work, we make use of the English-toCzech translation implemented within the Tecˇ toMT system (Zabokrtsk´ y et al., 2008). In contrast to the phrase-based approach (Koehn et al., 2003), TectoMT performs a tree-to-tree machine translation. An input English sentence is first analyzed into its deep-syntactic representation, which is subsequently transferred into Czech. The pipeline ends with generating a surface form of the Czech translation from its deep representation. The deep syntactic representation of a sentence in TectoMT follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986). It is a dependency tree whose nodes correspond to content words. Personal pronouns missing on the surface are re"
I13-1142,2010.iwslt-papers.10,0,0.183124,"he models are integrated into an English-Czech syntax-based MT framework. Several works have previously focused on translating pronouns. The linguistic study of Morin (2009) investigated the translation of pronouns, proper names and kinship terms from Indonesian into English. Onderkov´a (2010) has conducted a corpus-based research on possesive pronouns in Czech and English, focusing especially on their use with parts of the human body. From the perspective of MT, translating personal pronouns from English to morphologically richer languages, such as French (Le Nagard and Koehn, 2010), German (Hardmeier and Federico, 2010) and Czech (Guillou, 2012) has recently aroused higher interest. In these languages, one usually has to ensure agreement in gender and number between the pronoun and its direct antecedent, which requires a coreference resolver to be involved. In this work, we make use of the English-toCzech translation implemented within the Tecˇ toMT system (Zabokrtsk´ y et al., 2008). In contrast to the phrase-based approach (Koehn et al., 2003), TectoMT performs a tree-to-tree machine translation. An input English sentence is first analyzed into its deep-syntactic representation, which is subsequently trans"
I13-1142,N03-1017,0,0.00324045,"the perspective of MT, translating personal pronouns from English to morphologically richer languages, such as French (Le Nagard and Koehn, 2010), German (Hardmeier and Federico, 2010) and Czech (Guillou, 2012) has recently aroused higher interest. In these languages, one usually has to ensure agreement in gender and number between the pronoun and its direct antecedent, which requires a coreference resolver to be involved. In this work, we make use of the English-toCzech translation implemented within the Tecˇ toMT system (Zabokrtsk´ y et al., 2008). In contrast to the phrase-based approach (Koehn et al., 2003), TectoMT performs a tree-to-tree machine translation. An input English sentence is first analyzed into its deep-syntactic representation, which is subsequently transferred into Czech. The pipeline ends with generating a surface form of the Czech translation from its deep representation. The deep syntactic representation of a sentence in TectoMT follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986). It is a dependency tree whose nodes correspond to content words. Personal pronouns missing on the surface are reconstructed in special nodes. All nodes are assigned semantic r"
I13-1142,W10-1737,0,0.102829,"Missing"
I13-1142,2008.eamt-1.16,1,0.905989,"Missing"
L16-1015,J92-4003,0,0.304047,"we say that something is an adverb in language X, we should be able to support such a claim by some measurable evidence rather than just by saying that it becomes an adverb if translated to English. 2. Related Work There is a body of literature about POS tagging of under-resourced languages. Most approaches rely on the existence of some form of parallel (or comparable) data. We will discuss only those approaches that attempt at using the same tagset across languages, and not those aiming at unsupervised induction, such as the well-known Brown clusters induced in a fully unsupervised fashion (Brown et al., 1992). An overview of such truly unsupervised approaches can be found in (Christodouloupoulos et al., 2010).2 • For some multilingual NLP tasks, such as unsupervised dependency parsing (or parser transfer), it might be more important to preprocess all languages under study as similarly as possible (including POS tagging), rather than to maximize accuracy with respect to highly different gold-standard data in individual languages. 1 However, we do not say that our method is completely language-independent. For instance, we rely on the existence of a meaningful tokenization in the target language. 2"
L16-1015,D10-1056,0,0.0699114,"Missing"
L16-1015,W02-2006,0,0.0990035,"Missing"
L16-1015,P11-1061,0,0.321767,"because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different method named delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained on annotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency, neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certain extent, serve as predictors of the part of speech, represented by the universal POS tag (Das and Petrov, 2011). Keywords: delexicalized tagging, HamdleDT 2.0, features expansion, classifier 1. Introduction languages); the model is independent of individual word forms. In delexicalized parsing, word form sequences are substituted by sequences of POS tags, which—of course— is not extendable to tagging. Instead, we substitute word forms by vectors of numerical features that can be computed using only unannotated monolingual texts. The background intuition is that the individual POS categories will tend to manifest similar statistical properties across languages (e.g., prepositions tend to be short, relat"
L16-1015,P13-2112,0,0.0359974,"Missing"
L16-1015,I05-1075,0,0.080472,"Missing"
L16-1015,majlis-zabokrtsky-2012-language,1,0.885772,"Missing"
L16-1015,L16-1262,1,0.852652,"Missing"
L16-1015,petrov-etal-2012-universal,0,0.0316314,"sulting classifier is used to assign POS tags to all words’ feature vectors in the target languages, X NN = f (y) y∈N ext(w) X − y∈N ext(w) 4. we evaluate our approach on the target languages for which there are labeled data available, and assume that reasonably similar accuracies are reached also for the other target languages. f (y) f (y) log NN NN 5. substituting word entropy X SN = f (y) y∈Subst(w) 3.2. Tagset X A prerequisite to our approach is a common tagset for both the source and the target languages. We use the same tagset as (Das and Petrov, 2011), the Google Universal POS tag set (Petrov et al., 2012). With just 12 tags it is fairly y∈Subst(w) 3 97 − f (y) f (y) log SN SN http://universaldependencies.org/ 3.4. 6. is number – binary value is number(w), In our approach, we need two types of data resources: 7. is punctuation – binary value is punctuation(w), • raw monolingual texts for both source and target languages; this data is used for extracting feature vectors for words in individual languages; we use W2C, a web-based corpus of 120 languages (Majliˇs and ˇ Zabokrtsk´ y, 2012), 8. relative frequency after number log |i : ci = w ∧ is number(ci−1 )| f (w) • POS-tagged data for source lang"
L16-1015,N01-1026,0,0.208375,"Missing"
L16-1015,I08-3008,1,0.770159,"n the existence of a meaningful tokenization in the target language. 2 There is a certain terminological confusion in this area: sometimes the word “unsupervised” is used also for situations in which there are no hand-tagged data available for the target language, but some manual annotation of the source language exists and is projected across parallel data like in (Das and Petrov, 2011). We prefer to avoid the term “unsupervised” when manual annotation is used in any language. We propose “delexicalized tagging”, a new method for under-resourced languages. In analogy to delexicalized parsing (Zeman and Resnik, 2008), we transfer a tagging model from a resource-rich language (or a set of 96 (Yarowsky and Ngai, 2001) project POS tags from English to French and Chinese via both automatic and gold alignment, and report substantial growth of accuracy after using de-noising postprocessing. (Fossum and Abney, 2005) extend this approach by projecting multiple source languages onto a target language. (Das and Petrov, 2011) use graph-based label propagation for cross-lingual knowledge transfer, and estimate emission distributions in the target language using a loglinear model. (Duong et al., 2013) choose only auto"
L16-1015,P15-2044,0,\N,Missing
L16-1208,baranes-sagot-2014-language,0,0.122369,"n the alternations by the context. The DeriNet network, which is – as a part of our approach – described in Section 4., has been developed as a publicly accessible, large-coverage resource of Czech derivational data and is, to the best of our knowledge, one of only few specialized resources of derivational data even in a broader context of different langauges. Most of the resources are very recent such as DerivBase for German 1307 ˇ (Zeller et al., 2013), DerivBase.Hr for Croatian (Snajder, 2014), D´emonette network for French (Hathout and Namer, 2014), or the language-independent approach by Baranes and Sagot (2014). 3. Inflectional resource: MorfFlex CZ MorfFlex CZ is a Czech morphological dictionary developed originally by Jan Hajiˇc as a spelling checker and lemmatization dictionary; it is a plain list of lemma-tagform triples and the latest version contains nearly 985 thousand unique lemmas and more than 120 million word forms (Hajiˇc and Hlav´acˇ ov´a, 2013). For each word form, full inflectional information is available, using the positional tagging scheme proposed by Hajiˇc (2004). In addition, lemmas can also contain basic derivational, semantic and named entity information. The dictionary deals"
L16-1208,2014.lilt-11.6,0,0.12504,", it suffers from overgeneration as it is not possible to condition the alternations by the context. The DeriNet network, which is – as a part of our approach – described in Section 4., has been developed as a publicly accessible, large-coverage resource of Czech derivational data and is, to the best of our knowledge, one of only few specialized resources of derivational data even in a broader context of different langauges. Most of the resources are very recent such as DerivBase for German 1307 ˇ (Zeller et al., 2013), DerivBase.Hr for Croatian (Snajder, 2014), D´emonette network for French (Hathout and Namer, 2014), or the language-independent approach by Baranes and Sagot (2014). 3. Inflectional resource: MorfFlex CZ MorfFlex CZ is a Czech morphological dictionary developed originally by Jan Hajiˇc as a spelling checker and lemmatization dictionary; it is a plain list of lemma-tagform triples and the latest version contains nearly 985 thousand unique lemmas and more than 120 million word forms (Hajiˇc and Hlav´acˇ ov´a, 2013). For each word form, full inflectional information is available, using the positional tagging scheme proposed by Hajiˇc (2004). In addition, lemmas can also contain basic derivati"
L16-1208,sevcikova-zabokrtsky-2014-word,1,0.736633,"and DeriNet Search tool). Keywords: derivation, inflection, morphology 1. Introduction The present paper deals with merging two complementary resources of morphological data previously existing for Czech: first, the inflectional dictionary MorfFlex CZ used by a morphological analyzer capable of analyzing/generating several million Czech word forms according to the rules of Czech inflection (Hajiˇc and Hlav´acˇ ov´a, 2013), and second, the recently developed lexical network DeriNet that stores derivational relations among ˇ c´ıkov´a and several hundred thousand Czech lemmas (Sevˇ ˇ Zabokrtsk´ y, 2014). The resulting publicly available data resource interconnects both types of information and thus allows exploiting the two morphological phenomena in NLP applications in a unified way. In addition, using the full inflectional dictionary led to a considerable growth of both recall and precision of the captured derivational relations. 2. Related work Czech is a language with both a rich inflectional system and a complex derivational morphology. In the theoretical description, inflectional and derivational morphology have been traditionally separated from each other, derivations being described"
L16-1208,snajder-2014-derivbase,0,0.13068,"t common alternations into the queries; however, it suffers from overgeneration as it is not possible to condition the alternations by the context. The DeriNet network, which is – as a part of our approach – described in Section 4., has been developed as a publicly accessible, large-coverage resource of Czech derivational data and is, to the best of our knowledge, one of only few specialized resources of derivational data even in a broader context of different langauges. Most of the resources are very recent such as DerivBase for German 1307 ˇ (Zeller et al., 2013), DerivBase.Hr for Croatian (Snajder, 2014), D´emonette network for French (Hathout and Namer, 2014), or the language-independent approach by Baranes and Sagot (2014). 3. Inflectional resource: MorfFlex CZ MorfFlex CZ is a Czech morphological dictionary developed originally by Jan Hajiˇc as a spelling checker and lemmatization dictionary; it is a plain list of lemma-tagform triples and the latest version contains nearly 985 thousand unique lemmas and more than 120 million word forms (Hajiˇc and Hlav´acˇ ov´a, 2013). For each word form, full inflectional information is available, using the positional tagging scheme proposed by Hajiˇc (2"
L16-1208,P14-5003,1,0.889221,"Missing"
L16-1208,P13-1118,0,0.26161,"Missing"
L18-1291,baranes-sagot-2014-language,0,0.349301,"Missing"
L18-1291,2014.lilt-11.6,0,0.183743,"ional rules developed by a linguist, resulting in the biggest word-formation network for that language. The presented approach is general enough to be adopted for other languages. Keywords: derivation, derivational morphology, Polish, Spanish, lexical network, learning to rank, sequential pattern mining 1. Introduction Derivational morphology has moved into focus of Natural Language Processing (NLP) only recently. For some languages, we observe a significant research effort in the construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, D´emonette (Hathout and Namer, 2014) for ˇ French, DerivBase.Hr (Snajder, 2014) for Croatian, Derˇ ˇ ˇ iNet (Sevˇc´ıkov´a and Zabokrtsk´y, 2014; Zabokrtsk´ y et al., 2016) for Czech, or Word Formation Latin (Litta et al., 2016). However, for many other languages the data resources which provide information about derived words are scarce or even lacking. Unfortunately, the creation of such resources requires a considerable human effort and is highly time-consuming. In this paper, we propose a method for semi-automatic construction of a derivational network which can be applied to under-resourced languages. The proposed approach r"
L18-1291,piasecki-etal-2012-recognition,0,0.159192,"e consider word-formation networks whose connected components have a tree structure (see an example on Figure 1). This means that derivatives can have only one base word, hence, e.g. compounding is not considered. Although such networks were created for some languages (see Sect. 1.), there is still a demand for the creation of such resources for many other languages. For instance, there is no word-formation network for two languages which are considered in the present work, Polish and Spanish. For those languages the number of works on automatic detection of derivatives is also quite limited. Piasecki et al. (2012) propose an approach based on bootstrapping and supervised learning in order to construct derivational rules for Polish. Also, a hand-crafted list of alternation groups is used in order to make the derived words as similar as possible to the base word, allowing for a more effective application of derivational rules. Besides using this additional resource, the approach uses a relatively large training set of 15 718 examples (approx. 10 times bigger than the one used in this work) and deals only with derivation by suffixation or prefixation. Recently, an algorithm for the automatic pairing of pe"
L18-1291,sevcikova-zabokrtsky-2014-word,1,0.879465,"Missing"
L18-1291,snajder-2014-derivbase,0,0.396613,"Missing"
L18-1291,N12-1026,0,0.019648,"he sequence a. Due to the importance of the task, a lot of approaches have been proposed. Among them, SPADE (Zaki, 2001) which bases on breadth-first search and Apriori pruning on the vertical data format. For formal definitions and a detailed review consult e.g. Mabroukeh and Ezeife (2010). Learning to rank Learning to rank is a widely studied area of machine learning which was originally researched in the context of automatic ranking of web search results in the information retrieval community. However, it proved to be useful in many other areas such as statistical machine translation, see (Watanabe, 2012). The task of learning to rank is the construction of a model which is able to sort new objects according to their degrees of importance. The approaches for machine-learned ranking can be divided into three groups: the pointwise, the pairwise and the listwise approaches. Pointwise methods make use of classification or regression techniques in order to predict a score for each object given the query. An idea of predicting the order of each pair of objects is explored by pairwise methods. Finally, listwise approaches directly optimize metrics defined on a whole list of objects. For a review of t"
L18-1291,P13-1118,0,0.543239,"Missing"
L18-1291,L16-1208,1,0.801695,"Missing"
L18-1551,W14-3348,0,0.0170761,"tionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇze"
L18-1551,W13-3102,0,0.0353555,"Missing"
L18-1551,D13-1155,0,0.0333726,"2003 and DUC-2004 competitions (Over et al., 2007), which provided a standard evaluation set consisting of 500 news articles from New York Times and Associated Press Wire, each paired with 4 different human-generated reference summaries. For training, the Gigaword dataset (Graff et al., 2003) has been used frequently, offering 4 million news articles including their headlines. Recently, Nallapati et al. (2016a) modified the CNN/Daily Mail corpus constructed by Hermann et al. (2015) to serve for multi-sentence summarization. The corpus consists of approximately 300 000 documents. Additionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been"
L18-1551,D15-1042,0,0.0198908,"hile abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to produce a dataset that would be sufficiently large for deep learning methods to be applicable to it. However, we possessed limited hum"
L18-1551,W15-4638,0,0.170383,"ence summaries. For training, the Gigaword dataset (Graff et al., 2003) has been used frequently, offering 4 million news articles including their headlines. Recently, Nallapati et al. (2016a) modified the CNN/Daily Mail corpus constructed by Hermann et al. (2015) to serve for multi-sentence summarization. The corpus consists of approximately 300 000 documents. Additionally, Filippova and Altun (2013) proposed a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, w"
L18-1551,P16-1014,0,0.0577471,"Missing"
L18-1551,N09-1041,0,0.042182,"re generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to"
L18-1551,W04-1013,0,0.10006,"tion is performed using a language-agnostic variant of ROUGE. Keywords: SumeCzech, summarization dataset, document summarization, ROUGE, Czech 1. Introduction Similarly to many other NLP tasks, performance of automatic document summarization has been improving with the recent rise of neural network methods. While deep neural network models can leverage large datasets, only a few moderately-sized datasets are available for document summarization when compared to, e.g., machine translation. Additionally, document summarization has been explored mostly on English, with the dominant ROUGE metric (Lin, 2004) being English-specific (utilizing English stemmer, stop words and synonyms). In order to provide more data for document summarization in Czech, this paper introduces SumeCzech – a collection of one million Czech news articles, each consisting of a headline, a several sentence abstract and a full text. The documents originate from five Czech Internet news sites. The dataset can be downloaded using our provided scripts. Headline-abstract-text structure of the documents allows the dataset to be used for multiple summarization setups: headline generation either from an abstract or a full text, or"
L18-1551,W04-3252,0,0.074127,"sentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset,"
L18-1551,K16-1028,0,0.042758,"Missing"
L18-1551,D15-1044,0,0.0696861,"earch-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), SumBasic (Vanderwende et al., 2007) or KL-Sum (Haghighi and Vanderwende, 2009). However, very good results in extractive summarization were achieved recently with recurrent neural networks (Filippova et al., 2015; Filippova and Alfonseca, 2015; Nallapati et al., 2016b; Nallapati et al., 2017). Abstractive approach relies predominantly on the machine translation paradigm, also boosted by the recent success of neural machine translation (Rush et al., 2015; Nallapati et al., 2016a; G¨ulc¸ehre et al., 2016; See et al., 2017). 3. 3.1. The Dataset Choice of Data Sources When designing the dataset, we considered two main requirements. First, and most importantly, we wanted to produce a dataset that would be sufficiently large for deep learning methods to be applicable to it. However, we possessed limited human and time resources making it impossible to accomplish this task by creating summaries manually. This implied an automatic or a semi-automatic method of collecting the data, facilitating the need for a data source consisting of documents that"
L18-1551,P17-1099,0,0.441174,"d a method for constructing datasets for extractive sentence summarization.1 To our best knowledge, only small summarization datasets exist for Czech: Czech part of the MultiLing dataset (Giannakopoulos et al., 2015; Li et al., 2013; Elhadad et al., 2013) containing 40 Wikipedia articles, and SummEC (Rott ˇ and Cerva, 2013) containing 50 news articles. 2.2. Metrics ROUGE (Lin, 2004) is the most commonly used metric, proposed as an English-specific recall-based metric. It utilizes English stemmer, stop words and synonyms. Recently, the METEOR metric (Denkowski and Lavie, 2014) has been used by See et al. (2017) to evaluate multisentence summarization. 2.3. Summarization Methods Summarization methods are generally either extractive or abstractive. Extractive methods only select suitable parts (sentences, words or phrases) from the document, while abstractive methods can produce an arbitrary text as the summary. The extractive summarization methods are typically unsupervised, for example Luhn (Luhn, 1958), Latent Se1 The dataset has been recently released at https://github. com/google-research-datasets/sentence-compression. 3488 mantic Analysis (Steinberger and Jeˇzek, 2004), LexRank (Erkan and Radev,"
L18-1584,W13-1609,0,0.0180442,"It is assumed that the prior texts are objective and the latter subjective. This process of automatic mining naturally introduces a noise in the generated annotations, i.e. some objective sentences are probably classified as subjective and vise-versa. In order to evaluate also non-English datasets, we selected three Czech/Slovak datasets. All of them aim to test sentiment analysis, i.e. the goal of the model is to distinguish among positive, negative, bipolar and neutral texts. The three selected datasets (Social Media Dataset, Movie Review Dataset and Product Review Dataset, all described by Habernal et al. (2013)) represent texts regarding various topics and were automatically collected from the internet. 3 We implemented and employed deep learning framework cxflow (v0.4) for all our experiments (https://cxflow.org) as it supports quick experiment creation, logging and result management. Their sentiment was automatically derived based on the rating assigned to the reviewed item (number of stars). The final selected dataset is the one introduced as Discriminating between Similar Language (DSL) Shared Task 2015 at LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varietie"
L18-1584,D17-1215,0,0.0235629,"isualization techniques in order to understand the effect of constructed adversarial embeddings. Nonetheless, the results of their experiments did not support any hypothesis of the method usefulness. Miyato et al. (2016b) were the first who constructed adversarial perturbations of word embeddings. In addition, the authors employed a so-called virtual adversarial training which introduces a new loss term based on KL-divergence (Miyato et al., 2016b, Eq. 3,4). The authors evaluated five data sources and both adversarial and virtual adversarial learning outperformed the state-of-the-art models.2 Jia and Liang (2017) used adversarial generation of the source text instead of embeddings. They introduced various methods fooling the models for Standford Question Answering Dataset (Rajpurkar et al., 2016) by generating additional sentences to the original source text. Lu et al. (2017) aimed for adversarial examples detection in the context of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014a). In addition, Miyato et al. (2016a) employed RNN-based GANs to semi-supervised text classification. Adversarial examples are often created as a perturbation of the original input. Goodfellow et al. (2014b)"
L18-1584,P11-1015,0,0.0345546,"s models including both deep and shallow neural networks, support vector machines (SVM) even with radial basis function kernel (RBF) and decision trees. Other noteworthy works regarding the adversarial examples in image processing include the work of Fawzi et al. (2016), Moosavi-Dezfooli et al. (2016b), Papernot et al. (2016b) and Papernot et al. (2016a). Regarding adversarial examples used in the context of NLP, there is much less published research. Caswell et al. (2016) have been, to our knowledge, the first who experimented with using adversarial embedding perturbations. The IMDB dataset (Maas et al., 2011) was used as a binary sentiment classification task. The authors introduced visualization techniques in order to understand the effect of constructed adversarial embeddings. Nonetheless, the results of their experiments did not support any hypothesis of the method usefulness. Miyato et al. (2016b) were the first who constructed adversarial perturbations of word embeddings. In addition, the authors employed a so-called virtual adversarial training which introduces a new loss term based on KL-divergence (Miyato et al., 2016b, Eq. 3,4). The authors evaluated five data sources and both adversarial"
L18-1584,P04-1035,0,0.137163,"2015). Each of the datasets models a different aspect of natural language understanding and reasoning. The typical dataset example contains a sequence of facts regarding position of various objects and a question testing the understanding and reasoning based on these facts. To be specific, we decided to choose the English versions of bAbI tasks #1, #2 and #6, because they represent both simple and advanced problems. Secondly, we focused on subjectivity detection, i.e. distinguishing between subjective and objective texts. For this reason, we selected Movie Review Data (originally evaluated by Pang and Lee (2004)). This dataset contains short texts (typically single sentences), which were automatically mined either from the official movie description provided by the distributor or from the user reviews. It is assumed that the prior texts are objective and the latter subjective. This process of automatic mining naturally introduces a noise in the generated annotations, i.e. some objective sentences are probably classified as subjective and vise-versa. In order to evaluate also non-English datasets, we selected three Czech/Slovak datasets. All of them aim to test sentiment analysis, i.e. the goal of the"
L18-1584,D16-1264,0,0.0514028,"Missing"
L18-1584,W15-5401,0,0.0534216,"Missing"
lopatkova-etal-2006-valency,W04-2704,0,\N,Missing
lopatkova-etal-2006-valency,P03-1068,0,\N,Missing
majlis-zabokrtsky-2012-language,E12-3006,1,\N,Missing
majlis-zabokrtsky-2012-language,kilgarriff-etal-2010-corpus,0,\N,Missing
P09-2037,W04-2201,0,0.0120485,"finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representaWe would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of depen"
P09-2037,J93-2003,0,0.0228161,"Missing"
P09-2037,N03-1017,0,0.021086,"used for labeling nodes of a dependency tree, interpreted as revealing the hidden states1 in the tree nodes, given another (observable) labeling of the nodes of the same tree. The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees. Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model. This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (Koehn et al., 2003)), in which translation and language models are trainable separately too. Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967))"
P09-2037,W01-1406,0,0.0229009,"l target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representaWe would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we s"
P09-2037,W08-0325,1,0.877289,"Missing"
P09-2037,2001.mtsummit-ebmt.4,0,\N,Missing
P13-1051,C00-2143,0,0.133964,"rinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling"
P13-1051,W06-2920,0,0.0228052,"section, we identify the CS styles defined in the previous section as used in the primary treebank data sources; statistical observations (such as the amount of annotated shared modifiers) presented here, as well as experiments on CS-style convertibility presented in Section 5.2, are based on the normalized shapes of the treebanks as contained in the HamleDT 1.0 treebank collection (Zeman et al., 2012).15 Some of the treebanks were downloaded individually from the web, but most of them came from previously published collections for dependency parsing campaigns: six languages from CoNLL-2006 (Buchholz and Marsi, 2006), seven languages from CoNLL-2007 (Nivre et al., 2007), two languages from CoNLL-2009 (Hajiˇc and others, 2009), three languages from ICON-2010 (Husain et al., 2010). Obviously, there is a certain risk that the CS-related information contained in the source treebanks was slightly biased by the properties of the CoNLL format upon conversion. In addition, many of the treebanks were natively dependency-based (cf. the 2nd column of Table 1), but some were originally based on constituents and thus specific converters to the CoNLL format had to be created (for instance, the Spanish phrase-structure"
P13-1051,dzeroski-etal-2006-towards,0,0.138307,"CS head, or attaching shared modifiers below the nearest conjunct). Even if it does not make sense to create the full Cartesian product of all dimensions because some values cannot be combined, it allows to explore the space of possible CS styles systematically.9 One can find various arguments supporting the particular choices. MTT possesses a complex set of linguistic criteria for identifying the governor of a relation (see Mazziotta (2011) for an overview), which lead to MS. MS is preferred in a rule-based dependency parsing system of Lombardo and Lesmo (1998). PS is advocated by ˇ ep´anek (2006) who claims that it can represent Stˇ shared modifiers using a single additional binary attribute, while MS would require a more complex co-indexing attribute. An argumentation of Tratz and Hovy (2011) follows a similar direction: We would like to change our [MS] handling of coordinating conjunctions to treat the coordinating conjunction as the head [PS] because this has fewer ambiguities than [MS]. . . We conclude that the influence of the choice of coordination style is a well-known problem in dependency syntax. Nevertheless, published works usually focus only on a narrow ad-hoc selection of"
P13-1051,W12-0503,1,0.894933,"Missing"
P13-1051,afonso-etal-2002-floresta,0,0.0183801,"hers, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinatio"
P13-1051,W03-2405,0,0.0337781,"Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling variants defined in Section 3.2. Variations in representing coordination structures Our analysis of variations in representing coordination structures is based on observations from a set of dependency treebanks for 26 languages.7 5 We use the already establishe"
P13-1051,W09-1201,1,0.0605496,"Missing"
P13-1051,W12-3603,1,0.894647,"Missing"
P13-1051,ramasamy-zabokrtsky-2012-prague,1,0.894745,"Missing"
P13-1051,E09-1047,0,0.144276,"Missing"
P13-1051,W98-0502,0,0.490473,"tactic surface means of expressing coordination relations is fuzzy. Some languages can use enclitics instead of conjunctions/prepositions, e.g. Latin “Senatus Populusque Romanus”. Purely hypotactic surface means such as the preposition in “John with Mary” occur too.4 Related work Let us first recall the basic well-known characteristics of CSs. In the simplest case of a CS, a coordinating conjunction joins two (usually syntactically and semantically compatible) words or phrases called conjuncts. Even this simplest case is difficult to represent within a dependency tree because, in the words of Lombardo and Lesmo (1998): Dependency paradigms exhibit obvious difficulties with coordination because, differently from most linguistic structures, it is not possible to characterize the coordination construct with a general schema involving a head and some modifiers of it. Proper formal representation of CSs is further complicated by the following facts: • Careful semantic analysis of CSs discloses additional complications: if a node is modified by a CS, it might happen that it is the node itself (and not its modifiers) what should be semantically considered as a conjunct. Note the difference between “red and white"
P13-1051,J93-2004,0,0.0420156,"rent granularity of syntactic labels. 3 3.1 Topological variations We distinguish the following dimensions of topological variations of CS styles (see Figure 1): Family – configuration of conjuncts. We divide the topological variations into three main groups, labeled as Prague (fP), Moscow (fM), and vided by IXA Group) (Aduriz and others, 2003), Bulgarian: BulTreeBank (Simov and Osenova, 2005), Czech: Prague Dependency Treebank 2.0 (Hajiˇc et al., 2006), Danish: Danish Dependency Treebank (Kromann et al., 2004), Dutch: Alpino Treebank (van der Beek and others, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treeban"
P13-1051,D07-1013,0,0.158065,"Missing"
P13-1051,taule-etal-2008-ancora,0,0.0175746,"Missing"
P13-1051,zeman-etal-2012-hamledt,1,0.0729458,"Missing"
P13-1051,D11-1116,0,\N,Missing
P13-1051,J03-4003,0,\N,Missing
P13-1051,D07-1096,0,\N,Missing
P15-2040,P15-2044,0,0.0650444,"Missing"
P15-2040,P05-1012,0,0.0425171,"spanning tree over the graph, using the algorithm of Chu and Liu (1965) and Edmonds (1967). 4 KLcpos 3 Language Similarity Delexicalized Parser Transfer We introduce KLcpos 3 , a language similarity measure based on distributions of coarse POS tags in source and target POS-tagged corpora. This is motivated by the fact that POS tags constitute a key feature for delexicalized parsing. The distributions are estimated as frequencies of UPOS trigrams3 in the treebank training sections: Throughout this work, we use MSTperl (Rosa, 2015b), an implementation of the unlabelled single-best MSTParser of McDonald et al. (2005b), with first-order features and nonprojective parsing, trained using 3 iterations of MIRA (Crammer and Singer, 2003).1 Our delexicalized feature set is based on the set of McDonald et al. (2005a) with lexical features removed. It consists of combinations of signed edge length (distance of head and parent, bucketed for values above 4 and for values above 10) with POS tag of the head, dependent, their neighbours, and all nodes between them.2 We use the Universal POS Tagset (UPOS) of Petrov et al. (2012). f (cpos i−1 , cpos i , cpos i+1 ) = count(cpos i−1 , cpos i , cpos i+1 ) =P ; (1) ∀cpos a,"
P15-2040,H05-1066,0,0.147077,"Missing"
P15-2040,D11-1006,0,0.699345,"language similarity estimation, and the test sections for evaluation.6 5.1 Avg KL−4 (tgt, src) cpos 3 Table 1: Weighted multi-source transfer using various similarity measures. Evaluation using average UAS on the development set. 4.1 KLcpos 3 for Source Selection 5 Measure 5.2 Other datasets Additionally, we also report preliminary results on the Prague style conversion of HamleDT, which loosely follows the style of the Prague Dependency Treebank of B¨ohmov´a et al. (2003), and on the subset of CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nilsson et al., 2007) that was used by McDonald et al. (2011).8 Tuning To avoid overfitting the exact definition of KLcpos 3 and KL−4 to the 30 treebanks, we used only 12 cpos 3 6 4 The KL divergence is non-symmetric; DKL (P ||Q) expresses the amount of information lost when a distribution Q is used to approximate the true distribution P . Thus, in our setting, we use DKL (tgt||src), as we try to minimize the loss of using a src parser as an approximation of a tgt parser. 5 A high value of the exponent strongly promotes the most similar source language, giving minimal power to the other languages, which is good if there is a very similar source language"
P15-2040,W06-2920,0,0.0983583,"ts. We use the treebank training sections for parser training and language similarity estimation, and the test sections for evaluation.6 5.1 Avg KL−4 (tgt, src) cpos 3 Table 1: Weighted multi-source transfer using various similarity measures. Evaluation using average UAS on the development set. 4.1 KLcpos 3 for Source Selection 5 Measure 5.2 Other datasets Additionally, we also report preliminary results on the Prague style conversion of HamleDT, which loosely follows the style of the Prague Dependency Treebank of B¨ohmov´a et al. (2003), and on the subset of CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nilsson et al., 2007) that was used by McDonald et al. (2011).8 Tuning To avoid overfitting the exact definition of KLcpos 3 and KL−4 to the 30 treebanks, we used only 12 cpos 3 6 4 The KL divergence is non-symmetric; DKL (P ||Q) expresses the amount of information lost when a distribution Q is used to approximate the true distribution P . Thus, in our setting, we use DKL (tgt||src), as we try to minimize the loss of using a src parser as an approximation of a tgt parser. 5 A high value of the exponent strongly promotes the most similar source language, giving minimal power to the other lang"
P15-2040,P12-1066,0,0.515752,"st source treebank. An alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2013) to estimate source-target language similarity for delexicalized transfer, focusing on genealogy distance and word-order features. Søgaard and Wulff (2012) also introduced weighting into the treebank concatenation approach, using a POS ngram model trained on a target-language corpus Introduction The approach of delexicalized dependency parser transfer is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably part-of-speech (POS) tags, and to apply"
P15-2040,P11-1061,0,0.077807,"Missing"
P15-2040,petrov-etal-2012-universal,0,0.116175,"Missing"
P15-2040,P11-1157,0,0.0820505,"Missing"
P15-2040,rosa-etal-2014-hamledt,1,0.346083,"Missing"
P15-2040,zeman-2008-reusable,0,0.164507,"Missing"
P15-2040,W15-2131,1,0.650057,"plied by its weight. 4. Find the final dependency parse tree as the maximum spanning tree over the graph, using the algorithm of Chu and Liu (1965) and Edmonds (1967). 4 KLcpos 3 Language Similarity Delexicalized Parser Transfer We introduce KLcpos 3 , a language similarity measure based on distributions of coarse POS tags in source and target POS-tagged corpora. This is motivated by the fact that POS tags constitute a key feature for delexicalized parsing. The distributions are estimated as frequencies of UPOS trigrams3 in the treebank training sections: Throughout this work, we use MSTperl (Rosa, 2015b), an implementation of the unlabelled single-best MSTParser of McDonald et al. (2005b), with first-order features and nonprojective parsing, trained using 3 iterations of MIRA (Crammer and Singer, 2003).1 Our delexicalized feature set is based on the set of McDonald et al. (2005a) with lexical features removed. It consists of combinations of signed edge length (distance of head and parent, bucketed for values above 4 and for values above 10) with POS tag of the head, dependent, their neighbours, and all nodes between them.2 We use the Universal POS Tagset (UPOS) of Petrov et al. (2012). f (c"
P15-2040,N06-2033,0,0.490553,"ng treebank annotation styles into a common style, which later developed into the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delexicalized transfer in a setting with multiple source treebanks available, finding that the problem of selecting the best source treebank without access to a target language treebank for evaluation is non-trivial. They combined all source treebanks by concatenating them but noted that this yields worse results than using only the best source treebank. An alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2"
P15-2040,C12-2115,0,0.472057,"n alternative is the (monolingual) parse tree combination method of Sagae and Lavie (2006), who apply several independent parsers to the input sentence and combine the resulting parse trees using a maximum spanning tree algorithm. Surdeanu and Manning (2010) enrich tree combination with weighting, assigning each parser a weight based on its Unlabelled Attachment Score (UAS). In our work, we introduce an extension of this method to a crosslingual setting by combining parsers for different languages and using sourcetarget language similarity to weight them. Several authors (Naseem et al., 2012; Søgaard and Wulff, 2012; T¨ackstr¨om et al., 2013b) employed WALS (Dryer and Haspelmath, 2013) to estimate source-target language similarity for delexicalized transfer, focusing on genealogy distance and word-order features. Søgaard and Wulff (2012) also introduced weighting into the treebank concatenation approach, using a POS ngram model trained on a target-language corpus Introduction The approach of delexicalized dependency parser transfer is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably part-of-speech (POS) tags, and to apply that parser to POS-tagged"
P15-2040,Q13-1001,0,0.0109865,"Missing"
P15-2040,N13-1126,0,0.496894,"Missing"
P15-2040,I08-3008,0,0.700849,"Missing"
P15-2040,de-marneffe-etal-2014-universal,0,\N,Missing
P15-2040,N10-1091,0,\N,Missing
P15-2040,D07-1096,0,\N,Missing
R19-1007,C14-1111,0,0.0487234,"Missing"
R19-1007,E06-2023,0,0.0373692,", and verbal stems. Hesabi (1988) claimed that Persian can derive more than 226 million words (Hesabi, 1988). To the best of our knowledge, the research on morphology of the Persian language is very limited. Rasooli et al. (2013) claimed that performing morphological segmentation in the preprocessing phase of statistical machine translation could improve the quality of translations for morphology rich and complex languages. Although they segmented very low portion of Persian words (only some Persian verbs), the quality of their machine translation system increases by 1.9 points of BLEU score. Arabsorkhi and Shamsfard (2006) proposed a Minimum Description Length (MDL) based algorithm with some improvements for discovering the morphemes of Persian language through automatic analysis of corpora. 3 3.1 Classification-Based Segmentation Models In the first setup, we convert the segmentation task (the task of segmenting a word into a sequence morphemes) simply to a set of independent binary decisions capturing the presence or absence of a segmentation boundary in front of each letter in the word. For this task, we use various standard off-the-shelf classifiers available in the Scikit-learn toolkit (Pedregosa et al., 2"
R19-1007,N18-1005,0,0.0343264,"Missing"
R19-1007,W16-2010,0,0.0716835,"Missing"
R19-1007,W16-1603,0,0.0251525,"Persian (Farsi) is one of the languages of the IndoEuropean language family within the Indo-Iranian branch and is spoken in Iran, Afghanistan, Tajikistan and some other regions related to ancient Persian. In addition, we evaluated our models on Czech and Finnish, however, the amount of annotated data for them is substantially lower. Automatic morphological segmentation was firstly introduced by Harris (1970). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Gr¨onroos et al., 2014). Another version of the latter which includes a semisupervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a loglinear model which uses overlapping features for unsupervised morphological segmentation. In spite of the dominance of the unsupervised system"
R19-1007,W02-0603,0,0.674761,"the first and the only such computer-readable dataset for Persian. Persian (Farsi) is one of the languages of the IndoEuropean language family within the Indo-Iranian branch and is spoken in Iran, Afghanistan, Tajikistan and some other regions related to ancient Persian. In addition, we evaluated our models on Czech and Finnish, however, the amount of annotated data for them is substantially lower. Automatic morphological segmentation was firstly introduced by Harris (1970). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Gr¨onroos et al., 2014). Another version of the latter which includes a semisupervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a loglinear model which uses overlapping features for unsupervised morphological se"
R19-1007,L18-1549,1,0.705625,"first three models are illustrated in Figures 1 and 2. The presented seq2seq model, is 54 similar to the model described in (Gr¨onroos et al., 2019). The last presented model is an attention based model, which is shown in Figure 3. In this model, we use Bi-LSTM as encoder and LSTM as attention layer, and finally, outputs of encoder and attention layers are added together. et al., 2011) as well as the Czech dataset used in our experiments are described. 4.1 We extracted our primary word list from three different corpora. The first corpus contains sentences extracted from the Persian Wikipedia (Karimi et al., 2018). The second one is popular Persian mono-lingual corpus BijanKhan (Bijankhan et al., 2011), and the last one is Persian-NER1 (Poostchi et al., 2018). For all introduced corpora, using Hazm toolset (Persian preprocessing and tokenization tools)2 and the stemming tool presented by Taghi-Zadeh et al. (2015), we extracted and normalized all sentences and in the final steps using our rulebased stemmer and a Persian lemma collection, all words are lemmatized and stemmed. Finally all semi-spaces are automatically detected and fixed. Words with more than 10 occurrences in the corpora were selected for"
R19-1007,J01-2001,0,0.0741243,"s far as we know, the first and the only such computer-readable dataset for Persian. Persian (Farsi) is one of the languages of the IndoEuropean language family within the Indo-Iranian branch and is spoken in Iran, Afghanistan, Tajikistan and some other regions related to ancient Persian. In addition, we evaluated our models on Czech and Finnish, however, the amount of annotated data for them is substantially lower. Automatic morphological segmentation was firstly introduced by Harris (1970). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Gr¨onroos et al., 2014). Another version of the latter which includes a semisupervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a loglinear model which uses overlapping features for unsup"
R19-1007,N09-1024,0,0.319246,"such computer-readable dataset for Persian. Persian (Farsi) is one of the languages of the IndoEuropean language family within the Indo-Iranian branch and is spoken in Iran, Afghanistan, Tajikistan and some other regions related to ancient Persian. In addition, we evaluated our models on Czech and Finnish, however, the amount of annotated data for them is substantially lower. Automatic morphological segmentation was firstly introduced by Harris (1970). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Gr¨onroos et al., 2014). Another version of the latter which includes a semisupervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a loglinear model which uses overlapping features for unsupervised morphological segmentation. In spit"
R19-1007,L18-1701,0,0.0314649,"9). The last presented model is an attention based model, which is shown in Figure 3. In this model, we use Bi-LSTM as encoder and LSTM as attention layer, and finally, outputs of encoder and attention layers are added together. et al., 2011) as well as the Czech dataset used in our experiments are described. 4.1 We extracted our primary word list from three different corpora. The first corpus contains sentences extracted from the Persian Wikipedia (Karimi et al., 2018). The second one is popular Persian mono-lingual corpus BijanKhan (Bijankhan et al., 2011), and the last one is Persian-NER1 (Poostchi et al., 2018). For all introduced corpora, using Hazm toolset (Persian preprocessing and tokenization tools)2 and the stemming tool presented by Taghi-Zadeh et al. (2015), we extracted and normalized all sentences and in the final steps using our rulebased stemmer and a Persian lemma collection, all words are lemmatized and stemmed. Finally all semi-spaces are automatically detected and fixed. Words with more than 10 occurrences in the corpora were selected for manual annotation. We decided to send all 80K words to our 16 annotators in the way that each word is checked and annotated by two independent pers"
R19-1007,L16-1208,1,0.900256,"Missing"
R19-1007,I13-1144,0,0.0768039,"Missing"
R19-1007,W13-3504,0,0.29612,"g, pages 52–61, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_007 niques into their CRF-based model via a feature set augmentation. (Ruokolainen et al., 2014) segmented training data is available, then the entirely unsupervised systems tend not to be competitive. Furthermore, unsupervised segmentation still has considerable weaknesses, including over-segmentation of roots and erroneous segmentation of affixes (Wang et al., 2016). To deal with those limitations, recent works show a growing interest in semi-supervised and supervised approaches (Kohonen et al., 2010; Ruokolainen et al., 2013, 2014; Sirts and Goldwater, 2013; Wang et al., 2016; Kann and Sch¨utze, 2016; Kann et al., 2018; Cotterell and Sch¨utze, 2017; Gr¨onroos et al., 2019) which employ annotated morpheme boundaries in the training phase. In our work we designed and evaluated various machine learning models and trained them using only the annotated lexicon in a supervised manner. Our models do not leverage the unannotated data nor context information and only use the primary hand-annotated segmentation lexicons. Experimental results show that our Bi-LSTM model perform slightly better than other models in boundary"
R19-1007,E14-4017,0,0.101635,"e translation, speech recognition, and information retrieval. Morphological segmentation of words is the process of dividing a word into smaller units called morphemes. Morphological segmentation task is harder for those languages which are morphologically rich and complex like Persian, Arabic, Czech, Finnish or Turkish, especially when there are not enough annotated data 52 Proceedings of Recent Advances in Natural Language Processing, pages 52–61, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_007 niques into their CRF-based model via a feature set augmentation. (Ruokolainen et al., 2014) segmented training data is available, then the entirely unsupervised systems tend not to be competitive. Furthermore, unsupervised segmentation still has considerable weaknesses, including over-segmentation of roots and erroneous segmentation of affixes (Wang et al., 2016). To deal with those limitations, recent works show a growing interest in semi-supervised and supervised approaches (Kohonen et al., 2010; Ruokolainen et al., 2013, 2014; Sirts and Goldwater, 2013; Wang et al., 2016; Kann and Sch¨utze, 2016; Kann et al., 2018; Cotterell and Sch¨utze, 2017; Gr¨onroos et al., 2019) which emplo"
R19-1007,Q13-1021,0,0.0212191,"Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_007 niques into their CRF-based model via a feature set augmentation. (Ruokolainen et al., 2014) segmented training data is available, then the entirely unsupervised systems tend not to be competitive. Furthermore, unsupervised segmentation still has considerable weaknesses, including over-segmentation of roots and erroneous segmentation of affixes (Wang et al., 2016). To deal with those limitations, recent works show a growing interest in semi-supervised and supervised approaches (Kohonen et al., 2010; Ruokolainen et al., 2013, 2014; Sirts and Goldwater, 2013; Wang et al., 2016; Kann and Sch¨utze, 2016; Kann et al., 2018; Cotterell and Sch¨utze, 2017; Gr¨onroos et al., 2019) which employ annotated morpheme boundaries in the training phase. In our work we designed and evaluated various machine learning models and trained them using only the annotated lexicon in a supervised manner. Our models do not leverage the unannotated data nor context information and only use the primary hand-annotated segmentation lexicons. Experimental results show that our Bi-LSTM model perform slightly better than other models in boundary prediction for our hand-segmented"
ramasamy-zabokrtsky-2012-prague,J93-2004,0,\N,Missing
ramasamy-zabokrtsky-2012-prague,W09-3030,0,\N,Missing
ramasamy-zabokrtsky-2012-prague,A00-1031,0,\N,Missing
ramasamy-zabokrtsky-2012-prague,2005.mtsummit-papers.11,0,\N,Missing
ramasamy-zabokrtsky-2012-prague,I08-2099,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2006-generating,0,\N,Missing
rosa-etal-2014-hamledt,zeman-2008-reusable,1,\N,Missing
rosa-etal-2014-hamledt,J93-2004,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2014-universal,0,\N,Missing
rosa-etal-2014-hamledt,C00-2143,0,\N,Missing
rosa-etal-2014-hamledt,W08-1301,0,\N,Missing
rosa-etal-2014-hamledt,W13-3721,0,\N,Missing
rosa-etal-2014-hamledt,D11-1006,0,\N,Missing
rosa-etal-2014-hamledt,P13-1051,1,\N,Missing
rosa-etal-2014-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
rosa-etal-2014-hamledt,berovic-etal-2012-croatian,0,\N,Missing
rosa-etal-2014-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
rosa-etal-2014-hamledt,W03-2405,0,\N,Missing
rosa-etal-2014-hamledt,P13-2017,0,\N,Missing
rosa-etal-2014-hamledt,taule-etal-2008-ancora,0,\N,Missing
rosa-etal-2014-hamledt,W10-1819,0,\N,Missing
rosa-etal-2014-hamledt,afonso-etal-2002-floresta,0,\N,Missing
sevcikova-zabokrtsky-2014-word,W07-1710,0,\N,Missing
sevcikova-zabokrtsky-2014-word,W03-2901,0,\N,Missing
sevcikova-zabokrtsky-2014-word,W07-1709,0,\N,Missing
sevcikova-zabokrtsky-2014-word,P13-1118,0,\N,Missing
sevcikova-zabokrtsky-2014-word,W10-1730,1,\N,Missing
W04-2711,hajicova-kucerova-2002-argument,0,0.04848,"Missing"
W04-2711,kingsbury-palmer-2002-treebank,0,0.0494269,"Verbs and frames are organized in several ways, following various criteria. Printable version. For those who prefer to have a paper version in hand. For a sample from the printable version, see the Appendix. XML version. Programmers can run sophisticated queries (e.g. based on XPATH query language) on this machine-tractable data, or use it in their applications. Structure of the XML file is defined using a DTD file (Document Type Definition), which naturally mirrors logical structure of the data (described in Sec. 3). 2 Similar Projects for English Verbs4 2.3 PropBank In the PropBank corpus ((Kingsbury and Palmer, 2002)) sentences are annotated with predicate-argument structure. The human annotators use the lexicon containing verbs and their ‘frames’ – lists of their possible complementations. The lexicon is called ‘Frame Files’. Frame Files are mapped to individual members of Levin classes. There is only a minimal specification of the connections between the argument types and semantic roles – in principle, a one-argument verb has arg0 in its frame, a two-argument verb has arg0 and arg1, etc. Frame Files store all the meanings of the verbs, with their description and examples. 3 Logical Structure of the VAL"
W04-2711,stranakova-lopatkova-zabokrtsky-2002-valency,0,0.0606339,"Missing"
W05-1518,W98-1118,0,0.0274576,"more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and"
W05-1518,P98-1029,0,0.0277036,"hich often leads to the development of several different approaches to the same problem. If these approaches are independent enough in terms of not producing the same kinds of errors, there is a hope that their combination can bring further improvement to the field. While improving any single approach gets more and more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parser"
W05-1518,A00-2018,0,0.103377,"rough N) to each word. In that sense, a dependency parser is similar to classifiers like POS taggers. Unless it deliberately fails to assign a parent to a word (or assigns 171 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 171–178, c Vancouver, October 2005. 2005 Association for Computational Linguistics Par- Author ser ec Eugene Charniak mc Michael Collins zž dz thr thl thp Zden k Žabokrtský Daniel Zeman Tomáš Holan Brief description Accuracy Tune Test A maximum-entropy inspired parser, home in constituency-based structures. English version described in Charniak (2000), Czech adaptation 2002 – 2003, unpublished. Uses a probabilistic context-free grammar, home in constituencybased structures. Described in (Haji et al., 1998; Collins et al., 1999). Purely rule-based parser, rules are designed manually, just a few lexical lists are collected from the training data. 2002, unpublished. A statistical parser directly modeling syntactic dependencies as word bigrams. Described in (Zeman, 2004). 83.6 85.0 81.7 83.3 74.3 76.2 73.8 75.5 71.0 Three parsers. Two of them use a sort of push-down automata and differ from each other only in the way they process the sentence"
W05-1518,N03-1004,0,0.0246351,"r more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and naïve Bayes), and two ways of their application to the (full) parsing: parser switching, and similarity switching. They were able to gain 1.6 con"
W05-1518,P99-1065,0,0.0923575,"Missing"
W05-1518,W02-1004,0,0.0198748,"ombination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). The authors have investigated two combination techniques (constituent voting and naïve Bayes), and two ways of their application to the (full) parsing: parser switching, and si"
W05-1518,A94-1016,0,0.0162445,"can bring further improvement to the field. While improving any single approach gets more and more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available. Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 19"
W05-1518,P98-1081,0,0.0278628,"Missing"
W05-1518,J01-2002,0,0.0363613,"Missing"
W05-1518,C00-1051,0,0.0362285,"Missing"
W05-1518,W99-0623,0,\N,Missing
W05-1518,C98-1078,0,\N,Missing
W05-1518,C98-1029,0,\N,Missing
W08-0325,A00-1031,0,0.00984455,"Procedure The structure of this section directly renders the sequence of blocks currently used for English-Czech translation in TectoMT. The intermediate stages of the translation process are illustrated in Figure 1; identifiers of the blocks affecting on the translation of the sample sentence are typeset in bold. 2.1 From English w-layer to English m-layer Segment the source English text into sentences. Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). B1: B2: 2.2 From English m-layer to English p-layer Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). B6: 2.3 From English p-layer to English a-layer In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition constructions. B10: Apply another heuristic rules for reattaching incorrectly positioned no"
W08-0325,H05-1066,0,0.0722218,"Missing"
W08-0325,W01-1406,0,0.0250838,"h as 1970’s, July 1, etc.). B40: Fix grammateme values in places where the English-Czech grammateme correspondence is not trivial (e.g., if an English gerund expression is translated using Czech subordinating clause, the B33: 2 The translation mapping from English formemes to Czech formemes was obtained as follows: we analyzed 10,000 sentence pairs from the WMT’08 training data up to the t-layer (using a tagger shipped with the PDT and parser (McDonald et al., 2005) for Czech), added formemes to t-trees on both sides, aligned the t-trees (using a set of weighted heuristic rules, similarly to (Menezes and Richardson, 2001)), and from the aligned t-node pairs extracted for each English formeme its most frequent Czech counterpart. 3 The dictionary was created by merging the translation dictionary from PCEDT ((Cuˇr´ın and others, 2004)) and a translation dictionary extracted from a part of the parallel corpus ˇ Czeng ((Bojar and Zabokrtsk´ y, 2006)) aligned at word-level by Giza++ ((Och and Ney, 2003)). 4 Czech nouns have grammatical gender which is (among others) important for resolving grammatical agreement. 169 tense grammateme has to be filled). B41: Negate verb forms where some arguments of the verbs bear neg"
W08-0325,W00-1427,0,0.0157742,"ediate stages of the translation process are illustrated in Figure 1; identifiers of the blocks affecting on the translation of the sample sentence are typeset in bold. 2.1 From English w-layer to English m-layer Segment the source English text into sentences. Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). B1: B2: 2.2 From English m-layer to English p-layer Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). B6: 2.3 From English p-layer to English a-layer In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition constructions. B10: Apply another heuristic rules for reattaching incorrectly positioned nodes. B11: Unify the way in which multiword prepositions (such as because of ) and subordinating conjunctions B7: 168 (such as provided that) are treated."
W08-0325,J03-1002,0,0.00479735,"Missing"
W08-0325,J93-2004,0,\N,Missing
W08-0325,2001.mtsummit-ebmt.4,0,\N,Missing
W08-0325,J03-4003,0,\N,Missing
W09-0422,bojar-etal-2008-czeng,1,0.888991,"Missing"
W09-0422,W08-0309,0,0.0625391,"ion of prepositional group would be difficult otherwise. After the capitalization of the beginning of each sentence (and each named entity instance), we obtain the final translation by flattening the surface tree. Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in English→Czech translation, as evaluated on the official WMT09 unseen test set. Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but “lower scoring” PC Translator this year and also in Callison-Burch et al. (2008). System BLEU Moses T 14.24 Moses T+C 13.86 Google 13.59 U. of Edinburgh 13.55 Moses T+C+C&T+T+G 84k 10.01 Eurotran XP 09.51 PC Translator 09.42 TectoMT 07.29 NIST 5.175 5.110 4.964 5.039 4.360 4.381 4.335 4.173 Rank -3.02 (4) – -2.82 (3) -3.24 (5) -2.81 (2) -2.77 (1) -3.35 (6) Table 3: Automatic scores and preliminary human rank for English→Czech translation. Systems in italics are provided for comparison only. Best results in bold. Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly 4.4 Preliminary Error Analysis because it is easier to tune them pr"
W09-0422,P05-1045,0,0.0043121,"ons: 15 33 43 . 5 Later, we found out that the grow-diag-final-and heuristic provides insignificantly superior results. 4 ˇ In some previous experiments (e.g.Zabokrtsk´ y et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. 2 126 with the option to resort to (2) an independent translation of lemma→lemma and tag→tag finished by a generation step that combines target-side lemma and tag to produce the final target-side form. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization). This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation. We use three language models in this setup (3-grams of forms, 3-grams of lemmas, and 10-grams of tags). Due to the increased complexity of the setup, we were able to train this model on 84k parallel sentences only (the Commentaries section) and we use the target-side"
W09-0422,P07-2045,1,0.010104,"naming conventions. However, we were unable to reliably determine the series number and the episode number from the file names. We employed a two-step procedure to automatically pair the TV series subtitle files. For every TV series: We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for English→Czech translation, and (2) a complex ˇ system in the TectoMT platform (Zabokrtsk´ y et al., 2008). 1. We clustered the files on both sides to remove duplicates 2. We found the best matching using a provisional translation dictionary. This proved to be a successful technique on a small sample of manually paired test data. The process was facilitated by the fact that the correct pairs of episodes usually share some named entities which the human translator chose to keep in the original English form. 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the"
W09-0422,J03-1002,0,0.00228104,"0.4 Table 2: Czech-English data sizes and sources. ∗ The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK ˇ ˇ LC536 and FP6-IST-5-034291-STP 52408/2008, MSMT CR (EuroMatrix). 1 www.opensubtitles.org and titulky.com Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125–129, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 125 2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0"
W09-0422,W07-1709,0,0.170323,"Missing"
W09-0422,W08-0325,1,0.861517,"emented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajiˇc and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer). The analysis was implemented using TectoMT, ˇ (Zabokrtsk´y et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajiˇc’s Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set ˇ of features as described in Nov´ak and Zabokrt"
W09-0422,J03-4003,0,\N,Missing
W09-0422,H05-1066,0,\N,Missing
W09-0422,W08-0319,1,\N,Missing
W09-0422,2008.eamt-1.16,1,\N,Missing
W09-3538,C02-1130,0,0.0249284,", which is structured as follows. In 2.2 Annotation NE Instances with Two-level NE Classification There is no generally accepted typology of Named Entities. One can see two trends: from the viewpoint of unsupervised learning, it is advantageous to have just a few coarse-grained categories (cf. the NE classification developed for MUC conferences or the classification proposed in (Collins and Singer, 1999), where only persons, locations, and organizations were distinguished), whereas those interested in semantically oriented applications prefer more informative (finer-grained) categories (e.g. (Fleischman and Hovy, 2002) with 1 http://ucnk.ff.cuni.cz The query is trivially motivated by the fact that NEs in Czech (as well as in many other languages) are often marked by capitalization of the first letter. Annotation of NEs in a corpus without such selection would lower the bias, but would be more expensive due to the lower density of NE instances in the annotated material. 2 194 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 194–201, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP ah - street numbers a - Numbers in addresses at - phone/fax numbers az - zip codes cb - volume numbe"
W09-3538,C96-1079,0,0.052211,"d recognizer outperforms the results previously reported for NE recognition in Czech. 1 2 Manually Annotated Corpus 2.1 Data Selection We have randomly selected 6000 sentences from the Czech National Corpus1 from the result of the query ([word="".*[a-z0-9]""] [word=""[A-Z].*""]). This query makes the relative frequency of NEs in the selection higher than the corpus average, which makes the subsequent manual annotation much more effective, even if it may slightly bias the distribution of NE types and their observed density.2 Introduction After the series of Message Understanding Conferences (MUC; (Grishman and Sundheim, 1996)), processing of named entities (NEs) became a well established discipline within the NLP domain, usually motivated by the needs of Information Extraction, Question Answering, or Machine Translation. For English, one can find literature about attempts at rule-based solutions for the NE task as well as machine-learning approaches, be they dependent on the existence of labeled data (such as CoNLL-2003 shared task data), unsupervised (using redundancy in NE expressions and their contexts, see e.g. (Collins and Singer, 1999)) or a combination of both (such as (Talukdar et al., 2006), in which labe"
W09-3538,H05-1066,0,0.0693406,"Missing"
W09-3538,W99-0613,0,0.0225405,"roduction After the series of Message Understanding Conferences (MUC; (Grishman and Sundheim, 1996)), processing of named entities (NEs) became a well established discipline within the NLP domain, usually motivated by the needs of Information Extraction, Question Answering, or Machine Translation. For English, one can find literature about attempts at rule-based solutions for the NE task as well as machine-learning approaches, be they dependent on the existence of labeled data (such as CoNLL-2003 shared task data), unsupervised (using redundancy in NE expressions and their contexts, see e.g. (Collins and Singer, 1999)) or a combination of both (such as (Talukdar et al., 2006), in which labeled data are used as a source of seed for an unsupervised procedure exploiting huge unlabeled data). A survey of research on named entity recognition is available in (Ekbal and Bandyopadhyay, 2008). There has been considerably less research done in the NE field in Czech, as discussed in ˇ c´ıkov´a et al., 2007b). Therefore we focus on (Sevˇ it in this paper, which is structured as follows. In 2.2 Annotation NE Instances with Two-level NE Classification There is no generally accepted typology of Named Entities. One can se"
W09-3538,W06-2919,0,0.0183837,"s (MUC; (Grishman and Sundheim, 1996)), processing of named entities (NEs) became a well established discipline within the NLP domain, usually motivated by the needs of Information Extraction, Question Answering, or Machine Translation. For English, one can find literature about attempts at rule-based solutions for the NE task as well as machine-learning approaches, be they dependent on the existence of labeled data (such as CoNLL-2003 shared task data), unsupervised (using redundancy in NE expressions and their contexts, see e.g. (Collins and Singer, 1999)) or a combination of both (such as (Talukdar et al., 2006), in which labeled data are used as a source of seed for an unsupervised procedure exploiting huge unlabeled data). A survey of research on named entity recognition is available in (Ekbal and Bandyopadhyay, 2008). There has been considerably less research done in the NE field in Czech, as discussed in ˇ c´ıkov´a et al., 2007b). Therefore we focus on (Sevˇ it in this paper, which is structured as follows. In 2.2 Annotation NE Instances with Two-level NE Classification There is no generally accepted typology of Named Entities. One can see two trends: from the viewpoint of unsupervised learning,"
W09-3538,W03-0434,0,0.068754,"Missing"
W09-3538,C02-1054,0,\N,Missing
W09-3538,W08-0325,1,\N,Missing
W09-3939,J94-4002,0,0.112674,"Missing"
W09-3939,J01-4004,0,0.282532,"tectogrammatical lemma and their joint feature; a joint feature of the pair of the tectogrammatical lemma of the candidate and the effective parent’s lemma of the anaphor; and a feature indicating whether the candidate and the anaphor are siblings.13 relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai , feature vectors Φ(c, ai ) are computed for all candidates c ∈ Cand(ai ) and passed to the trained decision tree. The candidate classified as positive is returned as the predicted antecedent. If there are more candidates"
W09-3939,P03-1023,0,0.122492,"Missing"
W09-3939,P02-1014,0,0.0825035,"nd their joint feature; a joint feature of the pair of the tectogrammatical lemma of the candidate and the effective parent’s lemma of the anaphor; and a feature indicating whether the candidate and the anaphor are siblings.13 relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai , feature vectors Φ(c, ai ) are computed for all candidates c ∈ Cand(ai ) and passed to the trained decision tree. The candidate classified as positive is returned as the predicted antecedent. If there are more candidates classified as positive, t"
W09-3939,P06-1006,0,0.0492369,"ving, etc. For the majority of English synsets (set of synonyms, the basic unit of EWN), the appropriate subset of these concepts are listed. Using the Inter Lingual Index that links the synsets of different languages, the set of relevant concepts can be found also for Czech lemmas. 14 − yˆi = argmax Φ(c, ai ) · → w c∈Cand(ai ) − The weights → w of the linear model are trained using a modification of the averaged perceptron al18 Besides C5.0, we plan to use also other classifiers in the future (especially Support Vector Machine, which is often employed in AR experiments, e.g. by Ng (2005) and Yang et al. (2006)) in order to study how the classifier choice influences the AR system performance on our data and feature sets. 280 gorithm (Collins, 2002). This is averaged perceptron learning with a modified loss function adapted to the ranking scenario. The loss function is tailored to the task of correctly ranking the true antecedent, the ranking of other candidates is irrelevant. The algorithm (without averaging the parameters) is listed as Algorithm 1. Note that the training instances where yi ∈ / Cand(ai ) were excluded from the training. fully indicate its antecedent, which can be any antecedent from"
W09-3939,D08-1067,0,0.0569992,"and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and 1 Currently one can see a growing interest in unsupervised techniques, e.g. Charniak and Elsner (2009) and Ng (2008). However, we make only a very tiny step in this direction: we use a probabilistic feature based on collocation counts in large unannotated data (namely in the Czech National Corpus). ∗ The work on this project was supported by the ˇ 1ET101120503 and grants MSM 0021620838, GAAV CR ˇ ˇ LC536, and GAUK 4383/2009 1ET201120505, MSMT CR Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 276 The most important result claimed in th"
W09-3939,W02-1001,0,\N,Missing
W09-3939,D08-1069,0,\N,Missing
W09-3939,H05-1004,0,\N,Missing
W09-3939,P95-1017,0,\N,Missing
W09-3939,W08-0325,1,\N,Missing
W09-3939,E09-1018,0,\N,Missing
W10-1730,N07-1008,0,0.0186993,"(SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of th"
W10-1730,W04-3250,0,0.149325,"mes) instead of word forms. In particular, our target-language tree model (TreeLM) predicts the probability of node’s lemma and formeme given its parent’s lemma and formeme. The optimal (lemma and formeme) labeling is found by tree-modified Viterbi search; ˇ for details see (Zabokrtsk´ y and Popel, 2009). 4 Experiments When included into the above described translation scenario, the MaxEnt TM outperforms the baseline TM, be it used together with or without TreeLM. The results are summarized in Table 1. The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004). In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models. 205 5 Conclusions Jan Hajiˇc. 2004. Disambiguation of Rich Inflection – Computational Morphology of Czech. Charles University – The Karolinum Press, Prague. We have introduced a maximum entropy translation model in dependency-based MT which enables exploiting a large number of feature functions in order to obtain more accurate translations. The BLEU evaluation p"
W10-1730,H05-1066,0,0.157359,"Missing"
W10-1730,P02-1038,0,0.0667274,"antic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combin"
W10-1730,W03-0420,0,0.0157367,"el data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abstracts from language-specific means (inflection, agg"
W10-1730,J96-1002,0,0.0200957,"model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The idea of using maximum entropy for constructing forward translation models is not new. It naturally allows to make use of various features potentially important for correct choice of targetlanguage expressions. Let us adopt a motivating example of such a feature from (Berger et al., 1996) (which contains the first usage of maxent translation model we are aware of): “If house appears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely [French] translation [of in].” Incorporating non-local features extracted from the source sentence into the standard noisychannel model in which only the backward translation model is available, is not possible. This drawback of the noisy-channel approach is typically compensated by using large target-language n-gram models, which can – in a result – play a role similar to that of a m"
W10-1730,W96-0213,0,0.208466,"wever, we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) t"
W10-1730,A00-2018,0,0.0559398,"d be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abs"
W10-1730,W07-1709,0,0.179472,"Missing"
W10-1730,P00-1006,0,0.0343477,"v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the tran"
W10-1730,P09-2037,1,0.49307,"; coreference is also resolved. Collapsing edges are depicted by wider lines in the Figure 1a. 3 Training the two models In this section we describe two translation models used in the experiments: a baseline translation model based on maximum likelihood estimates (3.2), and a maximum entropy based model (3.3). Both models are trained using the same data (3.1). In addition, we describe a target-language tree model (3.4), which can be combined with both the translation models using the Hidden Tree Markov Model approach and tree-modified Viterbi ˇ search, similarly to the approach of (Zabokrtsk´ y and Popel, 2009). 5. The transfer phase follows, whose most difficult part consists in labeling the tree with target-side lemmas and formemes5 (changes of tree topology are required relatively infrequently). See Figure 1c. 3.1 6. Finally, surface sentence shape (Figure 1d) is synthesized from the tectogrammatical tree, which is basically a reverse operation for the Data preprocessing common for both models We used Czech-English parallel corpus CzEng 0.9 ˇ (Bojar and Zabokrtsk´ y, 2009) for training the translation models. CzEng 0.9 contains about 8 million sentence pairs, and also their tectogrammatical analy"
W10-1730,D09-1023,0,0.0237895,"e implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as foll"
W10-1730,W08-0325,1,0.702701,"(1) tectogrammatics largely abstracts from language-specific means (inflection, agglutination, functional words etc.) of expressing non-lexical meanings and thus tectogrammatical trees are supposed to be highly similar across languages,2 (2) it enables a natural transfer factorization,3 (3) and local tree contexts in tectogrammatical trees carry more information (especially for lexical choice) than local linear contexts in the original sentences.4 In order to facilitate transfer of sentence ‘syntactization’, we work with tectogrammatical nodes ˇ enhanced with the formeme attribute (Zabokrtsk´ y et al., 2008), which captures the surface morphosyntactic form of a given tectogrammatical node in a compact fashion. For example, the value n:pˇred+4 is used to label semantic nouns that should appear in an accusative form in a prepositional group with the preposition pˇred in Czech. For English we use formemes such as n:subj (semantic noun (SN) in subject position), n:for+X (SN with preposition for), n:X+ago (SN with postposition ago), n:poss (possessive form of SN), v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), ad"
W11-2153,P05-1022,0,0.248854,"g to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU."
W11-2153,W07-2416,0,0.584308,"might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 However, the parser bottleneck of the dependency-based MT approach was observed also by other researchers (R"
W11-2153,P03-1054,0,0.0229106,"have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 Howeve"
W11-2153,W10-1730,1,0.897432,"Missing"
W11-2153,E06-1011,0,0.0143919,"ion that, n:sb – semantic noun in a subject position, n:for+X – semantic noun in a prepositional group introduced with preposition for, adj:attr – semantic adjective in an attributive position. 435 Involved Parsers We performed experiments with parsers from three families: graph-based parsers, transitionbased parsers, and phrase-structure parsers (with constituency-to-dependency postprocessing). 3.1 Graph-based Parser In graph-based parsing, we learn a model for scoring graph edges, and we search for the highest-scoring tree composed of the graph’s edges. We used Maximum Spanning Tree parser (Mcdonald and Pereira, 2006) which is capable of incorporating second order features (MST for short). 3.2 Transition-based Parsers Transition-based parsers utilize the shift-reduce algorithm. Input words are put into a queue and consumed by shift-reduce actions, while the output parser is gradually built. Unlike graph-based parsers, transition-based parsers have linear time complexity and allow straightforward application of non-local features. We included two transition-based parsers into our experiments: • Malt – Malt parser introduced by Nivre et al. (2007) 6 5 http://ucnk.ff.cuni.cz We used stackeager algorithm, libl"
W11-2153,H05-1066,0,0.288682,"Missing"
W11-2153,W07-1709,0,0.0270164,"Missing"
W11-2153,P07-1031,0,0.0148678,"d be parsed independently of the rest of the sentence. This was shown to improve not only parsing accuracy of the parenthesed word sequence (which is forced to remain in one subtree), but also the rest of the sentence.10 In our experiments, SentChunk is used only in combination with the three genuine dependency parsers. 4 Experiments and Evaluation 4.1 Data for Parsers’ Training and Evaluation The dependency trees needed for training the parsers and evaluating their UAS were created from the Penn Treebank data (enriched first with internal noun phrase structure applied via scripts provided by Vadas and Curran (2007)) by Penn Converter (Johansson and Nugues, 2007) with the -conll2007 option (PennConv for short). All the parsers were evaluated on the same data – section 23. All the parsers were trained on sections 02–21, except for the Stanford parser which was trained on sections 01–21. We were able to retrain the parser models only for MST and Malt. For the other parsers we used pretrained models available on the Internet: CJ’s default model ec50spfinal, Stanford’s wsjPCFG.ser.gz model, and 10 Edge length is a common feature in dependency parsers, so “deleting” parenthesed words may give higher scores to"
W11-2153,P09-2037,1,0.946992,"ependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors have already been addressed by improvements based on Hidden Markov Tree Models for lexical and syntactic choice as shown by ˇ Zabokrtsk´ y and Popel (2009), and by context sensitive translation models based on maximum entropy as described by Mareˇcek et al. (2010). Our study proceeds along two directions. First, we train two state-of-the-art dependency parsers on training sets with varying size. Second, we use five parsers based on different parsing techniques. In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric). The motivation behind the first set of experiments is that we can extrapolate the learning curve and try to pred"
W11-2153,W08-0325,1,0.950709,"study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU. 1 Introduction In the last years, statistical n-gram models dominated the field of Machine Translation (MT). However, their results are still far from perfect. Therefore we believe it makes sense to investigate alternative statistical approaches. This paper is focused on an analysis-transfer-synthesis translation system called TectoMT whose transfer representation has a shape of a deep-syntactic dependency tree. The system has ˇ been introduced by Zabokrtsk´ y et al. (2008). The translation direction under consideration is Englishto-Czech. It has been shown by Popel (2009) that the current accuracy of the dependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors h"
W11-2153,P11-2033,0,0.167717,"of pronouns (personal and possessive pronouns referring to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence"
W11-2153,zhang-etal-2004-interpreting,0,0.0936613,"Missing"
W11-3901,N09-1012,0,0.10418,"nguages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate"
W11-3901,P04-1061,0,0.226422,", stressing especially the treeness constraint. The best configuration is then applied to 19 languages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buch"
W11-3901,H05-1066,0,0.1583,"Missing"
W11-3901,D10-1118,0,0.614386,"d Dependency Parsing ˇ David Mareˇcek and Zdenˇek Zabokrtsk´ y Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics {marecek,zabokrtsky}@ufal.mff.cuni.cz Abstract The attachment scores are very high for English, for which the methods seems to be optimized, but the scores are quite low for some other languages. In this paper, we describe our new approach to unsupervised dependency parsing. Unlike DMV, it is not based on constituency trees, which cannot handle non-projectivities. We have been inspired rather by the experiment described in (Brody, 2010), in which the dependency parsing task is formulated as a problem of word alignment; every sentence is aligned with itself with one constraint: no word can be attached to itself. However, unlike (Brody, 2010), where the output structures might not be trees and could contain cycles, we introduce a sampling method with the acyclicity constraint. Our approach attempts at optimizing the overall probability of tree structures given the corpus. We perform the optimization using Gibbs sampling (Gilks et al., 1996). We employ several ways of incorporating prior knowledge about dependency trees into th"
W11-3901,P11-1067,0,0.0193213,"was measured both for CoarsePOS and FinePOS tags and evaluated with all three measures. We can see that CoarsePOS tags work better if we do not use SingleRoot constraint or NounRoot model. Adding NounRoot model improves the UAS by 8 percent. We choose the settings of the experiment number 10 (which uses all our improvements and constraints) as the best configuration for Czech. It has the highest UUAS score and the values of the other scores are very close to the maximum achieved values. • UUAS - undirected UAS (edge direction is disregarded), 6.4 • NED - neutral edge direction, introduced in (Schwartz et al., 2011), which treats not only a node’s gold parent and child as the correct answer, but also its gold grandparent. Learning curves It is useful to draw learning curves in order to see how well the learning algorithm can exploit additional data. Figure 4 shows the speed of growth of UAS for our best unsupervised configuration in 5 n. Initialization Tags Models Baseline configuration: 1 Random CoarsePOS Dep+Dist 2 Random FinePOS Dep+Dist Parsing with Maximum spanning tree algorithm: 3 Random CoarsePOS Dep+Dist 4 Random FinePOS Dep+Dist Using tree-sampling: 5 RandomTree CoarsePOS Dep+Dist 6 RandomTree"
W11-3901,W06-2920,0,0.80465,"004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature selection – we use words’ distance and their POS tags as the main indicators for predicting a dependency relation, • hard constraints – some knowledge on dependency tree properties (such as acyclicity) is difficult to represent by local models, therefore we implement it as a hard constraint in the sampling procedure, • corpus initialization – we study the effect of different initializations of trees in the corpus, 1 Robust Uns"
W11-3901,D11-1117,0,0.152154,"ate of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature select"
W11-3901,W11-0303,0,0.148644,"ate of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature select"
W11-3901,J93-2003,0,\N,Missing
W11-3901,D07-1096,0,\N,Missing
W12-0503,W06-2920,0,0.197803,"rics for comparing dependency parsing systems emerged. Labeled attachment score (LAS) and unlabeled attachment score (UAS). UAS studies the structure of a dependency tree and assesses whether the output has the correct head and dependency arcs. In addition to the structure score in UAS, LAS also measures the accuracy of the dependency labels on each arc. A third, but less common metric, is used to judge the percentage of sentences that are completely correct in regards to their LAS score. For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). 3.5 Weighting Currently we are applying four weighting algorithms to the graph structure. First we give each parser the same uniform weight. Second we examine weighting each parser output by the UAS score of the individual parser taken from our tuning data. Third we use plural voting weights (De Pauw et al., 2006) based on parser ranks from our tuning data. Due to the success of Plural voting, we try to exaggerate the differences in the parsers by using UAS10 weighting. All four of these are simple weighting techniques but even in their simplicity we can see the benefit of this type of combi"
W12-0503,P05-1022,0,0.143912,"uent Transformation While not a true dependency parser, one technique often applied is to take a state-of-the-art constituent parser and transform its phrase based output into dependency relations. This has been shown to also be state-of-the-art in accuracy for dependency parsing in English. In this paper we transformed the constituency structure into dependencies using the Penn Converter conversion tool (Johansson and Nugues, 2007). A version of this converter was used in the CoNLL shared task to create dependency treebanks as well. For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford’s (Klein and Manning, 2003) constituent parsers. In addition to these 5 parsers, we also report the accuracy of an Oracle Parser. This parser is simply the best possible parse of all the edges of the combined dependency trees. If the reference, gold standard, tree has an edge that any of the 5 parsers contain, we include that edge in the Oracle parse. Initially all nodes of the tree are attached to an artificial root in order to maintain connectedness. Since only edges that exist in a reference tree are added, the Oracle Parser maintains the acyclic constraint. This can be viewed"
W12-0503,C96-1058,0,0.0567727,"orm a better overall parse using prior knowledge of each individual parser. This is often done by different weighting or voting schemes. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman and ˇ Zabokrtsk´ y, 2005). However, the various techniques have generally examined similar parsers 19 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19–26, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics or parsers which have generated various different models. To the best of our knowledge, our experiments are the first to look at the accuracy and par"
W12-0503,P11-2125,0,0.181515,"pendency parsing often is in the realm of feature tweaking and optimization. The idea behind ensemble learning is to take the best of each parser as it currently is and allow the ensemble system to combine the outputs to form a better overall parse using prior knowledge of each individual parser. This is often done by different weighting or voting schemes. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman and ˇ Zabokrtsk´ y, 2005). However, the various techniques have generally examined similar parsers 19 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19–26, c Avignon, Franc"
W12-0503,D07-1097,0,0.0690386,"ous techniques have generally examined similar parsers 19 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19–26, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics or parsers which have generated various different models. To the best of our knowledge, our experiments are the first to look at the accuracy and part of speech error distribution when combining together constituent and dependency parsers that use many different techniques. However, POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with success. Other methods of parser combinations have shown to be successful such as using one parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008), in which Malt Parser was used as a feature to MST Parser. The result was a successful combination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework. 3 Methodology The following sections describe the process flow, choice of parsers, and datasets needed for others to recreate the results listed in this"
W12-0503,W07-2416,0,0.379892,"needed for others to recreate the results listed in this paper. Although we describe the specific parsers and datasets used in this paper, this process flow should work for any number of hybrid combinations of parsers and datasets. 3.1 Figure 1: General flow to create an ensemble parse tree. 3.2 Parsers To get a complete representation of parsers in our ensemble learning framework we use 5 of the most commonly used parsers. They range from graph-based approaches to transition-based approaches to constituent parsers. Constituency output is converted to dependency structures using a converter (Johansson and Nugues, 2007). All parsers are integrated into the Treex frameˇ work (Zabokrtsk´ y et al., 2008; Popel et al., 2011) using the publicly released parsers from the respective authors but with Perl wrappers to allow them to work on a common tree structure. Process Flow • Graph-Based: A dependency tree is a special case of a weighted edge graph that spawns from an artificial root and is acyclic. Because of this we can look at a large history of work in graph theory to address finding the best spanning tree for each dependency graph. In this paper we use MST Parser (McDonald et al., 2005) as an input to our ens"
W12-0503,P03-1054,0,0.00391054,"ncy parser, one technique often applied is to take a state-of-the-art constituent parser and transform its phrase based output into dependency relations. This has been shown to also be state-of-the-art in accuracy for dependency parsing in English. In this paper we transformed the constituency structure into dependencies using the Penn Converter conversion tool (Johansson and Nugues, 2007). A version of this converter was used in the CoNLL shared task to create dependency treebanks as well. For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford’s (Klein and Manning, 2003) constituent parsers. In addition to these 5 parsers, we also report the accuracy of an Oracle Parser. This parser is simply the best possible parse of all the edges of the combined dependency trees. If the reference, gold standard, tree has an edge that any of the 5 parsers contain, we include that edge in the Oracle parse. Initially all nodes of the tree are attached to an artificial root in order to maintain connectedness. Since only edges that exist in a reference tree are added, the Oracle Parser maintains the acyclic constraint. This can be viewed as the maximum accuracy that a hybrid ap"
W12-0503,J93-2004,0,0.040479,"3 Datasets Much of the current progress in dependency parsing has been a result of the availability of common data sets in a variety of languages, made available through the CoNLL shared task (Nivre et al., 2007a). This data is in 13 languages and 7 language families. Later shared tasks also released data in other genres to allow for domain adaptation. The availability of standard competition, gold level, data has been an important factor in dependency based research. For this study we use the English CoNLL data. This data comes from the Wall Street Journal (WSJ) section of the Penn treebank (Marcus et al., 1993). All parsers are trained on sections 02-21 of the WSJ except for the Stanford parser which uses sections 01-21. Charniak, Stanford and Zpar use pre-trained models ec50spfinal, wsjPCFG.ser.gz, english.tar.gz respectively. For testing we use section 23 of the WSJ for comparability reasons with other papers. This test data contains 56,684 tokens. For tuning we use section 22. This data is used for determining some of the weighting features. 3.4 Evaluation As an artifact of the CoNLL shared tasks competition, two standard metrics for comparing dependency parsing systems emerged. Labeled attachmen"
W12-0503,H05-1066,0,0.148817,"sing a converter (Johansson and Nugues, 2007). All parsers are integrated into the Treex frameˇ work (Zabokrtsk´ y et al., 2008; Popel et al., 2011) using the publicly released parsers from the respective authors but with Perl wrappers to allow them to work on a common tree structure. Process Flow • Graph-Based: A dependency tree is a special case of a weighted edge graph that spawns from an artificial root and is acyclic. Because of this we can look at a large history of work in graph theory to address finding the best spanning tree for each dependency graph. In this paper we use MST Parser (McDonald et al., 2005) as an input to our ensemble parser. To generate a single ensemble parse tree, our system takes N parse trees as input. The inputs are from a variety of parsers as described in 3.2. All edges in these parse trees are combined into a graph structure. This graph structure accepts weighted edges. So if more than one parse tree contains the same tree edge, the graph is weighted appropriately according to a chosen weighting algorithm. The weighting algorithms used in our experiments are described in 3.5. • Transition-Based: Transition-based parsing creates a dependency structure that is parameteriz"
W12-0503,P08-1108,0,0.0521948,"012 Association for Computational Linguistics or parsers which have generated various different models. To the best of our knowledge, our experiments are the first to look at the accuracy and part of speech error distribution when combining together constituent and dependency parsers that use many different techniques. However, POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with success. Other methods of parser combinations have shown to be successful such as using one parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008), in which Malt Parser was used as a feature to MST Parser. The result was a successful combination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework. 3 Methodology The following sections describe the process flow, choice of parsers, and datasets needed for others to recreate the results listed in this paper. Although we describe the specific parsers and datasets used in this paper, this process flow should work for any number of hybrid combinations of parsers and datasets. 3.1 Figure 1: General flow to create an ensemble parse t"
W12-0503,W11-2153,1,0.904615,"Missing"
W12-0503,N06-2033,0,0.101053,"y different weighting or voting schemes. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman and ˇ Zabokrtsk´ y, 2005). However, the various techniques have generally examined similar parsers 19 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19–26, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics or parsers which have generated various different models. To the best of our knowledge, our experiments are the first to look at the accuracy and part of speech error distribution when combining together constituent and dependency parsers that use many"
W12-0503,D07-1111,0,0.149314,"r voting schemes. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman and ˇ Zabokrtsk´ y, 2005). However, the various techniques have generally examined similar parsers 19 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19–26, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics or parsers which have generated various different models. To the best of our knowledge, our experiments are the first to look at the accuracy and part of speech error distribution when combining together constituent and dependency parsers that use many different techniques. Ho"
W12-0503,W08-0325,1,0.847841,"fic parsers and datasets used in this paper, this process flow should work for any number of hybrid combinations of parsers and datasets. 3.1 Figure 1: General flow to create an ensemble parse tree. 3.2 Parsers To get a complete representation of parsers in our ensemble learning framework we use 5 of the most commonly used parsers. They range from graph-based approaches to transition-based approaches to constituent parsers. Constituency output is converted to dependency structures using a converter (Johansson and Nugues, 2007). All parsers are integrated into the Treex frameˇ work (Zabokrtsk´ y et al., 2008; Popel et al., 2011) using the publicly released parsers from the respective authors but with Perl wrappers to allow them to work on a common tree structure. Process Flow • Graph-Based: A dependency tree is a special case of a weighted edge graph that spawns from an artificial root and is acyclic. Because of this we can look at a large history of work in graph theory to address finding the best spanning tree for each dependency graph. In this paper we use MST Parser (McDonald et al., 2005) as an input to our ensemble parser. To generate a single ensemble parse tree, our system takes N parse t"
W12-0503,W05-1518,1,0.873398,"Missing"
W12-0503,J11-1005,0,0.018236,"ion across each transition (K¨ubler et al., 2009). We make use Once the system has a weighted graph, it then uses an algorithm to find a corresponding tree structure so there are no cycles. In this set of experiments, we constructed a tree by finding the maximum spanning tree using ChuLiu/Edmonds’ algorithm, which is a standard choice for MST tasks. Figure 1 graphically shows the decisions one needs to make in this framework to create an ensemble parse. 20 of Malt Parser (Nivre et al., 2007b), which in the shared tasks was often tied with the best performing systems. Additionally we use Zpar (Zhang and Clark, 2011) which is based on Malt Parser but with a different set of non-local features. • Constituent Transformation While not a true dependency parser, one technique often applied is to take a state-of-the-art constituent parser and transform its phrase based output into dependency relations. This has been shown to also be state-of-the-art in accuracy for dependency parsing in English. In this paper we transformed the constituency structure into dependencies using the Penn Converter conversion tool (Johansson and Nugues, 2007). A version of this converter was used in the CoNLL shared task to create de"
W12-0503,N10-1091,0,\N,Missing
W12-0503,D07-1096,0,\N,Missing
W12-1911,N09-1012,0,0.137884,"Missing"
W12-1911,P04-1061,0,0.123137,"(4) where ti is part-of-speech tag of the word on the position i, c−i (“ti , fi ”) stands for the count of words with PoS tag ti and fertility fi in the history, and P0 is a prior probability for the given fertility which depends on the total number of node dependents denoted by |fi |(the sum of numbers of left and right dependents): P0 (fi ) = 1 2|fi |+1 (5) This prior probability has a nice property: for a given number of nodes, the product of fertility probabilities over all the nodes is equal for all possible dependency trees. This ensures balance of this model during inference. 5 In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated. Given a certain head, all its dependents in left direction are generated first, then the STOP sign in that direction, then all its right dependents and then STOP in the other direction. This process continues recursively for all generated dependents. 6 For example, fertility “1-3” means that the node has one left and three right dependents, fertility “0-0” indicates that it is a leaf. 7 If a specific fertility has been frequent for a given P"
W12-1911,majlis-zabokrtsky-2012-language,1,0.867069,"Missing"
W12-1911,D11-1117,0,0.0206557,"e occurs at a particular location in the corpus. This counts are collected over the whole corpus with the collection-rate 0.01.12 When the sampling is finished, the final dependency trees are built using such edges that belonged to the most frequent ones during the sampling. We employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them.13 Tree projectivity is not guaranteed by the MST algorithm. 5 Experiments We evaluated our parser on 10 treebanks included in the WILS shared-task data. Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011), the tuning experiments were performed on English only. We used 12 11 dog)(was) in ((the) dog) was (in)((the) dog) was) in ((the) dog) was (in ((the) dog) was (in ((the) After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 13 The weights of edges needed in MST algorithm correspond to the number of times they were present during the sampling. language tokens (mil.) Arabic 19.7 Basque 14.1 Czech 20.3 Danish 15.9 Dutch 27.1 language tokens (mil.) English 85.0 Portuguese 31.7 Slovenian 13.7 Swedish 19.2 αe = 0.01, Table 2: Wikipedia texts statist"
W12-1911,A00-1031,0,\N,Missing
W12-3132,hajic-etal-2012-announcing,1,0.802661,"Missing"
W12-3132,W04-3250,0,0.264493,"Missing"
W12-3132,W06-1606,0,0.0610868,"Missing"
W12-3132,W10-1730,1,0.899159,"Missing"
W12-3132,W01-1406,0,0.0190747,"actic description, mainly within valency lexicons, starting probably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysi"
W12-3132,P02-1040,0,0.0829475,". 6.1 Czech Synthesis The synthesis phase of the TectoMT system relies heavily on the information included in formemes, as its rule-based blocks use solely formemes and grammar rules to gradually change a deep tree node into a surface subtree. To directly measure the suitability of our changes for the synthesis stage of the TectoMT system, we used a Czech-to-Czech round trip—deep analysis of Czech PDT 2.0 development set sentences using the CzEng 1.0 pipeline (Bojar et al., 2012b), followed directly by the synthesis part of the TectoMT system. The results were evaluated using the BLEU metric (Papineni et al., 2002) with the original sentences as reference; they indicate a higher suitability of the new formemes for deep Czech synthesis (see Table 2). 6.2 Version Original formemes Revised formemes BLEU 0.6818 0.7092 Table 2: A comparison of formeme versions in Czech-toCzech round trip. Version Original formemes Revised formemes BLEU 0.1190 0.1199 Table 3: A comparison of formeme versions in Englishto-Czech TectoMT translation on the WMT12 test set. two translation scenarios—one using the original formemes and the second using the revised formemes in the formeme-to-formeme translation model. Due to time re"
W12-3132,W11-2153,1,0.890651,"Missing"
W12-3132,P05-1034,0,0.0232767,"robably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory"
W12-3132,W08-0325,1,0.921231,"Missing"
W12-3132,P09-2037,1,0.875964,"oMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986), proceeding over two layers of structural description, from shallow (analytical) to deep (tectogrammatical) (see Section 3.1). The transfer phase of the system is based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It is factorized into three subtasks: lemma, formeme and grammatemes translation (see Sections 3.2 and 3.3). The subsequent generation phase consists of rulebased components that gradually change the deep target language representation into a shallow one, which is then converted to text (cf. Section 6.1). The version of TectoMT submitted to WMT122 builds upon the WMT11 version. Several rule-based components were slightly refined. However, most of the effort was devoted to creating a better and bigger parallel treebank—CzEng 1.03 (Bojar et al., 2012b), and re-training the statistical componen"
W12-3132,W09-0422,1,\N,Missing
W12-3132,2001.mtsummit-ebmt.4,0,\N,Missing
W12-3132,bojar-etal-2012-joy,1,\N,Missing
W12-3410,W06-2920,0,0.344999,"Missing"
W12-3410,C96-1058,0,0.215638,"A dependency tree is a special case of a dependency graph that spawns from an artificial root, is connected, follows a single-head constraint and is acyclic. Because of this we can look at a large history of work in graph theory to address finding the best spanning tree for each dependency graph. The most common form of this type of dependency parsing is Graph-Based parsing also called arc-factored parsing and deals with the parameterization of the edge weights. The main drawback of these methods is that for projective trees, the worst case scenario for most methods is a complexity of O(n3 ) (Eisner, 1996). However, for non-projective parsing Chu-Liu-Edmond’s algorithm has a complexity of O(n2 ) (McDonald et al., 2005). The most common tool for doing this is MST parser (McDonald et al., 2005). For this parser we generate two models, one projective and one non-projective to use in our ensemble system. Transition-based parsing creates a dependency structure that is parameterized over the transitions. This is closely related to shift-reduce constituency parsing algorithms. The benefit of transition-based parsing is the use greedy algorithms which have a linear time complexity. However, due to the"
W12-3410,W12-0503,1,0.847154,"Missing"
W12-3410,P11-2125,0,0.0753138,"rection for lesser resourced languages, often a large portion of morphologically rich languages. Ensemble methods are robust as data sizes grow, since the classifier can easily be retrained with additional data and the ensemble model chooses the best model on an edge by edge basis. This cost is substantially less than retraining multiple dependency models. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. Parser combination with dependency trees have been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, ˇ 2007; Zeman and Zabokrtsk´ y, 2005; Søgaard and Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with an SVM cla"
W12-3410,D07-1097,0,0.0631975,"ferent levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. Parser combination with dependency trees have been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, ˇ 2007; Zeman and Zabokrtsk´ y, 2005; Søgaard and Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with an SVM classifier with success, however we believe our work is novel in its use of an SVM classifier solely on model agreements. Other methods of parse combinations have shown to be successful such as using one parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008; Martins et al., 2008), in which Malt Parser was used as a feature to MST Parser. Few attempts were reported in the literature on the development of a treebank for Tamil. Our experiments are based on the openly available treebank ˇ (TamilTB) (Ramasa"
W12-3410,D08-1017,0,0.0199136,"d Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with an SVM classifier with success, however we believe our work is novel in its use of an SVM classifier solely on model agreements. Other methods of parse combinations have shown to be successful such as using one parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008; Martins et al., 2008), in which Malt Parser was used as a feature to MST Parser. Few attempts were reported in the literature on the development of a treebank for Tamil. Our experiments are based on the openly available treebank ˇ (TamilTB) (Ramasamy and Zabokrtsk´ y, 2012). Development of TamilTB is still in progress and the initial results for TamilTB appeared in (Ramasamy and ˇ Zabokrtsk´ y, 2011). Previous parsing experiments in Tamil were done using a rule based approach which utilized morphological tagging and identification of clause boundaries to parse the sentences. The results were also reported for Malt"
W12-3410,H05-1066,0,0.108807,"d, follows a single-head constraint and is acyclic. Because of this we can look at a large history of work in graph theory to address finding the best spanning tree for each dependency graph. The most common form of this type of dependency parsing is Graph-Based parsing also called arc-factored parsing and deals with the parameterization of the edge weights. The main drawback of these methods is that for projective trees, the worst case scenario for most methods is a complexity of O(n3 ) (Eisner, 1996). However, for non-projective parsing Chu-Liu-Edmond’s algorithm has a complexity of O(n2 ) (McDonald et al., 2005). The most common tool for doing this is MST parser (McDonald et al., 2005). For this parser we generate two models, one projective and one non-projective to use in our ensemble system. Transition-based parsing creates a dependency structure that is parameterized over the transitions. This is closely related to shift-reduce constituency parsing algorithms. The benefit of transition-based parsing is the use greedy algorithms which have a linear time complexity. However, due to the greedy algorithms, longer arc parses can cause error propagation across each transition (K¨ubler et al., 2009). We"
W12-3410,P08-1108,0,0.047611,"krtsk´ y, 2005; Søgaard and Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with an SVM classifier with success, however we believe our work is novel in its use of an SVM classifier solely on model agreements. Other methods of parse combinations have shown to be successful such as using one parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008; Martins et al., 2008), in which Malt Parser was used as a feature to MST Parser. Few attempts were reported in the literature on the development of a treebank for Tamil. Our experiments are based on the openly available treebank ˇ (TamilTB) (Ramasamy and Zabokrtsk´ y, 2012). Development of TamilTB is still in progress and the initial results for TamilTB appeared in (Ramasamy and ˇ Zabokrtsk´ y, 2011). Previous parsing experiments in Tamil were done using a rule based approach which utilized morphological tagging and identification of clause boundaries to parse the sentences. The results were"
W12-3410,ramasamy-zabokrtsky-2012-prague,1,0.756212,"Missing"
W12-3410,N06-2033,0,0.19672,"e model chooses the best model on an edge by edge basis. This cost is substantially less than retraining multiple dependency models. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. Parser combination with dependency trees have been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, ˇ 2007; Zeman and Zabokrtsk´ y, 2005; Søgaard and Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser models with an SVM classifier with success, however we believe our work is novel in its use of an SVM classifier solely on model agreements. Other methods of parse combinations have shown to be successful such as using one parser to generate featur"
W12-3410,D07-1111,0,0.140952,"Missing"
W12-3410,C10-1120,0,0.0392738,"Missing"
W12-3410,N10-1091,0,0.050134,"ations are an appropriate direction for lesser resourced languages, often a large portion of morphologically rich languages. Ensemble methods are robust as data sizes grow, since the classifier can easily be retrained with additional data and the ensemble model chooses the best model on an edge by edge basis. This cost is substantially less than retraining multiple dependency models. 2 Related Work Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. Parser combination with dependency trees have been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, ˇ 2007; Zeman and Zabokrtsk´ y, 2005; Søgaard and Rishøj, 2010). (Sagae and Lavie, 2006; Green and ˇ Zabokrtsk´ y, 2012) differ in part since their method guarantees a tree while our system can, in some situations, produce a forest. POS tags were used in parser combination in (Hall et al., 2007) for combining a set of Malt Parser"
W12-3410,W05-1518,1,0.863473,"Missing"
W12-3903,P08-1084,0,0.0219926,"tic languages). Though both supervised (Koskenniemi, 1983) and unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2005) are extensively studied for morphological segmentation, unsupervised techniques have the appeal of application to multilingual data with cost effective manner. Within unsupervised paradigm, various methods have been explored. Minimum Description Length (MDL) (Goldsmith, 2001; Creutz and Lagus, 2005) based approaches are most popular in which the best segmentation corresponds to the compact representation of morphology and the resulting lexicon. (Goldwater et al., 2009; Snyder and Barzilay, 2008) attempted word segmentation and joint segmentation of related languages using Bayesian approach. (Demberg, 2007; Dasgupta and Ng, 2007) applied various probabilistic measures to discover affixes of wordforms. (Naradowsky and Goldwater, 2009; Yarowsky and Wicentowski, 2000) explored ways to model orthographic rules of wordforms. In this work, we are mainly going to focus on Bayesian approach. Bayesian approaches provide natural way of modeling subjective knowledge as well as separating problem specific aspects from general aspects. In the case of agglutinative lan18 Proceedings of the First Wo"
W12-3903,P00-1027,0,0.0459577,"ffective manner. Within unsupervised paradigm, various methods have been explored. Minimum Description Length (MDL) (Goldsmith, 2001; Creutz and Lagus, 2005) based approaches are most popular in which the best segmentation corresponds to the compact representation of morphology and the resulting lexicon. (Goldwater et al., 2009; Snyder and Barzilay, 2008) attempted word segmentation and joint segmentation of related languages using Bayesian approach. (Demberg, 2007; Dasgupta and Ng, 2007) applied various probabilistic measures to discover affixes of wordforms. (Naradowsky and Goldwater, 2009; Yarowsky and Wicentowski, 2000) explored ways to model orthographic rules of wordforms. In this work, we are mainly going to focus on Bayesian approach. Bayesian approaches provide natural way of modeling subjective knowledge as well as separating problem specific aspects from general aspects. In the case of agglutinative lan18 Proceedings of the First Workshop on Multilingual Modeling, pages 18–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics guages, the number of morphemes in a word as well as morph length play a major role in morphological process. The main rationale for this"
W12-3903,N09-1024,0,0.0298385,"Missing"
W12-3903,P11-1090,0,0.0418438,"Missing"
W12-3903,J01-2001,0,0.0727419,"inguistic sense, morphological segmentation is also widely used as an easy alternative to full fledged morphological analysis. In this paper we mainly focus on the task of morphological segmentation. The main task in morphological segmentation is to segment the given token or wordform into set of morphs or identifying the location of each morpheme boundary within the token. Morphological segmentation is most suitable for agglutinative languages (such as Finnish or Turkish) than fusional languages (such as Semitic languages). Though both supervised (Koskenniemi, 1983) and unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2005) are extensively studied for morphological segmentation, unsupervised techniques have the appeal of application to multilingual data with cost effective manner. Within unsupervised paradigm, various methods have been explored. Minimum Description Length (MDL) (Goldsmith, 2001; Creutz and Lagus, 2005) based approaches are most popular in which the best segmentation corresponds to the compact representation of morphology and the resulting lexicon. (Goldwater et al., 2009; Snyder and Barzilay, 2008) attempted word segmentation and joint segmentation of related languages u"
W12-3903,P03-1036,0,0.0457454,"Missing"
W12-3903,P01-1063,0,0.0698513,"Missing"
W12-3903,N07-1020,0,0.0374611,"Missing"
W12-3903,P06-1085,0,0.116525,"d Applied Linguistics Faculty of Mathematics and Physics, Charles University in Prague {ramasamy, zabokrtsky}@ufal.mff.cuni.cz Sowmya Vajjala Seminar f¨ur Sprachwissenschaft Universit¨at T¨ubingen sowmya@sfs.uni-tuebingen.de Abstract Morph length is one of the indicative feature that helps learning the morphology of languages, in particular agglutinative languages. In this paper, we introduce a simple unsupervised model for morphological segmentation and study how the knowledge of morph length affect the performance of the segmentation task under the Bayesian framework. The model is based on (Goldwater et al., 2006) unigram word segmentation model and assumes a simple prior distribution over morph length. We experiment this model on two highly related and agglutinative languages namely Tamil and Telugu, and compare our results with the state of the art Morfessor system. We show that, knowledge of morph length has a positive impact and provides competitive results in terms of overall performance. 1 Introduction Most of the NLP tasks require one way or another the handling of morphology. The task becomes very crucial when the language in question is morphologically rich as is the case in many Indo-European"
W12-3903,N04-4015,0,0.0409137,"on two highly related and agglutinative languages namely Tamil and Telugu, and compare our results with the state of the art Morfessor system. We show that, knowledge of morph length has a positive impact and provides competitive results in terms of overall performance. 1 Introduction Most of the NLP tasks require one way or another the handling of morphology. The task becomes very crucial when the language in question is morphologically rich as is the case in many Indo-European languages. The application of morphology is evident in applications such as Statistical Machine Translation (SMT) (Lee, 2004), dependency parsing, information retrieval and so on. Apart from the morphological analysis as in the traditional linguistic sense, morphological segmentation is also widely used as an easy alternative to full fledged morphological analysis. In this paper we mainly focus on the task of morphological segmentation. The main task in morphological segmentation is to segment the given token or wordform into set of morphs or identifying the location of each morpheme boundary within the token. Morphological segmentation is most suitable for agglutinative languages (such as Finnish or Turkish) than f"
W12-3903,P07-1116,0,\N,Missing
W12-5611,W11-2101,1,0.850569,"Missing"
W12-5611,W10-1703,0,0.0197028,"Missing"
W12-5611,P05-1033,0,0.0246605,"t Dev News 108,332 12,037 1000 Cinema 34,690 3854 1000 Bible 26,884 2987 1000 All 171,706 19,078 1000 Corpus Training data English Tamil 2.9M 2.1M 3.4M 3.8M 529K 353K 605K 610K 668K 352K 733K 731K 4.1M 2.9M 4.8M 5.3M Test data English Tamil 328K 247K 386K 447K 60K 40K 68K 69K 94K 50K 103K 103K 459K 318K 534K 586K Dev data English Tamil 27K 20K 32K 37K 15K 10K 18K 18K 22K 12K 24K 24K 23K 16K 27K 30K Table 1: Corpus statistics. For each corpus, the upper and lower row correspond to the number of tokens before and after the suffix splitting. 4.2 Systems Used We use phrase-based and hierarchical (Chiang, 2005) MT systems as implemented by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each sett"
W12-5611,W01-1409,0,0.579788,"propose morphological processing aimed at reducing data sparsity. In Section 4, we describe our English-Tamil parallel corpora collection and the system configurations we use. In Section 5, we report the results and analyze them in Section 6. 2 Related Work Research into SMT involving Tamil language is not very common, the main reason perhaps being the lack of parallel corpora. Nevertheless there have been efforts for other Indian languages such as Hindi (Udupa U. and Faruquie, 2004), (Ramanathan et al., 2008) and (Bojar et al., 2008). The earliest work that appeared on English-Tamil SMT was (Germann, 2001) which described building a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new"
W12-5611,P07-2045,1,0.0102404,"0 3854 1000 Bible 26,884 2987 1000 All 171,706 19,078 1000 Corpus Training data English Tamil 2.9M 2.1M 3.4M 3.8M 529K 353K 605K 610K 668K 352K 733K 731K 4.1M 2.9M 4.8M 5.3M Test data English Tamil 328K 247K 386K 447K 60K 40K 68K 69K 94K 50K 103K 103K 459K 318K 534K 586K Dev data English Tamil 27K 20K 32K 37K 15K 10K 18K 18K 22K 12K 24K 24K 23K 16K 27K 30K Table 1: Corpus statistics. For each corpus, the upper and lower row correspond to the number of tokens before and after the suffix splitting. 4.2 Systems Used We use phrase-based and hierarchical (Chiang, 2005) MT systems as implemented by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each settings, we report BLEU (Papineni et al., 2002) scor"
W12-5611,N04-4015,0,0.0325478,"a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil"
W12-5611,J04-2003,0,0.0355256,"rpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil verb, in addition to the root bearing the le"
W12-5611,P02-1040,0,0.0834182,"nted by Koehn et al. (2007) for our experiments. We use the default system settings for all experiments and report results for individual datasets as well as for the entire training data, All. 4.3 Examined Configurations Our experiments consist of the following settings for both phrase based and hierarchical systems: • baseline: The default, no suffix splitting. • targetmor : No change in English side of the data. Our suffix splitter is run on Tamil. • source+targetmor : Both the English and Tamil suffix splitters are run on the respective sides of the data. For each settings, we report BLEU (Papineni et al., 2002) scores in three variations: BLEUsu f f _sep , BLEUsu f f _r e j and BLEUst em_onl y . In the case of BLEUsu f f _sep evaluation, both the reference and hypothesis translations are suffixseparated before the evaluation, allowing a better match with the reference but also risking more false positives. The BLEUsu f f _r e j evaluation corresponds to what Tamil readers would like to see: the suffixes are rejoined (if they were separated) prior to evaluation. BLEUst em_onl y ignores suffixes altogether, both hypothesis and reference translations contain only stem words. Manual sentence level ranki"
W12-5611,W12-3152,0,0.0497214,"Missing"
W12-5611,I08-1067,0,0.026118,"l differences contribute to data sparsity. We attempt to address both issues in this paper. In Section 3, we propose morphological processing aimed at reducing data sparsity. In Section 4, we describe our English-Tamil parallel corpora collection and the system configurations we use. In Section 5, we report the results and analyze them in Section 6. 2 Related Work Research into SMT involving Tamil language is not very common, the main reason perhaps being the lack of parallel corpora. Nevertheless there have been efforts for other Indian languages such as Hindi (Udupa U. and Faruquie, 2004), (Ramanathan et al., 2008) and (Bojar et al., 2008). The earliest work that appeared on English-Tamil SMT was (Germann, 2001) which described building a small English-Tamil parallel corpus as well as an SMT system. So far, the efforts for building English-Tamil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose ste"
W12-5611,P09-1090,0,0.0146616,"amil parallel corpora are moderate and the readily available parallel data amount just to a few thousand sentences. One of our goals in this work is to perform experiments with a larger corpus that we collect on our own (see Section 4.1) from various web sources. The main focus of this work is to address morphological differences between English and Tamil propose steps that improve the performance of SMT systems. Applying morphological processing to SMT is not new, the idea goes back to (Lee, 2004) for Arabic-English or (Nießen and Ney, 2004) for German-English. (Ramanathan et al., 2008) and (Ramanathan et al., 2009) are the first to experiment an Indian language, namely in English-Hindi translation. We apply similar techniques to English-Tamil pair. 3 Suffix Splitting English and Tamil morphologies follow different inflectional patterns. While English morphology can be adequately described with a few morphological suffixes, thousands of wordforms can be built from a single root in Tamil. As expected, verbs and nouns are the main productive parts of speech in Tamil. For example, a Tamil verb, in addition to the root bearing the lexical information, can include suffixes corresponding to person, number, gen"
W12-5611,2009.mtsummit-papers.7,0,\N,Missing
W13-2805,W12-3410,1,0.872083,"Missing"
W13-2805,P11-2125,0,0.0144783,"ated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures. The structures may have more complexity due to their coordination structure or attachment rules. As dependency parses are basic structures in which other systems are built upon, it would seem more reasonable to judge these parsers down the NLP pipeline. 1 Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman ˇ ˇ and Zabokrtsk´ y, 2005; Holan and Zabokrtsk´ y, 2006). Other methods of parser combinations have shown to be successful such as using one 2.1 Methodology Annotation To find the maximum effect that dependency parsing can have on the NLP pipeline,"
W13-2805,W06-2920,0,0.167138,"Missing"
W13-2805,W07-2416,0,0.0697458,"Missing"
W13-2805,P03-1054,0,0.0105348,"Missing"
W13-2805,D07-1111,0,0.0192026,"parsers down the NLP pipeline. 1 Ensemble learning (Dietterich, 2000) has been used for a variety of machine learning tasks and recently has been applied to dependency parsing in various ways and with different levels of success. (Surdeanu and Manning, 2010; Haffari et al., 2011) showed a successful combination of parse trees through a linear combination of trees with various weighting formulations. To keep their tree constraint, they applied Eisner’s algorithm for reparsing (Eisner, 1996). Parser combination with dependency trees has been examined in terms of accuracy (Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zeman ˇ ˇ and Zabokrtsk´ y, 2005; Holan and Zabokrtsk´ y, 2006). Other methods of parser combinations have shown to be successful such as using one 2.1 Methodology Annotation To find the maximum effect that dependency parsing can have on the NLP pipeline, we annotated English dependency trees to form a gold standard. Annotation was done with two annotators using a tree editor, Tred (Pajas and Fabian, 2011), on data that was preprocessed using MST parser. For the annotation of our gold data, we used the standard developed by the Prague Dependency Treebank (PDT) (Hajiˇc, 1998). PDT is annotate"
W13-2805,H05-1066,0,0.182356,"Missing"
W13-2805,P08-1108,0,0.0211745,"ty in Prague Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics Prague, Czech Republic {green,zabokrtsky}@ufal.mff.cuni.cz Abstract We show results from 7 individual parsers, including dependency and constituent parsers, and 3 ensemble parsing techniques with their overall effect on a Machine Translation system, Treex, for English to Czech translation. We show that parsers’ UAS scores are more correlated to the NIST evaluation metric than to the BLEU Metric, however we see increases in both metrics. parser to generate features for another parser. This was shown in (Nivre and McDonald, 2008), in which Malt Parser was used as a feature to MST Parser. The result was a successful combination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework. We will use three ensemble approaches. First a fixed weight ensemble approach in which edges are added together in a weighted graph. Second, we added the edges using weights learned through fuzzy clustering based on POS errors. Third, we will use a meta-classifier that uses an SVM to predict the correct model for edge using only model agreements without any linguistic information a"
W13-2805,W05-1518,1,0.77942,"Missing"
W13-2805,W03-3017,0,0.0561262,"lation Data Sets All the parsers were trained on sections 02-21 of the WSJ, except the Stanford parser which also uses section 01. We retrained MST and Malt parsers and used pre-trained models for the other parsers. Machine translation data was used from WMT 2010, 2011, and 2012. Using our gold standard we are able to evaluate the effectiveness of different parser types from graph-base, transition-based, constituent conversion to ensemble approaches on the 2012 data while finding data trends using previous years data. • Malt: Implementation of Nivre’s Malt Parser trained on the Penn Treebank (Nivre, 2003) • Malt with chunking: Same implementation as above but with chunked parsing • ZPar: Yue Zhang’s statistical parser. We used the pretrained English model (english.tar.gz) available on the ZPar website for all tests (Zhang and Clark, 2011) • Charniak: A constituent based parser (ec50spfinal model) in which we transform 1 When available the data and description will be at www.nathangreen.com/wmtdata 20 the correct head and dependency arcs (Buchholz and Marsi, 2006). We report UAS scores for each parser on section 23 of the WSJ. the results using the Pennconverter (Johansson and Nugues, 2007) • S"
W13-2805,P02-1040,0,0.0864745,"uation scores, BLEU and NIST. We examine parser accuracy using UAS. This paper compares a machine translation system integrating 10 different parsing systems against each other, using the below metrics. The BLEU (BiLingual Evaluation Understudy) and NIST(from the National Institute of Standards and Technology), are automatic scoring mechanisms for machine translation that are quick and can be reused as benchmarks across machine translation tasks. BLEU and NIST are calculated as the geometric mean of n-grams multiplied by a brevity penalty, comparing a machine translation and a reference text (Papineni et al., 2002). NIST is based upon the BLEU n-gram approach however it is also weighted towards discovering more “informative” n-grams. The more rare an n-gram is, the higher the weight for a correct translation of it will be. Made a standard in the CoNLL shared tasks competition, UAS studies the structure of a dependency tree and assesses how often the output has 3.3 3.3.1 Parsing Errors Effect on MT MT Results in WMT with Ensemble Parsers WMT 2010 As seen in Table 1, the highest resulting BLEU score for the 2010 data set is from the fixed weight ensemble system. The other two ensemble systems are beaten b"
W13-2805,W11-2153,1,0.861361,"Missing"
W13-2805,W06-1608,0,0.0161101,"ombination of a transition-based and graph-based parser, but did not address adding other types of parsers into the framework. We will use three ensemble approaches. First a fixed weight ensemble approach in which edges are added together in a weighted graph. Second, we added the edges using weights learned through fuzzy clustering based on POS errors. Third, we will use a meta-classifier that uses an SVM to predict the correct model for edge using only model agreements without any linguistic information added. Parsing accuracy and machine translation has been examined in terms of BLEU score (Quirk and Corston-Oliver, 2006). However, we believe our work is the first to examine the NLP pipeline for ensemble parsing for both dependency and constituent parsers as well as examining both BLEU and NIST scores’ relationship to their Unlabeled Accuracy Score(UAS). Introduction 2 Dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures. The structures may have more complexity due to their coordination structure or attachment rules. As dependency parses are basic structures in which other systems are built upon, it"
W13-2805,C96-1058,0,\N,Missing
W13-2805,N10-1091,0,\N,Missing
W13-2805,J11-1005,0,\N,Missing
W13-2805,N06-2033,0,\N,Missing
W13-2805,Y12-1014,1,\N,Missing
W13-2805,C12-1052,0,\N,Missing
W13-2805,W12-0503,1,\N,Missing
W13-3307,N03-1017,0,0.00364996,"n gender and number with its antecedent. There are cases, though, when demonstrative pronoun to fits better and grammatical categories are not propagated. Keeping grammatical information on its antecedent may in this case result in probably not harmful but still superfluous partitioning the training data. Our work deals also with the second issue, however, at the cost of partial manual annotating. The most significant difference of our work compared to the abovementioned ones lies in the MT systems used. Whereas they tackle the issue of pronoun translation within the Moses phrasebased system (Koehn et al., 2003), we rely on the translation via deep syntax with TectoMT system ˇ (Zabokrtsk´ y et al., 2008). Our approach is more linguistically oriented, working with deep syntactic representations and postponing the decisions about the concrete forms to the synthesis stage. (2) Peter has discussed the issue with his supervisor and it helped him to finish the article. Pleonastic it has no antecedent in the preceding/following context and its presence is imposed only by the syntactic rules of English. (3) It is difficult to give a good example. From the perspective of Czech, there are also three prevailing"
W13-3307,P12-1041,0,0.0301211,"of the automatically annotated6 Czech-English corpus CzEng 1.0 (Bojar et al., 2012) that comprises more than 15 million sentence pairs. In the manner described in Section 5.1, we collected co-occurrence counts between a functor that the given it possesses concatenated with a lemma of its verbal parent and a Czech counterpart having the same functor (denoted as csit). We filtered out all occurrences where csit was neither #PersPron nor ten. Then, for both values of csit a feature is constructed by looking up counts for a concrete occurrence in the collected counts and quantized into 4-5 bins (Bansal and Klein, 2012) following the formula: 2. Otherwise, ignore the Czech translation provided in the corpus and follow the most simplistic possible translation which would still be correct. Assign the instance to the class which fits it the best. Note that it may happen that none of the three options fits, because it is either an idiomatic expression or larger structural modifications are required. Such cases are very rare and we left them out of the data. The manual annotation was a bottleneck. We managed to tag the complete testing data, but were only able to annotate more than just 1/6 of the training data d"
W13-3307,W10-1737,0,0.225919,"Missing"
W13-3307,2008.eamt-1.16,1,0.898207,"Missing"
W13-3307,W13-2208,0,0.0280176,"s provided through a deep-syntactic layer. The extrinsic evaluation of the proposed method was carried out on the English-Czech test set for WMT 2011 Shared Translation Task (CallisonBurch et al., 2011).10 This data set contains 3,003 English sentences with one Czech reference translation, out of which 430 contain at least one occurrence of it. Since this test set is provided with no annotation of coreferential links, the model of it that is involved in experiments on the end-to-end translation was trained on a complete feature set exclud10 11 For comparison, the best system so far – Chimera (Bojar et al., 2013) achieves 0.1994 on the same test set. Chimera combines Moses, TectoMT and rule-based corrections. http://www.statmt.org/wmt11/test.tgz 57 new better than old old better than new both equally wrong both equally correct 24 13 9 4 “it is”. These errors stem mostly from incorrect activation of syntactic features due to parsing and POS tagging errors. Example 11 (the Czech sentence is an MT output) shows the latter, when the POS tagger erroneously labeled the word soy as an adjective. That resulted in activating the feature for adjectival predicates followed by that (Figure 2c) instead of a featur"
W13-3307,E12-3001,0,0.405011,"ria, August 9, 2013. 2013 Association for Computational Linguistics model. The former allows us to feed it with many syntactic and lexical features that may affect the output, which would hardly be possible in the latter. points to a noun phrase in the preceding or the following context: (1) Peter has finished writing an article and showed it to his supervisor. 2 Related Work Anaphoric it refers to a verbal phrase or larger discourse segments (so-called discourse deixis). Our work addresses a similar issue that has been explored by Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012). These works attempted to incorporate information on coreference relations into MT, aiming to improve the translation of English pronouns into morphologically richer languages. The poor results in the first two works were mainly due to imperfect automatic coreference annotation. The work of Guillou (2012) is of special interest to this work because it is also focused on English to Czech translation and makes an extensive use of the Prague Czech-English Dependency Treebank 2.0 (PCEDT). Instead of automatic coreference links, they employed gold annotation, revealing further reasons of small imp"
W13-3307,W08-0325,1,0.945556,"xpression that would comprise all functions of the English it. Moreover, there is no simple one-to-one mapping from categories of it to Czech expressions. For instance, one would expect that the translation of it which is coreferential with a noun phrase has to agree in number and gender with the translation of its antecedent. However, there are cases when it is more suitable to translate it as the demonstrative pronoun to, whose gender is always neuter. The aim of this work is to build an English-toCzech translation model for the personal pronoun ˇ it within the TectoMT framework (Zabokrtsk´ y et al., 2008). TectoMT is a tree-to-tree translation system with transfer via tectogrammatical layer, a deep syntactic layer which follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986) Therefore, its translation model outputs the deep syntactic representation of a Czech expression. Selecting the correct grammatical categories and thus producing a concrete surface form of a deep syntactic representation is provided by the translation synthesis stage, which we do not focus on in this work. The mapping between it and corresponding Czech expressions depends on many aspects. We address the"
W13-3307,2010.iwslt-papers.10,0,0.192925,"coMT), pages 51–59, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics model. The former allows us to feed it with many syntactic and lexical features that may affect the output, which would hardly be possible in the latter. points to a noun phrase in the preceding or the following context: (1) Peter has finished writing an article and showed it to his supervisor. 2 Related Work Anaphoric it refers to a verbal phrase or larger discourse segments (so-called discourse deixis). Our work addresses a similar issue that has been explored by Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012). These works attempted to incorporate information on coreference relations into MT, aiming to improve the translation of English pronouns into morphologically richer languages. The poor results in the first two works were mainly due to imperfect automatic coreference annotation. The work of Guillou (2012) is of special interest to this work because it is also focused on English to Czech translation and makes an extensive use of the Prague Czech-English Dependency Treebank 2.0 (PCEDT). Instead of automatic coreference links, they employed gold annotation, revealing further r"
W13-3307,hajic-etal-2012-announcing,1,\N,Missing
W15-2209,D11-1006,0,0.0844784,"ce language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tre"
W15-2209,P15-2044,0,0.0820359,"Missing"
W15-2209,P12-1066,0,0.234958,"e use the Universal POS Tagset (UPOS) of Petrov et al. (2012). The parser configuration files containing the full feature set, together with the scripts we used for our experiments, are available in (Rosa, 2015a). the method to a crosslingual setting by combining delex parsers for different languages, weighted by src-tgt language similarity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser train"
W15-2209,petrov-etal-2012-universal,0,0.188168,"Missing"
W15-2209,P15-2040,1,0.601957,"Missing"
W15-2209,D11-1005,0,0.0130867,"arity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser training). We use a simplified version of normalization by standard deviation. First, we compute the uncorrected sample standard deviation of the weights of the features in the model as s 1 X sM = (wf − w) ¯ 2, (2) |M | Method In this section, we present our suggested approach of combining information from multiple src treebanks for parsin"
W15-2209,rosa-etal-2014-hamledt,1,0.867626,"Missing"
W15-2209,P11-1061,0,0.0578228,"to the tgt language, and is defined as the negative fourth power of the KL divergence (Kullback and Leibler, 1951) of coarse POS tag trigram distributions in tgt and src corpora: KL−4 (tgt, src) = cpos 3  We carry out all experiments using HamleDT 2.0 (Rosa et al., 2014), a collection of 30 treebanks converted into Universal Stanford Dependencies (de Marneffe et al., 2014). We use goldstandard UPOS tags in all experiments; while this is not fully realistic in the setting of underresourced languages, there exist high-performance semi-supervised taggers that could be used instead of gold tags (Das and Petrov, 2011; Agi´c et al., 2015), which we plan to evaluate in future. We use the treebank training sections for parser training and KL−4 computation, and the test sections cpos 3 for evaluation. We used 12 of the treebanks as a development set to select the model normalization method to avoid overfitting it to the dataset.4 −4  X ftgt (cpos 3 )  3   f (cpos ) · log tgt  fsrc (cpos 3 )  , (6) ∀cpos 3 ∈tgt where cpos 3 is a UPOS trigram, and f (cpos 3 ) is its relative frequency in a src or tgt corpus.3 4 Baseline Methods In this section, we describe the two baseline resource combination methods ag"
W15-2209,N06-2033,0,0.109557,"th multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tree combination method was introduced by Sagae and Lavie (2006) for a supervised monolingual setting, optionally weighting each src parser with a weight based on its accuˇ racy. In (Rosa and Zabokrtsk´ y, 2015), we ported Introduction The task of delexicalized dependency parser transfer (or delex transfer for short) is to train a parser on a treebank for a source language (src), using only non-lexical features, most notably partof-speech (POS) tags, and to apply that parser to POS-tagged sentences of a target language (tgt) to obtain dependency parse trees. Delex transfer yields worse results than a supervised lexicalized parser trained on the tgt languag"
W15-2209,de-marneffe-etal-2014-universal,0,0.0580442,"Missing"
W15-2209,C12-2115,0,0.213776,"and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weighted perceptron learning scenario (Cavallanti et al., 2010); due to its large computational complexity, we only compare to the unweighted variant in our paper. The parse tree combination method was introduced by Sagae and Lavie (2006) for a supervised monolingual setting, optionally weighting each src parser with a weight based on its accuˇ racy. In (Rosa and Zabokrtsk´ y, 2015), we ported Introduction The task of delexicalized dependency parser transfer (or delex"
W15-2209,N13-1126,0,0.318788,"Missing"
W15-2209,I08-3008,0,0.117669,"of separate src parsers, only one parser is run. We introduce interpolation of trained MSTParser models as a resource combination method for multi-source delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff"
W15-2209,N10-1004,0,0.0238571,"sers for different languages, weighted by src-tgt language similarity; we largely build upon that work in this paper. Other possibilities of estimating src-tgt language similarity for delex transfer include employment of WALS (Dryer and Haspelmath, 2013), focusing e.g. on genealogy distance and wordorder features, as done by Naseem et al. (2012) and T¨ackstr¨om et al. (2013), among others. We are not aware of any prior work on interpolating dependency parser models. However, there is work on interpolating trained phrase-structure parsers, both in a monolingual setting for domain adaptation by McClosky et al. (2010), as well as in a multilingual setting by Cohen et al. (2011). 3 3.2 An important preliminary step to model interpolation is to normalize each of the trained models, as the feature weights in models trained over different treebanks are often not on the same scale (we do not perform any regularization during the parser training). We use a simplified version of normalization by standard deviation. First, we compute the uncorrected sample standard deviation of the weights of the features in the model as s 1 X sM = (wf − w) ¯ 2, (2) |M | Method In this section, we present our suggested approach of"
W15-2209,zeman-etal-2012-hamledt,1,0.899437,"Missing"
W15-2209,P05-1012,0,0.632893,"(Section 3.2). 3. Interpolate the parser models (Section 3.3). 4. Parse the tgt text with a delex parser using the interpolated model. 3.1 Model Normalization where w ¯ is the average feature weight, and |M |is the number of feature weights in model M ; only features that were assigned a weight by the training algorithm are taken into account. We then divide each feature weight by the standard deviation:1 wf ∀f ∈ M : wf := . (3) sM Delexicalized MSTParser Throughout this work, we use MSTperl (Rosa, 2015b), an unlabelled first-order non-projective single-best implementation of the MSTParser of McDonald et al. (2005b), trained using 3 iterations of MIRA (Crammer and Singer, 2003). The MSTParser model uses a set of binary features F that are assigned weights wf by training on a treebank. When parsing a sentence, the parser constructs a complete weighted directed graph over the tokens of the input sentence, and assigns each edge e a score se which is the sum of weights of features that are active for that edge: X se = f (e) · wf . (1) The choice of normalization by standard deviation is based on its high and stable performance on our development set, and Occam’s razor.2 3.3 Model Interpolation The interpol"
W15-2209,zeman-2008-reusable,0,0.0143199,"rce delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 1 2 Related Work Delex transfer was conceived by Zeman and Resnik (2008), who also introduced two important preprocessing steps – mapping treebank-specific POS tagsets to a common set using Interset (Zeman, 2008), and harmonizing treebank annotation styles into a common style, which later led to the HamleDT harmonized treebank collection (Zeman et al., 2012). McDonald et al. (2011) applied delex transfer in a setting with multiple src treebanks available, finding that the problem of selecting the best src treebank without access to a tgt language treebank for evaluation is non-trivial, and proposed the treebank concatenation method as a solution. Søgaard and Wulff (2012) introduced weighting into the method, using a POS n-gram model trained on a tgt POS-tagged corpus to weight src sentences in a weigh"
W15-2209,H05-1066,0,0.190315,"Missing"
W17-0412,P05-1013,0,0.0703427,"Block ud.AddMwt splits multi-word tokens into words based on language-specific rules – there are subclasses for several languages, e.g. ud.cs.AddMwt for Czech. Overall statistics (number of words, empty words, multi-word tokens, sentences) can be printed with util.Wc. Advanced statistics about nodes matching a given condition (relative to other nodes) can be printed with util.See. For evaluation, eval.Parsing computes the standard UAS and LAS, while eval.F1 computes Precision/Recall/F1 of various attributes based on the longest common subsequence. Tree projectivization and deprojectivization (Nivre and Nilsson, 2005) can be performed using transform.Proj and transform.Deproj. 3 load (s) 2,501 158 24 7 9 CoNLL-U, iterating over all nodes sorted by word order while allowing changes of the word order too) and other technical issues was much bigger than the effort spent on the use cases described in Section 2. For example, to provide access to structured attributes FEATS and MISC (e.g. node .feats['Case'] = 'Nom') while allowing access to the serialized data (e.g. node.feats = 'Case=Nom|Person=1'), Udapi maintains both representations (string and dict) and synchronizes them transparently, but lazily. Table 1"
W17-0412,L16-1680,0,0.0889923,"Missing"
W17-0412,E12-2021,0,\N,Missing
W17-1226,C16-1012,0,0.0121703,"ctly, as there are various spelling and morpho211 proach is documented by T¨ackstr¨om et al. (2013). logical differences even between very close languages. Using such shared features allows a parser that was trained on a source treebank to be used directly on target texts; i.e. the source-target “transfer” of the parser is trivial, compared to a sourcetarget transfer of the treebank as described in §2.1. The common abstraction features used by the parser can be linguistically motivated, or induced by mathematical methods such as clustering and vector space representation: 2.3 Other variations Aufrant et al. (2016) combines both main strategies described above by adapting the word order in source sentences to be more similar to that of the target language, e.g. by swapping the order of an attribute and its nominal head; the information about these configurations was extracted from the WALS World Atlas of Language Structures (Dryer and Haspelmath, 2013). Such processing of source language trees fits to the first family of approaches, as it resembles a (very limited) MT preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a"
W17-1226,J92-4003,0,0.101154,"gs: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question"
W17-1226,W06-2920,0,0.108524,"ions whether a tree (and what kind of tree) is a reasonable representation for a sentence structure, and whether all languages do really share their structural properties to such an extent that a single type of representation is viable for all of them. Though such issues deserve intensive attention, and perhaps even more so now when UD have gained such a fascinating momentum, we take the two assumptions simply for granted. Neither do we present the genesis of the current UD collection, preceded by HamleDT treebank collection by Zeman et al. (2014), going back to the CoNLL 2006 and 2007 tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), and to earlier POS standardization efforts. In this overview, we limit ourselves to the scope outlined by the VarDial shared task, whose goal is to develop a parser for a (virtually) underresourced language closely related to a resourcerich language.2 We believe that most of the published approaches could be classified into two broad families which we call tree-transfer-based methods and common-abstraction-based methods. The former project individual dependency trees across the language boundary prior to training a target parser. The latter methods transfer a parser mode"
W17-1226,J03-1002,0,0.00596816,"translation. To reduce the OOV rate, two backoff layers are also stored, the first disregarding the morpho feats, and the second also disregarding the UPOS. An option that we leave for future research is to use the alignment scores provided by the MGA when constructing the translation table. For simplicity, we create only one joint translation table for translating DS into NO. Word-alignment Since the source and target languages in our task are very close to each other, we decided to use the heuristic Monolingual Greedy Aligner (MGA) of Rosa et al. (2012),9 rather than e.g. the usual Giza++ (Och and Ney, 2003) – most standard word aligners ignore word similarity, which we believe to be useful and important in our setting. MGA utilizes the word, lemma, and tag similarity based on Jaro-Winkler distance (Winkler, 1990), and the similarity of relative positions in the sentences, to devise a score for each potential alignment link as a linear combination of these, weighted by pre-set weights. The iterative alignment process then greedily chooses the currently highest scoring pair of words to align in each step; each word can only be aligned once. The process stops when one of the sides is fully aligned,"
W17-1226,P99-1065,0,0.341674,"Missing"
W17-1226,D15-1039,0,0.0140857,"inks. In addition, such alignment typically has a higher amount of one-to-one word alignments, which facilitates tree projection; in case of extremely close languages, as in this paper, the MT system can be constrained to produce only 1:1 translations. There are two additional advantages of the treetransfer-based approach: • the feature set used by the target language parser is independent of the features that are applicable to the source language, • we can easily use only sentence pairs (or tree fragments) with a reasonably high correspondence between source and target structures, as done by Rasooli and Collins (2015). Tree-transfer-based approaches In the tree-transfer-based approaches, a synthetic pseudo-target treebank is created by some sort of projection of individual source trees into the target language. Then a standard monolingual parser can be trained using the pseudo-target treebank in a more or less standard way. As it is quite unlikely that a manually annotated source treebank 2.2 2 Crosslingual transfer is not used only in truly underresourced scenarios, but also in situations in which it is hoped that features explicitly manifested in one language (such as morphological agreement) could boost"
W17-1226,K15-1012,0,0.0123173,"n since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question arises what is the best granularity of shared tags. The more simplified tags, the more languageuniversal information is captured, but the more information is lost at the same time. Moreover, even if two languages share a particular morphological category, e.g. pronoun reflexivity, it is hard to predict whether adding this distinction into the shared tagset"
W17-1226,P15-2040,1,0.922144,"Missing"
W17-1226,W12-4205,1,0.895855,"Missing"
W17-1226,D11-1006,0,0.0460896,"preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a few underresourced languages, choosing the best source language should be ideally automatized too. One could rely on language phylogenetic trees or on linguistic information available e.g. in WALS, or on more mechanized measures, such as KullbackLeibler divergence of POS trigram distributions ˇ (Rosa and Zabokrtsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harm"
W17-1226,L16-1680,0,0.150363,"Missing"
W17-1226,N12-1052,0,0.242999,"Missing"
W17-1226,N13-1126,0,0.0549572,"Missing"
W17-1226,C14-1175,0,0.407656,"et parser. The latter methods transfer a parser model trained directly on the source treebank, but limited only to abstract features shared by both languages. 2.1 with high-quality human-made target translations and high-quality alignment exists, one or more of the necessary components must be approximated. And even if all these data components existed, the task of dependency tree projection would inevitably lead to collisions that have to be resolved heuristically, especially in the case of many-to-one or many-to-many alignments, as investigated e.g. by Hwa et al. (2005) and more recently by Tiedemann (2014) or Ramasamy et al. (2014). This family embraces the following approaches: • using a parallel corpus and projecting the trees through word-alignment links, with authentic texts in both languages but an automatically parsed source side, • using a machine-translated parallel corpus, with only one side containing authentic texts and the other being created by MT; both translation directions have pros and cons: – source-to-target MT allows for using a gold treebank on the source side, – target-to-source MT allows the parser to learn to work with real texts in the target language, for which, in add"
W17-1226,W17-1216,0,0.0957939,"Missing"
W17-1226,N01-1026,0,0.132429,"er is trained on monolingually predicted tags, as explained in §4.1. We have found source-xtag to work well for heterogeneous source data, such as the DS mixture. Conversely, target-xtag proved useful for SK, where the source treebank is much larger than the target data used to train the target tagger. A tagger trained on the large source treebank provides much better tags, which in turn boosts the parsing accuracy, despite the noise from MT and xtag. Note that if no target tagger is available, we must either use target-xtag, or we may project a tagger across the parallel data in the style of Yarowsky and Ngai (2001) and use the resulting tagger in our baseline or source-xtag scenarios.12 We also experimented with cross-tagging of only the UPOS or only the morpho feats, with different setups being useful for different languages. Although the UDPipe tagger can also be trained to perform lemmatization, we have not found any way to obtain and utilize lemmas that would improve the cross-lingual parsing.13 12 Our approach still needs a target tagger to perform the word alignment, but we believe that for very close languages, the word forms alone might be sufficient to obtain a goodenough alignment; or, a diffe"
W17-1226,W17-1201,0,0.0972651,"Missing"
W17-1226,I08-3008,1,0.743945,"tsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual pa"
W17-1226,zeman-2008-reusable,1,0.792983,"eights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obviou"
W17-1508,Q14-1037,0,0.0561731,", Spain, April 4, 2017. 2017 Association for Computational Linguistics 3.1 part-of-speech tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), opinion mining (Almeida et al., 2015), etc. Coreference resolution is no exception in this respect. The source-language side of the parallel corpus must get labeled with coreference. In our case, the English side of the parallel corpus already contained annotation of coreference provided by the shared task’s organizers. The annotation is obtained by Berkeley Entity Resolution system (Durrett and Klein, 2014), trained on the English section of OntoNotes 5.0 (Pradhan et al., 2013). Although Berkeley system is a state-of-the-art performing coreference resolver, we found that it rarely addresses relative and demonstrative pronouns. To label coreference for relative pronouns, we introduced a module from the Treex framework2 that employs a simple heuristics based on syntactic trees. Coreference of demonstratives has not been further resolved. Coreference projection is generally approached in two ways. They differ in how they obtain the translation to the language for which a coreference resolver exits."
W17-1508,P15-1040,0,0.158798,"urces in the target language. So far, they have been quite successfully applied to 1 Details on the shared task are available in its overview paper (Grishina, 2017) and at http://corbon.nlp.ipipan. waw.pl/index.php/shared-task/ 56 Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 56–64, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 3.1 part-of-speech tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), opinion mining (Almeida et al., 2015), etc. Coreference resolution is no exception in this respect. The source-language side of the parallel corpus must get labeled with coreference. In our case, the English side of the parallel corpus already contained annotation of coreference provided by the shared task’s organizers. The annotation is obtained by Berkeley Entity Resolution system (Durrett and Klein, 2014), trained on the English section of OntoNotes 5.0 (Pradhan et al., 2013). Although Berkeley system is a state-of-the-art performing coreference resolver, we found that it rarely addresses relative and demonstrative pronouns. T"
W17-1508,W17-1507,0,0.0432723,"aining data was the English part of the OntoNotes corpus (Pradhan et al., 2013). Alternatively, any publicly available res2 Related Work Approaches of cross-lingual projection have received attention with the advent of parallel corpora. They are usually aimed to bridge the gap of missing resources in the target language. So far, they have been quite successfully applied to 1 Details on the shared task are available in its overview paper (Grishina, 2017) and at http://corbon.nlp.ipipan. waw.pl/index.php/shared-task/ 56 Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 56–64, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 3.1 part-of-speech tagging (T¨ackstr¨om et al., 2013), syntactic parsing (Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), opinion mining (Almeida et al., 2015), etc. Coreference resolution is no exception in this respect. The source-language side of the parallel corpus must get labeled with coreference. In our case, the English side of the parallel corpus already contained annotation of coreference provided by the shared task’s organizers. The annotatio"
W17-1508,H05-1004,0,0.0806228,"all NP PP3 PPo demonstrative reflexive reflexive possessive relative Russian DevAdd DevOff DevAdd 24.2 -8.7 -11.7 +0.5 +0.5 0 – 0 22.4 -4.6 -11.3 -1.0 -0.1 0 – -1.9 24.2 -7.6 -7.5 0 0 0 -4.1 0 31.8 -3.0 -10.4 -1.1 0 0 -6.4 -3.4 Table 5: Results of model ablation. The all line describes the complete resolver. Every following line represent an ablated resolver with a model for a given mention type left out. Differences in scores are listed in such line. Metrics. We present the results in terms of four standard coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the CoNLL score (Pradhan et al., 2014). The CoNLL score is an average of Fscores of the previous three measures. It was the main score of some previous coreference-related shared tasks, e.g., CoNLL 2012 (Pradhan et al., 2012), and it remains so for the CORBON 2017 Shared task. in Table 3. There is a big disproportion in score between the two datasets after the model for NPs is removed. This may be a consequence of different ratios of anaphoric NPs to all the anaphoric mentions. Multiple models seem to have marginal, zero, or even negative impact on the final performance. The reasons are t"
W17-1508,benikova-etal-2014-nosta,0,0.0282355,"e Prague style within the HamleDT project.5 Although UDPipe trained on this data is able to lemmatize, we used lemmas produced by TreeTagger instead, as they seemed to be of better quality. In the same fashion as for German, tectogrammatical tree is built from the surface dependency tree using the Treex pipeline adjusted to Russian. We also included named entity recognition, namely NameTag tool (Strakov´a et al., 2014), to the pipeline. We had trained it on an extended version of the Persons-1000 collection (Mozharova and Loukachevitch, 2016) and named entity annotation of the NoSta-D corpus (Benikova et al., 2014) for Russian and German, respectively. Coreference resolution in German and Russian At this point, projected links are ready to serve as training data for a coreference resolver. We make use of an updated version of the already existing resolver implemented within the Treex framework, which operates on a level of deep syntax. All the texts must thus be analyzed and the projected mentions must be transferred up to this level before being used for training. Analysis up to the tectogrammatical layer. Treex coreference resolver operates on a level of deep syntax, in Prague theory (Sgall et al., 19"
W17-1508,P15-1138,0,0.37453,"orpus of the two languages. Unlike the first approach, the translation must be provided already in train time. Postolache et al. (2006) followed this approach using an English-Romanian corpus. They projected manually annotated coreference, which was then postprocessed by linguists to acquire high quality annotation in Romanian. de Souza and Or˘asan (2011) applied projection in a parallel English-Portuguese corpus to build a resolver for Portuguese. Our work practically follows this schema, differing in some design details (e.g., using specialized models, resolution on a level of deep syntax). Martins (2015) extended this approach by learning coreference with a specific type of regularization at the end. Their gains over the standard projection come from ability of their method to recover links missing due to projection over inaccurate alignment. 3 Coreference relations in English 3.2 Cross-lingual projection of coreference The second stage the proposed schema is to project coreference relations from the sourcelanguage to the target-language side of the parallel corpus. Specifically, we make use of word-level alignment, which allows for potentially more accurate projection. As the parallel data p"
W17-1508,C10-3009,0,0.0241942,"Missing"
W17-1508,C00-2143,0,0.0199156,"j¨orkelund et al., 2010) that includes lemmatization, part-of-speech tagging, and transition-based dependency parsing (Bohnet and Nivre, 2012; Seeker and Kuhn, 2012). The surface dependency tree is then converted to the Prague style of annotation using a converter from the HamleDT project (Zeman et al., 2014). Transformation to tectogrammatics is then performed by a general Treex pipeline, with some languagedependent adjustments. Russian texts are being parsed directly to the Prague style of surface dependency tree. We trained a UDPipe tool (Straka et al., 2016) on data from SynTagRus corpus (Boguslavsky et al., 2000) converted to the Prague style within the HamleDT project.5 Although UDPipe trained on this data is able to lemmatize, we used lemmas produced by TreeTagger instead, as they seemed to be of better quality. In the same fashion as for German, tectogrammatical tree is built from the surface dependency tree using the Treex pipeline adjusted to Russian. We also included named entity recognition, namely NameTag tool (Strakov´a et al., 2014), to the pipeline. We had trained it on an extended version of the Persons-1000 collection (Mozharova and Loukachevitch, 2016) and named entity annotation of the"
W17-1508,D12-1133,0,0.0414065,"Missing"
W17-1508,P00-1056,0,0.141938,"come from ability of their method to recover links missing due to projection over inaccurate alignment. 3 Coreference relations in English 3.2 Cross-lingual projection of coreference The second stage the proposed schema is to project coreference relations from the sourcelanguage to the target-language side of the parallel corpus. Specifically, we make use of word-level alignment, which allows for potentially more accurate projection. As the parallel data provided for the task are aligned only on the sentence level, word alignment must be acquired on our own. For this purpose, we used GIZA++ (Och and Ney, 2000) a tool particularly popular in the community of statistical machine translation. Even though GIZA++ implements a fully unsupervised approach, which allows for easy extension of the training data with raw parallel texts, it did not prove to be useful for us. We thus obtained word alignment for both the language pairs by running the tool solely on the parallel corpora coming from the organizers.3 Since both German and Russian are morphologically rich languages, we expected word alignment to work better on lemmatized texts. We applied TreeTagger (Schmid, 1995), and MATE tools (Bj¨orkelund et al."
W17-1508,L16-1680,0,0.0511868,"Missing"
W17-1508,postolache-etal-2006-transferring,0,0.839173,"Missing"
W17-1508,P14-5003,0,0.0447118,"Missing"
W17-1508,W12-4501,0,0.0628831,"6.4 -3.4 Table 5: Results of model ablation. The all line describes the complete resolver. Every following line represent an ablated resolver with a model for a given mention type left out. Differences in scores are listed in such line. Metrics. We present the results in terms of four standard coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the CoNLL score (Pradhan et al., 2014). The CoNLL score is an average of Fscores of the previous three measures. It was the main score of some previous coreference-related shared tasks, e.g., CoNLL 2012 (Pradhan et al., 2012), and it remains so for the CORBON 2017 Shared task. in Table 3. There is a big disproportion in score between the two datasets after the model for NPs is removed. This may be a consequence of different ratios of anaphoric NPs to all the anaphoric mentions. Multiple models seem to have marginal, zero, or even negative impact on the final performance. The reasons are threefold: Results. In Table 4, we report the results of evaluating the submitted systems. Comparison across languages shows very similar performance on the DevOff set. However, evaluation on the larger DevAdd set suggests the Russ"
W17-1508,Q13-1001,0,0.122214,"Missing"
W17-1508,tiedemann-2012-parallel,0,0.0240011,"Missing"
W17-1508,W13-3516,0,0.154635,"this language. Instead, far more easily available parallel corpora are used as a means to transfer the labels to this language from a language, for which such a tool or manual annotation exists. This paper presents a system submitted to the closed track of the shared task collocated with the Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017).1 The task was to build coreference resolution systems for German and Russian without coreference-annotated training data in these languages. The only allowed coreference-annotated training data was the English part of the OntoNotes corpus (Pradhan et al., 2013). Alternatively, any publicly available res2 Related Work Approaches of cross-lingual projection have received attention with the advent of parallel corpora. They are usually aimed to bridge the gap of missing resources in the target language. So far, they have been quite successfully applied to 1 Details on the shared task are available in its overview paper (Grishina, 2017) and at http://corbon.nlp.ipipan. waw.pl/index.php/shared-task/ 56 Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 56–64, c Valencia, Spain, April"
W17-1508,M95-1005,0,0.753055,"the News-Commentary11 collection by the task’s organizers. all NP PP3 PPo demonstrative reflexive reflexive possessive relative Russian DevAdd DevOff DevAdd 24.2 -8.7 -11.7 +0.5 +0.5 0 – 0 22.4 -4.6 -11.3 -1.0 -0.1 0 – -1.9 24.2 -7.6 -7.5 0 0 0 -4.1 0 31.8 -3.0 -10.4 -1.1 0 0 -6.4 -3.4 Table 5: Results of model ablation. The all line describes the complete resolver. Every following line represent an ablated resolver with a model for a given mention type left out. Differences in scores are listed in such line. Metrics. We present the results in terms of four standard coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the CoNLL score (Pradhan et al., 2014). The CoNLL score is an average of Fscores of the previous three measures. It was the main score of some previous coreference-related shared tasks, e.g., CoNLL 2012 (Pradhan et al., 2012), and it remains so for the CORBON 2017 Shared task. in Table 3. There is a big disproportion in score between the two datasets after the model for NPs is removed. This may be a consequence of different ratios of anaphoric NPs to all the anaphoric mentions. Multiple models seem to have marginal, zero, or even negative i"
W17-1508,P14-2006,0,0.0127618,"lexive reflexive possessive relative Russian DevAdd DevOff DevAdd 24.2 -8.7 -11.7 +0.5 +0.5 0 – 0 22.4 -4.6 -11.3 -1.0 -0.1 0 – -1.9 24.2 -7.6 -7.5 0 0 0 -4.1 0 31.8 -3.0 -10.4 -1.1 0 0 -6.4 -3.4 Table 5: Results of model ablation. The all line describes the complete resolver. Every following line represent an ablated resolver with a model for a given mention type left out. Differences in scores are listed in such line. Metrics. We present the results in terms of four standard coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and the CoNLL score (Pradhan et al., 2014). The CoNLL score is an average of Fscores of the previous three measures. It was the main score of some previous coreference-related shared tasks, e.g., CoNLL 2012 (Pradhan et al., 2012), and it remains so for the CORBON 2017 Shared task. in Table 3. There is a big disproportion in score between the two datasets after the model for NPs is removed. This may be a consequence of different ratios of anaphoric NPs to all the anaphoric mentions. Multiple models seem to have marginal, zero, or even negative impact on the final performance. The reasons are threefold: Results. In Table 4, we report th"
W17-1508,N12-1090,0,0.537012,"lative and demonstrative pronouns. To label coreference for relative pronouns, we introduced a module from the Treex framework2 that employs a simple heuristics based on syntactic trees. Coreference of demonstratives has not been further resolved. Coreference projection is generally approached in two ways. They differ in how they obtain the translation to the language for which a coreference resolver exits. The first approach applies a machine-translation service to create synthetic data in this language. This usually happens at test times on previously unseen texts. Such approach was used by Rahman and Ng (2012) on Spanish and Italian, and by Ogrodniczuk (2013) on Polish. The other approach, which we employ in this work, takes advantage of the human-translated parallel corpus of the two languages. Unlike the first approach, the translation must be provided already in train time. Postolache et al. (2006) followed this approach using an English-Romanian corpus. They projected manually annotated coreference, which was then postprocessed by linguists to acquire high quality annotation in Romanian. de Souza and Or˘asan (2011) applied projection in a parallel English-Portuguese corpus to build a resolver f"
W17-1508,seeker-kuhn-2012-making,0,0.0391409,"Missing"
W17-7615,P15-2044,0,0.0655623,"Missing"
W17-7615,W17-0401,0,0.0776366,"Missing"
W17-7615,P15-1165,0,0.0334271,"Missing"
W17-7615,L16-1680,0,0.1026,"Missing"
W17-7615,N12-1052,0,0.0258171,"words: cross-lingual parsing, cross-lingual tagging, Universal Dependencies 2 Cross-lingual parsing Cross-lingual parsing is the task of performing syntactic analysis of a target language with no treebank available for that language by using annotated data for a different source language and a method for transferring the knowledge about syntactic structures from that source language into the target language. It has already been studied for over a decade, starting with the works of Hwa et al. (2005) and Zeman and Resnik (2008), and then continued by many others, such as McDonald et al. (2011), Täckström et al. (2012), Georgi et al. (2013), Agi´c et al. (2015), Søgaard et al. (2015), and Duong et al. (2015). A thorough overview, analysis and comparison of existing methods can be found in (Tiedemann et al., 2016). The authors also include a detailed analysis of the performance of the systems based on various factors, such as part-of-speech (POS) labelling accuracy or size of training data. Another work dealing with error analysis of cross-lingual parsing systems is that of Ramasamy et al. (2014). The system evaluated in this paper is a new version of the aforementioned SFNW (Rosa et al., 2017), improved and"
W19-8508,P19-1310,0,0.0195764,"er of languages. The vast majority of the world’s languages are under-resourced, lacking such datasets or tools, which gravely limits any research on such languages. The ability to perform the inflection-derivation distinction automatically, assuming only the availability of a plain text corpus of the language, would thus be of great value. Admittedly, for many languages, no plain text corpus of a considerable size is available; in such cases, we are out of luck. Nevertheless, medium-size plain text corpora exist for hundreds of languages – Wikipedia1 covers 300 languages (Rosa, 2018), JW300 (Agić and Vulić, 2019) features texts 1https://www.wikipedia.org/ 61 Proceedings of the 2nd Int. Workshop on Resources and Tools for Derivational Morphology (DeriMo 2019), pages 61–70, Prague, Czechia, 19-20 September 2019. from Watchtower2 for 300 languages (around 100k sentences each), and the text of the whole or a part of the Bible is available for as many as 1,400 languages (Mayer and Cysouw, 2014). Still, in this work, our goal is not (yet) practical, i.e. devising a tool applicable to under-resourced languages, but rather exploratory, investigating the mere feasibility of such an approach. Therefore, we only"
W19-8508,J01-2001,0,0.19746,"s. We utilize their work to provide categories of derivational operations, which are not yet annotated in DeriNet and have to be estimated heuristically. While the methods used by us and previously mentioned authors are rather simple, we are unaware of any other substantial research in this direction. There is research on unsupervised morphology induction, represented by the well-known Morfessor system of Creutz and Lagus (2007), the interesting ParaMor system (Monson et al., 2008) which attempts to find inflectional paradigms, as well as the earlier minimum description length-based system of Goldsmith (2001). While some ideas behind these systems are related to our interests and may potentially be useful to us, their goal is to perform morphological segmentation, which is a related but different task. Another related area is stemming (Lovins, 1968; Porter, 2001), which can be thought of as simple lemmatization. However, stemmers tend to be too coarse, often assigning the same stem to both inflections and derivations. Moreover, they are typically rule-based and thus language-specific, which is not in line with our goals. 3 Approach Our central hypothesis is that word forms that are inflections of"
W19-8508,L18-1550,0,0.156192,"ls. 3 Approach Our central hypothesis is that word forms that are inflections of the same lemma tend to be more similar than inflections of different lemmas. To measure the similarity of word forms, we investigate two somewhat orthogonal simple approaches. Our first method is to use string edit distances, which measure how much the word forms differ on the character level. In our work, we use the Jaro-Winkler (JW) edit distance (Winkler, 1990) and the Levenshtein edit distance (Levenshtein, 1966). As the second method, we propose to measure similarity of word embeddings (Mikolov et al., 2013; Grave et al., 2018). It has been shown that cosine similarity of word embeddings tends to capture various kinds of word similarities, including morphological, syntactic, and semantic similarities, and can be thought of as a proxy to meaning similarity. We then apply the methods to sets of corpus-attested words belonging to one derivational family, i.e. a set of words that are, according to a database of word formation relations, all derived from a common root, together with their inflections extracted from a lemmatized corpus. Some words in the set are thus inflections of a common lemma, while others are inflect"
W19-8508,mayer-cysouw-2014-creating,0,0.0262775,"n text corpus of a considerable size is available; in such cases, we are out of luck. Nevertheless, medium-size plain text corpora exist for hundreds of languages – Wikipedia1 covers 300 languages (Rosa, 2018), JW300 (Agić and Vulić, 2019) features texts 1https://www.wikipedia.org/ 61 Proceedings of the 2nd Int. Workshop on Resources and Tools for Derivational Morphology (DeriMo 2019), pages 61–70, Prague, Czechia, 19-20 September 2019. from Watchtower2 for 300 languages (around 100k sentences each), and the text of the whole or a part of the Bible is available for as many as 1,400 languages (Mayer and Cysouw, 2014). Still, in this work, our goal is not (yet) practical, i.e. devising a tool applicable to under-resourced languages, but rather exploratory, investigating the mere feasibility of such an approach. Therefore, we only use a single resource-rich language for the investigation, so that we can reliably analyze the performance of our approach, for which we need annotated datasets. Moreover, as an outlook to future work, we are also interested in empirically exploring the boundary between derivation and inflection, which is notoriously vague. We hope that empirical computational methods could provid"
W19-8508,W19-4818,0,0.0897627,"s, and both works make the usual choice of using word embeddings as a proxy to word meanings. As the criterion that we test is simpler, our method is also simpler: we directly measure the difference of word embeddings to estimate the distance of meanings. To estimate the regularity of meaning change, Bonami and Paperno (2018) take a further step of estimating an embedding vector shift corresponding to a particular morphological operation, and observe that the vector shift tends to be more regular for inflectional operations than for derivational operations. A partially related work is that of Musil et al. (2019), showing that there is some regularity in the vector shift corresponding to individual derivational operations. However, the authors do not contrast this with inflectional operations. We utilize their work to provide categories of derivational operations, which are not yet annotated in DeriNet and have to be estimated heuristically. While the methods used by us and previously mentioned authors are rather simple, we are unaware of any other substantial research in this direction. There is research on unsupervised morphology induction, represented by the well-known Morfessor system of Creutz an"
W19-8508,sevcikova-zabokrtsky-2014-word,1,0.818083,"s are naturally hard to scale to bigger data and/or more languages. In our study, we take the existence of a crisp inflection-derivation boundary as an assumption, and we try to get close to the boundary in a fully unsupervised way, using only unlabelled corpus data. For evaluation purposes, we accept the boundary as technically defined in existing morphological NLP resources for Czech. More specifically, we use MorfFlex CZ (Hajič and Hlaváčová, 2016) to bind inflected word forms with their lemmas (more exactly, we use only corpus-attested word forms), and the word-formation database DeriNet (Ševčíková and Žabokrtský, 2014), in which relations between derivationally related lexemes are represented in the form of rooted trees (one tree per a derivational family). To the best of our knowledge, the only work to investigate a similar question is the recent research of Bonami and Paperno (2018). Similar to us, the authors are interested in a way to turn the human-centered criteria of distinguishing inflection from derivation into something empirically testable. The authors investigated the semantic regularity criterion (“inflection is semantically more regular than derivation”), 2https://www.jw.org/ 3The debate seems"
W19-8508,L16-1208,1,0.838022,"the pairwise method, with the objective that inflections should fall into common clusters and non-inflections should fall into different clusters. We define Winf l as in (4), and Wclust as the set of all pairs of word forms that fell into the same cluster: Wclust = {w1, w2|clust(w1 ) = clust(w2 )} (7) We then compute the precision, recall, and F1 score of inflection pairs clustered together: Pclust = 5.2 |Winf l ∩ Wclust | |Winf l ∩ Wclust | 2 · Pclust · Rclust ; Rclust = ; Fclust = |Wclust | |Winf l | Pclust + Rclust (8) Experiment setting We extract derivational families from DeriNet v1.7 (Žabokrtský et al., 2016),11 a database of Czech word formation relations. As the database only contains word lemmas, we enrich the extracted lemma sets with inflections of the lemmas found in the Czech National Corpus, subcorpus SYN v4 (Křen et al., 2016), a large corpus of Czech lemmatized automatically using morphological analyzer MorfFlex CZ (Hajič and Hlaváčová, 2016).12 We lowercase all the word forms. 10https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering 11http://ufal.mff.cuni.cz/derinet 12MorfFlex CZ (Hajič and Hlaváčová, 2016) offers complete inflectional paradigms, which leads to"
W19-8510,L16-1262,0,0.0508461,"Missing"
W19-8510,sevcikova-zabokrtsky-2014-word,1,0.815475,"ions, a pilot annotation of parents of compounds, and another pilot annotation of so-called fictitious lexemes, which connect related derivational families without a common synchronous parent. The new pieces of annotation could be added thanks to a new file format for storing the network, which aims to be general and extensible, and therefore possibly usable to other similar projects. 1 Motivation The paper deals with extending DeriNet, a lexical database developed for Czech, which contains around 1 million lexemes connected with app. 810 thousand edges representing morphological derivations (Ševčíková and Žabokrtský, 2014), forming app. 220 thousand tree-shaped derivational families. The resulting version is labelled DeriNet 2.0 (Vidra et al., 2019) and it is available for download under a free non-commercial license. The extension is mostly qualitative: we extended the expressive power of the underlying data structure (and of the associated file format) substantially and thus enabled capturing language phenomena which were impossible to handle in the previous versions of DeriNet. More specifically, there are five newly supported annotation components in the DeriNet annotation scheme: • morphological categories"
W19-8511,E06-2023,0,0.0154218,"ctives, and verb stems. Hesabi (1988) claimed that Persian can derive more than 226 million word forms. To our knowledge, research on Persian morphology is very limited. Rasooli et al. (2013) claimed that performing morphological segmentation in the pre-processing phase of statistical machine translation could improve the quality of translations for morphology rich and complex languages. Although they segmented only an extremely limited and non-representative sample of Persian words (tens of Persian verbs), the quality of their machine translation system increases by 1.9 points of BLEU score. Arabsorkhi and Shamsfard (2006) proposed an algorithm based on Minimum Description Length with certain improvements for discovering the morphemes of the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually fo"
W19-8511,baranes-sagot-2014-language,0,0.0234316,"[danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is Persian, which belongs to morphologically rich languages and is powerful and versatile in word formation. Having many affixes to form new words (a few hundred), the Persian languag"
W19-8511,W16-1603,0,0.015841,"the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological segmentation. 3 Data: New Persian Segmented Lexicon We extracted"
W19-8511,W02-0603,0,0.109242,"on Length with certain improvements for discovering the morphemes of the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological s"
W19-8511,J01-2001,0,0.25129,"Minimum Description Length with certain improvements for discovering the morphemes of the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsu"
W19-8511,C14-1111,0,0.0969379,"for this language. At the same time, to the best of our knowledge, this lexicon could be considered as the biggest publicly available manually segmented lexicon at all (for any language). Moreover, we expand the existing morphological network by adding new words into the current network by using our proposed core algorithm. In order to segment new words, we used both supervised and 91 Proceedings of the 2nd Int. Workshop on Resources and Tools for Derivational Morphology (DeriMo 2019), pages 91–100, Prague, Czechia, 19-20 September 2019. unsupervised version of MORFESSOR (Creutz et al., 2007; Grönroos et al., 2014), which is a popular automatic segmentation toolkit. After segmentation, the process of inducing morphological trees is the same as for hand-segmented words. The paper is organized as follows: Section 2 addresses related work on derivational morphology networks and morphological segmentation. Section 3 introduces our hand-segmented Persian lexicon as well as related pre-processing phases. Section 4 describes the approach used in this work. Section 5 presents experiment results and finally Section 6 concludes the paper. Figure 1: A sample of a Persian morpohological tree for root [dan] which me"
W19-8511,2014.lilt-11.6,0,0.120828,"6 concludes the paper. Figure 1: A sample of a Persian morpohological tree for root [dan] which means &quot;to know&quot;. The two children of the root node have the meaning of &quot;knowledge&quot; [danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is P"
W19-8511,L18-1549,1,0.735372,"r unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological segmentation. 3 Data: New Persian Segmented Lexicon We extracted our primary word list from a collection composed of three corpora. The first corpus contains sentences extracted from the Persian Wikipedia (Karimi et al., 2018). The second one is a popular Persian corpus BijanKhan (Bijankhan et al., 2011), and the last one is the Persian Named Entity corpus1 (Poostchi et al., 2018). For all those corpora, we used the Hazm toolkit (Persian pre-processing and tokenization tools)2 and the stemming tool presented by Taghi-Zadeh et al. (2015). We extracted and normalized all sentences and lemmatized and stemmed all words using our rule-based stemmer and a lemmatizer that uses our collection of Persian lemmas. Finally all semi-spaces are automatically detected and fixed. An important feature of the Persian and Arabic lang"
W19-8511,L18-1291,1,0.743626,"omatically with reasonable accuracy. 1 Introduction Even though the Natural Language community put more focus on inflectional morphology in the past, one can observe a growing interest in research on derivational morphology (and other aspects of word formation) recently, leading to existence of various morphological data resources. One relatively novel type of such resources are word-formation networks, some of which represent information about derivational morphology in the shape of a rooted tree. In such networks, the derivational relations are represented as directed edges between lexemes (Lango et al., 2018). In our work, we present a procedure that builds a morphological network for the Persian language using a word segmentation lexicon. The resulting network (a directed graph) represents each cluster of morphologically related word forms as a tree-shaped component of the overall graph. The specific feature of such network is that it captures both derivational and inflectional relations. Figure 1 shows an example of such a tree for the Persian language which represents a base morpheme meaning “to know” and all derived and inflected descendants. In this example, the path from the root to one of t"
W19-8511,Q15-1012,0,0.0236415,"covering the morphemes of the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological segmentation. 3 Data: New Persian Segmented L"
W19-8511,piasecki-etal-2012-recognition,0,0.0176689,"o the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is Persian, which belongs to morphologically rich languages and is powerful and versatile in word formation. Having many affixes to form new words (a few hundred), the Persian language is considered to be an agglutinative language since it also frequently uses derivational agglutination"
W19-8511,N09-1024,0,0.0322094,"mprovements for discovering the morphemes of the Persian language through automatic analysis of corpora. However, since no Persian segmentation lexicon was made publicly available, we decided to 92 create a manually segmented lexicon for Persian that contains 45K words now. As we discussed before, we also trained and evaluated our methods using automatic morph-segmented data. Automatic morphological segmentation was firstly introduced by Harris (1955). More recent research on morphological segmentation has been usually focused on unsupervised learning (Goldsmith, 2001; Creutz and Lagus, 2002; Poon et al., 2009; Narasimhan et al., 2015; Cao and Rei, 2016), whose goal is to find the segmentation boundaries using an unlabeled set of word forms (or possibly a corpus too). Probably the most popular unsupervised systems are LINGUISTICA (Goldsmith, 2001) and MORFESSOR, with a number of variants (Creutz and Lagus, 2002; Creutz et al., 2007; Grönroos et al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological segmentation. 3 Data"
W19-8511,L18-1701,0,0.0130738,"al., 2014). Another version of the latter which includes a semi-supervised extension was introduced by (Kohonen et al., 2010). Poon et al. (2009) presented a log-linear model which uses overlapping features for unsupervised morphological segmentation. 3 Data: New Persian Segmented Lexicon We extracted our primary word list from a collection composed of three corpora. The first corpus contains sentences extracted from the Persian Wikipedia (Karimi et al., 2018). The second one is a popular Persian corpus BijanKhan (Bijankhan et al., 2011), and the last one is the Persian Named Entity corpus1 (Poostchi et al., 2018). For all those corpora, we used the Hazm toolkit (Persian pre-processing and tokenization tools)2 and the stemming tool presented by Taghi-Zadeh et al. (2015). We extracted and normalized all sentences and lemmatized and stemmed all words using our rule-based stemmer and a lemmatizer that uses our collection of Persian lemmas. Finally all semi-spaces are automatically detected and fixed. An important feature of the Persian and Arabic languages is the existence of semi-space. For example word “( &quot;کتابهاbooks) is a combination of word “ &quot;کتابand “&quot;ها, in which the former is Persian transl"
W19-8511,I13-1144,0,0.0713076,"Missing"
W19-8511,sevcikova-zabokrtsky-2014-word,1,0.812165,"] which means &quot;to know&quot;. The two children of the root node have the meaning of &quot;knowledge&quot; [danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is Persian, which belongs to morphologically rich languages and is powerful and versatile in word f"
W19-8511,snajder-2014-derivbase,0,0.351647,"sian morpohological tree for root [dan] which means &quot;to know&quot;. The two children of the root node have the meaning of &quot;knowledge&quot; [danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is Persian, which belongs to morphologically"
W19-8511,L16-1208,1,0.787688,"children of the root node have the meaning of &quot;knowledge&quot; [danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčová, 2013). The language studied in our work is Persian, which belongs to morphologically rich languages and is powerful and versatile in word formation. Having many affi"
W19-8511,P13-1118,0,0.311225,"sents experiment results and finally Section 6 concludes the paper. Figure 1: A sample of a Persian morpohological tree for root [dan] which means &quot;to know&quot;. The two children of the root node have the meaning of &quot;knowledge&quot; [danesh] and &quot;smart&quot; [dana], respectively. The path from the root to one of the deepest leaf corresponds to the following meanings: (1) &quot;to know&quot;, (2) &quot;knowledge&quot;/&quot;science&quot;, (3) &quot;scientist&quot;, (4) &quot;scientists&quot;, (5) &quot;some scientists&quot;. 2 Related work For some languages, intensive research exists with focus on construction of resources specialized in derivation, e.g. DerivBase (Zeller et al., 2013) for German, Démonette (Hathout and Namer, 2014) for French, DerivBase.Hr (Šnajder, 2014) for Croatian, DeriNet (Ševčíková and Žabokrtský, 2014; Žabokrtský et al., 2016) for Czech, (Vilares et al., 2001; Baranes and Sagot, 2014; Lango et al., 2018) for Spanish, Word Formation Latin (Litta et al., 2016), and (Piasecki et al., 2012; Kaleta, 2017; Lango et al., 2018) for Polish. However, for many other languages the data resources which provide information about derived words are scarce or lacking. Simultaneously, inflectional resources are further developed in recent years too (Hajič and Hlaváčo"
W19-8512,W06-2920,0,0.109227,"ge resources differ greatly in many aspects. This fact complicates usability of the data in multilingual projects, including a potential data-oriented research in derivational morphology across languages. Last but not least, for developers of new data, it can be highly time-consuming to deal with various technical and other issues that somebody else may have already successfully solved. The current situation with derivational resources is sort of similar to recent developments in treebanking. Efforts have been made to harmonize syntactic treebanks, for instance, in the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), in the HamleDT treebank collection (Zeman et al., 2014), or in Google Universal Treebanks (McDonald et al., 2013), converging into the Universal Dependencies project (Nivre et al., 2016), and that has become a significant milestone in the applicability of the treebanks.1 Being inspired by the harmonization of syntactic treebanks, we harmonized eleven selected derivational resources to a unified scheme in order to verify the feasibility of such undertaking, and to open a discussion on this topic, so far without any specific NLP application in mind. The collection is introduced under the, admi"
W19-8512,de-paiva-etal-2014-nomlex,0,0.0541557,"Missing"
W19-8512,W19-8511,1,0.723913,"d on a substantially revised lexeme set used originally in the Spanish Word-Formation Network (Lango et al., 2018). In DeriNet.ES, derivational relations were created using substitution rules covering Spanish affixation (Faryad, 2019). Resulting derivational families are organized into rooted trees. DeriNet.FA is a lexical database capturing derivations in Persian, which was created on top of manually compiled Persian Morphologically Segmented Lexicon (Ansari et al., 2019). By using automatic methods, derivationally related lexemes were identified and organized into DeriNet-like rooted trees (Haghdoost et al., 2019). DErivBase is a large-coverage lexicon for German (Zeller et al., 2013) in which derivational relations were created by using more than 190 derivational rules extracted from reference grammars of German. The resulting derivational families were automatically split into semantically consistent clusters, forming weakly connected subgraphs. The Morphosemantic Database from English WordNet 3.0 (hereafter, English WordNet) is a stand-off database linking morphologically related nouns and verbs from English WordNet (Miller, 1995) in which synonymous lexemes are grouped into so-called synsets, which"
W19-8512,L18-1291,1,0.916961,"es. Derivational families are represented by weakly connected subgraphs. 2Keeping the quadratic number of edges in the data might seem rather artificial at the beginning, however, it is a good starting point as it allows for applying graph algorithms analogously to other types. 102 DeriNet is a lexical database of Czech that captures derivational relations between lexemes. Each derivational family is represented as a rooted tree. DeriNet.ES is a DeriNet-like lexical database for Spanish which is based on a substantially revised lexeme set used originally in the Spanish Word-Formation Network (Lango et al., 2018). In DeriNet.ES, derivational relations were created using substitution rules covering Spanish affixation (Faryad, 2019). Resulting derivational families are organized into rooted trees. DeriNet.FA is a lexical database capturing derivations in Persian, which was created on top of manually compiled Persian Morphologically Segmented Lexicon (Ansari et al., 2019). By using automatic methods, derivationally related lexemes were identified and organized into DeriNet-like rooted trees (Haghdoost et al., 2019). DErivBase is a large-coverage lexicon for German (Zeller et al., 2013) in which derivatio"
W19-8512,C16-1213,0,0.0578999,"Missing"
W19-8512,2002.jeptalnrecital-long.22,0,0.114698,"es are then present only implicitly (based on shared sequences of morphemes). 3 Data resources selected for harmonization For the pilot stage of the harmonization project, we selected 11 data resources, all of them based either on rooted trees or weakly connected subgraphs (see B and C in Figure 1). The original resources (in alphabetical order) are briefly described below in this section. Démonette is a network containing lexemes assigned with morphological and semantic features. It was created by merging existing derivational resources for French (cf. Morphonette, Hathout, 2010; VerbAction, Tanguy and Hathout, 2002; and DériF, Namer, 2003). Démonette focuses on suffixation and captures also so-called indirect relations (representing sub-paradigms) and derivational series among lexemes. Derivational families are represented by weakly connected subgraphs. 2Keeping the quadratic number of edges in the data might seem rather artificial at the beginning, however, it is a good starting point as it allows for applying graph algorithms analogously to other types. 102 DeriNet is a lexical database of Czech that captures derivational relations between lexemes. Each derivational family is represented as a rooted t"
W19-8512,W19-8510,1,0.882564,"ivational relations within the groups underspecified (cf. DerivBase.hr for Croatian, Šnajder, 2014). Such derivational families could be represented as complete subgraphs. However, given that the structure models linguistic derivation, we should represent such derivational families rather by complete directed subgraphs (see A in Figure 1).2 B. If at most one base lexeme is captured for any derived lexeme, then the derivational family can be naturally represented as a rooted tree with a designated root node representing a lexeme that is considered as further unmotivated (cf. DeriNet for Czech, Vidra et al., 2019a; B in Figure 1). C. A weakly connected subgraph (in which any lexeme can have more than one base lexeme) is used for representing derivational families in resources in which the rooted-tree constraint does not hold, e.g. in Démonette for French (Hathout and Namer, 2014; C in Figure 1). D. A derivation tree (in the terminology of Context Free Grammars), with morphemes in its leaf nodes and artificial symbols in non-terminal nodes, can be used for describing how a lexeme is composed of individual morphemes (cf. Dutch section of CELEX2, Baayen et al., 1995, D in Figure 1); derivational relation"
W19-8512,P13-1118,0,0.0303653,"-Formation Network (Lango et al., 2018). In DeriNet.ES, derivational relations were created using substitution rules covering Spanish affixation (Faryad, 2019). Resulting derivational families are organized into rooted trees. DeriNet.FA is a lexical database capturing derivations in Persian, which was created on top of manually compiled Persian Morphologically Segmented Lexicon (Ansari et al., 2019). By using automatic methods, derivationally related lexemes were identified and organized into DeriNet-like rooted trees (Haghdoost et al., 2019). DErivBase is a large-coverage lexicon for German (Zeller et al., 2013) in which derivational relations were created by using more than 190 derivational rules extracted from reference grammars of German. The resulting derivational families were automatically split into semantically consistent clusters, forming weakly connected subgraphs. The Morphosemantic Database from English WordNet 3.0 (hereafter, English WordNet) is a stand-off database linking morphologically related nouns and verbs from English WordNet (Miller, 1995) in which synonymous lexemes are grouped into so-called synsets, which are further organized according to the hyponymy/hyperonymy relations. D"
W19-8512,snajder-2014-derivbase,0,0.0184192,"subgraph, B. rooted tree, C. weakly connected subgraph, D. derivation tree. into four types listed below. In the first three types, lexemes are represented as nodes and derivational relations as directed edges, pointing to a derived lexeme from its base lexeme, while in the fourth type the basic building unit is the morpheme. A. In some resources, derivationally related lexemes (i.e. lexemes that share a common root morpheme; hereafter, a derivational family) are simply grouped together, leaving particular derivational relations within the groups underspecified (cf. DerivBase.hr for Croatian, Šnajder, 2014). Such derivational families could be represented as complete subgraphs. However, given that the structure models linguistic derivation, we should represent such derivational families rather by complete directed subgraphs (see A in Figure 1).2 B. If at most one base lexeme is captured for any derived lexeme, then the derivational family can be naturally represented as a rooted tree with a designated root node representing a lexeme that is considered as further unmotivated (cf. DeriNet for Czech, Vidra et al., 2019a; B in Figure 1). C. A weakly connected subgraph (in which any lexeme can have m"
Y16-2018,I05-1075,0,\N,Missing
Y16-2018,N01-1026,0,\N,Missing
Y16-2018,D10-1056,0,\N,Missing
Y16-2018,petrov-etal-2012-universal,0,\N,Missing
Y16-2018,I08-3008,1,\N,Missing
Y16-2018,W02-2006,0,\N,Missing
Y16-2018,D11-1006,0,\N,Missing
Y16-2018,P13-2112,0,\N,Missing
Y16-2018,P15-2044,0,\N,Missing
Y16-2018,L16-1262,1,\N,Missing
Y16-2018,L16-1497,0,\N,Missing
zabokrtsky-etal-2002-machine,stranakova-lopatkova-zabokrtsky-2002-valency,0,\N,Missing
zeman-etal-2012-hamledt,zeman-2008-reusable,1,\N,Missing
zeman-etal-2012-hamledt,bosco-etal-2010-comparing,0,\N,Missing
zeman-etal-2012-hamledt,W08-2121,0,\N,Missing
zeman-etal-2012-hamledt,C00-2143,0,\N,Missing
zeman-etal-2012-hamledt,P06-1033,0,\N,Missing
zeman-etal-2012-hamledt,W08-0325,1,\N,Missing
zeman-etal-2012-hamledt,D11-1036,0,\N,Missing
zeman-etal-2012-hamledt,D11-1006,0,\N,Missing
zeman-etal-2012-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
zeman-etal-2012-hamledt,R09-1007,0,\N,Missing
zeman-etal-2012-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
zeman-etal-2012-hamledt,taule-etal-2008-ancora,0,\N,Missing
zeman-etal-2012-hamledt,afonso-etal-2002-floresta,0,\N,Missing
