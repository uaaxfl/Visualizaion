2003.mtsummit-papers.37,J93-2003,0,0.00482477,"Missing"
2003.mtsummit-papers.37,carbonell-etal-2002-automatic,0,0.0305704,"period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of resources that that we used in our experiments:1 • B: 912,775 words of verse-aligned parallel text from the Bible, provided by the University of Maryland. Both the English and the Cebuano bibles were obtained from the Web in character-coded form. The English Bible was the World English Bible (WEB) version. Verse alignment was used in lieu of sentence alignment for this data. • E: 214,327 words of parallel text from examples of usage that were automatically extracted fro"
2003.mtsummit-papers.37,W01-1409,0,0.1079,"999). The first foray that we are aware of into rapid development of statistical machine translation systems was by a team at the 1999 Johns Hopkins Summer Workshop. They built a Chinese-to-English MT system in one day, although the parallel text collection that they used had been assembled over an extended period at considerable expense by the Linguistic Data Consortium (AlOnizan et al., 1994). Germann was the first to try similar techniques with rapidly developed resources, building a Tamil-to-English MT system by manually translating 24,000 words of Tamil into English in a six week period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002"
2003.mtsummit-papers.37,P98-2160,0,0.0236882,"this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of resources that that we used in our experiments:1 • B: 912,775 words of verse-aligned parallel text from the Bible, provided by the University of Maryland. Both the English and the Cebuano bibles were obtained from the Web in character-coded form. The English Bible was the World English Bible (WEB) version. Verse alignment was used in lieu of sentence alignment for this data. • E: 214,327 words of parallel text from examples of usage that were automatically extracted from a printed bilingual dictionary af"
2003.mtsummit-papers.37,N03-2026,1,0.814744,"ting the language resources that this community will need in June. The Philippine language Cebuano was chosen for the dry run, and eleven institutions contributed to the resulting data collection and construction effort over the next ten days (Oard, 2003). Several teams used the resulting resources as a basis for constructing systems that worked with English and Cebuano. Dictionary-based CrossLanguage Information Retrieval (CLIR) proved to be a tractable task, with batch experiments demonstrating respectable retrieval effectiveness after three ∗ This work was performed while at USC-ISI. days (Oard et al., 2003) and two fully integrated interactive CLIR systems available by the tenth day. Machine Translation (MT) proved to be a bigger challenge, however. The interactive CLIR systems were forced to rely on term-by-term gloss translation, because suitable statistical machine translation results were not available during the dry run. Our purpose in this paper is to explore the potential for building MT systems using the resources that were constructed, with an eye towards contributing MT results for use in integrated systems during the surprise language experiment in June. Interactive CLIR systems that"
2003.mtsummit-papers.37,P02-1038,1,0.674161,"by using a log-linear model. In this framework, we have a set of M feature functions hm (e, f ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: Pr(e|f ) = pλM (e|f ) 1 (1) P exp[ M m=1 λm hm (e, f )] =P PM 0I e0 I exp[ m=1 λm hm (e 1 , f )] (2) 1 In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task. The basic translation model feature functions (FF) of our model are identical to the ones used in (Och and Ney, 2002): an alignment template FF, alignment template reordering FF, lexicon FF. The training process to obtain these feature functions makes use of a word-to-word alignment. More details can be found in (Och et al., 1999). In addition, we use a word penalty and an alignment template penalty feature function that count the number of produced words and the number of used alignment templates used to translate the sentence. Four English language models are used as separate feature functions: two from a large collection of news, one from the English Bible, and one from the English side of the training da"
2003.mtsummit-papers.37,W99-0604,1,0.761479,"ation probability is given by: Pr(e|f ) = pλM (e|f ) 1 (1) P exp[ M m=1 λm hm (e, f )] =P PM 0I e0 I exp[ m=1 λm hm (e 1 , f )] (2) 1 In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task. The basic translation model feature functions (FF) of our model are identical to the ones used in (Och and Ney, 2002): an alignment template FF, alignment template reordering FF, lexicon FF. The training process to obtain these feature functions makes use of a word-to-word alignment. More details can be found in (Och et al., 1999). In addition, we use a word penalty and an alignment template penalty feature function that count the number of produced words and the number of used alignment templates used to translate the sentence. Four English language models are used as separate feature functions: two from a large collection of news, one from the English Bible, and one from the English side of the training data. From separating out different language models into different feature functions, we expect that the used discriminative training procedure (see below) can counteract the fact that the dominating domains of our tr"
2003.mtsummit-papers.37,P03-1021,1,0.154965,"nts to obtaining suitable parameter values λM 1 for the M = 10 different parameters of the log-linear model given the development corpus (f1S , eS1 ): ˆ M = argmax M SOME-CRITERION(f S , eS ) λ 1 1 1 λ1 (3) A standard criterion for log-linear models is the MMI (maximum mutual information) criterion, which can be derived from the maximum entropy principle. Here, we use a different discriminative training procedure that directly optimizes translation quality measured by the BLEU metric on our development corpus. The used greedy search algorithm for the optimal parameter setting is described in (Och, 2003). The search problem for log-linear models amounts to solve the following optimization problem: ˆ = argmaxe pλˆM (e|f ) e (4) 1 = argmaxe M X λm hm (f , e) (5) m=1 We use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extract n-best candidate translations using A* search (Ueffing et al., 2002). These n-best candidate translations are the basis for discriminative training of the model parameters with respect to translation quality. 6 Results Figure 1 shows the results of our experiments. English is the language of instruction"
2003.mtsummit-papers.37,2001.mtsummit-papers.68,0,0.0712429,"eam at the 1999 Johns Hopkins Summer Workshop. They built a Chinese-to-English MT system in one day, although the parallel text collection that they used had been assembled over an extended period at considerable expense by the Linguistic Data Consortium (AlOnizan et al., 1994). Germann was the first to try similar techniques with rapidly developed resources, building a Tamil-to-English MT system by manually translating 24,000 words of Tamil into English in a six week period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of"
2003.mtsummit-papers.37,W02-1021,1,0.848088,"Missing"
2003.mtsummit-papers.37,P02-1040,0,\N,Missing
2003.mtsummit-papers.37,C98-2155,0,\N,Missing
2020.acl-main.29,D16-1211,0,0.0649997,"Missing"
2020.acl-main.29,Q17-1010,0,0.0302678,"label case. 4.5 Model Setup For each task and dataset, we use the same set of hyperparameters: Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 and weight decay 0.9. Dropout (Srivastava et al., 2014) is applied after each layer except the final classification layers; we use a single dropout probability of 0.1 for every instance. For models with exploration, we employ teacher forcing for 10 epochs. Model weights are initialized using Xavier normal initialization (Glorot and Bengio, 2010). All LSTM hidden-layer sizes are set to 200. We use fixed 300-dimensional FastText embeddings (Bojanowski et al., 2017) for both English and German, and project them down to 200 dimensions using a trainable linear layer. 5 Results and Analysis There are five major takeaways from the experimental results and analysis. First, the jointly trained S-LSTM model shows major improvement over prior work that modeled document segmentation and segment labeling tasks separately. Second, segment alignment and exploration during training reduces error rates. Third, the segment pooling layer leads to improvements for both segmentation and segment labeling. Fourth, S-LSTM outperforms an IOB-tagging CRF-decoded model for sing"
2020.acl-main.29,A00-2004,0,0.711231,"two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation that the segment bounds and topicality are tightly interwoven, and should ideally be considered jointly rather than sequentially. We start by examining three properties about text segmentation: (1) segment bounds and segment labels contain co"
2020.acl-main.29,N19-1423,0,0.0265637,"ls on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a single-label labeling task. 3 Mo"
2020.acl-main.29,D08-1035,0,0.852696,"ng a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivat"
2020.acl-main.29,S16-2016,0,0.785267,"pics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation that the segment bounds a"
2020.acl-main.29,P96-1024,0,0.149931,"Missing"
2020.acl-main.29,J97-1003,0,0.913733,"lem because it requires solving a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perf"
2020.acl-main.29,P18-1031,0,0.0202187,"(Section 3.2), and a segment pooling network which pools over predicted segments to classify them (Section 3.3). The segment predictor is allowed to make mistakes that the labeler must learn to be robust to, a process which we refer to as exploration, and accomplish by aligning predicted and ground truth segments (Section 3.4). The full architecture is presented in Figure 1, and the loss is discussed in Section 3.5. 3.1 Encoding Sentences The first stage is encoding sentences. S-LSTM is agnostic to the choice of sentence encoder, though in this work we use a concat pooled bi-directional LSTM (Howard and Ruder, 2018). First, the embedded words are passed through the LSTM encoder. Then, the maximum and mean of all hidden states are concatenated with the final hidden states, and this is used as the sentence encoding. 3.2 Predicting Segment Bounds The second step of our model is a Segment Predictor LSTM, which predicts segment boundaries within the document. For this step we use a bidirectional LSTM that consumes each sentence vector and predicts an indicator variable, (B)eginning or (I)nside a segment. It is trained from presegmented documents using a binary cross entropy loss. This indicator variable deter"
2020.acl-main.29,N18-2075,0,0.226637,"on (Swaffar et al., 1991; Ajideh, 2003). ? ∗ Work done while interning at Adobe. Uncovering latent, topically coherent segments of text is a difficult problem because it requires solving a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models ("
2020.acl-main.29,N16-1030,0,0.20192,"y, evaluating the pretrained models on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a"
2020.acl-main.29,W03-0430,0,0.0365605,"nt technique to assign partial credit to labels of incorrect segmentations, both for 314 training and evaluation. In addition, we explicitly consider the problem of model transferability, evaluating the pretrained models on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the lab"
2020.acl-main.29,P12-1009,1,0.810565,"s of coherence to find topic shifts in documents. Hearst (1997) introduced the TextTiling algorithm, which uses term co-occurrences to find coherent segments in a document. Eisenstein and Barzilay (2008) introduced BayesSeg, a Bayesian method that can incorporate other features such as cue phrases. Riedl and Biemann (2012) later introduced TopicTiling, which uses coherence shifts in topic vectors to find segment bounds. Glavaš et al. (2016) proposed GraphSeg, which constructs a semantic relatedness graph over the document using lexical features and word embeddings, and segments using cliques. Nguyen et al. (2012) proposed SITS, a model for topic segmentation in dialogues that incorporates a per-speaker likelihood to change topics. While the above models are unsupservised, Arnold et al. introduced a supervised method to compute sentence-level topic vectors using Wikipedia articles. The authors created the WikiSection dataset and proposed the SECTOR neural model. The SECTOR model predicts a label for each sentence, and then performs post hoc segmentation looking at the coherence of the latent sentence representations, addressing segmentation and labeling separately. We propose a model capable of jointly"
2020.acl-main.29,P03-1021,0,0.297244,"Missing"
2020.acl-main.29,N18-1202,0,0.0122959,"Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a single-label labeling task. 3 Modeling In order to jointly mode"
2020.acl-main.29,W12-3307,0,0.742529,"etermining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation tha"
2020.acl-main.29,tepper-etal-2012-statistical,0,0.449585,"o learn text segmentation as a supervised task. However, learning only to predict segment bounds does not necessarily capture the topicality of a segment that is useful for informative labeling. The task of document segmentation and labeling is well-studied in the clinical domain, where both segmenting and learning segment labels are important tasks. Pomares-Quimbaya et al. (2019) provide a current overview of work on clinical segmentation. Ganesan and Subotin (2014) trained a logistic regression model on a clinical segmentation task, though they did not consider the task of segment labeling. Tepper et al. (2012) considered both tasks of segmentation and segment labeling, and proposed a two-step pipelined method that first segments and then classifies the segments. Our proposed model is trained jointly on both the segmentation and segment labeling tasks. Concurrent work considers the task of document outline generation (Zhang et al., 2019). The goal of outline generation is to segment and generate (potentially hierarchical) headings for each segment. The authors propose the HiStGen model, a hierarchical LSTM model with a sequence decoder. The work offers an alternative view of the joint segmentation a"
2020.acl-main.723,W18-2501,0,0.0301728,"Missing"
2020.acl-main.723,D15-1162,0,0.0500021,"Missing"
2020.acl-main.723,N19-1357,0,0.0624915,"tention Attention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a"
2020.acl-main.723,W16-0312,0,0.135864,"07) and the rise of research on mental 1 Approximately: ACL is international, but these figures use prevalence statistics for U.S. adults (SAMHSA, 2019). Douglas W. Oard iSchool/UMIACS University of Maryland College Park, MD oard@umd.edu health using social media (Choudhury, 2013), algorithmic classification has reached the point where it can now dramatically outstrip performance of prior, more traditional prediction methods (Linthicum et al., 2019; Coppersmith et al., 2018). Further progress is on the way as the community shows increasing awareness and enthusiasm in this problem space (e.g., Milne et al., 2016; Losada et al., 2020; Zirikly et al., 2019). The bad news is that moving these methods from the lab into practice will create a major new challenge: identifying larger numbers of people who may require clinical assessment and intervention will increase stress on a severely resource-limited mental health ecosystem that cannot easily scale up.2 This motivates a reformulation of the technological problem from classification to prioritization of individuals who might be at risk, for clinicians or other suitably trained staff as downstream users. Perhaps the most basic way to do prioritization is"
2020.acl-main.723,N18-1100,0,0.0146865,"the downstream users’ prioritization task as taking a key step closer to the real-world problem. Hierarchical Attention Attention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a d"
2020.acl-main.723,D14-1162,0,0.0904087,"Missing"
2020.acl-main.723,D19-1002,0,0.0981771,"ttention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a"
2020.acl-main.723,N16-1174,0,0.137332,"of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a level up the representational hierarchy, learning also to weight documents to form representations of individuals. This allows us to jointly model ranking individuals and ranking their documents as potentially relevant evidence, without document-level"
2020.acl-main.723,D17-1322,0,0.446084,"d ** .. Figure 1: Illustration of an assessment framework in which individuals are ranked by predicted suicide risk based on social media posts, posts are ranked by expected usefulness for downstream review by a clinician, and word-attention highlighting helps foreground important information for risk assessment. Real Reddit posts, obfuscated and altered for privacy. Biased Gain (TBG, Smucker and Clarke, 2012), an IR evaluation measure that models the expected number of relevant items a user can find in a ranked list given a time budget. We observe that in many risk assessment settings (e.g., Yates et al. (2017); Coppersmith et al. (2018); Zirikly et al. (2019)), the available information comprises a (possibly large and/or longitudinal) set of documents, e.g. social media posts, associated with each individual, of which possibly only a small number contain a relevant signal.3 This gives rise to a formulation of our scenario as a nested, or hierarchical, ranking problem, in which individuals are ordered by priority, but each individual’s documents must also be ranked (Figure 1). Accordingly, we introduce hierarchical Time-Biased Gain (hTBG), a variant of TBG in which individuals are the top level rank"
2020.acl-main.723,W19-3003,1,0.83684,"Approximately: ACL is international, but these figures use prevalence statistics for U.S. adults (SAMHSA, 2019). Douglas W. Oard iSchool/UMIACS University of Maryland College Park, MD oard@umd.edu health using social media (Choudhury, 2013), algorithmic classification has reached the point where it can now dramatically outstrip performance of prior, more traditional prediction methods (Linthicum et al., 2019; Coppersmith et al., 2018). Further progress is on the way as the community shows increasing awareness and enthusiasm in this problem space (e.g., Milne et al., 2016; Losada et al., 2020; Zirikly et al., 2019). The bad news is that moving these methods from the lab into practice will create a major new challenge: identifying larger numbers of people who may require clinical assessment and intervention will increase stress on a severely resource-limited mental health ecosystem that cannot easily scale up.2 This motivates a reformulation of the technological problem from classification to prioritization of individuals who might be at risk, for clinicians or other suitably trained staff as downstream users. Perhaps the most basic way to do prioritization is with a single priority queue that the user s"
2020.acl-main.723,W18-0603,1,0.887191,"Missing"
2020.clssts-1.3,P19-3004,0,0.0613604,"Missing"
2020.clssts-1.3,1998.amta-tutorials.5,0,0.165633,"on Retrieval (CLIR) has repeatedly been a casualty of its own success. Research in the 1970’s focused on extending monolingual thesauri to multilingual thesauri. Although there were some issues to address involving the ways conceptual differences were reflected in different cultures (and thus in different languages), the thesaurus-based retrieval systems of the day proved to be relatively easily extended to include entry vocabulary from different languages. Thus, after publication of an ISO multilingual thesaurus standard in 1986 there was little further research left to do along those lines (Oard and Diekema, 1998). The 1990’s saw the rapid development of a different paradigm for CLIR, one in which queries were expressed in natural language and the system’s goal was to rank, not to select, documents. Much of the initial work focused on dictionary-based techniques and on techniques based on comparable corpora, but it was the introduction of techniques based on parallel text around the turn of the century that essentially solved the crosslanguage ranking problem (Nie, 2010). Of course, ranking is only useful in interactive applications if the searcher can recognize relevant documents, so success with cros"
2020.clssts-1.3,2020.clssts-1.2,0,0.0409311,"amount of Social Media/Blog content. Similarly, the amounts of News Broadcast and Topical Broadcast recordings are similar, and about two times larger than the amount of Conversational Speech. Experiments Corpus Description The IARPA MATERIAL corpus currently consists of document collections in six languages: Swahili, Tagalog, Somali, Lithuanian, Bulgarian, and Pashto. Our analysis is based on the Lithuanian collection, for which we have results from all three participating teams. Collection statistics are given in Table 1. Details of the collection and the annotation process can be found in (Zavorin et al., 2020). 3.2. Official Results We refer to the three participating systems as Teams A, B, and C to preserve anonymity. A comparison of scores for each team from the October 2019 evaluation is shown in Table 3. AQWV is the official program measure (NIST, 2016). Although the program objective is set-based retrieval, documents returned by each team also have a confidence score that can be used as a basis for ranking. This enables us to compute Mean Average Precision (MAP) on the returned list of documents, although we note that different systems return different numbers of relevant documents so the MAP"
2020.clssts-1.3,D19-6129,0,0.0130655,"ents can be translated into English, or queries and documents can be transformed into some other shared space (e.g., using embeddings). Evidence from multiple systems can also be combined by a variety of methods. Available data sources can be combined before retrieval, evidence from different systems can be combined during the matching phase, or the documents retrieved by different systems can be combined after the matching phase. Details on the approaches used by the SARAL team are described in (Boschee et al., 2019), the approaches used by the FLAIR team are described in (Zbib et al., 2019; Zhao et al., 2019), and the approaches used by the SCRIPTS team are described in (Oard et al., 2019). 3. 3.1. Document Genres The corpus contains both text documents and speech recordings, which can be further subdivided by the source. There are a total of 10,203 text documents and 3,297 speech recordings, each modality being broken into 3 different genres. Documents (a term used inclusively in MATERIAL to refer to both text documents and speech recordings) are thus provided in six genres (NIST, 2016): 1. News Text (Text) - newswire or reports. Formal language. 2. Topical Text (Text) - specialty articles or rep"
2020.eval4nlp-1.7,abdelali-etal-2014-amara,0,0.042145,"Missing"
2020.eval4nlp-1.7,P19-1310,0,0.0259153,"Missing"
2020.eval4nlp-1.7,P02-1040,0,0.112512,"se: r2 for Spearman’s = -0.001, slope = -0.010; r2 for Pearson’s = -0.002, slope = -0.083. Best viewed in color. tem would serve as an upstream component of the pipeline. It is less likely for translations to need to be fully valid to be useful as compared to some other MT tasks. CLIR could benefit from combining the terms from several translation outputs regardless of if each entire sentence is perfectly valid. In this way, a measure that can assign partial credit to translations by matching n-grams as well as weighting all translations equally may be appropriate. For this, we turn to B LEU (Papineni et al., 2002), which computes n-gram overlap between a system’s translations and the available references. This raises the question of how many references we should use when we have very many available, and which of the system translations we should be using in this computation. The STAPLE dataset provides an opportunity to explore this question. In this section, we compute B LEU measures with different numbers of references, to different depths in the n-best list. We find that at deep depths with many references B LEU ranks systems similarly to MAP, but that with fewer references its behavior is quite dif"
2020.eval4nlp-1.7,2021.eacl-main.115,0,0.0417359,"Missing"
2020.eval4nlp-1.7,N12-1017,0,0.0231632,"conditions are a representative set of sentences to be translated (in the STAPLE task, the prompts), the items are system-produced translations, and validity is whether a translation is proper (i.e., present in the STAPLE gold translations). 3.2 MAP’s reliance on binary validity rather than the preference order among valid translations simplified the generation of a gold standard, but the implicit assumption that the reference set of valid translations is complete is a potential concern. Due to the richness of human language, most sentences would admit an immense number of valid translations (Dreyer and Marcu, 2012). Even the STAPLE dataset used in this paper, which contains hundreds of valid reference translations for many sentences, is surely still not complete. This effect results in systems being penalized for false negatives, receiving lower MAP scores than they should. However, when our goal is to compare systems, we are most interested in relative, not absolute, scores. So the question to be answered is whether missing data in the ground truth adversely affects comparisons between systems. Zobel (1998) introduced a clever way to characterize such an effect. The key idea is to ablate the ground tru"
2020.eval4nlp-1.7,2020.ngt-1.22,1,0.770259,"Missing"
2020.eval4nlp-1.7,2005.mtsummit-papers.11,0,0.0371673,"Missing"
2020.eval4nlp-1.7,L16-1147,0,0.0568413,"Missing"
2020.eval4nlp-1.7,2020.ngt-1.28,0,0.012118,"fully consider what questions to ask when evaluating systems. The measures we propose are illustrative as answers to our research questions, but are not the only solutions; many others might work. We aim to provide groundwork and encourage future work on the topic. Our investigation is made possible by the recent availability of annotations created for the Duolingo Simultaneous Translation and Paraphrase for Language Education (STAPLE) shared task, which contains an extensive (although not necessarily exhaustive) set of valid translations for each of several thousand “input prompt” sentences (Mayhew et al., 2020). 2 JA some bad systems, many good ones, and many incremental variations in between, especially at the top end. These systems ranked among the best for these languages on the STAPLE leaderboard. All were variations of the following standard training procedure. We used Transformer architectures (Vaswani et al., 2017) trained with fairseq (Ott et al., 2019). Models included 6 encoder and decoder layers, a model size of 512, a feed forward layer size of 2048, and 8 attention heads. Models were trained with the A DAM optimizer (Kingma and Ba, 2015) with a dropout size of 0.1 and an effective batch"
2021.acl-long.126,D19-1290,1,0.78767,", or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First"
2021.acl-long.126,E17-1024,0,0.105542,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,W17-5104,0,0.114145,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,2020.emnlp-main.3,0,0.031921,"approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our syntopical graph is to systematically model the salient interactions of all claims in a col"
2021.acl-long.126,P19-3022,0,0.0191178,"order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express: V = (Nuclear Energy, env. impact, PRO) T"
2021.acl-long.126,C04-1051,0,0.324634,"nodes can have multiple edges of different types between them; a claim can both contradict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-cla"
2021.acl-long.126,P19-1456,0,0.0971144,"lated claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also"
2021.acl-long.126,W16-2816,0,0.0149183,"ce- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it woul"
2021.acl-long.126,P19-1049,0,0.0191126,"ose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance ba"
2021.acl-long.126,I13-1191,0,0.0251487,"as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and par"
2021.acl-long.126,2020.emnlp-main.4,0,0.0759605,"Missing"
2021.acl-long.126,W17-5114,0,0.0222256,"rt-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for ed"
2021.acl-long.126,C18-1316,0,0.0233389,"ructure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft lo"
2021.acl-long.126,2021.findings-acl.126,0,0.137173,"(Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the"
2021.acl-long.126,2021.ccl-1.108,0,0.0468629,"Missing"
2021.acl-long.126,N15-1046,0,0.0300103,"s. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation g"
2021.acl-long.126,2020.emnlp-demos.16,0,0.0599366,"Missing"
2021.acl-long.126,D15-1110,0,0.0280648,". Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to"
2021.acl-long.126,N13-1123,0,0.0269051,"erms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we inst"
2021.acl-long.126,W13-4008,0,0.0304941,"unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics,"
2021.acl-long.126,D19-1410,0,0.0877671,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,P19-1054,0,0.105891,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,2020.coling-main.426,0,0.0250927,", basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents"
2021.acl-long.126,P09-1026,0,0.0669409,"rk architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporatin"
2021.acl-long.126,W16-2814,0,0.0237338,"ntextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs."
2021.acl-long.126,2020.argmining-1.5,0,0.0354702,"uage inference model that predicts whether the claim entails the topic. We initialize the document representations with a sentence vectorizer over the text of the document. 4 Viewpoint Reconstruction A viewpoint can be understood as a judgment of some aspect of a topic that conveys a stance towards the topic. The goal of viewpoint reconstruction is to identify the set of viewpoints in a collection given a topic, starting with the claims. An example of this process is shown on the right in Figure 1. To denote viewpoints, we borrow notation in line with the idea of aspect-based argument mining (Trautmann, 2020), which in turn was inspired by aspectbased sentiment analysis. In particular, we express a viewpoint as a triple V : V = (topic, aspect, stance) A claim is an expression of a viewpoint in natural language, and a single viewpoint can be expressed in several ways throughout a collection in many claims. Aspects are facets of the broader argument around the topic. While some actual claims may encode multiple viewpoints simultaneously, henceforth we consider each claim to encode one viewpoint for simplicity. To tackle viewpoint reconstruction computationally, we decompose it into two sub-tasks, st"
2021.acl-long.126,D17-1165,0,0.0242068,"h different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our synt"
2021.acl-long.126,W17-5106,1,0.932597,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,E17-1105,1,0.929104,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,N18-1101,0,0.0323565,"adict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-claim edges have a single type, contains, with an edge weight of 1. We compute each of the"
2021.acl-long.126,2020.acl-main.291,0,0.0143002,"in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures i"
2021.acl-long.300,D19-1352,0,0.0174858,"26 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus. Lang. Pair Augmented Dataset Size EN-SO EN-SW EN-TL 1,649,484 2,014,838 2,417,448 Table 2: Augmented dataset statistics; “augmented dataset size” refers to total number of positive and negative query-sentence samples in the augmented dataset. bel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019). Given a document D = [S1 , . . . , S|D |], which is a sequence of sentences, and a query Q, following Liu and Croft (2002) we assign a relevance score by: rˆ = max p(r = 1|Q, S; W ) S∈D 4.3 We initialize English word embeddings with word2vec (Mikolov et al., 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al., 2018). For training we use a SparseAdam (Kingma and Ba, 2015) optimizer with learning rate 0.001. The hyperparameter λ2 in Section 3.3 is set to be 3 so that Lrel and λ2 Lrat are approximately on the same scale during training. More details on experiments are inc"
2021.acl-long.300,P18-1073,0,0.0160745,". In other work, Xu and Weischedel (2000) use a 2-state hidden Markov model (HMM) to estimate the probability that a passage is relevant given the query. Cross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vuli´c and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018). Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings. Our approach differs from previous cross-lingual word embedding methods in two aspects. First, the focus of previous work has mostly been on learning a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approa"
2021.acl-long.300,D18-1216,0,0.0231306,"provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synon"
2021.acl-long.300,P19-3004,0,0.0145662,"a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approach trains embeddings on an artificial labeled dataset augmented from a parallel corpus and directly represents relevance across languages. Our data augmentation scheme to build a relevance model is inspired by Boschee et al. (2019), but we achieve significant performance improvement by incorporating rationale information into the embedding training process and provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorpor"
2021.acl-long.300,P17-1171,0,0.022581,"’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanovi´c et al., 2010). These findings are validated by empirical results of experiments in a low-resource sentence selection task, with English queries over sentence collections of text and speech in Somali, Swahili, and Tagalog. 2 Related Work Query-focused Sentence Selection Sentencelevel query relevance prediction is important for various downstream NLP tasks such as queryfocused summarization (Baumel et al., 2016, 2018; Feigenblat et al., 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018). Such applications often depend on a sentence selection system to provide attention signals on which sentences to focus upon to generate a query-focused summary or answer a question. Cross-language Sentence Selection A common approach to cross-language sentence selection is to use MT to first translate either the query or the sentence to the same language and then perform standard monolingual IR (Nie, 2010). The risk of this approach is that errors in translation cascade to the IR system. As an alternative to generating full translations, PSQ (Darwish"
2021.acl-long.300,2016.amta-researchers.10,0,0.0221643,"do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synonym can be found in E 0 is a monolingual task. If we can verify that there is no direct match or synonym equivalent of q in E 0 then by transitivity it is unlikely there exists a translat"
2021.acl-long.300,2020.acl-main.747,0,0.0940514,"Missing"
2021.acl-long.300,N19-1423,0,0.0174726,"T) and speech (S) for Tagalog. data as our SECLR models. The 1-best output from each MT system is then scored with Indri (Strohman et al., 2005) to obtain relevance scores. Details of NMT and SMT systems are included in Appendix C.2. PSQ. To implement the PSQ model of Darwish and Oard (2003), we use the same alignment matrix as in rationale training (see Section 3.3) exMultilingual XLM-RoBERTa. We compare our model to the cross-lingual model XLM-RoBERTa (Conneau et al., 2020), which in previous research has been shown to have better performance on lowresource languages than multilingual BERT (Devlin et al., 2019). We use the Hugging Face implementation (Wolf et al., 2019) of XLM-RoBERTa (Base). We fine-tuned the model on the same augmented dataset of labeled query-sentence pairs as the SECLR models, but we apply the XLMRoBERTa tokenizer before feeding examples to the model. We fine-tuned the model for four epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 2 × 10−5 . Since XLMRoBERTa is pretrained on Somali and Swahili but not Tagalog, we only compare our models to XLMRoBERTa on Somali and Swahili. 3886 5 Results and Discussion 6 We report Mean Average Precision (MAP) of"
2021.acl-long.300,W11-2123,0,0.217567,"l corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired"
2021.acl-long.300,N19-1357,0,0.104894,"rnational Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech. Additionally, (iii) we propose a rationale training secondary objective to further improve SECLR performance, which we call SECLR-RT. Finally, (iv) we conduct training data ablation and h"
2021.acl-long.300,D18-1330,0,0.0676993,"Missing"
2021.acl-long.300,W18-6478,0,0.0350646,"Missing"
2021.acl-long.300,kamholz-etal-2014-panlex,0,0.0210779,"none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus."
2021.acl-long.300,P07-2045,0,0.0263233,"these noisy parallel corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale tr"
2021.acl-long.300,E17-1072,0,0.303623,"allel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and"
2021.acl-long.300,W15-1521,0,0.157957,"MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with bin"
2021.acl-long.300,J03-1002,0,0.0454723,"n task: Lrel = − log p(r|q, S; W ) 3.3 Guided Alignment with Rationale Training We can improve SECLR by incorporating additional alignment information as a secondary training objective, yielding SECLR-RT. Our intuition is that after training, the word sˆ = arg maxs∈S ws |wq should correspond to a translation of q. However, it is possible that sˆ simply co-occurs frequently with the true translation in our parallel data but its association is coincidental or irrelevant outside the training contexts. We use alignment information to correct for this. We run two SMT word alignment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al., 2009), on the orginal parallel sentence corpus. The two resulting alignments are concatenated as in Zbib et al. (2019) to estimate a unidirectional probabilistic word translation matrix A ∈ [0, 1]|VQ |×|VS |, such that A maps each word in the query language vocabulary to a list of document language words with different probabilities, i.e. P Aq,s is the probability of translating q to s and s∈VS Aq,s = 1. For each relevant training sample, i.e. (q, S, r = 1), we create a rationale distribution ρ ∈ [0, 1]|S| αs = P for s ∈ S. To encourage α to match ρ, we"
2021.acl-long.300,D17-1039,0,0.0752852,"Missing"
2021.acl-long.300,2020.clssts-1.1,0,0.0136766,"system development, and the latter being a larger evaluation corpus. In our main experiments we do not use Analysis or Dev for development and so we report results for all three (the ground truth relevance judgements for the TL Eval collection have not been released yet so we do not report Eval for TL). See Table 3 for evaluation statistics. All queries are text. The speech documents are first transcribed with an ASR system (Ragni and Gales, 2018), and the 1-best ASR output is used in the sentence selection task. Examples of the evaluation datasets are shown in Appendix B. We refer readers to Rubino (2020) for further details about MATERIAL test collections used in this work. While our model and baselines work at the sentence-level, the MATERIAL relevance judgements are only at the document level. Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and ZoExperiment Settings Baselines Cross-Lingual Word Embeddings. We compare our model with three other cross-lingual embedding methods, Bivec (Luong et al., 2015), MUSE (Lample et al., 2018), and SID-SGNS (Levy et al., 2017). Bivec and SID-SGNS are tr"
2021.acl-long.300,tiedemann-2012-parallel,0,0.049924,"he translation matrix, and positive samples where none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tok"
2021.acl-long.300,W00-1312,0,0.613969,"on. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevanc"
C12-1164,P05-1033,0,0.0157314,"and (3). Within the #weight structure, terms follow their probabilities, which correspond to the P rtoken values in these equations. Notice that the translation distribution for the source token leave is uninformed by the context maternity leave, therefore the candidates laisser (Eng. let go, allow) and quitter (Eng. quit) have higher probabilities than congé (Eng. vacation, day off) in this model. 2.2 Machine Translation for Cross-Language IR State-of-the-art statistical MT systems typically use hierarchical phrase-based translation models based on a Synchronous Context-Free Grammar (SCFG) (Chiang, 2005). In an SCFG, the rule [X] ||α ||β ||A ||ℓ(α → β) indicates that the context free expansion X → α in the source language occurs synchronously with X → β in the target language, with a likelihood of ℓ(α → β).1 In this case, we call α the Left-Hand Side (LHS) of the rule, and β the Right-Hand Side (RHS) of the rule. We use indexed nonterminals (e.g., [X,1]) since in principle more than one nonterminal can appear on the right side. A sequence of token alignments A indicates which token in α is aligned to which target token in β. Consider the following four rules from an SCFG: R1 .[S] ||[S,1] ||[S"
C12-1164,J07-2003,0,0.327277,"of s, computed by: t (1) = arg max[ max ℓ(t, D|s)] = arg max[ max TM(t, D|s)LM(t)] t t D∈D(s,t) D∈D(s,t) Y   = arg max LM(t) max ℓ(r) t D∈D(s,t) (5) r∈D where TM and LM correspond to the translation and language model scores, and D(s, t) is the set of possible derivations that generates the pair of sentences (s, t) (e.g., the sequence of four rules that translate the example query in Section 2.2 is one such derivation). The likelihood of each grammar rule r, ℓ(r), is learned as part of the training process of the translation model, by generalizing from token alignments on the training data (Chiang, 2007). Decoders produce a set of candidate sentence translations in the process of computing equation (5), so we can generalize our model to consider the n candidates with the highest likelihoods, for some n > 1. We start by preprocessing the source query s and each candidate translation t (k) . For each source token s j , we use the derivation output to determine which grammar rules were used to produce t (k) , and the token alignments in these rules to determine which target tokens are associated with s j in the derivation. By doing this for each translation candidate t (k) , we construct a proba"
C12-1164,P10-4002,1,0.900468,"-Free Grammar (SCFG), in which probabilistic rules describe the translation of larger units of text. Finally, the translation grammar is combined with a language model to produce translations of entire sentences. As the whole process is statistically generated, it is at any point able to produce a ranked list of the highest scoring translations rather than only the one best choice. Although it is desirable to exploit these internal representations when performing retrieval, one possible disadvantage of using such a complex translation model is efficiency. However, modern decoders, e.g., cdec (Dyer et al., 2010), use pruning methods to efficiently search for the most likely translations of a given text. In this paper, we describe two ways to exploit these internal representations and construct context-sensitive term translation probabilities. One method is to extract a context-aware portion of the SCFG by selecting only the grammar rules that apply to a given query. Using token alignments within each rule, a probability distribution can be constructed to represent the translation candidates for each query token, an approach that we refer to as “phrase-based.” Another solution is to perform translatio"
C12-1164,D12-1061,0,0.0108839,"Missing"
C12-1164,N03-1017,0,0.0176968,"rnel, ...) #weight(1.0 cong) #weight(1.0 europ)) The overfitting issue is partially mitigated by using the n-best translation derivations, as opposed to the “1-best translation” approach, which treats the MT system as a black box. However, the lack of textual variety in the n most probable derivations is a known issue, caused by the fact that statistical MT systems identify the most probable derivations (not the most probable strings), many of which can correspond to the same surface form. This phenomenon is called “spurious ambiguity” in the MT literature, and it occurs in both phrase-based (Koehn et al., 2003) and hierarchical phrase-based MT systems (Chiang, 2007). For instance, according to Li et al. (2009), a string has an average of 115 distinct derivations in Chiang’s Hiero system. Researchers have proposed several ways to cope with this situation, and we plan to integrate some of these in our future work. However, an alternative approach is to exploit grammar rules directly: this allows us to increase variety without introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn con"
C12-1164,J03-3003,0,0.0243537,"Missing"
C12-1164,P09-1067,0,0.0130128,"he n-best translation derivations, as opposed to the “1-best translation” approach, which treats the MT system as a black box. However, the lack of textual variety in the n most probable derivations is a known issue, caused by the fact that statistical MT systems identify the most probable derivations (not the most probable strings), many of which can correspond to the same surface form. This phenomenon is called “spurious ambiguity” in the MT literature, and it occurs in both phrase-based (Koehn et al., 2003) and hierarchical phrase-based MT systems (Chiang, 2007). For instance, according to Li et al. (2009), a string has an average of 115 distinct derivations in Chiang’s Hiero system. Researchers have proposed several ways to cope with this situation, and we plan to integrate some of these in our future work. However, an alternative approach is to exploit grammar rules directly: this allows us to increase variety without introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn context-sensitive translation probabilities directly from the translation grammar. Hierarchical phrase-b"
C12-1164,D07-1104,0,0.0379392,"ut introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn context-sensitive translation probabilities directly from the translation grammar. Hierarchical phrase-based MT systems use suffix arrays to extract all rules in an SCFG which apply to a given source text, requiring a 3 Since a source token may be aligned to multiple target tokens in the same query translation, we still need to normalize the final likelihood values. 2690 smaller memory footprint in the decoding phase (Lopez, 2007). We can use this feature to learn a token translation probability mapping that is a middle point between P rtoken and P rnbest in terms of context-aware choices and providing a varied set of translation alternatives. We propose the following method to construct a probability distribution from a set of SCFG rules: For each grammar rule, we use the token alignments to determine which source token translates to which target token(s) in the phrase pair. Going over all grammar rules that apply to a given query, we construct a probability distribution for each token that appears on the LHS. More sp"
C12-1164,P99-1027,0,0.443368,"cal Papers, pages 2685–2702, COLING 2012, Mumbai, December 2012. 2685 1 Introduction Cross-Language Information Retrieval (CLIR) is the problem of retrieving documents relevant to a query written in a different language. There are two main approaches to tackle this problem: translating the query into the document language, or translating documents into the query language. Query translation has become the more popular approach for experimental work due to the computational feasibility of trying different system variants without repeatedly translating the entire document collection (Oard, 1998; McCarley, 1999). Query translation approaches for CLIR can be pursued either by applying a Machine Translation (MT) system or by using a token-to-token bilingual mapping, with or without translation probabilities. These approaches have complementary strengths: MT makes good use of context but at the cost of typically producing only one-best results, while token-to-token mappings can produce n-best token translations but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as “context recovery” in which postprocessing techniques are used to select o"
C12-1164,E12-1012,0,0.19374,"Missing"
C12-1164,oard-1998-comparative,1,0.792241,"2012: Technical Papers, pages 2685–2702, COLING 2012, Mumbai, December 2012. 2685 1 Introduction Cross-Language Information Retrieval (CLIR) is the problem of retrieving documents relevant to a query written in a different language. There are two main approaches to tackle this problem: translating the query into the document language, or translating documents into the query language. Query translation has become the more popular approach for experimental work due to the computational feasibility of trying different system variants without repeatedly translating the entire document collection (Oard, 1998; McCarley, 1999). Query translation approaches for CLIR can be pursued either by applying a Machine Translation (MT) system or by using a token-to-token bilingual mapping, with or without translation probabilities. These approaches have complementary strengths: MT makes good use of context but at the cost of typically producing only one-best results, while token-to-token mappings can produce n-best token translations but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as “context recovery” in which postprocessing techniques are"
C12-1164,J03-1002,0,0.014117,"ocessing. The collections contain 383,872, 388,589 and 177,452 documents, and 50, 50, and 73 topics, respectively. We learned our English-to-Arabic translation model using 3.4 million aligned sentence pairs from the GALE 2010 evaluation. Our English-to-Chinese translation model was trained on 302,996 aligned sentence pairs from the FBIS parallel text collection. We trained an Englishto-French translation model using 2.2 million aligned sentence pairs from the latest Europarl corpus (version 7) that was built from the European parliament proceedings.4 Token alignments were learned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. An SCFG serves as the basis for the translation model (Chiang, 2007), which was extracted from these token alignments using a suffix array (Lopez, 2007). We used cdec for decoding, due to its support for SCFG-based models and its efficient C-based implementation, making it faster than most of the other state-of-the-art systems (Dyer et al., 2010). A 3-gram language model was trained from the target side of the training data for Chinese and Arabic, using the SRILM toolkit (Stolcke, 2002). For French, we trained a 5-gram LM from the monolingual dataset pro"
C12-1164,I05-3027,0,\N,Missing
C12-2034,P07-1003,0,0.0197426,"cal OCR error modeling, and their combination. 2 Statistical Transliteration for English-Bengali To begin we used the transliteration method described by Virga and Khudanpur [Virga and Khudanpur, 2003]. In this method, transliteration is viewed as a simple character translation task. We used the Joshua open source statistical machine translation system [Li et al., 2009] which is reconfigured in [Irvine et al., 2010] for transliteration. Pairs of transliterated words and characterbased n-gram language models are used in place of parallel sentences and word n-grams models. The Berkeley aligner [DeNero and Klein, 2007] is used to automatically align characters in pairs of transliterations. The language models are then trained on 2- through 10-gram sequences of target language characters. The goal is to minimize the edit distance between the system&apos;s output and the reference transliterations. This optimization is done by using the Joshua&apos;s Minimum Error Rate Training (MERT) and a character based BLEU score objective function (BLEU-4). 340 FIGURE 1 - Plots of (a) transliteration accuracy (1 best) and average normalized edit distance with the number of training samples and (b) N-best transliteration accuracy."
C12-2034,P80-1024,0,0.624146,"Missing"
C12-2034,2010.amta-papers.12,0,0.031054,"OCR-based CLIR results for any language (the first being English-to-Chinese [Tseng, 2001]). Our results show large and statistically significant improvements from statistical transliteration, statistical OCR error modeling, and their combination. 2 Statistical Transliteration for English-Bengali To begin we used the transliteration method described by Virga and Khudanpur [Virga and Khudanpur, 2003]. In this method, transliteration is viewed as a simple character translation task. We used the Joshua open source statistical machine translation system [Li et al., 2009] which is reconfigured in [Irvine et al., 2010] for transliteration. Pairs of transliterated words and characterbased n-gram language models are used in place of parallel sentences and word n-grams models. The Berkeley aligner [DeNero and Klein, 2007] is used to automatically align characters in pairs of transliterations. The language models are then trained on 2- through 10-gram sequences of target language characters. The goal is to minimize the edit distance between the system&apos;s output and the reference transliterations. This optimization is done by using the Joshua&apos;s Minimum Error Rate Training (MERT) and a character based BLEU score"
C12-2034,W09-0424,0,0.0648471,"he best of our knowledge is only the second OCR-based CLIR results for any language (the first being English-to-Chinese [Tseng, 2001]). Our results show large and statistically significant improvements from statistical transliteration, statistical OCR error modeling, and their combination. 2 Statistical Transliteration for English-Bengali To begin we used the transliteration method described by Virga and Khudanpur [Virga and Khudanpur, 2003]. In this method, transliteration is viewed as a simple character translation task. We used the Joshua open source statistical machine translation system [Li et al., 2009] which is reconfigured in [Irvine et al., 2010] for transliteration. Pairs of transliterated words and characterbased n-gram language models are used in place of parallel sentences and word n-grams models. The Berkeley aligner [DeNero and Klein, 2007] is used to automatically align characters in pairs of transliterations. The language models are then trained on 2- through 10-gram sequences of target language characters. The goal is to minimize the edit distance between the system&apos;s output and the reference transliterations. This optimization is done by using the Joshua&apos;s Minimum Error Rate Tr"
C12-2034,W03-1508,0,0.0355165,"ngali-to-Bengali) OCR&apos;d document retrieval [Garain, 2011b; Ghosh, 2011]. In this paper we report the first CLIR results for OCR’d Bengali documents using English queries, which to the best of our knowledge is only the second OCR-based CLIR results for any language (the first being English-to-Chinese [Tseng, 2001]). Our results show large and statistically significant improvements from statistical transliteration, statistical OCR error modeling, and their combination. 2 Statistical Transliteration for English-Bengali To begin we used the transliteration method described by Virga and Khudanpur [Virga and Khudanpur, 2003]. In this method, transliteration is viewed as a simple character translation task. We used the Joshua open source statistical machine translation system [Li et al., 2009] which is reconfigured in [Irvine et al., 2010] for transliteration. Pairs of transliterated words and characterbased n-gram language models are used in place of parallel sentences and word n-grams models. The Berkeley aligner [DeNero and Klein, 2007] is used to automatically align characters in pairs of transliterations. The language models are then trained on 2- through 10-gram sequences of target language characters. The"
H01-1033,J93-1003,0,0.127112,"Missing"
H01-1033,P99-1068,1,0.821638,"Missing"
H01-1050,1998.amta-tutorials.5,0,\N,Missing
H01-1050,W98-1005,0,\N,Missing
H01-1050,J95-4004,0,\N,Missing
H01-1050,A97-1029,0,\N,Missing
I11-1029,W08-1402,0,0.0140177,"i et al. (2011) present a comprehensive survey of the topic. Name matching does not demand transliteration though; transliteration is a generative process, and name matching requires only that a known name pair be given a score representing the degree of match. Snae (2007) presents a survey of popular name matching algorithms from the record linkage perspective. Monolingually, Levenshtein distance (1966) and its variants are used for basic string matching in many contexts. Cross-language approaches typically combine cross-language mappings of some sort with edit distance metrics. For example, Mani et al. (2008) demonstrate a machine learning approach to the problem. The second crucial underlying technology is context matching. Monolingually, context matching can match on many contextual attributes, including words, entities, topics, or graph structures. Context matching in the translingual setting is closely related to cross-language information retrieval (CLIR); both tasks attempt to estimate the degree of similarity between texts written 1 2 in different languages. Kishida (2005) presents an overview of the key methods in CLIR. 3 Cross-Language Entity Linking Our approach to entity linking breaks"
I11-1029,E06-1002,0,0.00655203,"set of structured entity representations (usually called the knowledge base). Ji and Grishman (2011) present a good overview of the state of the art in monolingual entity linking, as practiced in the TAC evaluation. TAC data sets use a subset of Wikipedia entities for the knowledge base, manually curated query names, and ground truth identified by human assessors without pooling. Wikipedia has been another significant source of training and test data. Adafre and de Rijke (2005) explore automatically adding links between Wikipedia pages (albeit without focusing specifically on named entities). Bunescu and Pasca (2006) trained an SVM to predict whether a query entity matches a Wikipedia page by using hyperlinks within Wikipedia itself as the source of training and test data. Cucerzan (2007) studied identifying entity mentions in text and mapping them to Wikipedia articles. Mihalcea and Csomai (2007) and Milne and Witten (2008) each attempt to identify and properly induce hyperlinks for informative terms in Wikipedia articles (again without specific focus on named entities). Cross-language entity linking has not yet been Related Work Three types of named entity resolution are found in the literature: identit"
I11-1029,D07-1074,0,0.00862039,"racticed in the TAC evaluation. TAC data sets use a subset of Wikipedia entities for the knowledge base, manually curated query names, and ground truth identified by human assessors without pooling. Wikipedia has been another significant source of training and test data. Adafre and de Rijke (2005) explore automatically adding links between Wikipedia pages (albeit without focusing specifically on named entities). Bunescu and Pasca (2006) trained an SVM to predict whether a query entity matches a Wikipedia page by using hyperlinks within Wikipedia itself as the source of training and test data. Cucerzan (2007) studied identifying entity mentions in text and mapping them to Wikipedia articles. Mihalcea and Csomai (2007) and Milne and Witten (2008) each attempt to identify and properly induce hyperlinks for informative terms in Wikipedia articles (again without specific focus on named entities). Cross-language entity linking has not yet been Related Work Three types of named entity resolution are found in the literature: identity resolution, which matches structured or semi-structured entity descriptions, such as database records; coreference resolution, which clusters textual entity descriptions; an"
I11-1029,P09-1104,0,0.039447,"Missing"
I11-1029,P10-1142,0,0.0103292,"upporting law enforcement. Starting in 2009 the NIST Text Analysis Conference (TAC) began conducting evaluations 255 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 255–263, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP result. Coreference resolution operates over text, determining when two entity mentions refer to the same entity. Approaches to within-document coreference resolution typically exploit syntactic, grammatical and discourse-level features, information that is not available when trying to resolve references across documents. Ng (2010) presents a comprehensive review of recent approaches to within-document coreference resolution. In contrast, cross-document coreference resolution typically assumes that within-document references have been resolved, and tries to place all such mention chains that refer to the same entity into a single cluster that represents that entity. Because the kinds of document-specific features that guide within-document coreference resolution are missing, research in cross-document coreference resolution tends to be more directly applicable to entity linking (which also lacks those features). The Web"
I11-1029,2010.amta-papers.12,0,0.147775,"we use include: Table 1: Number of training pairs for transliterating to English from each language. • Name matching features between the query name (Qname ) and KB candidate (KBname ) • Text comparisons between the query document (Qdoc ) and the text associated with the KB candidate • Relation features, chiefly evidence from relations in the KB being evidenced in the Qdoc To perform cross-language candidate identification, we transliterate4 the query name to English, then apply our monolingual English heuristics. We used the multilingual transliteration system and training data developed by Irvine et al. (2010) in their experiments in orthographic transliteration. The number of training name/transliteration pairs varied by language and is given in Table 1. The source for most of this training data is Wikipedia, which contains links between article pages in multiple languages. 3.2 • Co-occurring entities, detected by running named entity recognition (NER) on the Qdoc and finding matching names in the candidate’s KB entry • Features pertaining to the entity type of the KB candidate • Indications that no candidate is correct and that NIL is therefore the appropriate response Candidate Ranking The secon"
I11-1029,P11-1115,0,0.00770026,"facti h/factsi hwiki textih![CDATA[Tony Blair Anthony Charles Lynton “Tony” Blair (born 6 May 1953) is a British politician who was Prime Minister of the United Kingdom from 2 May 1997 to 27 June 2007. He was Leader of the Labour Party from 1994 to 2007 and the Member of Parliament for Sedgefield from 1983 to 2007... Figure 2: Excerpt from the KB entry for Tony Blair (E0481157). From LDC2009E58. 2 Entity linking is a hybrid of the preceding two types of named entity resolution, matching a textual entity mention to a set of structured entity representations (usually called the knowledge base). Ji and Grishman (2011) present a good overview of the state of the art in monolingual entity linking, as practiced in the TAC evaluation. TAC data sets use a subset of Wikipedia entities for the knowledge base, manually curated query names, and ground truth identified by human assessors without pooling. Wikipedia has been another significant source of training and test data. Adafre and de Rijke (2005) explore automatically adding links between Wikipedia pages (albeit without focusing specifically on named entities). Bunescu and Pasca (2006) trained an SVM to predict whether a query entity matches a Wikipedia page b"
I11-1029,W09-1119,0,0.00836234,"ld be to manually translate the TAC documents and queries into each desired language. This would be prohibitively expensive. Instead, we use parallel document collections and crowdsourcing to generate ground truth in other languages. A fundamental insight on which our work is based is that if we build an entity linking test collection using the English half of a parallel text collection, we can make use of readily available annotators and tools developed specifically for English, then project the English results onto the other language. Thus, we apply English NER to find person names in text (Ratinov and Roth, 2009), our English entity linking system to identify candidate entity IDs, and English annotators on Amazon’s Mechanical Turk to select the correct Relation Features As can be seen in Figure 2, the KB contains a set of attributes and relations associated with each entity (e.g., age, employer, spouses, etc.). While one could run a relation extractor over the query document and look for relational equivalences, or contradictions, we chose a more straightforward approach: we simply treat the words from all facts as a surrogate “document” and calculate document similarity with the query document. 3.2.5"
I11-1029,1998.amta-tutorials.5,0,\N,Missing
L18-1328,L16-1593,0,0.0310394,"evance judgments, and some rank-based or set-based evaluation measures. We are, however, aware of only one shared task evaluation involving SMS messages: The FIRE SMS-Based FAQ Retrieval task. In that task, queries were posed using SMS, and a preexisting set of answers to Frequently Asked Questions (FAQ) provided the “document” set to be searched (Contractor and others, 2013). Our goal in this paper is to switch the focus from searching using SMS queries to searching SMS content itself. We studied the process adopted by LDC to create open-domain queries as part of the same DARPA BOLT program (Griffitt and Strassel, 2016). Those queries were developed for discussion forum posts and intended to be cross-lingual to some extent. We used a modified version of that process to create our queries to search the SMS content as described in section 4.. 3. Selecting the Messages The SMS collection that we have used was assembled by the LDC for the DARPA BOLT program and released in three phases (Song and others, 2014)1 . As released, the LDC corpora contained both English SMS messages and English text chat logs that were contributed for research use (in exchange for compensation) by individuals. Contributors were offered"
lawrie-etal-2012-creating,E06-1002,0,\N,Missing
lawrie-etal-2012-creating,D08-1027,0,\N,Missing
lawrie-etal-2012-creating,W09-1119,0,\N,Missing
lawrie-etal-2012-creating,W10-0701,0,\N,Missing
lawrie-etal-2012-creating,P09-1104,0,\N,Missing
lawrie-etal-2012-creating,2005.mtsummit-papers.11,0,\N,Missing
lawrie-etal-2012-creating,D07-1074,0,\N,Missing
lawrie-etal-2012-creating,W10-0700,0,\N,Missing
N03-2026,H01-1033,1,\N,Missing
N09-1021,D07-1091,0,0.0121536,"e non-zero fertility). Second, there is nothing to discourage query degradations which are unlikely under an (errorful) language model—that is, degradations that are not observed in the speech hypotheses. Finally, CMQD doesn’t account for similarities between phoneme classes. While some of these deficiencies could be addressed with an extension to CMQD (e.g., by expanding the degradation lattices to include language model scores), we can do better using a more powerful modeling framework. In particular, we adopt the approach of phrase-based statistical machine translation (Koehn et al., 2003; Koehn and Hoang, 2007). This approach allows for multiple-phoneme to multiple-phoneme substitutions, as well as the soft incorporation of additional linguistic knowledge (e.g., phoneme classes). This is related to previous work allowing higher order phoneme confusions in bigram or trigram contexts (Chaudhari and Picheny, 2007), although they used a fuzzy edit distance measure and did not incorporate other evidence in their model (e.g., the phoneme language model score). The reader is referred to (Koehn and Hoang, 2007; Koehn et al., 2007) for detailed information about phrase-based statistical machine translation."
N09-1021,N03-1017,0,0.00805944,"Missing"
N09-1021,P07-2045,0,0.0149078,"Missing"
N09-1021,P08-1053,1,0.916261,"ur new approach, which we call query degradation, hypothesizes many alternate “pronunciations” for the query word and incorporates them into the ranking function. These degradations are translations of the lexical phoneme sequence into the errorful recognition language, which we hypothesize using a factored phrase-based statistical machine translation system. Our speech collection is a set of oral history interviews from the MALACH collection (Byrne et al., 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007; Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b). The interviews were conducted with survivors and witnesses of the Holocaust, who discuss their experiences before, during, and after the Second World War. Their speech is predominately spontaneous and conversational. It is often also emotional and heavily accented. Because the speech contains many words unlikely to occur within a general purpose speech recognition lexicon, it repreHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182–190, c Boulder, Colorado, June 2009. 2009 Association"
N09-1021,N04-1017,0,0.0164205,"2 we introduce our baseline RUR methods. In Section 3 we introduce our query degradation approach. We introduce our experimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degradations can significantly improve upon a strong RUR baseline. Finally, in Section 6 we conclude and outline several directions for future work. 2 Generative Baseline Each method we present in this paper ranks the utterances by the term’s estimated frequency within the corresponding phoneme lattice. This general approach has previously been considered (Yu and Seide, 2005; Saraclar and Sproat, 2004), on the basis that it provides a minimum Bayes-risk ranking criterion (Yu et al., Sept 2005; Robertson, 1977) for the utterances. What differs for each method is the particular estimator of term frequency which is used. We first outline our baseline approach, a generative model for term frequency estimation. Recall that our vocabulary-independent indices contain the expected counts of phoneme sequences from our recognition lattices. Yu and Seide (2005) used these expected phoneme sequence counts to estimate term frequency in the following way. For a b is query term Q and lattice L, term frequ"
N09-1023,P08-1095,0,0.703717,"Missing"
N09-1023,N06-1052,0,0.0193909,"an exponential decay model to naturally encode time effect and assign differential weights to messages in its contexts. Another thread of related work is document expansion. It was previously studied in (Singhal et al., 1999) in the context of the speech retrieval, helping to overcome limitations in the transcription accuracy by selecting additional terms from lexically similar (text) documents. Document expansion has also been applied to cross-language retrieval in (Levow et al., 2005), in that case to overcome limitations in translation resources. The technique has recently been re-visited (Tao et al., 2006; Kurland et al., 2004; Liu et al., 2004) in the language modeling framework, where lexically related documents are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to"
N09-1023,E03-1058,0,0.0675736,"Missing"
N09-1023,P03-1071,0,0.00918932,"s are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to text segmentation (Ji et al., 2003) and meeting segmentation (Malioutov et al., 2006; Malioutov et al., 2007; Galley et al., 2003; Eisenstein et al., 2008). Text segmentation identifies boundaries of topic changes in long text documents, but we form threads of messages from streams consisting of short messages. Meeting conversations are not as highly interleaving as chat conversations, where participants can create a new conversation at any time. 3 Method This section describes our technique for clustering messages into threads based on the lexical similarity of documents that have been expanded based on social and temporal evidence. 202 3.1 Context-Free Message Model To represent the semantic information of messages an"
N09-1023,D08-1035,0,0.0379225,"Missing"
N09-1023,P07-1064,0,0.0425521,"xically related documents are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to text segmentation (Ji et al., 2003) and meeting segmentation (Malioutov et al., 2006; Malioutov et al., 2007; Galley et al., 2003; Eisenstein et al., 2008). Text segmentation identifies boundaries of topic changes in long text documents, but we form threads of messages from streams consisting of short messages. Meeting conversations are not as highly interleaving as chat conversations, where participants can create a new conversation at any time. 3 Method This section describes our technique for clustering messages into threads based on the lexical similarity of documents that have been expanded based on social and temporal evidence. 202 3.1 Context-Free Message Model To represent the semantic infor"
N09-1023,P06-1004,0,\N,Missing
N12-1046,W09-0432,0,0.0157583,"uch situations, which is why we implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that w"
N12-1046,2008.amta-papers.2,0,0.0151767,"ntence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in fav"
N12-1046,W09-2404,0,0.486585,"slating each sentence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat sug"
N12-1046,P05-1033,0,0.0855443,"e hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, although of course ﬁner-scale or broader-scale discourse units might also be explored in future work. Moreover, throughout this paper we use the hierarchical phrase-based translation system (Hiero), which is based on a synchronous contextfree grammar (SCFG) model (Chiang, 2005). In a can occur synchronously with X → β in the target language. In this case, we call α the left hand side (LHS) of the rule, and β the right hand side (RHS) of the rule. To determine the extent and nature of translation consistency choices made by human translators, we randomly selected one of the four sets of reference translations (ﬁrst set, with id 0) and we used forced decoding to ﬁnd all possible sequences of rules that could transform the source sentence into the target sentence. In forced decoding, given a pair of source and target sentences, and a grammar consisting of learned trans"
N12-1046,J07-2003,0,0.131737,"s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing"
N12-1046,P02-1033,1,0.5986,"utively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-translation-per-discourse hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, alt"
N12-1046,P08-1115,1,0.830792,"for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with mu"
N12-1046,P10-4002,1,0.127564,"fi is aligned to ej . After the ﬁrst pass, we compute the feature value of each observed pair, based on this count and the DF of the target-side of the pair. We chose to use only the target token in the DF computation (i.e., aggregating over all source tokens) to 422 C3 (r) = max f ∈LHS(r) e∈RHS(r) ⟨f,e⟩ aligned bm25(⟨f, e⟩) (4) Since each variant has its beneﬁts and drawbacks, we can include all three in the system and let the tuning process decide on how each should be weighted. 5 Evaluation and Discussion We have evaluated the one-translation-per-discourse feature using the cdec MT system (Dyer et al., 2010). We started by building a baseline system using standard features in cdec: lexical and phrase translation probabilities in both directions, word and arity penalty features, and a 5-gram language model. We then added each of the three consistency feature variants, along with all two-way and the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pa"
N12-1046,H92-1045,0,0.63396,"ation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more integrated approaches. Xiao et al. (2011) took this one step further and implement an approach where they identiﬁed ambiguous translations within each document, and at417 2012 Conference of the North Am"
N12-1046,W07-0719,0,0.0457346,"Missing"
N12-1046,2005.eamt-1.19,0,0.0291934,"we implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in t"
N12-1046,P10-1085,0,0.0189264,"l approach of translating each sentence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, wh"
N12-1046,P11-1124,0,0.0618186,"Missing"
N12-1046,maamouri-etal-2008-enhancing,0,0.0221488,"d the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignme"
N12-1046,D11-1080,0,0.0203345,"9 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En with a BLEU score of 30.96. In any case, all three variants outperformed the baseline (see Table 2). When multiple features were combined, we generally observed an increase in BLEU,"
N12-1046,P03-1058,0,0.0267734,"on choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-translation-per-discourse hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, although of course ﬁn"
N12-1046,J03-1002,0,0.00364752,"n tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For"
N12-1046,P02-1040,0,0.106883,"as trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing, we used the MT08 dataset described above (74 documents, 813 sentences"
N12-1046,W10-2602,0,0.588246,"score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of s"
N12-1046,2011.mtsummit-papers.13,0,0.170413,"ndently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more"
N12-1046,P95-1026,0,0.399604,"n per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more integrated approaches. Xiao et al. (2011) took this one step further and implement an approach where they identiﬁed ambiguous translations within each document, and at417 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human La"
N12-1046,C04-1059,0,0.0176229,"each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-t"
N12-1046,P11-1085,0,0.0142718,"ments. When we used NIST’s ofﬁcial metric (BLEU-4) to compare our results to the ofﬁcial NIST evaluation (NIST, 2006; NIST, 2008), our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En wit"
N12-1046,Y11-1003,0,0.0111899,"ments. When we used NIST’s ofﬁcial metric (BLEU-4) to compare our results to the ofﬁcial NIST evaluation (NIST, 2006; NIST, 2008), our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En wit"
N12-1046,P10-2033,0,\N,Missing
N12-1046,I05-3027,0,\N,Missing
N13-3008,W12-3012,1,0.890313,"Missing"
N15-1061,D10-1045,1,0.928361,"however, introduces the new challenge of properly leveraging the additional matching potential of verbose multiterm queries (White et al., 2013). To this end, our work builds on two components: a term matching system, and a test collection. As a term matching system, we used our zero-knowledge speech matching system. In MediaEval 2012, this system achieved an ATWV of 0.321 in the Spoken Web Search task (Jansen et al., 2012). A version of this system has previously been evaluated in an example-based topic classification task using English speech, achieving a classification accuracy of 0.8683 (Drezde et al., 2010). Ranked retrieval using naturally occurring queries is more challenging, however, both because topics in information retrieval are often not easily separable, and because the form of a query may be unlike the form of the responses that are sought. Our goal now, therefore, is to use an information retrieval evaluation framework to drive the development of robust techniques for accommodating representational uncertainty. Traditional spoken term detection (STD) tries to address uncertainty by learning speech-signal to language-model mappings; using neural networks (Cui et al., 2013; Gales et al."
N15-1061,D13-1126,1,0.818632,"te low precision, moderate levels of recall are possible. Speech search arguably demands higher precision than does Web search, however, since browsing multiple alternatives is easier in text than in speech. One way of potentially improving retrieval performance is to encourage a searcher to speak at length about what they are look2 For example, Gujarati, isiNdebele, isiXhosa, Sepedi, Setswana, Telugu, Tshivenda, and Xitsonga. 589 ing for (Oard, 2012). Such an approach, however, introduces the new challenge of properly leveraging the additional matching potential of verbose multiterm queries (White et al., 2013). To this end, our work builds on two components: a term matching system, and a test collection. As a term matching system, we used our zero-knowledge speech matching system. In MediaEval 2012, this system achieved an ATWV of 0.321 in the Spoken Web Search task (Jansen et al., 2012). A version of this system has previously been evaluated in an example-based topic classification task using English speech, achieving a classification accuracy of 0.8683 (Drezde et al., 2010). Ranked retrieval using naturally occurring queries is more challenging, however, both because topics in information retriev"
P08-1107,W03-0405,0,0.0256597,"tion(Malin, 2005; Bhattacharya and Getoor, 2007; Reuther, 2006) and using a probabilistic generative 941 Proceedings of ACL-08: HLT, pages 941–949, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics model (Bhattacharya and Getoor, 2006). None of these approaches, however, both make use of conversational, topical, and time aspects, shown important in resolving personal names (Reuther, 2006), and take into account global relational information. Similarly, approaches in unstructured data (e.g., text) have involved using clustering techniques over biographical facts (Mann and Yarowsky, 2003), within-document resolution (Blume, 2005), and discriminative unsupervised generative models (Li et al., 2005). These too are insufficient for our problem since they suffer from inability scale or to handle early negotiation. Specific to the problem of resolving mentions in email collections, Abadi (Abadi, 2003) used email orders from an online retailer to resolve product mentions in orders and Holzer et al. (Holzer et al., 2005) used the Web to acquire information about individuals mentioned in headers of an email collection. Our work is focused on resolving personal name references in the f"
W00-0504,P99-1027,0,0.0327709,"ed words may be helpful. We thus plan to exp]Iore the potential for integrated sequential model]ling of both words and syllables [Meng et al., 20013]. 4. Multiseale Embedded Translation Figures 1 and 2 illustrate two translingual retrieval strategies. In query translation, English text queries are transformed into Mandarin and then used to retrieve Mandarin documents. For document translation, Mandarin documents are translated into English before they are indexed and then matched with English queries. McCarley has reported improved effectiveness from techniques that couple the two techniques [McCarley, 1999], but time constraints may limit us to explonng only the query translation strategy dunng the six-week Workshop. 4,1 W o r d T r a n s l a t i o n While we make use of sub-word transcription to smooth out-of-vocabulary(OOV) problems in speech recognition as described above, and to alleviate the OOV problem :for translation as we discuss in the next section, accurate translation generally relies on the additional information available at the word and phrase levels. Since the &quot;bag of words&quot; information retrieval techniques do not incorporate any meaningful degree of language understanding to as"
W00-0504,O99-2001,1,0.804809,"t takes place frequently...) The above considerations lead to a number of techniques we plan to use for our task. We concentrate on three equally critical problems related to our theme of translingual speech retrieval: (i) indexing Mandarin Chinese audio with word and subword units, (ii) translating variable-size units for cross-language information retrieval, and (iii) devising effective retrieval strategies for English text queries and Mandarin Chinese news audio. 3. Multiscale Audio Indexing A popular approach to retrieval is to apply spoken document Large-Vocabulary s Examples drawn from [Meng and Ip, 1999]. Continuous Speech Recognition (LVCSR) 9 for audio indexing, followed by text retrieval techniques. Mandarin Chinese presents a challenge for word-level indexing by LVCSR, because of the ambiguity in tokenizing a sentence into words (as mentioned earlier). Furthermore, LVCSR with a static vocabulary is hampered by the out-of-vocabulary (OOV) problem, especially when searching sources with topical coverage as diverse as that found in broadcast news. By virtue of the monosyllabic nature of the Chinese language and its dialects, the syllable inventory can provide a complete phonological coverag"
W00-0504,P97-1017,0,0.139368,"Missing"
W00-0504,1998.amta-tutorials.5,0,\N,Missing
W00-0504,W98-1005,0,\N,Missing
W00-0504,J95-4004,0,\N,Missing
W00-0504,J98-4003,0,\N,Missing
W00-0504,X93-1008,0,\N,Missing
W00-0504,A97-1029,0,\N,Missing
W12-3012,D07-1074,0,0.0601069,"which entity it represents. State-of-the-art entity linking systems are quite good at linking person names (Ji et al., 2011). They rely on a variety of Machine Learning approaches and may incorporate different external resources such as name Gazetteers (Burman et al., 2011), precompiled estimates of entity popularities (Han and Sun, 2011) and modules trained to recognize name and acronym matches (Zhang et al., 2011). Two areas are handled less well by current entity linking systems. First, it has been recognized that collective inference over a set of entities can lead to better performance (Cucerzan, 2007; Kulkarni et al., 2009; Hoffart et al., 2011; Ratinov et al., 2011). While the field has begun to move in the direction of collective (or joint) inference, such inference is a computationally hard problem. As a result, current joint inference approaches rely on different heuristics to limit the search space. Thus, collective classification approaches are yet to gain wide acceptance. In fact, only four of the 35 systems that submitted runs to the 2011 TAC KBP task go beyond a single query in a single document. Ji et al. (2011) cite the need for (more) joint inference as one of the avenues for"
W12-3012,P11-1095,0,0.0178586,"are provided with a knowledge base derived from Wikipedia Infoboxes. Each query comprises a text document and a mention string found in that document. The entity linking system must determine whether the entity referred to by the men62 tion is represented in the KB, and if so, which entity it represents. State-of-the-art entity linking systems are quite good at linking person names (Ji et al., 2011). They rely on a variety of Machine Learning approaches and may incorporate different external resources such as name Gazetteers (Burman et al., 2011), precompiled estimates of entity popularities (Han and Sun, 2011) and modules trained to recognize name and acronym matches (Zhang et al., 2011). Two areas are handled less well by current entity linking systems. First, it has been recognized that collective inference over a set of entities can lead to better performance (Cucerzan, 2007; Kulkarni et al., 2009; Hoffart et al., 2011; Ratinov et al., 2011). While the field has begun to move in the direction of collective (or joint) inference, such inference is a computationally hard problem. As a result, current joint inference approaches rely on different heuristics to limit the search space. Thus, collective"
W12-3012,D11-1072,0,0.0725985,"Missing"
W12-3012,I11-1029,1,0.844097,"liminary: we currently use simple string-match based resolvers and incorporate only a subset of the contexts that we intend to implement. On newswire, we rely on a parameter that sets the maximum number of entities returned by the trigger. When we set the parameter to 500, the context recall on non-NIL is 0.735 and the average number of entities per document returned is 452 (some documents return less than the maximum number, 500). When we set the parameter to 5,000, the context recall on non-NIL is 0.829 and the average number of entities is 4,515. We contrast this to the triage mechanism of McNamee et al. (2011), which relies on name and alias matching to obtain all potential entity matches. This mechanism achieves recall of 0.905 on non-NIL with average context size of 52. The set of entities returned by the triage mechanism are much most ambiguity as all of the entities in the set share the same name or alias (or character n-grams found in the mention). The overall accuracy of the system in the two settings that rely on our document trigger is around 0.6 in both settings (including NILs), while the accuracy of the system using McNamee et al.’s (2011) triage is around 0.3 (including NILs). As discus"
W12-3012,P11-1138,0,0.108175,"systems are quite good at linking person names (Ji et al., 2011). They rely on a variety of Machine Learning approaches and may incorporate different external resources such as name Gazetteers (Burman et al., 2011), precompiled estimates of entity popularities (Han and Sun, 2011) and modules trained to recognize name and acronym matches (Zhang et al., 2011). Two areas are handled less well by current entity linking systems. First, it has been recognized that collective inference over a set of entities can lead to better performance (Cucerzan, 2007; Kulkarni et al., 2009; Hoffart et al., 2011; Ratinov et al., 2011). While the field has begun to move in the direction of collective (or joint) inference, such inference is a computationally hard problem. As a result, current joint inference approaches rely on different heuristics to limit the search space. Thus, collective classification approaches are yet to gain wide acceptance. In fact, only four of the 35 systems that submitted runs to the 2011 TAC KBP task go beyond a single query in a single document. Ji et al. (2011) cite the need for (more) joint inference as one of the avenues for improvement. The second area not handled well is the notion of disco"
W16-1305,P08-1107,1,0.828547,"Missing"
