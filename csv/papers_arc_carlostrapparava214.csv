2020.lrec-1.178,{D}ec{O}p: A Multilingual and Multi-domain Corpus For Detecting Deception In Typed Text,2020,-1,-1,3,0,16978,pasquale capuozzo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In recent years, the increasing interest in the development of automatic approaches for unmasking deception in online sources led to promising results. Nonetheless, among the others, two major issues remain still unsolved: the stability of classifiers performances across different domains and languages. Tackling these issues is challenging since labelled corpora involving multiple domains and compiled in more than one language are few in the scientific literature. For filling this gap, in this paper we introduce DecOp (Deceptive Opinions), a new language resource developed for automatic deception detection in cross-domain and cross-language scenarios. DecOp is composed of 5000 examples of both truthful and deceitful first-person opinions balanced both across five different domains and two languages and, to the best of our knowledge, is the largest corpus allowing cross-domain and cross-language comparisons in deceit detection tasks. In this paper, we describe the collection procedure of the DecOp corpus and his main characteristics. Moreover, the human performance on the DecOp test-set and preliminary experiments by means of machine learning models based on Transformer architecture are shown."
2020.lrec-1.186,{E}mo{E}vent: A Multilingual Emotion Corpus based on different Events,2020,-1,-1,2,0,15301,flor arco,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In recent years emotion detection in text has become more popular due to its potential applications in fields such as psychology, marketing, political science, and artificial intelligence, among others. While opinion mining is a well-established task with many standard data sets and well-defined methodologies, emotion mining has received less attention due to its complexity. In particular, the annotated gold standard resources available are not enough. In order to address this shortage, we present a multilingual emotion data set based on different events that took place in April 2019. We collected tweets from the Twitter platform. Then one of seven emotions, six Ekman{'}s basic emotions plus the {``}neutral or other emotions{''}, was labeled on each tweet by 3 Amazon MTurkers. A total of 8,409 in Spanish and 7,303 in English were labeled. In addition, each tweet was also labeled as offensive or no offensive. We report some linguistic statistics about the data set in order to observe the difference between English and Spanish speakers when they express emotions related to the same events. Moreover, in order to validate the effectiveness of the data set, we also propose a machine learning approach for automatically detecting emotions in tweets for both languages, English and Spanish."
2020.lrec-1.742,{VROAV}: Using Iconicity to Visually Represent Abstract Verbs,2020,-1,-1,2,0,18096,simone scicluna,Proceedings of the 12th Language Resources and Evaluation Conference,0,"For a long time, philosophers, linguists and scientists have been keen on finding an answer to the mind-bending question {``}what does abstract language look like?{''}, which has also sprung from the phenomenon of mental imagery and how this emerges in the mind. One way of approaching the matter of word representations is by exploring the common semantic elements that link words to each other. Visual languages like sign languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of identifying clusters of iconic meanings. With this insight, merged with an understanding of verb predicates achieved from VerbNet, this study presents a novel verb classification system based on visual shapes, using graphic animation to visually represent 20 classes of abstract verbs. Considerable agreement between participants who judged the graphic animations based on representativeness suggests a positive way forward for this proposal, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text."
W19-4429,Anglicized Words and Misspelled Cognates in Native Language Identification,2019,-1,-1,3,1,442,ilia markov,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories: misspelled cognates, {``}L2-ed{''} (in our case, anglicized) words, and all other spelling errors. We test the assumption that such errors contain clues about the native language of an essay{'}s author through the task of native language identification. The results of the experiments show that the information brought by each of these categories is complementary. We also note that while the distribution of such features changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels."
W19-3411,Personality Traits Recognition in Literary Texts,2019,0,0,2,0,24462,daniele pizzolli,Proceedings of the Second Workshop on Storytelling,0,"Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current personality traits theories to literal characters, focusing on the analysis of utterances in theatre scripts. And, at the opposite, we try to find significant traits for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using machine learning for gaining insight into the personality traits of fictional characters can make sense."
W18-6218,The Role of Emotions in Native Language Identification,2018,0,0,3,1,442,ilia markov,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"We explore the hypothesis that emotion is one of the dimensions of language that surfaces from the native language into a second language. To check the role of emotions in native language identification (NLI), we model emotion information through polarity and emotion load features, and use document representations using these features to classify the native language of the author. The results indicate that emotion is relevant for NLI, even for high proficiency levels and across topics."
J18-1007,"{M}etaphor: A Computational Perspective by Tony Veale, Ekaterina {S}hutova and Beata Beigman Klebanov",2018,-1,-1,1,1,16980,carlo strapparava,Computational Linguistics,0,None
D18-1367,A Computational Exploration of Exaggeration,2018,0,1,2,0,410,enrica troiano,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Several NLP studies address the problem of figurative language, but among non-literal phenomena, they have neglected exaggeration. This paper presents a first computational approach to this figure of speech. We explore the possibility to automatically detect exaggerated sentences. First, we introduce HYPO, a corpus containing overstatements (or hyperboles) collected on the web and validated via crowdsourcing. Then, we evaluate a number of models trained on HYPO, and bring evidence that the task of hyperbole identification can be successfully performed based on a small set of semantic features."
C18-1293,Punctuation as Native Language Interference,2018,0,1,3,1,442,ilia markov,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this paper, we describe experiments designed to explore and evaluate the impact of punctuation marks on the task of native language identification. Punctuation is specific to each language, and is part of the indicators that overtly represent the manner in which each language organizes and conveys information. Our experiments are organized in various set-ups: the usual multi-class classification for individual languages, also considering classification by language groups, across different proficiency levels, topics and even cross-corpus. The results support our hypothesis that punctuation marks are persistent and robust indicators of the native language of the author, which do not diminish in influence even when a high proficiency level in a non-native language is achieved."
W17-5042,{CIC}-{FBK} Approach to Native Language Identification,2017,0,6,3,1,442,ilia markov,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approach combines features commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and features that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags. We use log-entropy weighting scheme and perform classification using the Support Vector Machines (SVM) algorithm. Our system achieved 0.8808 macro-averaged F1-score and shared the 1st rank in the NLI Shared Task 2017 scoring."
P17-2086,Improving Native Language Identification by Using Spelling Errors,2017,0,1,2,0,30849,lingzhen chen,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we explore spelling errors as a source of information for detecting the native language of a writer, a previously under-explored area. We note that character n-grams from misspelled words are very indicative of the native language of the author. In combination with other lexical features, spelling error features lead to 1.2{\%} improvement in accuracy on classifying texts in the TOEFL11 corpus by the author{'}s native language, compared to systems participating in the NLI shared task."
E17-2022,A Computational Analysis of the Language of Drug Addiction,2017,34,3,1,1,16980,carlo strapparava,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We present a computational analysis of the language of drug users when talking about their drug experiences. We introduce a new dataset of over 4,000 descriptions of experiences reported by users of four main drug types, and show that we can predict with an F1-score of up to 88{\%} the drug behind a certain experience. We also perform an analysis of the dominant psycholinguistic processes and dominant emotions associated with each drug type, which sheds light on the characteristics of drug users."
E17-2048,To Sing like a Mockingbird,2017,0,2,4,0,27674,lorenzo gatti,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a system for automatically producing a musical parody, starting from a corpus of songs. The system can automatically identify characterizing words and concepts related to a novel text, which are taken from the daily news. These concepts are then used as seeds to appropriately replace part of the original lyrics of a song, using metrical, rhyming and lexical constraints. Finally, the parody can be sung with a singing speech synthesizer, with no intervention from the user."
D17-1286,Word Etymology as Native Language Interference,2017,4,1,2,0,24179,vivi nastase,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present experiments that show the influence of native language on lexical choice when producing text in another language {--} in this particular case English. We start from the premise that non-native English speakers will choose lexical items that are close to words in their native language. This leads us to an etymology-based representation of documents written by people whose mother tongue is an Indo-European language. Based on this representation we grow a language family tree, that matches closely the Indo-European language tree."
W16-4310,Innovative Semi-Automatic Methodology to Annotate Emotional Corpora,2016,20,0,2,0,32433,lea canales,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",0,"Detecting depression or personality traits, tutoring and student behaviour systems, or identifying cases of cyber-bulling are a few of the wide range of the applications, in which the automatic detection of emotion is a crucial element. Emotion detection has the potential of high impact by contributing the benefit of business, society, politics or education. Given this context, the main objective of our research is to contribute to the resolution of one of the most important challenges in textual emotion detection task: the problems of emotional corpora annotation. This will be tackled by proposing of a new semi-automatic methodology. Our innovative methodology consists in two main phases: (1) an automatic process to pre-annotate the unlabelled sentences with a reduced number of emotional categories; and (2) a refinement manual process where human annotators will determine which is the predominant emotion between the emotional categories selected in the phase 1. Our proposal in this paper is to show and evaluate the pre-annotation process to analyse the feasibility and the benefits by the methodology proposed. The results obtained are promising and allow obtaining a substantial improvement of annotation time and cost and confirm the usefulness of our pre-annotation process to improve the annotation task."
W16-0430,Emotions and {NLP}: Future Directions,2016,1,3,1,1,16980,carlo strapparava,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,None
L16-1600,{PROMETHEUS}: A Corpus of Proverbs Annotated with Metaphors,2016,0,0,2,1,30615,gozde ozbal,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Proverbs are commonly metaphoric in nature and the mapping across domains is commonly established in proverbs. The abundance of proverbs in terms of metaphors makes them an extremely valuable linguistic resource since they can be utilized as a gold standard for various metaphor related linguistic tasks such as metaphor identification or interpretation. Besides, a collection of proverbs fromvarious languages annotated with metaphors would also be essential for social scientists to explore the cultural differences betweenthose languages. In this paper, we introduce PROMETHEUS, a dataset consisting of English proverbs and their equivalents in Italian.In addition to the word-level metaphor annotations for each proverb, PROMETHEUS contains other types of information such as the metaphoricity degree of the overall proverb, its meaning, the century that it was first recorded in and a pair of subjective questions responded by the annotators. To the best of our knowledge, this is the first multi-lingual and open-domain corpus of proverbs annotated with word-level metaphors."
D16-1220,Learning to Identify Metaphors from a Corpus of Proverbs,2016,7,1,2,1,30615,gozde ozbal,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-1404,Exploring Sensorial Features for Metaphor Identification,2015,22,6,3,1,7694,serra tekirouglu,Proceedings of the Third Workshop on Metaphor in {NLP},0,"Language is the main communication device to represent the environment and share a common understanding of the world that we perceive through our sensory organs. Therefore, each language might contain a great amount of sensorial elements to express the perceptions both in literal and figurative usage. To tackle the semantics of figurative language, several conceptual properties such as concreteness or imegeability are utilized. However, there is no attempt in the literature to analyze and benefit from the sensorial elements for figurative language processing. In this paper, we investigate the impact of sensorial features on metaphor identification. We utilize an existing lexicon associating English words to sensorial modalities and propose a novel technique to automatically discover these associations from a dependency-parsed corpus. In our experiments, we measure the contribution of the sensorial features to the metaphor identification task with respect to a state of the art model. The results demonstrate that sensorial features yield better performance and show good generalization properties."
S15-2077,{S}em{E}val-2015 Task 9: {CLIPE}val Implicit Polarity of Events,2015,16,11,3,0,1876,irene russo,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Sentiment analysis tends to focus on the polarity of words, combining their values to detect which portion of a text is opinionated. CLIPEval wants to promote a more holistic approach, looking at psychological researches that frame the connotations of words as the emotional values activated by them. The implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge."
S15-2147,"{S}em{E}val 2015, Task 7: Diachronic Text Evaluation",2015,5,17,2,0.19025,21558,octavian popescu,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper we describe a novel task, namely the Diachronic Text Evaluation task. A corpus of snippets which contain relevant information for the time when the text was created is extracted from a large collection of newspapers published between 1700 and 2010. The task, subdivided in three subtasks, requires the automatic system to identify the time interval when the piece of news was written. The subtasks concern specific type of information that might be available in news. The intervals come in three grades: fine, medium and coarse according to their length. The systems participating in the tasks have proved that this a doable task with very interesting possible continuations."
N15-1172,Echoes of Persuasion: The Effect of Euphony in Persuasive Communication,2015,29,5,3,0.961538,7695,marco guerini,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and non-persuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion."
W14-4716,A Computational Approach to Generate a Sensorial Lexicon,2014,32,2,3,1,7694,serra tekirouglu,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"While humans are capable of building connections between words and sensorial modalities by using commonsense knowledge, it is not straightforward for machines to interpret sensorial information. To this end, a lexicon associating words with human senses, namely sight, hearing, taste, smell and touch, would be crucial. Nonetheless, to the best of our knowledge, there is no systematic attempt in the literature to build such a resource. In this paper, we propose a computational method based on bootstrapping and corpus statistics to automatically associate English words with senses. To evaluate the quality of the resulting lexicon, we create a gold standard via crowdsourcing and show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task, both at word and sentence level. The results confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications."
W14-0140,Aligning an {I}talian {W}ord{N}et with a Lexicographic Dictionary: Coping with limited data,2014,18,2,2,0.766738,6,tommaso caselli,Proceedings of the Seventh Global {W}ordnet Conference,0,"This work describes the evaluations of two approaches, Lexical Matching and Sense Similarity, for word sense alignment between MultiWordNet and a lexicographic dictionary, Senso Comune De Mauro, when having few sense descriptions (MultiWordNet) and no structure over senses (Senso Comune De Mauro). The results obtained from the merging of the two approaches are satisfying, with F1 values of 0.47 for verbs and 0.64 for nouns."
P14-2058,Automation and Evaluation of the Keyword Method for Second Language Learning,2014,16,5,3,1,30615,gozde ozbal,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning."
gella-etal-2014-mapping,"Mapping {W}ord{N}et Domains, {W}ord{N}et Topics and {W}ikipedia Categories to Generate Multilingual Domain Specific Resources",2014,17,5,2,0,9729,spandana gella,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present the mapping between WordNet domains and WordNet topics, and the emergent Wikipedia categories. This mapping leads to a coarse alignment between WordNet and Wikipedia, useful for producing domain-specific and multilingual corpora. Multilinguality is achieved through the cross-language links between Wikipedia categories. Research in word-sense disambiguation has shown that within a specific domain, relevant words have restricted senses. The multilingual, and comparable, domain-specific corpora we produce have the potential to enhance research in word-sense disambiguation and terminology extraction in different languages, which could enhance the performance of various NLP tasks."
strapparava-etal-2014-creative,Creative language explorations through a high-expressivity {N}-grams query language,2014,14,0,1,1,16980,carlo strapparava,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In computation linguistics a combination of syntagmatic and paradigmatic features is often exploited. While the first aspects are typically managed by information present in large n-gram databases, domain and ontological aspects are more properly modeled by lexical ontologies such as WordNet and semantic similarity spaces. This interconnection is even stricter when we are dealing with creative language phenomena, such as metaphors, prototypical properties, puns generation, hyperbolae and other rhetorical phenomena. This paper describes a way to focus on and accomplish some of these tasks by exploiting NgramQuery, a generalized query language on Google N-gram database. The expressiveness of this query language is boosted by plugging semantic similarity acquired both from corpora (e.g. LSA) and from WordNet, also integrating operators for phonetics and sentiment analysis. The paper reports a number of examples of usage in some creative language tasks."
caselli-etal-2014-enriching,Enriching the {``}Senso Comune{''} Platform with Automatically Acquired Data,2014,26,0,3,0.766738,6,tommaso caselli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper reports on research activities on automatic methods for the enrichment of the Senso Comune platform. At this stage of development, we will report on two tasks, namely word sense alignment with MultiWordNet and automatic acquisition of Verb Shallow Frames from sense annotated data in the MultiSemCor corpus. The results obtained are satisfying. We achieved a final F-measure of 0.64 for noun sense alignment and a F-measure of 0.47 for verb sense alignment, and an accuracy of 68{\textbackslash}{\%} on the acquisition of Verb Shallow Frames."
D14-1046,Automatic Domain Assignment for Word Sense Alignment,2014,17,0,2,0.766738,6,tommaso caselli,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This paper reports on the development of a hybrid and simple method based on a machine learning classifier (Naive Bayes), Word Sense Disambiguation and rules, for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary, the Senso Comune De Mauro Lexicon. The system obtained an F1 score of 0.58, with a Precision of 0.70. We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune. This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments."
D14-1160,{S}ensicon: An Automatically Constructed Sensorial Lexicon,2014,35,11,3,1,7694,serra tekirouglu,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Connecting words with senses, namely, sight, hearing, taste, smell and touch, to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge. With this in mind, a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language. However, to the best of our knowledge, there is no systematic attempt in the literature to build such a resource. In this paper, we present a sensorial lexicon that associates English words with senses. To obtain this resource, we apply a computational method based on bootstrapping and corpus statistics. The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing. The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task, both at word and sentence level, and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications."
W13-5502,Linguistic Linked Data for Sentiment Analysis,2013,11,10,5,0,6276,paul buitelaar,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,"In this paper we describe the specification of amodel for the semantically interoperable representation of language resources for sentiment analysis. The model integrates lemon, an RDF-based model for the specification of ontology-lexica (Buitelaar et al. 2009), which is used increasinglyfor the representation of language resources asLinked Data, with Marl, an RDF-based model for the representation of sentiment annotations (West-erski et al., 2011; Sanchez-Rada et al., 2013)"
W13-3816,Aligning Verb Senses in Two {I}talian Lexical Semantic Resources,2013,19,1,2,0.766738,6,tommaso caselli,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"This work describes the evaluations of three different approaches, Lexical Match, Sense Similarity based on Personalized Page Rank, and Semantic Match based on Shallow Frame Structures, for word sense alignment of verbs between two Italian lexical-semantic resources, MultiWordNet and the Senso Comune Lexicon. The results obtained are quite satisfying with a final F1 score of 0.47 when merging together Lexical Match and Sense Similarity."
P13-1064,Bridging Languages through Etymology: The case of cross language text categorization,2013,14,4,2,0,24179,vivi nastase,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement xe2x80x90 a jump of almost 40 points in F1-score xe2x80x90 over the raw (vanilla bag-ofwords) representation."
P13-1142,{BRAINSUP}: Brainstorming Support for Creative Sentence Generation,2013,19,20,3,1,30615,gozde ozbal,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present BRAINSUP, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain relatedness and phonetic properties. We evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters."
I13-1040,Behind the Times: Detecting Epoch Changes using Large Corpora,2013,11,19,2,0.19025,21558,octavian popescu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Using large corpora of chronologically ordered language, it is possible to explore diachronic phenomena, identifying previously unknown correlations between language usage and time periods, or epochs. We focused on a statistical approach to epoch delimitation and introduced the task of epoch characterization. We investigated the significant changes in the distribution of terms in the Google N-gram corpus and their relationships with emotion words. The results show that the method is reliable and the task is feasible."
P12-1074,A Computational Approach to the Automation of Creative Naming,2012,14,24,2,1,30615,gozde ozbal,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation of the naming process."
P12-1104,Ecological Evaluation of Persuasive Messages Using {G}oogle {A}d{W}ords,2012,13,8,2,1,7695,marco guerini,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach."
ozbal-etal-2012-brand,Brand Pitt: A Corpus to Explore the Art of Naming,2012,11,18,2,1,30615,gozde ozbal,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The name of a company or a brand is the key element to a successful business. A good name is able to state the area of competition and communicate the promise given to customers by evoking semantic associations. Although various resources provide distinct tips for inventing creative names, little research was carried out to investigate the linguistic aspects behind the naming mechanism. Besides, there might be latent methods that copywriters unconsciously use. In this paper, we describe the annotation task that we have conducted on a dataset of creative names collected from various resources to create a gold standard for linguistic creativity in naming. Based on the annotations, we compile common and latent methods of naming and explore the correlations among linguistic devices, provoked effects and business domains. This resource represents a starting point for a corpus based approach to explore the art of naming."
strapparava-etal-2012-parallel,A Parallel Corpus of Music and Lyrics Annotated with Emotions,2012,11,3,1,1,16980,carlo strapparava,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we introduce a novel parallel corpus of music and lyrics, annotated with emotions at line level. We first describe the corpus, consisting of 100 popular songs, each of them including a music component, provided in the MIDI format, as well as a lyrics component, made available as raw text. We then describe our work on enhancing this corpus with emotion annotations using crowdsourcing. We also present some initial experiments on emotion classification using the music and the lyrics representations of the songs, which lead to encouraging results, thus demonstrating the promise of using joint music-lyric models for song processing."
aleksandrov-strapparava-2012-ngramquery,{N}gram{Q}uery - Smart Information Extraction from {G}oogle N-gram using External Resources,2012,8,3,2,0,43170,martin aleksandrov,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the implementation of a generalized query language on Google Ngram database. This language allows for very expressive queries that exploit semantic similarity acquired both from corpora (e.g. LSA) and from WordNet, and phonetic similarity available from the CMU Pronouncing Dictionary. It contains a large number of new operators, which combined in a proper query can help users to extract n-grams having similarly close syntactic and semantic relational properties. We also characterize the operators with respect to their corpus affiliation and their functionality. The query syntax is considered next given in terms of Backus-Naur rules followed by a few interesting examples of how the tool can be used. We also describe the command-line arguments the user could input comparing them with the ones for retrieving n-grams through the interface of Google Ngram database. Finally we discuss possible improvements on the extraction process and some relevant query completeness issues."
D12-1054,"Lyrics, Music, and Emotions",2012,35,29,2,0,1124,rada mihalcea,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%."
C12-2117,Corpus-based Explorations of Affective Load Differences in {A}rabic-{H}ebrew-{E}nglish,2012,13,1,1,1,16980,carlo strapparava,Proceedings of {COLING} 2012: Posters,0,"This work is about connotative aspects of words, often not carried over in translation, which depend on specific cultures. A cross-language computational study is presented, based on exploitation of similarity techniques on large corpora of news documents in English, Arabic, and Hebrew. In particular, focus of the exploration is on specific terms expressing emotion, negotiation and conflict."
W10-3405,The Color of Emotions in Texts,2010,17,16,1,1,16980,carlo strapparava,Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon,0,"Color affects every aspect of our lives. There has been a considerable interest in the psycholinguistic research area addressing the impact of color on emotions. In the experiments conducted by these studies, subjects have usually been asked to indicate their emotional responses to different colors. On the other side, sensing emotions in text by using NLP techniques has recently become a popular topic in computational linguistics. In this paper, we introduce a semantic similarity mechanism acquired from a large corpus of texts in order to check the similarity of colors and emotions. Then we investigate the correlation of our results with the outcomes of some psycholinguistic experiments. The conclusions are quite interesting. The correlation varies among different colors and globally we achieve very good results."
guerini-etal-2010-evaluation,Evaluation Metrics for Persuasive {NLP} with {G}oogle {A}d{W}ords,2010,5,14,2,1,7695,marco guerini,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Evaluating systems and theories about persuasion represents a bottleneck for both theoretical and applied fields: experiments are usually expensive and time consuming. Still, measuring the persuasive impact of a message is of paramount importance. In this paper we present a new ``cheap and fast'' methodology for measuring the persuasiveness of communication. This methodology allows conducting experiments with thousands of subjects for a few dollars in a few hours, by tweaking and using existing commercial tools for advertising on the web, such as Google AdWords. The central idea is to use AdWords features for defining message persuasiveness metrics. Along with a description of our approach we provide some pilot experiments, conducted both with text and image based ads, that confirm the effectiveness of our ideas. We also discuss the possible application of research on persuasive systems to Google AdWords in order to add more flexibility in the wearing out of persuasive messages."
novielli-strapparava-2010-studying,Studying the Lexicon of Dialogue Acts,2010,26,2,2,1,37256,nicole novielli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Dialogue Acts have been well studied in linguistics and attracted computational linguistics research for a long time: they constitute the basis of everyday conversations and can be identified with the communicative goal of a given utterance (e.g. asking for information, stating facts, expressing opinions, agreeing or disagreeing). Even if not constituting any deep understanding of the dialogue, automatic dialogue act labeling is a task that can be relevant for a wide range of applications in both human-computer and human-human interaction. We present a qualitative analysis of the lexicon of Dialogue Acts: we explore the relationship between the communicative goal of an utterance and its affective content as well as the salience of specific word classes for each speech act. The experiments described in this paper fit in the scope of a research study whose long-term goal is to build an unsupervised classifier that simply exploits the lexical semantics of utterances for automatically annotate dialogues with the proper speech acts."
strapparava-etal-2010-predicting,Predicting Persuasiveness in Political Discourses,2010,11,9,1,1,16980,carlo strapparava,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In political speeches, the audience tends to react or resonate to signals of persuasive communication, including an expected theme, a name or an expression. Automatically predicting the impact of such discourses is a challenging task. In fact nowadays, with the huge amount of textual material that flows on the Web (news, discourses, blogs, etc.), it can be useful to have a measure for testing the persuasiveness of what we retrieve or possibly of what we want to publish on Web. In this paper we exploit a corpus of political discourses collected from various Web sources, tagged with audience reactions, such as applause, as indicators of persuasive expressions. In particular, we use this data set in a machine learning framework to explore the possibility of classifying the transcript of political discourses, according to their persuasive power, predicting the sentences that possibly trigger applause. We also explore differences between Democratic and Republican speeches, experiment the resulting classifiers in grading some of the discourses in the Obama-McCain presidential campaign available on the Web."
P09-2078,The Lie Detector: Explorations in the Automatic Recognition of Deceptive Language,2009,7,168,2,0,1124,rada mihalcea,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"In this paper, we present initial experiments in the recognition of deceptive language. We introduce three data sets of true and lying texts collected for this purpose, and we show that automatic classification is a viable technique to distinguish between truth and falsehood as expressed in language. We also introduce a method for class-based feature analysis, which sheds some light on the features that are characteristic for deceptive text."
N09-3015,Towards Unsupervised Recognition of Dialogue Acts,2009,20,3,2,1,37256,nicole novielli,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium",0,"When engaged in dialogues, people perform communicative actions to pursue specific communicative goals. Speech acts recognition attracted computational linguistics since long time and could impact considerably a huge variety of application domains. We study the task of automatic labeling dialogues with the proper dialogue acts, relying on empirical methods and simply exploiting lexical semantics of the utterances. In particular, we present some experiments in supervised and unsupervised framework on both an English and an Italian corpus of dialogue transcriptions. The evaluation displays encouraging results in both languages, especially in the unsupervised version of the methodology."
J09-4007,Kernel Methods for Minimally Supervised {WSD},2009,29,27,3,0.583333,38355,claudio giuliano,Computational Linguistics,0,"We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner. In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of unlabeled data. The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods."
guerini-etal-2008-resources,Resources for Persuasion,2008,13,3,2,1,7695,marco guerini,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents resources and strategies for persuasive natural language processing. After the introduction of a specifically tagged corpus, some techniques for affective language processing and for persuasive lexicon extraction are provided together with prospective scenarios of application."
guerini-etal-2008-valentino,{V}alentino: A Tool for Valence Shifting of Natural Language Texts,2008,7,29,2,1,7695,marco guerini,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper a first implementation of a tool for valence shifting of natural language texts, named Valentino (VALENced Text INOculator), is presented. Valentino can modify existing textual expressions towards more positively or negatively valenced versions. To this end we built specific resources gathering various valenced terms that are semantically or contextually connected, and implemented strategies that uses these resources for substituting input terms."
S07-1013,{S}em{E}val-2007 Task 14: Affective Text,2007,8,402,1,1,16980,carlo strapparava,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The Affective Text task focuses on the classification of emotions and valence (positive/negative polarity) in news headlines, and is meant as an exploration of the connection between emotions and lexical semantics. In this paper, we describe the data set used in the evaluation and the results obtained by the participating systems."
S07-1029,{FBK}-irst: Lexical Substitution Task Exploiting Domain and Syntagmatic Coherence,2007,7,30,3,0.641026,38355,claudio giuliano,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper summarizes FBK-irst participation at the lexical substitution task of the Semeval competition. We submitted two different systems, both exploiting synonym lists extracted from dictionaries. For each word to be substituted, the systems rank the associated synonym list according to a similarity metric based on Latent Semantic Analysis and to the occurrences in the Web 1T 5-gram corpus, respectively. In particular, the latter system achieves the state-of-the-art performance, largely surpassing the baseline proposed by the organizers."
W06-2608,Syntagmatic Kernels: a Word Sense Disambiguation Case Study,2006,10,5,3,0.714286,38355,claudio giuliano,Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,0,None
P06-1057,Direct Word Sense Matching for Lexical Substitution,2006,20,37,5,0,955,ido dagan,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper investigates conceptually and empirically the novel sense matching task, which requires to recognize whether the senses of two synonymous words match in context. We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research."
P06-1070,Exploiting Comparable Corpora and Bilingual Dictionaries for Cross-Language Text Categorization,2006,14,43,2,1,3532,alfio gliozzo,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language (e.g. English) while the system is trained using labeled documents in a source language (e.g. Italian).In this work we present many solutions according to the availability of bilingual resources, and we show that it is possible to deal with the problem even when no such resources are accessible. The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora.Experiments show the effectiveness of our approach, providing a low cost solution for the Cross Language Text Categorization task. In particular, when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization."
strapparava-etal-2006-affective,The Affective Weight of Lexicon,2006,9,65,1,1,16980,carlo strapparava,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents resources and functionalities for the recognition and selection of affective evaluative terms. An affective hierarchy as an extension of the WordNet-Affect lexical database was developed in the first place. The second phase was the development of a semantic similarity function, acquired automatically in an unsupervised way from a large corpus of texts, which allows us to put into relation concepts and emotional categories. The integration of the two components is a key element for several applications."
W05-0802,Cross Language Text Categorization by Acquiring Multilingual Domain Models from Comparable Corpora,2005,15,40,2,1,3532,alfio gliozzo,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"In a multilingual scenario, the classical monolingual text categorization problem can be reformulated as a cross language TC task, in which we have to cope with two or more languages (e.g. English and Italian). In this setting, the system is trained using labeled examples in a source language (e.g. English), and it classifies documents in a different target language (e.g. Italian).n n In this paper we propose a novel approach to solve the cross language text categorization problem based on acquiring Multilingual Domain Models from comparable corpora in a totally unsupervised way and without using any external knowledge source (e.g. bilingual dictionaries). These Multilingual Domain Models are exploited to define a generalized similarity function (i.e. a kernel function) among documents in different languages, which is used inside a Support Vector Machines classification framework. The results show that our approach is a feasible and cheap solution that largely outperforms a baseline."
W05-0608,Domain Kernels for Text Categorization,2005,19,41,2,1,3532,alfio gliozzo,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"In this paper we propose and evaluate a technique to perform semi-supervised learning for Text Categorization. In particular we defined a kernel function, namely the Domain Kernel, that allowed us to plug external knowledge into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models.n n We evaluated the Domain Kernel in two standard benchmarks for Text Categorization with good results, and we compared its performance with a kernel function that exploits a standard bag-of-words feature representation. The learning curves show that the Domain Kernel allows us to reduce drastically the amount of training data required for learning."
P05-3029,{HAHA}cronym: A Computational Humor System,2005,9,23,2,0,32986,oliviero stock,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"Computational humor will be needed in interfaces, no less than other cognitive capabilities. There are many practical settings where computational humor will add value. Among them there are: business world applications (such as advertisement, e-commerce, etc.), general computer-mediated communication and human-computer interaction, increase in the friendliness of natural language interfaces, educational and edutainment systems. In particular in the educational field it is an important resource for getting selective attention, help in memorizing names and situations etc. And we all know how well it works with children.Automated humor production in general is a very difficult task but we wanted to prove that some results can be achieved even in short time. We have worked at a concrete limited problem, as the core of the European Project HAHAcronym. The main goal of HAHAcronym has been the realization of an acronym ironic re-analyzer and generator as a proof of concept in a focalized but non restricted context. To implement this system some general tools have been adapted, or developed for the humorous context. Systems output has been submitted to evaluation by human subjects, with a very positive result."
P05-1050,Domain Kernels for Word Sense Disambiguation,2005,13,73,3,1,3532,alfio gliozzo,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions. In particular a combination of kernel functions is adopted to estimate independently both syntagmatic and domain similarity. We defined a kernel function, namely the Domain Kernel, that allowed us to plug external knowledge into the supervised learning process. External knowledge is acquired from unlabeled data in a totally unsupervised way, and it is represented by means of Domain Models. We evaluated our methodology on several lexical sample tasks in different languages, outperforming significantly the state-of-the-art for each of them, while reducing the amount of labeled training data required for learning."
H05-1017,Investigating Unsupervised Learning for Text Categorization Bootstrapping,2005,13,20,2,1,3532,alfio gliozzo,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-the-art performance using only the category names as initial seeds."
H05-1067,Making Computers Laugh: Investigations in Automatic Humor Recognition,2005,11,80,2,0,1124,rada mihalcea,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Humor is one of the most interesting and puzzling aspects of human behavior. Despite the attention it has received in fields such as philosophy, linguistics, and psychology, there have been only few attempts to create computational models for humor recognition or generation. In this paper, we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition. Through experiments performed on very large data sets, we show that automatic classification techniques can be effectively used to distinguish between humorous and non-humorous texts, with significant improvements observed over apriori known baselines."
W04-3249,Unsupervised Domain Relevance Estimation for Word Sense Disambiguation,2004,9,16,3,1,3532,alfio gliozzo,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents Domain Relevance Estimation (DRE), a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category. We use a pre-defined set of categories (we call them domains) which have been previously associated to WORDNET word senses. Given a certain domain, DRE distinguishes between relevant and non-relevant texts by means of a Gaussian Mixture model that describes the frequency distribution of domain words inside a large-scale corpus. Then, an Expectation Maximization algorithm computes the parameters that maximize the likelihood of the model on the empirical data. The correct identification of the domain of the text is a crucial point for Domain Driven Disambiguation, an unsupervised Word Sense Disambiguation (WSD) methodology that makes use of only domain information. Therefore, DRE has been exploited and evaluated in the context of a WSD task. Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution."
W04-0856,Pattern abstraction and term similarity for Word Sense Disambiguation: {IRST} at Senseval-3,2004,10,47,1,1,16980,carlo strapparava,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
W04-0861,The {``}Meaning{''} system on the {E}nglish all-words task,2004,0,4,6,0,49095,luis villarejo,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
strapparava-valitutti-2004-wordnet,{W}ord{N}et Affect: an Affective Extension of {W}ord{N}et,2004,10,995,1,1,16980,carlo strapparava,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present a linguistic resource for the lexical representation of affective knowledge. This resource (named WORDNETAFFECT) was developed starting from WORDNET, through a selection and tagging of a subset of synsets representing the affective"
S01-1027,Using Domain Information for Word Sense Disambiguation,2001,5,51,2,0,1501,bernardo magnini,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"The major goal in ITC-irst's participation at Senseval-2 was to test the role of domain information in word sense disambiguation. The underlying working hypothesis is that domain labels, such as Medicine, Architecture and Sport provide a natural way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. For each task in which we participated (i.e. English all words, English 'lexical sample' and Italian 'lexical sample') a different mix of knowledge based and statistical techniques were implemented."
W00-0804,Experiments in Word Domain Disambiguation for Parallel Texts,2000,11,44,2,0,1501,bernardo magnini,{ACL}-2000 Workshop on Word Senses and Multi-linguality,0,"This paper describes some preliminary results about Word Domain Disambiguation, a variant of Word Sense Disambiguation where words in a text are tagged with a domain label in place of a sense label. The English WORDNET and its aligned Italian version, MULTIWORDNET, both augmented with domain labels, are used as the main information repositories. A baseline algorithm for Word Domain Disambiguation is presented and then compared with a mutual help disambiguation strategy, which takes advantages of the shared senses of parallel bilingual texts."
W97-0805,Lexical Discrimination with the {I}talian Version of {W}ord{N}et,1997,6,10,3,0,55546,alessandro artale,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
A92-1003,An Approach to Multilevel Semantics for Applied Systems,1992,20,9,3,0,14633,alberto lavelli,Third Conference on Applied Natural Language Processing,0,"Multilevel semantics has been proposed as a powerful architecture for semantic analysis. We propose a methodology that, while maintaining the generality of the multilevel approach, is able to establish formal constraints over the possible ways to organize the level hierarchy. More precisely, we propose a strong version of the multilevel approach in which a level can be defined if and only if it is possible to characterize a meaningfulness notion peculiar to that level. Within such an architecture each level reached during the analysis computes its meaningfulness value; this result is then handled according to modalities that are peculiar to that level.The component described in this paper was designed to be portable with respect to the application domain and so far has been tested as the semantic analysis component of two multimedial dialog systems, ALFresco and MAIA."
