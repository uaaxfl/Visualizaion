2012.eamt-1.2,2010.jec-1.7,1,0.894254,"Missing"
2013.mtsummit-posters.9,W12-3102,0,0.256094,"ity. (b) size 8.5 J x 19 at front, size 11 J x 19 at rear, with 235/35 R 19 tires at front and 295/30 R 19 tires at rear Tailoring SMT systems to producing good pretranslations for our entire domain is thus challenging. Before describing our corresponding experiments in Section 4, we outline the technical setup and report on the performance of simple baseline systems in our use case. 3 3.1 Baseline Translation Systems Technical Setup The technical setup of all experiments closely resembles the setup of the baseline systems in the shared task of the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The only difference is that we used IRSTLM for language modeling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless o"
2013.mtsummit-posters.9,P11-2031,0,0.0301857,"(Callison-Burch et al., 2012). The only difference is that we used IRSTLM for language modeling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existin"
2013.mtsummit-posters.9,W11-2107,0,0.0304458,"luation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existing infrastructure. 3.2 Baseline Systems SemioticTransfer has been using Across for several years, through which they have accumulated a lot of quality-checked translations in their translation memories. We extracted all"
2013.mtsummit-posters.9,N09-1046,0,0.146612,"ng” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain"
2013.mtsummit-posters.9,P08-1115,0,0.0346988,"n method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain corpora to the training material. The specific nature of the indomain data makes it necessary to use domain adaptation techniques, to avoid having the bigger general-domain data override the original Metric Mode Avg ssel BLEU ↑ In-Domain Only Domai"
2013.mtsummit-posters.9,2011.mtsummit-papers.25,0,0.0163812,"sed on the translation system. The focus of the paper is the suitability of such texts for SMT; we present experiments in domain adaptation and decompounding that improve the baseline translation systems, the results of which are evaluated using automatic metrics as well as manual evaluation. 1 Introduction As machine translation and post-editing are gaining popularity on an industrial level, global companies turn to using their accumulated translations for setting up in-house SMT services. Substantial cost and time savings have been reported in various sectors, such as software localization (Flournoy, 2011; Zhechev, 2012) or film and television subtitling (Volk et al., 2010). In a joint project between the University of Zurich and SemioticTransfer AG, we target a new domain: automobile marketing texts. Martin Volk1 SemioticTransfer AG Bruggerstrasse 37 CH-5400 Baden manuela.weibel @semiotictransfer.ch We show that even limited amounts of indomain translation material allow for building domain-specific SMT systems (see Section 3), and that translation quality can be significantly improved by using out-of-domain material and language-specific preprocessing (see Section 4). Our aim is to give an e"
2013.mtsummit-posters.9,W07-0717,0,0.0332822,"s. OOV rates and automatic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on t"
2013.mtsummit-posters.9,W10-1710,0,0.0618482,"problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively"
2013.mtsummit-posters.9,P05-1058,0,0.0753085,"Missing"
2013.mtsummit-posters.9,2005.mtsummit-papers.11,0,0.0358338,"ssel 0.1 0.2 0.2 0.3 0.00 + 0.00 + 0.00 – 32.3 33.0 33.2 33.2 1.4 1.5 1.5 1.5 0.3 0.1 0.1 0.2 0.00 + 0.40 0.74 1.2 1.2 1.2 1.2 0.3 0.6 0.4 0.5 0.00 + 0.00 + 0.00 – 55.9 53.3 53.1 53.7 1.4 1.3 1.3 1.4 0.6 0.5 0.3 0.4 0.00 + 0.53 0.00 – 1.1 1.1 1.1 1.1 0.1 0.2 0.2 0.3 0.00 + 0.00 + 0.80 p-value Table 2: Automatic evaluation of the baseline and combined systems. p-values are relative to the preceding system and indicate whether a score improves (+) or decreases (–) significantly. domain-specific translations. We used two freely available out-of-domain corpora for these experiments: Europarl v7 (Koehn, 2005) and OpenSubtitles (Tiedemann, 2009). Stand-alone systems trained on these corpora result in unusable translations with very low scores (see Table 1). For domain adaptation, we used multidomain mixture-modeling (Foster and Kuhn, 2007) for language and translation models. The main distinctive feature of that approach is that instead of a binary in-domain/out-ofdomain treatment of the data sets, each separate domain is assigned a weight, reflecting its similarity to in-domain (parallel) texts. Mixture-modeling and optimization of these weights is implemented in IRSTLM for language models (Federi"
2013.mtsummit-posters.9,E03-1076,0,0.307702,"ds to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger g"
2013.mtsummit-posters.9,W07-0733,0,0.024757,".4 73.0 Table 1: Training data for in- and out-of-domain language and translation models. OOV rates and automatic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their"
2013.mtsummit-posters.9,P03-1021,0,0.0241122,"ling (Federico et al., 2008) because of licensing issues. We used a test set of 500 in-domain segments for automatic evaluation; these were randomly drawn from contracts that were processed after compiling the training and development sets (see section 3.2). Using a moderate test set size enabled detailed manual inspection and categorization of the machine translations, e.g., identifying the number of compounds in out-of-vocabulary (OOV) types. All automatic metric scores were calculated using multeval (Clark et al., 2011). Unless otherwise stated, the scores are averages over five MERT runs (Och, 2003; Bertoldi et al., 2009). METEOR scores are only given for DE–FR systems since Italian is not fully supported (Denkowski and Lavie, 2011). As SemioticTransfer’s translation workflow is based on the Across workbench1 , we implemented an RPC layer that allows for integrating Moses server instances into Across. In this way, we were able to apply various pre- and post-processing methods to translations while ensuring seamless integration of the Moses systems as a pre-translation service into SemioticTransfer’s existing infrastructure. 3.2 Baseline Systems SemioticTransfer has been using Across for"
2013.mtsummit-posters.9,E12-1055,0,0.0755085,"tic evaluation metrics refer to a test set of 500 in-domain segments (see Section 3.1). quality by reducing the rate of OOV input types. This was done by adding generaldomain corpora and combining them with our in-domain data via domain adaptation, as well as by decompounding methods on both the indomain data and the mixed-domain set. 4.1 Related Work Domain adaptation has been applied to most components of statistical machine translation: language models (Clarkson and Robinson, 1997; Koehn and Schroeder, 2007), word alignment (Hua et al., 2005), and translation models (Foster and Kuhn, 2007; Sennrich, 2012). Combinations of these methods often show that there is an overlap in the translation problems that the methods fix, which leads to one method “stealing” part of the effect of the others (Koehn and Schroeder, 2007; Sennrich, 2012). We perform domain adaptation with language and translation models, following the mixture-modeling approach by Foster and Kuhn (2007) and Sennrich (2012). A common method to handle complex compounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of"
2013.mtsummit-posters.9,2010.jec-1.7,1,0.909451,"Missing"
2013.mtsummit-posters.9,W12-3157,0,0.0127644,"ounding in languages such as German is to split unknown compounds into their parts, if 267 they occur in the training data, and join the translated parts on the output side of the translation system. The splitting can be motivated morphologically (Stymne, 2009; Hardmeier et al., 2010) or empirically (Koehn and Knight, 2003; Dyer, 2009). Also, instead of making a final decision on whether to split a compound or not, both alternatives can be passed to the translation system by representing the input text with a lattice of phrases instead of just a single sentence (Dyer et al., 2008; Dyer, 2009; Wuebker and Ney, 2012). Our approach lies between the splitting and lattice-based approaches and is especially tailored for translation between languages with heavy compounding and word order differences. 4.2 Domain Adaptation The moderate size of our in-domain data set and relatively high OOV rates suggest adding bigger general-domain corpora to the training material. The specific nature of the indomain data makes it necessary to use domain adaptation techniques, to avoid having the bigger general-domain data override the original Metric Mode Avg ssel BLEU ↑ In-Domain Only Domain Adaptation (DA) DA + Decompounding"
2013.mtsummit-posters.9,2012.amta-wptp.10,0,0.040833,"lation system. The focus of the paper is the suitability of such texts for SMT; we present experiments in domain adaptation and decompounding that improve the baseline translation systems, the results of which are evaluated using automatic metrics as well as manual evaluation. 1 Introduction As machine translation and post-editing are gaining popularity on an industrial level, global companies turn to using their accumulated translations for setting up in-house SMT services. Substantial cost and time savings have been reported in various sectors, such as software localization (Flournoy, 2011; Zhechev, 2012) or film and television subtitling (Volk et al., 2010). In a joint project between the University of Zurich and SemioticTransfer AG, we target a new domain: automobile marketing texts. Martin Volk1 SemioticTransfer AG Bruggerstrasse 37 CH-5400 Baden manuela.weibel @semiotictransfer.ch We show that even limited amounts of indomain translation material allow for building domain-specific SMT systems (see Section 3), and that translation quality can be significantly improved by using out-of-domain material and language-specific preprocessing (see Section 4). Our aim is to give an example of how ev"
2013.mtsummit-user.10,D07-1091,0,0.0423564,"T quality, we explored several approaches, taking into account issues of training and decoding efficiency, as well as issues regarding the integration of data from different sources and domains. The baseline SMT phrase-based systems were trained on large numbers of translated subtitles provided by the subtitling companies (between 200,000 and 2 million subtitles per language pair), using the Moses framework (Koehn et al., 2007). To improve the baselines, two sets of experiments were performed: incorporating linguistic information (including factored models in various configurations (Koehn and Hoang, 2007), syntax-based statistical translation and decompounding), and development of larger models by combining in-domain and out-of-domain data via mixture-modeling and perplexity minimization techniques (Sennrich, 2012). Overall, the first approach provided little to no improvement over the baselines, whereas the second one proved successful at a comparatively lower cost. In this talk, we will describe the main experiments and their results, offering insight on the optimal balance between development costs and the requirement for better systems accuracy in professional applications. Sima’an, K., Fo"
2013.mtsummit-user.10,P07-2045,0,0.00764762,"Missing"
2013.mtsummit-user.10,E12-1055,0,0.0253877,"phrase-based systems were trained on large numbers of translated subtitles provided by the subtitling companies (between 200,000 and 2 million subtitles per language pair), using the Moses framework (Koehn et al., 2007). To improve the baselines, two sets of experiments were performed: incorporating linguistic information (including factored models in various configurations (Koehn and Hoang, 2007), syntax-based statistical translation and decompounding), and development of larger models by combining in-domain and out-of-domain data via mixture-modeling and perplexity minimization techniques (Sennrich, 2012). Overall, the first approach provided little to no improvement over the baselines, whereas the second one proved successful at a comparatively lower cost. In this talk, we will describe the main experiments and their results, offering insight on the optimal balance between development costs and the requirement for better systems accuracy in professional applications. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 369–370. c 2013 The authors. This article is licensed under a Creative Com"
2013.mtsummit-wptp.10,aziz-etal-2012-pet,0,0.0868482,"nto French: A reference translation produced by a specialized translator prior to our experiment and translations by participants in the TM-Only and Post-Edit conditions as well as the English gloss. project is to build a domain-specific machine translation system for the LSP to use in a post-editing scenario. There have been a number of studies that assess the efficiency of post-editing (e.g., Guerberof, 2009; Sousa et al., 2011; Green et al., 2013). Most of these set up controlled environments for their experiments and develop specially tailored user interfaces for post-editing tasks (e.g., Aziz et al., 2012). The main reason for this is the priority placed on precise measurements of translation time, pause durations and input device activities. Some evaluations of post-editing in an industrial context have also been reported. For example, Plitt and Masselot (2010) replace manual translation with post-editing for software localization. Other industry-oriented studies (Volk et al., 2010; Flournoy, 2011) focus more on the challenges of deploying machine translation in the respective sector or company. Our work strives to combine the key elements of these two approaches: We ensure precise time and ac"
2013.mtsummit-wptp.10,W12-3102,0,0.0235574,"condition. 5.2 Pairwise Ranking In addition, we tested whether the participants (P1–P6) prefer professional translations produced by the LSP staff over those produced in the study. We applied a pairwise ranking procedure4 in which evaluators compare two translations ht1 , t2 i of a given segment in the source language and choose the better fit, with ties allowed. We used six random German segments from each text (A–D) and had all participants compare the corresponding professional translation to those produced by all 4 A similar procedure was used at the 2012 Workshop for Machine Translation (Callison-Burch et al., 2012). In contrast to other human evaluation metrics such as fluency and adequacy judgments on ordinal scales, pairwise rankings are usually more comprehensible and better reproducible for non-expert evaluators. Condition TM-Only Post-Edit Wins Ties Losses p-value 112 128 94 96 154 136 0.012 0.667 Table 5: Pairwise ranking of translations produced in the study against professional reference translations. p-values indicate genuine differences between the number of wins and losses (Sign Test). other participants, such that each participant evaluated 120 htprof essional , tparticipanti i tuples in tot"
2013.mtsummit-wptp.10,W12-3123,0,0.119063,"the catalogue [to be translated] I would probably prefer the mode with pre-translations. P3 indicated that the machine translations were helpful for translating difficult texts in terms of vocabulary, [...] but I think [for translating] the two easiest texts [...], the pre-translations would have only confused me. This stands in sharp contrast to the fact that postediting resulted in significantly faster and even slightly better translations in our study. However, the discrepancy between translators’ perceptions and post-editing performance is a well-known phenomenon in the field (see, e.g., Koponen, 2012). On the other hand, our participants were by no means technology-averse in general: All of them used various computer-aided translation tools in the tasks and deemed both the domain-specific translation memory and the bilingual terminology database as either “sometimes useful” (3/3), “useful” (1/2), or “very useful” (2/1). 7 Conclusion We have proposed a design for translation efficiency experiments that compares post-editing to computer-aided translation using a fully-featured translation workbench. In contrast to the simplified user interfaces deployed in other studies (e.g., Sousa et al.,"
2013.mtsummit-wptp.10,2013.mtsummit-posters.9,1,0.825903,"ranslators. The participants were asked to translate the German source text (TM-Only) or revise the French MT output (Post-Edit) as needed to produce high-quality French target texts. They were encouraged to work however they wanted and had access to the fuzzy TM matches, the terminology database, and online resources. Pre-edit translation drafts were produced by a domain-specific statistical machine translation system. It was built using the same translation memory data that was used for the present study, as well as out-of-domain parallel corpora; a more detailed description can be found in Läubli et al. (2013a,b). We implemented a simple RPC-based software link to enable seamless integration of the translation system into the translation workbench. The German source texts for the translation tasks were provided by the LSP. We selected four typical texts (A–D) that cover specific aspects of our domain in scope (see Table 2). Since all of the texts had been translated by professional translators at the LSP in their normal workflow, we also had access to the corresponding reference translations into French. This allowed us to compare the output of translators experienced in the automobile industry te"
2013.mtsummit-wptp.10,R11-1014,0,0.0967918,"Missing"
2013.mtsummit-wptp.10,2010.jec-1.7,1,0.893471,"Missing"
2013.mtsummit-wptp.10,2012.eamt-1.31,0,\N,Missing
2013.mtsummit-wptp.10,2012.tc-1.5,0,\N,Missing
2014.eamt-1.37,P11-2031,0,0.0492909,"Missing"
2014.eamt-1.37,W10-1734,0,0.013883,"ting The German language has a productive compounding system, which increases vocabulary size and exacerbates the data sparsity effect. Many compounds are domain-specific and are unlikely to be learned from larger general-domain corpora. Compound splitting, however, has the potential to also work on our in-domain texts. We evaluate two methods of compound splitting. Koehn and Knight (2003) describe a purely data-driven approach, in which frequency statistics are collected from the unsplit corpus, and words are split so that the geometric mean of the word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this h"
2014.eamt-1.37,E03-1076,0,0.292035,"xample: Ger: siehe auch ecl kd042 de crm basis MP-MAR-11, kapitel 9.2.1.1 Eng: see also ecl kd042 de crm basis MP-MAR-11, chapter 9.2.1.1 c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 159 While these technical tokens need no translation, our baseline system also suffers from a large number of out-of-vocabulary tokens (OOVs) that should be translated. The concatenative morphology of German compounds is a classical problem for machine translation, as it leads to an increased vocabulary and exacerbates data sparsity (Koehn and Knight, 2003). In our case the problem is inflated due to the domain-specific compound terms like Tabellenattribute (table attribute) or Nachbuchungen (subsequent postings): many of these are not seen in the smaller in-domain parallel corpus and they are too specific to be present in general-domain corpora. Technical tokens like URLs and alphanumeric IDs do not require translation and should be transferred into the output verbatim. However, since they are also unknown to the translation system, they still present a number of problems. They are often broken by tokenization and not restored properly by subse"
2014.eamt-1.37,P07-2045,0,0.00544158,"Missing"
2014.eamt-1.37,2005.mtsummit-papers.11,0,0.08957,"Missing"
2014.eamt-1.37,schmid-etal-2004-smor,0,0.0197712,"he word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this hybrid approach, we only consider nouns for compound splitting; with the datadriven approach on the other hand we have no control over which word classes are split. 2 http://www.wiktionary.org 161 Source: Reference: Masking: Reduction: erweiterung tabellen TX VL und TXTSVL . extension of tables TX VL and TXTSVL . extension of tables TX VL TXTSVL and . extension of tables TX VL and TXTSVL . Table 2: An example of the effect of reducing: the correct order of technical tokens is preserved. 5 Experiments and Results We evaluated our experiments"
2014.eamt-1.37,sennrich-kunz-2014-zmorge,1,0.82568,"corpus, and words are split so that the geometric mean of the word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this hybrid approach, we only consider nouns for compound splitting; with the datadriven approach on the other hand we have no control over which word classes are split. 2 http://www.wiktionary.org 161 Source: Reference: Masking: Reduction: erweiterung tabellen TX VL und TXTSVL . extension of tables TX VL and TXTSVL . extension of tables TX VL TXTSVL and . extension of tables TX VL and TXTSVL . Table 2: An example of the effect of reducing: the correct order of technical tokens is preserved."
2014.eamt-1.37,E12-1055,1,0.900155,"Missing"
2014.eamt-1.37,tiedemann-2012-parallel,0,0.0568003,"Missing"
2014.eamt-1.37,steinberger-etal-2006-jrc,0,\N,Missing
2014.eamt-1.37,W13-2201,0,\N,Missing
2020.tacl-1.35,D16-1025,0,0.021127,"the advent of neural models, Machine Translation (MT) systems have made substantial progress, reportedly achieving near-human quality for high-resource language pairs (Hassan et al., 2018; Barrault et al., 2019). However, translation quality is not consistent across language pairs, domains, and datasets. This is problematic for low-resource scenarios, where there is not enough training data and translation quality significantly lags behind. Additionally, neural MT (NMT) systems can be deceptive to the end user as they can generate fluent translations that differ in meaning from the original (Bentivogli et al., 2016; Castilho et al., 2017). 539 Transactions of the Association for Computational Linguistics, vol. 8, pp. 539–555, 2020. https://doi.org/10.1162/tacl a 00330 Action Editor: Stefan Riezler. Submission batch: 1/2020; Revision batch: 4/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. probability mass to predictions that are far from the training data (Gal and Ghahramani, 2016). To overcome such deficiencies, we propose ways to exploit output distributions beyond the top-1 prediction by exploring uncertainty quantification methods for"
2020.tacl-1.35,2020.eamt-1.16,1,0.787809,"/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding to the best epoch as identified by the metric of refe"
2020.tacl-1.35,C04-1046,0,0.353281,"Missing"
2020.tacl-1.35,W17-4755,0,0.0539918,"Missing"
2020.tacl-1.35,W16-2302,0,0.0478511,"Missing"
2020.tacl-1.35,D19-1308,0,0.0163557,"experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and model probabilities (orange), and Met"
2020.tacl-1.35,W14-3348,0,0.0203514,"ity outputs on NMT training with back translations. Second, we measure lexical variation between the MT outputs generated for the same source segment when running inference with dropout. We posit that differences between likely MT hypotheses may also capture uncertainty and potential ambiguity and complexity of the original sentence. We compute an average similarity score (sim) between the set H of translation hypotheses: |H ||H| 1 XX D-Lex-Sim = sim(hi , hj ) C i=1 j =1 where hi , hj ∈ H, i 6= j and C = 2−1 |H|(|H |− 1) is the number of pairwise comparisons for |H| hypotheses. We use Meteor (Denkowski and Lavie, 2014) to compute similarity scores. 3.3 Attention Attention weights represent the strength of connection between source and target tokens, which may be indicative of translation quality (Rikters and Fishel, 2017). One way to measure it is to compute the entropy of the attention distribution: I Att-Ent = − J 1 XX αji log αji I i=1 j =1 where α represents attention weights, I is the number of target tokens and J is the number of source tokens. This mechanism can be applied to any NMT model with encoder-decoder attention. We focus on attention in Transformer models, as it is currently the most widely"
2020.tacl-1.35,N19-1423,0,0.0129677,"ration (Guo et al., 2017). On the other hand, due to the small amount of training data the model can overfit, resulting in inferior results both in terms of translation quality and correlation. It is noteworthy, however, that supervised QE system suffers a larger drop in performance than unsupervised indicators, as its 6 We note that PredEst models are systematically and significantly outperformed by BERT-BiRNN. This is not surprising, as large-scale pretrained representations have been shown to boost model performance for QE (Kepler et al., 2019a) and other natural language processing tasks (Devlin et al., 2019). 7 Models for these languages were trained using Transformer-Big architecture from Vaswani et al. (2017). Low-resource Mid-resource High-resource Method Si-En Ne-En Et-En Ro-En En-De En-Zh I TP Softmax-Ent (-) Sent-Std (-) 0.399 0.457 0.418 0.482 0.528 0.472 0.486 0.421 0.471 0.647 0.613 0.595 0.208 0.147 0.264 0.257 0.251 0.301 II D-TP D-Var (-) D-Combo (-) D-Lex-Sim 0.460 0.307 0.286 0.513 0.558 0.299 0.418 0.600 0.642 0.356 0.475 0.612 0.693 0.332 0.383 0.669 0.259 0.164 0.189 0.172 0.321 0.232 0.225 0.313 III AW : Ent-Min (-) AW : Ent-Avg (-) AW : best head/layer (-) 0.097 0.10 0.255 0.26"
2020.tacl-1.35,P18-1069,0,0.0928735,"ed. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defi"
2020.tacl-1.35,C16-1133,0,0.0483943,"Missing"
2020.tacl-1.35,C16-1294,0,0.0197172,"me of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 201"
2020.tacl-1.35,N15-1124,0,0.135368,"translation quality can be extracted from multihead attention. To evaluate our approach in challenging settings, we collect a new dataset for QE with 6 language pairs representing NMT training in high, medium, and low-resource scenarios. To reduce the chance of overfitting to particular domains, our dataset is constructed from Wikipedia documents. We annotate 10K segments per language pair. By contrast to the vast majority of work on QE that uses semi-automatic metrics based on post-editing distance as gold standard, we perform quality labeling based on the Direct Assessment (DA) methodology (Graham et al., 2015b), which has been widely used for popular MT evaluation campaigns in the recent years. At the same time, the collected data differs from the existing datasets annotated with DA judgments for the well known WMT Metrics task1 in two important ways: We provide enough data to train supervised QE models and access to the NMT systems used to generate the translations, thus allowing for further exploration of the glass-box unsupervised approach to QE for NMT introduced in this paper. Our main contributions can be summarized as follows: (i) A new, large-scale dataset for sentencelevel2 QE annotated w"
2020.tacl-1.35,W13-2305,0,0.432265,"tion of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that closely preserves the semantics of the source sentence; and 91–100, a perfect translation. Each segment was evaluated indep"
2020.tacl-1.35,W19-6721,0,0.0372157,"Missing"
2020.tacl-1.35,D19-1632,1,0.899651,"Missing"
2020.tacl-1.35,P19-3020,0,0.0842974,"Missing"
2020.tacl-1.35,W17-4763,0,0.168703,"London 5 Facebook AI 1 {m.fomicheva,f.blain,n.aletras,l.specia}@sheffield.ac.uk 2 {ssun32}@jhu.edu 3 {lisa.yankovskaya,fishel}@ut.ee 5 {fguzman,vishrav}@fb.com Abstract Thus, it is crucial to have a feedback mechanism to inform users about the trustworthiness of a given MT output. Quality estimation (QE) aims to predict the quality of the output provided by an MT system at test time when no gold-standard human translation is available. State-of-the-art (SOTA) QE models require large amounts of parallel data for pretraining and in-domain translations annotated with quality labels for training (Kim et al., 2017a; Fonseca et al., 2019). However, such large collections of data are only available for a small set of languages in limited domains. Current work on QE typically treats the MT system as a black box. In this paper we propose an alternative glass-box approach to QE that allows us to address the task as an unsupervised problem. We posit that encoder-decoder NMT models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) offer a rich source of information for directly estimating translation quality: (a) the output probability distribution from the NMT system (i.e., the probabilit"
2020.tacl-1.35,K18-1056,0,0.0255158,"del ensembling). For this smallscale experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and"
2020.tacl-1.35,2005.mtsummit-papers.11,0,0.0830211,"nese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that clos"
2020.tacl-1.35,C18-1266,1,0.840227,"he DA judgments are available at https://github. com/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding"
2020.tacl-1.35,1983.tc-1.13,0,0.671274,"Missing"
2020.tacl-1.35,D15-1166,0,0.133746,"Missing"
2020.tacl-1.35,J82-2005,0,0.698459,"Missing"
2020.tacl-1.35,N19-4009,0,0.237923,"nd the empirical frequencies of the predicted labels, or by assessing generalization of uncertainty under domain shift (see §6). Only a few studies have analyzed calibration in NMT and they came to contradictory conclusions. Kumar and Sarawagi (2019) measure calibration error by comparing model probabilities and the percentage of times NMT output matches reference translation, and conclude that NMT probabilities are poorly calibrated. However, the calibration error metrics they use are designed for binary classification tasks and cannot be easily transferred to NMT (Kuleshov and Liang, 2015). Ott et al. (2019) analyze uncertainty in NMT by comparing predictive probability distributions with the empirical distribution observed in human translation data. They conclude that NMT models ministic NMT (§3.1) or (ii) using uncertainty quantification (§3.2), and (iii) attention weights (§3.3). are well calibrated. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides"
2020.tacl-1.35,W18-6450,0,0.0135015,"he current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Existing datasets with sentence-level DA judgments from the WMT Metrics Task could in principle be used for benchmarking QE systems. However, they contain only a few hundred segments per language pair and thus hardly allow for training supervised systems, as illustrated by the weak correlation results for QE on DA judgments based on the Metrics Task data recently reported by Fonseca et al. (2019). Furthermore, for each language pair the data contains translations from a number of MT systems often using different architectures, and these MT systems are not readily available, making it"
2020.tacl-1.35,W18-6301,0,0.1208,"e sampled documents and then translated them into English using the MT models described below. For German and Chinese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conve"
2020.tacl-1.35,W19-5302,0,0.0298412,"Missing"
2020.tacl-1.35,W12-3116,0,0.0450527,"Missing"
2020.tacl-1.35,2021.eacl-main.115,1,0.859327,"Missing"
2020.tacl-1.35,W12-3114,0,0.0324498,"system. Existing work on glass-box QE is limited to features extracted from statistical MT, such as language model probabilities or number of hypotheses in the n-best list (Blatz et al., 2004; Specia et al., 2013). The few approaches for unsupervised QE are also inspired by the work on statistical MT 2 While the paper covers QE at sentence level, the extension of our unsupervised metrics to word-level QE would be straightforward and we leave it for future work. 1 h t t p : / /www.statmt.org/wmt19/metricstask.html. 540 and perform significantly worse than supervised approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based"
2020.tacl-1.35,D15-1182,0,0.0718096,"Missing"
2020.tacl-1.35,W19-8671,0,0.0592008,"Missing"
2020.tacl-1.35,2006.amta-papers.25,0,0.114,"nterpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WM"
2020.tacl-1.35,W19-4808,0,0.0153618,"l information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the re"
2020.tacl-1.35,P13-4014,1,0.932381,"Missing"
2020.tacl-1.35,P19-1580,0,0.162993,"models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this met"
2020.tacl-1.35,2009.eamt-1.5,1,0.860291,"t unsupervised QE indicators obtained from well-calibrated NMT model probabilities rival strong supervised SOTA models in terms of correlation with human judgments. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require"
2020.tacl-1.35,W18-6465,0,0.0275911,"s. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require a significant amount of in-domain labeled data for training. They do not use any internal information from the MT system. Existing work on glass-box QE is lim"
2020.tacl-1.35,D19-1073,0,0.314906,"ns for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defining a simple QE measure based on sequence-level translation probability normalized by length: TP"
2020.tacl-1.35,W18-6466,1,0.857973,"d approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challe"
2020.tacl-1.35,W18-6451,1,\N,Missing
2020.tacl-1.35,W19-5301,1,\N,Missing
2020.tacl-1.35,W19-5401,1,\N,Missing
2020.wmt-1.116,C04-1046,0,0.103907,"Missing"
2020.wmt-1.116,2020.acl-main.747,1,0.846095,"Missing"
2020.wmt-1.116,N19-1423,0,0.0390624,"data for all languages and use it for training our regression models. Note that we do not add any language identification markers and the system does not require them for making predictions. This can be useful for multilingual translation systems where the user does not need to identify the input languages, and especially for zero-shot settings where a given language pair may not have been seen at training time. 2.2 Black-box We explore a baseline neural QE model and a multitask learning QE model, both of which are built on top of pre-trained contextualised representations (CR) such as BERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020). Baseline QE model (BASE) Given a source sentence sX in language X and a target sentence sY in language Y , we model the QE function f by stacking a 2-layer multilayer perceptron (MLP) on the vector representation of the [CLS] token from a contextualised representations model (CR): f (sX , sY ) =W2 · ReLU ( W1 · Ecls (sX , sY ) + b1 (1) ) + b2 where W2 ∈ R1×4h , b2 ∈ R, W1 ∈ R4h×h and b1 ∈ R4h . Ecls is a function that extracts the vector representation of the [CLS] token after encoding the concatenation of sX and sY with CR and 1011 ReLU is the Rectified Linea"
2020.wmt-1.116,C18-1266,1,0.719629,"Unit activation function. Note that h is the output dimension of Ecls . We explore two training strategies: The bilingual (BL) strategy trains a QE model for every language pair while the multilingual (ML) strategy trains a single multilingual QE model for all language pairs, where the training data is simply pooled together without any language identifier. We note that this multilingual model here corresponds to a pooled, single-task learning approach. MTL-LS submodel trained on the same language pair as the test set. BiRNN We compared the above approaches to the BiRNN model from deepQuest (Ive et al., 2018). The BiRNN model uses an encoder-decoder architecture: it encodes both source and translation sentences independently using two bi-directional Recurrent Neural Networks (RNNs). The two resulting sentence representations are concatenated afterwards as the weighted sum of their word vectors, generated by an attention mechanism. For predictions at sentence-level, the weighted representation of the two input sentences is passed through a dense layer with sigmoid activation to generate the quality estimates. This is a light-weight variant of the black-box approaches above that does not rely on hea"
2020.wmt-1.116,P19-3020,0,0.0589584,"Missing"
2020.wmt-1.116,2006.amta-papers.25,0,0.110843,"QE) (Blatz et al., 2004; Specia et al., 2009) is an important part of Machine Translation (MT) pipeline. It allows us to evaluate how good a translation is without comparison to reference sentences. As part of the WMT20 Shared Task on Quality Estimation, two sentence-level tasks were proposed. In Task 1, participants are asked to predict human judgements of MT quality generated following a methodology similar to Direct Assessment (DA) (Graham et al., 2017). The goal of Task 2 is to estimate the post-editing effort required in order to correct the MT outputs and measured using the HTER metric (Snover et al., 2006). 1 http://www.statmt.org/wmt20/ quality-estimation-task.html ∗ Equal contribution. Approach Below we first describe our glass-box submissions based on the quality indicators that can be obtained as a by-product of decoding with an NMT system. Second, we present our neural-based QE submissions, which explore transfer learning with pretrained representations. In both cases, we describe how QE is addressed as a multilingual task. 2.1 Glass-box Glass-box approaches to QE are based on information from the NMT system used to translate the sentences, rather than looking at source and target sentence"
2020.wmt-1.116,2009.eamt-1.5,1,0.913127,"Missing"
2020.wmt-1.116,N15-1124,0,0.0451071,"Missing"
berka-etal-2012-automatic,niessen-etal-2000-evaluation,0,\N,Missing
berka-etal-2012-automatic,C08-1141,0,\N,Missing
berka-etal-2012-automatic,J03-1002,0,\N,Missing
berka-etal-2012-automatic,fishel-etal-2012-terra,1,\N,Missing
berka-etal-2012-automatic,vilar-etal-2006-error,0,\N,Missing
berka-etal-2012-automatic,W11-2107,0,\N,Missing
etchegoyhen-etal-2014-machine,D11-1033,0,\N,Missing
etchegoyhen-etal-2014-machine,P07-2045,0,\N,Missing
etchegoyhen-etal-2014-machine,E12-1055,0,\N,Missing
etchegoyhen-etal-2014-machine,N03-1017,0,\N,Missing
etchegoyhen-etal-2014-machine,P13-4014,0,\N,Missing
etchegoyhen-etal-2014-machine,petukhova-etal-2012-sumat,1,\N,Missing
etchegoyhen-etal-2014-machine,tiedemann-2012-parallel,0,\N,Missing
fishel-etal-2012-terra,specia-etal-2010-dataset,0,\N,Missing
fishel-etal-2012-terra,W10-1755,0,\N,Missing
fishel-etal-2012-terra,J11-4002,1,\N,Missing
fishel-etal-2012-terra,P02-1040,0,\N,Missing
fishel-etal-2012-terra,W10-1738,0,\N,Missing
fishel-etal-2012-terra,P11-1103,0,\N,Missing
fishel-etal-2012-terra,W09-0401,0,\N,Missing
fishel-etal-2012-terra,P07-2045,1,\N,Missing
fishel-etal-2012-terra,P11-1022,0,\N,Missing
fishel-etal-2012-terra,C08-1141,0,\N,Missing
fishel-etal-2012-terra,P11-4010,0,\N,Missing
fishel-etal-2012-terra,2011.eamt-1.12,0,\N,Missing
fishel-etal-2012-terra,vilar-etal-2006-error,0,\N,Missing
fishel-etal-2012-terra,2010.eamt-1.12,0,\N,Missing
fishel-kaalep-2008-experiments,steinberger-etal-2006-jrc,0,\N,Missing
fishel-kaalep-2008-experiments,2005.mtsummit-papers.11,0,\N,Missing
fishel-kirik-2010-linguistically,N04-4015,0,\N,Missing
fishel-kirik-2010-linguistically,P02-1040,0,\N,Missing
fishel-kirik-2010-linguistically,D07-1091,0,\N,Missing
fishel-kirik-2010-linguistically,P07-2045,0,\N,Missing
fishel-kirik-2010-linguistically,J04-2003,0,\N,Missing
fishel-kirik-2010-linguistically,P08-2039,0,\N,Missing
fishel-kirik-2010-linguistically,2007.mtsummit-papers.65,0,\N,Missing
fishel-kirik-2010-linguistically,J03-1002,0,\N,Missing
fishel-kirik-2010-linguistically,W05-0908,0,\N,Missing
P15-3002,N09-1014,0,0.0144851,"the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual informa"
P15-3002,W09-2404,0,0.492585,"ask. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly"
P15-3002,W10-1737,0,0.0683154,"Missing"
P15-3002,2012.eamt-1.60,0,0.0984312,"ignment. We verify if the translations of Y in both noun phrases are identical or different. Both elements comprising the compound structure XY /Y are identified, for the standard cases, with only one possible XY referring to one Y . The translation of both words are provided by the baseline SMT system, and our system subsequently verifies if the translations of Y in both noun phrases are identical or different. We keep them intact in the first case, while in the second 3 Experimental Settings The experiments are carried out on two different parallel corpora: the WIT3 Chinese-English dataset (Cettolo et al., 2012) with transcripts of TED lectures and their translations, and the Text+Berg German-French corpus (Bubenhofer et al., 2013), a collection of articles from the year4 Upon manual examination, we found that using the most recent XY was not a reliable candidate for the antecedent. 5 In fact, we can use the translation of Y as a translation candidate for XY . Our observations show that this helps to improve BLEU scores, but does not affect the specific scoring of Y in Section 4. 1 Using the Stanford Word Segmenter available from http://nlp.stanford.edu/software/segmenter.shtml. 2 Using the Stanford"
P15-3002,loaiciga-etal-2014-english,1,0.84855,"idates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this sec"
P15-3002,H92-1045,0,0.725085,"derico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrase"
P15-3002,D11-1084,0,0.315107,"2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences h"
P15-3002,E12-3001,0,0.0176957,"Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its a"
P15-3002,W12-0117,1,0.841939,"since it only enforces a translation when it appears as one of the translation candidates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system tran"
P15-3002,2010.iwslt-papers.10,0,0.0165377,"tems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al.,"
P15-3002,P03-1021,0,0.0144953,"imize the likelihood of coreference. In German, less restrictive rules selected 7,365 XY /Y pairs (a rate of one every 40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable."
P15-3002,D12-1108,0,0.0604372,"vements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which th"
P15-3002,P02-1040,0,0.0921765,"40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable. Therefore, we analyzed these “undecided” situations BASELINE C ACHING P OST- EDITING O RACLE ZH/EN 11.18 11.23 11.27 11.30 DE/FR"
P15-3002,D07-1091,0,0.018175,"-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which the noun phrase Y had multiple meanings, our system reduced the frequency of mistranslations in"
P15-3002,W10-2602,0,0.770009,"anslated word), in which case we apply post-editing as above on the word preceding Y , if it is aligned. In the caching method (Mascarell et al., 2014), once an XY compound is identified, we obtain the translation of the Y part of the compound through the word alignment given by the SMT decoder. Next, we check that this translation appears as a translation of Y in the phrase table, and if so, we cache both Y and the obtained translation. We then enforce the cached translation every time a coreference Y to XY is identified. Note that this is different from the probabilistic caching proposed by Tiedemann (2010), because in our case the cached translation is deterministically enforced as the translation of Y . Enforcing the Translation of Y Two language-independent methods have been designed to ensure that the translations of XY and Y are a consistent: post-editing and caching. The second one builds upon an earlier proposal tested only on DE/FR with subjective evaluations (Mascarell et al., 2014). In the post-editing method, for each XY /Y pair, the translations of XY and Y by a baseline SMT system (see Section 3) are first identified through word alignment. We verify if the translations of Y in both"
P15-3002,2011.mtsummit-papers.13,0,0.364573,"other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we com"
P15-3002,N12-1046,0,\N,Missing
P15-3002,P07-2045,0,\N,Missing
petukhova-etal-2012-sumat,W09-4610,1,\N,Missing
petukhova-etal-2012-sumat,P07-2045,0,\N,Missing
petukhova-etal-2012-sumat,2005.eamt-1.29,0,\N,Missing
petukhova-etal-2012-sumat,R11-1014,0,\N,Missing
petukhova-etal-2012-sumat,2010.eamt-1.23,1,\N,Missing
W07-2443,J03-1002,0,0.00567239,"Missing"
W07-2443,E06-1032,0,0.0118302,"orpora are combined by simple concatenation. The BLEU score of the SMT models trained on the combined corpus is 41.6 and 45.22 when tested on the UT and JRC-Acquis test set, respectively. When compared to the intra-corpus translation results, the improvement of the UT test set score (2.34 BLEU) is slightly lower than the improvement of the JRC-Acquis one (2.84 BLEU). This supports the hypothesis made in the inter-corpus experiment section: the models built on the UT training set generalize better on the JRC-Acquis test set than vice versa. 4.3 Manual Output Evaluation It has been pointed out (Callison-Burch et al, 2006) that while BLEU attempts to capture allowable variation in the translation, it allows random permuting of phrases in the hypothesis compared with the reference translation. In our opinion it also explains the relatively high BLEU score in our experiments. In order to balance these shortcomings, we carried out a limited human evaluation of the results. The human evaluator gave 6 x 250 output sentences one of the following ratings: 1) good translation, i.e. expresses the same meaning as the source sentence and is grammatically correct; 2) an acceptable translation with minor errors, i.e. expres"
W07-2443,J93-1004,0,0.122274,"dly, the contained language is considerably more constrained than spoken language – it should therefore be easier to model it. Thirdly, the law text domain potentially has a higher demand for translating huge amounts of texts, and would therefore benefit from a semi- or fully automatic translation system. 3.1 The UT Corpus The first of the abovementioned corpora 1 was created at the university of Tartu. The corpus contains 7.8 million words in English and 5.0 million in Estonian. The corpus is sentence-aligned using the Vanilla aligner (Danielsson and Ridings, 1997), based on the algorithm by Gale and Church (1993). The total number of aligned units is 435 700. 3.2 The JRC-Acquis Corpus The second used corpus consists of the Estonian and English parts of the JRC-Acquis multilingual parallel corpus (Steinberger et al., 2006). The used corpus contains 7.6 million English and 5 million Estonian words. The corpus is initially aligned on the level of paragraphs, but these are usually short and do usually contain one sentence, or even only part of a sentence. Automatic alignment was also performed using the Vanilla aligner. The total number of aligned units is 295 000. Regardless of the amount of words being"
W07-2443,E03-1076,0,0.161291,"ns measured by BLEU. Trained on Tested on UT JRC UT JRC Combined 39.26 38.45 29.80 42.38 41.60 45.22 Table 1. Translation quality of SMT systems trained /tested on different corpora as measured by BLEU. Intra-corpus translation In the first set of experiments we trained and tested the SMT model on the same corpora. This would show the relative corpus performance when used in SMT. The BLEU scores for UT and JRC-Acquis corpora were 39.26 and 42.38 respectively. The scores are noticeably higher than the ones, published for spoken/written language baseline translation – e.g. (Bojar et al, 2006), (Koehn and Knight, 2003) – which is most probably explained by the highly constrained nature of the legislation language. Inter-Corpus Translation We continued by taking a SMT model trained on one corpus and testing it on another. This would show how similar the two corpora are from the SMT perspective. Training the model using the UT corpus and testing it on the JRC-Acquis test set produced a BLEU score of 38.45, this is only slightly lower than the JRC-Acquis-trained model score. On the other hand, the JRC-Acquis-trained model gave a 29.8 BLEU score when trained on the UT test set. This suggests that the SMT model,"
W07-2443,2001.mtsummit-papers.45,0,0.0195158,"es of application&apos; and as &apos;the application&apos; in the system output. But the order of the phrases in the system output follows too much the phrase order in the source sentence - the system has failed to make the long-distance permutations. The phrase order of the source sentence is 1-2-3-4-5-6; the order of these constituents in the reference sentence is 3-2-1-6-5-4, but our system produces 12-3-5-4-6. At the moment reordering is purely the task of the distortion model of the SMT algorithm, and as indicated by the results, this is not enough. One of the ways to solve the problem is described in (Nießen and Ney, 2001). According to this method the input sentence can be reordered using morphosyntactic information, so that the word order resembles better that of the target language. Another approach to the same problem would be to 282 output: this certificate shall be submitted to the results of verification reference translation: this certificate shall reproduce the findings of the examination Another frequently examined disadvantage of the SMT output is the failure to translate several Estonian word-forms into English. The probable cause is the data sparseness, caused by the Estonian morphology and free co"
W07-2443,steinberger-etal-2006-jrc,0,\N,Missing
W08-2136,J02-3001,0,0.153068,"between speed and accuracy. Performance consistently deteriorated when splitting into smaller bins. The final system contained two variants, one with more bins based on a combination of PoS-tags and lemma frequency information, and one with fewer bins 249 based only on PoS-tag information. The three learning tasks used different splits. In general, the argument identification step was the most difficult and therefore required a larger number of bins. 3.3 Features We implemented a large number of features (over 50)1 for the SRL system. Many of them can be found in the literature, starting from Gildea and Jurafsky (2002) and onward. All features, except bag-of-words, take nominal values, which are binarized for the vectors used as input to the SVM classifier. Low-frequency feature values (except for Voice, Initial Letter, Number of Words, Relative Position, and the Distance features), below a threshold of 20 occurrences, were given a default value. We distinguish between single node and node pair features. The following single node features were used for all three learning tasks and for both the predicate and argument node:2 • Lemma, PoS, and Dependency relation (DepRel) for the node itself, the parent, and t"
W08-2136,D07-1097,1,0.889154,"Missing"
W08-2136,W05-0625,0,0.0315274,"belling, different syntactic structures risk being assigned to the same sentence, depending on which predicate is currently processed. This means that several, possibly different, parses have to be combined into one. In this experiment, the head and the dependency label were concatenated, and the most frequent one was used. In case of a tie, the first one to appear was used. The likelihood of the chosen labelling was also used as a confidence measure for the syntactic blender. 3.5 Blending and Post-Processing Combining the output from several different systems has been shown to be beneficial (Koomen et al., 2005). For the final submission, we combined the output of two variants of the pipelined SRL system, each using different data splits, with 3 The version of the joint system used in the submission was based on an early predicate prediction. More accurate predicates would give a major improvement for the results. 250 Test set WSJ Brown Pred PoS All NN* VB* All NN* VB* Labelled F1 82.90 81.12 85.52 67.48 58.34 73.24 Unlabelled F1 90.90 86.39 96.49 85.49 75.35 91.97 Syn + Sem Syn Sem Table 2: Semantic predicate results on the test sets. the SRL output of the joint system. A simple uniform weight major"
W08-2136,P07-1122,0,0.0461056,"Missing"
W08-2136,W04-0308,0,0.0872979,"Missing"
W08-2136,W08-2121,0,0.138033,"Missing"
W09-4631,W06-2920,0,0.10314,"combined to produce an analysis supported by a majority of component systems. This technique was first ˇ proposed by Zeman and Zabokrtsk´ y (2005) and further refined by Sagae and Lavie (2006), who showed that it could be construed as a special form of spanning tree parsing. In parser combination by stacking, the outputs of one or more parsers are used as features for a data-driven parser that can learn from the predictions of other models. Parser stacking was recently used by Nivre and McDonald (2008) to advance the state of the art on the multilingual test sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). We describe a series of experiments, where we first try to optimize the voting strategy, by investigating different schemes for assigning weights to Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 219–222 Joakim Nivre Uppsala University Uppsala, Sweden joakim.nivre@lingfil.uu.se the votes of different systems. We then compare voting to the alternative method of stacking. The paper is organized as follows. Section 2 describes the tools, resources and methods, common to all experiments, as well as the component parsers, used for voting and stacking. Optimiza"
W09-4631,D07-1097,1,0.889458,"e maximum spanning tree algorithm previously proposed for dependency parsing by McDonald et al. (2005). If all dependency arcs proposed by some parser are stored in a graph and weighted by their number of votes, then extracting the maximum spanning tree (MST) from this graph yields the optimal dependency tree. Sagae and Lavie (2006) also showed that accuracy can be further improved if votes are weighted by the accuracy of the component parser on all arcs where the dependent token has the same part of speech. This weighting scheme, which we will refer to as the default model, was later used by Hall et al. (2007) to achieve the best overall score in the CoNLL 2007 shared task by combining six different parsers. Samuelsson et al. (2008) used a variation on the default model, where weights are first set according to accuracy but are then iteratively updated using the following simple principle: at each iteration the MST is compared to the reference parse, after which all weights of correct arcs are given a small increase and all incorrect ones a small decrease. This technique resulted in minor score improvements over the default model. In order to obtain a baseline, the default model was applied to the"
W09-4631,H05-1066,0,0.281259,"Missing"
W09-4631,P08-1108,1,0.942743,"arsers: voting and stacking. In parser combination by voting, the outputs of (at least three) independent parsers are combined to produce an analysis supported by a majority of component systems. This technique was first ˇ proposed by Zeman and Zabokrtsk´ y (2005) and further refined by Sagae and Lavie (2006), who showed that it could be construed as a special form of spanning tree parsing. In parser combination by stacking, the outputs of one or more parsers are used as features for a data-driven parser that can learn from the predictions of other models. Parser stacking was recently used by Nivre and McDonald (2008) to advance the state of the art on the multilingual test sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). We describe a series of experiments, where we first try to optimize the voting strategy, by investigating different schemes for assigning weights to Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 219–222 Joakim Nivre Uppsala University Uppsala, Sweden joakim.nivre@lingfil.uu.se the votes of different systems. We then compare voting to the alternative method of stacking. The paper is organized as follows. Section 2 describes the tools, reso"
W09-4631,P81-1022,0,0.757514,"Missing"
W09-4631,W03-3017,1,0.731395,"n systems, and the joint system was trained on the resulting training set. All results are evaluated using the labeled attachment score, which is the percentage of tokens with correctly determined heads and dependency relations in the test corpus. Intermediate models are evaluated on the WSJ testing corpus, whereas the final scores are presented for both WSJ and Brown. All component parsers (as well as the stacking parser) were trained using MaltParser (Nivre et al., 2006), a data-driven dependency parser generator that implements two parsing algorithms: the shift-reduce algorithm proposed by Nivre (2003), Mark Fishel and Joakim Nivre in an arc-eager and an arc-standard variant (NivreEager and Nivre-Std), and the incremental parsing algorithm first described in Covington (2001), in a projective and a non-projective variant (Cov-Proj and Cov-NonProj). We used eight component parsers, defined by the four algorithm variants times two directions (forward and reverse), which is the same setup as in Samuelsson et al. (2008). Feature models and parameter settings were taken from Hall et al. (2007). The scores of the eight parsers on the two test corpora are presented in Table 1. The highest score is"
W09-4631,N06-2033,0,0.0641483,"neral technique that can be used to boost accuracy in natural language processing tasks. By combining several models for performing the same task, we can exploit the unique advantage of each model and reduce some of the random errors. In this paper, we study two techniques for combining data-driven dependency parsers: voting and stacking. In parser combination by voting, the outputs of (at least three) independent parsers are combined to produce an analysis supported by a majority of component systems. This technique was first ˇ proposed by Zeman and Zabokrtsk´ y (2005) and further refined by Sagae and Lavie (2006), who showed that it could be construed as a special form of spanning tree parsing. In parser combination by stacking, the outputs of one or more parsers are used as features for a data-driven parser that can learn from the predictions of other models. Parser stacking was recently used by Nivre and McDonald (2008) to advance the state of the art on the multilingual test sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). We describe a series of experiments, where we first try to optimize the voting strategy, by investigating different schemes for assigning weights to Kristiina Jokine"
W09-4631,W08-2136,1,0.886879,"Missing"
W09-4631,W08-2121,1,0.868696,"Missing"
W09-4631,W05-1518,0,0.259765,"Missing"
W09-4631,nivre-etal-2006-maltparser,1,\N,Missing
W12-3105,W10-1703,0,0.0513929,"Missing"
W12-3105,W11-2106,0,0.0901512,"Missing"
W12-3105,fishel-etal-2012-terra,1,0.86613,"Missing"
W12-3105,N06-1014,0,0.0173292,"Zeman et al., 2011) and Hjerson (Popovi´c and Ney, 2011) use different methods for automatic error analysis. Addicter explicitly aligns the hypothesis and reference translations and induces error categories based on the alignment coverage while Hjerson compares words encompassed in the WER (word error rate) and PER (position-independent word error rate) scores to the same end. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al., 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. Both tools use word lemmas for their analysis; we used TreeTagger (Schmid, 1995) for analyzing English, Spanish, German and French and Morˇce (Spoustov´a et al., 2007) to analyze Czech. The same tools are used for PoS-tagging in some experiments. 2.2 HYP -1 Binary Classification Pairwise comparison of sentence pairs is achieved with a binary SVM classifier, trained via sequential minimal optimization (Platt, 1998), implemented in Weka (Hall et al., 2009). The input feature vectors are comp"
W12-3105,P02-1040,0,0.0842937,"Missing"
W12-3105,W11-2111,0,0.0174685,"e scores remain to be checked against the human 69 judgments from WMT’12. The introduced TerrorCat metric has certain dependencies. For one thing, in order to apply it to new languages, a training set of manual rankings is required – although this can be viewed as an advantage, since it enables the user to tune the metric to his/her own preference. Additionally, the metric depends on lemmatization and PoS-tagging. There is a number of directions to explore in the future. For one, both Addicter and Hjerson report MT errors related more to adequacy than fluency, although it was shown last year (Parton et al., 2011) that fluency is an important component in rating translation quality. It is also important to test how well the metric performs if lemmatization and PoStagging are not available. For this year’s competition, training data was taken separately for every language pair; it remains to be tested whether combining human judgements with the same target language and different source languages leads to better or worse performance. To conclude, we have described TerrorCat, one of the submissions to the metrics shared task of WMT’12. TerrorCat is rather demanding to apply on one hand, having more requir"
W12-3105,J11-4002,1,0.872172,"Missing"
W12-3105,W07-1709,0,0.0315439,"Missing"
W13-2251,W12-3102,0,0.129546,"Missing"
W13-2251,W12-3105,1,0.898502,"Missing"
W13-2251,J11-4002,0,0.0485197,"Missing"
W13-5630,W12-3102,0,0.0245979,"onal effort. We provided our subject with a source segment (DE) alongside a reference translation (FR/IT) and two translation hypotheses for 150 randomly selected segments. One hypothesis was produced by the baseline, the other by the weighted TM system. The subject’s task was to rate which hypothesis was better, (a) in general and (b) with regard to domain technology, with ties allowed. This evaluation setup, known as pairwise system comparison, has lately been favored by the MT community for it is simpler and better reproducible than, e.g., fluency/adequacy judgements on a five-point scale (Callison-Burch et al., 2012). Furthermore, genuine differences between two systems can easily be quantified using the Sign Test for paired observations. The results, shown in Table 3, reveal the evaluator’s clear preference towards the weighted TM system’s translations. This holds especially for DE–IT. Statistically, the weighted TM systems outperform the baseline in all regards at p ≤ 0.001, except for the adequacy of domain terminology in the DE–FR systems (p ≤ 0.01). 4 Discusson Our evaluation (see Section 3.3) shows that adding large amounts of out-of-domain data without adequate weights is no suitable means of impro"
W13-5630,P11-2031,0,0.0816429,"Missing"
W13-5630,W11-2107,0,0.029414,"Missing"
W13-5630,N09-1046,0,0.0474811,"Missing"
W13-5630,W07-0712,0,0.0321312,"ing Data Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 334 of 474] 3.2 Domain-Specific SMT Systems We have trained an in-domain-only baseline and three systems that combine in-domain and out-of-domain data (see Section 3.1) in different weighting modes for each language pair. 3.2.1 Baseline Systems Using only the in-domain data, we trained a standard phrase-based statistical machine translation system for DE–FR and DE–IT. 5-gram language models were trained using the IRST Language Modeling Toolkit (Federico and Cettolo, 2007). Otherwise, we relied on the Moses decoder (Koehn et al., 2007) and its dedicated scripts for training, tokenization, and truecasing. 3.2.2 Combined Systems Additionally, we trained separate translation and language models using our out-of-domain corpora. Apart from the training data, the setup was the same as for our baseline systems. This left us with three phrase tables and language models per language pair: in-domain, EP7, and OS11. All combined systems presented in this section comprise all of these models, but they are differently weighted. The weighting modes are defined as follows. Un"
W13-5630,W07-0717,0,0.532367,"Senellart (2010) have proposed to retrieve a fuzzy match in the TM for each source segment to be translated, identify the mismatched parts, and replace these parts by an SMT translation. Their approach relies on automatic word alignment to find the target words that are affected by the mismatch. In contrast to Koehn and Senellart (2010) we approach combining parallel texts from domainspecific translation memories and general-domain corpora as a domain adaptation problem. We use the approach of mixture-modeling, commonly used for language model adaptation and extended to translation models by Foster and Kuhn (2007). The main distinctive feature of this approach for both language and translation models is that instead of separating the data and models into in-domain/out-of-domain in a binary setting, different domains are assigned real-valued weights, reflecting their similarity to in-domain text material. Using these weights the single domain models p = {pi }i=1...N are combined into a single adapted model: p(x) = wp(x) = N X w i pi (x). i=1 The weights w are selected to optimize the performance of the adapted model on an in-domain development set. Language model performance is estimated with its entrop"
W13-5630,W10-1710,0,0.0258641,"Missing"
W13-5630,2010.jec-1.3,0,0.0226418,"ublished by the European Union (Pym et al., 2012), there is evidence that the per-word rate for professional translations has decreased significantly in some western European countries. On the other hand, computer-assisted translation tools such as translation memory (TM) systems—allowing translators to store their translations in a personal or corporate database and reuse them on future occasions—have become an integral part of state-of-the-art translation workflows, even for freelancers. More recently, there have been efforts to combine machine translation systems with translation memories (Kanavos and Kartsaklis, 2010; Koehn and Senellart, 2010). As a result, major commercial systems such as SDL TradosStudio1 , Across LanguageServer2 , and even open source alternatives such as OmegaT3 now offer machine translation interfaces, allowing translators to have segments automatically (pre-)translated in case there is no corresponding translation available in their translation memory. In a joint project between the UZH Institute of Computational Linguistics and SemioticTransfer AG, we currently explore the potential of using domain-specific statistical machine translation (SMT) systems in human-based translation w"
W13-5630,2005.mtsummit-papers.11,0,0.0237065,"t may refer to a whole sentence, but also to smaller units such as short phrases or even single words of table entries. After cleaning the memories, we were able to extract 166’957 segments for DE–FR and 112’166 segments for DE–IT, which corresponds to ∼2.0 and ∼1.5 million tokens, respectively. Please note that all segments are unique. Unlike text from a corpus of running words, each segment from a translation memory occurs only once. The exact numbers are shown in Table 1. 3.1.2 Out-of-Domain As for out-of-domain data, we have chosen two freely available parallel corpora: Europarl v7 (EP7) (Koehn, 2005) and OpenSubtitles 2011 (OS11) (Tiedemann, 2009). We extracted the DE–FR and DE–IT translations from each of them, resulting in ∼48.5 (EP7) and∼16.0 (OS11) million tokens per language pair. See Table 1. We point out that these parallel corpora are not thematically related to our in-domain data. Rather than that, they are much more extensive and thus cover a broader vocabulary, which is missing in our in-domain data due to its limited size. In-Domain Segments Tokens F Tokens E Tokens/Segment F Tokens/Segment E DE–FR DE–IT 166’957 Europarl OpenSubtitles DE–FR DE–IT DE–FR DE–IT 112’166 1’903’628"
W13-5630,P07-2045,0,0.00689617,"uistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 334 of 474] 3.2 Domain-Specific SMT Systems We have trained an in-domain-only baseline and three systems that combine in-domain and out-of-domain data (see Section 3.1) in different weighting modes for each language pair. 3.2.1 Baseline Systems Using only the in-domain data, we trained a standard phrase-based statistical machine translation system for DE–FR and DE–IT. 5-gram language models were trained using the IRST Language Modeling Toolkit (Federico and Cettolo, 2007). Otherwise, we relied on the Moses decoder (Koehn et al., 2007) and its dedicated scripts for training, tokenization, and truecasing. 3.2.2 Combined Systems Additionally, we trained separate translation and language models using our out-of-domain corpora. Apart from the training data, the setup was the same as for our baseline systems. This left us with three phrase tables and language models per language pair: in-domain, EP7, and OS11. All combined systems presented in this section comprise all of these models, but they are differently weighted. The weighting modes are defined as follows. Unweighted Combination (unweighted) In the unweighted mode, we sim"
W13-5630,E03-1076,0,0.0214312,"Missing"
W13-5630,2010.jec-1.4,0,0.14978,"(Pym et al., 2012), there is evidence that the per-word rate for professional translations has decreased significantly in some western European countries. On the other hand, computer-assisted translation tools such as translation memory (TM) systems—allowing translators to store their translations in a personal or corporate database and reuse them on future occasions—have become an integral part of state-of-the-art translation workflows, even for freelancers. More recently, there have been efforts to combine machine translation systems with translation memories (Kanavos and Kartsaklis, 2010; Koehn and Senellart, 2010). As a result, major commercial systems such as SDL TradosStudio1 , Across LanguageServer2 , and even open source alternatives such as OmegaT3 now offer machine translation interfaces, allowing translators to have segments automatically (pre-)translated in case there is no corresponding translation available in their translation memory. In a joint project between the UZH Institute of Computational Linguistics and SemioticTransfer AG, we currently explore the potential of using domain-specific statistical machine translation (SMT) systems in human-based translation workflows. We hypothesize tha"
W13-5630,P03-1021,0,0.0330494,"Missing"
W13-5630,E12-1055,0,0.0154273,"ain text material. Using these weights the single domain models p = {pi }i=1...N are combined into a single adapted model: p(x) = wp(x) = N X w i pi (x). i=1 The weights w are selected to optimize the performance of the adapted model on an in-domain development set. Language model performance is estimated with its entropy on the said set. Unlike Foster and Kuhn (2007) who use a monolingual performance measure for translation Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 333 of 474] models we follow Sennrich (2012) and use the cross-entropy of the adapted translation model on the development set, a bilingual performance measure: X ˆ (x) = arg min H(p), where H(p) = − ˜p(x) log2 p(x) p w x∈X(d) Here H(p) is the cross-entropy of the adapted language or translation model p and X(d) is the development set. The empirical probability distribution ˜p is based on the development set. The nature of x ∈ X(d) depends on which models are handled: in case of language models, it represents a single sentence; in case of translation models it stands for a 〈sour ce, t r ansl at ion〉 tuple. For more details see (Sennrich"
W15-2506,W09-2404,0,0.072161,"Missing"
W15-2506,P14-1137,0,0.184341,"Missing"
W15-2506,2012.eamt-1.60,0,0.0177002,"ans, for example, that Experiments In this preliminary evaluation of our method we focus on the specific case of co-references to compounds, where the co-reference is an ambiguous word with several translations. The co-reference is disambiguated using a trigger word from the preceding context (i.e. the compound that the word co-refers to). The idea is that knowing which these compounds are we assess whether our method is able to detect them as relevant triggers. p(tgt = “pilote”|src = “driver”, trig = “road”) should be low, while The data comes from the German-English part of the WIT3 corpus (Cettolo et al., 2012), which is a collection of TED talks in multiple languages. The corpus consists of 194’533 sentences and 3.6 million tokens split into 1’596 talks (i.e. documents). The test set is also a collection of TED talks, consisting of 6’047 sentences and about 100’000 tokens. The talks differ greatly in terms of the covered topics, and therefore, have a high potential for ambiguous translations between them. This topic variety is so high that it is not feasible to tune SMT systems separately to each topic. However, it makes the corpus a feasible target for dynamic adaptation like our method. p(tgt = “"
W15-2506,D11-1084,0,0.0208297,"lated words, instead of only filtering out infrequent words. The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-base"
W15-2506,W14-3358,0,0.0170521,"Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) introduce a method that translates the coherence chain of the source, and uses it to produce a coherent translation. This topic modeling line of research can be combined with our own by including preceding sentences or their parts into the topic model training process. Figure 1: Comparison of the KL divergence rankings consi"
W15-2506,D14-1060,0,0.0177559,"n of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) introduce a method that translates the coherence chain of the source, and uses it to produce a coherent translation. This topic modeling line of research can be combined with our own by including preceding sentences or their parts into the topic model training process. Figure 1: Comparison of the KL divergence rankings considering up to 4 previous sentences. The position of the compounds listed in table 3 are pointed out among all trigger candidates. larity can be m"
W15-2506,P15-3002,1,0.870699,"Missing"
W15-2506,H94-1013,0,0.0160155,"to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximum entropy classifiers. Meng et al. (2014) propose three term translation models to disambiguate, enforce consistency and guarantee integrity. Finally, Xiong et al. (2013) intro"
W15-2506,W10-2602,0,0.0276668,"rch to semantically related words, instead of only filtering out infrequent words. The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (20"
W15-2506,2011.mtsummit-papers.13,0,0.0431267,"The reason is that trigger words that only appear in the context of an ambiguous term would be detected as infrequent, and therefore, incorrectly discarded. We are then planning to combine the distribution difference (measured with the KL divergence or other metrics) with a measure of similarity between the trigger candidate and the ambiguous word. Their simi49 post-editing method, they show improvement of translation correctness of co-referring terms in German-French and Chinese-English. Other approaches (see (Tiedemann, 2010) and (Gong et al., 2011)) use a cache-model for the same purpose. Xiao et al. (2011) enforce the translation of ambiguous words to be consistent across the document by applying a three-steps procedure. The term “trigger” is first introduced by Rosenfeld (1994). The approach to adaptive language modeling uses a maximum entropy model, showing perplexity improvements over the conventional trigram model. A recently popular approach is to include topic modeling into the SMT pipeline and to use topic distributions to disambiguate phrase translations (see e.g. (Hasler et al., 2014)). Xiong et al. (2014) present a sense-based translation model that integrates word senses using maximu"
W17-4738,J03-1002,0,0.0215891,"enforced the translation of named entities (NE) using a dictionary which we built on the training data distributed for WMT 2017. First, we performed named entity recognition (NER) using spaCy2 for German and NLTK3 for English. The reason for using different tools is that the spaCy output for English differed largely from the German one. NLTK performed much more similarly to the German spaCy output and, thus, it was easier to find NE translation pairs. We only considered NEs of type “person”, “organisation” and “geographic location” for our dictionary. Then we did word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method. We created an entry in our translation dictionary for every pair of aligned (multi-word) NEs. Per entry we only kept the three most frequent translation options. Since there was still a lot of noise in the resulting dictionary, we decided to filter it automatically by removing entries that: • did not contain alphabetical characters e.g. filtering out “2/3” aligned to “June” 3.3 Coverage Penalties Under-translation and over-translation problems are results of lacking coverage in modern NMT systems (Tu et al., 2016). Attempts"
W17-4738,P02-1040,0,0.105423,"Missing"
W17-4738,E17-3017,0,0.0803861,"Missing"
W17-4738,P16-1162,0,0.0493508,"systems with SMT. The described methods give 0.7 - 1.8 BLEU point improvements over our baseline systems. 1 Baseline Systems Our baseline systems were trained with two NMT and one statistical machine translation (SMT) framework. For English↔German we only trained NMT systems, for which we used Nematus (NT) (Sennrich et al., 2017). For English↔Latvian, apart from NT systems, we additionally trained NMT systems with Neural Monkey (NM) (Helcl and Libovick`y, 2017) and SMT systems with LetsMT! (LMT) (Vasil¸jevs et al., 2012). In all of our NMT experiments we used a shared subword unit vocabulary (Sennrich et al., 2016b) of 35000 tokens. We clipped the gradient norm to 1.0 (Pascanu et al., 2013) and used a dropout of 0.2. Our models were trained with Adadelta (Zeiler, 2012) and after 7 days of training we performed early stopping. For training the NT models we used a maximum sentence length of 50, word embeddings of size 512, and hidden layers of size 1000. For decoding with NT we used beam search with a beam size of 12. For training the NM models we used a maximum sentence length of 70, word embeddings and hidden layers of size 600. For decoding with NM a greedy decoder was used. Unfortunately, at the time"
W17-4738,P16-5005,0,0.0362274,"Missing"
W17-4738,P12-3008,0,0.0323761,"Missing"
W17-4771,D16-1157,0,0.038533,"Missing"
W17-4771,Q15-1017,0,0.0324782,"defined as follows: • a hypothesis translation word or n-gram present in the reference translation is considered precise (weight 1) • all other words and n-grams in the hypothesis are aligned to same-length n-grams in the reference by greedily selecting the most similar pair first. Similarity is computed via the cosine of the embeddings, and is used as the pair’s weight alice was beginning to … Figure 1: Example of skip-gram training for words and n-grams. Boxes show the input entries and arrows point to output entries; context window width of 1 is used for a simpler figure’s sake. We follow (Yu and Dredze, 2015) and predict single words on the output side while feeding words and n-grams on the input side. • overlaps are not allowed: once a pair is aligned it is removed from the search space for the next n-grams The rationale behind this simple modification is that partially correct words will be hopefully considered similar by the embedding model, while completely wrong words will only find alignments with lower similarity. In addition to frequency filtering we also sample the n-grams randomly, sometimes including or excluding them from training. To increase the chances of more rare n-grams being inc"
W17-4771,W16-2302,0,0.033851,"Missing"
W17-4771,W07-0718,0,0.135331,"Missing"
W17-4771,E06-1032,0,0.0698838,"embeddings, the aim of which is to find similarities between short phrases like “research paper” and “scientific article”, or “do not like” and “hate”. We propose two solutions, both reducing the problem to the original WORD 2 VEC ; the first one only handles n-grams of the same length while the second one is more general. These are described in the following sections. The Painfully Familiar Metric The BLEU metric (Papineni et al., 2002) has deeply rooted in the machine translation community and is used in virtually every paper on machine translation methods. Despite the wellknown criticism (Callison-Burch et al., 2006) and a decade of collective efforts to come up with a better translation quality metric (from CallisonBurch et al., 2007 to Bojar et al., 2016) it still appeals with its ease of implementation, language independence and competitive agreement rate with human judgments, with the only viable alternative on all three accounts being the recently introduced CHR F (Popovic, 2015). The original version of BLEU is harsh on single sentences: one of the factors of the score is a geometric mean of n-gram precisions between the translation hypothesis and reference(s) and as a result sentences without 4-gra"
W17-4771,P02-1040,0,0.105224,"relational database (Barkan and Koenigstein, 2016), sentences and documents (Le and Mikolov, 2014) and even users (Amir et al., 2017). The core part of this work consists of n-gram embeddings, the aim of which is to find similarities between short phrases like “research paper” and “scientific article”, or “do not like” and “hate”. We propose two solutions, both reducing the problem to the original WORD 2 VEC ; the first one only handles n-grams of the same length while the second one is more general. These are described in the following sections. The Painfully Familiar Metric The BLEU metric (Papineni et al., 2002) has deeply rooted in the machine translation community and is used in virtually every paper on machine translation methods. Despite the wellknown criticism (Callison-Burch et al., 2006) and a decade of collective efforts to come up with a better translation quality metric (from CallisonBurch et al., 2007 to Bojar et al., 2016) it still appeals with its ease of implementation, language independence and competitive agreement rate with human judgments, with the only viable alternative on all three accounts being the recently introduced CHR F (Popovic, 2015). The original version of BLEU is harsh"
W17-4771,W15-3049,0,0.0303511,"miliar Metric The BLEU metric (Papineni et al., 2002) has deeply rooted in the machine translation community and is used in virtually every paper on machine translation methods. Despite the wellknown criticism (Callison-Burch et al., 2006) and a decade of collective efforts to come up with a better translation quality metric (from CallisonBurch et al., 2007 to Bojar et al., 2016) it still appeals with its ease of implementation, language independence and competitive agreement rate with human judgments, with the only viable alternative on all three accounts being the recently introduced CHR F (Popovic, 2015). The original version of BLEU is harsh on single sentences: one of the factors of the score is a geometric mean of n-gram precisions between the translation hypothesis and reference(s) and as a result sentences without 4-gram matches get a score of 0, even if there are good unigram, bigram and possibly trigram matches. There have been several attempts to “soften” this approach by using arithmetic mean instead (NIST, Doddington, 2002), allowing for partial matches using 2.1 Separate N-gram Embeddings Our first approach is learning separate embedding models for unigrams, bigrams and trigrams. W"
W17-4771,W15-3052,0,0.045512,"Missing"
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W18-6407,P17-1042,0,0.185687,"entivogli et al., 2016). For some language pairs the size of parallel corpus ranges from extremely low to almost zero. These extremely low-resource language pairs were a motivation for the Unsupervised Machine Translation (Lample et al., 2017; Artetxe et al., 2017b) that aims to translate language without usage of the parallel corpora for training. The first step of the unsupervised approach to translation is the same for all methods: learning word level embedding spaces for source and target languages and then aligning these spaces. Next, one of the unsupervised vector space mapping methods (Artetxe et al., 2017a; Conneau et al., 2017) is applied to align spaces together and perform word by word translation. This mapping is only possible because of the linear properties of the word embedding method that is used to get word vector representations. Lastly, to improve system’s performance, iterative refining using either neural network models or parts of SMT pipeline is done (Mikolov et al., 2013b; Lample et al., 2018). In this work we implement a system that is similar to the latter approach. Statistical Machine Translation systems that are based on the phrase representations require smaller amounts of"
W18-6407,P16-5005,0,0.0400463,"ited set of phrases. This paper is organized as follows: In Section 2 we describe our unsupervised translation system baseline. Section 3 describes our approach to computing phrase embeddings for arbitrary phrases for UMT. In Section 4 we describe our experiments for compositional phrase embeddings standalone and in context of UMT. Section 5 concludes the work. 2 one word in one language can be a phrase in another language, like Estonian word ”laualt”, which means ”from the table”. The extraction of n-grams is done as in Blue2vec algorithm (T¨attar and Fishel, 2017). To compute embeddings (S. Harris, 1954; Mikolov et al., 2013b) we use FastText2 (Bojanowski et al., 2017) embeddings instead of word2vec (Mikolov et al., 2013a). We prefer FastText because it produces embeddings that incorporate subword level information which is proven to be helpful. After finding vectors for words and phrases, we need to project the source and target language embeddings into the same space (Artetxe et al., 2017a; Conneau et al., 2017; Artetxe et al., 2018). Projecting is done using the MUSE 3 library. We use cross lingual similarity scores to score Ngrams. Baseline UMT System Our baseline systems relies on the r"
W18-6407,D16-1025,0,0.0297181,"Missing"
W18-6407,W18-1819,0,0.0130668,"lation paradigm where we experimented with phrase lengths of up to 3. As a main contribution, we performed a set of standalone experiments with compositional phrase embeddings as a substitute for phrases as individual vocabulary entries. Results show that reasonable n-gram vectors can be obtained by simply summing up individual word vectors which retains or improves the performance of phrase-based unsupervised machine tranlation systems while avoiding limitations of atomic phrase vectors. 1 Introduction Most successful approaches to machine translation (Wu et al., 2016; Bahdanau et al., 2014; Vaswani et al., 2018; Gehring et al., 2017) rely on the availability of parallel corpora. Supervised neural machine translation (NMT) employs the encoderdecoder architecture, where the encoder reads the source sentence and produces its representation which is then fed to the decoder that tries to generate the target sentence word by word. Crossentropy loss is usually used as a training objective and beam search algorithm is used for inference. These neural models show state-of-the art performance but rely on vast amounts of parallel data. On the other hand, there is a Statistical Machine Translation paradigm that"
W18-6407,Q17-1010,0,0.216573,": In Section 2 we describe our unsupervised translation system baseline. Section 3 describes our approach to computing phrase embeddings for arbitrary phrases for UMT. In Section 4 we describe our experiments for compositional phrase embeddings standalone and in context of UMT. Section 5 concludes the work. 2 one word in one language can be a phrase in another language, like Estonian word ”laualt”, which means ”from the table”. The extraction of n-grams is done as in Blue2vec algorithm (T¨attar and Fishel, 2017). To compute embeddings (S. Harris, 1954; Mikolov et al., 2013b) we use FastText2 (Bojanowski et al., 2017) embeddings instead of word2vec (Mikolov et al., 2013a). We prefer FastText because it produces embeddings that incorporate subword level information which is proven to be helpful. After finding vectors for words and phrases, we need to project the source and target language embeddings into the same space (Artetxe et al., 2017a; Conneau et al., 2017; Artetxe et al., 2018). Projecting is done using the MUSE 3 library. We use cross lingual similarity scores to score Ngrams. Baseline UMT System Our baseline systems relies on the recently proposed Phrase-based UMT framework (Lample et al., 2018)."
W18-6407,D15-1201,0,0.134867,"an occur in the corpus. The size of the vocabulary grows almost exponentially over the length of the phrase, and thus vocabularies of that order of magnitude do not fit into the computer memory in most cases. Secondly, there is a data sparsity problem since some (even two-word) phrases occur rarely even in the very large corpus (it makes learning embedding vectors hard for some phrases). The idea to combine embeddings to get a phrase embedding (Mikolov et al., 2013b) was successfully used in context of tasks such phrase similarity (Muraoka et al., 2014) and non-compositional phrase detection (Yazdani et al., 2015). We follow similar idea and show why it is highly suitable specifically for unsupervised translation. In summary, we first learn vectors for reasonable amount of phrases as single-token vocabulary entries (e.g. ”research paper”). At the same time (as a part of the same training procedure), we learn embeddings for individual words (”research” and ”paper” separately). Finally, we train a regression 2 3 http://www.statmt.org/moses/ 362 https://github.com/facebookresearch/fastText https://github.com/facebookresearch/MUSE data from the Step 4. At this point we use the dataset from previous step to"
W18-6407,J82-2005,0,0.735476,"Missing"
W18-6407,Y14-1010,0,0.142217,"infeasible to learn and store vocabulary of all phrases that can occur in the corpus. The size of the vocabulary grows almost exponentially over the length of the phrase, and thus vocabularies of that order of magnitude do not fit into the computer memory in most cases. Secondly, there is a data sparsity problem since some (even two-word) phrases occur rarely even in the very large corpus (it makes learning embedding vectors hard for some phrases). The idea to combine embeddings to get a phrase embedding (Mikolov et al., 2013b) was successfully used in context of tasks such phrase similarity (Muraoka et al., 2014) and non-compositional phrase detection (Yazdani et al., 2015). We follow similar idea and show why it is highly suitable specifically for unsupervised translation. In summary, we first learn vectors for reasonable amount of phrases as single-token vocabulary entries (e.g. ”research paper”). At the same time (as a part of the same training procedure), we learn embeddings for individual words (”research” and ”paper” separately). Finally, we train a regression 2 3 http://www.statmt.org/moses/ 362 https://github.com/facebookresearch/fastText https://github.com/facebookresearch/MUSE data from the"
W18-6466,2006.amta-papers.25,0,0.0257093,"other languages only SMT translations were given. The number of sentences for each language pair and each machine translation system is shown in Table 1. • we find the optimal alignment between the words, n-grams or subwords of the input and output segments using beam search • using this alignment we compute the BLEU score’s (Papineni et al., 2002) n-gram precisions, giving partial credit to aligned n-gram (or word/sub-word) pairs equal to the cosine similarity of their cross-lingual embeddings 3.2 Experiments The main goal of our experiments is to predict the normalized edit distance (HTER) (Snover et al., 2006). To estimate the quality of prediction we used the Pearson correlation coefficient. As a regression model we used Random Forest (Ho, 1995) with a grid search algorithm for the optimization of parameters. To get force-decoded attention weights and We can see examples of words/phrases after training cross-lingual embeddings in Figure 2. The nearest neighbor for a source word or phrase is visualized in the figure, which can be words or phrases in target language. pervised word Embeddings, https://github.com/ facebookresearch/MUSE 818 EN-DE DE-EN EN-CS EN-LV nmt smt nmt smt nmt smt nmt smt train"
W18-6466,W18-6451,0,0.0695292,"Missing"
W18-6466,P13-4014,0,0.376558,"can get attention weights for any source/translation pair without even knowing anything about the system that produced this translation output. To get features for a regression model we have computed the following metrics proposed by RikIntroduction Over the last several years the quality of machine translation has grown significantly. However even today most machine translation systems produce a lot of unreliable translations, with translation quality varying greatly between different input and output segments. To estimate the quality of these translations several methods have been proposed (Specia et al., 2013; Martins et al., 2017; Kim et al., 2017a,b). In this article we propose an approach to quality estimation that is based on a regression model with different sets of features stemming from the internal parameters of a neural machine translation (NMT) system. We investigate how different input features of the regression model affect the correlation between the automatic quality estimation score and human assessment. We show that our models work for any translation output, without access to the translation system that produced the translations in question. 2 Attention Weights Method The main ide"
W18-6466,W17-4771,1,0.928701,"alignment visualization alone that the quality/confidence of the translation system is high: each input/output token has a strong connection to one or at most two tokens on the other side. • Word-level embeddings were trained on tokenized data that consisted only of unigram words. ters and Fishel (2017) (see their paper for a more detailed definition): • Coverage Deviation Penalty (CDP) penalizes the sum of attentions per input token, so tokens with less or too much attention get lower scores. • Phrase-level embeddings were trained on data that concatenated words into phrases stochastically (Tättar and Fishel, 2017). Phrases consisted of up to three words concatenated with underscores. • Absentmindedness Penalties (APin and APout) compute the dispersion via the entropy of the attention distribution of input and output tokens. • BPE-level embeddings use the embeddings from NMT systems that are trained on byte pair encoded data (Sennrich et al., 2015). BPE (byte-pair encoding) splits words into sub-word units in order to reduce the number of unique tokens. • Total is the sum of all three metrics described above. In addition to the metrics above we have calculated the ratio between input and output absentmi"
W18-6466,W17-4763,0,0.184899,"anslation pair without even knowing anything about the system that produced this translation output. To get features for a regression model we have computed the following metrics proposed by RikIntroduction Over the last several years the quality of machine translation has grown significantly. However even today most machine translation systems produce a lot of unreliable translations, with translation quality varying greatly between different input and output segments. To estimate the quality of these translations several methods have been proposed (Specia et al., 2013; Martins et al., 2017; Kim et al., 2017a,b). In this article we propose an approach to quality estimation that is based on a regression model with different sets of features stemming from the internal parameters of a neural machine translation (NMT) system. We investigate how different input features of the regression model affect the correlation between the automatic quality estimation score and human assessment. We show that our models work for any translation output, without access to the translation system that produced the translations in question. 2 Attention Weights Method The main idea of our method is to use features based"
W18-6466,W17-4764,0,0.0164622,"ghts for any source/translation pair without even knowing anything about the system that produced this translation output. To get features for a regression model we have computed the following metrics proposed by RikIntroduction Over the last several years the quality of machine translation has grown significantly. However even today most machine translation systems produce a lot of unreliable translations, with translation quality varying greatly between different input and output segments. To estimate the quality of these translations several methods have been proposed (Specia et al., 2013; Martins et al., 2017; Kim et al., 2017a,b). In this article we propose an approach to quality estimation that is based on a regression model with different sets of features stemming from the internal parameters of a neural machine translation (NMT) system. We investigate how different input features of the regression model affect the correlation between the automatic quality estimation score and human assessment. We show that our models work for any translation output, without access to the translation system that produced the translations in question. 2 Attention Weights Method The main idea of our method is to"
W18-6466,P02-1040,0,0.103235,"ed in the WMT18 shared task on sentence-level quality estimation (Specia et al., 2018): German-English, English-German, English-Latvian and English-Czech. For EnglishGerman and English-Latvian language pairs the translation output was produced by NMT and SMT systems, for other languages only SMT translations were given. The number of sentences for each language pair and each machine translation system is shown in Table 1. • we find the optimal alignment between the words, n-grams or subwords of the input and output segments using beam search • using this alignment we compute the BLEU score’s (Papineni et al., 2002) n-gram precisions, giving partial credit to aligned n-gram (or word/sub-word) pairs equal to the cosine similarity of their cross-lingual embeddings 3.2 Experiments The main goal of our experiments is to predict the normalized edit distance (HTER) (Snover et al., 2006). To estimate the quality of prediction we used the Pearson correlation coefficient. As a regression model we used Random Forest (Ho, 1995) with a grid search algorithm for the optimization of parameters. To get force-decoded attention weights and We can see examples of words/phrases after training cross-lingual embeddings in Fi"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5342,N19-1388,0,0.0274199,"an fine-tuning to each domain separately. Our solution is to specify the output text domain as another word factor. One peculiarity of multilingual NMT is that the model performs back-translation for itself, therefore avoiding the necessity of training more than one translation system. Introduction Typically the majority of WMT news translation shared task submissions are based on language pair-specific machine translation (MT) systems (Bojar et al., 2016, 2017, 2018). However, recently several multilingual approaches to MT have been proposed (e.g. Johnson et al., 2017; V´azquez et al., 2018; Aharoni et al., 2019). With them as inspiration, the goal of this paper is to describe our submission to the WMT’2019 news translation shared task, where we trained a single multilingual translation system using the constrained parallel and monolingual data for several language pairs. In addition to multilinguality we wanted to incorporate the multiple text domains that constitute the constrained set of parallel corpora in the WMT shared task. We approach multi-domain NMT using the method of (Tars and Fishel, 2018): namely, by treating domains as separate languages, therefore creating a “double-multilingual” syste"
W19-5342,E17-3017,0,0.142826,"ults. We only used the constrained data from the shared task. We describe our approach and its results and discuss the technical issues we faced. 1 2 Architecture Our model is a neural MT system based on autoregressive self-attention in the encoder and decoder (Vaswani et al., 2017). We achieve multilinguality in a similar fashion to (Johnson et al., 2017): using an additional input specifying the output language, so that the system would know which language to generate. Differently from Johnson et al. (2017), who include the output language into the input segment itself, we use word factors (Hieber et al., 2017) and specify the output language as a factor of each input token. In addition to multilinguality, our NMT system also uses the information on which domain the parallel or monolingual corpora come from. The WMT data consist of a variety of text domains (parliamentary speeches, crawled web and news texts, press releases, Wikipedia titles, etc.) and it has been shown (Tars and Fishel, 2018) that multidomain NMT can get much better results than the default approach of mixing heterogeneous corpora together, as well as yield more efficient solutions than fine-tuning to each domain separately. Our so"
W19-5401,D18-1274,0,0.107749,"ntence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word vector in the target with the embeddings of its neighbours and a mismatch feature indicating the difference between the prediction score of the target word and the highest one in the vocabulary. 5.4 Unbabel Unbabel participated in Tasks 1 and 2 for all language pairs. Their submissions were built upon the OpenKiwi framework: they combined linear, neural, and predictor-estimator systems (Chollampatt and Ng, 2018) with new transfer learning approaches using BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) pre-trained models. They proposed new ensemble techniques for word and sentence-level predictions. For Task 2, they combined a predictor-estimator for wordlevel predictions with a simple technique for converting word labels into document-level predictions. 5.5 UTartu UTartu participated in the sentence-level track of task 1 and in task 3. They combined BERT (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2018) embeddings to train a regression neural network model. The output objecti"
W19-5401,N19-1423,0,0.330155,"y describe their strategies and which sub-tasks they participated in. 5.1 MIPT MIPT only participated in the word-level ENDE task. They used a BiLSTM, BERT and a baseline hand designed-feature extractor to generate word representations, followed by Conditional Random Fields (CRF) to output token labels. Their BiLISTM did not have any pretraining, unlike BERT, and combined the source and target vectors using a global attention mechanism. Their submitted runs combining the baseline features with the BiLSTM and with BERT. 5.2 ETRI ETRI participated in Task 1 only. They pretrained bilingual BERT (Devlin et al., 2019) models (one for EN-RU and another for EN-DE), and then finetuned them to predict all the outputs for each language pair, using different output weight matrices for each subtask (predicting source tags, target word tags, target gap tags, and the HTER score). Training the same model for both subtasks effectively enhanced the amount of training data. 5.3 CMU CMU participated only in the sentence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word"
W19-5401,N13-1073,0,0.0598994,"ing and another in the end of the sentence. Words correctly aligned with the source are tagged as OK, and BAD otherwise. If one or more words are missing in the translation, the gap where they should have been is tagged as BAD, and OK otherwise. As in previous years, in order to obtain word level labels, first both the machine translated sentence and the source sentence are aligned with the post-edited version. Machine translation and post-edited pairs are aligned using the TERCOM tool (https://github. com/jhclark/tercom);2 source and postedited use the IBM Model 2 alignments from fast align (Dyer et al., 2013). Target word and gap labels Target tokens originating from insertion or substitution errors were labeled as BAD (i.e., tokens absent in the postedit sentence), and all other tokens were labeled as OK. Similarly to last year, we interleave these target word labels with gap labels: gaps were labeled as BAD in the presence of one or more deletion errors (i.e., a word from the source missing in the translation) and OK otherwise. Source word labels For each token in the postedited sentence deleted or substituted in the machine translated text, the corresponding aligned 1 This is true for tasks 1 a"
W19-5401,C18-1266,0,0.0919623,"ion task data, and to train the estimator, the WMT 2019 sentence level QE task data. 5.9 DCU DCU submitted two unsupervised metrics to task 3, both based on the IBM1 word alignment model. The main idea is to align the source and hypothesis using a model trained on a parallel corpus, and then use the average alignment strength (average word pair probabilities) as the metric. The varieties and other details are described in (Popovi´c et al., 2011). 5.10 USFD The two Sheffield submissions to the task 3 are based on the BiRNN sentence-level QE model from the deepQuest toolkit for neural-based QE (Ive et al., 2018). The BiRNN model uses two bi-directional recurrent neural networks (RNNs) as encoders to learn the representation of a ¡source,translation¿ sentence pair. The two encoders are trained independently from each other, before being combined as the weighted sum of the two sentence representations, using an attention mechanism. The first variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT"
W19-5401,P19-3020,1,0.848097,"Missing"
W19-5401,W19-5358,0,0.0834842,"t variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT News task from 2011 to 2017, with sent-BLEU as a quality label. We only considered the best performing submission, as well as one of the worst performing one. The model is then adapted to the downstream task of predicting DA score, using a transfer learning and fine-tuning approach. 5.11 NRC-CNRC The submissions from NRC-CNRC (kiu Lo, 2019) included two metrics submitted to task 3. They constitute a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as"
W19-5401,P15-4020,0,0.123226,"ps inside an error annotation are given BAD tags, and all others are given OK. Then, we train the same wordlevel estimator as in the baseline for Task 1. At test time, for the fine-grained subtask, we group consecutive BAD tags produced by the word-level baseline in a single error annotation and always give it severity major (the most common in the training data). As such, the baseline only produces error annotations with a single error span. For the MQM score, we consider the ratio of bad tags to the document size: nbad (3) n This simple baseline contrasts with last year, which used QuEst++ (Specia et al., 2015), a QE tool based on training an SVR on features extracted from the data. We found that the new baseline performed better than QuEst++ on the development data, and thus adopted it as the official baseline. MQM = 1 − have an HTER of 0, this is not always the case. When preprocessing the shared task data, word-level tags were determined in a case-sensitive fashion, while sentence-level scores were not. The same issue also happened last year, but unfortunately we only noticed it after releasing the training data for this edition. 4.4 QE as a Metric The QE as a metric task included two baselines,"
W19-5401,W19-5410,1,0.843237,"Missing"
W19-5401,C00-2137,0,0.130863,"or languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as winning systems the ones which had significantly better scores than all the rest with p &lt; 0.05. For the word-level task, we used randomization tests (Yeh, 2000) with Bonferroni correction6 (Abdi, 2007); for Pearson correlation scores used in the sentence-level and MQM scoring tasks, we used William’s test7 . In the word-level task, there is a big gap between Unbabel’s winning submission and ETRI’s, which in turn also had significantly better results than MIPT and BOUN. Unfortunately, we cannot do a direct comparison with last year’s results, since i) we now evaluate a single score for target words and gaps, which were evaluated separately before, and ii) only two systems submitted results for source words last year. The newly proposed metric, MCC, is"
W19-5410,W18-6450,0,0.0884367,"Missing"
W19-5410,W15-3001,0,0.070011,"Missing"
W19-5410,P02-1040,0,0.104478,"second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality). 1 Introduction Quality estimation (Blatz et al., 2004; Specia et al., 2009) aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Most approaches to quality estimation are trained to predict the post-editing effort, i.e. the number of corrections the translators have to make in order to get an adequate translation. The effort is measured by the HTER metric (Snover et al., 2006) applied to human post-edits. In this paper, we introduce a light-weight neural method with pre-trained embeddings, that means it does not require any pre-training. The second proposed method is the extension of the first one: besides pre-trained embeddings, it takes log probability from any MT system as an input feat"
W19-5410,W17-4770,0,0.103558,"Missing"
W19-5410,2006.amta-papers.25,0,0.160935,"s an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality). 1 Introduction Quality estimation (Blatz et al., 2004; Specia et al., 2009) aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Most approaches to quality estimation are trained to predict the post-editing effort, i.e. the number of corrections the translators have to make in order to get an adequate translation. The effort is measured by the HTER metric (Snover et al., 2006) applied to human post-edits. In this paper, we introduce a light-weight neural method with pre-trained embeddings, that means it does not require any pre-training. The second proposed method is the extension of the first one: besides pre-trained embeddings, it takes log probability from any MT system as an input feature. In addition to the offic"
W19-5410,W17-4755,0,0.101281,"Missing"
W19-5410,W18-6451,0,0.0405361,"Missing"
W19-5410,W16-2302,0,0.0428043,"Missing"
W19-5410,2009.eamt-1.5,0,0.0374282,"vel quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality). 1 Introduction Quality estimation (Blatz et al., 2004; Specia et al., 2009) aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Most approaches to quality estimation are trained to predict the post-editing effort, i.e. the number of corrections the translators have to make in order to get an adequate translation. The effort is measured by the HTER metric (Snover et al., 2006) applied to human post-edits. In this paper, we introduce a light-weight neural method with pre-trained embeddings, that means it does not req"
W19-5410,W15-3031,0,0.0369493,"Missing"
W19-5410,W19-5401,1,0.844062,"lts for DAseg-newstest2016 The both proposed methods are supervised, so to train models we need labels. As DA data is scarce resource we trained models using chrF++ (Popovi´c, 2017) (with default hyper-parameters) as labels. To investigate how the number of language pairs affects the performance of models, we trained several models: with one language pair in the training Data and Results for human assessment prediction Data We took data from News Translation Tasks 2015-2018 years (Bojar et al., 2015, 2016a, 1 http://statmt.org/wmt18/ quality-estimation-task.html#results 103 will be available (Fonseca et al., 2019). Pearson correlation coefficient set, with four (De-En, En-De, En-Cs, En-Ru) and with seven language pairs. As can be seen in the Figure 2, the best results were achieved with the mono language pair models, although the difference between mono- and multimodels is not large. We also fine-tuned our models by using human assessment data. Fine-tuned models showed a little bit better results compared to the non-tuned models (Figure: 2). We compared the obtained results to the metrics results. For De-En the best resulting Pearson correlation coefficient for metrics is 0.601 and for EnRu is 0.666 (B"
W19-5410,C18-1266,0,0.0522484,"atasets contain outputs only from statistical MT. However, for our method there is no difference between neural and statistical MT output. En-De and En-Cs sentences on the IT domain and De-En — on the pharmaceutical domain. We removed duplicated sentences and randomly split data into training, dev and test sets in the 70/20/10 ratio. As a result, we got the following number of sentences: Experimental Settings In this section we analyze the performance of proposed methods on different prediction outputs (HTER and DA) and different datasets and compare them with another neural method DeepQuest (Ive et al., 2018) that does not require additional data. To predict HTER we take a dataset that contains source sentences, their translated outputs and HTER scores. It is domain-specific: IT or pharmaceutical depending on the language pair. As there is no large enough corpus with DA labels, we use a dataset that consists only of source sentences and their machine translation output. The domain of this corpus is more general and source sentences have taken from the open resources. 3.1 Data and Results of HTER Prediction • En-De: ≈ 55K/16K/8K • De-En: ≈ 37K/10K/5K Experiments • En-Cs: ≈ 29K/8K/4K We have impleme"
W19-5410,W17-4763,0,0.0342677,"second proposed method is the extension of the first one: besides pre-trained embeddings, it takes log probability from any MT system as an input feature. In addition to the official datasets provided for this year’s WMT sentence level shared task, we analyze the performance of our methods against the extended datasets made from previous years data. Using the extended datasets allows to get a more reliable score and avoid skewed distributions of the predicted metrics. 2 Architecture Our method performs sentence-level quality estimation of machine translation. As other stateof-the-art methods (Kim et al., 2017; Fan et al., 2018), we use a neural-based architecture. However, compared to the other neural-based methods, we do not train embeddings from scratch, that usually takes a lot of data and computational resources. Instead of that, we use already well trained and freely available embeddings. For our method we have picked BERT (Devlin et al., 2018) and LASER (Artetxe and Schwenk, 2018) multilingual embeddings toolkits. We extract both BERT and LASER embeddings and feed them into a feed-forward neural network. A sigmoid output layer produces the desirable score. In case of HTER prediction we can a"
Y10-1043,J93-2003,0,0.0535473,"-the-art statistical machine translation (SMT) systems operate with multiword units but still use word alignment as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of (Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systems – to extract the synchronous grammar rules. The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model (Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known implementation of the models, GIZA++, is derived from (Och and Ney, 2003) and involves training the following models in sequence: IBM model 1, HMM-based model, IBM model 3 and IBM model 4. Although Och and Ney (2003) mention briefly that “improved alignment quality yields an improved subjective quality of the statistical machine trans"
Y10-1043,P05-1033,0,0.0611169,"nally less expensive and faster algorithms to train and align new sentence pairs. Empirical evaluation is performed on a phrase-based and a parsing-based translation system. Keywords: Word alignment, machine translation, relative reordering 1 Introduction A majority of state-of-the-art statistical machine translation (SMT) systems operate with multiword units but still use word alignment as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of (Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systems – to extract the synchronous grammar rules. The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model (Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known implementation of the models, GIZA++, is derived fro"
Y10-1043,P07-1003,0,0.0160184,"model, IBM model 3 and IBM model 4. Although Och and Ney (2003) mention briefly that “improved alignment quality yields an improved subjective quality of the statistical machine translation system as well”, a number of recent studies suggest otherwise – namely, that the correlation between the alignment quality and the quality of the resulting translation is rather weak (see the following section on related work). This suggests that the best default word alignment models are not necessarily optimal in terms of the resulting translation quality. Some recent works (e.g. (Liang et al., 2006) or (DeNero and Klein, 2007)) are already based on the sequential HMM word alignment model, rather than the fertility-based models 3 or 4 . The former is computationally less complex and at the same time still includes the essential parts of the word alignment (i.e. lexical correspondence and changes in word order). Copyright 2010 by Mark Fishel 381 382 Poster Papers In this work we show that a simpler alignment model introduced together with HMM-based alignment in (Vogel et al., 1996), but discarded due to worse alignment error rate, results in essentially the same translation quality like HMM-based alignment in almost"
Y10-1043,J07-3002,0,0.0205757,"ing section we review the related work on the link between the word alignment quality and translation quality. Section 3 consists of a theoretical overview of different aspects of the word alignment task in the models in question. In section 4 we present the experiments with simpler default and alternative models on translations from Chinese, Czech, Estonian, Finnish, German and Korean into English and back. 2 Related Work Several papers point out that the scores designed for word alignment (alignment error-rate, Fscore) and translation (BLEU, NIST), are not heavily correlated. In particular (Fraser and Marcu, 2007) and (Ayan and Dorr, 2006) distinctly state that alignment error rate is a poor indicator of translation quality. (Lopez and Resnik, 2006) artificially degrade the alignment quality in order to show that it does not cause a significant drop in translation quality. They further show that with careful feature engineering the flaws of the underlying word alignment can be compensated. (Vilar et al., 2006) give two examples of word alignment modifications which cause worse alignment quality and nevertheless better translation quality. This is achieved by adapting the alignments to the specific requ"
Y10-1043,P08-1112,0,0.0463905,"Missing"
Y10-1043,2009.mtsummit-papers.5,0,0.0113467,"distinctly state that alignment error rate is a poor indicator of translation quality. (Lopez and Resnik, 2006) artificially degrade the alignment quality in order to show that it does not cause a significant drop in translation quality. They further show that with careful feature engineering the flaws of the underlying word alignment can be compensated. (Vilar et al., 2006) give two examples of word alignment modifications which cause worse alignment quality and nevertheless better translation quality. This is achieved by adapting the alignments to the specific requirements of translation. (Guzman et al., 2009) inspect word alignments and their characteristics, especially the number of unaligned words, and their influence on phrase pair extraction. They show that an increased number of unaligned words causes degraded translation quality. Analyzing manually evaluated phrase pairs they come up with translation model features that account for the number of unaligned words and improve the translation quality. (Lambert et al., 2009) tune alignment for the F-score and the BLEU score. They show that the two objectives are not the same and produce different translation models. (Ganchev et al., 2008) use agr"
Y10-1043,W07-0711,0,0.0378938,"Missing"
Y10-1043,2005.mtsummit-papers.11,0,0.00629634,"eady included in GIZA++ (abbreviated as IBM2), the relative reordering-based model (abbreviated as IBM2(r)) and the latter, augmented with lexicalization, as suggested by (Och and Ney, 2000) (abbreviated as IBM2(r-l)). We performed all of the experiments on the following language pairs and corpora: the ChineseEnglish and Korean-English parts of the OPUS KDE4 corpus (Tiedemann, 2009), Czech-English ˇ technical documentations from CzEng (Bojar and Zabokrtsk´ y, 2009), the Estonian-English part of the JRC-Acquis (Steinberger et al., 2006) and Finnish-English and German-English parts of Europarl (Koehn, 2005); all experiments included both translation directions. Two independent held-out sets, each 2500 sentence pairs, were reserved for minimum error-rate training and validation; the resulting sizes of the training parts after preprocessing and separating the dev and test sets are summarized in table 1. 4.2 Results We used the BLEU (Papieni et al., 2001) and NIST (NIST, 2002) scores for evaluation and paired bootstrap resampling (Riezler and Maxwell, 2005) for significance testing. The results of the first part of the experiments, which is stopping midway in training the word alignment models, are"
Y10-1043,P07-2045,0,0.00551489,"t an earlier stage of word alignment influences translation quality. The hypothesis here, dictated by common knowledge, is that HMM-based alignment can perform just as well or even better than IBM model 4. As a next step we compare the default alignment model training sequence to an alternative, which uses different variants of IBM model 2 as a final step. Our aim is to see whether some variant can match the performance of the HMM-based model and IBM model 4. 4.1 Experiment Setup We evaluate the influence of word alignment on two translation systems – a phrase-based system trained with Moses (Koehn et al., 2007) and a hierarchical phrase-based system trained with Joshua (Li et al., 2009). In both cases we used 5-gram language models from SRI LM (Stolcke, 2002) and minimum error rate training included in the toolkits. Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems. We modified its implementation to support three kinds of IBM2-based models: the absolute reordering-based model already included in GIZA++ (abbreviated as IBM2), the relative reordering-based model (abbreviated as IBM2(r)) and the latter, augmented with lexicalization, as suggested by (Och and Ney, 2000) (abbreviat"
Y10-1043,N03-1017,0,0.102212,"rformance of the HMM-based model, whereas using computationally less expensive and faster algorithms to train and align new sentence pairs. Empirical evaluation is performed on a phrase-based and a parsing-based translation system. Keywords: Word alignment, machine translation, relative reordering 1 Introduction A majority of state-of-the-art statistical machine translation (SMT) systems operate with multiword units but still use word alignment as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of (Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systems – to extract the synchronous grammar rules. The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model (Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known"
Y10-1043,2009.mtsummit-posters.12,0,0.0136523,"ions which cause worse alignment quality and nevertheless better translation quality. This is achieved by adapting the alignments to the specific requirements of translation. (Guzman et al., 2009) inspect word alignments and their characteristics, especially the number of unaligned words, and their influence on phrase pair extraction. They show that an increased number of unaligned words causes degraded translation quality. Analyzing manually evaluated phrase pairs they come up with translation model features that account for the number of unaligned words and improve the translation quality. (Lambert et al., 2009) tune alignment for the F-score and the BLEU score. They show that the two objectives are not the same and produce different translation models. (Ganchev et al., 2008) use agreement-driven training of alignment models and replace Viterbi decoding with posterior decoding. This results in improvements both in the alignment quality as well as translation quality. A brief comparison of the IBM models in SMT context is performed in (Koehn et al., 2003). The comparison is based on the BLEU scores and covers IBM models 1 to 4. The given brief conclusions are that using different alignment models does"
Y10-1043,W09-0424,0,0.0201311,"sis here, dictated by common knowledge, is that HMM-based alignment can perform just as well or even better than IBM model 4. As a next step we compare the default alignment model training sequence to an alternative, which uses different variants of IBM model 2 as a final step. Our aim is to see whether some variant can match the performance of the HMM-based model and IBM model 4. 4.1 Experiment Setup We evaluate the influence of word alignment on two translation systems – a phrase-based system trained with Moses (Koehn et al., 2007) and a hierarchical phrase-based system trained with Joshua (Li et al., 2009). In both cases we used 5-gram language models from SRI LM (Stolcke, 2002) and minimum error rate training included in the toolkits. Word alignment was done with GIZA++ (Och and Ney, 2003) for both systems. We modified its implementation to support three kinds of IBM2-based models: the absolute reordering-based model already included in GIZA++ (abbreviated as IBM2), the relative reordering-based model (abbreviated as IBM2(r)) and the latter, augmented with lexicalization, as suggested by (Och and Ney, 2000) (abbreviated as IBM2(r-l)). We performed all of the experiments on the following langua"
Y10-1043,N06-1014,0,0.0286208,"IBM model 1, HMM-based model, IBM model 3 and IBM model 4. Although Och and Ney (2003) mention briefly that “improved alignment quality yields an improved subjective quality of the statistical machine translation system as well”, a number of recent studies suggest otherwise – namely, that the correlation between the alignment quality and the quality of the resulting translation is rather weak (see the following section on related work). This suggests that the best default word alignment models are not necessarily optimal in terms of the resulting translation quality. Some recent works (e.g. (Liang et al., 2006) or (DeNero and Klein, 2007)) are already based on the sequential HMM word alignment model, rather than the fertility-based models 3 or 4 . The former is computationally less complex and at the same time still includes the essential parts of the word alignment (i.e. lexical correspondence and changes in word order). Copyright 2010 by Mark Fishel 381 382 Poster Papers In this work we show that a simpler alignment model introduced together with HMM-based alignment in (Vogel et al., 1996), but discarded due to worse alignment error rate, results in essentially the same translation quality like HM"
Y10-1043,2006.amta-papers.11,0,0.0186754,"oretical overview of different aspects of the word alignment task in the models in question. In section 4 we present the experiments with simpler default and alternative models on translations from Chinese, Czech, Estonian, Finnish, German and Korean into English and back. 2 Related Work Several papers point out that the scores designed for word alignment (alignment error-rate, Fscore) and translation (BLEU, NIST), are not heavily correlated. In particular (Fraser and Marcu, 2007) and (Ayan and Dorr, 2006) distinctly state that alignment error rate is a poor indicator of translation quality. (Lopez and Resnik, 2006) artificially degrade the alignment quality in order to show that it does not cause a significant drop in translation quality. They further show that with careful feature engineering the flaws of the underlying word alignment can be compensated. (Vilar et al., 2006) give two examples of word alignment modifications which cause worse alignment quality and nevertheless better translation quality. This is achieved by adapting the alignments to the specific requirements of translation. (Guzman et al., 2009) inspect word alignments and their characteristics, especially the number of unaligned words"
Y10-1043,J03-1002,0,0.0420147,"nt as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of (Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systems – to extract the synchronous grammar rules. The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model (Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known implementation of the models, GIZA++, is derived from (Och and Ney, 2003) and involves training the following models in sequence: IBM model 1, HMM-based model, IBM model 3 and IBM model 4. Although Och and Ney (2003) mention briefly that “improved alignment quality yields an improved subjective quality of the statistical machine translation system as well”, a number of recent studies suggest otherwise – namely, that the correlation between t"
Y10-1043,C00-2163,0,0.209916,"ord in one sentence to at most one word in the other one. For simplicity’s sake we will use the standard notation of f and e for the two sentences. The aim is therefore to find an alignment a, which is a vector of indexes indicating which words in e the words in f are aligned to; in other words, the word fj is aligned to the word ei if aj = i. Also any aj can be equal to 0, in which case the word fj is said to be unaligned. Here we will focus on the IBM models (Brown et al., 1993), the HMM-based alignment model (Vogel et al., 1996) and a modification of the original IBM model 2 introduced in (Och and Ney, 2000) and referred to as diagonal-oriented model 2. 3.1 Lexical Correspondence Lexical (or translational) correspondence of the single words in the two sentences is perhaps the main aspect of word alignment and is present in all of the described models. In all the models considered here lexical correspondence is treated as independent of the word positions in the sentences or any context of either words; it is modeled via a probability distribution p(f |e). Although lexical correspondence is a very important aspect, constricting an alignment model to it (as it is the case with model 1) results in s"
Y10-1043,2001.mtsummit-papers.68,0,0.0340452,"of the OPUS KDE4 corpus (Tiedemann, 2009), Czech-English ˇ technical documentations from CzEng (Bojar and Zabokrtsk´ y, 2009), the Estonian-English part of the JRC-Acquis (Steinberger et al., 2006) and Finnish-English and German-English parts of Europarl (Koehn, 2005); all experiments included both translation directions. Two independent held-out sets, each 2500 sentence pairs, were reserved for minimum error-rate training and validation; the resulting sizes of the training parts after preprocessing and separating the dev and test sets are summarized in table 1. 4.2 Results We used the BLEU (Papieni et al., 2001) and NIST (NIST, 2002) scores for evaluation and paired bootstrap resampling (Riezler and Maxwell, 2005) for significance testing. The results of the first part of the experiments, which is stopping midway in training the word alignment models, are presented on table 1. The scores of translations based on IBM model 1 are PACLIC 24 Proceedings noticeably lower than all other models; also after the HMM-based model there is a noticeable drop at the IBM model 3 for almost every language pair, after which the IBM4 model scores rise to the level of the HMM model again. Significance testing reveals t"
Y10-1043,W05-0908,0,0.0215765,"jar and Zabokrtsk´ y, 2009), the Estonian-English part of the JRC-Acquis (Steinberger et al., 2006) and Finnish-English and German-English parts of Europarl (Koehn, 2005); all experiments included both translation directions. Two independent held-out sets, each 2500 sentence pairs, were reserved for minimum error-rate training and validation; the resulting sizes of the training parts after preprocessing and separating the dev and test sets are summarized in table 1. 4.2 Results We used the BLEU (Papieni et al., 2001) and NIST (NIST, 2002) scores for evaluation and paired bootstrap resampling (Riezler and Maxwell, 2005) for significance testing. The results of the first part of the experiments, which is stopping midway in training the word alignment models, are presented on table 1. The scores of translations based on IBM model 1 are PACLIC 24 Proceedings noticeably lower than all other models; also after the HMM-based model there is a noticeable drop at the IBM model 3 for almost every language pair, after which the IBM4 model scores rise to the level of the HMM model again. Significance testing reveals that only in case of Korean-English translation the NIST score of the HMM model is significantly lower (p"
Y10-1043,2006.iwslt-papers.7,0,0.0168922,"ck. 2 Related Work Several papers point out that the scores designed for word alignment (alignment error-rate, Fscore) and translation (BLEU, NIST), are not heavily correlated. In particular (Fraser and Marcu, 2007) and (Ayan and Dorr, 2006) distinctly state that alignment error rate is a poor indicator of translation quality. (Lopez and Resnik, 2006) artificially degrade the alignment quality in order to show that it does not cause a significant drop in translation quality. They further show that with careful feature engineering the flaws of the underlying word alignment can be compensated. (Vilar et al., 2006) give two examples of word alignment modifications which cause worse alignment quality and nevertheless better translation quality. This is achieved by adapting the alignments to the specific requirements of translation. (Guzman et al., 2009) inspect word alignments and their characteristics, especially the number of unaligned words, and their influence on phrase pair extraction. They show that an increased number of unaligned words causes degraded translation quality. Analyzing manually evaluated phrase pairs they come up with translation model features that account for the number of unaligne"
Y10-1043,C96-2141,0,0.743305,"operate with multiword units but still use word alignment as an intermediate step for learning the translation models. Such is the case for two wide-spread machine translation frameworks: phrase-based SMT of (Koehn et al., 2003) and hierarchical phrase-based SMT of (Chiang, 2005). In phrase-based systems word alignment is used to construct phrase tables and in hierarchical phrase-based systems – to extract the synchronous grammar rules. The word alignment models that are currently considered as default are the so-called IBM models 1 to 5 (Brown et al., 1993) and the HMM-based alignment model (Vogel et al., 1996). The main work evaluating them is (Och and Ney, 2003) where they are compared in the context of word alignment only (i.e. based on the alignment error rate). Specifically, the default setup of the well known implementation of the models, GIZA++, is derived from (Och and Ney, 2003) and involves training the following models in sequence: IBM model 1, HMM-based model, IBM model 3 and IBM model 4. Although Och and Ney (2003) mention briefly that “improved alignment quality yields an improved subjective quality of the statistical machine translation system as well”, a number of recent studies sugg"
Y10-1043,steinberger-etal-2006-jrc,0,\N,Missing
Y10-1043,P02-1040,0,\N,Missing
Y10-1043,P06-1002,0,\N,Missing
