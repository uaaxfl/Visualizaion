2021.inlg-1.11,"Generating Racing Game Commentary from Vision, Language, and Structured Data",2021,-1,-1,6,1,5924,tatsuya ishigaki,Proceedings of the 14th International Conference on Natural Language Generation,0,"We propose the task of automatically generating commentaries for races in a motor racing game, from vision, structured numerical, and textual data. Commentaries provide information to support spectators in understanding events in races. Commentary generation models need to interpret the race situation and generate the correct content at the right moment. We divide the task into two subtasks: utterance timing identification and utterance generation. Because existing datasets do not have such alignments of data in multiple modalities, this setting has not been explored in depth. In this study, we introduce a new large-scale dataset that contains aligned video data, structured numerical data, and transcribed commentaries that consist of 129,226 utterances in 1,389 races in a game. Our analysis reveals that the characteristics of commentaries change over time or from viewpoints. Our experiments on the subtasks show that it is still challenging for a state-of-the-art vision encoder to capture useful information from videos to generate accurate commentaries. We make the dataset and baseline implementation publicly available for further research."
2021.argmining-1.11,{B}ayesian Argumentation-Scheme Networks: {A} Probabilistic Model of Argument Validity Facilitated by Argumentation Schemes,2021,-1,-1,4,0,12294,takahiro kondo,Proceedings of the 8th Workshop on Argument Mining,0,"We propose a methodology for representing the reasoning structure of arguments using Bayesian networks and predicate logic facilitated by argumentation schemes. We express the meaning of text segments using predicate logic and map the boolean values of predicate logic expressions to nodes in a Bayesian network. The reasoning structure among text segments is described with a directed acyclic graph. While our formalism is highly expressive and capable of describing the informal logic of human arguments, it is too open-ended to actually build a network for an argument. It is not at all obvious which segment of argumentative text should be considered as a node in a Bayesian network, and how to decide the dependencies among nodes. To alleviate the difficulty, we provide abstract network fragments, called idioms, which represent typical argument justification patterns derived from argumentation schemes. The network construction process is decomposed into idiom selection, idiom instantiation, and idiom combination. We define 17 idioms in total by referring to argumentation schemes as well as analyzing actual arguments and fitting idioms to them. We also create a dataset consisting of pairs of an argumentative text and a corresponding Bayesian network. Our dataset contains about 2,400 pairs, which is large in the research area of argumentation schemes."
2021.alvr-1.3,Leveraging Partial Dependency Trees to Control Image Captions,2021,-1,-1,2,0,12361,wenjie zhong,Proceedings of the Second Workshop on Advances in Language and Vision Research,0,"Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a metric to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard datasets: Microsoft COCO and Flickr30k. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures.The code is available on GitHub."
2020.signlang-1.3,Utterance-Unit Annotation for the {JSL} Dialogue Corpus: Toward a Multimodal Approach to Corpus Linguistics,2020,-1,-1,4,0,14790,mayumi bono,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives",0,"This paper describes a method for annotating the Japanese Sign Language (JSL) dialogue corpus. We developed a way to identify interactional boundaries and define a {`}utterance unit{'} in sign language using various multimodal features accompanying signing. The utterance unit is an original concept for segmenting and annotating sign language dialogue referring to signer{'}s native sense from the perspectives of Conversation Analysis (CA) and Interaction Studies. First of all, we postulated that we should identify a fundamental concept of interaction-specific unit for understanding interactional mechanisms, such as turn-taking (Sacks et al. 1974), in sign-language social interactions. Obviously, it does should not relying on a spoken language writing system for storing signings in corpora and making translations. We believe that there are two kinds of possible applications for utterance units: one is to develop corpus linguistics research for both signed and spoken corpora; the other is to build an informatics system that includes, but is not limited to, a machine translation system for sign languages."
2020.sdp-1.16,Towards Grounding of Formulae,2020,-1,-1,4,0,15434,takuto asakura,Proceedings of the First Workshop on Scholarly Document Processing,0,"A large amount of scientific knowledge is represented within mixed forms of natural language texts and mathematical formulae. Therefore, a collaboration of natural language processing and formula analyses, so-called mathematical language processing, is necessary to enable computers to understand and retrieve information from the documents. However, as we will show in this project, a mathematical notation can change its meaning even within the scope of a single paragraph. This flexibility makes it difficult to extract the exact meaning of a mathematical formula. In this project, we will propose a new task direction for grounding mathematical formulae. Particularly, we are addressing the widespread misconception of various research projects in mathematical information retrieval, which presume that mathematical notations have a fixed meaning within a single document. We manually annotated a long scientific paper to illustrate the task concept. Our high inter-annotator agreement shows that the task is well understood for humans. Our results indicate that it is worthwhile to grow the techniques for the proposed task to contribute to the further progress of mathematical language processing."
2020.rail-1.5,Comparing Neural Network Parsers for a Less-resourced and Morphologically-rich Language: {A}mharic Dependency Parser,2020,-1,-1,2,1,14002,binyam seyoum,Proceedings of the first workshop on Resources for African Indigenous Languages,0,"In this paper, we compare four state-of-the-art neural network dependency parsers for the Semitic language Amharic. As Amharic is a morphologically-rich and less-resourced language, the out-of-vocabulary (OOV) problem will be higher when we develop data-driven models. This fact limits researchers to develop neural network parsers because the neural network requires large quantities of data to train a model. We empirically evaluate neural network parsers when a small Amharic treebank is used for training. Based on our experiment, we obtain an 83.79 LAS score using the UDPipe system. Better accuracy is achieved when the neural parsing system uses external resources like word embedding. Using such resources, the LAS score for UDPipe improves to 85.26. Our experiment shows that the neural networks can learn dependency relations better from limited data while segmentation and POS tagging require much data."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,16,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.225,Analyzing Word Embedding Through Structural Equation Modeling,2020,-1,-1,3,0,17066,namgi han,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Many researchers have tried to predict the accuracies of extrinsic evaluation by using intrinsic evaluation to evaluate word embedding. The relationship between intrinsic and extrinsic evaluation, however, has only been studied with simple correlation analysis, which has difficulty capturing complex cause-effect relationships and integrating external factors such as the hyperparameters of word embedding. To tackle this problem, we employ partial least squares path modeling (PLS-PM), a method of structural equation modeling developed for causal analysis. We propose a causal diagram consisting of the evaluation results on the BATS, VecEval, and SentEval datasets, with a causal hypothesis that linguistic knowledge encoded in word embedding contributes to solving downstream tasks. Our PLS-PM models are estimated with 600 word embeddings, and we prove the existence of causal relations between linguistic knowledge evaluated on BATS and the accuracies of downstream tasks evaluated on VecEval and SentEval in our PLS-PM models. Moreover, we show that the PLS-PM models are useful for analyzing the effect of hyperparameters, including the training algorithm, corpus, dimension, and context window, and for validating the effectiveness of intrinsic evaluation."
2020.inlg-1.21,Market Comment Generation from Data with Noisy Alignments,2020,-1,-1,4,0,5926,yumi hamazono,Proceedings of the 13th International Conference on Natural Language Generation,0,"End-to-end models on data-to-text learn the mapping of data and text from the aligned pairs in the dataset. However, these alignments are not always obtained reliably, especially for the time-series data, for which real time comments are given to some situation and there might be a delay in the comment delivery time compared to the actual event time. To handle this issue of possible noisy alignments in the dataset, we propose a neural network model with multi-timestep data and a copy mechanism, which allows the models to learn the correspondences between data and text from the dataset with noisier alignments. We focus on generating market comments in Japanese that are delivered each time an event occurs in the market. The core idea of our approach is to utilize multi-timestep data, which is not only the latest market price data when the comment is delivered, but also the data obtained at several timesteps earlier. On top of this, we employ a copy mechanism that is suitable for referring to the content of data records in the market price data. We confirm the superiority of our proposal by two evaluation metrics and show the accuracy improvement of the sentence generation using the time series data by our proposed method."
2020.coling-main.213,Learning with Contrastive Examples for Data-to-Text Generation,2020,-1,-1,8,0,19017,yui uehara,Proceedings of the 28th International Conference on Computational Linguistics,0,"Existing models for data-to-text tasks generate fluent but sometimes incorrect sentences e.g., {``}Nikkei gains{''} is generated when {``}Nikkei drops{''} is expected. We investigate models trained on contrastive examples i.e., incorrect sentences or terms, in addition to correct ones to reduce such errors. We first create rules to produce contrastive examples from correct ones by replacing frequent crucial terms such as {``}gain{''} or {``}drop{''}. We then use learning methods with several losses that exploit contrastive examples. Experiments on the market comment generation task show that 1) exploiting contrastive examples improves the capability of generating sentences with better lexical choice, without degrading the fluency, 2) the choice of the loss function is an important factor because the performances on different metrics depend on the types of loss functions, and 3) the use of the examples produced by some specific rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples."
2020.coling-main.465,An empirical analysis of existing systems and datasets toward general simple question answering,2020,-1,-1,5,0,17066,namgi han,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we evaluate the progress of our field toward solving simple factoid questions over a knowledge base, a practically important problem in natural language interface to database. As in other natural language understanding tasks, a common practice for this task is to train and evaluate a model on a single dataset, and recent studies suggest that SimpleQuestions, the most popular and largest dataset, is nearly solved under this setting. However, this common setting does not evaluate the robustness of the systems outside of the distribution of the used training data. We rigorously evaluate such robustness of existing systems using different datasets. Our analysis, including shifting of training and test datasets and training on a union of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal."
W19-8640,Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels,2019,0,0,9,0,21311,kasumi aoki,Proceedings of the 12th International Conference on Natural Language Generation,0,"We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However, because depending on users, it differs what they are interested in, so it is necessary to develop a method to generate various summaries according to users{'} interests. We develop a model to generate various summaries and to control their contents by providing the explicit targets for a reference to the model as controllable factors. In the experiments, we used five-minute or one-hour charts of 9 indicators (e.g., Nikkei225), as time-series data, and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation."
P19-1202,"Learning to Select, Track, and Generate for Data-to-Text",2019,0,1,7,0,7220,hayate iso,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization."
N19-1129,Does My Rebuttal Matter? Insights from a Major {NLP} Conference,2019,0,0,5,0,7649,yang gao,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP. However, only few studies have evaluated its properties empirically. Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018. We quantitatively and qualitatively assess the corpus. This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses. We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a reviewer{'}s final score is largely determined by her initial score and the distance to the other reviewers{'} initial scores. In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research. We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences."
W18-6515,Generating Market Comments Referring to External Resources,2018,0,3,8,0,21286,tatsuya aoki,Proceedings of the 11th International Conference on Natural Language Generation,0,"Comments on a stock market often include the reason or cause of changes in stock prices, such as {``}Nikkei turns lower as yen{'}s rise hits exporters.{''} Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a model for automatically generating such informative market comments that refer to external resources. We evaluated our model through an automatic metric in terms of BLEU and human evaluation done by an expert in finance. The results show that our model outperforms the existing model both in BLEU scores and human judgment."
W18-6009,Coordinate Structures in {U}niversal {D}ependencies for Head-final Languages,2018,0,0,5,0.324659,1260,hiroshi kanayama,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept in head-final languages. This paper describes the status in the current Japanese and Korean corpora and proposes alternative designs suitable for these languages."
N18-1166,Inducing Temporal Relations from Time Anchor Annotation,2018,0,0,2,1,1612,fei cheng,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Recognizing temporal relations among events and time expressions has been an essential but challenging task in natural language processing. Conventional annotation of judging temporal relations puts a heavy load on annotators. In reality, the existing annotated corpora include annotations on only {``}salient{''} event pairs, or on pairs in a fixed window of sentences. In this paper, we propose a new approach to obtain temporal relations from absolute time value (a.k.a. time anchors), which is suitable for texts containing rich temporal information such as news articles. We start from time anchors for events and time expressions, and temporal relation annotations are induced automatically by computing relative order of two time anchors. This proposal shows several advantages over the current methods for temporal relation annotation: it requires less annotation effort, can induce inter-sentence relations easily, and increases informativeness of temporal relations. We compare the empirical statistics and automatic recognition results with our data against a previous temporal relation corpus. We also reveal that our data contributes to a significant improvement of the downstream time anchor prediction task, demonstrating 14.1 point increase in overall accuracy."
L18-1287,{U}niversal {D}ependencies Version 2 for {J}apanese,2018,0,1,4,0,13282,masayuki asahara,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1350,{U}niversal {D}ependencies for {A}mharic,2018,0,0,2,1,14002,binyam seyoum,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1260,An Empirical Investigation of Error Types in {V}ietnamese Parsing,2018,0,0,2,1,30900,quy nguyen,Proceedings of the 27th International Conference on Computational Linguistics,0,"Syntactic parsing plays a crucial role in improving the quality of natural language processing tasks. Although there have been several research projects on syntactic parsing in Vietnamese, the parsing quality has been far inferior than those reported in major languages, such as English and Chinese. In this work, we evaluated representative constituency parsing models on a Vietnamese Treebank to look for the most suitable parsing method for Vietnamese. We then combined the advantages of automatic and manual analysis to investigate errors produced by the experimented parsers and find the reasons for them. Our analysis focused on three possible sources of parsing errors, namely limited training data, part-of-speech (POS) tagging errors, and ambiguous constructions. As a result, we found that the last two sources, which frequently appear in Vietnamese text, significantly attributed to the poor performance of Vietnamese parsing."
W17-6929,Evaluation Metrics for Automatically Generated Metaphorical Expressions,2017,0,2,2,1,23343,akira miyazawa,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
P17-2001,Classifying Temporal Relations by Bidirectional {LSTM} over Dependency Paths,2017,6,16,2,1,1612,fei cheng,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a {``}common root{''} assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.)."
P17-1126,Learning to Generate Market Comments from Stock Prices,2017,0,13,7,0,10697,soichiro murakami,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts."
E17-1067,On-demand Injection of Lexical Knowledge for Recognising Textual Entailment,2017,17,7,3,1,9258,pascual martinezgomez,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We approach the recognition of textual entailment using logical semantic representations and a theorem prover. In this setup, lexical divergences that preserve semantic entailment between the source and target texts need to be explicitly stated. However, recognising subsentential semantic relations is not trivial. We address this problem by monitoring the proof of the theorem and detecting unprovable sub-goals that share predicate arguments with logical premises. If a linguistic relation exists, then an appropriate axiom is constructed on-demand and the theorem proving continues. Experiments show that this approach is effective and precise, producing a system that outperforms other logic-based systems and is competitive with state-of-the-art statistical methods."
W16-0109,Paraphrase for Open Question Answering: New Dataset and Methods,2016,14,1,3,1,4212,ying xu,Proceedings of the Workshop on Human-Computer Question Answering,0,"We propose a new open question answering framework for question answering over a knowledge base (KB). Our system uses both a curated KB, Freebase, and one that is extracted automatically by an open information extraction model, IE KB. Our system consists of only one layer of paraphrase, compared to the three layers used in a previous open question answering system (Fader et al., 2014). However, because of the more accurately extracted relation triples in IE KB, combined with linked entities from IE KB to Freebase, our system achieves a 7% absolute gain in F1 score over the previous system."
P16-4015,ccg2lambda: A Compositional Semantics System,2016,3,9,3,1,9258,pascual martinezgomez,Proceedings of {ACL}-2016 System Demonstrations,0,"We demonstrate a simple and easy-to-use system to produce logical semantic representations of sentences. Our software operates by composing semantic formulas bottom-up given a CCG parse tree. It uses flexible semantic templates to specify semantic patterns. Templates for English and Japanese accompany our software, and they are easy to understand, use and extend to cover other linguistic phenomena or languages. We also provide scripts to use our semantic representations in a textual entailment task, and a visualization tool to display semantically augmented CCG trees in HTML."
P16-4018,{J}igg: A Framework for an Easy Natural Language Processing Pipeline,2016,11,4,2,1,5927,hiroshi noji,Proceedings of {ACL}-2016 System Demonstrations,0,"We present Jigg, a Scala (or JVMbased) NLP annotation pipeline framework, which is easy to use and is extensible. Jigg supports a very simple interface similar to Stanford CoreNLP, the most successful NLP pipeline toolkit, but has more flexibility to adapt to new types of annotation. On this framework, system developers can easily integrate their downstream system into a NLP pipeline from a raw text by just preparing a wrapper of it."
L16-1243,Challenges and Solutions for Consistent Annotation of {V}ietnamese Treebank,2016,7,2,2,1,30900,quy nguyen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Treebanks are important resources for researchers in natural language processing, speech recognition, theoretical linguistics, etc. To strengthen the automatic processing of the Vietnamese language, a Vietnamese treebank has been built. However, the quality of this treebank is not satisfactory and is a possible source for the low performance of Vietnamese language processing. We have been building a new treebank for Vietnamese with about 40,000 sentences annotated with three layers: word segmentation, part-of-speech tagging, and bracketing. In this paper, we describe several challenges of Vietnamese language and how we solve them in developing annotation guidelines. We also present our methods to improve the quality of the annotation guidelines and ensure annotation accuracy and consistency. Experiment results show that inter-annotator agreement ratios and accuracy are higher than 90{\%} which is satisfactory."
L16-1261,{U}niversal {D}ependencies for {J}apanese,2016,2,2,2,0,29830,takaaki tanaka,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an attempt to port the international syntactic annotation scheme, Universal Dependencies, to the Japanese language in this paper. Since the Japanese syntactic structure is usually annotated on the basis of unique chunk-based dependencies, we first introduce word-based dependencies by using a word unit called the Short Unit Word, which usually corresponds to an entry in the lexicon UniDic. Porting is done by mapping the part-of-speech tagset in UniDic to the universal part-of-speech tagset, and converting a constituent-based treebank to a typed dependency tree. The conversion is not straightforward, and we discuss the problems that arose in the conversion and the current solutions. A treebank consisting of 10,000 sentences was built by converting the existent resources and currently released to the public."
L16-1607,Typed Entity and Relation Annotation on Computer Science Papers,2016,0,2,4,0,35318,yuka tateisi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe our ongoing effort to establish an annotation scheme for describing the semantic structures of research articles in the computer science domain, with the intended use of developing search systems that can refine their results by the roles of the entities denoted by the query keys. In our scheme, mentions of entities are annotated with ontology-based types, and the roles of the entities are annotated as relations with other entities described in the text. So far, we have annotated 400 abstracts from the ACL anthology and the ACM digital library. In this paper, the scheme and the annotated dataset are described, along with the problems found in the course of annotation. We also show the results of automatic annotation and evaluate the corpus in a practical setting in application to topic extraction."
L16-1630,Towards Comparability of Linguistic Graph {B}anks for Semantic Parsing,2016,20,7,3,0,2623,stephan oepen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We announce a new language resource for research on semantic parsing, a large, carefully curated collection of semantic dependency graphs representing multiple linguistic traditions. This resource is called SDP{\textasciitilde}2016 and provides an update and extension to previous versions used as Semantic Dependency Parsing target representations in the 2014 and 2015 Semantic Evaluation Exercises. For a common core of English text, this third edition comprises semantic dependency graphs from four distinct frameworks, packaged in a unified abstract format and aligned at the sentence and token levels. SDP 2016 is the first general release of this resource and available for licensing from the Linguistic Data Consortium in May 2016. The data is accompanied by an open-source SDP utility toolkit and system results from previous contrastive parsing evaluations against these target representations."
D16-1002,Rule Extraction for Tree-to-Tree Transducers by Cost Minimization,2016,23,1,2,1,9258,pascual martinezgomez,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1004,Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction,2016,27,10,2,1,5927,hiroshi noji,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1242,Building compositional semantics and higher-order inference system for a wide-coverage {J}apanese {CCG} parser,2016,24,3,4,1,7511,koji mineshima,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1005,Generating Video Description using Sequence-to-sequence Model with Temporal Attention,2016,14,3,7,0,30047,natsuda laokulrat,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Automatic video description generation has recently been getting attention after rapid advancement in image caption generation. Automatically generating description for a video is more challenging than for an image due to its temporal dynamics of frames. Most of the work relied on Recurrent Neural Network (RNN) and recently attentional mechanisms have also been applied to make the model learn to focus on some frames of the video while generating each word in a describing sentence. In this paper, we focus on a sequence-to-sequence approach with temporal attention mechanism. We analyze and compare the results from different attention model configuration. By applying the temporal attention mechanism to the system, we can achieve a METEOR score of 0.310 on Microsoft Video Description dataset, which outperformed the state-of-the-art system so far."
C16-1313,Video Event Detection by Exploiting Word Dependencies from Image Captions,2016,3,1,2,0,35684,sang phan,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Video event detection is a challenging problem in information and multimedia retrieval. Different from single action detection, event detection requires a richer level of semantic information from video. In order to overcome this challenge, existing solutions often represent videos using high level features such as concepts. However, concept-based representation can be confusing because it does not encode the relationship between concepts. This issue can be addressed by exploiting the co-occurrences of the concepts, however, it often leads to a very huge number of possible combinations. In this paper, we propose a new approach to obtain the relationship between concepts by exploiting the syntactic dependencies between words in the image captions. The main advantage of this approach is that it significantly reduces the number of informative combinations between concepts. We conduct extensive experiments to analyze the effectiveness of using the new dependency representation for event detection on two large-scale TRECVID Multimedia Event Detection 2013 and 2014 datasets. Experimental results show that i) Dependency features are more discriminative than concept-based features. ii) Dependency features can be combined with our current event detection system to further improve the performance. For instance, the relative improvement can be as far as 8.6{\%} on the MEDTEST14 10Ex setting."
Y15-1058,Paraphrase Detection Based on Identical Phrase and Similar Word Matching,2015,18,2,2,0,4618,hoangquoc nguyenson,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Paraphrase detection has numerous important applications in natural language processing (such as clustering, summarizing, and detecting plagiarism). One approach to detecting paraphrases is to use predicate argument tuples. Although this approach achieves high paraphrase recall, its accuracy is generally low. Other approaches focus on matching similar words, but word meaning is often contextual (e.g., xe2x80x98get along with,xe2x80x99 xe2x80x98look forward toxe2x80x99). An effective approach to detecting plagiarism would take into account the fact that plagiarists frequently cut and paste whole phrases and/or replace several words with similar words. This generally results in the paraphrased text containing identical phrases and similar words. Moreover, plagiarists usually insert and/or remove various minor words (prepositions, conjunctions, etc.) to both improve the naturalness and disguise the paraphrasing. We have developed a similarity matching (SimMat) metric for detecting paraphrases that is based on matching identical phrases and similar words and quantifying the minor words. The metric achieved the highest paraphrase detection accuracy (77.6%) when it was combined with eight standard machine translation metrics. This accuracy is better than the 77.4% rate achieved with the state-of-the-art approach for paraphrase detection."
W15-2203,Incorporating Complementary Annotation to a {CCG}bank for Improving Derivations for {J}apanese,2015,17,0,2,0.833333,29831,sumire uematsu,Proceedings of the 14th International Conference on Parsing Technologies,0,"Wide-coverage resources for lexicalized grammars have been obtained by converting the existing treebanks into collections of derivations. Additional annotations to the source treebank can be used to improve these derivations. A treebank annotation called the NTT treebank was used for this paper to improve a CCGbank for Japanese. The source treebank of the CCGbank itself is created by automatically converting chunk-dependencies, but the CCGbank contains errors caused by noisier phrase structures and a lack of linguistic information, which is difficult to represent in chunk-dependency. The NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations."
S15-2153,{S}em{E}val 2015 Task 18: Broad-Coverage Semantic Dependency Parsing,2015,-1,-1,3,0,2623,stephan oepen,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,None
P15-2023,Discriminative Preordering Meets Kendall{'}s $\\tau$ Maximization,2015,23,7,2,1,37418,sho hoshino,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper explores a simple discriminative preordering model for statistical machine translation. Our model traverses binary constituent trees, and classifies whether children of each node should be reordered. The model itself is not extremely novel, but herein we introduce a new procedure to determine oracle labels so as to maximize Kendallxe2x80x99s xcfx84 . Experiments in Japanese-to-English translation revealed that our simple method is comparable with, or superior to, state-of-the-art methods in translation accuracy."
P15-2046,A Lexicalized Tree Kernel for Open Information Extraction,2015,16,3,6,1,4212,ying xu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In contrast with traditional relation extraction, which only considers a fixed set of relations, Open Information Extraction (Open IE) aims at extracting all types of relations from text. Because of data sparseness, Open IE systems typically ignore lexical information, and instead employ parse trees and Part-of-Speech (POS) tags. However, the same syntactic structure may correspond to different relations. In this paper, we propose to use a lexicalized tree kernel based on the word embeddings created by a neural network model. We show that the lexicalized tree kernel model surpasses the unlexicalized model. Experiments on three datasets indicate that our Open IE system performs better on the task of relation extraction than the stateof-the-art Open IE systems of Xu et al. (2013) and Mesquita et al. (2013)."
P15-1148,Optimal Shift-Reduce Constituent Parsing with Structured Perceptron,2015,25,9,3,0,37534,le thang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present a constituent shift-reduce parser with a structured perceptron that finds the optimal parse in a practical runtime. The key ideas are new feature templates that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes.1"
D15-1244,Higher-order logical inference with compositional semantics,2015,30,19,3,1,7511,koji mineshima,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a higher-order inference system based on a formal compositional semantics and the wide-coverage CCG parser. We develop an improved method to bridge between the parser and semantic composition. The system is evaluated on the FraCaS test suite. In contrast to the widely held view that higher-order logic is unsuitable for efficient logical inferences, the results show that a system based on a reasonably-sized semantic lexicon and a manageable number of non-first-order axioms enables efficient logical inferences, including those concerned with generalized quantifiers and intensional operators, and outperforms the state-of-the-art firstorder inference system."
Y14-1067,Encoding Generalized Quantifiers in Dependency-based Compositional Semantics,2014,17,3,3,0,38084,yubing dong,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"For textual entailment recognition systems, it is often important to correctly handle Generalized quantifiers (GQ). In this paper, we explore ways of encoding GQs in a recent framework of Dependency-based Compositional Semantics, especially aiming to correctly handle linguistic knowledge like hyponymy when GQs are involved. We use both the selection operator mechanism and a new relation extension to implement some major properties of GQs, reducing 69% errors of a previous system, and a further error analysis suggests extensions towards more powerful logical systems."
W14-7008,{J}apanese to {E}nglish Machine Translation using Preordering and Compositional Distributed Semantics,2014,0,3,3,1,37418,sho hoshino,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
W14-5205,Significance of Bridging Real-world Documents and {NLP} Technologies,2014,7,0,3,1,38278,tadayoshi hara,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"Most conventional natural language processing (NLP) tools assume plain text as their input, whereas real-world documents display text more expressively, using a variety of layouts, sentence structures, and inline objects, among others. When NLP tools are applied to such text, users must first convert the text into the input/output formats of the tools. Moreover, this awkwardly obtained input typically does not allow the expected maximum performance of the NLP tools to be achieved. This work attempts to raise awareness of this issue using XML documents, where textual composition beyond plain text is given by tags. We propose a general framework for data conversion between XML-tagged text and plain text used as input/output for NLP tools and show that text sequences obtained by our framework can be much more thoroughly and efficiently processed by parsers than naively tag-removed text. These results highlight the significance of bridging real-world documents and NLP technologies."
W14-2414,Efficient Logical Inference for Semantic Processing,2014,6,1,2,0,20138,ran tian,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"Dependency-based Compositional Semantics (DCS) provides a precise and expressive way to model semantics of natural language queries on relational databases, by simple dependency-like trees. Recently abstract denotation is proposed to enable generic logical inference on DCS. In this paper, we discuss some other possibilities to equip DCS with logical inference, and we discuss further on how logical inference can help textual entailment recognition, or other semantic precessing tasks."
S14-2008,{S}em{E}val 2014 Task 8: Broad-Coverage Semantic Dependency Parsing,2014,30,71,3,0,2623,stephan oepen,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Task 18 at SemEval 2015 defines Broad-Coverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicatexe2x80x93argument relationships for all content words, i.e. the sema ..."
S14-2056,In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers,2014,22,5,1,1,5928,yusuke miyao,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This submission to the open track of Task 8 at SemEval 2014 seeks to connect the Task to pre-existing, xe2x80x98in-housexe2x80x99 parsing systems for the same types of target semantic dependency graphs."
P14-1008,Logical Inference on Dependency-based Compositional Semantics,2014,50,25,2,0,20138,ran tian,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denotations. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results."
tateisi-etal-2014-annotation,Annotation of Computer Science Papers for Semantic Relation Extrac-tion,2014,15,11,3,0.513481,35318,yuka tateisi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We designed a new annotation scheme for formalising relation structures in research papers, through the investigation of computer science papers. The annotation scheme is based on the hypothesis that identifying the role of entities and events that are described in a paper is useful for intelligent information retrieval in academic literature, and the role can be determined by the relationship between the author and the described entities or events, and relationships among them. Using the scheme, we have annotated research abstracts from the IPSJ Journal published in Japanese by the Information Processing Society of Japan. On the basis of the annotated corpus, we have developed a prototype information extraction system which has the facility to classify sentences according to the relationship between entities mentioned, to help find the role of the entity in which the searcher is interested."
fujita-etal-2014-overview,Overview of {T}odai Robot Project and Evaluation Framework of its {NLP}-based Problem Solving,2014,8,18,4,0,35179,akira fujita,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We introduce the organization of the Todai Robot Project and discuss its achievements. The Todai Robot Project task focuses on benchmarking NLP systems for problem solving. This task encourages NLP-based systems to solve real high-school examinations. We describe the details of the method to manage question resources and their correct answers, answering tools and participation by researchers in the task. We also analyse the answering accuracy of the developed systems by comparing the systemsÂ answers with answers given by human test-takers."
D14-1143,Formalizing Word Sampling for Vocabulary Prediction as Graph-based Active Learning,2014,10,5,2,0,260,yo ehara,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Predicting vocabulary of second language learners is essential to support their language learning; however, because of the large size of language vocabularies, we cannot collect information on the entire vocabulary. For practical measurements, we need to sample a small portion of words from the entire vocabulary and predict the rest of the words. In this study, we propose a novel framework for this sampling method. Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts. We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph. We show that by extending the graph, we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora. In our experiments, we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small."
C14-1202,Left-corner Transitions on Dependency Parsing,2014,40,7,2,1,5927,hiroshi noji,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We propose a transition system for dependency parsing with a left-corner parsing strategy. Unlike parsers with conventional transition systems, such as arc-standard or arc-eager, a parser with our system correctly predicts the processing difficulties people have, such as of center-embedding. We characterize our transition system by comparing its oracle behaviors with those of other transition systems on treebanks of 18 typologically diverse languages. A crosslinguistical analysis confirms the universality of the claim that a parser with our system requires less memory for parsing naturally occurring sentences."
Y13-1026,Effects of Parsing Errors on Pre-Reordering Performance for {C}hinese-to-{J}apanese {SMT},2013,19,3,3,0,33211,dan han,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Linguistically motivated reordering methods have been developed to improve word alignment especially for Statistical Machine Translation (SMT) on long distance language pairs. However, since they highly rely on the parsing accuracy, it is useful to explore the relationship between parsing and reordering. For Chinese-toJapanese SMT, we carry out a three-stage incremental comparative analysis to observe the effects of different parsing errors on reordering performance by combining empirical and descriptive approaches. For the empirical approach, we quantify the distribution of general parsing errors along with reordering qualities whereas for the descriptive approach, we extract seven influential error patterns and examine their correlation with reordering errors."
W13-4403,Deep Context-Free Grammar for {C}hinese with Broad-Coverage,2013,10,0,3,1,40670,xiangli wang,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"The accuracy of Chinese parsers trained on Penn Chinese Treebank is evidently lower than that of the English parsers trained on Penn Treebank. It is plausible that the essential reason is the lack of surface syntactic constraints in Chinese. In this paper, we present evidences to show that strict deep syntactic constraints exist in Chinese sentences and such constraints cannot be effectively described with context-free phrase structure rules as in the Penn Chinese Treebank annotation; we show that such constraints may be described precisely by the idea of Sentence Structure Grammar; we introduce how to develop a broad-coverage rule-based grammar for Chinese based on this idea; we evaluated the grammar and the evaluation results show that the coverage of the current grammar is 94.2%."
W13-2806,Using unlabeled dependency parsing for pre-reordering for {C}hinese-to-{J}apanese statistical machine translation,2013,24,6,3,0,33211,dan han,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods."
W13-2303,Utilizing State-of-the-art Parsers to Diagnose Problems in Treebank Annotation for a Less Resourced Language,2013,10,4,3,1,30900,quy nguyen,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"The recent success of statistical parsing methods has made treebanks become important resources for building good parsers. However, constructing highquality annotated treebanks is a challenging task. We utilized two publicly available parsers, Berkeley and MST parsers, for feedback on improving the quality of part-of-speech tagging for the Vietnamese Treebank. Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties of Vietnamese parsing that required further improvements to existing parsing technologies."
W13-2318,Relation Annotation for Understanding Research Papers,2013,17,2,3,0.555135,35318,yuka tateisi,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"We describe a new annotation scheme for formalizing relation structures in research papers. The scheme has been developed through the investigation of computer science papers. Using the scheme, we are building a Japanese corpus to help develop information extraction systems for digital libraries. We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal."
P13-2049,Building {J}apanese Textual Entailment Specialized Data Sets for Inference of Basic Sentence Relations,2013,3,4,2,0,33473,kimi kaneko,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper proposes a methodology for generating specialized Japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations. We experimented with our methodology over a number of pairs taken from the RITE-2 data set. We compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy."
P13-1103,Integrating Multiple Dependency Corpora for Inducing Wide-coverage {J}apanese {CCG} Resources,2013,0,6,4,0.833333,29831,sumire uematsu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
I13-1090,Alignment-based Annotation of Proofreading Texts toward Professional Writing Assistance,2013,16,4,2,1,1934,ngan nguyen,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This work aims at constructing a corpus to satisfy such requirements to support research towards professional writing assistance. Our corpus is a collection of scientific work written by non-native speakers that has been proofread by native English experts. A new annotation scheme, which is based on word-alignments, is then proposed that is used to capture all types of inarticulations and their corrections including both spelling/grammatical error corrections and paraphrases made by proofreaders. The resulting corpus contains 3,485 pairs of original and revised sentences, of which, 2,516 pairs contain at least one articulation."
I13-1147,Two-Stage Pre-ordering for {J}apanese-to-{E}nglish Statistical Machine Translation,2013,15,12,2,1,37418,sho hoshino,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES.
I13-1192,University Entrance Examinations as a Benchmark Resource for {NLP}-based Problem Solving,2013,16,5,1,1,5928,yusuke miyao,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper describes a corpus comprised of university entrance examinations, which is aimed to promote research on NLP-based problem solving. Since entrance examinations are created for quantifying human ability of problem solving, they are a desirable resource for benchmarking NLP-based problem solving systems. However, as entrance examinations involve a variety of subjects and types of questions, in order to pursue focused research on specific NLP technologies, it is necessary to break down entire examinations into individual NLP subtasks. For this purpose, we provide annotations of question classifications in terms of answer types and knowledge types. In this paper, we also describe research issues by referring to results of question classification, and introduce two international shared tasks that employed our resource for developing their evaluation data sets."
D13-1118,Improvements to the {B}ayesian Topic N-Gram Models,2013,20,5,3,1,5927,hiroshi noji,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document."
W12-5005,Comparing Different Criteria for {V}ietnamese Word Segmentation,2012,25,6,3,1,30900,quy nguyen,Proceedings of the 3rd Workshop on South and Southeast {A}sian Natural Language Processing,0,"Syntactically annotated corpora have become important resources for natural language processing due in part to the success of corpus-based methods. Since words are often considered as primitive units of language structures, the annotation of word segmentation forms the basis of these corpora. This is also an issue for the Vietnamese Treebank (VTB), which is the first and only publicly available syntactically annotated corpus for the Vietnamese language. Although word segmentation is straight-forward for space-delimited languages like English, this is not the case for languages like Vietnamese for which a standard criterion for word segmentation does not exist. This work explores the challenges of Vietnamese word segmentation through the detection and correction of inconsistency for VTB. Then, by combining and splitting the inconsistent annotations that were detected, we are able to observe the influence of dierent word segmentation criteria on automatic word segmentation, and the applications of word segmentation, including text classification and English-Vietnamese statistical machine translation. The analysis and experimental results showed that our methods improved the quality of VTB, which positively aected the performance of its applications. Title and Abstract in another language, L2 (optional, and on same page) So sanh cac tieu chi tach t khac nhau thong qua"
P12-1046,{B}ayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing,2012,31,29,2,0,7901,hiroyuki shindo,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers."
P12-1110,"Incremental Joint Approach to Word Segmentation, {POS} Tagging, and Dependency Parsing in {C}hinese",2012,18,54,3,1,42725,jun hatori,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models."
grissom-ii-miyao-2012-annotating,Annotating Factive Verbs,2012,9,0,2,0,19397,alvin ii,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We have created a scheme for annotating corpora designed to capture relevant aspects of factivity in verb-complement constructions. Factivity constructions are a well-known linguistic phenomenon that embed presuppositions about the state of the world into a clause. These embedded presuppositions provide implicit information about facts assumed to be true in the world, and are thus potentially valuable in areas of research such as textual entailment. We attempt to address both clear-cut cases of factivity and non-factivity, as well as account for the fluidity and ambiguous nature of some realizations of this construction. Our extensible scheme is designed to account for distinctions between claims, performatives, atypical uses of factivity, and the authority of the one making the utterance. We introduce a simple XML-based syntax for the annotation of factive verbs and clauses, in order to capture this information. We also provide an analysis of the issues which led to these annotative decisions, in the hope that these analyses will be beneficial to those dealing with factivity in a practical context."
matsubayashi-etal-2012-building,Building {J}apanese Predicate-argument Structure Corpus using Lexical Conceptual Structure,2012,7,2,2,0,9338,yuichiroh matsubayashi,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper introduces our study on creating a Japanese corpus that is annotated using semantically-motivated predicate-argument structures. We propose an annotation framework based on Lexical Conceptual Structure (LCS), where semantic roles of arguments are represented through a semantic structure decomposed by several primitive predicates. As a first stage of the project, we extended Jackendoff 's LCS theory to increase generality of expression and coverage for verbs frequently appearing in the corpus, and successfully created LCS structures for 60 frequent Japanese predicates in Kyoto university Text Corpus (KTC). In this paper, we report our framework for creating the corpus and the current status of creating an LCS dictionary for Japanese predicates."
E12-1070,Framework of Semantic Role Assignment based on Extended Lexical Conceptual Structure: Comparison with {V}erb{N}et and {F}rame{N}et,2012,15,3,2,0,9338,yuichiroh matsubayashi,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Widely accepted resources for semantic parsing, such as PropBank and FrameNet, are not perfect as a semantic role labeling framework. Their semantic roles are not strictly defined; therefore, their meanings and semantic characteristics are unclear. In addition, it is presupposed that a single semantic role is assigned to each syntactic argument. This is not necessarily true when we consider internal structures of verb semantics. We propose a new framework for semantic role annotation which solves these problems by extending the theory of lexical conceptual structure (LCS). By comparing our framework with that of existing resources, including VerbNet and FrameNet, we demonstrate that our extended LCS framework can give a formal definition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally."
C12-1084,Answering Yes/No Questions via Question Inversion,2012,15,20,2,0.324659,1260,hiroshi kanayama,Proceedings of {COLING} 2012,0,"This paper investigates a solution to yes/no question answering, which can be mapped to the task of determining the correctness of a given proposition. Generally it is hard to obtain explicit evidence to conclude a proposition is false from an information source, so we convert this task to a set of factoid-style questions and use an existing question answering system as a subsystem. By aggregating the answers and confidence values from a factoid-style question answering system we can determine the correctness of the entire proposition or the substitutions that make the proposition false. We evaluated the system on multiple-choice questions from a university admission test on world history, and found it to be highly accurate."
W11-2907,Analysis of the Difficulties in {C}hinese Deep Parsing,2011,33,9,2,1,44132,kun yu,Proceedings of the 12th International Conference on Parsing Technologies,0,"This paper discusses the difficulties in Chinese deep parsing, by comparing the accuracy of a Chinese HPSG parser to the accuracy of an English HPSG parser and the commonly used Chinese syntactic parsers. Analysis reveals that deep parsing for Chinese is more challenging than for English, due to the shortage of syntactic constraints of Chinese verbs, the widespread pro-drop, and the large distribution of ambiguous constructions. Moreover, the inherent ambiguities caused by verbal co-ordination and relative clauses make semantic analysis of Chinese more difficult than the syntactic analysis of Chinese."
W11-0407,A Collaborative Annotation between Human Annotators and a Statistical Parser,2011,11,0,4,0,44409,shunya iwasawa,Proceedings of the 5th Linguistic Annotation Workshop,0,"We describe a new interactive annotation scheme between a human annotator who carries out simplified annotations on CFG trees, and a statistical parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed scheme's effectiveness, we performed automatic pseudo-annotations that emulate the system's idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set."
W11-0328,Learning with Lookahead: Can History-Based Models Rival Globally Optimized Models?,2011,28,39,2,0.225122,338,yoshimasa tsuruoka,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"This paper shows that the performance of history-based models can be significantly improved by performing lookahead in the state space when making each classification decision. Instead of simply using the best action output by the classifier, we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences. We present a perceptron-based parameter optimization method for this learning framework and show its convergence properties. The proposed framework is evaluated on part-of-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons."
W11-0221,Parsing Natural Language Queries for Life Science Knowledge,2011,25,0,4,1,38278,tadayoshi hara,Proceedings of {B}io{NLP} 2011 Workshop,0,"This paper presents our preliminary work on adaptation of parsing technology toward natural language query processing for biomedical domain. We built a small treebank of natural language queries, and tested a state-of-the-art parser, the results of which revealed that a parser trained on Wall-Street-Journal articles and Medline abstracts did not work well on query sentences. We then experimented an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results are encouraging, enlightening the direction for effective improvement."
I11-1084,Exploring Difficulties in Parsing Imperatives and Questions,2011,19,2,3,1,38278,tadayoshi hara,Proceedings of 5th International Joint Conference on Natural Language Processing,0,This paper analyzes the effect of the structural variation of sentences on parsing performance. We examine the performance of both shallow and deep parsers for two sentence constructions: imperatives and questions. We first prepare an annotated corpus for each of these sentence constructions by extracting sentences from a fiction domain that cover various types of imperatives and questions. The target parsers are then adapted to each of the obtained corpora as well as the existing query-focused corpus. Analysis of the experimental results reveals that the current mainstream parsing technologies and adaptation techniques cannot cope with different sentence constructions even with much in-domain data.
I11-1136,Incremental Joint {POS} Tagging and Dependency Parsing in {C}hinese,2011,21,60,3,1,42725,jun hatori,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We address the problem of joint part-of-speech (POS) tagging and dependency parsing in Chinese. In Chinese, some POS tags are often hard to disambiguate without considering longrange syntactic information. Also, the traditional pipeline approach to POS tagging and dependency parsing may suffer from the problem of error propagation. In this paper, we propose the first incremental approach to the task of joint POS tagging and dependency parsing, which is built upon a shift-reduce parsing framework with dynamic programming. Although the incremental approach encounters difficulties with underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task."
Y10-1055,A Modular Architecture for the Wide-Coverage Translation of Natural Language Texts into Predicate Logic Formulas,2010,16,1,1,1,5928,yusuke miyao,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We present a new method for translating unrestricted natural language texts into predicate logic formulas. This relies on the semantic evaluation procedure of Scope Control Theory (SCT), a variant of Dynamic Semantic formalisms. The key benefit is that parsed syntactic structures are shown to form sufficient input for semantic evaluation, eliminating the need to build distinct semantic expressions to feed semantic evaluation. To have parsed syntactic structures for SCT to evaluate we apply an existing wide-coverage syntactic parser by converting the parser output into a form SCT can receive. This modularity has led to the rapid attainment of a broad coverage on real text. An experiment revealed our system achieved 82.7% coverage on real-world sentences, generating representations that make explicit the scopes of quantifiers (e.g., xe2x88x83x), operators (e.g., negation), connectives (e.g., conjunction) and embedding predicates (e.g., thinks), while also capturing the inter and intra sentential dependencies and cross-sentential anaphoric dependencies that connect predicates."
W10-1816,The Deep Re-Annotation in a {C}hinese Scientific Treebank,2010,13,0,3,1,44132,kun yu,Proceedings of the Fourth Linguistic Annotation Workshop,0,"In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific treebank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described."
P10-5001,Wide-Coverage {NLP} with Linguistically Expressive Grammars,2010,3,0,2,0,8351,julia hockenmaier,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"In recent years, there has been a lot of research on wide-coverage statistical natural language processing with linguistically expressive grammars such as Combinatory Categorial Grammars (CCG), Head-driven Phrase-Structure Grammars (HPSG), Lexical-Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG). But although many young researchers in natural language processing are very well trained in machine learning and statistical methods, they often lack the necessary background to understand the linguistic motivation behind these formalisms. Furthermore, in many linguistics departments, syntax is still taught from a purely Chomskian perspective. Additionally, research on these formalisms often takes place within tightly-knit, formalismspecific subcommunities. It is therefore often difficult for outsiders as well as experts to grasp the commonalities of and differences between these formalisms."
C10-2162,Semi-automatically Developing {C}hinese {HPSG} Grammar from the {P}enn {C}hinese Treebank for Deep Parsing,2010,29,16,2,1,44132,kun yu,Coling 2010: Posters,0,"In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and annotation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexicon is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accuracy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English."
C10-1089,Entity-Focused Sentence Simplification for Relation Extraction,2010,19,36,3,0.689655,3222,makoto miwa,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Relations between entities in text have been widely researched in the natural language processing and information-extraction communities. The region connecting a pair of entities (in a parsed sentence) is often used to construct kernels or feature vectors that can recognize and extract interesting relations. Such regions are useful, but they can also incorporate unnecessary distracting information. In this paper, we propose a rule-based method to remove the information that is unnecessary for relation extraction. Protein-protein interaction (PPI) is used as an example relation extraction problem. A dozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora."
Y09-2048,Design of {C}hinese {HPSG} Framework for Data-Driven Parsing,2009,15,7,3,1,40670,xiangli wang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Data-driven parsing has been a main method for analyzing natural languages. We aim at exploring a data-driven Chinese parser, by basing it on Head-driven Phrase Structure Grammar (HPSG). Unlike for English, there is still no available Chinese HPSG framework. As the first step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework."
W09-3828,Effective Analysis of Causes and Inter-dependencies of Parsing Errors,2009,12,2,2,1,38278,tadayoshi hara,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model.n n The first method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PP-attachment, identification of antecedent of relative clauses, etc. On the other hand, the second method, which is based on reparsing with one of observed errors corrected, assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a specific error was corrected.n n Experiments show that these two methods are complementary and by being combined, they can provide useful clues as to how to improve a given grammar."
D09-1013,A Rich Feature Vector for Protein-Protein Interaction Extraction from Multiple Corpora,2009,28,64,3,0.689655,3222,makoto miwa,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Because of the importance of protein-protein interaction (PPI) extraction from text, many corpora have been proposed with slightly differing definitions of proteins and PPI. Since no single corpus is large enough to saturate a machine learning system, it is necessary to learn from multiple different corpora. In this paper, we propose a solution to this challenge. We designed a rich feature vector, and we applied a support vector machine modified for corpus weighting (SVM-CW) to complete the task of multiple corpora PPI extraction. The rich feature vector, made from multiple useful kernels, is used to express the important information for PPI extraction, and the system with our feature vector was shown to be both faster and more accurate than the original kernel-based system, even when using just a single corpus. SVM-CW learns from one corpus, while using other corpora for support. SVM-CW is simple, but it is more effective than other methods that have been successfully applied to other NLP tasks earlier. With the feature vector and SVM-CW, our system achieved the best performance among all state-of-the-art PPI extraction systems reported so far."
D09-1121,Descriptive and Empirical Approaches to Capturing Underlying Dependencies among Parsing Errors,2009,9,5,2,1,38278,tadayoshi hara,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we provide descriptive and empirical approaches to effectively extracting underlying dependencies among parsing errors. In the descriptive approach, we define some combinations of error patterns and extract them from given errors. In the empirical approach, on the other hand, we re-parse a sentence with a target error corrected and observe errors corrected together. Experiments on an HPSG parser show that each of these approaches can clarify the dependencies among individual errors from each point of view. Moreover, the comparison between the results of the two approaches shows that combining these approaches can achieve a more detailed error analysis."
D09-1138,Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes,2009,26,3,1,1,5928,yusuke miyao,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes. We intend for the probabilistic model to provide a probability distribution of verb-class associations, over known and unknown verbs, including polysemous words. In our approach, training instances are obtained from an existing lexicon and/or from an annotated corpus, while the features, which represent syntactic frames, semantic similarity, and selectional preferences, are extracted from unannotated corpora. Our model is evaluated in type-level verb classification tasks: we measure the prediction accuracy of VerbNet classes for unknown verbs, and also measure the dissimilarity between the learned and observed probability distributions. We empirically compare several settings for model learning, while we vary the use of features, source corpora for feature extraction, and disam-biguated corpora. In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model."
2009.iwslt-evaluation.15,The {UOT} system,2009,-1,-1,4,0,6319,xianchao wu,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We present the UOT Machine Translation System that was used in the IWSLT-09 evaluation campaign. This year, we participated in the BTEC track for Chinese-to-English translation. Our system is based on a string-to-tree framework. To integrate deep syntactic information, we propose the use of parse trees and semantic dependencies on English sentences described respectively by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets."
W08-1305,Parser Evaluation Across Frameworks without Format Conversion,2008,8,2,3,0,47752,wai tam,Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,0,"In the area of parser evaluation, formats like GR and SD which are based on dependencies, the simplest representation of syntactic information, are proposed as framework-independent metrics for parser evaluation. The assumption behind these proposals is that the simplicity of dependencies would make conversion from syntactic structures and semantic representations used in other formalisms to GR/SD a easy job. But (Miyao et al., 2007) reports that even conversion between these two formats is not easy at all. Not to mention that the 80% success rate of conversion is not meaningful for parsers that boast 90% accuracy. In this paper, we make an attempt at evaluation across frameworks without format conversion. This is achieved by generating a list of names of phenomena with each parse. These names of phenomena are matched against the phenomena given in the gold standard. The number of matches found is used for evaluating the parser that produces the parses. The evaluation method is more effective than evaluation methods which involve format conversion because the generation of names of phenomena from the output of a parser loaded is done by a recognizer that has a 100% success rate of recognizing a phenomenon illustrated by a sentence. The success rate is made possible by the reuse of native codes: codes used for writing the parser and rules of the grammar loaded into the parser."
W08-0504,Evaluating the Effects of Treebank Size in a Practical Application for Parsing,2008,14,2,2,0.517017,6910,kenji sagae,"Software Engineering, Testing, and Quality Assurance for Natural Language Processing",0,"Natural language processing modules such as part-of-speech taggers, named-entity recognizers and syntactic parsers are commonly evaluated in isolation, under the assumption that artificial evaluation metrics for individual parts are predictive of practical performance of more complex language technology systems that perform practical tasks. Although this is an important issue in the design and engineering of systems that use natural language input, it is often unclear how the accuracy of an end-user application is affected by parameters that affect individual NLP modules. We explore this issue in the context of a specific task by examining the relationship between the accuracy of a syntactic parser and the overall performance of an information extraction system for biomedical text that includes the parser as one of its components. We present an empirical investigation of the relationship between factors that affect the accuracy of syntactic analysis, and how the difference in parse accuracy affects the overall system."
P08-1006,Task-oriented Evaluation of Syntactic Parsers and Their Representations,2008,41,71,1,1,5928,yusuke miyao,Proceedings of ACL-08: HLT,1,"This paper presents a comparative evaluation of several state-of-the-art English parsers based on different frameworks. Our approach is to measure the impact of each parser when it is used as a component of an information extraction system that performs protein-protein interaction (PPI) identification in biomedical papers. We evaluate eight parsers (based on dependency parsing, phrase structure parsing, or deep parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data."
tateisi-etal-2008-genia,{GENIA}-{GR}: a Grammatical Relation Corpus for Parser Evaluation in the Biomedical Domain,2008,21,5,2,0.58664,35318,yuka tateisi,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report the construction of a corpus for parser evaluation in the biomedical domain. A 50-abstract subset (492 sentences) of the GENIA corpus (Kim et al., 2003) is annotated with labeled head-dependent relations using the grammatical relations (GR) evaluation scheme (Carroll et al., 1998) ,which has been used for parser evaluation in the newswire domain."
J08-1002,Feature Forest Models for Probabilistic {HPSG} Parsing,2008,82,171,1,1,5928,yusuke miyao,Computational Linguistics,0,"Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.n n This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.n n This article also describes methods for representing HPSG syntactic structures and predicate-argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed."
I08-2122,Towards Data and Goal Oriented Analysis: Tool Inter-operability and Combinatorial Comparison,2008,8,2,6,0,8106,yoshinobu kano,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Recently, NLP researches have advanced using F-scores, precisions, and recalls with gold standard data as evaluation measures. However, such evaluations cannot capture the different behaviors of varying NLP tools or the different behaviors of a NLP tool that depends on the data and domain in which it works. Because an increasing number of tools are available nowadays, it has become increasingly important to grasp these behavioral differences, in order to select a suitable set of tools, which forms a complex workflow for a specific purpose. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need sharable and comparable type systems with an automatic combinatorial comparison generator, which would allow systematic comparisons of available tools. In this paper, we describe such an environment, which we developed based on UIMA, and we show its feasibility through an example of a protein-protein interaction (PPI) extraction system."
C08-2011,Word Sense Disambiguation for All Words using Tree-Structured Conditional Random Fields,2008,13,2,2,1,42725,jun hatori,Coling 2008: Companion volume: Posters,0,"We propose a supervised word sense disambiguation (WSD) method using tree-structured conditional random fields (TCRFs). By applying TCRFs to a sentence described as a dependency tree structure, we conduct WSD as a labeling problem on tree structures. To incorporate dependencies between word senses, we introduce a set of features on tree edges, in combination with coarse-grained tagsets, and show that these contribute to an improvement in WSD accuracy. We also show that the tree-structured model outperforms the linear-chain model. Experiments on the SENSEVAL-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems."
C08-2016,Exact Inference for Multi-label Classification using Sparse Graphical Models,2008,12,1,1,1,5928,yusuke miyao,Coling 2008: Companion volume: Posters,0,"This paper describes a parameter estimation method for multi-label classification that does not rely on approximate inference. It is known that multi-label classification involving label correlation features is intractable, because the graphical model for this problem is a complete graph. Our solution is to exploit the sparsity of features, and express a model structure for each object by using a sparse graph. We can thereby apply the junction tree algorithm, allowing for efficient exact inference on sparse graphs. Experiments on three data sets for text categorization demonstrated that our method increases the accuracy for text categorization with a reasonable cost."
W07-2202,Evaluating Impact of Re-training a Lexical Disambiguation Model on Domain Adaptation of an {HPSG} Parser,2007,28,39,2,1,38278,tadayoshi hara,Proceedings of the Tenth International Conference on Parsing Technologies,0,"This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain. In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser. Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data. Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost. We also evaluated our method in the Brown corpus to show the portability of our approach in another domain."
W07-2208,A log-linear model with an n-gram reference distribution for accurate {HPSG} parsing,2007,32,22,3,1,3213,takashi ninomiya,Proceedings of the Tenth International Conference on Parsing Technologies,0,"This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model."
P07-1079,{HPSG} Parsing with Shallow Dependency Constraints,2007,23,38,2,0.517017,6910,kenji sagae,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing."
W06-1619,Extremely Lexicalized Models for Accurate and Fast {HPSG} Parsing,2006,30,34,4,1,3213,takashi ninomiya,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing. In this model, the probabilities of parse trees are defined with only the probabilities of selecting lexical entries. The proposed model is very simple, and experiments revealed that the implemented parser runs around four times faster than the previous model and that the proposed model has a high accuracy comparable to that of the previous model for probabilistic HPSG, which is defined over phrase structures. We also developed a hybrid of our probabilistic model and the conventional phrase-structure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model."
W06-1634,Automatic Construction of Predicate-argument Structure Patterns for Biomedical Information Extraction,2006,19,30,2,1,49766,akane yakushiji,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus. Because PASs represent generalized structures for syntactical variants, patterns on PASs are expected to be more generalized than those on surface words. In addition, patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns."
P06-4005,An Intelligent Search Engine and {GUI}-based Efficient {MEDLINE} Search Tool Based on Deep Syntactic Parsing,2006,14,31,2,0.136175,35319,tomoko ohta,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"We present a practical HPSG parser for English, an intelligent search engine to retrieve MEDLINE abstracts that represent biomedical events and an efficient MEDLINE search tool helping users to find information about biomedical entities such as genes, proteins, and the interactions between them."
P06-2091,Translating {HPSG}-Style Outputs of a Robust Parser into Typed Dynamic Logic,2006,13,2,3,0,49947,manabu sato,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus. With its higher-order representations of contexts, TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus."
P06-2109,Trimming {CFG} Parse Trees for Sentence Compression Using Machine Learning Approaches,2006,13,18,3,0,49953,yuya unno,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning. Existing methods learn statistics on trimming context-free grammar (CFG) rules. However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents' and daughters' non-terminals in applied CFG rules. We apply a maximum entropy model to the above method. Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain. We evaluated the method using manually compressed sentences and human judgments. We found that our method produced more grammatical and informative compressed sentences than other methods."
P06-1059,Improving the Scalability of Semi-{M}arkov Conditional Random Fields for Named Entity Recognition,2006,16,66,2,0,44082,daisuke okanohara,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost. Our framework can handle an NER task that has long named entities and many labels which increase the computational cost. To reduce the computational cost, we propose two techniques: the first is the use of feature forests, which enables us to pack feature-equivalent states, and the second is the introduction of a filtering process which significantly reduces the number of candidate states. This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities. We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels. Experimental results show that our model achieves an F-score of 71.48% on the JNLPBA 2004 shared task without using any external resources or post-processing techniques."
P06-1128,Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases,2006,18,83,1,1,5928,yusuke miyao,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts. Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer. During the run time, user requests are converted into queries of region algebra on these annotations. Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts. This framework was applied to a text retrieval system for MEDLINE. Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved."
W05-1510,Probabilistic Models for Disambiguation of an {HPSG}-Based Chart Generator,2005,22,48,2,0,50782,hiroko nakanishi,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe probabilistic models for a chart generator based on HPSG. Within the research field of parsing with lexicalized grammars such as HPSG, recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models. The focus of this paper is to show that two essential techniques -- model estimation on packed parse forests and beam search during parsing -- are successfully exported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data."
W05-1511,"Efficacy of Beam Thresholding, Unification Filtering and Hybrid Parsing in Probabilistic {HPSG} Parsing",2005,49,20,3,1,3213,takashi ninomiya,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively."
P05-1010,Probabilistic {CFG} with Latent Annotations,2005,16,236,2,1,32672,takuya matsuzaki,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences xe2x89xa4 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection."
P05-1011,Probabilistic Disambiguation Models for Wide-Coverage {HPSG} Parsing,2005,26,104,1,1,5928,yusuke miyao,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Tree-bank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences."
I05-1018,Adapting a Probabilistic Disambiguation Model of an {HPSG} Parser to a New Domain,2005,12,50,2,1,38278,tadayoshi hara,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper describes a method of adapting a domain-independent HPSG parser to a biomedical domain. Without modifying the grammar and the probabilistic model of the original HPSG parser, we develop a log-linear model with additional features on a treebank of the biomedical domain. Since the treebank of the target domain is limited, we need to exploit an original disambiguation model that was trained on a larger treebank. Our model incorporates the original model as a reference probabilistic distribution. The experimental results for our model trained with a small amount of a treebank demonstrated an improvement in parsing accuracy."
P04-3017,Finding Anchor Verbs for Biomedical {IE} Using Predicate-Argument Structures,2004,7,4,3,1,49766,akane yakushiji,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"For biomedical information extraction, most systems use syntactic patterns on verbs (anchor verbs) and their arguments. Anchor verbs can be selected by focusing on their arguments. We propose to use predicate-argument structures (PASs), which are outputs of a full parser, to obtain verbs and their arguments. In this paper, we evaluated PAS method by comparing it to a method using part of speech (POSs) pattern matching. POS patterns produced larger results with incorrect arguments, and the results will cause adverse effects on a phase selecting appropriate verbs."
C04-1204,Deep Linguistic Analysis for the Accurate Identification of Predicate-Argument Relations,2004,31,18,1,1,5928,yusuke miyao,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations."
W03-0401,A model of syntactic disambiguation based on lexicalized grammars,2003,22,7,1,1,5928,yusuke miyao,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars."
W03-0416,An efficient clustering algorithm for class-based language models,2003,11,11,2,1,32672,takuya matsuzaki,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper defines a general form for class-based probabilistic language models and proposes an efficient algorithm for clustering based on this. Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy."
P03-2033,A Debug Tool for Practical Grammar Development,2003,10,3,3,1,49766,akane yakushiji,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"We have developed willex, a tool that helps grammar developers to work efficiently by using annotated corpora and recording parsing errors. Willex has two major new functions. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example."
N03-2020,A Robust Retrieval Engine for Proximal and Structural Search,2003,5,6,3,0,49887,katsuya masuda,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"In the text retrieval area including XML and Region Algebra, many researchers pursued models for specifying what kinds of information should appear in specified structural positions and linear positions (Chinenyanga and Kushmerick, 2001; Wolff et al., 1999; Theobald and Weilkum, 2000; Clarke et al., 1995). The models attracted many researchers because they are considered to be basic frameworks for retrieving or extracting complex information like events. However, unlike IR by keyword-based search, their models are not robust, that is, they support only exact matching of queries, while we would like to know to what degree the contents in specified structural positions are relevant to those in the query even when the structure does not exactly match the query."
E03-1047,Lexicalized Grammar Acquisition,2003,8,7,1,1,5928,yusuke miyao,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus.
W02-2227,A Formal Proof of Strong Equivalence for a Grammar Conversion from {LTAG} to {HPSG}-style,2002,14,0,2,1,4617,naoki yoshinaga,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"This paper presents a sketch of a formal proof of strong equivalence,1 where both grammars generate equivalent parse results, between any LTAG (Lexicalized Tree Adjoining Grammar: Schabes, Abeille and Joshi (1988)) G and an HPSG (Head-Driven Phrase Structure Grammar: Pollard and Sag (1994))-style grammar converted from G by a grammar conversion (Yoshinaga and Miyao, 2001). Our proof theoretically justifies some applications of the grammar conversion that exploit the nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the field of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeille, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we defined according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentences in the ATIS corpus (Marcus, Santorini and Marcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from maximizing their true potential. In this paper we give a formal proof of strong equivalence between any LTAG G and an HPSG-style grammar converted from G by our grammar conversion in order to remove such restrictions on the applications."
W02-2232,Clustering for obtaining syntactic classes of words from automatically extracted {LTAG} grammars,2002,5,2,2,1,38278,tadayoshi hara,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"We propose a method for obtaining syntactic classes of words from a lexicalized tree adjoining grammar (LTAG: Schabes, Abeille and Joshi (1988)) automatically extracted from a corpus. Since elementary trees in LTAG grammars represent syntactic roles of a word, we can obtain syntactic classes by clustering words having the similar elementary trees. With our method, automatically extracted LTAG grammars will be arranged according to the syntactic classes of words, and the grammars can be improved from various points of view. For example, we can improve the coverage of the grammars by supplementing to a word the elementary trees of the syntactic class of the word. An LTAG grammar consists of elementary trees, which determine the position where the word can be put in a sentence, that is, an elementary tree corresponds to a certain syntactic role. Hence, a syntactic class of a word is represented as a set of elementary trees assigned to the word. Since the words of the same syntactic class are expected to have similar elementary trees, we can obtain syntactic classes by clustering words having similar sets of elementary trees. We applied our clustering algorithm to an LTAG grammar automatically extracted from sections 02xe2x80x9321 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz (1994)), and investigated the obtained clusters with changing the number of clusters. We successfully obtained some of the clusters that correspond to certain syntactic classes. On the other hand, we could not obtain some clusters, such as the one for ditransitive verbs, and obtained the clusters that we could not associate clearly with syntactic classes. This is because our method was strongly affected by the difference in the number of words in each part-of-speech class. We concluded that, although our clustering method needs to be improved for practical use, it is effective to automatically obtain syntactic classes of words. The XTAG English grammar (The XTAG Research Group (1995)) is a handmade LTAG grammar which is arranged according to syntactic classes of words, xe2x80x9ctree families.xe2x80x9d Each tree family corresponds to a certain subcategorization frame, and determines elementary trees to be assigned to a word. Thanks to the tree families, the XTAG grammar is independent of a corpus. However, it needs considerable human effort to manually construct such a grammar. Automatically extracted LTAG grammars are superior to manually developed grammars in the sense that it takes much less costs to construct the grammars. Chiang (2000) and Xia (1999) gave the methods of automatically extracting LTAG grammars from a bracketed corpus. They first decided a trunk path of the tree structure of a bracketed sentence, and the relationship (substitution or adjunction) between the trunk and branches. The methods then cut off the branches of them according to the relationship. Because the sentences used for extraction are in real-world texts, extracted grammars are practical for natural language processing. However, automatically extracted grammars are not systematically arranged according to syntactic classes their anchors belong to, like the XTAG grammar. Because of this, automatically extracted grammars tend to be strongly dependent on the corpus. This limitation can be a critical disadvantage of such extracted grammars when the grammars are used for various applications. Then, we want to arrange an extracted grammar according to the syntactic classes of words, without loosing the benefit for the cost. Chen and Vijay-Shanker (2000) proposed the solution to the issue. To improve the coverage of an extracted LTAG grammar, they classified the extracted elementary trees according to the tree families in the XTAG English grammar. First, the method searches for a tree family that contains an elementary tree template of extracted elementary tree et. Next, the method collects other possible tree templates in the tree family and makes elementary trees with the anchor of et and the tree templates. By using tree families, the method can add only proper elementary trees that correspond to the syntactic class of anchors. Chen and Vijay-Shanker (2000) applied this method to an extracted LTAG grammar, and showed the improvement of the coverage of the grammar. Although their method showed the effectiveness of arranging a grammar according to syntactic classes, the method depends on"
C02-1100,Lenient Default Unification for Robust Processing within Unification Based Grammar Formalisms,2002,13,4,2,1,3213,takashi ninomiya,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes new default unification, lenient default unification. It works efficiently, and gives more informative results because it maximizes the amount of information in the result, while other default unification maximizes it in the default. We also describe robust processing within the framework of HPSG. We extract grammar rules from the results of robust parsing using lenient default unification. The results of a series of experiments show that parsing with the extracted rules works robustly, and the coverage of a manually-developed HPSG grammar for Penn Treebank was greatly increased with a little overgeneration."
W01-1510,Resource Sharing Amongst {HPSG} and {LTAG} Communities by a Method of Grammar Conversion between {FB}-{LTAG} and {HPSG},2001,23,0,2,1,4617,naoki yoshinaga,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques."
P99-1075,Packing of Feature Structures for Efficient Unification of Disjunctive Feature Structures,1999,13,7,1,1,5928,yusuke miyao,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a method for packing feature structures, which automatically collapses equivalent parts of lexical/phrasal feature structures of HPSG into a single packed feature structure. This method avoids redundant repetition of unification of those parts. Preliminary experiments show that this method can significantly improve a unification speed in parsing."
W98-0127,Packing of feature structures for optimizing the {HPSG}-style grammar translated from {TAG},1998,0,1,1,1,5928,yusuke miyao,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
W98-0141,Translating the {XTAG} {E}nglish grammar to {HPSG},1998,2,22,3,0,35318,yuka tateisi,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
