2020.acl-main.729,D19-1029,0,0.0199932,"o manipulate it is through its GUIs. These hurdles while working with a vast array of existing apps are like physical obstacles that cannot be ignored and must be negotiated contextually in their given environment. 7 Conclusion Our work provides an important first step on the challenging problem of grounding natural language instructions to mobile UI actions. Our decomposition of the problem means that progress on either can improve full task performance. For example, action span extraction is related to both semantic role labeling (He et al., 2018) and extraction of multiple facts from text (Jiang et al., 2019) and could benefit from innovations in span identification and multitask learning. Reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in UIs and improve direct grounding from hidden state representations. Lastly, our work provides a technical foundation for investigating user experiences in language-based human computer interaction. Acknowledgements We would like to thank our anonymous reviewers for their insightful comments that improved the paper. Many thanks to the Google Data Compute team, especially Ashwin Kakarl"
2020.acl-main.729,P09-1010,0,0.102854,"fforts and accomplish tasks involving complex sequences of actions. This takes advantage of the abilities of different members of a speech community, e.g. a child asking a parent for a cup she cannot reach, or a visually impaired individual asking for assistance from a friend. Building computational agents able to help in such interactions is an important goal that requires true language grounding in environments where action matters. An important area of language grounding involves tasks like completion of multi-step actions in a graphical user interface conditioned on language instructions (Branavan et al., 2009, 2010; Liu et al., 2018; Gur et al., 2019). These domains matter for accessibility, where language interfaces could help visually impaired individuals perform tasks with Operation CLICK CLICK CLICK Object OBJ_2 OBJ_6 OBJ_5 INPUT OBJ_9 Argument [Starbucks] Executable actions based on the screen at each step Figure 1: Our model extracts the phrase tuple that describe each action, including its operation, object and additional arguments, and grounds these tuples as executable action sequences in the UI. interfaces that are predicated on sight. This also matters for situational impairment (Sarsen"
2020.acl-main.729,P10-1129,0,0.0920187,"Missing"
2020.acl-main.729,P18-2058,0,0.0246327,"h, each app is opaque to the outside world and the only way to manipulate it is through its GUIs. These hurdles while working with a vast array of existing apps are like physical obstacles that cannot be ignored and must be negotiated contextually in their given environment. 7 Conclusion Our work provides an important first step on the challenging problem of grounding natural language instructions to mobile UI actions. Our decomposition of the problem means that progress on either can improve full task performance. For example, action span extraction is related to both semantic role labeling (He et al., 2018) and extraction of multiple facts from text (Jiang et al., 2019) and could benefit from innovations in span identification and multitask learning. Reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in UIs and improve direct grounding from hidden state representations. Lastly, our work provides a technical foundation for investigating user experiences in language-based human computer interaction. Acknowledgements We would like to thank our anonymous reviewers for their insightful comments that improved the paper. Many"
2020.acl-main.729,D17-1018,0,0.396291,"r, o, u} (8) As shown in Figure 3, qjy indicates task-specific query vectors for y∈{r, o, u}. They are computed as qjy =φ(qj , θy )Wy , a multi-layer perceptron followed by a linear transformation. θy and Wy are trainable parameters. We use separate parameters for each of r, o and u. Wy ∈ R|φy |×|h |where |φy | is the output dimension of the multi-layer perceptron. The alignment function α(·) scores how a query vector qjy matches a span whose vector representation hb:d is computed from encodings hb:d . Span Representation. There are a quadratic number of possible spans given a token sequence (Lee et al., 2017), so it is important to design a fixed-length representation hb:d of a variablelength token span that can be quickly computed. Beginning-Inside-Outside (BIO) (Ramshaw and Marcus, 1995)–commonly used to indicate spans in tasks such as named entity recognition–marks whether each token is beginning, inside, or outside a span. However, BIO is not ideal for our task because subsequences for describing different actions can overlap, e.g., in click X and Y, click participates in both actions click X and click Y. In our experiments we consider several recent, more flexible span representations (Lee et"
2020.acl-main.729,D15-1166,0,0.0258883,"sed to indicate spans in tasks such as named entity recognition–marks whether each token is beginning, inside, or outside a span. However, BIO is not ideal for our task because subsequences for describing different actions can overlap, e.g., in click X and Y, click participates in both actions click X and click Y. In our experiments we consider several recent, more flexible span representations (Lee et al., 2016, 2017; Li et al., 2019) and show their impact in Section 5.2. With fixed-length span representations, we can use common alignment techniques in neural networks (Bahdanau et al., 2014; Luong et al., 2015). We use the dot product between the query vector and the span representation: α(qjy , hb:d )=qjy · hb:d At each step of decoding, we feed the previously decoded phrase tuples, a ¯&lt;j into the decoder. We can use the concatenation of the vector representations of the three elements in a phrase tuple or the sum their vector representations as the input for each decoding step. The entire phrase tuple extraction model is trained by minimizing the softmax cross entropy loss between the predicted and ground-truth spans of a sequence of phrase tuples. 4.2 Grounding Model Having computed the sequence"
2020.acl-main.729,L18-1683,0,0.0848308,"steps for 3 tasks, but did not skip steps for any tasks. These errors could be caused by different language styles manifested by the three datasets. Synthesized commands in R ICO SCA tend to be brief. Instructions in A NDROID H OW T O seem to give more contextual description and involve diverse language styles, while P IXEL H ELP often has a more consistent language style and gives concise description for each step. 6 Related Work Previous work (Branavan et al., 2009, 2010; Liu et al., 2018; Gur et al., 2019) investigated approaches for grounding natural language on desktop or web interfaces. Manuvinakurike et al. (2018) contributed a dataset for mapping natural language instructions to actionable image editing commands in Adobe Photoshop. Our work focuses on a new domain of grounding natural language instructions into executable actions on mobile user interfaces. This requires addressing modeling challenges due to the lack of paired natural language and action data, which we supply by harvesting rich instruction data from the web and synthesizing UI commands based on a large scale Android corpus. Our work is related to semantic parsing, particularly efforts for generating executable outputs such as SQL queri"
2020.acl-main.729,D17-1106,0,0.380579,"Missing"
2020.acl-main.729,W95-0107,0,0.0603042,"inear transformation. θy and Wy are trainable parameters. We use separate parameters for each of r, o and u. Wy ∈ R|φy |×|h |where |φy | is the output dimension of the multi-layer perceptron. The alignment function α(·) scores how a query vector qjy matches a span whose vector representation hb:d is computed from encodings hb:d . Span Representation. There are a quadratic number of possible spans given a token sequence (Lee et al., 2017), so it is important to design a fixed-length representation hb:d of a variablelength token span that can be quickly computed. Beginning-Inside-Outside (BIO) (Ramshaw and Marcus, 1995)–commonly used to indicate spans in tasks such as named entity recognition–marks whether each token is beginning, inside, or outside a span. However, BIO is not ideal for our task because subsequences for describing different actions can overlap, e.g., in click X and Y, click participates in both actions click X and click Y. In our experiments we consider several recent, more flexible span representations (Lee et al., 2016, 2017; Li et al., 2019) and show their impact in Section 5.2. With fixed-length span representations, we can use common alignment techniques in neural networks (Bahdanau et"
2020.acl-main.729,N18-1203,0,0.0555317,"Missing"
2020.emnlp-main.356,2020.tacl-1.30,0,0.0227507,"2009). The issue is particularly severe for VLN. Chen and Mooney (2011) translated(∼1K) English navigation instructions into Chinese for a game-like simulated 3D environment. Otherwise, all publicly available VLN datasets we are aware of have English instructions. To enable multilingual progress on VLN, RxR includes instructions for three typologically diverse languages: English (en), Hindi (hi), and Telugu (te). The English portion includes instructions by speakers in the USA (en-US) and India (en-IN). Unlike Chen and Mooney (2011) and like the TyDiQA multilingual question answering dataset (Clark et al., 2020), RxR’s instructions are not translations: all instructions are created from scratch by native speakers. This especially matters for VLN, as different languages encode spatial and temporal information in idiosyncratic ways–e.g., how contact/support relationships are expressed (Munnich et al., 2001), frame of reference (Haun et al., 2011), and how temporal accounts are expressed (Bender and Beller, 2014). Scale. Embodied language tasks suffer from a relative paucity of training data; for VLN, this has 1 https://github.com/google-research-datasets/RxR Number of: Includes: Lang Instruct Words Pat"
2020.emnlp-main.356,2020.emnlp-main.271,0,0.0268587,"lusion RxR represents a significant evolution in the scale, scope and possibilities for research on embodied language agents in simulated, photo-realistic 3D environments. RxR’s paths better ensure that language itself will play a fundamental role in better agents. Evaluating on three typologically diverse languages will help the community avoid overfitting to a particular language and dataset. We have only begun to explore the possibilities opened up by pose traces. Whereas others have retro-actively refined R2R’s annotations to get alignments between sub-instructions and panorama sequences (Hong et al., 2020), RxR provides wordlevel alignments to specific pixels in panoramas. This is obtained as a by-product of significant work on the annotation tooling itself and designing the process to be more natural for Guides. Finally, every instruction is accompanied by a Follower demonstration, including a perspective camera pose trace that shows a play-by-play account of how a human interpreted the instructions given their position and progress through the path. We have shown that these can help with agent training, but they also open up new possibilities for studying grounded language pragmatics in the V"
2020.emnlp-main.356,D19-1159,0,0.0985325,"1 https://github.com/google-research-datasets/RxR Number of: Includes: Lang Instruct Words Paths Text Ground Demos CVDN 1 2K† 167K 7K X R2R 1 22K 625K 7K X Touchdown 1 9K 1.0M 9K X X‡ REVERIE 1 22K 388K 7K X X‡ RxR 3 126K 9.8M 16.5K X X X † The number of dialogues. ‡ Grounding limited to one object per instruction. Table 1: VLN dataset comparison. RxR is larger, multilingual, and includes dense spatiotemporal groundings (Ground) and follower demonstrations (Demos). led to a focus on data augmentation (Fried et al., 2018; Tan et al., 2019), pre-training (Wang et al., 2019; Huang et al., 2019; Li et al., 2019), multi-task learning (Wang et al., 2020) and better generalization through piece-wise curriculum design (Zhu et al., 2020). To address this shortage, for each language RxR contains 14K paths with 3 instructions per path, for a total of 126K instructions and 10M words (based on whitespace tokenization). As illustrated in Table 1, this is an order of magnitude larger than previous datasets. Fine-Grained Grounding. Like R2R, RxR’s instructions are collected by immersing Guide annotators in a simulated first-person environment backed by the Matterport3D dataset (Chang et al., 2017) and asking the"
2020.splu-1.7,2020.emnlp-main.356,1,0.815314,"Missing"
2020.splu-1.7,D18-1287,1,0.888381,"Missing"
2020.splu-1.7,D17-1106,1,0.89166,"Missing"
2020.splu-1.7,H93-1005,0,0.397591,"). We compare our model results to those given in Chen et al. (2019) and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison. 1 Jason Baldridge Google Research jridge@google.com Introduction Following natural language navigation instructions in visual environments requires addressing multiple challenges in dynamic, continuously changing environments, including language understanding, object recognition, grounding and spatial reasoning. Until recently, the most commonly studied domains were map-based (Thompson et al., 1993) or game-like (Macmahon et al., 2006; Misra et al., 2017, 2018; Hermann et al., 2017; Hill et al., 2017). These environments enabled substantial progress, but the complexity and diversity of the visual input they provide is limited. This greatly simplifies both the language and vision challenges. To address this, recent tasks based on simulated environments include photo-realistic visual input, such as Roomto-Room (R2R; Anderson et al., 2018), Talk-the1 https://www.google.com/help/terms_ maps/ 56 Proceedings of the Third International Workshop on Spatial Language Understanding (SpLU 2020), pag"
2020.splu-1.7,2020.findings-emnlp.62,0,0.0732223,"Missing"
2021.alvr-1.5,D19-1218,0,0.0631463,"Missing"
2021.alvr-1.5,2020.emnlp-main.59,1,0.736294,"Chang et al., 2017; Mirowski et al., 2019; Mehta et al., 2020; Xia et al., 2018; Straub et al., 2019) has galvanized interest in developing embodied navigation agents that can operate in complex human environments. Based on these environments, annotations have been collected for a variety of tasks including navigating to a particular class of object (ObjectNav) (Batra et al., 2020), navigating from language instructions aka visionand-language navigation (VLN) (Anderson et al., 2018b; Chen et al., 2019; Qi et al., 2020; Ku et al., 2020), and vision-and-dialog navigation (Thomason et al., 2020; Hahn et al., 2020). To date, most of these data collection efforts have required the development of custom annotation tools. ∗ 1 First two authors contributed equally. github.com/google-research/pangea 29 Proceedings of the Second Workshop on Advances in Language and Vision Research, pages 29–33 ©2021 Association for Computational Linguistics Figure 1: Screenshots of the PanGEA Guide and Follower interfaces. In the Guide task (left), Guides explore a given path while attempting to create a navigation instruction for others to follow. Guides can pause and restart the audio recording at any time. After recording"
2021.alvr-1.5,2021.eacl-main.111,1,0.849457,"pause and restart the audio recording at any time. After recording is completed, Guides transcribe their own audio. In the Follower task (right), annotators listen to a Guide’s instructions and attempt to follow the intended path. Followers can skip around the Guide’s audio using the audio waveform at bottom right. In both tasks, PanGEA tracks the annotators virtual camera pose and automatically aligns it with the Guide’s audio transcript. which is the largest VLN dataset by an order of magnitude. PanGEA was also used to perform human evaluations of model-generated navigation instructions in Zhao et al. (2021). It could be trivially adapted to other tasks that combine annotation with movement, such as annotating walking tours, or finding and labeling particular landmarks or objects. We next describe PanGEA’s capabitilities in more detail. In the final section we share some best practices learned from using PanGEA to collect RxR, which required more than 20,000 annotation hours. 2 scription results. During the Guide task, in parallel to the annotator’s voice recording, PanGEA captures a timestamped record of the annotator’s virtual camera movements, which we call a pose trace. By default, PanGEA is"
2021.alvr-1.5,2020.emnlp-main.356,1,0.929953,"on et al. (2018b). However, compared to similar annotation tools, PanGEA includes substantial additional capabilities, notably: • annotation via voice recording (in addition to text entry) • virtual pose tracking to record what annotators look at • utilities for aligning a transcript of the words heard or uttered by each annotator with their visual perceptions and actions • integration with cloud database and storage platforms • a modular API facilitating easy extension to new tasks and new environments PanGEA has already been used in two papers. It was used to collect Room-Across-Room (RxR) (Ku et al., 2020), a dataset of human-annotated navigation instructions in English, Hindi and Telugu PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks – collecting"
2021.eacl-main.111,W14-3348,0,0.0146004,"split, 340 in the valseen split (same environments, new paths), and an additional 783 paths in the val-unseen split (new environments, new paths). However, our findings are also relevant for similar datasets such as Touchdown (Chen et al., 2019; Mehta et al., 2020), CVDN (Thomason et al., 2019), REVERIE (Qi et al., 2020), and the multilingual Room-acrossRoom (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully model-based approach combining large-scale synthetic pretraining and domain specific finetuning. However, all of the aforementioned metrics are reference-based, and none is specifically designed for assessing navigation instructions ass"
2021.eacl-main.111,N19-1423,0,0.06028,"the multilingual Room-acrossRoom (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully model-based approach combining large-scale synthetic pretraining and domain specific finetuning. However, all of the aforementioned metrics are reference-based, and none is specifically designed for assessing navigation instructions associated with 3D trajectories for an embodied agent, which requires not only languageto-vision grounding but also correct sequencing. Instruction-Trajectory Compatibility Models Our model builds on that of Huang et al. (2019), but differs in loss (using focal and contrastive 1303 losses), input features (adding action and"
2021.eacl-main.111,E12-1077,0,0.0245762,"Missing"
2021.eacl-main.111,P19-1181,1,0.890226,"Missing"
2021.eacl-main.111,2020.emnlp-main.356,1,0.549971,"s. Most recent efforts (e.g., Fu et al., 2019; Huang et al., 2019; Jain et al., 2019; Wang et al., 2019, etc.) have used the Room-to-Room (R2R) dataset (Anderson et al., 2018b), which contains 4675 unique paths in the train split, 340 in the valseen split (same environments, new paths), and an additional 783 paths in the val-unseen split (new environments, new paths). However, our findings are also relevant for similar datasets such as Touchdown (Chen et al., 2019; Mehta et al., 2020), CVDN (Thomason et al., 2019), REVERIE (Qi et al., 2020), and the multilingual Room-acrossRoom (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully mo"
2021.eacl-main.111,D19-1159,0,0.0140822,"ns given by people. In the context of Vision-and-Language Navigation (VLN) datasets such as Room-to-Room (R2R) (Anderson et al., 2018b), models for generating navigation instructions have improved agents’ wayfinding performance in at least two ways: (1) by synthesizing new instructions for data augmentation (Fried et al., 2018; Tan et al., 2019), and (2) by fulfilling the role of a probabilistic speaker in a pragmatic reasoning setting (Fried et al., 2018). Such data augmentation is so effective that it is nearly ubiquitous in the best performing agents (Wang et al., 2019; Huang et al., 2019; Li et al., 2019). To make further advances in the generation of visually-grounded navigation instructions, accurate evaluation of the generated text is essential. However, the performance of existing instruction generators has not yet been evaluated using human wayfinders, and the efficacy of the automated evaluation metrics used to develop them has not been established. This paper addresses both gaps. To establish benchmarks for navigation instruction generation, we evaluate existing English models (Fried et al., 2018; Tan et al., 2019) using human wayfinders. These models are effective for data augmentation"
2021.eacl-main.111,W04-1013,0,0.0616287,"me environments, new paths), and an additional 783 paths in the val-unseen split (new environments, new paths). However, our findings are also relevant for similar datasets such as Touchdown (Chen et al., 2019; Mehta et al., 2020), CVDN (Thomason et al., 2019), REVERIE (Qi et al., 2020), and the multilingual Room-acrossRoom (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully model-based approach combining large-scale synthetic pretraining and domain specific finetuning. However, all of the aforementioned metrics are reference-based, and none is specifically designed for assessing navigation instructions associated with 3D tra"
2021.eacl-main.111,P02-1040,0,0.109747,"4675 unique paths in the train split, 340 in the valseen split (same environments, new paths), and an additional 783 paths in the val-unseen split (new environments, new paths). However, our findings are also relevant for similar datasets such as Touchdown (Chen et al., 2019; Mehta et al., 2020), CVDN (Thomason et al., 2019), REVERIE (Qi et al., 2020), and the multilingual Room-acrossRoom (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully model-based approach combining large-scale synthetic pretraining and domain specific finetuning. However, all of the aforementioned metrics are reference-based, and none is specifically designed for a"
2021.eacl-main.111,C10-1108,0,0.0256744,"nstructions and human evaluation data we collected. 2 Related Work Navigation Instruction Generation Until recently, most methods for generating navigation instructions were focused on settings in which a system has access to a map representation of the environment, including the locations of objects and named items (e.g. of streets and buildings) (Richter and Klippel, 2005). Some generate route instructions interactively given the current position and goal location (Dr¨ager and Koller, 2012), while others provide in-advance instructions that must be more robust to possible misinterpretation (Roth and Frank, 2010; Mast and Wolter, 2013). Recent work has focused on instruction generation to improve the performance of wayfinding agents. Two instruction generators, SpeakerFollower (Fried et al., 2018) and EnvDrop (Tan et al., 2019), have been widely used for R2R data augmentation. They provide ∼170k new instruction-trajectory pairs sampled from training environments. Both are seq-to-seq models with attention. They take as input a sequence of panoramas grounded in a 3D trajectory, and output a textual instruction intended to describe it. Vision-and-Language Navigation For VLN, embodied agents in 3D enviro"
2021.eacl-main.111,2020.acl-main.704,0,0.0183239,"om (RxR) dataset (Ku et al., 2020). Text Generation Metrics There are many automated metrics that assess textual similarity; we focus on five that are extensively used in the context of image captioning: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). More recently, model- and semi-model-based metrics have been proposed. BERTScore (Zhang et al., 2019) takes a semi-model-based approach to compute token-wise similarity using contextual embeddings learned with BERT (Devlin et al., 2019). BLEURT (Sellam et al., 2020) is a fully model-based approach combining large-scale synthetic pretraining and domain specific finetuning. However, all of the aforementioned metrics are reference-based, and none is specifically designed for assessing navigation instructions associated with 3D trajectories for an embodied agent, which requires not only languageto-vision grounding but also correct sequencing. Instruction-Trajectory Compatibility Models Our model builds on that of Huang et al. (2019), but differs in loss (using focal and contrastive 1303 losses), input features (adding action and geometry representation), and"
2021.eacl-main.111,N19-1268,0,0.368923,"ied problem with clear practical applications (Richter and Klippel, 2005). Whereas earlier work sought to create instructions for human wayfinders, recent work has focused on using instruction-generation models to improve the performance of agents that follow instructions given by people. In the context of Vision-and-Language Navigation (VLN) datasets such as Room-to-Room (R2R) (Anderson et al., 2018b), models for generating navigation instructions have improved agents’ wayfinding performance in at least two ways: (1) by synthesizing new instructions for data augmentation (Fried et al., 2018; Tan et al., 2019), and (2) by fulfilling the role of a probabilistic speaker in a pragmatic reasoning setting (Fried et al., 2018). Such data augmentation is so effective that it is nearly ubiquitous in the best performing agents (Wang et al., 2019; Huang et al., 2019; Li et al., 2019). To make further advances in the generation of visually-grounded navigation instructions, accurate evaluation of the generated text is essential. However, the performance of existing instruction generators has not yet been evaluated using human wayfinders, and the efficacy of the automated evaluation metrics used to develop them"
2021.eacl-main.249,S17-2001,1,0.926212,"rating. Caption-caption and image-image candidates are referred to as C2C and I2I, respectively. I2I pairs are selected with the above other-modality method. For C2C pairs, we sample half the pairs using the other-modality method and half from within cocaptions. The latter introduces (mostly) positive associations between caption pairs describing the same image. This gives a balanced set of caption pairs describing same and different images. Pairs in C2C and I2I are scored by in-house raters using a continuous scale between 0 and 5. We adopt the widely used Semantic Textual Similarity (STS) (Cer et al., 2017) for text pairs and extend it to images to define Semantic Image Similarity (SIS). To recognize that this is a graded (rather than discrete) judgment, we encouraged raters to select scores like 1.3 and obtain the final score for a pair as the average of five individual ratings. Intermodality We select caption-image candidates C2I based on human ratings for I2I and C2C pairs. We mainly seek new positive matches like those identified by annotators in Ilharco et al. (2019). For each I2I pair (ij , ik ), a C2I pair (ck , ij ) is generated, where ck is a MS-COCO caption for ik . We generate pairs f"
2021.eacl-main.249,D14-1162,0,0.0885562,"vn ) and C (c1 ...cn ) (the latter representing cocaption groups of five captions each). Each item is encoded with an off-the-shelf unimodal model. Cosine similarity between items defines two symmetric matrices: S C (pairwise caption similarities) and S V (pairwise image similarities). The diagonals are set to zero to not sample identical items. We encode images with Graph-RISE (486) and construct S I , the image-based similarity for pairs of co-caption groups. We encode captions with Universal Sentence Encoder (USE) (Cer et al., 2018) and average bag of words (BoW) based on GloVe embeddings (Pennington et al., 2014). Co-caption representations are averaged to create a single representation. From these, we construct S C , the caption-based similarity for images pairs. USE and BoW embeddings produce two S C matrices, but we gloss over this detail below. We use S C to select image pairs and S I for caption pairs. Because of the cross-modal semantic gap, diversity and size of the underlying data, these pairs exhibit a wide range of similarity. Selecting the five most similar items (according to modelbased S V and S C ) thus produces good representation of varying amounts of similarity as judged by people. Be"
2021.eacl-main.249,W10-0721,0,0.0606111,"nstrates language’s power to improve image representations (Juan et al., 2020). Learning representations for both vision and language jointly should be even more effective—indeed, much progress has been made on such cross-modal learning using image captioning data (Karpathy and Li, 2015; Harwath and Glass, 2017; Faghri et al., 2018; Li et al., 2019). However, it is not yet clear whether learning representations in multimodal contexts improves performance within as well as across modalities as there are no datasets ideally suited for this at present. Image captioning datasets such as Flickr8k (Rashtchian et al., 2010), Flickr30k (Young et al., 2014), Multi30k (Elliott et al., 2016), Microsoft Common Objects in COntext (MS-COCO) (Lin et al., 2014), and Conceptual Captions (Sharma et al., 2018) only capture relationships between images and textual captions created for them. They miss many valid relationships between unassociated images and captions, from captions to other captions, and from images to other images. We address this gap with Crisscrossed Captions (CxC, exemplified in Figure 1), a dataset with graded, denser annotations for relationships between and among captions and images in the MS-COCO evalu"
2021.findings-emnlp.293,W17-4718,0,0.0209667,"guages. web; we call their dataset as Mined Bilingual Translation (MBT) Pairs. It has 6 billion pairs (up to 100 million per language) for 109 languages. 2.2 Evaluation datasets Flickr30K (Young et al., 2014) has 31k images, with five English captions per image. Multi30K extends Flickr30k with German, French, and Czech captions. Elliott et al. (2016) introduces German annotations by 1) translating some Flickr30k English captions and 2) crowdsourcing new German captions for Flickr30K images. Following prior work (Burns et al., 2020), we report results on the independent 5 captions/image split. Elliott et al. (2017) and Barrault et al. (2018) further extend the dataset by collecting human translations of English Flickr30k captions to French and Czech. MS-COCO (Lin et al., 2014) also has five human generated English captions per image. We report results on both the 1k and 5k splits defined by Karpathy and Li (2015). The STAIR dataset (Yoshikawa et al., 2017) adds human crowdsourced Japaneses captions for MSCOCO images. XTD Aggarwal and Kale (2020) created the Cross-lingual Test Dataset for evaluating multimodal retrieval models. XTD does not include any training examples, but it supports retrieval evaluat"
2021.findings-emnlp.293,D19-1632,0,0.05807,"Missing"
2021.findings-emnlp.293,2020.wmt-1.21,0,0.0344086,"Missing"
2021.findings-emnlp.293,I13-1185,0,0.0123457,"l multimodal repgests that it may be worth trying to improve mul- resentations. See Appendix A.5 for more examples. timodal, multilingual representations for a given Across languages, for both Image→Text relower-resource language by pivoting on a well- trieval and Text→Image, we observed that MUresourced language that is linguistically related RAL displays better fidelity to the concepts deor which has been in significant contact with it– scribed in the image and text. For instance, in Fig. similar to previous studies for machine translation 4 ALIGN’s top five results are somewhat scattered, (Islam and Hoenen, 2013). whereas MURAL’s results cohere better around Retrieval Error Analysis. We analyzed zero- boats with people (water taxis) near land (islands). shot retrieved examples on WIT for ALIGN-BASE For under-resourced languages like Hindi, MUand MURAL-BASE for English (en), Hindi (hi), RAL shows an improvement with respect to re3456 7 Figure 6: Image → Text examples where recognizing text in the input image would greatly help. trieving results that are culturally more suited to the language (Fig. 5). Finally, with both models, retrieval for some examples could greatly benefit from better recognition o"
2021.findings-emnlp.293,2005.mtsummit-papers.11,0,0.420443,"Missing"
2021.findings-emnlp.293,D19-1167,1,0.888562,"comes back with better text-text and image- Canonical Correlation Analysis (SVCCA) (Raghu image scores. This indicates that MURAL’s text- et al., 2017), which allows similarity scores to text task balances both encoders better than a be computed between languages. Using SVCCA loss focused only on image-text pairs. Similarly, scores computed for 100 languages, we plot a 2MURAL-LARGE beats ALIGN-L2 for both text-text dimensional visualization using Laplacian Eigenand image-image retrieval, despite the fact that maps (Belkin and Niyogi, 2003). Following ALIGN-L2 uses a much larger image encoder. Kudugunta et al. (2019), we do so for a subset of languages belonging to the Germanic, Romance, The correlation results given in Table 6 tell an Slavic, Uralic, Finnic, Celtic, and Finno-Ugric laninteresting story. Contrary to intuition and retrieval guage families (widely spoken in Europe and Westresults, Semantic Image Similarity (SIS) seems connected with multilinguality, as all Alt-Text mod- ern Asia). For a fair evaluation, we artificially creels (ALIGN-BASE, MURAL-BASE, MURAL-LARGE) ate a multilingual aligned dataset by using Google’s Translation system to translate 1K English captions perform nearly the same"
2021.findings-emnlp.293,N19-1392,0,0.046076,"Missing"
2021.findings-emnlp.293,2021.eacl-main.249,1,0.905712,"ders for both language and images by combining both image-text matching and text-text matching tasks, using scalable dual encoder models trained with contrastive losses. detection, machine translation, bilingual dictionaries and many losses. In contrast, multimodal dual encoders can be learned directly on noisy, massive image-caption datasets using a simple loss based on in-batch bidirectional retrieval (Jia et al., 2021; Radford et al., 2021). These support efficient retrieval via approximate nearest neighbors search (Guo et al., 2020) and can predict similarity within and across modalities (Parekh et al., 2021). With MURAL: MUltimodal, MUltitask Representations Across Languages (Fig. 1), we explore dual encoder learning from both image-caption and translation pairs at massive scale: 6 billion translation pairs (Feng et al., 2020) and 1.8 billion imagecaption pairs (Jia et al., 2021). We particularly seek 1 Introduction to improve performance for under-resourced languages. Addressing this was infeasible until now Multilingual captions for images provide indirect because existing multilingual image-text datasets— but valuable associations between languages (Gella Multi30k (Elliott et al., 2016)), STAI"
2021.findings-emnlp.293,L18-1182,0,0.0562082,"Missing"
2021.findings-emnlp.293,2021.eacl-main.115,0,0.0773967,"Missing"
2021.findings-emnlp.293,P18-1238,0,0.0379476,"Missing"
2021.findings-emnlp.293,2020.acl-main.252,1,0.885498,"Missing"
2021.findings-emnlp.293,P17-2066,0,0.142696,"ith MURAL: MUltimodal, MUltitask Representations Across Languages (Fig. 1), we explore dual encoder learning from both image-caption and translation pairs at massive scale: 6 billion translation pairs (Feng et al., 2020) and 1.8 billion imagecaption pairs (Jia et al., 2021). We particularly seek 1 Introduction to improve performance for under-resourced languages. Addressing this was infeasible until now Multilingual captions for images provide indirect because existing multilingual image-text datasets— but valuable associations between languages (Gella Multi30k (Elliott et al., 2016)), STAIR (Yoshikawa et al., 2017). Burns et al. (2020) exploit this to et al., 2017), and XTD (Aggarwal and Kale, 2020)– scale multimodal representations to support more languages with a smaller model than prior stud- support only high-resource languages. However, the recent Wikipedia Image-Text (WIT) dataset ies. More recent work learns cross encoder models with multitask training objectives (Ni et al., 2021; (Srinivasan et al., 2021), which covers 108 languages, addresses this gap. Zhou et al., 2021); in these, a single multimodal encoder attends to both inputs and exploits deep Our results, as a whole, demonstrate that ALI"
2021.findings-emnlp.293,2020.lrec-1.789,0,0.0971179,"Missing"
2021.findings-emnlp.293,tiedemann-2012-parallel,0,0.0892307,"Missing"
2021.findings-emnlp.293,Q14-1006,0,0.0413663,"uli´c, 2019)—see Appendix A.2 for a full list. image-image pairs. EOBT has ≈500 million pairs across all languages. Finally, we show that multilingual representaFeng et al. (2020) mine translations from the 3450 Figure 2: Alt-Text language distribution: (left) linear scale, which clearly conveys the skew toward well-resourced languages; (right) log-scale, which provides a better view of under-represented languages. web; we call their dataset as Mined Bilingual Translation (MBT) Pairs. It has 6 billion pairs (up to 100 million per language) for 109 languages. 2.2 Evaluation datasets Flickr30K (Young et al., 2014) has 31k images, with five English captions per image. Multi30K extends Flickr30k with German, French, and Czech captions. Elliott et al. (2016) introduces German annotations by 1) translating some Flickr30k English captions and 2) crowdsourcing new German captions for Flickr30K images. Following prior work (Burns et al., 2020), we report results on the independent 5 captions/image split. Elliott et al. (2017) and Barrault et al. (2018) further extend the dataset by collecting human translations of English Flickr30k captions to French and Czech. MS-COCO (Lin et al., 2014) also has five human g"
2021.findings-emnlp.293,2020.acl-main.148,0,0.0502601,"Missing"
2021.findings-emnlp.293,2020.acl-demos.12,1,0.592887,"nt cross-modal retrieval tasks. Ranking a set of items in a manner consistent with human similarity judgments is arguably a harder task than getting a single paired item to be more similar than nearly all others. These two perspectives may reveal useful tensions in finer-grained semantic distinctions. In fact, it is with these correlation measures that we expect cross-encoders to shine compared to the retrievaloriented dual encoders. WIT-en (18, 19, 20 vs 21). XTD. As shown in Table 4, both ALIGN and MURAL obtain massive gains over the best strategy reported by Aggarwal and Kale (2020)— mUSE (Yang et al., 2020) with a multimodal metric loss (M3L). MURAL-LARGE shows especially strong performance across all languages. Note that we only obtained these scores after all experimentation was done on other datasets—this is methodologically important as there is neither training data nor development data for XTD. Crisscrossed Captions. For CxC image-text retrieval (Table 5), ALIGN-L2 scores highest across 5 Analysis all metrics; it is the largest model and was trained only on English Alt-Text. ALIGN-BASE also beats Embedding Visualization. We visualize multiMURAL-BASE for image-text retrieval, but the lat- l"
2021.findings-emnlp.293,D19-1382,1,0.834556,"re the potential of large, image-text contrastive loss, and the other for text- diverse translations pairs for learning better multext contrastive loss (more details in A.1). timodal encoders, including a single multilingual Fine-tuning: single-task vs. multi-task. Our text encoder. We compare this strategy to the wellprimary goal with MURAL is to improve zero-shot established, effective baselines of translate-train performance by learning with both image-text and and translate-test using a strong Neural Machine text-text pairs. Nevertheless, fine-tuning has a large Translation (NMT) system3 (Yang et al., 2019b). impact on performance for any given dataset. After Translate-train: To reduce the heavy bias toinitial experiments, we find that single-task fine- ward English and to support other languages for tuning using image-text pairs performed slightly models training only on image-text pairs (e.g. for better than multitask finetuning using co-captions. 2 The vocabulary is built using the standard wpm library For further discussion on this comparison, see Ap- from tensorflow_text. 3 pendix A.1. For all models, we report results using https://cloud.google.com/translate 3452 Fine-tuned Zero-shot Mult"
2021.splurobonlp-1.9,W16-1721,1,0.880105,"Missing"
2021.splurobonlp-1.9,D10-1124,0,0.201132,"Missing"
2021.splurobonlp-1.9,D19-5528,0,0.0568069,"ot generalize well to examples with different distributions from training resources. Spatial hierarchies based on containment relations among entities rely heavily on metadata like GeoNames (Kamalloo and Rafiei, 2018). Polygons for geopolitical entities such as city, state, and country (Martins et al., 2015) are perhaps ideal, but these too require detailed metadata for all toponyms, managing non-uniformity of the polygons, and general facility with GIS tools. The Point-to-City (P2C) method applies an iterative k-d tree-based method for clustering coordinates and associating them with cities (Fornaciari and Hovy, 2019b). S2 can represent such hierarchies in various levels without relying on external metadata. Spatial representations Geocoders map text spans to geo-coordinates—a prediction over a continuous space representing the surface of a sphere. We relax the problem from continuous space to discrete space by quantizing the Earth’s surface as a grid and performing multi-class prediction over the grid’s cells. We construct a hierarchical grid using the S2 library.3 S2 projects the six faces of a cube onto the Earth’s surface and each face is recursively divided into 4 quadrants, as shown in Figure 1. Cel"
2021.splurobonlp-1.9,D19-5530,0,0.0488952,"ot generalize well to examples with different distributions from training resources. Spatial hierarchies based on containment relations among entities rely heavily on metadata like GeoNames (Kamalloo and Rafiei, 2018). Polygons for geopolitical entities such as city, state, and country (Martins et al., 2015) are perhaps ideal, but these too require detailed metadata for all toponyms, managing non-uniformity of the polygons, and general facility with GIS tools. The Point-to-City (P2C) method applies an iterative k-d tree-based method for clustering coordinates and associating them with cities (Fornaciari and Hovy, 2019b). S2 can represent such hierarchies in various levels without relying on external metadata. Spatial representations Geocoders map text spans to geo-coordinates—a prediction over a continuous space representing the surface of a sphere. We relax the problem from continuous space to discrete space by quantizing the Earth’s surface as a grid and performing multi-class prediction over the grid’s cells. We construct a hierarchical grid using the S2 library.3 S2 projects the six faces of a cube onto the Earth’s surface and each face is recursively divided into 4 quadrants, as shown in Figure 1. Cel"
2021.splurobonlp-1.9,P18-1119,0,0.0757431,"Missing"
2021.splurobonlp-1.9,D14-1162,0,0.0843442,"Missing"
2021.splurobonlp-1.9,D17-1016,0,0.0292666,"Missing"
2021.splurobonlp-1.9,P16-4022,0,0.0333738,"Missing"
2021.splurobonlp-1.9,N15-1153,0,0.075226,"al., 2018a), which uses arbitrary square-degree cells, e.g. 2◦ -by-2◦ cells (∼48K km2 ). Unlike previous work that relies on external gazetteer information, MLG is more flexible and can predict geolocation only from context. For instance, it predicts the location of Manhattan from the surrounding words (The five boroughs - Brooklyn, Queens, the Bronx and Geocoding is the task of resolving location references in text to geographic coordinates or regions. It is often studied in social networks, where metadata and the network itself provide additional non-textual signals (Backstrom et al., 2010; Rahimi et al., 2015). If locations can be mapped to an entity in a knowledge graph, toponym resolution – a special case of entity resolution – can be used to resolve references to locations. Past work used heuristics based on location popularity (Leidner, 2007) and distance between candidate locations (Speriosu and Baldridge, 2013), as well as learned associations from text to locations. However, such approaches have a strong bias for highly-populated locations, especially for social media. We present Multi-Level Geocoder (MLG, Fig. 1), a model that learns spatial language representations and maps toponyms to coo"
2021.splurobonlp-1.9,D12-1137,1,0.734468,"e Pixelation (Melo and Martins, 2015) are alternatives that preserve cell size better, but S2 is easier to work with and has strong, standard tooling. Our experiments go as far as S2 level eight (of thirty), but our approach is extendable to any level of granularity and could support very fine-grained locations like buildings and landmarks. The built-in hierarchical nature of S2 cells makes it well suited as a scaffold for models that learn and combine evidence from multiple levels. This combines the best of both worlds: specificity at finer levels and aggregation/smoothing at coarser levels. Roller et al. (2012) use adaptive, variable shaped cells based on k-d trees; such grids can adapt to the different In accordance with the nature of the problem over continuous space, studies using bivariate Gaussians on multiple flattened regions (Eisenstein et al., 2010; Priedhorsky et al., 2014)) perform well on distance based metrics, but this involves difficult trade-offs between flattened region sizes and the level of distortion they introduce. Some of the early models used with grid-based representations were probabilistic language models that produce document likelihoods in different geospatial cells (Serd"
2021.splurobonlp-1.9,P13-1144,1,0.879745,"(The five boroughs - Brooklyn, Queens, the Bronx and Geocoding is the task of resolving location references in text to geographic coordinates or regions. It is often studied in social networks, where metadata and the network itself provide additional non-textual signals (Backstrom et al., 2010; Rahimi et al., 2015). If locations can be mapped to an entity in a knowledge graph, toponym resolution – a special case of entity resolution – can be used to resolve references to locations. Past work used heuristics based on location popularity (Leidner, 2007) and distance between candidate locations (Speriosu and Baldridge, 2013), as well as learned associations from text to locations. However, such approaches have a strong bias for highly-populated locations, especially for social media. We present Multi-Level Geocoder (MLG, Fig. 1), a model that learns spatial language representations and maps toponyms to coordinates on Earth’s surface. This geocoder is not restricted to resolving toponyms to specific location entities, but rather to geo-coordinates directly. MLG can thus be extended to any arbitrary location references in future without having to rely on its presence in the gazetteer. For comparative evaluation, we"
2021.splurobonlp-1.9,D14-1039,1,0.897272,"arative evaluation, we use three English toponym resolution datasets from † L7 Projections to multiple levels Introduction ∗ L6 Equal contribution Work done during internship at Google 1 https://s2geometry.io/ 79 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 79–88 August 5–6, 2020. ©2021 Association for Computational Linguistics S2 Level L4 L5 L6 L7 L8 Staten Island - . . . ). Earlier approaches instead relied on a knowledge graph that had Manhattan as an entity. While the hierarchical geolocation model of Wing and Baldridge (2014) over kd-trees has some more finegrained cells, MLG predicts over a much larger set of smaller cells. Furthermore, MLG is a single model that jointly incorporates multiple levels rather than ensembling independent per-cell models for each level. Our main contributions are the following. • We define MLG, a model that jointly predicts cells at multiple levels, including finer-grained cells than previous work. • We show that S2 provides a strong and standardized hierarchical discretization of the Earth’s surface for cell-based geocoders. • We show that it is possible and even preferable to eschew"
2021.splurobonlp-1.9,P11-1096,1,0.884402,"te space by quantizing the Earth’s surface as a grid and performing multi-class prediction over the grid’s cells. We construct a hierarchical grid using the S2 library.3 S2 projects the six faces of a cube onto the Earth’s surface and each face is recursively divided into 4 quadrants, as shown in Figure 1. Cells at each level are indexed using a Hilbert curve. Each S2 cell is represented as a 64-bit unsigned integer and can correspond to areas as small as ≈1cm2 . S2 cells preserve cell size across the globe better than commonly-used degree◦ ◦ square grids (e.g. 1 x1 ) (Serdyukov et al., 2009; Wing and Baldridge, 2011). Hierarchical triangular meshes (Szalay et al., 2007) and Hierarchical Equal Area isoLatitude Pixelation (Melo and Martins, 2015) are alternatives that preserve cell size better, but S2 is easier to work with and has strong, standard tooling. Our experiments go as far as S2 level eight (of thirty), but our approach is extendable to any level of granularity and could support very fine-grained locations like buildings and landmarks. The built-in hierarchical nature of S2 cells makes it well suited as a scaffold for models that learn and combine evidence from multiple levels. This combines the b"
baldridge-etal-2002-leo,J88-1004,0,\N,Missing
baldridge-etal-2002-leo,C88-1012,0,\N,Missing
baldridge-etal-2002-leo,P01-1022,1,\N,Missing
baldridge-etal-2002-leo,P93-1008,1,\N,Missing
baldridge-etal-2002-leo,H93-1008,1,\N,Missing
baldridge-etal-2002-leo,C94-2144,0,\N,Missing
baldridge-etal-2002-leo,P84-1075,0,\N,Missing
baldridge-etal-2002-leo,C00-2097,0,\N,Missing
baldridge-etal-2002-leo,P94-1040,0,\N,Missing
C04-1028,P02-1041,1,0.830537,"es the representation in (2). (2) @e (prove ∧ hTENSEipast ∧ hACTi(m ∧ Marcel) ∧ hPATi(c ∧ comp.)) In this example, e is a nominal that labels the predications and relations for the head prove, and m and c label those for Marcel and completeness, respectively. The relations ACT and PAT represent the dependency roles Actor and Patient, respectively. By using the @ operator, hierarchical terms such as (2) can be flattened to an equivalent conjunction of fixed-size elementary predications (EPs): (3) @e prove ∧ @e hTENSEipast ∧ @e hACTim ∧ @e hPATic ∧ @m Marcel ∧ @c comp. 2.3 Semantic Construction Baldridge and Kruijff (2002) show how HLDS representations can be built via CCG derivations. White (2004) improves HLDS construction by operating on flattened representations such as (3) and using a simple semantic index feature in the syntax. We adopt this latter approach, described below. EPs are paired with syntactic categories in the lexicon as shown in (4)–(6) below. Each atomic category has an index feature, shown as a subscript, which makes a nominal available for capturing syntactically induced dependencies. (4) prove ` (se 
px )/npy : @e prove ∧ @e hTENSEipast ∧ @e hACTix ∧ @e hPATiy (5) Marcel ` npm : @m Marce"
C04-1028,E03-1036,1,0.904986,"perspicuous manner by using our generalized signs. The main outcomes of the proposal are threefold: (1) CCG gains a more flexible and general kind of sign; (2) these signs contain multiple levels that interact in a modular fashion and are built via CCG derivations without increasing parsing complexity; and (3) we use these signs to simplify previous CCG’s accounts of the effects of word order and prosody on information structure. 2 Combinatory Categorial Grammar In this section, we give an overview of syntactic combination and semantic construction in CCG. We use CCG’s multi-modal extension (Baldridge and Kruijff, 2003), which enriches the inventory of slash types. This formalization renders constraints on rules unnecessary and supports a universal set of rules for all grammars. 2.1 Categories and combination Nearly all syntactic behavior in CCG is encoded in categories. They may be atoms, like np, or functions which specify the direction in which they seek their arguments, like (s
p)/np. The latter is the category for English transitive verbs; it first seeks its object to its right and then its subject to its left. Categories combine through a small set of universal combinatory rules. The simplest are appl"
C04-1028,E95-1034,0,0.442775,"ucture, an inherent aspect of the (linguistic) meaning of an utterance. Speakers use information structure to present some parts of that meaning as depending on the preceding discourse context and others as affecting the context by adding new content. Languages may realize information structure using different, often interacting means, such as word order, prosody, (marked) syntactic constructions, or morphological marking (Vallduv´ı and Engdahl, 1996; Kruijff, 2002). The literature presents various proposals for how information structure can be captured in categorial grammar (Steedman, 2000a; Hoffman, 1995; Kruijff, 2001). Here, we model the essential aspects of these accounts in a more perspicuous manner by using our generalized signs. The main outcomes of the proposal are threefold: (1) CCG gains a more flexible and general kind of sign; (2) these signs contain multiple levels that interact in a modular fashion and are built via CCG derivations without increasing parsing complexity; and (3) we use these signs to simplify previous CCG’s accounts of the effects of word order and prosody on information structure. 2 Combinatory Categorial Grammar In this section, we give an overview of syntactic"
C04-1028,J93-4001,0,0.0119353,"982) also use complex signs. However, these signs are monolithic structures which permit information to be Jason Baldridge ICCS, Division of Informatics University of Edinburgh Edinburgh, Scotland jbaldrid@inf.ed.ac.uk freely shared across all dimensions: any given dimension can place restrictions on another. For example, variables resolved during the construction of the logical form can block a syntactic analysis. This provides a clean, unified formal system for dealing with the different levels, but it also can adversely affect the complexity of parsing grammars written in these frameworks (Maxwell and Kaplan, 1993). We thus find two competing perspectives on communication between levels in a sign. In this paper, we propose a generalization of linguistic signs for Combinatory Categorial Grammar (CCG) (Steedman, 2000b). This generalization enables different levels of linguistic information to be represented but limits their interaction in a resource-bounded manner, following White (2004). This provides a clean separation of the levels and allows them to be designed and utilized in a more modular fashion. Most importantly, it allows us to retain the parsing complexity of CCG while gaining the representatio"
C08-1008,J99-2004,0,0.791574,"and a prepositional phrase to the right of that. Supertags convey such detailed syntactic subcategorization information that supertag disambiguation is referred to as almost parsing (Bangalore and Joshi, 1999). Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006). The original motivation for supertags–parse prefiltering for lexicalized grammars–of Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy. Espinosa et al. (2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG. Supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (Birch et al., 2007; Hassan et al., 2007). Supertaggers typically rely on"
C08-1008,W07-0702,0,0.0273605,"arse prefiltering for lexicalized grammars–of Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy. Espinosa et al. (2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG. Supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (Birch et al., 2007; Hassan et al., 2007). Supertaggers typically rely on a significant amount of carefully annotated sentences. As with many problems, there is pressing need to find strategies for reducing the amount of supervision required for producing accurate supertaggers, but as yet, no one has explored the use of weak supervision for the task. In particular, there are many dialog systems which rely on hand-crafted lexicons that both provide a starting point for bootstrapping a supertagger and which could benefit greatly from supertag pre-parse filter. For example, the dialog system used by Kruijff et al."
C08-1008,W06-1620,0,0.0176414,"nguistics The University of Texas at Austin jbaldrid@mail.utexas.edu Abstract to its left, another to its right, and a prepositional phrase to the right of that. Supertags convey such detailed syntactic subcategorization information that supertag disambiguation is referred to as almost parsing (Bangalore and Joshi, 1999). Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006). The original motivation for supertags–parse prefiltering for lexicalized grammars–of Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy. Espinosa et al. (2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG. Supertags have gained further relevance as they are increasingly used as features for oth"
C08-1008,W02-2203,0,0.0571626,"Missing"
C08-1008,J07-4004,0,0.448508,"informed initialization Jason Baldridge Department of Linguistics The University of Texas at Austin jbaldrid@mail.utexas.edu Abstract to its left, another to its right, and a prepositional phrase to the right of that. Supertags convey such detailed syntactic subcategorization information that supertag disambiguation is referred to as almost parsing (Bangalore and Joshi, 1999). Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006). The original motivation for supertags–parse prefiltering for lexicalized grammars–of Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy. Espinosa et al. (2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG. Supertags have gained further"
C08-1008,P96-1011,0,0.132843,"tate (and thus predict the same label for adjacent words). For CCG supertagging, the initialization should discourage diagonalization and establish a preference for some transitions over others. Category transition distribution CCG analyses of sentences are built up from lexical categories combining to form derived categories, until an entire sentence is reduced to a single derived category with corresponding dependencies. One of CCG’s most interesting linguistic properties is it allows alternative constituents. Consider the derivations in Figures 1 and 2, which show a normal form derivation (Eisner, 1996) and fully incremental derivation, respectively. Both produce the same dependencies, guaranteed by the semantic consistency of CCG’s rules (Steedman, 2000). This property of CCG of supporting multiple derivations of the same analysis has been termed spurious ambiguity. However, the extra constituents are anything but spurious: they are implicated in a range of CCG (along with other forms of categorial grammar) linguistic analyses, including coordination, long-distance extraction, intonation, and incremental processing. This all boils down to associativity: just as There are many ways to define"
C08-1008,P08-1022,0,0.0122411,"ediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006). The original motivation for supertags–parse prefiltering for lexicalized grammars–of Bangalore and Joshi (1999) has been realized to good effect: the supertagger of Clark and Curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy. Espinosa et al. (2008) have shown that hypertagging (predicting the supertag associated with a logical form) can improve both speed and accuracy of wide-coverage sentence realization with CCG. Supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (Birch et al., 2007; Hassan et al., 2007). Supertaggers typically rely on a significant amount of carefully annotated sentences. As with many problems, there is pressing need to find strategies for reducing the amount of supervision required for producing accurate supertaggers, but as yet, no one ha"
C08-1008,P07-1094,0,0.323767,"the accuracy of bitag HMMs is not far behind. The goal here is to investigate the relative gains of using CCG-based information in weakly supervised HMM learning. Second, the expectationmaximization algorithm for bitag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions (Banko and Moore, 2004). Third, the model’s simplicity makes it straightforward to test the idea of CCG-initialization on tag transitions. Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths, 2007; Johnson, 2007), which is especially useful in the weakly supervised setting considered here. Following Johnson (2007), I use variational Bayes EM (Beal, 2003) during the M-step for the transition distribution: l+1 θj|i = f (E[ni,j ] + αi ) f (E[ni ] + |C |× αi ) f (v) = exp(ψ(v)) 60 (3) (4) ( ψ(v) = g(v − 12 ) if v > 7 1 v ψ(v + 1) − o.w. 1 7 g(x) ≈ log(x) + − 2 24x 960x4 127 31 − + 8064x6 30720x8 Dataset train dev test (5) (6) E[ni,k ] + |Ei |× 1 |V| E[ni ] + |Ei | Sentences 38,015 5484 5435 erage ambiguity of 1.17 per word and a maximum of 7 tags in train.5 The set of supertags was not red"
C08-1008,W03-2316,1,0.812925,"category indicates that join requires a noun phrase c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 57 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57–64 Manchester, August 2008 ical linear direction in which they seek their arguments. Some example entries from CCGbank are: the := NPnb /N of := (NPNP)/NP of := ((SNP)(SNP)/NP were := (Sdcl NP)/(Spss NP) buy := (Sdcl NP)/NP crafted CCG grammar for OpenCCG (White and Baldridge, 2003). It is important to stress that there are many such uses of CCG and related frameworks which do not rely on first annotating (even a small number of) sentences in a corpus: these define a lexicon that maps from words to categories (supertags) for a particular domain/application. This scenario is a natural fit for learning taggers from tag dictionaries using hidden Markov models with Expectation-Maximization (EM). Here, I investigate such weakly supervised learning for supertagging and demonstrate the importance of proper initialization of the tag transition distributions of the HMM. In partic"
C08-1008,P05-1046,0,0.155926,"mmediate neighbors. For example, six of seven pairs of adjacent lexical categories in the sentence in Figure 1 can combine. Only N PP/NP of board as cannot.3 This observation can be used in different ways by different models for CCG supertagging. For example, discriminative tagging models could include features that capture whether or not the current supertag can combine with the previous one and possibly via which CCG rule. Here, I show how it can be used to provide a non-uniform starting point for the transition distributions θj|i in a first-order Hidden Markov Model. This is similar to how Grenager et al. (2005) use diagonal initialization in an HMM for field segmentation to encourage the model to remain in the same state (and thus predict the same label for adjacent words). For CCG supertagging, the initialization should discourage diagonalization and establish a preference for some transitions over others. Category transition distribution CCG analyses of sentences are built up from lexical categories combining to form derived categories, until an entire sentence is reduced to a single derived category with corresponding dependencies. One of CCG’s most interesting linguistic properties is it allows"
C08-1008,N06-1041,0,0.0611311,"Missing"
C08-1008,P07-1037,0,0.0722938,"Missing"
C08-1008,J07-3004,0,0.566118,"e effective than starting with uniform transitions, especially when the tag dictionary is highly ambiguous. 1 Introduction Supertagging involves assigning words lexical entries based on a lexicalized grammatical theory, such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) Tree-adjoining Grammar (Joshi, 1988), or Head-driven Phrase Structure Grammar (Pollard and Sag, 1994). Supertag sets are larger than part-of-speech (POS) tag sets and their elements are generally far more articulated. For example, the English verb join has the POS VB and the CCG category ((Sb NP)/PP)/NP in CCGbank (Hockenmaier and Steedman, 2007). This category indicates that join requires a noun phrase c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 57 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57–64 Manchester, August 2008 ical linear direction in which they seek their arguments. Some example entries from CCGbank are: the := NPnb /N of := (NPNP)/NP of := ((SNP)(SNP)/NP were := (Sdcl NP)/(Spss NP) buy := (Sdcl NP)/NP crafted CCG grammar for Ope"
C08-1008,D07-1031,0,0.513726,"not far behind. The goal here is to investigate the relative gains of using CCG-based information in weakly supervised HMM learning. Second, the expectationmaximization algorithm for bitag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions (Banko and Moore, 2004). Third, the model’s simplicity makes it straightforward to test the idea of CCG-initialization on tag transitions. Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths, 2007; Johnson, 2007), which is especially useful in the weakly supervised setting considered here. Following Johnson (2007), I use variational Bayes EM (Beal, 2003) during the M-step for the transition distribution: l+1 θj|i = f (E[ni,j ] + αi ) f (E[ni ] + |C |× αi ) f (v) = exp(ψ(v)) 60 (3) (4) ( ψ(v) = g(v − 12 ) if v > 7 1 v ψ(v + 1) − o.w. 1 7 g(x) ≈ log(x) + − 2 24x 960x4 127 31 − + 8064x6 30720x8 Dataset train dev test (5) (6) E[ni,k ] + |Ei |× 1 |V| E[ni ] + |Ei | Sentences 38,015 5484 5435 erage ambiguity of 1.17 per word and a maximum of 7 tags in train.5 The set of supertags was not reduced: any catego"
C08-1008,C04-1080,0,\N,Missing
D07-1041,J95-4004,0,0.0567077,"test it on tagged Middle English text. This leads to tagging accuracy in the low 80’s on Biblical test material and in the 60’s on other Middle English material. Our results suggest that our bootstrapping methods have considerable potential, and could be used to semi-automate an approach based on incremental manual annotation. One approach to get around the annotation bottleneck is to use semi-automation. For example, when producing part-of-speech tags for the Tycho Brahe corpus of Historical Portuguese (Britto et al., 2002), a set of seed sentences was manually tagged, and the Brill tagger (Brill, 1995) was then trained on those and consequently used to tag other sentences. The output was inspected for errors, the tagger was retrained and used again to tag new sentences, for several iterations. 1 Introduction Annotated corpora of historical texts provide an important resource for studies of syntactic variation and change in diachronic linguistics. For example, the Penn-Helsinki Parsed Corpus of Middle English (PPCME) (Kroch and Taylor, 2000) has been used to show the existence of syntactic dialectal differences between northern and southern Middle English (Kroch et al., 2000) and to examine"
D07-1041,W06-1009,0,0.0342433,"Missing"
D07-1041,E03-1071,0,0.194739,"language to a greater degree than the previously mentioned studies that attempted language neutrality; that is, it directly exploits the genetic similarity between the source and target language. Some amount of surface structural similarity between a diachronic dialect and its derivatives is to be expected, and in the case of Middle English and Modern English, such similarities are not negligible. The automation process is further aided through the use of two versions of the Bible, which obviates the need for sentence alignment. The modern Bible is tagged using the C&C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. A simple bigram tagger is trained from the resulting target texts and then used to relabel the same texts as Middle English training material for the C&C tagger. This tagger utilizes a rich set of features and a wider context, so it can exploit surface similarities between the source and target language. By training it with both the original (Modern English) Penn Treebank Wall Street Journal (WSJ) material and our automatically tagged Middle English Wycliffe material"
D07-1041,W05-0807,0,0.193796,"Missing"
D07-1041,J93-1006,0,0.182216,"Missing"
D07-1041,J03-1002,0,0.00336774,"ed to a majority POS tag. For example, the word like which had been assigned four different POS tags, IN, NN, RB, VB, by the C&C tagger in the NET Bible was only mapped to IN since the pairings of the two occurred the most frequently. The result is a mapping from one or more target lexemes to a source lexeme to a majority POS tag. In the case of like, two words from the target, as and lijk, were mapped thereto and to the majority tag IN. Later, we will refer to the Wycliffe text (partially) labeled with tags projected using the Dice coefficient as D ICE 1 TO 1. 3.1.2 GIZA++ alignments Giza++ (Och and Ney, 2003) was also used to derive 1-to-n word alignments between the NET Bible and the Wycliffe Bible. This produces a tagged version of the Wycliffe text which we will refer to as G IZA 1 TO N. In our alignment experiment, we used a combination of IBM Model 1, Model 3, Model 4, and an HMM model in configuring Giza++. G IZA 1 TO N was further processed to remove noise from the transferred tag set by creating a 1-to-1 word alignment: each word in the target Middle English text was given its majority tag based on the assignment of tags to G IZA 1 TO N as a whole. We call this version of the tagged Wyclif"
D07-1041,W06-2008,0,0.145428,"Missing"
D07-1041,W96-0213,0,0.264667,"oothing. In order to diversify the maximum likelihood estimates and provide robustness against the errors of any one alignment method, we concatenate several tagged versions of the Wycliffe Bible with tags projected from each of our methods (D ICE 1 TO 1, G IZA 1 TO N, and G IZA 1 TO 1) and the NET Bible (and its tags from the C&C tagger). 3.3 Training C&C on projected tags The bigram tagger learned from the aligned text has very limited context and cannot use rich features such as prefixes and suffixes of words in making its predictions. In contrast, the C&C tagger, which is based on that of Ratnaparkhi (1996), utilizes a wide range of features and a larger contextual window including the previous two tags and the two previous and two following words. However, the C&C tagger cannot train on texts which are not fully tagged for POS, so we use the bigram tagger to produce a completely labeled version of the Wycliffe text and train the C&C tagger on this material. The idea is that even though it is training on imperfect material, it will actually be able to correct many errors by virtue of its greater discriminitive power. Evaluate on PPCME Wycliffe Model (a) Baseline, tag NN (b) C&C, trained on gold"
D07-1041,N01-1026,0,0.218549,"perative construction (Han, 2000). However, their utility rests on their having coverage of a significant amount of annotated We also seek to reduce the human effort involved in producing part-of-speech tags for historical corpora. However, our approach does so by leveraging existing resources for a language’s modern varieties along with parallel diachronic texts to produce accurate taggers. This general technique has worked well for bilingual bootstrapping of language processing resources for one language based on already available resources from the other. The first to explore the idea were Yarowsky and Ngai (2001), who induced a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. They built a highly accurate POS tagger by labeling English text with an existing tagger (trained on English resources), aligning that text with parallel French, projecting the automatically assigned English POS tags across these alignments, and then using the automatically labeled French text to train a new French tagger. This tech390 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural L"
D07-1041,H01-1035,0,0.16014,"Missing"
D07-1041,J93-2004,0,\N,Missing
D08-1069,P05-1022,0,0.0119118,"ns. Not only do they model each coreference decision separately, they actually model each pair of mentions as a separate event. Recasting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of the ranking approach is that its com661 plexity is only square in the nu"
D08-1069,P02-1034,0,0.0157351,"ng independence assumptions. Not only do they model each coreference decision separately, they actually model each pair of mentions as a separate event. Recasting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of the ranking approach is that its com661 ple"
D08-1069,H05-1013,0,0.193271,"Missing"
D08-1069,N07-1030,1,0.954014,"assification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of the ranking approach is that its com661 plexity is only square in the number of mentions, while that of the twin-candidate model is cubic (see Denis and Baldridge (2007b) for a more detailed comparison in the context of pronoun resolution). Our ranking models for coreference take the following log-linear form: exp Prk (αi |π) = P k m P wj fj (π, αi ) j=1 m P exp (1) wj fj (π, αk ) j=1 where π stands for the anaphoric expression, αi for an antecedent candidate, fj the weighted features of the model. The denominator consists of a normalization factor over the k candidate mentions. Model parameters were estimated with the limited memory variable metric algorithm and Gaussian smoothing (σ 2 =1000), using TADM (Malouf, 2002). For the training of the different ran"
D08-1069,J95-2003,0,0.334927,"form: the referential form of the antecedent candidate: a proper name, a definite de5 6 http://opennlp.sf.net. http://wordnet.princeton.edu/ Features/Types Ling. form Context Distance Agreement. Sem. compat. Str. sim. Apposition Acronym 3P √ √ √ √ √ SP √ √ √ √ √ PN √ √ √ √ √ √ √ √ Def-NP √ √ √ √ √ √ √ Oth √ √ √ √ √ √ Table 3: Features for each type of referential expression. scription, an indefinite NP, or a pronoun. Context: the context of the antecedent candidate: these features can be seen as approximations of the grammatical roles, as indicators of the salience of the potential candidate (Grosz et al., 1995). For instance, this includes the part of speech tags surrounding the candidate, as well as a feature that indicates whether the potential antecedent is the first mention in a sentence (approximating subjecthood), and a feature indicating whether the candidate is embedded inside another mention. Distance: the distance between the anaphor and the candidate, measured by the number of sentences and mentions between them. Morphosyntactic agreement: indicators of the gender, number, and person of the two mentions. These are determined for non-pronominal NPs with heuristics based on POS tags (e.g.,"
D08-1069,P07-1107,0,0.0763662,"Missing"
D08-1069,N04-1037,0,0.0162125,"Missing"
D08-1069,H05-1004,0,0.274837,"Missing"
D08-1069,W02-2018,0,0.0249605,"didate model is cubic (see Denis and Baldridge (2007b) for a more detailed comparison in the context of pronoun resolution). Our ranking models for coreference take the following log-linear form: exp Prk (αi |π) = P k m P wj fj (π, αi ) j=1 m P exp (1) wj fj (π, αk ) j=1 where π stands for the anaphoric expression, αi for an antecedent candidate, fj the weighted features of the model. The denominator consists of a normalization factor over the k candidate mentions. Model parameters were estimated with the limited memory variable metric algorithm and Gaussian smoothing (σ 2 =1000), using TADM (Malouf, 2002). For the training of the different ranking models, we use the following procedure. For each model, instances are created by pairing each anaphor of the proper type (e.g., definite description) with a set of candidates which contains: (i) a true antecedent, and (ii) a set of non-antecedents. The selection of the true antecedent varies depending on the model we are training: for pronominal forms, the antecedent is selected as the closest preceding mention in the chain; for non-pronominal forms, we used the closest preceding non-pronominal mention in the chain as the antecedent. For the creation"
D08-1069,P00-1023,0,0.0526434,"be the set of mentions present in a document. For all models, each mention m ∈ M is associated at test time with a set of antecedent candidates Cm , which includes all the mentions that linearly precede m. The best candidate is determined by the model in use. The final output of each system consists in a list of mention pairs (i.e., the coreference links) which in turn defines (through reflexive, transitive closure) a partition over the set M. Our models are summarized in Table 4. The use of the discourse status filter is straightforward. For each mention m∈M, the discourse status 7 In fact, Morton (2000) does not use distance in this case. Model Name CLASS CLASS + DS CLASS + SP CLASS + DS + SP RANK + DS + SP Model Type class class class class rank Specialized? No No Yes Yes Yes Disc. Status No Yes No Yes Yes System 3rd pron. speech pron. proper names def. NPs others Accuracy 82.2 66.9 83.5 66.5 63.6 Table 5: Accuracy of the different ranker models. Table 4: Model names and their properties. model is first applied to determine whether m introduces a new discourse entity (i.e., it is classified as “new”) or refers back to an existing entity (i.e., it is classified as “old”). If m is classified"
D08-1069,C02-1139,0,0.849912,"raining the competition among potential antecedent candidates, instead of considering them independently. This gives the ranker additional discriminative power and in turn better antecedent selection accuracy. Here, we show that ranking is also effective for the wider task of coreference resolution. Introduction Coreference resolution is the task of partitioning a set of entity mentions in a text, where each partition corresponds to some entity in an underlying discourse model. While early machine learning approaches for the task relied on local, discriminative classifiers (Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004), more recent approaches use joint and/or global models (McCallum and Wellner, 2004; Ng, 2004; Daum´e III and Marcu, 2005; Denis and Baldridge, 2007a). This shift improves performance, but the systems are considerably more complex and often less efficient. Here, we explore two simple modifications of the first type of approach that yield performance gains which are comparable, and sometimes better, to those obtained with these more complex systems. These modifications involve: (i) the use of rankers instead of clasCoreference resolution involves several dif"
D08-1069,P02-1014,0,0.943055,"raining the competition among potential antecedent candidates, instead of considering them independently. This gives the ranker additional discriminative power and in turn better antecedent selection accuracy. Here, we show that ranking is also effective for the wider task of coreference resolution. Introduction Coreference resolution is the task of partitioning a set of entity mentions in a text, where each partition corresponds to some entity in an underlying discourse model. While early machine learning approaches for the task relied on local, discriminative classifiers (Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004), more recent approaches use joint and/or global models (McCallum and Wellner, 2004; Ng, 2004; Daum´e III and Marcu, 2005; Denis and Baldridge, 2007a). This shift improves performance, but the systems are considerably more complex and often less efficient. Here, we explore two simple modifications of the first type of approach that yield performance gains which are comparable, and sometimes better, to those obtained with these more complex systems. These modifications involve: (i) the use of rankers instead of clasCoreference resolution involves several dif"
D08-1069,P04-1020,0,0.107439,"Missing"
D08-1069,P05-1020,0,0.0148466,"naphoric expressions: third-person pronouns, speech pronouns (i.e., first and second person pronouns), proper names, definite descriptions and other types of nominals (e.g., anaphoric uses of indefinite, quantified, and bare noun phrases). Different anaphoric expressions exhibit different patterns of resolution and are sensitive to different factors (Ariel, 1988; van der Sandt, 1992; Gundel et al., 1993), yet most machine learning approaches have ignored these differences and handle these different phenomena with a single, monolithic model. A few exceptions are worth noting. Morton (2000) and Ng (2005b) propose different classifiers models for different NPs for coreference resolution and pronoun resolution, respectively. Other partially capture the differential preferences between different anaphors via different sample selection strategies during training (Ng and Cardie, 2002b; Uryupina, 2004). More recently, Haghighi and Klein (2007) use the distinction between pronouns, nominals and proper nouns 660 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660–669, c Honolulu, October 2008. 2008 Association for Computational Linguistics in their unsup"
D08-1069,P07-1068,0,0.0344073,"Missing"
D08-1069,N04-1012,1,0.698655,"with a greedy right-to-left clustering, these approaches make very strong independence assumptions. Not only do they model each coreference decision separately, they actually model each pair of mentions as a separate event. Recasting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at onc"
D08-1069,W03-1209,0,0.0437797,"ion separately, they actually model each pair of mentions as a separate event. Recasting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of the ranking approach is that its com661 plexity is only square in the number of mentions, while that of the twin-candida"
D08-1069,J01-4004,0,0.949482,"ly capture during training the competition among potential antecedent candidates, instead of considering them independently. This gives the ranker additional discriminative power and in turn better antecedent selection accuracy. Here, we show that ranking is also effective for the wider task of coreference resolution. Introduction Coreference resolution is the task of partitioning a set of entity mentions in a text, where each partition corresponds to some entity in an underlying discourse model. While early machine learning approaches for the task relied on local, discriminative classifiers (Soon et al., 2001; Ng and Cardie, 2002b; Morton, 2000; Kehler et al., 2004), more recent approaches use joint and/or global models (McCallum and Wellner, 2004; Ng, 2004; Daum´e III and Marcu, 2005; Denis and Baldridge, 2007a). This shift improves performance, but the systems are considerably more complex and often less efficient. Here, we explore two simple modifications of the first type of approach that yield performance gains which are comparable, and sometimes better, to those obtained with these more complex systems. These modifications involve: (i) the use of rankers instead of clasCoreference resolution"
D08-1069,W04-3222,0,0.0191846,"lustering, these approaches make very strong independence assumptions. Not only do they model each coreference decision separately, they actually model each pair of mentions as a separate event. Recasting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of t"
D08-1069,M95-1005,0,0.893601,"Missing"
D08-1069,P03-1023,0,0.250188,"ting these tasks as ranking tasks partly addresses this problem by directly making the comparison between different candidate antecedents for an anaphor part of the training criterion. Each candidate is assigned a conditional probability with respect to the entire candidate set. (Re)rankers have been successfully applied to numerous NLP tasks, such as parse selection (Osborne and Baldridge, 2004; Toutanova et al., 2004), parse reranking (Collins and Duffy, 2002; Charniak and Johnson, 2005), question-answering (Ravichandran et al., 2003). The twin-candidate classification approach proposed by (Yang et al., 2003) shares some similarities with the ranker in making the comparison between candidate antecedents part of training. An important difference however is that under the twin-candidate approach, candidates are compared in pairwise fashion (and the best overall candidate is the one that has won the most round robin contests), while the ranker considers the entire candidate set at once. Another advantage of the ranking approach is that its com661 plexity is only square in the number of mentions, while that of the twin-candidate model is cubic (see Denis and Baldridge (2007b) for a more detailed compa"
D08-1069,W03-0403,1,\N,Missing
D08-1069,N04-4009,0,\N,Missing
D09-1031,W09-1905,1,0.934703,"CL and AFNLP Language Danish Dutch English Swedish Uspanteko #words-tr 62825 129586 167593 127684 43473 #words-dev 31561 65483 131768 63783 19906 #tags 10 13 45 41 69 #sents-tr 3570 9365 6945 7326 7423 #sents-dev 1618 3982 5527 3714 3288 Avg.sent 18.18 14.61 24.00 17.34 5.92 Avg.tr.sent 17.60 13.84 24.13 17.43 5.86 Avg.dev.sent 19.50 16.44 23.84 17.17 6.05 Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length. Data We use a collection of 32 interlinear glossed texts (IGT) in the Mayan language Uspanteko. This corpus was cleaned up and adapted by Palmer et al. (2009) from an original collection of 67 texts that were collected, transcribed, translated and annotated by the OKMA language documentation project (Pixabaj et al., 2007). Two core tasks in creating IGT are morphological analysis and tagging morphemes with their glosses (labels indicating part-of-speech and/or grammatical function). We deal with the latter task and assume texts are morphologically segmented. Standard four-line IGT has morphemes on one line and their glosses on the next. The gloss line includes labels for grammatical morphemes (e.g. PL or COM) and translations of stems (e.g. hablar"
D09-1031,W09-1903,0,0.0461665,"Missing"
D09-1031,W06-2920,0,0.0525978,"Missing"
D09-1031,D08-1027,0,0.142097,"Missing"
D09-1031,E03-1071,0,0.0102236,"cost-sensitive selection, but our results—from live (non-simulated) active learning experiments of real-world scale— empirically support the need to consider costsensitive selection if better cost reductions are to be achieved. Classification model. We use a standard maximum entropy classifier for tagging Danish, Dutch, English, and Swedish words with POS-tags and tagging Uspanteko morphemes with Gloss/POS tags. The label for a word/morpheme is predicted based on the word/morpheme itself plus a window of two units before and after. Standard part-of-speech tagging features (Ratnaparkhi, 1998; Curran and Clark, 2003) are extracted from the morpheme to help with predicting labels for previously unseen morphemes. This is a strong but standard model; better, more complex models could be used, but the gains are likely to be small. Thus, we opted for simplicity in our model so as to focus more on the interaction between the annotator and different levels of machine involvement. The accuracy of the tagger on the datasets when trained on all available training material is given in the following table, along with accuracy of a unigram model (learned from the training set and constrained by a tag dictionary for kn"
D09-1031,W09-1906,0,0.0336635,"Missing"
D09-1031,W05-0619,0,0.0173153,"orpheme; clauses with the highest average entropy are selected for labeling. A recent development in active learning is cost298 Measuring annotation cost. Active learning studies usually simulate annotation and use a unit cost assumption that each word, sentence, constituent, document, etc. takes the same time to annotate. This is often the only option since corpora typically do not retain annotation time, but it is likely to exaggerate the annotation cost reductions achieved. This is exacerbated with active learning: the informative examples it seeks to find are typically harder to annotate (Hachey et al., 2005). Baldridge and Osborne (2008) correlate a unit cost in terms of discriminants (decisions made by annotators about valid parses) to annotation time. This is a better approximation than unit costs where such a relationship cannot be established. However, it is based on a static measurement of annotation time, and clearly the time taken to annotate an example is not a function of the example alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selecte"
D09-1031,J93-2004,0,0.0315661,"j inyolj iin M ORPH: kita’ t-in-ch’abe-j laj in-yol-j iin G LOSS: NEG INC-E1S-hablar-SC PREP A1S-idioma-SC yo POS: PART TAM-PERS-VT-SUF PREP PERS-S-SUF PRON T RANS: ‘No le hablo en mi idioma.’ We use a single layer that is a combination of the G LOSS and POS layers (Palmer et al., 2009). For (1), the morphemes and labels for our task are: (2) kita’ t- in- ch’abe -j laj in- yol -j iin NEG INC E1S VT SC PREP A1S S SC PRON We also consider POS-tagging for Danish, Dutch, English, and Swedish; the English is from sections 00-05 (as training set) and 19-21 (as development set) of the Penn Treebank (Marcus et al., 1993), and the other languages are from the CoNLL-X dependency parsing shared task (Buchholz and Marsi, 2006).1 We split the original training data into training and development sets. Table 1 shows the number of words and sentences in each split of each dataset, as well as the number of possible labels and the average sentence length. The Uspanteko data is counted in morphemes rather than words; also, the Uspanteko texts are divided at the clause rather than sentence level. This gives the corpus a much lower average clause length than the other languages (Table 1). 1 The subset of the Penn Treebank"
D09-1031,P00-1016,0,0.0183417,"alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selected early than it would after the annotator has seen many other examples. Thus, it is important to measure annotation time embedded in the context of a particular annotation experiment with the sample selection/labeling strategies of interest. In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). In the simulation studies, as we are unable to measure time, we measure cost by sentence/clause and word/morpheme. fitted nonlinear regression models rather than averaging over a subset of data points. Specifically, we fit a modified Michaelis-Menton model: f (cost, (K, Vm , A)) = Vm (A + cost) K + cost The (original) parameters Vm and K respectively correspond to the horizontal asymptote and the cost where accuracy is halfway between 0 and Vm . The additional parameter A allows for a better fit to our data by allowing for less sharp elbows and letting cost be zero. Model parameters were det"
D09-1070,W02-0606,0,0.552097,"stering around a common stem, and generation of new word forms with productive affixes. Intuitively, there are straightforward, but non-trivial, challenges that arise when evaluating a model. One large challenge is distinguishing derivational from inflectional morphology. Most approaches deal with tokens without considering context. Since inflectional morphology is virtually always driven by syntax and word context, such approaches are unable to learn only inflectional morphology or only derivational morphology. Even approaches which take context into consideration (Schone and Jurafsky, 2000; Baroni et al., 2002; Freitag, 2005) cannot learn specifically for one or the other. In addition, the evaluation of both segmentation and clustering involves arbitrary judgment calls. Concerning segmentation, should altimeter and altitude be one morpheme or two? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unprod"
D09-1070,W06-3210,0,0.0285117,"Missing"
D09-1070,W06-3209,0,0.0941825,"many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the"
D09-1070,W02-0603,0,0.0781327,"Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the"
D09-1070,P04-2012,0,0.421757,"t whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et al. (2002) takes a similar approach but uses edit distance to cluster words that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. Th"
D09-1070,W09-1905,1,0.882512,"other approaches that typically require a number of arbitrary thresholds and parameters yet provide little intuitive justification for them. (We give examples of these in §3.) We evaluate our approach on two languages, English and Uspanteko, and compare its performance to two benchmark systems, Morfessor (Creutz and Lagus, 2007) and Linguistica (Goldsmith, 2001). English is commonly used in other studies and permits the use of CELEX as a gold standard for evaluation. Uspanteko is an endangered Mayan language for which we have a set of interlinearized glossed texts (IGT) (Pixabaj et al., 2007; Palmer et al., 2009). IGT provides wordby-word morpheme segmenation, which we use to create a synthetic gold standard. In addition to evaluation against this standard, Telma Kaan Pixabaj—a Mayan linguist who helped create the annotated corpus—reviewed by hand 100 word clusters produced by our system, Morfessor and Linguistica. Note that because English is suffixal and Uspanteko is both prefixal and suffixal, we use a slightly modified model for Uspanteko. The approach introduced in this paper compares favorably to Linguistica and Morfessor, two models that employ much more complex strategies and rely on experimen"
D09-1070,W04-0106,0,0.160943,"ls require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the tendency of raw MDL to"
D09-1070,W00-0712,0,0.405885,"graphy, then the two words are likely to be related morphologically. We measure how integrating these assumptions into our model at different stages affects performance. We define a simple pipeline model. After generating candidate stems and affixes (possibly constrained by document boundaries), a χ2 test based on global corpus counts filters out unlikely affixes. Mutually consistent affix pairs are then clustered to form affix groups. These in turn are used to build morphologically related word clusters, possibly constrained by evidence from co-occurence of word forms in documents. Following Schone and Jurafsky (2000), clusters are evaluated for Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and affixes. This typically involves heuristic search procedures and calibrating multiple arbitrary thresholds. We present a simple approach that uses no thresholds other than those involved in standard application of χ2 significance testing. A key part of our approach is using document boundaries to constrain generation of candidate stems and affixes and clustering morphological variants of a given word stem. We"
D09-1070,N01-1024,0,0.304218,"wo? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unproductive one? Yet, many studies have evaluated their segmentations and clusters by going over their results word by word, cluster by cluster and judging by sight whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et"
D09-1070,W02-0602,0,0.174996,"egant, a pure implementation on real data results in descriptions that do not reflect actual morphology. Creutz and Lagus (2005) report that, “frequent word forms remain unsplit, whereas rare word forms are excessively split.” In the end, every MDL approach uses probabilistically motivated refinements that restrict the tendency of raw MDL to generate descriptions that do not fit linguistic notions of morphology. Despite the sophistication of the models in this group, there are many parameters that need to be set, and heuristic search procedures are crucial for their success (Goldwater, 2007). Snover et al. (2002) present a Bayesian model that uses a prior distribution to refine disjoint clusters of morphologically related words. It disposes with parameter setting by selecting the highest ranking hypothesis. 4 Model2 Our goal is to generate conflation sets: sets of word types that are related through either inflectional or derivational morphology (Schone and Jurafsky, 2000). Solving this task requires learning how individual types are segmented (though the segmentation itself is not evaluated). For present purposes, we assume that the affixal pattern of the language is known: whether it is prefixal, su"
D09-1070,N07-1020,0,0.0468931,"s that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. This group is the only one to evaluate against CELEX (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001; Freitag, 2005). Keshava and Pitler, 2005; Hammarstr¨om, 2006; Dasgupta and Ng, 2007; Demberg, 2007) use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries. LSV has several issues that require fine parameter tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have simi"
D09-1070,P07-1116,0,0.0688802,"do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and refines them through a graph walk algorithm. This group is the only one to evaluate against CELEX (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001; Freitag, 2005). Keshava and Pitler, 2005; Hammarstr¨om, 2006; Dasgupta and Ng, 2007; Demberg, 2007) use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries. LSV has several issues that require fine parameter tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specifi"
D09-1070,P08-1084,0,0.0723333,"count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corpus subject to a penalty based on the size of hypothesized morpheme lexicons they build on the basis of the segmentations. While theoretically elegant, a pure implementation on real data results in descriptions that"
D09-1070,P00-1027,0,0.099404,"tuning. For example, Hafer and Weiss (1974) counts how many types of characters appear after some initial string (the successor count) and how many types of characters appear before some final string (the predecessor count). A successful criterion for segmenting a word was if the predecessor count for the second part was greater than 17 and the successor count for the first part was greater than 5. Other studies have similar data specific parameters and restrictions. Others. Some other models require input such as POS tables and lexicons and use a wider range of information about the corpus (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2001; Chan, 2006). Because of the knowledge dependence of these models, they are able to properly induce inflectional morphology, as opposed to the studies cited above. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. MDL and Bayesian models. Minimum description length (MDL) models (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2004; Goldsmith, 2006; Creutz and Lagus, 2007) try to segment words by maximizing the probability of a training corp"
D09-1070,W05-0617,0,0.711796,"on stem, and generation of new word forms with productive affixes. Intuitively, there are straightforward, but non-trivial, challenges that arise when evaluating a model. One large challenge is distinguishing derivational from inflectional morphology. Most approaches deal with tokens without considering context. Since inflectional morphology is virtually always driven by syntax and word context, such approaches are unable to learn only inflectional morphology or only derivational morphology. Even approaches which take context into consideration (Schone and Jurafsky, 2000; Baroni et al., 2002; Freitag, 2005) cannot learn specifically for one or the other. In addition, the evaluation of both segmentation and clustering involves arbitrary judgment calls. Concerning segmentation, should altimeter and altitude be one morpheme or two? (The sample English gold standard for MorphoChallenge 2009 provides alti+meter but altitude.) Similar issues arise when evaluating clusters of related word forms if inflection and derivation are not distinguished. Does atheism belong to the same cluster as theism? Where is the frequency cutoff point between a productive derivational morpheme and an unproductive one? Yet,"
D09-1070,W99-0904,0,0.174257,"y word, cluster by cluster and judging by sight whether some segmentation or clustering is good (e.g., Goldsmith (2001)). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. 3 Related work There is a diverse body of existing work on unsupervised morphology acquisition. We summarize previous work, emphasizing some of its more arbitrary and ad hoc aspects. Letter successor variety. Letter successor variety (LSV) models (Hafer and Weiss, 1974; Gaussier, 1999; Bernhard, 2005; Bordag, 2005; Monson (2004) suggests, but does not actually use, χ2 . 669 parameters to handle circumfixation. Baroni et al. (2002) takes a similar approach but uses edit distance to cluster words that are similar but do not necessarily share a long, contiguous substring. They remove noise by constraining cluster membership with mutual information derived semantic similarity. Freitag (2005) uses a mutual information derived measure to learn the syntactic similarity between words and clusters them. Then he derives finite state machines across words in different clusters and re"
D09-1070,J01-2001,0,0.821667,"old and a lower bound on observed counts. These are the only manually set parameters we require—and we in fact use the widely accepted standard values for these thresholds without varying them in our experiments. This is a significant improvement over other approaches that typically require a number of arbitrary thresholds and parameters yet provide little intuitive justification for them. (We give examples of these in §3.) We evaluate our approach on two languages, English and Uspanteko, and compare its performance to two benchmark systems, Morfessor (Creutz and Lagus, 2007) and Linguistica (Goldsmith, 2001). English is commonly used in other studies and permits the use of CELEX as a gold standard for evaluation. Uspanteko is an endangered Mayan language for which we have a set of interlinearized glossed texts (IGT) (Pixabaj et al., 2007; Palmer et al., 2009). IGT provides wordby-word morpheme segmenation, which we use to create a synthetic gold standard. In addition to evaluation against this standard, Telma Kaan Pixabaj—a Mayan linguist who helped create the annotated corpus—reviewed by hand 100 word clusters produced by our system, Morfessor and Linguistica. Note that because English is suffix"
D09-1070,H01-1035,0,0.518764,"hmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values. 1 Introduction Unsupervised morphology acquisition attempts to learn from raw corpora one or more of the following about the written morphology of a language: (1) the segmentation of the set of word types in a corpus (Creutz and Lagus, 2007), (2) the clustering of word types in a corpus based on some notion of morphological relatedness (Schone and Jurafsky, 2000), (3) the generation of out-of-vocabulary items which are morphologically related to other word types in the corpus (Yarowsky et al., 2001). We take a novel approach to segmenting words and clustering morphologically related words. The approach uses no parameters that need to be tuned on data. The two main ideas of the approach are (a) the filtering of affixes by significant co-occurrence, and (b) the integration of knowledge of document boundaries when gener668 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668–677, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP like Uspanteko. whether they capture inflectional paradigms using CELEX (Baayen et al., 1993). We are unaware of other w"
D09-1070,P95-1026,0,0.0409762,"ary linguists to use our model as a preprocessing step for their manual analysis of stems and affixes. To require a documentary linguist–who is likely to have little to no knowledge of NLP methods–to tune parameters is unfeasible. Additionally, data-driven exploration of parameter settings is unlikely to be reliable in language documentation since datasets typically are quite small. To be relevant in this context, a model needs to produce useful results out of the box. Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation (Yarowsky, 1995). Many applications in information retrieval are built on the statistical correlation between documents and terms. However, we are unaware of cases where knowledge of document boundaries has been used for unsupervised learning for morphology. The intuition behind our approach is very simple: if two words in a single document are very similar in terms of orthography, then the two words are likely to be related morphologically. We measure how integrating these assumptions into our model at different stages affects performance. We define a simple pipeline model. After generating candidate stems a"
D09-1070,D08-1109,0,\N,Missing
D10-1020,P10-1132,0,0.126136,"as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterio"
D10-1020,afonso-etal-2002-floresta,0,0.0117489,"54 797328 447079 197422 70125 docs 1801 343 1090 1956 29 avg. 541 2325 410 101 2418 tags 43 80 58 19 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tag"
D10-1020,A00-1031,0,0.0288783,"tributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not w"
D10-1020,E03-1009,0,0.02832,"riments in this table use state sizes that correspond more closely to the size of the tag sets in the respective corpora. (Floresta), and 0.11 (Uspanteko). Clearly, all of the models easily outperform this baseline. F−SCORE VI 203 4 3 2 vi 0.3 f−score 1 0.2 0.1 0 0.0 Number of states. Figure 3 shows the change in accuracy for the different models for different corpora when the overall number of states is varied between 20 and 50. The figure shows results for M-to-1. All models with the exception of HMM + show improvements as the number of states is increased. This brings up the valid concern (Clark, 2003; Johnson, 2007) that a model could posit a very large number of states and obtain high M-to1 scores. However, it is neither the case here nor in any of the studies we cite. Furthermore, as is strongly suggested with HMM +, it does not seem as if all models will benefit from assuming a large number of states. Looking at the results by number of states on VI and f -score for CDHMM(Figure 5), it is clear that Floresta displays the reverse pattern of all other data sets where performance monotonically deteriorates as state sizes are increased. Though the exact reason is unknown, we believe it is"
D10-1020,P07-1035,0,0.0562127,", 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic model given data that had not been filtered of noise in the form of funct"
D10-1020,D08-1036,0,0.0601454,"of experts assumption allows us to capture high variance for certain states. To summarize, the CDHMM is a composite model where both the observed token and the hidden state variable are composite distributions. For the hidden state, this means that there is a “topical” element with high variance across contexts that is embedded in the state sequence for a subset of events. We embed this element through a PoE assumption where transitions into content states are modeled as a product of the transition probability and the local probability of the content state. Inference. We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. In this inference regime, two distributions are of particular interest. One is the posterior density and the other is the conditional distribution, neither of which can be learned in closed form. Letting Λ = (θ, δ, φ, ψ) and h = (α, β, γ, ξ), the posterior density is given as p(Λ|w, t; h) ∝ p(w, t|Λ)p(Λ; h) 199 Note that p(w, t|Λ) is equal to Nd D Y Y d φwi |ti θti |d δti |ti−1 i ψwi |ti δti |ti−1 I[ti ∈C] I[ti ∈F ] (1) where I[·] is the indicator function, D is the number of documents in the corpus and Nd is the numb"
D10-1020,P08-1085,0,0.0673554,"we do in that a parse tree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4 We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. 205 that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Mark"
D10-1020,P07-1094,0,0.533761,"and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being f"
D10-1020,N06-1041,0,0.0141746,"agging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS ta"
D10-1020,C08-1042,0,0.0313772,"Missing"
D10-1020,D07-1031,0,0.106208,"tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008)."
D10-1020,N09-1069,0,0.0203163,"be that Uspanteko has a relatively large number of tags in a very small corpus. 204 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment t"
D10-1020,W09-1905,1,0.791859,"sets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tagsets. • Accuracy: We use a greedy search algorithm to map each unsupervised tag to a gold label such that accuracy is maximized. We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to"
D10-1020,P09-1057,0,0.243953,"ly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained"
D10-1020,W10-2911,0,0.0118274,"al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeat"
D10-1020,W10-2909,0,0.402076,"al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeat"
D10-1020,P93-1034,0,0.0540538,"Missing"
D10-1020,P05-1044,0,0.471829,"ive data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative gen"
D10-1020,D08-1109,0,0.0162426,". 204 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, h"
D10-1020,N03-1033,0,0.0330607,"o show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative generative processes. This also applies to"
D10-1020,D09-1071,0,0.0335811,"Missing"
D10-1020,D09-1072,0,0.242287,"learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4 We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. 205 that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov model that allocates a subset of the states to have distributions conditioned on localized domains can significantly improve performance in unsupervised partof-speech tagging. We have also demonstrated that significant performance gains are possible simply by setting a different emission hyperparameter for a subgroup of the states. It is encouraging that these results hold for both models not just on the WSJ but across a diverse set of languages and measures. We believe our"
D10-1020,J93-2004,0,\N,Missing
D12-1075,C04-1080,0,0.267027,"Missing"
D12-1075,bosco-etal-2000-building,0,0.202112,"and the second from sections 00– 07 (379,908 tokens). The former contains 39,087 word types, 45,331 word/tag entries, a per-type ambiguity of 1.16 and yields a per-token ambiguity of 2.21 on the raw corpus (treating unknown words as having all 45 possible tags). The latter contains 26,652 word types, 30,662 word/tag entries, a pertype ambiguity of 1.15 and yields a per-token ambiguity of 2.03 on the raw corpus. In both cases, every word/tag pair found in the relevant sections was used in the tag dictionary: no pruning was performed. Italian data. As a second evaluation, we use the TUT corpus (Bosco et al., 2000). To verify that our approach is language-independent without the need for specific tuning, we executed our tests on the Italian data without any trial runs, parameter modifications, or other changes. We divided the TUT data, taking the first half of each of the five sections as input to the tag dictionary, the next quarter as raw data, and the last quarter as test data. All together, the tag dictionary was constructed from 41,000 tokens consisting of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of"
D12-1075,P96-1041,0,0.355467,"Missing"
D12-1075,D10-1056,0,0.29759,"Missing"
D12-1075,P11-1061,0,0.144415,"Missing"
D12-1075,P11-2008,0,0.0666234,"Missing"
D12-1075,P08-1085,0,0.215586,"containing previously unseen word types. Vaswani et al (2010) explore the use of minimum description length principles in a Bayesian model as a way of capturing model minimization, inspired by the MIN - GREEDY algorithm. The advantage there is that only a single objective function needs to be optimized, rather than having initialization followed by an iterative back and forth with pruning of tag-tag pairs. Our own next steps are to move in a similar direction to explore the possibilities for encoding the intuitions we developed for initialization and minimization as a single generative model. Goldberg et al. (2008) note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., supertagging for CCGbank (Hockenmaier and Steedman, 2007)). Such efforts require automatic expansion of tag dictionaries, which then need be constrained based on available raw"
D12-1075,E09-1042,0,0.289027,"Missing"
D12-1075,J07-3004,0,0.0861226,"e the possibilities for encoding the intuitions we developed for initialization and minimization as a single generative model. Goldberg et al. (2008) note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., supertagging for CCGbank (Hockenmaier and Steedman, 2007)). Such efforts require automatic expansion of tag dictionaries, which then need be constrained based on available raw token sequences using methods such as those explored here. In this respect, the somewhat idiosyncratic noise in the corpus-derived dictionaries used here make a good test. Acknowledgements We thank Yoav Goldberg, Sujith Ravi, and the reviewers for their feedback. This work was supported by the U.S. Department of Defense through the U.S. Army Research Office (grant number W911NF-101-0533) and via a National Defense Science and Engineering Graduate Fellowship for the first autho"
D12-1075,D07-1031,0,0.159303,"Missing"
D12-1075,J93-2004,0,0.0410499,"soon as they become completed by the minimization process, and the tag bigrams are chosen in order of frequency, there will be more highfrequency bigrams than low-frequency. As a result, this labeling will contain good tag transitions and token labelings. As such, the labeled data produced by the second phase provides useful information beyond a simple set of sufficient bigrams: it contains legitimate frequency information that can be used to initialize the HMM. We, therefore, initialize an HMM directly from this data to start EM. 4 Evaluation1 English data. We evaluate on the Penn Treebank (Marcus et al., 1993). In all cases we use the first 47,996 tokens of section 16 as our raw data, sections 19–21 as our development set, and perform the final evaluation on sections 22–24. 1 Source code, scripts, and data to reproduce the results presented here can be found at github.com/dhgarrette/ type-supervised-tagging-2012emnlp 828 We evaluate two differently sized tag dictionaries. The first is extracted directly from sections 00–15 (751,059 tokens) and the second from sections 00– 07 (379,908 tokens). The former contains 39,087 word types, 45,331 word/tag entries, a per-type ambiguity of 1.16 and yields a p"
D12-1075,J94-2001,0,0.936346,"Missing"
D12-1075,D10-1020,1,0.925376,"Missing"
D12-1075,P09-1057,0,0.647979,"Missing"
D12-1075,C10-1106,0,0.324381,"epartment of Computer Science The University of Texas at Austin dhg@cs.utexas.edu Jason Baldridge Department of Linguistics The University of Texas at Austin jbaldrid@utexas.edu Abstract Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the MIN GREEDY algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original MIN - GREEDY algorithm for both English and Italian data. 1 Introduction Learning accurate part-of-speech (POS) taggers based on plentiful labeled training material is generally considered a solved problem. The best taggers obtain accuracies of ov"
D12-1075,P10-2039,0,0.0229242,"Missing"
D12-1137,D10-1124,0,0.431733,"Missing"
D12-1137,P11-1096,1,0.762129,"ngs of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1500–1510, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics training document and a location chosen based on the location(s) of the most similar training document(s). For image geolocation, Chen and Grauman (2011) perform mean-shift clustering over training images to discretize locations, then estimate a test image’s location with weighted voting from the k most similar documents. For text, both Serdyukov et al. (2009) and Wing and Baldridge (2011) use a similar approach, but compute document similarity based on language models rather than image features. Additionally, they group documents via a uniform geodesic grid rather than a clustered set of locations. This reduces the number of similarity computations and removes the need to perform location clustering altogether, but introduces a new parameter controlling the granularity of the grid. Kinsella et al. (2011) predict the locations of tweets and users by comparing text in tweets to language models associated with zip codes and broader geopolitical enclosures. Sadilek et al. (2012) d"
D14-1035,H92-1026,0,0.296804,"Missing"
D14-1035,bosco-etal-2000-building,0,0.0819922,"yarwanda (KIN) and Malagasy (MLG) as examples of languages without substantial existing treebanks. Finally, as a middleground language, we use Italian (ITL). For English, we use the Wall-Street Journal section of the Penn Treebank (WSJ) (Marcus et al., 1993). The data split is sections 02-21 for training, section 22 for development, and section 23 for testing. For Chinese, the Chinese Treebank (CTB5) (Xue et al., 2005) was used. The data split is files 81-899 for training, files 4180 for development, and files 1-40/900-931 for testing. The ITL data is from the Turin University Treebank (TUT) (Bosco et al., 2000) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split. The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide provided by the Kigali Genocide Memorial Center, along with a few BBC news articles. The MLG texts are articles from the websites Lakroa and La Gazette and Malagasy Global Voices. Both datasets are described in Garrette and Baldridge (2013). The KIN and MLG data is very small compared to ENG and CHI: the KIN dataset con1. If node Ni is a pre-terminal node abo"
D14-1035,alicante-etal-2012-treebank,0,0.0152726,"a. We compare our results to a number of previously established baselines. First, for all languages, we use both a standard unsmoothed PCFG and the Bikel parser, trained on the training corpus. Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework. For Chinese, we compare to Huang & Harper (2009), using their results that only use the Chinese Treebank (CTB). For English, we compare to Liang et al. (2009). Prior results for parsing the constituency version of the Italian data are available from Alicante et al. (2012), but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs.2 Algorithm 3: Learning prior from training in parsed trees, which also takes two steps. The first step is to, for each node in the tree, calculate and store the probability that the node is annotated by x. The second step is to jointly sample latent annotations for child nodes of root nodes, and then continue this process from top to bottom until reaching the pre-terminal nodes. Step"
D14-1035,A00-2018,0,0.31595,"Missing"
D14-1035,W02-1502,0,0.0266797,"making use of a measure for data reliability in order to weight training data based on confidence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus. Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications. Kuhn (2004a) shows some of the difficulty in handling low-resource languages by examining various tasks using Q’anjob’al as an example. Another approach is that of Bender et al. (2002), who take a more linguistically-motivated approach by making use of linguistic universals to seed newly developed grammars. This substantially reduces the effort by making We use a Gibbs sampler to collect sampled trees theoretically distributed from the true posterior distribution in order to parse. Priors in a Bayesian model can control the sparsity of grammars (which the insideoutside algorithm fails to do), while naturally incorporating smoothing into the model (Johnson et al., 2007; Liang et al., 2009). We also build a Bayesian model for parsing with a treebank, and incorporate informati"
D14-1035,P12-1024,0,0.0139643,"2009; Shindo et al., 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen et al., 2013), and variational inference (Liang et al., 2009; Wang and Blunsom, 2013). Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relativel"
D14-1035,N13-1015,0,0.0736398,", 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen et al., 2013), and variational inference (Liang et al., 2009; Wang and Blunsom, 2013). Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relatively little work has con"
D14-1035,J98-4004,0,0.480284,"Missing"
D14-1035,P96-1025,0,0.320951,"Missing"
D14-1035,P03-1054,0,0.26122,"Missing"
D14-1035,P97-1003,0,0.589165,"Missing"
D14-1035,kuhn-mateo-toledo-2004-applying,0,0.0272388,"ng algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance. For instance, Hwa et al. (2005) use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar. Kuhn (2004b) also considers the benefits of using multiple languages to induce a monolingual grammar, making use of a measure for data reliability in order to weight training data based on confidence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus. Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications. Kuhn (2004a) shows some of the difficulty in handling low-resource langua"
D14-1035,J03-4003,0,0.385087,"Missing"
D14-1035,P04-1060,0,0.0412562,"ng algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance. For instance, Hwa et al. (2005) use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar. Kuhn (2004b) also considers the benefits of using multiple languages to induce a monolingual grammar, making use of a measure for data reliability in order to weight training data based on confidence of annotation. Bootstrapping approaches such as these achieve markedly improved results, but they are dependent on the existence of a parallel bilingual corpus. Very few such corpora are readily available, particularly for low-resource languages, and creating such corpora obviously presents a challenge for many practical applications. Kuhn (2004a) shows some of the difficulty in handling low-resource langua"
D14-1035,W06-1673,0,0.0265102,"(i) |t(i) )p(t(i) |θ)) i=1 Yn Y (i) = p(θ)( p(w(i) |t(i) ) θrfr (t )) (1) i=1 p(θ |t, w, α) = Yn Yi=1 p(ti |wi , θ) A∈N (3) Dir(θA |fA (t) + αA ) (4) Step 1: Sample Rule Probabilities. Given trees t and prior α, the production probabilities θA for each nonterminal A∈N are sampled from a Dirichlet distribution with parameters fA (t) + αA . fA (t) is a vector, and each component of fA (t), is the number of occurrences of one rule expanding nonterminal A. Step 2: Sample Tree Structures. To sample trees from p(ti |wi , θ), we use the efficient sampling scheme used in previous work (Goodman, 1998; Finkel et al., 2006; Johnson et al., 2007). There are two parts to this algorithm. The first constructs an inside table as in the Inside-Outside algorithm for PCFGs (Lary and Young, 1990). The second selects the tree by recursively sampling productions from top to bottom. r∈R Here fr (t) is the number of occurrences of rule r in the derivation of t; p(w(i) |t(i) ) = 1 if the yield of t(i) is the sequence w(i) , and 0 otherwise. We use a Dirichlet distribution parametrized by αA : Dir(αA ) as the prior of the probability distribution for all rules expanding non-terminal A (p(θA )). The prior for all θ, p(θ), is t"
D14-1035,N13-1014,1,0.754079,"raining, files 4180 for development, and files 1-40/900-931 for testing. The ITL data is from the Turin University Treebank (TUT) (Bosco et al., 2000) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split. The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide provided by the Kigali Genocide Memorial Center, along with a few BBC news articles. The MLG texts are articles from the websites Lakroa and La Gazette and Malagasy Global Voices. Both datasets are described in Garrette and Baldridge (2013). The KIN and MLG data is very small compared to ENG and CHI: the KIN dataset con1. If node Ni is a pre-terminal node above terminal symbol w, then for x∈H biT [x] = θNi [x]→w Experiments1 1 Code available at github.com/jmielens/gibbs-pcfg-2014, along with instructions for replicating experiments when possible 2 As part of a standardized pre-processing step, we strip functional tags, which makes a direct comparison to their results inappropriate. (18) After training, we take the average counts of sampled annotated rules and combinations of latent annotations as priors to parse raw sentences. 2"
D14-1035,P13-1057,1,0.88723,"Missing"
D14-1035,P95-1037,0,0.592379,"Missing"
D14-1035,J93-2004,0,0.0468341,"i bT [x] θNi [x]→Nj Nk βNi [x]Nj Nk →y,z bjT [y]bkT [z] Data English (ENG) and Chinese (CHI) are the two main languages used for this work; they are commonly used in parser evaluation and have previous examples of statistical parsers using a Bayesian framework. And since we primarily are interested in parsing low-resource languages, we include results for Kinyarwanda (KIN) and Malagasy (MLG) as examples of languages without substantial existing treebanks. Finally, as a middleground language, we use Italian (ITL). For English, we use the Wall-Street Journal section of the Penn Treebank (WSJ) (Marcus et al., 1993). The data split is sections 02-21 for training, section 22 for development, and section 23 for testing. For Chinese, the Chinese Treebank (CTB5) (Xue et al., 2005) was used. The data split is files 81-899 for training, files 4180 for development, and files 1-40/900-931 for testing. The ITL data is from the Turin University Treebank (TUT) (Bosco et al., 2000) and consists of 2,860 Italian sentences from a variety of domains. It was split into training, development, and test sets with a 70/15/15 percentage split. The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide"
D14-1035,P05-1010,0,0.238965,"we present an additional Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach. 1 Jason Baldridge2 Jason Mielens2 Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov et al., 2006) have shown them to be effective models for syntactic parsing, especially when less training material is available (Liang et al., 2009; Shindo et al., 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work fo"
D14-1035,D09-1087,0,0.038445,"mpled annotated trees end function Our goal is to understand parsing efficacy using sampling and latent annotations for low-resource languages, so we perform experiments on five languages with varying amount of training data. We compare our results to a number of previously established baselines. First, for all languages, we use both a standard unsmoothed PCFG and the Bikel parser, trained on the training corpus. Additionally, we compare to state-of-the-art results for both English and Chinese, which have an existing body of work in PCFGs using a Bayesian framework. For Chinese, we compare to Huang & Harper (2009), using their results that only use the Chinese Treebank (CTB). For English, we compare to Liang et al. (2009). Prior results for parsing the constituency version of the Italian data are available from Alicante et al. (2012), but as they make use of a different version of the treebank including extra sentences, and additionally use the extensive functional tags present in the corpus, we do not directly compare our results to theirs.2 Algorithm 3: Learning prior from training in parsed trees, which also takes two steps. The first step is to, for each node in the tree, calculate and store the pr"
D14-1035,P92-1017,0,0.705465,"Missing"
D14-1035,N07-1051,0,0.120164,"Missing"
D14-1035,N07-1018,0,0.411983,"on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen et al., 2013), and variational inference (Liang et al., 2009; Wang and Blunsom, 2013). Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource languages. Existing work in this area focuses primarily on approaches that use some form of cross-lingual bootstrapping to improve performance. For instance, Hwa et al. (2005) use a parallel Chinese/English corpus and an English dependency grammar to induce an annotated Chinese corpus in order to train a Chinese dependency grammar. Kuhn (2004b) also considers the benef"
D14-1035,D08-1091,0,0.0413666,"Missing"
D14-1035,P06-1055,0,0.774923,"l Gibbs sampler algorithm to learn annotations from training data, which are parse trees with coarse (unannotated) symbols. We show that a Gibbs sampling technique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach. 1 Jason Baldridge2 Jason Mielens2 Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov et al., 2006) have shown them to be effective models for syntactic parsing, especially when less training material is available (Liang et al., 2009; Shindo et al., 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem o"
D14-1035,P11-1108,1,0.905821,"Missing"
D14-1035,W13-2307,1,0.898877,"Missing"
D14-1035,P12-1046,0,0.0147519,"chnique is capable of parsing sentences in a wide variety of languages and producing results that are on-par with or surpass previous approaches. Our results for Kinyarwanda and Malagasy in particular demonstrate that low-resource language parsing can benefit substantially from a Bayesian approach. 1 Jason Baldridge2 Jason Mielens2 Recent work on Probabilistic Context-Free Grammars with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov et al., 2006) have shown them to be effective models for syntactic parsing, especially when less training material is available (Liang et al., 2009; Shindo et al., 2012). The coarse nonterminal symbols found in vanilla PCFGs are refined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen e"
D14-1035,W13-3519,0,0.0680342,"efined by latent variables; these latent annotations can model subtypes of grammar symbols that result in better grammars and enable better estimates of grammar productions. In this paper, we provide a Gibbs sampler for learning PCFGLA models and show its effectiveness for parsing lowresource languages such as Malagasy and Kinyawanda. Previous PCFG-LA work focuses on the problem of parameter estimation, including expectationmaximization (EM) (Matsuzaki et al., 2005; Petrov et al., 2006), spectral learning (Cohen et al., 2012; Cohen et al., 2013), and variational inference (Liang et al., 2009; Wang and Blunsom, 2013). Regardless of inference method, previous work has used the same method to parse new sentences: a Viterbi parse under a new sentence-specific PCFG obtained from an approximation of the original grammar (Matsuzaki et al., 2005). Here, we provide an alternative approach to parsing new sentences: an extension of the Gibbs sampling algorithm of Johnson et al. (2007), which learns rule probabilities in an unsupervised PCFG. Introduction Despite great progress over the past two decades on parsing, relatively little work has considered the problem of creating accurate parsers for low-resource langua"
D14-1035,J04-4004,0,\N,Missing
D14-1039,D10-1124,0,0.270893,"Missing"
D14-1039,N13-1121,0,0.024327,"Missing"
D14-1039,P13-4002,0,0.303166,"Missing"
D14-1039,C12-1064,0,0.62793,"s by a single user, as long as at least one of the user’s tweets is geotagged with specific, GPS-assigned latitude/longitude coordinates. The earliest such tweet determines the user’s location. Tweets outside of a bounding box covering the contiguous United States (including parts of Canada and Mexico) were discarded, as well as users that may be spammers or robots (based on the number of followers, followees and tweets). The resulting dataset contains 38M tweets from 450K users, of which 10,000 each are reserved for the development and test sets. T W W ORLD is a dataset of tweets compiled by Han et al. (2012). It was collected in a similar fashion to T W US but differs in that it covers the entire Earth instead of primarily the United States, and consists only of geotagged tweets. 3 Supervised models for document geolocation The dominant approach for text-based geolocation comes from language modeling approaches in information retrieval (Ponte and Croft, 1998; Manning et al., 2008). For this general strategy, the Earth is sub-divided into a grid, and then each training set document is associated with the cell that contains it. Some model (typically Naive Bayes) is then used to characterize each ce"
D14-1039,D12-1137,1,0.62326,"ages is then divided 80/10/10 into training, development and test sets. The tag set of each photo is concatenated into a single piece of text (in the process losing usersupplied tag boundary information in the case of multi-word tags). Our code and processed corpora are available for download.1 apply this to corpora in three languages (English, German and Portuguese). This method scales well to large training sets and greatly improves results across a wide variety of corpora, beating current state-of-the-art results by wide margins, including Twitter users (Han et al., 2014, henceforth Han14; Roller et al., 2012, henceforth Roller12); Wikipedia articles (Roller12; Wing and Baldridge, 2011, henceforth WB11); and Flickr images (O’Hare and Murdock, 2013, henceforth OM13). Importantly, this is the first method that improves upon straight uniform-grid Naive Bayes on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art technique for Twitter users of geographically-salient feature selection (Han14). We also show, contrary to Han14, that logistic regression when properly optimized is more accurate than state-of-the-art techniques, including feature selection, and fast"
D14-1039,P11-1096,1,0.612317,"he tag set of each photo is concatenated into a single piece of text (in the process losing usersupplied tag boundary information in the case of multi-word tags). Our code and processed corpora are available for download.1 apply this to corpora in three languages (English, German and Portuguese). This method scales well to large training sets and greatly improves results across a wide variety of corpora, beating current state-of-the-art results by wide margins, including Twitter users (Han et al., 2014, henceforth Han14; Roller et al., 2012, henceforth Roller12); Wikipedia articles (Roller12; Wing and Baldridge, 2011, henceforth WB11); and Flickr images (O’Hare and Murdock, 2013, henceforth OM13). Importantly, this is the first method that improves upon straight uniform-grid Naive Bayes on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art technique for Twitter users of geographically-salient feature selection (Han14). We also show, contrary to Han14, that logistic regression when properly optimized is more accurate than state-of-the-art techniques, including feature selection, and fast enough to run on large corpora. Logistic regression itself very effectively pi"
D18-1030,N18-1090,0,0.0721438,"Missing"
D18-1030,D17-1309,1,0.852415,"cies on 5 6 7 Identifying Language Spans in Codemixed Text https://plus.google.com/ https://www.youtube.com/ Hindi texts found in both Devanagari and Latin scripts. 330 Features Character n-gram Script Lexicon Window D V +/- 1 0 +/- 1 16 8 16 1000-5000 27 100 n. The model averages the embeddings according to the fractions of each n-gram string in the input token. For example, if the token is banana, then one of the extracted trigrams is ana and the corresponding fraction is 2/6. Note that there are six trigrams in total due to an additional boundary symbol at both ends of the token. Following Botha et al. (2017), we use feature hashing to control the size V and avoid storing a big string-to-id map in memory during runtime. The feature id of an n-gram string x is given by H(x)mod Vg (Ganchev and Dredze, 2008), where H is a well-behaved hash function. We set V = 1000, 1000, 5000, 5000 for n = 1, 2, 3, 4 respectively; these values yield good performance and are far smaller than the number of n-gram types. Table 3: Feature spaces of CMX. The window column indicates that CMX uses character n-gram and lexicon features extracted from the previous and following tokens as well as the current one. is significa"
D18-1030,W14-5152,0,0.0362497,"Missing"
D18-1030,W08-0804,0,0.0143375,"Script Lexicon Window D V +/- 1 0 +/- 1 16 8 16 1000-5000 27 100 n. The model averages the embeddings according to the fractions of each n-gram string in the input token. For example, if the token is banana, then one of the extracted trigrams is ana and the corresponding fraction is 2/6. Note that there are six trigrams in total due to an additional boundary symbol at both ends of the token. Following Botha et al. (2017), we use feature hashing to control the size V and avoid storing a big string-to-id map in memory during runtime. The feature id of an n-gram string x is given by H(x)mod Vg (Ganchev and Dredze, 2008), where H is a well-behaved hash function. We set V = 1000, 1000, 5000, 5000 for n = 1, 2, 3, 4 respectively; these values yield good performance and are far smaller than the number of n-gram types. Table 3: Feature spaces of CMX. The window column indicates that CMX uses character n-gram and lexicon features extracted from the previous and following tokens as well as the current one. is significantly faster than structured training (especially considering the large label set inherent in language ID), and (3) it is far easier to implement. 3.1 Token Model Simple feed-forward networks have achi"
D18-1030,hughes-etal-2006-reconsidering,0,0.103438,"Missing"
D18-1030,I13-1041,0,0.0498638,"document-level language ID. Document-level labels are often available in metadata, but token-level labels are not. Obtaining token-level labels for hundreds of languages is infeasible: candidate codemixed examples must be identified and multilingual speakers are required to annotate them. Furthermore, language ID models typically use character- and word-level statistics as signals, but shorter inputs have greater ambiguity and less context for predictions. Moreover, codemixing is common in informal contexts that often have non-standard words, misspellings, transliteration, and abbreviations (Baldwin et al., 2013). Consider (4), a French-Arabic utterance that has undergone transliteration, abbreviation and diacritic removal. Introduction Codemixed text is common in user-generated content, such as web articles, tweets, and message boards, but most current language ID models ignore it. Codemixing involves language switches within and across constituents, as seen in these English-Spanish and English-Hindi examples. (1) dame [N P ese book that you told me about] Give me this book that you told me about. (2) [N P aapki profile photo] [V P pyari hai] Your profile photo is lovely. como se llama un squirrel en"
D18-1030,N10-1027,0,0.043424,"Missing"
D18-1030,E17-2068,0,0.116779,"Missing"
D18-1030,W14-3914,0,0.0512279,"Missing"
D18-1030,P17-2009,0,0.0178816,"or inter-mix example, which are two of the most prominent types of codemixing in the real world (Barman et al., 2014; Das and Gamb¨ack, 2014).4 An intra-mix sentence like (1) starts with one language and switches to another language, while an inter-mix sentence like (2) has Language ID of monolingual text has been extensively studied (Hughes et al., 2006; Baldwin and Lui, 2010; Lui and Baldwin, 2012; King and Abney, 2013), but language ID for codemixed text has received much less attention. Some prior work has focused on identifying larger language spans in longer documents (Lui et al., 2014; Jurgens et al., 2017) or estimating proportions of multiple languages in a text (Lui et al., 2014; Kocmi and Bojar, 2017). Others have focused on token-level language ID; some work is constrained to predicting word-level labels from a single language pair (Nguyen and Do˘gru¨oz, 2013; Solorio et al., 2014; Molina et al., 2016a; Sristy et al., 2017), while others permit a handful of languages (Das and Gamb¨ack, 2014; Sristy et al., 2017; Rijhwani et al., 2017). In contrast, CMX supports 100 languages. Unlike most previous work–with Rijhwani et al. 2017 a notable exception–we do not assume a particular language pair"
D18-1030,W14-3902,0,0.0833535,"Missing"
D18-1030,N13-1131,0,0.18557,"nguage pairs, mainly including the combination of English and a non-English language. The full set is listed in the supplemental material. Then we choose uniformly between generating an intra-mix or inter-mix example, which are two of the most prominent types of codemixing in the real world (Barman et al., 2014; Das and Gamb¨ack, 2014).4 An intra-mix sentence like (1) starts with one language and switches to another language, while an inter-mix sentence like (2) has Language ID of monolingual text has been extensively studied (Hughes et al., 2006; Baldwin and Lui, 2010; Lui and Baldwin, 2012; King and Abney, 2013), but language ID for codemixed text has received much less attention. Some prior work has focused on identifying larger language spans in longer documents (Lui et al., 2014; Jurgens et al., 2017) or estimating proportions of multiple languages in a text (Lui et al., 2014; Kocmi and Bojar, 2017). Others have focused on token-level language ID; some work is constrained to predicting word-level labels from a single language pair (Nguyen and Do˘gru¨oz, 2013; Solorio et al., 2014; Molina et al., 2016a; Sristy et al., 2017), while others permit a handful of languages (Das and Gamb¨ack, 2014; Sristy"
D18-1030,P15-1032,1,0.820227,", 3, 4 respectively; these values yield good performance and are far smaller than the number of n-gram types. Table 3: Feature spaces of CMX. The window column indicates that CMX uses character n-gram and lexicon features extracted from the previous and following tokens as well as the current one. is significantly faster than structured training (especially considering the large label set inherent in language ID), and (3) it is far easier to implement. 3.1 Token Model Simple feed-forward networks have achieved near state-of-the-art performance in a wide range of NLP tasks (Botha et al., 2017; Weiss et al., 2015). CMX follows this strategy, with embedding, hidden, and softmax layers as shown in Figure 1. The inputs to the network are grouped feature matrices, e.g. character, script and lexicon features. Each group g’s features are represented by a sparse matrix Xg ∈ RFg ×Vg , where Fg is the number of feature templates and Vg is the vocabulary size of the feature group. The network maps sparse features to dense embedding vectors and concatenates them to form the embedding layer: h0 = vec[Xg Eg |∀g] Script features Some text scripts are strongly correlated with specific languages. For example, Hiragana"
D18-1030,E17-1087,0,0.0540301,"Missing"
D18-1030,P12-3005,0,0.101787,"Missing"
D18-1030,Q14-1003,0,0.0291027,"Missing"
D18-1030,W16-5805,0,0.452578,"has been extensively studied (Hughes et al., 2006; Baldwin and Lui, 2010; Lui and Baldwin, 2012; King and Abney, 2013), but language ID for codemixed text has received much less attention. Some prior work has focused on identifying larger language spans in longer documents (Lui et al., 2014; Jurgens et al., 2017) or estimating proportions of multiple languages in a text (Lui et al., 2014; Kocmi and Bojar, 2017). Others have focused on token-level language ID; some work is constrained to predicting word-level labels from a single language pair (Nguyen and Do˘gru¨oz, 2013; Solorio et al., 2014; Molina et al., 2016a; Sristy et al., 2017), while others permit a handful of languages (Das and Gamb¨ack, 2014; Sristy et al., 2017; Rijhwani et al., 2017). In contrast, CMX supports 100 languages. Unlike most previous work–with Rijhwani et al. 2017 a notable exception–we do not assume a particular language pair at inference time. Instead, we only assume a large fixed set of language pairs as a general constraint for all inputs. We define and evaluate CMX and show that it strongly outperforms state-of-the-art language ID models on three codemixed test sets covering ten languages, and a monolingual test set inclu"
D18-1030,D13-1084,0,0.0413714,"Missing"
D18-1030,P17-1180,0,0.0820539,"Missing"
D18-1030,W14-3907,0,0.0917334,"Missing"
D18-1080,D11-1038,0,0.525718,"its. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSpli"
D18-1080,P18-2114,0,0.345794,"ddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtained on WebSplit by Aharoni and Goldberg (2018). Both authors contributed equally. 732 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1080,Q15-1021,0,0.177909,"kipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and"
D18-1080,P17-1017,0,0.0261232,"oldberg, 2018). For training, we delimit the simple sentences with a special symbol. We depart from the prior work by only using a subset of the WebSplit training set: we take a fixed sub-sample such that each distinct C is paired with a single S, randomly selected from the multiple possibilities in the dataset. This scheme produced superior performance in preliminary experiments. As a quality measure, we report multi-reference corpus-level BLEU4 (Papineni et al., 2002), but Comparison to WebSplit Narayan et al. (2017) derived the WebSplit corpus by matching up sentences in the WebNLG corpus (Gardent et al., 2017) according to partitions of their underlying meaning representations (RDF triples). The WebNLG corpus itself was created by having crowd workers write sentential realizations of one or more RDF triples. The resulting language is often unnatural, for example, “Akeem Dent once played for the Houston Texans team which is based in Houston in Texas.”2 Repetition arises because the same sentence fragment may appear in many different examples. 3 We use WebSplit v1.0 throughout, which is the scaledup re-release by Narayan et al. (2017) at http://github. com/shashiongithub/Split-and-Rephrase, commit a9"
D18-1080,P08-2035,0,0.0421371,"lter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all c"
D18-1080,N09-2061,0,0.0244161,"napshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candidate split into S = (S1 , S2 ), we 2.2 Corpus Statistics and Quality Our extraction heuristic is imperfect, so we manually assess corpus quality using the same categorization schema proposed by Aharoni and Goldberg (2018); see Table 1 for"
D18-1080,D17-1213,0,0.0136428,"d pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we use a high-precision heuristic to retain only high quality splits. To extract a full sentence C and its candid"
D18-1080,P16-1154,0,0.06498,"ata produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resourc"
D18-1080,N10-1056,0,0.0293049,"pineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia page are then compared to check for sentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given all candidate examples extracted this way, we us"
D18-1080,D15-1076,0,0.0320725,"iddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containing one million rewrites: http://goo.gl/language/wiki-split • By incorporating WikiSplit into training, we more than double (30.5 to 62.4) the BLEU score obtaine"
D18-1080,D17-1004,0,0.0332047,"simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release"
D18-1080,P17-4012,0,0.0291392,"(sBLEU) for direct comparison to past work.5 We also report lengthbased statistics to quantify splitting. We use the same sequence-to-sequence architecture that produced the top result for Aharoni and Goldberg (2018), “Copy512”, which is a onelayer, bi-directional LSTM (cell size 512) with attention (Bahdanau et al., 2014) and a copying mechanism (See et al., 2017) that dynamically interpolates the standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of Aharoni and Goldberg (2018) using the OpenNMT-py framework (Klein et al., 2017).6 3.1 58.7 55.7 30.5 34.2 60.4 62.4 sBLEU – 56.1 53.0 25.5 30.5 58.0 60.1 In contrast, the W IKI S PLIT model achieves 59.4 BLEU on the WebSplit validation set, without observing any in-domain data. It also outperforms the two deterministic baselines on both validation sets by a non-trivial BLEU margin. This indicates that the WikiSplit training data enable better generalization than when using WebSplit by itself. Reintroducing the downsampled, in-domain training data (B OTH) further improves performance on the WebSplit evaluation. Results We compare to the S OURCE baseline, which is the prev"
D18-1080,C10-1152,0,0.275307,"heir extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatically better output for split and rephrase. A complex sentence can typically be rewritten into multiple simpler ones that together retain the same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. ("
D18-1080,W17-3204,0,0.0248613,"same meaning. Performing this split-and-rephrase task is one of the main operations in text simplification, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simplification has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in this way could also benefit systems where predictive quality degrades with sentence length, as observed in, e.g., relation extraction (Zhang et al., 2017) and translation (Koehn and Knowles, 2017). And the schema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and demonstrate that neural ∗ Our primary contributions are: • A scalable, language agnostic method for extracting split-and-rephrase rewrites from Wikipedia edits. • Public release of the English WikiSplit dataset, containi"
D18-1080,D17-1064,0,0.516619,"alex,jasonbaldridge,dipanjand}@google.com Google AI Language A classic leaf symptom is water-soaked lesions between the veins which appear as angular leaf-spots where the lesion edge and vein meet. Abstract Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia’s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et a"
D18-1080,P02-1040,0,0.102593,"t for training them. To that end, we introduce the WikiSplit corpus and detail its construction next. 2.1 Correct 161 168 169 Unsupp. 35 35 31 Miss. 6 4 4 Size 1.4m 1.0m 0.5m Table 2: Quality vs corpus size trade-off when setting the similarity threshold. The counts are for a random sample of 100 split-and-rephrase examples extracted using our method (i.e., 200 simple sentences). Keys: Unsupported; Missing require that C and S1 have the same trigram prefix, C and S2 have the same trigram suffix, and S1 and S2 have different trigram suffixes. To filter out misaligned pairs, we use BLEU scores (Papineni et al., 2002) to ensure similarity between the original and the split versions. Specifically, we discard pairs where BLEU(C, S1 ) or BLEU(C, S2 ) is less than δ (an empirically chosen threshold). If multiple candidates remain for a given sentence C, we retain arg maxS (BLEU(C, S1 ) + BLEU(C, S2 )).1 Mining Wikipedia Edits Wikipedia maintains snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010"
D18-1080,P17-1099,0,0.206771,"del with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark. 1 A classic leaf symptom is the appearance of angular, water-soaked lesions between the veins. The angular appearance results where the lesion edge and vein meet. Figure 1: A split-and-rephrase example extracted from a Wikipedia edit, where the top sentence had been edited into two new sentences by removing some words (yellow) and adding others (blue). encoder-decoder models (Bahdanau et al., 2014) perform poorly, even when enhanced with a copy mechanism (Gu et al., 2016; See et al., 2017). Introduction One limitation of the WebSplit examples themselves is that they contain fairly unnatural linguistic expression using a small vocabulary. We introduce new training data mined from Wikipedia edit histories that have some noise, but which have a rich and varied vocabulary over naturally expressed sentences and their extracted splits. Figure 1 gives an example of how a Wikipedia editor rewrote a single sentence into two simpler ones. We create WikiSplit, a set of one million such examples mined from English Wikipedia, and show that models trained with this resource produce dramatica"
D18-1080,D16-1033,0,0.104655,"Missing"
D19-1382,D18-1316,0,0.0315523,"Missing"
D19-1382,P18-1241,0,0.0668722,"Missing"
D19-1382,P17-1152,0,0.03558,"ails) 委 员 会 由 合 并 四河 (Four Rivers) 委员会和 奥杜邦 (Audubon) 委 员会成立. In the zh-s2 example, the parentheses give English glosses of Chinese entity mentions. 3 Evaluated Methods The goal of PAWS-X is to probe models’ ability to capture structure and context in a multilingual setting. We consider three models with varied complexity and expressiveness. The first baseline is a simple bag-of-words (BOW) encoder with cosine similarity. It uses unigram to bigram token encoding as input features and takes a cosine value above 0.5 as a paraphrase. The second model is ESIM, Enhanced Sequential Inference Model (Chen et al., 2017). Following Zhang et al. (2019), ESIM encodes each sentence using a BiLSTM, and passes the concatenation of encodings through a feed-forward layer for classification. The additional layers allow ESIM to capture more complex sentence interaction than cosine similarity. Third, Non-local context Word interaction Translate Train Translate Test Zero Shot Merged BOW × × X X × × ESIM X × X X × × BERT X X X X X X Table 3: Complexity of each evaluated model and the training/evaluation strategies being tested. we evaluate BERT, Bidirectional Encoder Representations from Transformers (Devlin et al., 2019"
D19-1382,D18-1269,0,0.128393,"Multi30k (Elliott et al., 2016) and Opusparcus (Creutz, 2018), lack challenging examples like PAWS. The lack of high-quality adversarial examples in other languages makes it difficult to benchmark model improvements. We bridge this gap by creating Cross-lingual PAWS (PAWS-X), an extension of the Wikipedia portion of the PAWS evaluation and test examples to six languages: Spanish, French, German, Chinese, Japanese, and Korean. This new corpus consists of 23,659 human translated example pairs with paraphrase judgments in each target language. Like previous work on multilingual corpus creation (Conneau et al., 2018), we machine translate the original PAWS English training set (49,401 pairs). Note that all translated pairs still have high word overlap and they inherit semantic similarity labels from the original PAWS examples; thus, the resulting dataset preserves the ability of probing structure and context sensitivity for models. We also machine translate the evaluation pairs of each language into English to establish the baseline performance of a translatethen-predict strategy. The PAWS-X dataset, including both the new human translated pairs and the machine translated examples, is available for downlo"
D19-1382,L18-1218,0,0.0346582,"emantic meaning. In addition to revealing failures of state-of-the-art models, research on adversarial examples has generally shown that augmenting training data with good adversarial examples can boost performance for some models—providing greater clarity to the modeling landscape as well providing new headroom for further improvements. Most previous work focuses only on English despite the fact that the problems highlighted by adversarial examples are shared by other languages. Existing multilingual datasets for paraphrase identification, e.g. Multi30k (Elliott et al., 2016) and Opusparcus (Creutz, 2018), lack challenging examples like PAWS. The lack of high-quality adversarial examples in other languages makes it difficult to benchmark model improvements. We bridge this gap by creating Cross-lingual PAWS (PAWS-X), an extension of the Wikipedia portion of the PAWS evaluation and test examples to six languages: Spanish, French, German, Chinese, Japanese, and Korean. This new corpus consists of 23,659 human translated example pairs with paraphrase judgments in each target language. Like previous work on multilingual corpus creation (Conneau et al., 2018), we machine translate the original PAWS"
D19-1382,N19-1423,0,0.0672669,"(Chen et al., 2017). Following Zhang et al. (2019), ESIM encodes each sentence using a BiLSTM, and passes the concatenation of encodings through a feed-forward layer for classification. The additional layers allow ESIM to capture more complex sentence interaction than cosine similarity. Third, Non-local context Word interaction Translate Train Translate Test Zero Shot Merged BOW × × X X × × ESIM X × X X × × BERT X X X X X X Table 3: Complexity of each evaluated model and the training/evaluation strategies being tested. we evaluate BERT, Bidirectional Encoder Representations from Transformers (Devlin et al., 2019), which recently achieved state-of-the-art results on eleven natural language processing tasks. We evaluate all models with two strategies (Conneau et al., 2018): (1) Translate Train: the English training data is machine-translated into each target language to provide data to train each model and (2) Translate Test: train a model using the English training data, and machine-translate all test examples to English for evaluation. Multilingual BERT is a single model trained on 104 languages, which enables experiments with cross-lingual training regimes. (1) Zero Shot: the model is trained on the"
D19-1382,W16-3210,0,0.0324928,"Missing"
D19-1382,P18-2103,0,0.0555793,"Missing"
D19-1382,N18-1170,0,0.0649638,"Missing"
D19-1382,D17-1215,0,0.0850713,"Missing"
D19-1382,P18-1079,0,0.0554674,"Missing"
D19-1382,N19-1131,1,0.850042,"Missing"
E03-1036,P02-1041,1,0.596625,"Missing"
E03-1036,E95-1018,0,0.029571,"ich it allows us to cast aside rule restrictions for controlling the grammar. This is a principled move which replaces arbitrary, globally declared restrictions with a small set of cross-linguistically motivated distinctions encoded in terms of the multiple slash types utilized in CTL. Baldridge (2002) sup216 ports this formulation with multi-modal analyses for a wide range of phenomena in English, Dutch, Turkish, Tagalog, and Toba Batak. 5 Extensions Modes lead not only to a very clean formalization of resource-sensitivity, but also give rise to an interesting linguistic perspective. As e.g. Hepple (1995) notes, each pair of decorated slashes { „ /i} can correspond to a particular grammatical phenomenon. Rules then model how different phenomena can be combined to form larger grammatical structures. For example, one way we can use modal decoration is to model dependency, the asymmetry between heads and dependents, which has been used in CTL to give accounts of coordination and word order, e.g. (Moortgat and Morrill, 1991; Kruijff, 2001). Observe that we really need modes here: function-argument structure does not correspond to dependency. For example, a sentential adjunct can have the category"
E03-1036,P90-1001,0,0.201512,"vely. Because of residuation, we can drop CIL<&gt;t r if we do not need to type-raise. Finally, the multi-modal setting also enables us to introduce more powerful combinators into the grammar, possibly taking it beyond mild contextsensitivity. Precisely because of the tight resourcesensitive control over the applicability of combinatory rules, we can avoid a collapse to a situation where ""anything goes"". - 6 Computational aspects and implementation CCG has mildly context-sensitive generative power and CCG grammars can be parsed in worstcase polynomial time by using a structure sharing algorithm (Vijay-Shanker and Weir, 1990). This algorithm does incur some computational overhead, and Komagata (1999) shows that the performance of a worst-case exponential CKY parser with a semantic equivalency check is cubic in the average case (tested on Japanese sentences averaging 20 words in length). This a major attraction of CCG over CTL, for which no reasonably efficient parsers have been constructed that can handle realistic grammars Multi-Modal CCG inherits CCG's attractive computational properties and adds the possibility to take advantage of some new strategies. Most importantly, it remains mildly context-sensitive. We h"
I11-1022,D10-1072,1,0.760614,"ermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the dependency is formed when the two heads combine, it is available to be used as a local feature by the semantic role labeler. This property of CCG and its impact on packed-chart SRL is described extensively in Boxwell et al. (2010). This ability to predict dependencies (and semantic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in"
I11-1022,C04-1041,0,0.204923,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,P04-1014,0,0.203309,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,E09-1026,0,0.0598503,"Missing"
I11-1022,W03-1008,0,0.195301,"entence if it could combine with a noun phrase to the right and another noun phrase to the left”. The process of automatically assigning CCG categories to words is called “supertagging”, and CCG categories are sometimes informally referred to as “supertags”. An example of how categories combine to make sentences is shown in Figure 1. CCG has many capabilities that go beyond that of a typical context-free grammar. First, it has a sophisticated internal system of managing syntactic heads and dependencies1 . These dependencies are used to great effect in CCG-based semantic role labeling systems (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), as they do not suffer the same data-sparsity effects encountered with treepath features in CFG-based SRL systems. Secondly, CCG permits these dependencies to be passed through intermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the depen"
I11-1022,J07-3004,0,0.0683133,"esource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conferen"
I11-1022,N10-1137,0,0.0213533,"like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 192–200, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP The phrases when combined with a noun to the right”. The rightmost category indicates the argument that the category is seeking, the leftmost category indicates the res"
I11-1022,C08-1008,1,0.899679,"Missing"
I11-1022,J93-2004,0,0.0366651,"cting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential."
I11-1022,boxwell-white-2008-projecting,1,0.896164,"ntic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in terminal indexation (Honnibal and Curran, 2007; Boxwell and White, 2008). The system is organized in a twostage pipeline of maximum entropy models3 , following the organization of a previous CFG-style approach (Punyakanok et al., 2008). The first stage is the identification stage, where, for each predicate in the sentence, each word is tagged as either a role or a nonrole (figure 3). The second stage is the classification stage, where the roles are sorted into A RG 0, A RG 1, and so on (figure 4). The identification model and the classification model share the same features, but they are trained and run separately. For the results presented here, we use a version"
I11-1022,J05-1004,0,0.0128898,"ics relationship, and then predicting roles on novel syntactic analyses. The gold standard syntactic training data can be eliminated from the process by extracting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combin"
I11-1022,P10-1051,1,0.895534,"Missing"
I11-1022,J08-2005,0,\N,Missing
I11-1022,P09-1005,1,\N,Missing
K15-1003,J07-3004,0,0.0232518,") 0 A(y |y) = min 1, p(y |θ LCTX , θ RCTX ) For completeness, we note that the probability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the tree"
K15-1003,C08-1008,1,0.86999,"equence ADJ NOUN frequently occurs between the tags DET and VERB. This DET—VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN . From this, we might deduce that DET — VERB is a likely context for a noun phrase. CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability. However, since there is nothing intrinsic about the POS pair DET—VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data. Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000; Steedman and Baldridge, 2011) categories reflect universal grammatical properties. CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents. For example, a category might encode that “this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence” instead of simply V"
K15-1003,N07-1018,0,0.284036,"design of models and learning procedures that result in better parsing tools. Given our desire to train NLP models in low-supervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model, we develop a blocked sampler based on that of Johnson et al. (2007) to sample parse trees for sentences in the raw training corpus according to their posterior probabilities. However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simp"
K15-1003,P02-1017,0,0.256356,"it the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured. 1 Introduction Learning parsers from incomplete or indirect supervision is an important component of moving NLP research toward new domains and languages. But with less information, it becomes necessary to devise ways of making better use of the information that is available. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. One important example is the constituentcontext model (CCM) of Klein and Manning (2002), which was specifically designed to capture the linguistic observation made by Radford (1988) that there are regularities to the contexts in which constituents appear. This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, 22 Proceedings of the 19th Conference on Computational Language Learning, pages 22–31, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics able data in the context of studying child language acquisition (e.g., Villavicencio, 2002; Goldwater, 2007), we are interested in applying t"
K15-1003,Q13-1007,0,0.0395254,"Missing"
K15-1003,D14-1107,0,0.0241254,"he interaction between universal grammar and observ23 n s s
p np np/n The pp n man (s
p)/pp pp/np walks to np work n/n n s
p The lazy dog sleeps Figure 2: Higher-level category n subsumes the categories of its constituents. Thus, n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associativ"
K15-1003,I11-1049,0,0.0375395,"Missing"
K15-1003,J93-2004,0,0.0511752,"robability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introd"
K15-1003,D10-1120,0,0.0495496,"Missing"
K15-1003,bosco-etal-2000-building,0,0.0377197,"≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Gigaword 5 corp"
K15-1003,J99-1004,0,0.0388977,"adverb that modifies a verb); and (4) operators may occur at different rates, as given by pfwd . We can use PCAT to define priors on our production parameters that bias our model toward rules 1 Note that this version has also updated the probability definitions for modifiers to be sums, incorporating the fact that any A/A is also a A/B (likewise for AA). This ensures that our grammar defines a valid probability distribution. 2 The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). Non-terminal production prior means For the root, binary, and unary parameters, we want to choose prior means that encode our bias 25 that result in a priori more likely categories:3 atoms have features associated, then the atoms are allowed to unify if the features match, or if at least one of them does not have a feature. In defining κ, it is also important to ignore possible arguments on the wrong side of the combination since they can be consumed without affecting the connection between the two. To achieve this for κ(t, u), it is assumed that it is possible to consume all preceding argum"
K15-1003,J07-4004,0,0.0423889,"n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associative since a constituent label is often able to combine on either the left or the right. As a motivating example, consider the sentence “The lazy dog sleeps”, as shown in Figure 2. The word lazy, with category n/n, can either combine"
K15-1003,D12-1075,1,0.861669,"categories:  σ · 1/|T | if κ(t, r) σ > 1 right P (r |t) = 1/|T | otherwise  σ · PCAT (r) if κ(t, r) σ > 1 right PCAT (r |t) = PCAT (r) otherwise θ ROOT-0 (t) = PCAT (t) θ BIN -0 (hu, vi) = PCAT (u) · PCAT (v) θ UN -0 (hui) = PCAT (u) For simplicity, we assume the production-type mixture prior to be uniform: λ0 = h 13 , 31 , 31 i. 3.2 Terminal production prior means We employ the same procedure as our previous work for setting the terminal production prior distributions θtTERM -0 (w) by estimating word-givencategory relationships from the weak supervision: the tag dictionary and raw corpus (Garrette and Baldridge, 2012; Garrette et al., 2015).4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the “openness” of each tag in order to assess its likelihood of appearing with new words. 3.3 Context parameter prior means In order to encourage our model to choose trees in which the constituent labels “fit” into their supertag contexts, we want to bias our context parameters toward context categories that"
K15-1003,C10-1122,0,0.0163412,"the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In"
K15-1003,W14-1615,1,0.846747,"allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014). In this paper, we present a novel parsing model that is designed specifically for the capacity to capture both of these universal, intrinsic properties of CCG. We do so by extending our previous, PCFG-based parsing model to include parameters that govern the relationship between constituent categories and the preterminal categories (also known as supertags) to the left and right. The advantage of modeling context within a CCG framework is that while CCM must learn which contexts are likely purely from the data, the CCG categories give us obvious a priori information about whether a context i"
K15-1003,D07-1071,0,0.0387248,"n we would have to take a score of zero for that sentence: every dependency would be “wrong”. Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either sdcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the “deletion” strategy employed by Zettlemoyer and Collins (2007), but we do it directly in the grammar. We add unary rules of the form hDi→u 29 the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels. Other researchers have shown positive results for grammar induction by introducing relatively small amounts of linguistic knowledge. Naseem et al. (2010) induced dependency parsers by handconstructing a small set of linguistically-universal dependency rules and using them as soft constraints during learning. These rules were useful for disambiguating between various structures in"
K19-1006,D18-1015,0,0.0647083,"Missing"
K19-1006,P19-1647,0,0.207845,"learning language representations that text-based work—which represents the vast majority—gets a free pass on. Recent work has explored various means to transform raw speech into symbolic forms with little or no supervision (Park and Glass, 2007; Varadarajan et al., 2008; Ondel et al., 2016; Kamper et al., We address the problem of relating images to audio captions that describe them (Figure 1), building on previous research into learning from visually grounded, untranscribed speech (Harwath and Glass, 2015; Sun et al., 2016; Harwath et al., 2016; Chrupała et al., 2017; Kamper et al., 2017b; Chrupała, 2019; Harwath and Glass, 2019). Such problem settings provide opportunities both to improve our theoretical understanding of language ∗ Work done as a member of the Google AI Residency Program. 55 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 55–65 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics as well as to realize gains on practical problems— including voice interaction with virtual assistants, image retrieval based on speech, and generally better supporting people with visual impairments. Our contribution is to impro"
K19-1006,P01-1005,0,0.217476,"Missing"
K19-1006,E17-1104,0,0.0459173,"Missing"
K19-1006,N16-3020,0,0.0301743,"ent space for each of them. For simplicity, we show the text associated with each spoken caption. (brown dogs versus multicolored ones), people descriptions (elderly woman versus male dirt biker), object identification (e.g. a yellow pool noodle viewed as similar to slides), processes (jumping versus sliding) and perspective (man looking up versus viewed from behind and climbing). As such, there is clearly significant headroom for better, more finegrained modeling of both captions and images. Additionally, cross-modal attention mechanisms (Xu et al., 2015) and other explainability techniques (Ribeiro et al., 2016) could help better inspect and understand a model’s predictions. 4) is highly correlated with the correctly spelled sentences. 4.4 Human evaluation We ran human evaluations to answer two questions: (1) how much does cropping limit model performance? and (2) how much do retrieval evaluations based only on positive associations underestimate model performance? Hints about both questions can be seen in the qualitative evaluation (Fig. 4). To answer the first question, Table 3 shows the ratings for ground truth image/caption pairs in the FACC test set. The uncropped row shows that overall the capt"
K19-1006,P16-1009,0,0.0441248,"f the data on automatic retrieval metrics used thus far. Unsurprisingly, models retrieve many compatible results that are unpaired in FACC: with the human evaluations, we find consistent increases in recall. 2 easier via crowd computing (Buhrmester et al., 2011), but is still expensive and remains a bottleneck for creating broad and representative datasets. This motivates the case for exploiting incidental annotation (Roth, 2017) and automating some aspects of dataset creation. The current trend of using machine translation systems to produce augmented datasets for machine translation itself (Sennrich et al., 2016) and for monolingual tasks like classification (Yu et al., 2018) and paraphrasing (Wieting and Gimpel, 2018) is a good example of this. For speech image captioning, Chrupała et al. (2017) used a Text-to-Speech (TTS) system to create audio from the textual captions given in the MS-COCO dataset, resulting in 300k unique images with 5 spoken captions each. We scale this idea to the larger and more diverse textual Conceptual Captions dataset with 3.3 million unique image and captions, additionally modifying the produced speech by using multiple voices and random perturbations to the rate, pitch an"
K19-1006,P18-1085,0,0.076854,"Missing"
K19-1006,P18-1238,0,0.125754,"Missing"
K19-1006,Q14-1017,0,0.135706,"Missing"
K19-1006,P08-2042,0,0.0288458,"little from one writer to another. Speech is continuous and messy: the sounds used to convey a given word are modified by those of surrounding words, and the rate of speech, its pitch, and more vary across speakers and even for the same speaker in different contexts. As such, problems involving speech provide distinct challenges and opportunities for learning language representations that text-based work—which represents the vast majority—gets a free pass on. Recent work has explored various means to transform raw speech into symbolic forms with little or no supervision (Park and Glass, 2007; Varadarajan et al., 2008; Ondel et al., 2016; Kamper et al., We address the problem of relating images to audio captions that describe them (Figure 1), building on previous research into learning from visually grounded, untranscribed speech (Harwath and Glass, 2015; Sun et al., 2016; Harwath et al., 2016; Chrupała et al., 2017; Kamper et al., 2017b; Chrupała, 2019; Harwath and Glass, 2019). Such problem settings provide opportunities both to improve our theoretical understanding of language ∗ Work done as a member of the Google AI Residency Program. 55 Proceedings of the 23rd Conference on Computational Natural Langu"
K19-1006,P18-1042,0,0.0263362,"results that are unpaired in FACC: with the human evaluations, we find consistent increases in recall. 2 easier via crowd computing (Buhrmester et al., 2011), but is still expensive and remains a bottleneck for creating broad and representative datasets. This motivates the case for exploiting incidental annotation (Roth, 2017) and automating some aspects of dataset creation. The current trend of using machine translation systems to produce augmented datasets for machine translation itself (Sennrich et al., 2016) and for monolingual tasks like classification (Yu et al., 2018) and paraphrasing (Wieting and Gimpel, 2018) is a good example of this. For speech image captioning, Chrupała et al. (2017) used a Text-to-Speech (TTS) system to create audio from the textual captions given in the MS-COCO dataset, resulting in 300k unique images with 5 spoken captions each. We scale this idea to the larger and more diverse textual Conceptual Captions dataset with 3.3 million unique image and captions, additionally modifying the produced speech by using multiple voices and random perturbations to the rate, pitch and audio. Our goal is to make the resulting data more effective for pretraining models so they can learn more"
K19-1049,Q15-1011,0,0.0512663,"Missing"
K19-1049,N19-1423,0,0.0462941,"ican authors cluster together, as do the Existentialists; the cities have some geographical proximity, though Brazil and Portugal are neighbors, presumably because of shared language and culture. 6 Conclusion Our results with DEER show that a single-stage retrieval approach for entities from mentions is highly effective: without any domain-specific tuning, it performs at least as well as the best comparable two-stage systems. While our bag-of-ngrams encoders provided a strong proof of concept, we can almost certainly improve results with more sophisticated encoders, using a BERT architecture (Devlin et al., 2019), for example. Further, by virtue of approximate search techniques, it can be used for very 534 Mention Costa has not played since being struck by the AC Milan forward Baseline predictions Costa Coffee, Paul Costa Jr, Comstock-Needham system, Costa Cruises, Achille Costa Australia beat West Indies by five wickets in a World Series limited overs match World Series, ATP International Series, 2010 World Series Justin made his second straight start for Harbaugh, who has a knee injury plays for the Cape Town-based Cobras franchise OSI reports profit on overseas cancer drug sales Justin (historian),"
K19-1049,Q14-1037,0,0.0407183,"d Dredze, 2008). These averaged embeddings are concatenated and then passed through a feed-forward layer. For the category features, each entity category name is treated as a sparse input, and the embeddings for all categories for an entity are averaged to produce a 300-dimensional representation, which in turn is passed through a feed-forward layer (Figure 1c). Our experiments show that this architecture is highly effective for both retrieval and resolution. Nevertheless, we expect that additional modeling ideas will further improve performance, especially for resolution. Recent work such as Durrett and Klein (2014) has shown improvements derived from better, longer-range, context features; similarly, there are many more potentially useful KB-derived features. More complex encoder architectures that use some form of attention over the input tokens and features could also be beneficial. 4.2 gets a score of 1 if the correct entity is ranked above all in-batch random negatives, 0 otherwise. We stop training after the metric flattens out (about 40M steps). For all experiments, we use a batch size of 100, standard SGD with Momentum of 0.9 and a fixed learning rate 0.01. Our aim here is to demonstrate a pure r"
K19-1049,K17-1008,0,0.042755,"these ideas for entity linking. As a result, we demonstrate the first accurate, robust, and highly efficient system that is actually a viable substitute for standard, more cumbersome twostage retrieval and re-ranking systems. In contrast with existing literature, which reports multiple seconds to resolve a single mention, we can provide strong retrieval performance across all 5.7 million Wikipedia entities in around 3ms per mention. 2 Related work Most recent work on entity resolution has focused on training neural network models for the candidate reranking stage (Francis-Landau et al., 2016; Eshel et al., 2017; Yamada et al., 2017a; Gupta et al., 2017; Sil et al., 2018). In general, this work explores useful context features and novel architectures for combining mention-side and entity-side features. Extensions include joint resolution over all entities in a document (Ratinov et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017), joint modeling with related tasks like textual similarity (Yamada et al., 2017b; Barrena et al., 2018) and cross-lingual modeling (Sil et al., 2018), for example. By contrast, since we are using a two-tower or dual encoder architecture (Gillick et al., 2018; Serba"
K19-1049,N16-1150,0,0.0268217,"is the first combination of these ideas for entity linking. As a result, we demonstrate the first accurate, robust, and highly efficient system that is actually a viable substitute for standard, more cumbersome twostage retrieval and re-ranking systems. In contrast with existing literature, which reports multiple seconds to resolve a single mention, we can provide strong retrieval performance across all 5.7 million Wikipedia entities in around 3ms per mention. 2 Related work Most recent work on entity resolution has focused on training neural network models for the candidate reranking stage (Francis-Landau et al., 2016; Eshel et al., 2017; Yamada et al., 2017a; Gupta et al., 2017; Sil et al., 2018). In general, this work explores useful context features and novel architectures for combining mention-side and entity-side features. Extensions include joint resolution over all entities in a document (Ratinov et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017), joint modeling with related tasks like textual similarity (Yamada et al., 2017b; Barrena et al., 2018) and cross-lingual modeling (Sil et al., 2018), for example. By contrast, since we are using a two-tower or dual encoder architecture (Gillick"
K19-1049,D14-1162,0,0.0880733,"Missing"
K19-1049,W08-0804,0,0.0127364,"aining time. In fact, more than 1M candidate entities available at retrieval time have no associated training examples, but this architecture allows these to be encoded using their feature representations. 530 A shared embedding look-up is used for all text features (Figure 1b). Specifically, we embed all unigrams and bigrams to get 300-dimensional averaged unigram embeddings and 300-dimensional averaged bigram embeddings for each text feature. Unigram embeddings are initialized from GloVe vectors (Pennington et al., 2014), and we use 5M hash buckets for out-of-vocabulary unigrams and bigrams(Ganchev and Dredze, 2008). These averaged embeddings are concatenated and then passed through a feed-forward layer. For the category features, each entity category name is treated as a sparse input, and the embeddings for all categories for an entity are averaged to produce a 300-dimensional representation, which in turn is passed through a feed-forward layer (Figure 1c). Our experiments show that this architecture is highly effective for both retrieval and resolution. Nevertheless, we expect that additional modeling ideas will further improve performance, especially for resolution. Recent work such as Durrett and Kle"
K19-1049,D17-1277,0,0.108744,"Missing"
K19-1049,D17-1284,0,0.0640345,"t, we demonstrate the first accurate, robust, and highly efficient system that is actually a viable substitute for standard, more cumbersome twostage retrieval and re-ranking systems. In contrast with existing literature, which reports multiple seconds to resolve a single mention, we can provide strong retrieval performance across all 5.7 million Wikipedia entities in around 3ms per mention. 2 Related work Most recent work on entity resolution has focused on training neural network models for the candidate reranking stage (Francis-Landau et al., 2016; Eshel et al., 2017; Yamada et al., 2017a; Gupta et al., 2017; Sil et al., 2018). In general, this work explores useful context features and novel architectures for combining mention-side and entity-side features. Extensions include joint resolution over all entities in a document (Ratinov et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017), joint modeling with related tasks like textual similarity (Yamada et al., 2017b; Barrena et al., 2018) and cross-lingual modeling (Sil et al., 2018), for example. By contrast, since we are using a two-tower or dual encoder architecture (Gillick et al., 2018; Serban et al., 2018), our model cannot use any"
K19-1049,P13-2006,0,0.176381,"Missing"
K19-1049,D17-1195,0,0.0547684,"Missing"
K19-1049,Q15-1023,0,0.0813246,"Missing"
K19-1049,P11-1138,0,0.0741583,"iple seconds to resolve a single mention, we can provide strong retrieval performance across all 5.7 million Wikipedia entities in around 3ms per mention. 2 Related work Most recent work on entity resolution has focused on training neural network models for the candidate reranking stage (Francis-Landau et al., 2016; Eshel et al., 2017; Yamada et al., 2017a; Gupta et al., 2017; Sil et al., 2018). In general, this work explores useful context features and novel architectures for combining mention-side and entity-side features. Extensions include joint resolution over all entities in a document (Ratinov et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017), joint modeling with related tasks like textual similarity (Yamada et al., 2017b; Barrena et al., 2018) and cross-lingual modeling (Sil et al., 2018), for example. By contrast, since we are using a two-tower or dual encoder architecture (Gillick et al., 2018; Serban et al., 2018), our model cannot use any kind of attention over both mentions and entities at once, nor feature-wise comparisons as done by Francis-Landau et al. (2016). This is a fairly severe constraint – for example, we cannot directly compare the mention span to the entity title"
K19-1049,K16-1025,0,0.0979667,"Missing"
K19-1049,Q17-1028,0,0.113534,"ty linking. As a result, we demonstrate the first accurate, robust, and highly efficient system that is actually a viable substitute for standard, more cumbersome twostage retrieval and re-ranking systems. In contrast with existing literature, which reports multiple seconds to resolve a single mention, we can provide strong retrieval performance across all 5.7 million Wikipedia entities in around 3ms per mention. 2 Related work Most recent work on entity resolution has focused on training neural network models for the candidate reranking stage (Francis-Landau et al., 2016; Eshel et al., 2017; Yamada et al., 2017a; Gupta et al., 2017; Sil et al., 2018). In general, this work explores useful context features and novel architectures for combining mention-side and entity-side features. Extensions include joint resolution over all entities in a document (Ratinov et al., 2011; Globerson et al., 2016; Ganea and Hofmann, 2017), joint modeling with related tasks like textual similarity (Yamada et al., 2017b; Barrena et al., 2018) and cross-lingual modeling (Sil et al., 2018), for example. By contrast, since we are using a two-tower or dual encoder architecture (Gillick et al., 2018; Serban et al., 2018), our"
K19-1049,W11-0329,0,0.100322,"quality candidate entities very efficiently. • Our model significantly outperforms discrete retrieval baselines like an alias table or BM25, and gives results competitive with the best reported accuracy on the standard TACKBP-2010 dataset. • We provide a qualitative analysis showing that the model integrates contextual information and world knowledge even while simultaneously managing mention-to-title similarity. We acknowledge that most of the components of our work are not novel in and of themselves. Dual encoder architectures have a long history (Bromley et al., 1994; Chopra et al., 2005; Yih et al., 2011), including for retrieval (Gillick et al., 2018). Negative sampling strategies have been employed for many models and applications, e.g. Shrivastava et al. (2016). Approximate nearest neighbor search is its own sub-field of study (Andoni and Indyk, 2008). Nevertheless, to our knowledge, our work is the first combination of these ideas for entity linking. As a result, we demonstrate the first accurate, robust, and highly efficient system that is actually a viable substitute for standard, more cumbersome twostage retrieval and re-ranking systems. In contrast with existing literature, which repor"
M98-1022,M95-1005,0,0.221885,"Missing"
N04-1012,W03-0403,1,\N,Missing
N04-1012,C02-2025,0,\N,Missing
N04-1012,W00-1306,0,\N,Missing
N04-1012,P02-1016,0,\N,Missing
N04-1012,P99-1069,0,\N,Missing
N04-1012,W02-2018,0,\N,Missing
N04-1012,P93-1024,0,\N,Missing
N04-1012,P01-1019,0,\N,Missing
N07-1030,W02-2018,0,0.0199765,"lapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: exp( PC (COREF|hi, ji) = n P λk fk (hi, ji, COREF)) k=1 Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬ COREF (i.e. a negative instance) depending on whether or not the two mentions corefer. In generating the training data, we followed the method of (Soon et al., 2001) creating for each anaphor: (i) a po"
N07-1030,W99-0212,0,0.0462316,"OREF|hi, ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) apply237 ing a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC (COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: exp( PC (COREF|hi, ji) = n P λk fk (hi, ji, COREF)) k=1 Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classi"
N07-1030,P00-1023,0,0.0261984,"of having a coreferential outcome given a pair of mentions hi, ji, and (ii) apply237 ing a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC (COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: exp( PC (COREF|hi, ji) = n P λk fk (hi, ji, COREF)) k=1 Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constr"
N07-1030,C02-1139,0,0.941419,"In the system of Soon et. al. (2001) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1 Available from tadm.sf.net. formed is then evaluated by the classifier, which returns a probability representing the likelihood that these two mentions are coreferential. Soon et. al. (2001) use “Closest-First” selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability &gt; .5) is found or the beginning of the text is reached. Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b). Our features for the coreference classifier fall into three main categories: (i) features of the anaphor, (ii) features of antecedent mention, and (iii) relational features (i.e., features that describe properties which hold between the two mentions, e.g. distance). This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a). We omit details here for the sake of brevity — the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets. 3 Base models: anaphoricity classifier As mentioned in the i"
N07-1030,P02-1014,0,0.795601,"In the system of Soon et. al. (2001) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1 Available from tadm.sf.net. formed is then evaluated by the classifier, which returns a probability representing the likelihood that these two mentions are coreferential. Soon et. al. (2001) use “Closest-First” selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability &gt; .5) is found or the beginning of the text is reached. Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b). Our features for the coreference classifier fall into three main categories: (i) features of the anaphor, (ii) features of antecedent mention, and (iii) relational features (i.e., features that describe properties which hold between the two mentions, e.g. distance). This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a). We omit details here for the sake of brevity — the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets. 3 Base models: anaphoricity classifier As mentioned in the i"
N07-1030,W04-2401,0,0.766997,"te models. The other thing to note is that the results in general provide a lot of room for improvement — this is true for other state-of-the-art systems as well. The integer programming formulation we provide here has qualities which address both of these issues. In particular, we define two objective functions for coreference resolution to be optimized with ILP. The first uses only information from the coreference classifier (COREF - ILP) and the second integrates both anaphoricity and coreference in a joint formulation (JOINT- ILP). Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver. 5.1 COREF - ILP : 5.2 JOINT- ILP : joint anaphoricity-coreference formulation coreference-only formulation Barzilay and Lapata (2006) use ILP for the problem of aggregation in natural language generation: clustering sets of propositions together to create more concise texts. They cast it as a set p"
N07-1030,N06-1046,0,0.0828113,"g to note is that the results in general provide a lot of room for improvement — this is true for other state-of-the-art systems as well. The integer programming formulation we provide here has qualities which address both of these issues. In particular, we define two objective functions for coreference resolution to be optimized with ILP. The first uses only information from the coreference classifier (COREF - ILP) and the second integrates both anaphoricity and coreference in a joint formulation (JOINT- ILP). Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver. 5.1 COREF - ILP : 5.2 JOINT- ILP : joint anaphoricity-coreference formulation coreference-only formulation Barzilay and Lapata (2006) use ILP for the problem of aggregation in natural language generation: clustering sets of propositions together to create more concise texts. They cast it as a set partitioning problem. This is ver"
N07-1030,J96-1002,0,0.00996022,"). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: exp( PC (COREF|hi, ji) = n P λk fk (hi, ji, COREF)) k=1 Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬ COREF (i.e. a negative instance)"
N07-1030,N04-1037,0,0.0893978,"Missing"
N07-1030,W97-0319,0,0.120232,"ability, PC (COREF|hi, ji), of having a coreferential outcome given a pair of mentions hi, ji, and (ii) apply237 ing a selection algorithm that will single out a unique candidate out of the subset of candidates i for which the probability PC (COREF|hi, ji) reaches a particular value (typically .5). We use a maximum entropy model for the coreference classifier. Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions. Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000). The model is defined in a standard fashion as follows: exp( PC (COREF|hi, ji) = n P λk fk (hi, ji, COREF)) k=1 Z(hi, ji) (1) Z(hi, ji) is a normalization factor over both outcomes (COREF and ¬COREF). Model parameters are estimated using maximum entropy (Berger et al., 1996). Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the core"
N07-1030,P04-1018,0,0.900102,"es where: (i) the system mistakenly identifies an antecedent for non-anaphoric mentions, and (ii) the The second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004). Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole. Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate. Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer linear programming (ILP) problem. This framework has several properties that make it highly suitable for addressing the two aforementioned problems. The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a 236 Proceedings of NAACL HLT 2007, pages 236–243, c Rochester, NY, April 2007. 2007 As"
N07-1030,P05-1020,0,0.557189,"second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004). Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole. Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate. Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. In this paper, we propose to recast the task of coreference resolution as an optimization problem, namely an integer linear programming (ILP) problem. This framework has several properties that make it highly suitable for addressing the two aforementioned problems. The first is that it can utilize existing classifiers; ILP performs global inference based on their output rather than formulating a 236 Proceedings of NAACL HLT 2007, pages 236–243, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics new inference procedure for solving the basic task. Secon"
N07-1030,J01-4004,0,0.947046,"t for Advanced Discriminative Modeling1 (Malouf, 2002). We use a Gaussian prior with a variance of 1000 — no attempt was made to optimize this value. Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively. Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬ COREF (i.e. a negative instance) depending on whether or not the two mentions corefer. In generating the training data, we followed the method of (Soon et al., 2001) creating for each anaphor: (i) a positive instance for the pair hi, ji where i is the closest antecedent for j, and (ii) a negative instance for each pair hi, ki where k intervenes between i and j. Once trained, the classifier is used to create a set of coreferential links for each test document; these links in turn define a partition over the entire set of mentions. In the system of Soon et. al. (2001) system, this is done by pairing each mention j with each preceding mention i. Each test instance hi, ji thus 1 Available from tadm.sf.net. formed is then evaluated by the classifier, which ret"
N07-1030,M95-1005,0,0.979946,"Missing"
N07-1030,P04-1020,0,\N,Missing
N13-1014,C04-1080,0,0.0535149,"r very small amounts of word-tag frequency information indirectly through LP (token-supervision).7 Some tokens might not have any associated tag labels after LP. This occurs when there is no path from a TOKEN node to any seeded nodes or when all tags for the TOKEN node have weights less than the threshold. Since we require a distribution for every token, we use a default distribution for such cases. Specifically, we use the unsupervised emission probability initialization of Garrette and Baldridge (2012), which captures both the estimated frequency of a tag and its openness using only a 7 See Banko and Moore (2004) for further discussion of these issues. hbi The man saw the saw hbi 1.0 D N V hbi 1.0 1.0 1.0 1.0 0.2 0.7 0.8 0.3 Figure 2: Weighted, greedy model minimization graph showing a potential state between the stages of the tag bigram choosing algorithm. Solid edges: selected bigrams. Dotted edges: holes in the path. small tag dictionary and unlabeled text. Finally, we ensure that tokens of words in the original tag dictionary are only assigned tags from its entry. With this filter, LP of course does not add new tags to known words (without it, we found performance drops). If the intersection of th"
N13-1014,P96-1041,0,0.104356,"s found for every raw corpus sentence. The result of weighted model minimization is this set of tag paths. Since each path represents a valid tagging of the sentence, we use this output as a noisily labeled corpus for initializing EM in stage three. 3.3 Tagger training Stage one provides an expansion of the initial labeled data and stage two turns that into a corpus of noisily labeled sentences. Stage three uses the EM algorithm initialized by the noisy labeling and constrained by the expanded tag dictionary to produce an HMM.10 The initial distributions are smoothed with one-count smoothing (Chen and Goodman, 1996). If human-tagged sentences are available as training data, then we use their counts to supplement the noisy labeled text for initialization and we add their counts into every iteration’s result. The HMM produced by stage three is not used directly for tagging since it will contain zeroprobabilities for test-corpus words that were unseen during training. Instead, we use it to provide a Viterbi labeling of the raw corpus, following the “auto-supervision” step of Garrette and Baldridge (2012). This material is then concatenated with the token-supervised corpus (when available), and used to train"
N13-1014,W02-2006,0,0.135403,"Missing"
N13-1014,P11-1061,0,0.668825,"ss digitized, for most under-studied languages. Subramanya et al. (2010) apply LP to the problem of tagging for domain adaptation. They construct an LP graph that connects tokens in low- and high-resource domains, and propagate labels from high to low. This approach addresses the problem of learning appropriate tags for unknown words within a language, but it requires that the language have at least one high-resource domain as a source of high quality information. For low-resource languages that have no significant annotated resources available in any domain, this technique cannot be applied. Das and Petrov (2011) and T¨ackstr¨om et al. (2013) learn taggers for languages in which there are no POS-annotated resources, but for which parallel texts are available between that language and a high-resource language. They project tag information from the high-resource language to the lowerresource language via alignments in the parallel text. However, large parallel corpora are not available for most low-resource languages. These are also expensive resources to create and would take considerably more effort to produce than the monolingual resources that our annotators were able to generate in a two-hour timef"
N13-1014,D12-1075,1,0.916907,"e column in the table gives the number of unique word/tag pairs derived from the data. We also wanted to directly compare the two annotators to see how the differences in their relative annotation speeds and quality would affect the overall ability to learn an accurate tagger. We thus had them complete the same two tasks for English. As can be seen in Table 1, there are clear differences between the two annotators. Most notably, annotator B was faster at annotating full sentences while annotator A was faster at annotating word types. 3 Approach Our approach to learning POS-taggers is based on Garrette and Baldridge (2012), which properly separated test data from learning data, unlike much previous work. The input to our system is a raw corpus and either a human-generated tag dictionary or human-tagged sentences. The majority of the system is the same for both kinds of labeled training data, but the following description will point out differences. The system has four main parts, in order: 1. 2. 3. 4. 3.1 Tag dictionary expansion Weighted model minimization Expectation maximization (EM) HMM training MaxEnt Markov Model (MEMM) training Tag dictionary expansion In a low-resource setting, most word types will not"
N13-1014,P08-1085,0,0.0390901,"ype. Likewise for the comma type, the annotator has incorrectly given “:” as a valid tag, and LP, which uses the tag dictionary, pushes this label to many tokens with high confidence. However, minimization is able to correct the problem. Finally, the word type “opposition” provides an example of the expected behavior for unknown words. The type is not in the tag dictionary, so EM assumes all tags are valid and uses many labels. LP expands the starting dictionary to cover the type, limiting it to only two tags. Minimization then determines that NN is the best tag for each token. 5 Related work Goldberg et al. (2008) trained a tagger for Hebrew using a manually-created lexicon which was not derived from an annotated corpus. However, their lexicon was constructed by trained lexicographers over a long period of time and achieves very high coverage of the language with very good quality. In contrast, our annotated data was created by untrained linguistics students working alone for just two hours. Cucerzan and Yarowsky (2002) learn a POS145 for *IN (1) EM 1,221 (2) LP 4,003 (3) min 4,004 gold 3,999 , (comma) *, (1) EM 24,708 (2) LP 15,505 (3) min 24,730 gold 24,732 opposition NN (1) EM 24 (2) LP 41 (3) min 4"
N13-1014,E09-1042,0,0.111334,"Missing"
N13-1014,D12-1127,0,0.423554,"Missing"
N13-1014,J93-2004,0,0.0439778,"(EM), and far outperforms just using EM on the raw annotations themselves. 2 Data Our experiments use Kinyarwanda (KIN), Malagasy (MLG), and English (ENG). KIN is a Niger-Congo language spoken in Rwanda. MLG is an Austronesian language spoken in Madagascar. Both KIN and 139 MLG are low-resource and KIN is morphologicallyrich. For each language, the word tokens are divided into four sets: training data to be labeled by annotators, raw training data, development data, and test data. For consistency, we use 100k raw tokens for each language. Data sources For ENG, we used the Penn Treebank (PTB) (Marcus et al., 1993). Sections 00-04 were used as raw data, 05-14 as a dev set, and 15-24 (473K tokens) as a test set. The PTB uses 45 distinct POS tags. The KIN texts are transcripts of testimonies by survivors of the Rwandan genocide provided by the Kigali Genocide Memorial Center. The MLG texts are articles from the websites1 Lakroa and La Gazette and Malagasy Global Voices,2 a citizen journalism site.3 Texts in both KIN and MLG were tokenized and labeled with POS tags by two linguistics graduate students, each of which was studying one of the languages. The KIN and MLG data have 14 and 24 distinct POS tags, r"
N13-1014,J94-2001,0,0.206809,"ggers trained on plentiful amounts of labeled word tokens is a success story of computational linguistics (Manning, 2011). However, research on learning taggers using type supervision (e.g. tag dictionaries or morphological transducers) has had a more checkered history. The setting is a seductive one: by labeling the possible parts-ofspeech for high frequency words, one might learn accurate taggers by incorporating the type information as constraints to a semi-supervised generative learning model like a hidden Markov model (HMM). Early work showed much promise for this strategy (Kupiec, 1992; Merialdo, 1994), but successive efforts in recent years have continued to peel away and address layers of unrealistic assumptions about the In most previous work, tag dictionaries are extracted from a corpus of annotated tokens. To explore the type-supervised scenario, these have been used as a proxy for dictionaries produced by linguists. However, this overstates their effectiveness. Researchers have often manually pruned tag dictionaries by removing low-frequency word/tag pairs; this violates the assumption that frequency information is not available. Others have also created tag dictionaries by extracting"
N13-1014,P09-1057,0,0.119791,"isier than the data from a typical corpus: the annotations were produced by a single non-native-speaker working alone for two hours. Therefore, dealing with the size and quality of training data were core challenges to our task. To learn a POS-tagger from so little labeled data, we developed an approach that starts by generalizing the initial annotations to the entire raw corpus. Our approach uses label propagation (LP) (Talukdar and Crammer, 2009) to infer tag distributions on unlabeled tokens. We then apply a novel weighted variant of the model minimization procedure originally developed by Ravi and Knight (2009) to estimate sequence and word-tag frequency information from an unlabeled corpus by approximating the minimal set of tag bigrams needed to explain the data. This combination of techniques turns a tiny, unweighted, initial tag dictionary into a weighted tag dictionary that covers the entire corpus’s vocabulary. This weighted information limits the potential damage of tag dictionary noise and bootstraps frequency information to approximate a good starting point for the learning of an HMM using expectation-maximization (EM), and far outperforms just using EM on the raw annotations themselves. 2"
N13-1014,C10-1106,0,0.564669,"d in the initial tag dictionary. EM-HMM training uses the tag dictionary to limit ambiguity, so a sparse tag dictionary is problematic because it does not sufficiently confine the parameter space.4 Small human sentences A KIN human TD A MLG human sentences B MLG human TD B ENG human sentences A ENG human TD A ENG human sentences B ENG human TD B KIN sent. 90 tok. 1537 92 1805 86 1897 107 2650 dict. 750 1798 666 1067 903 1644 959 1090 Table 1: Statistics for Kinyarwanda, Malagasy, and English data annotated by annotators A and B. dictionaries also interact poorly with the model minimization of Ravi et al. (2010): if there are too many unknown words, and every tag must be considered for them, then the minimal model will simply be the one that assumes that they all have the same tag. For these reasons, we automatically expand an initial small dictionary into one that has coverage for most of the vocabulary. We use label propagation (LP)—specifically, the Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)5 —which is a graph-based technique for spreading labels between related items. Our graphs connect token nodes to each other via feature nodes and are seeded with POS-tag labels from the h"
N13-1014,D10-1017,0,0.0478371,"ust two hours. Cucerzan and Yarowsky (2002) learn a POS145 for *IN (1) EM 1,221 (2) LP 4,003 (3) min 4,004 gold 3,999 , (comma) *, (1) EM 24,708 (2) LP 15,505 (3) min 24,730 gold 24,732 opposition NN (1) EM 24 (2) LP 41 (3) min 45 gold 45 *RP 2764 JJ NN 9 CD 5 JJS 4 PTD 3 VBP 3 1 DT 1 NNS 4 VBP 4 1 5 *: 9226 JJ 4 4 Table 5: Tag assignments in different scenarios. A star indicates an entry in the human-provided TD. tagger from existing linguistic resources, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages. Subramanya et al. (2010) apply LP to the problem of tagging for domain adaptation. They construct an LP graph that connects tokens in low- and high-resource domains, and propagate labels from high to low. This approach addresses the problem of learning appropriate tags for unknown words within a language, but it requires that the language have at least one high-resource domain as a source of high quality information. For low-resource languages that have no significant annotated resources available in any domain, this technique cannot be applied. Das and Petrov (2011) and T¨ackstr¨om et al. (2013) learn taggers for la"
N13-1014,Q13-1001,0,0.228117,"Missing"
N19-1131,D18-1316,0,0.022524,"tions for images (Lin et al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms. Our work presents two main departures. We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work. In addition, we release to public a large set of 108k example pairs with highquality human labels. We believe the new dataset will benefit future research on both adversarial example"
N19-1131,P18-1241,0,0.0371794,"ollecting paraphrases, e.g. from co-captions for images (Lin et al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms. Our work presents two main departures. We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work. In addition, we release to public a large set of 108k example pairs with highquality human labels. We believe the new dataset will benefit fu"
N19-1131,P17-1152,0,0.0541524,": two baseline encoders and four recent advanced models that achieved state-of-the-art or strong performance on paraphrase identification. Table 6 summarizes the models with respect to whether they represent non-local contexts or support crosssentential word interaction. The baseline models use cosine similarity with simple sentence encoders: a bag-of-words (BOW) encoder based on token unigram and bigram encodings and a bi-directional LSTM (BiLSTM) that produces a contextualized sentence encoding. A cosine value above .5 is taken as a paraphrase. ESIM. The Enhanced Sequential Inference Model (Chen et al., 2017) achieved competitive performance on eight sentence pair modeling tasks (Lan and Xu, 2018). It encodes each sentence using a BiLSTM, concatenates the encodings for each sentence in the pair, and passes them through a multi-layer perceptron (MLP) for classification. The additional layers allow ESIM to capture more complex sentence interaction than cosine similarity in the baseline models. DecAtt. The Decomposable Attention Model (Parikh et al., 2016) is one of the earliest models to introduce attention for paraphrase identification. It computes word pair interaction between two sentences and ag"
N19-1131,P18-1198,0,0.056873,"Missing"
N19-1131,L18-1218,0,0.0319698,"thousands of training examples. Our experimental results also demonstrate that PAWS effectively measures sensitivity of models to word order and structure. Unlike BERT, a simple BOW model fails to learn from PAWS training examples, demonstrating its weakness at capturing non-local contextual information. Our experiments show that the gains from PAWS examples correlate with the complexity of models. 2 Related Work Existing data creation techniques have focused on collecting paraphrases, e.g. from co-captions for images (Lin et al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notabl"
N19-1131,N18-1170,0,0.162946,"experimental results also demonstrate that PAWS effectively measures sensitivity of models to word order and structure. Unlike BERT, a simple BOW model fails to learn from PAWS training examples, demonstrating its weakness at capturing non-local contextual information. Our experiments show that the gains from PAWS examples correlate with the complexity of models. 2 Related Work Existing data creation techniques have focused on collecting paraphrases, e.g. from co-captions for images (Lin et al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they"
N19-1131,D17-1215,0,0.0524643,"ues have focused on collecting paraphrases, e.g. from co-captions for images (Lin et al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms. Our work presents two main departures. We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work. In addition, we release to public a large set of 108k example pairs with highquality human labels. We believe the new dat"
N19-1131,D18-1277,0,0.0379925,"Missing"
N19-1131,C18-1152,0,0.0230083,"ts [Flights] NNS [from] [New York] [to] [Florida] LOCATION IN NNS IN IN LOCATION LOCATION { Flights } { from, to } { New York, Florida } [Flights] [from] [Florida] [to] [New York] NNS IN LOCATION IN LOCATION LSTM LSTM LSTM LSTM LSTM Figure 2: Illustration of the generation method in three steps. (a) Tag words and phrases with part-of-speech NNS LOCATION (POS) and namedINentities. (b) Build candidate sets by grouping words and phrases with the same tag. (c) Under the constraints of tag sequence template and candidate sets, find sentences with high language model scores using beam search. 2019; Ettinger et al., 2018). There also exists prior work that directly uses structural information in modeling (Filice et al., 2015; Liu et al., 2018). All these prior approaches were evaluated on existing datasets. In contrast, we perform studies on PAWS, a new dataset that emphasizes the importance of capturing structural information in representation learning. While developing new models is beyond the scope of this paper, this new dataset can facilitate research in this direction. 3 PAWS Example Generation We define a PAWS pair to be a pair of sentences with high bag-of-words (BOW) overlap but different word order."
N19-1131,P15-1097,0,0.0713727,"Missing"
N19-1131,P18-2103,0,0.0697266,"slation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms. Our work presents two main departures. We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work. In addition, we release to public a large set of 108k example pairs with highquality human labels. We believe the new dataset will benefit future research on both adversarial example generation and improvement of model robustness. In our work, we demonstrate the importance of capturing non-local context"
N19-1131,Q18-1031,0,0.0601819,"Missing"
N19-1131,D17-1126,0,0.111012,"Missing"
N19-1131,C18-1328,0,0.022318,"trong performance on paraphrase identification. Table 6 summarizes the models with respect to whether they represent non-local contexts or support crosssentential word interaction. The baseline models use cosine similarity with simple sentence encoders: a bag-of-words (BOW) encoder based on token unigram and bigram encodings and a bi-directional LSTM (BiLSTM) that produces a contextualized sentence encoding. A cosine value above .5 is taken as a paraphrase. ESIM. The Enhanced Sequential Inference Model (Chen et al., 2017) achieved competitive performance on eight sentence pair modeling tasks (Lan and Xu, 2018). It encodes each sentence using a BiLSTM, concatenates the encodings for each sentence in the pair, and passes them through a multi-layer perceptron (MLP) for classification. The additional layers allow ESIM to capture more complex sentence interaction than cosine similarity in the baseline models. DecAtt. The Decomposable Attention Model (Parikh et al., 2016) is one of the earliest models to introduce attention for paraphrase identification. It computes word pair interaction between two sentences and aggregates aligned vectors for final classification. This model achieved state-ofthe-art res"
N19-1131,D18-1421,0,0.0420152,"Missing"
N19-1131,D18-1184,0,0.0440902,"Missing"
N19-1131,P08-1028,0,0.20918,"Missing"
N19-1131,D16-1244,0,0.118531,"Missing"
N19-1131,D14-1162,0,0.0862049,"Missing"
N19-1131,N18-1202,0,0.140469,"Missing"
N19-1131,P18-1079,0,0.0500962,"t al., 2014), tweets with shared URLs (Lan et al., 2017), subtitles (Creutz, 2018), and back translation (Iyyer et al., 2018). Unlike all previous work, we emphasize the collection of challenging negative examples. Our work closely relates to the idea of crafting adversarial examples to break NLP systems. Existing approaches mostly focused on adding labelpreserving perturbations to inputs, but with the effect of distracting systems from correct answers. Example perturbation rules include adding noise to inputs (Jia and Liang, 2017; Chen et al., 2018), word replacements (Alzantot et al., 2018; Ribeiro et al., 2018), and syntactic transformation (Iyyer et al., 2018). A notable exception is Glockner et al. (2018): they generated both entailment and contradiction examples by replacing words with their synonyms or antonyms. Our work presents two main departures. We propose a novel method that generates challenging examples with balanced class labels and more word reordering variations than previous work. In addition, we release to public a large set of 108k example pairs with highquality human labels. We believe the new dataset will benefit future research on both adversarial example generation and improvem"
N19-1131,N18-2002,0,0.0459614,"Missing"
N19-1131,Q18-1042,1,0.892556,"Missing"
N19-1131,1983.tc-1.13,0,0.148681,"Missing"
N19-1319,P18-1031,0,0.0235446,"th a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of bespoke, challenging text classification problems that crop up in practical settings (Yu et al., 2018). Obtaining representative labeled examples for classification problems with many labels, like taxonomies, is especially challen"
N19-1319,D14-1181,0,0.0498632,"specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of be"
N19-1319,S18-2031,1,0.938075,"same-label phenomenon holds; that depends on the granularity of the classes. Cross-example analysis is required to determine how neighbors at various distances are distributed among labels in the training data. This should allow us to include barley and peaches as evidence for a class like Agriculture but only barley for Grains. Most existing systems ignore cross-example parallelism and thus miss out on a strong classification signal. We introduce a flexible method for controlled generalization that selects syntactosemantic features from sparse representations constructed by Category Builder (Mahabal et al., 2018). Starting with sparse representations of words and their contexts, a tuning algorithm selects features with the relevant kinds and appropriate amounts of generalization, making use of parallelism among examples. This produces taskspecific dense embeddings for new texts that can be easily incorporated into classifiers. Our simplest model, CBC (Category Builder Classifier), is a feed-forward network that uses only CB embeddings to represent a document. For small amounts of training data, this simple model dramatically outperforms both CNNs and BERT (Devlin et al., 2018). When more data is avail"
N19-1319,N18-1202,0,0.0358907,"iring this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets. 1 Introduction Modern neural networks are highly effective for text classification, with convolutional neural networks (CNNs) as the de facto standard for classifiers that represent both hierarchical and ordering information implicitly in a deep network (Kim, 2014). Deep models pre-trained on language model objectives and fine-tuned to available training data have recently smashed benchmark scores on a wide range of text classification problems (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018). Despite the strong performance of these approaches for large text classification datasets, challenges still arise with small datasets with few, possibly imbalanced, training examples per class. LaDan Roth UPenn danroth@ seas.upenn.edu bels can be obtained cheaply from crowd workers for some languages, but there are a nearly unlimited number of bespoke, challenging text classification problems that crop up in practical settings (Yu et al., 2018). Obtaining representative labeled examples for classification problems with many labels, like taxonomie"
N19-1319,C18-1180,0,0.0323366,"Missing"
N19-1319,N16-2013,0,0.0170465,"mance with limited data. To that end, we obtain learning curves on four standard text classification datasets (Table 2) based on evaluating predictions on the full test sets. At each sample size, we produce multiple samples and run several text classification methods multiple times, measuring the following: 3159 • Macro-F1 score. Macro-F1 measures support for all classes better than accuracy, especially with imbalanced class distributions. • Recall for the rarest class. Many measures like F1 and accuracy often mask performance on infrequent but high impact classes, such as detecting toxicity (Waseem and Hovy, 2016)) • Degenerate solutions. Complex classifiers with millions of parameters sometimes produce degenerate classifiers when provided very few training examples; as a result, they can skip some output classes entirely. The datasets we chose for evaluation, while all multi-class, form a diverse set in terms of the number of classes and kinds of cohesion among examples in a single class. The former clearly affects training data needs, while the latter informs appropriate generalization. • 20 Newsgroups 20Newsgroups (20NG) contains documents from 20 different newsgroups with about 1000 messages from e"
N19-1319,N18-1109,0,0.0316836,"Missing"
N19-1319,D18-1030,1,0.865224,"Missing"
P02-1041,P02-1042,0,0.0314898,"representation to carry the burden. 4 CCG Coupled to HLDS In Dependency Grammar Logic (DGL), Kruijff (2001) couples HLDS to a resourcesensitive categorial proof theory (CTL) (Moortgat, 1997). Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG. First, parsing with CCG grammars for substantial fragments is generally more efficient than with CTL grammars with similar coverage. Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al., 2002). Second, syntactic features (modeled by unary modalities) in CTL have no intuitive semantic reflection, whereas CCG can relate syntactic and semantic features perspicuously using unification. Finally, CCG has a detailed syntactic account of the realization of information structure in English. To link syntax and semantics in derivations, every logical form in DGL expresses a nominal identifying its head in the format @i p. This handles dependents in a linguistically motivated way through a linking theory: given the form of a dependent, its (possible) role is established, after which its meanin"
P02-1041,P01-1019,0,0.0292759,"theories for discourse representation. InL attaches one to every formula representing its discourse referent. This results in a representation such as (4) for the sentence Ed came to the party. (4) [e][party(x); past(e); to(e; x); come(e; Ed)] InL thus flattens logical forms to some extent, using the indexes to spread a given entity or event through multiple predications. The use of indexes is crucial for UCG’s account of modifiers, and as we will see later, we exploit such referents to achieve similar ends when coupling HLDS and CCG. Minimal Recursion Semantics (MRS) (Copestake et al., 1999; Copestake et al., 2001) is a framework for computational semantics that is designed to simplify the work of algorithms which produce or use semantic representations. MRS provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. Flattening is achieved by using an indexation scheme involving labels that tag particular groups of elementary predications (EPs) and handles (here, h1 ; h2 ; :::) that reference those EPs. Underspecification is achieved by using unresolved handles as the arguments for scope-bearing elements and dec"
P02-1041,P02-1043,0,0.00864347,"phological information. This is particularly important since the system does not otherwise support typed feature structures with inheritance. Hybrid logics provide a perspicuous logical language for representing structures in temporal logic, description logic, AVMs, and indeed any relational structure. Terms of HLDS can thus be marshalled into terms of these other representations with the potential of taking advantage of tools developed for them or providing input to modules expecting them. In future work, we intend to combine techniques for building wide-coverage statistical parsers for CCG (Hockenmaier and Steedman, 2002; Clark et al., 2002) with corpora that have explicitly marked semantic dependency relations (such as the Prague Dependency Treebank and NEGRA) to produce HLDS terms as the parse output. Acknowledgements We would like to thank Patrick Blackburn, Johan Bos, Nissim Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and the ACL reviewers for helpful comments on earlier versions of this paper. All errors are, of course, our own. Jason Baldridge’s work is supported in part by Overseas Research Student Award ORS/98014014. Geert-Jan Kruijff’s work is supported by the DFG Sonderforschungsbereich 3"
P02-1041,J88-2003,0,0.143242,"particular conditions hold. Thus, assuming an accessibility relation X S, we can model the meaning of the pronoun he as in (12). (12) @i hXSi( j ^ male) During discourse interpretation, this statement is evaluated against the discourse model. The pronoun is resolvable only if a state where male holds is X Saccessible in the discourse model. Different accessibility relations can be modeled, e.g. to distinguish a local context (for resolving reflexive anaphors like himself ) from a global context (Kruijff, 2001). Finally, the rich temporal ontology underlying models of tense and aspect such as Moens and Steedman (1988) can be captured using the sorting strategy. Earlier work like Blackburn and Lascarides (1992) already explored such ideas. HLDS employs hybrid logic to integrate Moens and Steedman’s notion of the event nucleus directly into meaning representations. The event nucleus is a tripartite structure reflecting the underlying semantics of a type of event. The event is related to a preparation (an activity bringing the event about) and a consequent (a state ensuing to the event), which we encode as the modal relations P REP and C ONS, respectively. Different kinds of states and events are modeled as d"
P07-1113,P04-1014,0,0.0154558,"L WT F ORCE P RED G EN P RED H AS F IN H AS M ODAL F REQ A DV M ODAL A DV VOL A DV F IRST V B WTLG WTL V ERBS V ERBTAGS M AIN V B S UBJ S UPER (see above) all verbs in clause POS tags for all verbs main verb of clause subject of clause (lexical item) CCG supertag FACT P RED 4.2 Table 2: Feature sets for SE classification 3.3 Preprocessing The linguistic tests for SE classification appeal to multiple levels of linguistic information; there are lexical, morphological, syntactic, categorial, and structural tests. In order to access categorial and structural information, we used the C&C2 toolkit (Clark and Curran, 2004). It provides part-of-speech tags and Combinatory Categorial Grammar (CCG) (Steedman, 2000) categories for words and syntactic dependencies across words. 4 Features One of our goals in undertaking this study was to explore the use of linguistically-motivated features and deep syntactic features in probabilistic models for SE classification. The nature of the task requires features characterizing the entire clause. Here, we describe our four feature sets, summarized in table 2. The feature sets are additive, extending very basic feature sets first with linguistically-motivated features and then"
P07-1113,C92-4177,0,0.309751,"Missing"
P07-1113,J01-3003,0,0.0128459,"ristics. Linguistic indicators for aspectual classification are also used by Siegel (1999), who evaluates 14 indicators to test verbs for stativity and telicity. Many of his indicators overlap with our features. Siegel and McKeown (2001) address classification of verbs for stativity (event vs. state) and for completedness (culminated vs. non-culminated events). They compare three supervised and one unsupervised machine learning systems. The systems obtain relatively high accuracy figures, but they are domain-specific, require extensive human supervision, and do not address aspectual coercion. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and classify unergative, unaccusative, and object-drop verbs. Stevenson and Merlo note that statistical analysis cannot and should not be separated from deeper linguistic analysis, and our results support that claim. The advantages of our approach are the broadened conception of the classification task and the use of sequence prediction to capture a wider context. 8 References N. Asher. 1993. Reference to Abstract objects in Discourse. Kluwer Academic Publishers. A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy app"
P07-1113,J88-2003,0,0.539159,"lson and Pelletier, 1995). Consider the text passage below, which introduces an event-type entity in (1), a report-type entity in (2), and a statetype entity in (3). (1) Sony Corp. has heavily promoted the Video Walkman since the product’s introduction last summer , (2) but Bob Gerson , video editor of This Week in Consumer Electronics , says (3) Sony conceives of 8mm as a “family of products , camcorders and VCR decks , ” SE classification is a fundamental component in determining the discourse mode of texts (Smith, 2003) and, along with aspectual classification, for temporal interpretation (Moens and Steedman, 1988). It may be useful for discourse relation projection and discourse parsing. Though situation entities are well-studied in linguistics, they have received very little computational treatment. This paper presents the first data-driven models for SE classification. Our two main strategies are (a) the use of linguistically-motivated features and (b) the implementation of SE classification as a sequencing task. Our results also provide empirical support for the very notion of discourse modes, as we see clear genre effects in SE classification. We begin by discussing SEs in more detail. Section 3 de"
P07-1113,P99-1015,0,0.541871,"he effect of mixed domain training. 7 Related work Though we are aware of no previous work in SE classification, others have focused on automatic detection of aspectual and temporal data. Klavans and Chodorow (1992) laid the foundation for probabilistic verb classification with their interpretation of aspectual properties as gradient and their use of statistics to model the gradience. They implement a single linguistic test for stativity, treating lexical properties of verbs as tendencies rather than absolute characteristics. Linguistic indicators for aspectual classification are also used by Siegel (1999), who evaluates 14 indicators to test verbs for stativity and telicity. Many of his indicators overlap with our features. Siegel and McKeown (2001) address classification of verbs for stativity (event vs. state) and for completedness (culminated vs. non-culminated events). They compare three supervised and one unsupervised machine learning systems. The systems obtain relatively high accuracy figures, but they are domain-specific, require extensive human supervision, and do not address aspectual coercion. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and clas"
P07-1113,J96-1002,0,\N,Missing
P07-1113,J00-4004,0,\N,Missing
P08-1038,E03-1036,1,0.893159,"x/(w/z): λh.f (λx.ghx) We show that CCG augmented with this rule improves CCG’s empirical coverage by allowing better analyses of modal verbs in English and causatives in Spanish, and certain coordinate constructions. The D rules are well-behaved; we show this by deriving them both from unary composition and from the logic defined by Baldridge (2002). Both perspectives on D ensure that the new rules are compatible with normal form constraints (Eisner, 1996) for controlling spurious ambiguity. The logic also ensures that the new rules are subject to modalities consistent with those defined by Baldridge and Kruijff (2003). Furthermore, we define a logic that produces Eisner’s constraints as grammar internal theorems rather than parsing stipulations. 2 Combinatory Categorial Grammar CCG uses a universal set of syntactic rules based on the B, T, and S combinators of combinatory logic (Curry and Feys, 1958): (2) B: ((Bf )g)x = f (gx) T: Txf = f x S: ((Sf )g)x = f x(gx) CCG functors are functions over strings of symbols, so different linearized versions of each of the combinators have to be specified (ignoring S here): 326 Proceedings of ACL-08: HLT, pages 326–334, c Columbus, Ohio, USA, June 2008. 2008 Associatio"
P08-1038,C04-1009,0,0.0172253,"(1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities. 1 (1) Introduction Combinatory Categorial Grammar (CCG, Steedman (2000)) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), sentence realization (White, 2006), learning semantic parsers (Zettlemoyer and Collins, 2007), dialog systems (Kruijff et al., 2007), grammar engineering (Beavers, 2004; Baldridge et al., 2007), and modeling syntactic priming (Reitter et al., 2006). A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx) We show that CCG augmented with this rule improves CCG’s empirical coverage by allowing better analyses of modal verbs in English and causatives in Spanish, and certain coordinate"
P08-1038,P98-1025,0,0.0293627,"/y y$n ⇒ x$n 4.1 (34) &lt;B n : y$n xy ⇒ x$n The Spurious Ambiguity Problem CCG’s flexibility is useful for linguistic analyses, but leads to spurious ambiguity (Wittenburg, 1987) due to the associativity introduced by the B and T rules. This can incur a high computational cost which parsers must deal with. Several techniques have been proposed for the problem (Wittenburg, 1987; Karttunen, 1989; Hepple and Morrill, 1989; Eisner, 1996). The most commonly used are Karttunnen’s chart subsumption check (White and Baldridge, 2003; Hockenmaier and Steedman, 2002) and Eisner’s normal-form constraints (Bozsahin, 1998; Clark and Curran, 2007). Eisner’s normal form, referred to here as Eisner NF and paraphrased in (30), has the advantage of not requiring comparisons of logical forms: it functions purely on the syntactic types being combined. (30) For a set S of semantically equivalent2 parse trees for a string ABC, admit the unique parse tree such that at least one of (i) or (ii) holds: i. C is not the argument of (AB) resulting from application of &gt;B 1 + . ii. A is not the argument of (BC) resulting from application of &lt;B 1 + . The implication is that outputs of B1+ rules are inert, using the terminology o"
P08-1038,J07-4004,0,0.358142,"ow two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities. 1 (1) Introduction Combinatory Categorial Grammar (CCG, Steedman (2000)) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), sentence realization (White, 2006), learning semantic parsers (Zettlemoyer and Collins, 2007), dialog systems (Kruijff et al., 2007), grammar engineering (Beavers, 2004; Baldridge et al., 2007), and modeling syntactic priming (Reitter et al., 2006). A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx) We show t"
P08-1038,P96-1011,0,0.387896,"g-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx) We show that CCG augmented with this rule improves CCG’s empirical coverage by allowing better analyses of modal verbs in English and causatives in Spanish, and certain coordinate constructions. The D rules are well-behaved; we show this by deriving them both from unary composition and from the logic defined by Baldridge (2002). Both perspectives on D ensure that the new rules are compatible with normal form constraints (Eisner, 1996) for controlling spurious ambiguity. The logic also ensures that the new rules are subject to modalities consistent with those defined by Baldridge and Kruijff (2003). Furthermore, we define a logic that produces Eisner’s constraints as grammar internal theorems rather than parsing stipulations. 2 Combinatory Categorial Grammar CCG uses a universal set of syntactic rules based on the B, T, and S combinators of combinatory logic (Curry and Feys, 1958): (2) B: ((Bf )g)x = f (gx) T: Txf = f x S: ((Sf )g)x = f x(gx) CCG functors are functions over strings of symbols, so different linearized versio"
P08-1038,E89-1002,0,0.781943,"additionally allows Eisner’s normal form constraints to be derived as grammar internal theorems. Eisner uses a generalized form Bn (n≥0) of composition that subsumes function application:4 (33) &gt;B n : x/y y$n ⇒ x$n 4.1 (34) &lt;B n : y$n xy ⇒ x$n The Spurious Ambiguity Problem CCG’s flexibility is useful for linguistic analyses, but leads to spurious ambiguity (Wittenburg, 1987) due to the associativity introduced by the B and T rules. This can incur a high computational cost which parsers must deal with. Several techniques have been proposed for the problem (Wittenburg, 1987; Karttunen, 1989; Hepple and Morrill, 1989; Eisner, 1996). The most commonly used are Karttunnen’s chart subsumption check (White and Baldridge, 2003; Hockenmaier and Steedman, 2002) and Eisner’s normal-form constraints (Bozsahin, 1998; Clark and Curran, 2007). Eisner’s normal form, referred to here as Eisner NF and paraphrased in (30), has the advantage of not requiring comparisons of logical forms: it functions purely on the syntactic types being combined. (30) For a set S of semantically equivalent2 parse trees for a string ABC, admit the unique parse tree such that at least one of (i) or (ii) holds: i. C is not the argument of (AB"
P08-1038,P02-1043,0,0.219078,"D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities. 1 (1) Introduction Combinatory Categorial Grammar (CCG, Steedman (2000)) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), sentence realization (White, 2006), learning semantic parsers (Zettlemoyer and Collins, 2007), dialog systems (Kruijff et al., 2007), grammar engineering (Beavers, 2004; Baldridge et al., 2007), and modeling syntactic priming (Reitter et al., 2006). A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z)"
P08-1038,W06-1637,0,0.0123065,"D rules do not lead to spurious ambiguities. 1 (1) Introduction Combinatory Categorial Grammar (CCG, Steedman (2000)) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), sentence realization (White, 2006), learning semantic parsers (Zettlemoyer and Collins, 2007), dialog systems (Kruijff et al., 2007), grammar engineering (Beavers, 2004; Baldridge et al., 2007), and modeling syntactic priming (Reitter et al., 2006). A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx) We show that CCG augmented with this rule improves CCG’s empirical coverage by allowing better analyses of modal verbs in English and causatives in Spanish, and certain coordinate constructions. The D rules are well-behaved; we show this by deriving them both"
P08-1038,W03-2316,1,0.848209,"ses a generalized form Bn (n≥0) of composition that subsumes function application:4 (33) &gt;B n : x/y y$n ⇒ x$n 4.1 (34) &lt;B n : y$n xy ⇒ x$n The Spurious Ambiguity Problem CCG’s flexibility is useful for linguistic analyses, but leads to spurious ambiguity (Wittenburg, 1987) due to the associativity introduced by the B and T rules. This can incur a high computational cost which parsers must deal with. Several techniques have been proposed for the problem (Wittenburg, 1987; Karttunen, 1989; Hepple and Morrill, 1989; Eisner, 1996). The most commonly used are Karttunnen’s chart subsumption check (White and Baldridge, 2003; Hockenmaier and Steedman, 2002) and Eisner’s normal-form constraints (Bozsahin, 1998; Clark and Curran, 2007). Eisner’s normal form, referred to here as Eisner NF and paraphrased in (30), has the advantage of not requiring comparisons of logical forms: it functions purely on the syntactic types being combined. (30) For a set S of semantically equivalent2 parse trees for a string ABC, admit the unique parse tree such that at least one of (i) or (ii) holds: i. C is not the argument of (AB) resulting from application of &gt;B 1 + . ii. A is not the argument of (BC) resulting from application of &lt;B"
P08-1038,P87-1011,0,0.641108,"les can have implications for parsing efficiency. In this section, we show that the D rules fit naturally within standard normal form constraints for CCG parsing (Eisner, 1996), by providing both combinatory and logical bases for D. This additionally allows Eisner’s normal form constraints to be derived as grammar internal theorems. Eisner uses a generalized form Bn (n≥0) of composition that subsumes function application:4 (33) &gt;B n : x/y y$n ⇒ x$n 4.1 (34) &lt;B n : y$n xy ⇒ x$n The Spurious Ambiguity Problem CCG’s flexibility is useful for linguistic analyses, but leads to spurious ambiguity (Wittenburg, 1987) due to the associativity introduced by the B and T rules. This can incur a high computational cost which parsers must deal with. Several techniques have been proposed for the problem (Wittenburg, 1987; Karttunen, 1989; Hepple and Morrill, 1989; Eisner, 1996). The most commonly used are Karttunnen’s chart subsumption check (White and Baldridge, 2003; Hockenmaier and Steedman, 2002) and Eisner’s normal-form constraints (Bozsahin, 1998; Clark and Curran, 2007). Eisner’s normal form, referred to here as Eisner NF and paraphrased in (30), has the advantage of not requiring comparisons of logical f"
P08-1038,D07-1071,0,0.0410789,"a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities. 1 (1) Introduction Combinatory Categorial Grammar (CCG, Steedman (2000)) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), sentence realization (White, 2006), learning semantic parsers (Zettlemoyer and Collins, 2007), dialog systems (Kruijff et al., 2007), grammar engineering (Beavers, 2004; Baldridge et al., 2007), and modeling syntactic priming (Reitter et al., 2006). A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000). Here, we argue x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx) We show that CCG augmented with this rule improves CCG’s empirical coverage by allowing better analyses"
P08-1038,C98-1025,0,\N,Missing
P10-1051,W02-0603,0,0.0336295,"smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/ta"
P10-1051,D09-1123,0,0.0345392,"Missing"
P10-1051,P08-1085,0,0.0980955,"ctical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July"
P10-1051,J01-2001,0,0.0134756,"tion 1) finds the smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains al"
P10-1051,P07-1094,0,0.338937,"ictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Lingu"
P10-1051,J07-3004,0,0.125826,"e integer programming formulation that identifies minimal grammars efficiently and effectively. 2 NPAPER+CIVIL NPAPER CIVIL Max 126 Type ambig 1.69 Tok ambig 18.71 849 644 486 64 48 39 1.48 1.42 1.52 11.76 12.17 11.33 Table 1: Statistics for the training data used to extract lexicons for CCGbank and CCG-TUT. Distinct: # of distinct lexical categories; Max: # of categories for the most ambiguous word; Type ambig: per word type category ambiguity; Tok ambig: per word token category ambiguity. Data CCGbank. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions fo"
P10-1051,J93-2004,0,0.0402447,"ng accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the"
P10-1051,C08-1008,1,0.801717,"lly converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions for a bitag HMM supertagger based on two main properties. First, categories differ in their complexity and less complex categories tend to be used more frequently. For example, two categories for buy in CCGbank are (S[dcl]NP)/NP and ((((S[b]NP)/PP)/PP)/(S[adj]NP))/NP; the former occurs 33 times, the latter once. Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((SNP)/S) expects an NP to its left and an S to its right. Categories combine via r"
P10-1051,J94-2001,0,0.4331,"basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. 1 Kevin Knight1 Introduction Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the sta"
P10-1051,C04-1080,0,0.134526,") taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Asso"
P10-1051,P09-1057,1,0.867693,"has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July 2010. 2010 Association"
P10-1051,N06-1019,0,0.249753,"Missing"
P10-1051,J07-4004,0,0.0952989,"Missing"
P11-1096,N09-2030,0,0.00861597,"unk in more concentrated regions that are themselves less spread out globally. 6 Related work Lieberman and Lin (2009) also work with geotagged Wikipedia articles, but they do in order so to ana962 lyze the likely locations of users who edit such articles. Other researchers have investigated the use of Wikipedia as a source of data for other supervised NLP tasks. Mihalcea and colleagues have investigated the use of Wikipedia in conjunction with word sense disambiguation (Mihalcea, 2007), keyword extraction and linking (Mihalcea and Csomai, 2007) and topic identification (Coursey et al., 2009; Coursey and Mihalcea, 2009). Cucerzan (2007) used Wikipedia to do named entity disambiguation, i.e. identification and coreferencing of named entities by linking them to the Wikipedia article describing the entity. Some approaches to document geolocation rely largely or entirely on non-textual metadata, which is often unavailable for many corpora of interest, Nonetheless, our methods could be combined with such methods when such metadata is available. For example, given that both Wikipedia and Twitter have a linked structure between documents, it would be possible to use the link-based method given in Backstrom et al. ("
P11-1096,W09-1126,0,0.00607482,"er seem to have been sunk in more concentrated regions that are themselves less spread out globally. 6 Related work Lieberman and Lin (2009) also work with geotagged Wikipedia articles, but they do in order so to ana962 lyze the likely locations of users who edit such articles. Other researchers have investigated the use of Wikipedia as a source of data for other supervised NLP tasks. Mihalcea and colleagues have investigated the use of Wikipedia in conjunction with word sense disambiguation (Mihalcea, 2007), keyword extraction and linking (Mihalcea and Csomai, 2007) and topic identification (Coursey et al., 2009; Coursey and Mihalcea, 2009). Cucerzan (2007) used Wikipedia to do named entity disambiguation, i.e. identification and coreferencing of named entities by linking them to the Wikipedia article describing the entity. Some approaches to document geolocation rely largely or entirely on non-textual metadata, which is often unavailable for many corpora of interest, Nonetheless, our methods could be combined with such methods when such metadata is available. For example, given that both Wikipedia and Twitter have a linked structure between documents, it would be possible to use the link-based metho"
P11-1096,D07-1074,0,0.0355891,"ons that are themselves less spread out globally. 6 Related work Lieberman and Lin (2009) also work with geotagged Wikipedia articles, but they do in order so to ana962 lyze the likely locations of users who edit such articles. Other researchers have investigated the use of Wikipedia as a source of data for other supervised NLP tasks. Mihalcea and colleagues have investigated the use of Wikipedia in conjunction with word sense disambiguation (Mihalcea, 2007), keyword extraction and linking (Mihalcea and Csomai, 2007) and topic identification (Coursey et al., 2009; Coursey and Mihalcea, 2009). Cucerzan (2007) used Wikipedia to do named entity disambiguation, i.e. identification and coreferencing of named entities by linking them to the Wikipedia article describing the entity. Some approaches to document geolocation rely largely or entirely on non-textual metadata, which is often unavailable for many corpora of interest, Nonetheless, our methods could be combined with such methods when such metadata is available. For example, given that both Wikipedia and Twitter have a linked structure between documents, it would be possible to use the link-based method given in Backstrom et al. (2010) for predict"
P11-1096,D10-1124,0,0.692653,"Missing"
P11-1096,N07-1025,0,0.00646677,"perties). This also leads to generally more accurate geolocation of HMS ships over USS ships; the former seem to have been sunk in more concentrated regions that are themselves less spread out globally. 6 Related work Lieberman and Lin (2009) also work with geotagged Wikipedia articles, but they do in order so to ana962 lyze the likely locations of users who edit such articles. Other researchers have investigated the use of Wikipedia as a source of data for other supervised NLP tasks. Mihalcea and colleagues have investigated the use of Wikipedia in conjunction with word sense disambiguation (Mihalcea, 2007), keyword extraction and linking (Mihalcea and Csomai, 2007) and topic identification (Coursey et al., 2009; Coursey and Mihalcea, 2009). Cucerzan (2007) used Wikipedia to do named entity disambiguation, i.e. identification and coreferencing of named entities by linking them to the Wikipedia article describing the entity. Some approaches to document geolocation rely largely or entirely on non-textual metadata, which is often unavailable for many corpora of interest, Nonetheless, our methods could be combined with such methods when such metadata is available. For example, given that both Wikipe"
P11-1096,W03-0108,0,0.0150629,"Missing"
P11-1108,N10-1083,0,0.0790379,"Missing"
P11-1108,W06-2912,0,0.411632,"tion has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s resu"
P11-1108,E99-1016,0,0.279855,"level predictions. This result suggests that improvements to low-level constituent prediction will ultimately lead to further gains in overall constituent parsing. Here, we present such an improvement by using probabilistic finite-state models for phrasal segmentation from raw text. The task for these models is chunking, so we evaluate performance on identification of multiword chunks of all constituent types as well as only noun phrases. Our unsupervised chunkers extend straightforwardly to a cascade that predicts higher levels of constituent structure, similar to the supervised approach of Brants (1999). This forms an overall unsupervised parsing system that outperforms CCL by a wide margin. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics WSJ relieved                was        one for      Ward       Mrs. (a) Chunks: (Mrs. Ward), (for one), and (was relieved) Negra 1 All CTB came from              Research               Cray Chunks NPs Chnk ∩ NPs Chunks NPs Chnk ∩ NPs Chunks NPs Chnk ∩ NPs Ta"
P11-1108,N09-1009,0,0.0417624,"2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency"
P11-1108,P07-3008,0,0.148321,"ation and comparison.1 More importantly, until recently it was the only unsupervised raw text constituent parser to produce results competitive with systems which use gold POS tags (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006) – and the recent improved raw-text parsing results of Reichart and Rappoport (2010) make direct use of CCL without modification. There are other raw-text parsing systems of note, EMILE (Adriaans et al., 2000), ABL (van Zaanen, 2000) and ADIOS (Solan et al., 2005); however, there is little consistent treebank-based evaluation of these models. One study by Cramer (2007) found that none of the three performs particularly well under treebank evaluation. Finally, CCL outperforms most published POS-based models when those models are trained on unsupervised word classes rather than gold POS tags. The only exception we are aware of is H¨anig’s (2010) unsuParse+, which outperforms CCL on Negra, though this is shown only for sentences with ten or fewer words. Phrasal punctuation. Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation – punctuation symbols tha"
P11-1108,P03-1066,0,0.0301646,"course of EM. These correctly predicted constituents are not counted as such in the constituent chunking or base NP evaluations, but they factor directly into improved accuracy when this model is part of a cascade. 7 Related work Our task is the unsupervised analogue of chunking (Abney, 1991), popularized by the 1999 and 2000 Conference on Natural Language Learning shared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compare context-free parsers with finite-state partial parsing methods. They find that full parsing maintains a number of benefits, in spite of the greater training time required: they can train on less data more effectively"
P11-1108,W10-2901,0,0.116113,"Missing"
P11-1108,N09-1012,0,0.153363,"Missing"
P11-1108,H05-1099,0,0.0207212,"hared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compare context-free parsers with finite-state partial parsing methods. They find that full parsing maintains a number of benefits, in spite of the greater training time required: they can train on less data more effectively than chunkers, and are more robust to shifts in textual domain. Brants (1999) reports a supervised cascaded chunking strategy for parsing which is strikingly similar to the methods proposed here. In both, Markov models are used in a cascade to predict hierarchical constituent structure; and in both, the parameters for the model at each level are estimated independently. T"
P11-1108,P02-1017,0,0.849996,"ing, or simply unsupervised chunking, we mean the segmentation of raw text into (non-overlapping) multiword constituents. The models are intended to cap- CCL benchmark. We use Seginer’s CCL as a ture local constituent structure – the lower branches benchmark for several reasons. First, there is a of a constituent tree. For this reason we evaluate free/open-source implementation facilitating exper1078 imental replication and comparison.1 More importantly, until recently it was the only unsupervised raw text constituent parser to produce results competitive with systems which use gold POS tags (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006) – and the recent improved raw-text parsing results of Reichart and Rappoport (2010) make direct use of CCL without modification. There are other raw-text parsing systems of note, EMILE (Adriaans et al., 2000), ABL (van Zaanen, 2000) and ADIOS (Solan et al., 2005); however, there is little consistent treebank-based evaluation of these models. One study by Cramer (2007) found that none of the three performs particularly well under treebank evaluation. Finally, CCL outperforms most published POS-based models when those models are trained on unsupervised word"
P11-1108,P04-1061,0,0.873424,"tures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL"
P11-1108,J93-2004,0,0.0461076,"lass of predictions for both models. Two of the top classes of errors, MD VB and TO VB, represent verb phrase constituents, which are often predicted by the HMM chunker, but not by the PRLG. The class represented by NNP NNP corresponds with the tendency of the HMM chunker to split long proper names: for example, it systematically splits new york stock exchange into two chunks, (new york) (stock exchange), whereas the PRLG chunker predicts a single four-word chunk. The most interesting class is DT JJ, which represents the difficulty the HMM chunker has at dis3 For the Penn Treebank tagset, see Marcus et al. (1993). 1 Start with raw text: there is no asbestos in our products now 2 Apply chunking model: there (is no asbestos) in (our products) now Text : Mr. Vinken is chairman of Elsevier N.V. Level 1 : Level 2 : 3 Create pseudowords: there is in our now 4 Apply chunking model (and repeat 1–4 etc.): (there is ) (in our ) now 5 Unwind and create a tree: Level 3 : Mr. Vinken Mr. Vinken is chairman of of 1 is chairman 1 Elsevier N.V. in is no asbestos our tinguishing determiner-adjective from determinernoun pairs. The PRLG chunker systematically gets DT JJ NN trigrams as chunks. The greater context provided"
P11-1108,D10-1020,1,0.83575,"ish, German, and Chinese. Like CCL, our models operate from raw (albeit segmented) text, and like it our models decode very quickly; however, unlike CCL, our models are based on standard and well-understood computational linguistics technologies (hidden Markov models and related formalisms), and may benefit from new research into these core technologies. For instance, our models may be improved by the application of (unsupervised) discriminative learning techniques with features (Berg-Kirkpatrick et al., 2010); or by incorporating topic models and document information (Griffiths et al., 2005; Moon et al., 2010). UPPARSE, the software used for the experiments in this paper, is available under an open-source license to facilitate replication and extensions.6 Acknowledgments. This material is based upon work supported in part by the U. S. Army Research Laboratory and the U. S. Army Research Office under grant number W911NF-10-1-0533. Support for the first author was also provided by Mike Hogg Endowment Fellowship, the Office of Graduate Studies at The University of Texas at Austin. This paper benefited from discussion in the Natural Language Learning reading group at UT Austin, especially from Collin B"
P11-1108,P92-1017,0,0.745108,"relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Segine"
P11-1108,W95-0107,0,0.0654013,"often mark phrasal boundaries within a sentence. These are used in two ways: i) they impose a hard constraint on constituent spans, in that no constituent (other than sentence root) may extend over a punctuation symbol, and ii) they contribute to the model, specifically in terms of the statistics of words seen adjacent to a phrasal boundary. We follow this convention and use the following set: . ? ! ; , -- ◦ � The last two are ideographic full-stop and comma.2 4 Unsupervised partial parsing We learn partial parsers as constrained sequence models over tags encoding local constituent structure (Ramshaw and Marcus, 1995). A simple tagset is unlabeled BIO, which is familiar from supervised chunking and named-entity recognition: the tag B 1 http://www.seggu.net/ccl This set is essentially that of Seginer (2007). While it is clear from our analysis of CCL that it does make use of phrasal punctuation in Chinese, we are not certain whether ideographic comma is included. 2 1079 denotes the beginning of a chunk, I denotes membership in a chunk and O denotes exclusion from any chunk. In addition we use the tag STOP for sentence boundaries and phrasal punctuation. HMMs and PRLGs. The models we use for unsupervised par"
P11-1108,D10-1067,0,0.153666,"hich are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s results, they do so by selecting training sets to best match the particular test sentences—CCL itself is used without modification. Ponvert et al. (2010) explore an alternative strategy of unsupervised partial parsing: directly predicting low-level constituents based solely on word co-occurrence frequencies. Essentially, this means segmenting raw text into multiword constituents. In that paper, we show—somewhat surprisingly—that CCL’s performance is mostly dependent on its effectiveness at identifying low-level constituents. In fact, simply extracting non-hierarchical mul"
P11-1108,J10-1001,0,0.0484557,"onstituent structure; and in both, the parameters for the model at each level are estimated independently. There are major differences, though: the models here are learned from raw text without tree annotations, using EM to train parameters; Brants’ cascaded Markov models use supervised maximum likelihood estimation. Secondly, between the separate levels of the cascade, we collapse constituents into symbols which are treated as tokens in subsequent chunking levels; the Markov models in the higher cascade levels in Brants’ work actually emit constituent structure. A related approach is that of Schuler et al. (2010), who report a supervised hierarchical hidden Markov model which uses a right-corner transform. This allows the model to predict more complicated trees with fewer levels than in Brants’ work or this paper. 1085 8 Conclusion In this paper we have introduced a new subproblem of unsupervised parsing: unsupervised partial parsing, or unsupervised chunking. We have proposed a model for unsupervised chunking from raw text that is based on standard probabilistic finitestate methods. This model produces better local constituent predictions than the current best unsupervised parser, CCL, across dataset"
P11-1108,P07-1049,0,0.581647,", 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. This makes it hard to extend. Note that although Reichart and Rappoport (2010) improve on Seginer’s results, they do so by selecting training sets to best match the particular test sentences—CCL itself is used without modification. Ponvert et"
P11-1108,P04-1062,0,0.0804928,"c Rec F 53.6 50.0 51.7 48.2 43.6 45.8 60.0 49.4 54.2 German / Negra Prec Rec F 33.4 32.6 33.0 30.8 50.3 38.2 38.8 47.4 42.7 Chinese / CTB Prec Rec F 37.0 21.6 27.3 43.0 29.8 35.2 50.4 32.8 39.8 Table 6: Unlabeled PARSEVAL scores for cascaded models. source implementation by F. Luque,4 we compare on WSJ and Negra to the constituent context model (CCM) of Klein and Manning (2002). CCM learns to predict a set of brackets over a string (in practice, a string of POS tags) by jointly estimating constituent and distituent strings and contexts using an iterative EM-like procedure (though, as noted by Smith and Eisner (2004), CCM is deficient as a generative model). Note that this comparison is methodologically problematic in two respects. On the one hand, CCM is evaluated using gold standard POS sequences as input, so it receives a major source of supervision not available to the other models. On the other hand, the other models use punctuation as an indicator of constituent boundaries, but all punctuation is dropped from the input to CCM. Also, note that CCM performs better when trained on short sentences, so here CCM is trained only on the 10-wordor-less subsets of the training datasets.5 The results from the"
P11-1108,N10-1116,0,0.0312993,"are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. 1 Introduction Unsupervised grammar induction has been an active area of research in computational linguistics for over twenty years (Lari and Young, 1990; Pereira and Schabes, 1992; Charniak, 1993). Recent work (Headden III et al., 2009; Cohen and Smith, 2009; H¨anig, 2010; Spitkovsky et al., 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. This is also true for models which are not successors of Klein and Manning (Bod, 2006; H¨anig, 2010). An exception which learns from raw text and makes no use of POS tags is the common cover links parser (CCL, Seginer 2007). CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is al"
P11-1108,W00-0726,0,0.173471,"whether the longer chunks predicted correspond to actual constituents or not. Fig. 6c shows that the PRLG, when constrained by phrasal punctuation, does continue to improve its constituent prediction accuracy over the course of EM. These correctly predicted constituents are not counted as such in the constituent chunking or base NP evaluations, but they factor directly into improved accuracy when this model is part of a cascade. 7 Related work Our task is the unsupervised analogue of chunking (Abney, 1991), popularized by the 1999 and 2000 Conference on Natural Language Learning shared tasks (Tjong et al., 2000). In fact, our models follow Ramshaw and Marcus (1995), treating structure prediction as sequence prediction using BIO tagging. In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. (2004) also operates on raw text. Like us, their model gives special treatment to local constituents, using a language model to characterize phrases which are linked via a dependency model. Their output is not evaluated directly using treebanks, but rather applied to several information retrieval problems. In the supervised realm, Hollingshead et al. (2005) compa"
P11-1108,C00-2139,0,0.0602638,"Missing"
P13-1057,P10-1010,0,0.0186539,"s, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages. Haghighi and Klein (2006) develop a model in which a POS-tagger is learned from a list of POS tags and just three “prototype” word types for each tag, but their approach requires a vector space to compute the distributional similarity between prototypes and other word types in the corpus. Such distributional models are not feasible for low-resource languages because they require immense amounts of raw text, much more than is available in these settings (Abney and Bird, 2010). Further, they extracted their prototype lists directly from a labeled corpus, something we are specifically avoiding. T¨ackstr¨om et al. (2013) evaluate the use of mixed type and token constraints generated by projecting information from a highresource language to a low-resource language via a parallel corpus. However, large parallel corpora are not available for most low-resource languages. These are also expensive resources to create and would take considerably more effort to produce than the monolingual resources that our annotators were able to generate in a four-hour timeframe. Of cours"
P13-1057,D09-1031,1,0.474932,"Missing"
P13-1057,W02-2006,0,0.032254,"Missing"
P13-1057,P11-1061,0,0.0843649,"Missing"
P13-1057,E03-1068,0,0.00957233,"as active learning may better select sentences for token annotation, so this should be explored in future work. 81.5. This indicates that there are gains to be made by correcting mistakes in the annotations. This is true even after the point of diminishing returns on the learning curve, meaning that even when adding more annotations no longer improves performance, progress can still be made by correcting errors, so it may be reasonable to ask annotators to attempt to correct errors in their past annotations. Automated techniques for facilitating error identification can be employed for this (Dickinson and Meurers, 2003). 6 Acknowledgements We thank Kyle Jerro, Vijay John, Jim Evans, Yoav Goldberg, Slav Petrov, and the reviewers for their assistance and feedback. This work was supported by the U.S. Department of Defense through the U.S. Army Research Office (grant number W911NF-10-1-0533) and through a National Defense Science and Engineering Graduate Fellowship for the first author. Experiments were run on the UTCS Mastodon Cluster, provided by NSF grant EIA-0303609. Conclusions and Future Work Care must be taken when drawing conclusions from small-scale annotation studies such as those presented in this pap"
P13-1057,D12-1075,1,0.93781,"significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed lexicon for Hebrew in order to l"
P13-1057,N13-1014,1,0.61017,"Missing"
P13-1057,P08-1085,0,0.0291388,"Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed lexicon for Hebrew in order to learn an HMM tagger. However, this lexicon was constructed by trained lexicographers over a long period of time and achieves very high coverage of the language with very good quality, much better than could be achieved by our non-expert linguistics graduate student annotators in just a few hours. Cucerzan and Yarowsky Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are o"
P13-1057,N06-1041,0,0.0134534,"odels such as hidden Markov models (HMM) using the Expectation-Maximization algorithm (EM), however most work in this area has still relied on relatively large amounts of data, both annotated and 583 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583–592, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics collection: (2002) learn a POS-tagger from existing linguistic resources, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages. Haghighi and Klein (2006) develop a model in which a POS-tagger is learned from a list of POS tags and just three “prototype” word types for each tag, but their approach requires a vector space to compute the distributional similarity between prototypes and other word types in the corpus. Such distributional models are not feasible for low-resource languages because they require immense amounts of raw text, much more than is available in these settings (Abney and Bird, 2010). Further, they extracted their prototype lists directly from a labeled corpus, something we are specifically avoiding. T¨ackstr¨om et al. (2013)"
P13-1057,E09-1042,0,0.0157623,"process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed l"
P13-1057,D12-1127,0,0.0186412,"Missing"
P13-1057,J93-2004,0,0.0434251,"522 (1124) 1036 (2375) 1314 (3222) 1697 (4376) ENG type 210 631 1350 2185 - Novice token 308 (599) 646 (1429) 953 (2178) 1220 (2933) Table 1: Annotations for each language and annotator as time increases. Shows the number of tag dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses). For brevity, the table only shows hourly progress. enized and labeled with POS tags by two linguistics graduate students, each of which was studying one of the languages. The KIN and MLG data have 12 and 23 distinct POS tags, respectively. The Penn Treebank (PTB) (Marcus et al., 1993) is used as ENG data. Section 01 was used for token-supervised annotation, sections 02-14 were used as raw data, 15-18 for development of the FST, 19-21 as a dev set and 22-24 as a test set. The PTB uses 45 distinct POS tags. ENG time 1:00 2:00 3:00 4:00 type 0.05 0.15 0.24 0.32 - Exp. tok 0.03 0.05 0.06 0.08 ENG type 0.01 0.03 0.07 0.11 - Nov. tok 0.02 0.03 0.05 0.06 Table 2: Tag dictionary recall against the test set for ENG annotators on type and token annotations. Annotations Table 1 gives statistics for all languages and annotators showing progress during the 4-hour tasks. With token-anno"
P13-1057,J94-2001,0,0.455146,"Missing"
P13-1057,P00-1016,0,0.109289,"Missing"
P13-1057,P09-1057,0,0.0243214,"consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a ma"
P13-1057,C10-1106,0,0.0229287,"Missing"
P13-1057,J95-2004,0,0.0674576,"Missing"
P13-1057,D10-1017,0,0.112442,"Missing"
P13-1057,Q13-1001,0,0.0568903,"Missing"
P13-1144,D10-1124,0,0.192402,"Missing"
P13-1144,D11-1072,0,0.102525,"Missing"
P13-1144,W03-0106,0,0.0232371,"ng distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (201"
P13-1144,W03-0108,0,0.0106051,"s for toponyms relevant to our evaluation corpora. These instances are used to train logistic regression classifiers P (l|t, w) for location l and toponym t. To disambiguate a new toponym, WISTR chooses the location that maximizes this probability. Few such probabilistic toponym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only in a local context, and they require toponymlabeled training data. Our approach makes use of all words in local and document context and requires no explicitly labeled toponym tokens. TRAWL We bring TRIPDL, WISTR, and standard toponym resolution cues about administrative levels to"
P13-1144,P11-1096,1,0.949936,"ance can result in poor coverage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our resul"
P13-1144,D12-1137,1,0.804621,"verage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text cla"
P13-1144,W03-0107,0,\N,Missing
P15-1134,afonso-etal-2002-floresta,0,0.0689143,"ng set. For CHI we use the Chinese Treebank (CTB5) (Xue et al., 2005), also converted to dependencies. The testing set consisted of files 1-40/900-931, and the sentences presented for GFL annotation were randomly sampled from files 81-899. The POR data is from 2 The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2015 Figure 1: GFL example for Mr. Conlon was executive vice president and director of the equity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from ann"
P15-1134,D10-1117,0,0.0193365,"efforts in which tooling, data, and expertise in the language are scarce. The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, 2 Jason Baldridge1 Department of Mechanical Engineering The University of Texas at Austin sally722@utexas.edu many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another st"
P15-1134,P99-1059,0,0.0500448,"V Model CFG-DMV model The GPC is based on the DMV model, a generative model for the unsupervised learning of dependency structures (Klein and Manning, 2004). We denote the input corpus as ω = (ω 1 , · · · , ω N ), where each ω s is a sentence consisting of words and in a sentence ω, word ωi has an corresponding part-of-speech tag τi . We denote the set of all words as Vω and the set of all parts-of-speech as Vτ . We use the partof-speech sequence as our terminal strings, resulting in an unlexicalized grammar. Dependencies can be formulated as split head bilexical context free grammars (CFGs) (Eisner and Satta, 1999) and these bilexical CFGs require that each terminal τi in sentence ω is represented in a split form by two terminals, with labels marking the left and right heads (τi,L , τi,R ). Henceforth, we denote w = w0,n as our terminals in the split-form of sentence ω (e.g., the terminals for the dog walks are DTL DTR N NL N NR VL VR ). Table 1 shows the grammar rules for the DMV model, from Klein and Manning (2004). 1387 Require: Arcs is the set of all directed arcs extracted from annotation for sentence w function RULE P ROB - SENT(w, θ, Arcs) θw = θ for each directed arc wi < wj do if i < j then for"
P15-1134,N13-1014,1,0.834522,"The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2015 Figure 1: GFL example for Mr. Conlon was executive vice president and director of the equity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from annotators who have minimal training. Kong et al. (2014) demonstrate the feasibility of training a dependency parser based on a GFL-annotated corpus of English tweets. An example of GFL is shown in Figure 1: (a) is the GFL markup itself and (b) is a graphical representation"
P15-1134,P99-1010,0,0.889899,"tional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler (Johnson et al., 2007; Sun et al., 2014). The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this appro"
P15-1134,N07-1018,0,0.150021,"t obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler (Johnson et al., 2007; Sun et al., 2014). The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this approach 1 In fact, standardized writing systems have yet to be adopted for some languages. 1385 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1385–1394, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics"
P15-1134,P04-1061,0,0.0235945,"ating they are grouped together but otherwise underspecified. 1386 CFG Rule S → YH YH → LH RH LH → HL LH → L1H L0H → HL L0H → L1H L1H → YA L0H EVG distribution P (root = H) P (ST OP |dir = L, head = H, val = 0) P (CON T |dir = L, head = H, val = 0) P (ST OP |dir = L, head = H, val = 1) P (CON T |dir = L, head = H, val = 1) P (ArgA|dir = L, head = H, val = 1) Description The head of the sentence is H Split-head representation H has no left children H has at least one left child H has no more left children H has other left children A is a left child of H Table 1: The CFG-DMV grammar schema from Klein and Manning (2004). Note that in these rules H and A are parts-of-speech. For brevity, we omit the portion of the grammar that handles the righthand arguments since they are symmetric to the left. Valency (val) can take the value 1 (we have made attachments in the direction (dir) d) or 0 (not). Sentences Annotated Tokens Annotated Fully Specified Sentences CHI 24 820 4 ENG 34 798 15 KIN 69 988 31 POR 63 1067 20 Table 2: Two Hour GFL Annotation Statistics Kong et al. (2014) stipulate that the GFL annotations in their corpus must be fully-specified. They are thus unable to take advantage of such underspecified se"
P15-1134,D14-1108,0,0.17094,"s for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from annotators who have minimal training. Kong et al. (2014) demonstrate the feasibility of training a dependency parser based on a GFL-annotated corpus of English tweets. An example of GFL is shown in Figure 1: (a) is the GFL markup itself and (b) is a graphical representation of the dependencies it encodes. Figure 1 specifies several dependencies: of is a dependent of director, executive vice president and director are conjuncts and and is the coordinator. However, the complete internal structure of the phrase the equity division remains unspecified, other than division being marked as the head (via an asterisk).3 Finally, Mr. Conlon in square bracke"
P15-1134,J93-2004,0,0.0515216,"Furthermore, it relies on no outside tools or corpora other than a part-of-speech tagger; a resource that can be built with two hours of annotation time (Garrette and Baldridge, 2013). 2 Data Data sources We use four languages from three language families in an effort to both verify the cross-linguistic applicability of our approach, accounting for variations in linguistic properties, as well as to attempt to realistically simulate a realworld, low-resource environment. Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). For ENG we use the Penn Treebank (Marcus et al., 1993), converted into dependencies by the standard process. Section 23 was used as a test set, and a random sample of sentences from sections 02-21 were selected for annotation with GFL as described below and subsequently used as the minimal training set. For CHI we use the Chinese Treebank (CTB5) (Xue et al., 2005), also converted to dependencies. The testing set consisted of files 1-40/900-931, and the sentences presented for GFL annotation were randomly sampled from files 81-899. The POR data is from 2 The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2"
P15-1134,P13-1028,0,0.024813,"Missing"
P15-1134,D10-1004,0,0.0680068,"Missing"
P15-1134,D10-1120,0,0.0161036,"dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, 2 Jason Baldridge1 Department of Mechanical Engineering The University of Texas at Austin sally722@utexas.edu many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotat"
P15-1134,P92-1017,0,0.733689,"osition and rule to expand parent node. Gibbs sampler The split-head representation encodes dependencies as a CFG. This enables the use of a Gibbs sampler algorithm for estimating PCFGs (Johnson et al., 2007; Sun et al., 2014), and it is straightforward to incorporate constraints from partial annotations into this sampler. To do this, we modified the tree-sampling step to incorporate constraints derived from GFL annotations and thereby impute the missing dependencies. Given a string w = (w1 , · · · wn ), we define a span of w as wi,k = (wi+1 , · · · , wk ), so that w = w0,n . As introduced in Pereira and Schabes (1992), a bracketing B of w is a finite set of spans on w satisfying the requirement that no two spans in a bracketing may overlap unless one span contains the other. For each sentence w = w0,n we define the auxiliary function for each span wi,j , 0 ≤ i < j ≤ n: ( 1 c(i, j) = 0 if span wi,j is valid for B; otherwise. Here one span is valid for B if it doesn’t cross any brackets. Section 3.2 describes how to derive bracketing information from GFL annotations and how to determine if a span wi,j is valid or not. Note that for parsing a corpus without any annotations and constraints, c(i, j) = 1 for any"
P15-1134,W13-2307,1,0.91088,"of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The f"
P15-1134,N10-1116,0,0.085176,"stin sally722@utexas.edu many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most u"
P15-1134,P10-1130,0,0.0969099,"stin sally722@utexas.edu many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most u"
P15-1134,W11-0303,0,0.0289117,"Missing"
P15-1134,D12-1063,0,0.0127215,"r language documentation efforts in which tooling, data, and expertise in the language are scarce. The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, 2 Jason Baldridge1 Department of Mechanical Engineering The University of Texas at Austin sally722@utexas.edu many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estima"
P15-1134,D14-1035,1,0.866702,"Missing"
P15-1134,N03-1033,0,0.0957842,"Missing"
P15-1134,P14-5021,0,\N,Missing
P19-1181,W18-1406,1,0.803026,"only 34.6 CLS on paths in R4R’s Validation Unseen houses. Keeping in mind that humans have an average navigation error of 1.61 in R2R (Anderson et al., 2018b), the average navigation error of 8.08 meters for R4R by our best agent leaves plenty of headroom. Future agents will need to make effective use of language and its connection to the environment to both drive CLS up and bring NE down in R4R. We expect path fidelity to not only be interesting with respect to grounding language, but to be crucial for many VLN-based problems. For example, future extensions of VLN will likely involve games (Baldridge et al., 2018) where the instructions being given take the agent around a trap or help it avoid opponents. Similar constraints could hold in search-and-rescue human-robot teams (Kruijff et al., 2014; Kruijff-Korbayov et al., 2016), where the direct path could take a rolling robot into an area with greater danger of collapse. In such scenarios, going straight to the goal could be literally deadly to the robot or agent. Acknowledgments We would like to thank our anonymous reviewers and the Google Research team, especially Radu Soricut, for the insightful comments that contributed to this paper. 1870 Reference"
P19-1181,W19-1605,1,0.875003,"Missing"
P19-1181,D18-1287,0,0.141294,"Missing"
P19-1181,D14-1162,0,0.0827649,"tion 3.3) as well as success rate. 4.1 Navigator The reasoning navigator of Wang et al. (2019) learns a policy πθ over parameters θ that map the natural language instruction X and the initial visual scene v1 to a sequence of actions a1..T . At time step t, the agent state is modeled using a LSTM (Hochreiter and Schmidhuber, 1997) that encodes the trajectory of past visual scenes and agent actions, ht =LST M ([vt ; at−1 ], ht−1 ), where vt is the output of visual encoder as described below. Language Encoder Language instructions X = x1..n are initialized with pre-trained GloVe word embeddings (Pennington et al., 2014) that are finetuned during training. We restrict the GloVe vocabulary to tokens that occur at least five times in the instruction data set. All out of vocabulary tokens are mapped to a single OOV identifier. Using a bidirectional recurrent network (Schuster and Paliwal, 1997) we encode the instruction into language contextual representations w1..n . Visual Features As in Fried et al. (2018), at each time step t, the agent perceives a 360-degree panoramic view of its surroundings from the current location. The view is discretized into m view angles (m = 36 in our implementation, 3 elevations x"
P19-1181,N19-1197,0,0.129373,"d metrics are mostly based on goal completion rather than fidelity to the described path. To address this, we define a new metric, Coverage weighted by Length Score (CLS), and compose path pairs of R2R to create Room-forRoom (R4R), an algorithmically produced extension of R2R. Figure 1 illustrates path composition and the scores of two agent paths for both CLS and Success weighted by Path Length (SPL), a metric recently proposed by Anderson et al. (2018a). In the example, an agent which ignores the language but gets to the goal receives a perfect SPL score. Language is not irrelevant for R2R. Thomason et al. (2019) ablate visual and language inputs and find that withholding either from an action sampling agent reduces performance on unseen houses. Also, the generated instructions in the augmented paths of Fried et al. (2018) improved performance for several models. However, while many of these augmented instructions have clear starting or ending descriptions, the middle portions are often disconnected from the path they are paired with (see Huang et al. (2019) for in depth analysis of augmented path instructions). That these low-fidelity augmented instructions improve results indicates that current metr"
Q18-1042,P12-1041,0,0.0583717,"Missing"
Q18-1042,Q14-1037,0,0.0285349,"swani et al., 2017) encode coreference relationships, adding to the results by Voita et al. (2018) on machine translation, https://cloud.google.com/natural-language/. All examples extracted from the same URL are partitioned into the same set. 9 http://goo.gl/language/gap-coreference. 10 https://stanfordnlp.github.io/CoreNLP/ download.html. 11 https://github.com/swiseman/nn_coref. 12 https://github.com/kentonl/e2e-coref. 13 We run Lee et al. (2017) in the final (single-model) configuration, with NLTK preprocessing (Bird and Loper, 2004); for Wiseman et al. (2016) we use Berkeley preprocessing (Durrett and Klein, 2014); and the Stanford systems are run within Stanford CoreNLP (Manning et al., 2014). 8 6 For example, missing sentence breaks, list environments, and non-referential personal roles/nationalities. 609 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 55.4 58.5 68.4 67.2 F 45.5 51.3 59.9 62.2 B 0.82 0.88 0.88 0.92 O 50.5 55.0 64.2 64.7 80 70 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 47.7 64.3 61.9 68.9 F 53.2 63.9 58.0 51.9 B 1.12 0.99 0.94 0.75 Recall Table 4: Performance of off-the-shelf resolvers on the GAP development set, split by Masculine an"
Q18-1042,N04-1038,0,0.229149,"Missing"
Q18-1042,P06-1005,0,0.192723,"Missing"
Q18-1042,P04-3031,0,0.09072,"ne pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017) encode coreference relationships, adding to the results by Voita et al. (2018) on machine translation, https://cloud.google.com/natural-language/. All examples extracted from the same URL are partitioned into the same set. 9 http://goo.gl/language/gap-coreference. 10 https://stanfordnlp.github.io/CoreNLP/ download.html. 11 https://github.com/swiseman/nn_coref. 12 https://github.com/kentonl/e2e-coref. 13 We run Lee et al. (2017) in the final (single-model) configuration, with NLTK preprocessing (Bird and Loper, 2004); for Wiseman et al. (2016) we use Berkeley preprocessing (Durrett and Klein, 2014); and the Stanford systems are run within Stanford CoreNLP (Manning et al., 2014). 8 6 For example, missing sentence breaks, list environments, and non-referential personal roles/nationalities. 609 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 55.4 58.5 68.4 67.2 F 45.5 51.3 59.9 62.2 B 0.82 0.88 0.88 0.92 O 50.5 55.0 64.2 64.7 80 70 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 47.7 64.3 61.9 68.9 F 53.2 63.9 58.0 51.9 B 1.12 0.99 0.94 0.75 Recall Table 4: Perfo"
Q18-1042,K16-1023,0,0.0119538,"throughout the paper highlight the ambiguous pronoun in bold, the two potential coreferent names in italics, and the correct one also underlined. 2 http://goo.gl/language/gap-coreference. 605 Transactions of the Association for Computational Linguistics, vol. 6, pp. 605–618, 2018. Action Editor: Luke Zettlemoyer. Submission batch: 7/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Such approaches promise to improve equity of downstream models, such as triple extraction for knowledge-base populations. 2 in Wikipedia, the focus of Ghaddar and Langlais (2016a), who use WikiCoref, a corpus of 30 full articles annotated with coreferences (Ghaddar and Langlais, 2016b). GAP examples are not strictly Winograd schemas because they have no reference-flipping word. Nonetheless, they contain two person named entities of the same gender and an ambiguous pronoun that may refer to either (or neither). As such, they represent a similarly difficult challenge and require the same inferential capabilities. More importantly, GAP is larger than existing Winograd schema datasets, and the examples are from naturally occurring Wikipedia text. GAP complements OntoNote"
Q18-1042,L16-1021,0,0.115476,"throughout the paper highlight the ambiguous pronoun in bold, the two potential coreferent names in italics, and the correct one also underlined. 2 http://goo.gl/language/gap-coreference. 605 Transactions of the Association for Computational Linguistics, vol. 6, pp. 605–618, 2018. Action Editor: Luke Zettlemoyer. Submission batch: 7/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Such approaches promise to improve equity of downstream models, such as triple extraction for knowledge-base populations. 2 in Wikipedia, the focus of Ghaddar and Langlais (2016a), who use WikiCoref, a corpus of 30 full articles annotated with coreferences (Ghaddar and Langlais, 2016b). GAP examples are not strictly Winograd schemas because they have no reference-flipping word. Nonetheless, they contain two person named entities of the same gender and an ambiguous pronoun that may refer to either (or neither). As such, they represent a similarly difficult challenge and require the same inferential capabilities. More importantly, GAP is larger than existing Winograd schema datasets, and the examples are from naturally occurring Wikipedia text. GAP complements OntoNote"
Q18-1042,C96-1079,0,0.600385,"of resolving naturally occurring ambiguous pronouns and rewards systems that are gender-fair. • We run four state-of-the-art coreference resolvers and several competitive simple baselines on GAP to understand limitations in current modeling, including gender bias. We find that syntactic structure and transformer models (Vaswani et al., 2017) provide promising, complementary cues for approaching GAP. Introduction Coreference resolution involves linking referring expressions that evoke the same discourse entity, as defined in shared tasks such as CoNLL 2011/2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996). Unfortunately, high scores on these tasks do not necessarily translate into acceptable performance for downstream applications such as machine translation (Guillou, 2012) and fact extraction (Nakayama, 2008). In particular, high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems"
Q18-1042,P08-1090,0,0.081571,"Missing"
Q18-1042,E12-3001,0,0.0543334,"es on GAP to understand limitations in current modeling, including gender bias. We find that syntactic structure and transformer models (Vaswani et al., 2017) provide promising, complementary cues for approaching GAP. Introduction Coreference resolution involves linking referring expressions that evoke the same discourse entity, as defined in shared tasks such as CoNLL 2011/2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996). Unfortunately, high scores on these tasks do not necessarily translate into acceptable performance for downstream applications such as machine translation (Guillou, 2012) and fact extraction (Nakayama, 2008). In particular, high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems have caused a wide range of underrepresented groups to be served in an inequitable way by downstream applications (Hardt, 2014). We take the construction of the new GAP co"
Q18-1042,P15-1136,0,0.498753,"examples to re-achieve 1:1 gender balance. Additionally, we imposed the constraint that there be one example per Wikipedia article per pronoun form (e.g., his), to reduce similarity between examples. The final counts for each label are given in the second column of Table 3. Given that the 4,454 contexts each contain two annotated names, this constitutes 8,908 pronoun–name pair labels. 4 GAP Challenge 4.2 Off-the-Shelf Resolvers The first set of baselines we explore are four representative off-the-shelf coreference systems: the rule-based system of Lee et al. (2013) and three neural resolvers—Clark and Manning (2015),10 Wiseman et al. (2016),11 and Lee et al. (2017).12 All were trained on OntoNotes and run in as close to their out-of-the-box configuration as possible.13 System clusters were scored against GAP examples according to whether the cluster Experiments 7 We set up the GAP challenge and analyze the applicability of a range of off-the-shelf tools. We find that existing resolvers do not perform well and are biased to favor better resolution of masculine pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017) encode coreference relationships, adding to the re"
Q18-1042,C90-3063,0,0.560617,"Missing"
Q18-1042,D13-1203,0,0.0985533,"evoke the same discourse entity, as defined in shared tasks such as CoNLL 2011/2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996). Unfortunately, high scores on these tasks do not necessarily translate into acceptable performance for downstream applications such as machine translation (Guillou, 2012) and fact extraction (Nakayama, 2008). In particular, high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems have caused a wide range of underrepresented groups to be served in an inequitable way by downstream applications (Hardt, 2014). We take the construction of the new GAP corpus as an opportunity to reduce gender bias in coreference data sets; in this way, GAP can promote equitable modeling of reference phenomena complementary to the recent work of Zhao et al. (2018) and Rudinger et al. (2018). 1 The examples throughout the paper highlight the ambiguous pronoun in bold, the tw"
Q18-1042,N04-1037,0,0.0456721,"Missing"
Q18-1042,J13-4004,0,0.560815,". . . To correct this, we discarded masculine examples to re-achieve 1:1 gender balance. Additionally, we imposed the constraint that there be one example per Wikipedia article per pronoun form (e.g., his), to reduce similarity between examples. The final counts for each label are given in the second column of Table 3. Given that the 4,454 contexts each contain two annotated names, this constitutes 8,908 pronoun–name pair labels. 4 GAP Challenge 4.2 Off-the-Shelf Resolvers The first set of baselines we explore are four representative off-the-shelf coreference systems: the rule-based system of Lee et al. (2013) and three neural resolvers—Clark and Manning (2015),10 Wiseman et al. (2016),11 and Lee et al. (2017).12 All were trained on OntoNotes and run in as close to their out-of-the-box configuration as possible.13 System clusters were scored against GAP examples according to whether the cluster Experiments 7 We set up the GAP challenge and analyze the applicability of a range of off-the-shelf tools. We find that existing resolvers do not perform well and are biased to favor better resolution of masculine pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017"
Q18-1042,N15-1082,0,0.0572904,"ronouns in sufficient volume or diversity to benchmark systems for practical applications. 2.1 Data Sets with Ambiguous Pronouns Winograd schemas (Levesque et al., 2012) are closely related to our work as they contain ambiguous pronouns. These are pairs of short texts with an ambiguous pronoun and a special word (in square brackets) that switches its referent: (2) The trophy would not fit in the brown suitcase because it was too [big/small]. 2.2 The Definite Pronoun Resolution Data Set (Rahman and Ng, 2012) comprises 943 Winograd schemas written by undergraduate students and later extended by Peng et al. (2015). The First Winograd Schema Challenge (Morgenstern et al., 2016) released 60 examples adapted from published literary works (Pronoun Disambiguation Problem)3 and 285 manually constructed schemas (Winograd Schema Challenge).4 More recently, Rudinger et al. (2018) and Zhao et al. (2018) have created two Winograd schema-style datasets containing 720 and 3,160 sentences, respectively, where each sentence contains a gendered pronoun and two occupation (or participant) antecedent candidates that break occupational gender stereotypes. Overall, ambiguous pronoun datasets have been limited in size and,"
Q18-1042,D17-1018,0,0.49695,"we imposed the constraint that there be one example per Wikipedia article per pronoun form (e.g., his), to reduce similarity between examples. The final counts for each label are given in the second column of Table 3. Given that the 4,454 contexts each contain two annotated names, this constitutes 8,908 pronoun–name pair labels. 4 GAP Challenge 4.2 Off-the-Shelf Resolvers The first set of baselines we explore are four representative off-the-shelf coreference systems: the rule-based system of Lee et al. (2013) and three neural resolvers—Clark and Manning (2015),10 Wiseman et al. (2016),11 and Lee et al. (2017).12 All were trained on OntoNotes and run in as close to their out-of-the-box configuration as possible.13 System clusters were scored against GAP examples according to whether the cluster Experiments 7 We set up the GAP challenge and analyze the applicability of a range of off-the-shelf tools. We find that existing resolvers do not perform well and are biased to favor better resolution of masculine pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017) encode coreference relationships, adding to the results by Voita et al. (2018) on machine translatio"
Q18-1042,N06-1025,0,0.11961,"Missing"
Q18-1042,W12-4501,0,0.644086,"Missing"
Q18-1042,W10-3909,0,0.0480527,"Missing"
Q18-1042,L18-1275,0,0.0496045,"Missing"
Q18-1042,D12-1071,0,0.761767,"ing expressions that evoke the same discourse entity, as defined in shared tasks such as CoNLL 2011/2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996). Unfortunately, high scores on these tasks do not necessarily translate into acceptable performance for downstream applications such as machine translation (Guillou, 2012) and fact extraction (Nakayama, 2008). In particular, high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems have caused a wide range of underrepresented groups to be served in an inequitable way by downstream applications (Hardt, 2014). We take the construction of the new GAP corpus as an opportunity to reduce gender bias in coreference data sets; in this way, GAP can promote equitable modeling of reference phenomena complementary to the recent work of Zhao et al. (2018) and Rudinger et al. (2018). 1 The examples throughout the paper highlight the ambiguo"
Q18-1042,N18-2002,0,0.36026,"Missing"
Q18-1042,P14-5010,0,0.00557068,"et al. (2018) on machine translation, https://cloud.google.com/natural-language/. All examples extracted from the same URL are partitioned into the same set. 9 http://goo.gl/language/gap-coreference. 10 https://stanfordnlp.github.io/CoreNLP/ download.html. 11 https://github.com/swiseman/nn_coref. 12 https://github.com/kentonl/e2e-coref. 13 We run Lee et al. (2017) in the final (single-model) configuration, with NLTK preprocessing (Bird and Loper, 2004); for Wiseman et al. (2016) we use Berkeley preprocessing (Durrett and Klein, 2014); and the Stanford systems are run within Stanford CoreNLP (Manning et al., 2014). 8 6 For example, missing sentence breaks, list environments, and non-referential personal roles/nationalities. 609 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 55.4 58.5 68.4 67.2 F 45.5 51.3 59.9 62.2 B 0.82 0.88 0.88 0.92 O 50.5 55.0 64.2 64.7 80 70 Lee et al. (2013) Clark and Manning Wiseman et al. Lee et al. (2017) M 47.7 64.3 61.9 68.9 F 53.2 63.9 58.0 51.9 B 1.12 0.99 0.94 0.75 Recall Table 4: Performance of off-the-shelf resolvers on the GAP development set, split by Masculine and Feminine (Bias shows F/M), and Overall. Bold indicates best performance. O 49.2"
Q18-1042,N16-1114,0,0.143836,"gender balance. Additionally, we imposed the constraint that there be one example per Wikipedia article per pronoun form (e.g., his), to reduce similarity between examples. The final counts for each label are given in the second column of Table 3. Given that the 4,454 contexts each contain two annotated names, this constitutes 8,908 pronoun–name pair labels. 4 GAP Challenge 4.2 Off-the-Shelf Resolvers The first set of baselines we explore are four representative off-the-shelf coreference systems: the rule-based system of Lee et al. (2013) and three neural resolvers—Clark and Manning (2015),10 Wiseman et al. (2016),11 and Lee et al. (2017).12 All were trained on OntoNotes and run in as close to their out-of-the-box configuration as possible.13 System clusters were scored against GAP examples according to whether the cluster Experiments 7 We set up the GAP challenge and analyze the applicability of a range of off-the-shelf tools. We find that existing resolvers do not perform well and are biased to favor better resolution of masculine pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017) encode coreference relationships, adding to the results by Voita et al. (20"
Q18-1042,P05-1021,0,0.101967,"Missing"
Q18-1042,P09-1074,0,0.210902,"involves linking referring expressions that evoke the same discourse entity, as defined in shared tasks such as CoNLL 2011/2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996). Unfortunately, high scores on these tasks do not necessarily translate into acceptable performance for downstream applications such as machine translation (Guillou, 2012) and fact extraction (Nakayama, 2008). In particular, high-scoring systems successfully identify coreference relationships between string-matching proper names, but fare worse on anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems have caused a wide range of underrepresented groups to be served in an inequitable way by downstream applications (Hardt, 2014). We take the construction of the new GAP corpus as an opportunity to reduce gender bias in coreference data sets; in this way, GAP can promote equitable modeling of reference phenomena complementary to the recent work of Zhao et al. (2018) and Rudinger et al. (2018). 1 The examples throughout the paper"
Q18-1042,D17-1323,0,0.0578088,"ifferent from one another. Additionally, we do not allow intruders: There can be no other compatible mention (by gender, number, and entity type) between the pronoun and the two names. To limit the success of naïve resolution heuristics, we apply a small set of constraints to focus on those pronouns that are truly hard to resolve. The salesperson sold some books to the librarian because she was trying the sell them. The pervasive bias in existing datasets is concerning given that learned NLP systems often reflect and even amplify training biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2017). A growing body of work defines notions of fairness, bias, and equality in data and machine-learned systems (Pedreshi et al., 2008; Hardt et al., 2016; Skirpan and Gorelick, 2017; Zafar et al., 2017), 607 Type F INAL P RO Pattern (Name, Name, Pronoun) M EDIAL P RO (Name, Pronoun, Name) I NITIAL P RO (Pronoun, Name, Name) Example Preckwinkle criticizes Berrios’ nepotism: [. . . ] County’s ethics rules don’t apply to him. McFerran’s horse farm was named Glen View. After his death in 1885, John E. Green acquired the farm. Judging that he is suitable to join the team, Butcher injects Hughie with"
Q18-1042,N18-2003,0,0.533868,"anaphoric mentions such as pronouns and common noun phrases (Stoyanov et al., 2009; Rahman and Ng, 2012; Durrett and Klein, 2013). Coreference resolution decisions can drastically alter how automatic systems process text. Biases in automatic systems have caused a wide range of underrepresented groups to be served in an inequitable way by downstream applications (Hardt, 2014). We take the construction of the new GAP corpus as an opportunity to reduce gender bias in coreference data sets; in this way, GAP can promote equitable modeling of reference phenomena complementary to the recent work of Zhao et al. (2018) and Rudinger et al. (2018). 1 The examples throughout the paper highlight the ambiguous pronoun in bold, the two potential coreferent names in italics, and the correct one also underlined. 2 http://goo.gl/language/gap-coreference. 605 Transactions of the Association for Computational Linguistics, vol. 6, pp. 605–618, 2018. Action Editor: Luke Zettlemoyer. Submission batch: 7/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Such approaches promise to improve equity of downstream models, such as triple extraction for knowledge-bas"
Q18-1042,P18-1117,0,0.022253,"an et al. (2016),11 and Lee et al. (2017).12 All were trained on OntoNotes and run in as close to their out-of-the-box configuration as possible.13 System clusters were scored against GAP examples according to whether the cluster Experiments 7 We set up the GAP challenge and analyze the applicability of a range of off-the-shelf tools. We find that existing resolvers do not perform well and are biased to favor better resolution of masculine pronouns. We empirically validate the observation that Transformer models (Vaswani et al., 2017) encode coreference relationships, adding to the results by Voita et al. (2018) on machine translation, https://cloud.google.com/natural-language/. All examples extracted from the same URL are partitioned into the same set. 9 http://goo.gl/language/gap-coreference. 10 https://stanfordnlp.github.io/CoreNLP/ download.html. 11 https://github.com/swiseman/nn_coref. 12 https://github.com/kentonl/e2e-coref. 13 We run Lee et al. (2017) in the final (single-model) configuration, with NLTK preprocessing (Bird and Loper, 2004); for Wiseman et al. (2016) we use Berkeley preprocessing (Durrett and Klein, 2014); and the Stanford systems are run within Stanford CoreNLP (Manning et al."
W03-0403,C00-1085,1,\N,Missing
W03-0403,P97-1003,0,\N,Missing
W03-0403,C02-2025,0,\N,Missing
W03-0403,W00-1306,0,\N,Missing
W03-0403,W02-2030,0,\N,Missing
W03-0403,P02-1034,0,\N,Missing
W03-0403,P96-1042,0,\N,Missing
W03-0403,P02-1046,0,\N,Missing
W03-0403,P99-1069,0,\N,Missing
W03-0403,W02-2018,0,\N,Missing
W03-2316,E03-1036,1,\N,Missing
W03-2316,hockenmaier-steedman-2002-acquiring,0,\N,Missing
W03-2316,P02-1041,1,\N,Missing
W03-2316,J93-1008,0,\N,Missing
W03-2316,P99-1039,0,\N,Missing
W03-2316,W98-1415,0,\N,Missing
W03-2316,P89-1005,0,\N,Missing
W03-2316,P96-1027,0,\N,Missing
W03-2316,W02-2106,0,\N,Missing
W03-2316,P01-1019,0,\N,Missing
W04-3202,W00-1306,0,0.436543,"to be extended with a new labeled example {hxi , y i i}. The information gain for some model is maximized after selecting, labeling, and adding a new example xi to Dn such that the noise level of xi is low and both the bias and variance of some model using Dn ∪ {hxi , y i i} are minimized (Cohn et al., 1995). In practice, selecting data points for labeling such that a model’s variance and/or bias is maximally minimized is computationally intractable, so approximations are typically used instead. One such approximation is uncertainty sampling. Uncertainty sampling (also called tree entropy by Hwa (2000)), measures the uncertainty of a model over the set of parses of a given sentence, based on the conditional 1 This eyeball step is not always taken, but Redwoods does not contain information about when this occurred, so we apply the cost for the step uniformly for all examples. distribution it assigns to them. Following Hwa, we use the following measure to quantify uncertainty: AL results are usually presented in terms of the amount of labeling necessary to achieve given performance levels. We say that one method is betX ter than another method if, for a given performance fus (s, τ, Mk ) = − P"
W04-3202,W01-0710,0,0.0351381,"ling. For most situations, n-best automation is beneficial: the gap introduced by reuse can be reduced. nbest automation never results in an increase in cost. This is still true even if we do not allow ourselves to reuse those discriminants which were used to select the best parse from the n-best subset and the best parse was not actually present in that subset. 9 Related work There is a large body of AL work in the machine learning literature, but less so within natural language processing (NLP). Most work in NLP has primarily focused upon uncertainty sampling (Hwa, 2000; Tang et al., 2002). Hwa (2001) considered reuse of examples selected for one parser by another with uncertainty sampling. This performed better than sequential sampling but was only half as effective as self-selection. Here, we have considered reuse with respect to many models and their co-relatedness. Also, we compare reuse performance against against random sampling, which we showed previously to be a much stronger baseline than sequential sampling for the Redwoods corpus (Osborne and Baldridge, 2004). Hwa et al. (2003) showed that for parsers, AL outperforms the closely related co-training, and that some of the labeling"
W04-3202,P99-1069,0,0.00940286,"notator, annotators make use of discriminants which disambiguate the parse forest more rapidly, as described in section 3. In this paper, we report results using the third growth of Redwoods, which contains English sentences from appointment scheduling and travel planning domains of Verbmobil. In all, there are 5302 sentences for which there are at least two parses and a unique preferred parse is identified. These sentences have 9.3 words and 58.0 parses on average. 2.2 Modeling parse selection As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al., 1999). For log-linear models, the conditional probability of an analysis ti given a sentence with a set of analyses τ = {t . . .} is given as: Pm P (ti |s, Mk ) = exp( j=1 fj (ti )wj ) Z(s) (1) where fj (ti ) returns the number of times feature j occurs in analysis t, wj is a weight from model Mk , and Z(s) is a normalization factor for the sentence. The parse with the highest probability is taken as the preferred parse for the model. We use the limited memory variable metric algorithm to determine the weights. We do not regularize our loglinear models since labeled data -necessary to set hyperpara"
W04-3202,C02-2025,0,0.0209056,"Missing"
W04-3202,N04-1012,1,0.73703,"ng the later model to improve in quality, or else reusing the labeled training material using a different machine learning algorithm) performance of later models can be significantly undermined when training upon material created using AL. The key to knowing how well one model will be able to use material selected by another is their relatedness – yet there may be no means to determine this prior to annotation, leading to a chicken-and-egg problem. Our reusability results thus demonstrate that, additionally, other strategies must be adopted to ensure we reduce the total cost of annotation. In Osborne and Baldridge (2004), we showed that ensemble models can increase model performance and also produce annotation savings when incorporated into the AL process. An obvious next step is automating some decisions. Here, we consider a simple automation strategy that reduces annotation costs independently of AL and examine its effect on reusability. We find that using both semi-automation and AL with high-quality models can eliminate the performance gap found in many reuse scenarios. However, for weak models, we show that semi-automation with random sampling is more effective for improving reusability than using it wit"
W04-3202,P02-1016,0,0.282827,"ing with random sampling. For most situations, n-best automation is beneficial: the gap introduced by reuse can be reduced. nbest automation never results in an increase in cost. This is still true even if we do not allow ourselves to reuse those discriminants which were used to select the best parse from the n-best subset and the best parse was not actually present in that subset. 9 Related work There is a large body of AL work in the machine learning literature, but less so within natural language processing (NLP). Most work in NLP has primarily focused upon uncertainty sampling (Hwa, 2000; Tang et al., 2002). Hwa (2001) considered reuse of examples selected for one parser by another with uncertainty sampling. This performed better than sequential sampling but was only half as effective as self-selection. Here, we have considered reuse with respect to many models and their co-relatedness. Also, we compare reuse performance against against random sampling, which we showed previously to be a much stronger baseline than sequential sampling for the Redwoods corpus (Osborne and Baldridge, 2004). Hwa et al. (2003) showed that for parsers, AL outperforms the closely related co-training, and that some of"
W05-0613,W01-1605,0,0.620526,"Missing"
W05-0613,J03-4003,0,0.673334,"robust dialogue processing has been statistical models for identifying dialogue acts (e.g., Stolcke et al. (2000)). However, dialogue acts are properties of utterances rather than hierarchically arranged relations between them, so they do not provide a basis for resolving semantic underspecification generated by the grammar (Asher and Lascarides, 2003). Here, we present the first probabilistic approach to parsing the discourse structure of dialogue. We use dialogues from Redwoods’ appointment scheduling domain and adapt head-driven generative parsing strategies from sentential parsing (e.g., Collins (2003)) for discourse parsing. The discourse structures we build conform to Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). SDRT provides a precise dynamic semantic interpretation for its discourse structures which augments the conventional semantic representations that are built by most grammars. We thus view the task of learning a model of SDRT-style discourse structures as one step towards achieving the goal of robust and precise semantic interpretations. We describe SDRT in the context of our domain in Section 2. Section 3 discusses how we encode and annotate disco"
W05-0613,P97-1013,0,0.146557,"retation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Stru¨ be and M¨uller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al. (1998)). This specialization makes it hard to assess how they would perform in the context of a more comprehensive set of interpretation tasks. To date, most methods for constructing discourse structures are not robust. They typically rely on grammatical input and use symbolic methods which inevitably lack coverage. One exception is Marcu’s work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Marcu (1999) uses a decision-tree learner and shallow syntactic features 96 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 96–103, Ann Arbor, June 2005. 2005 Association for Computational Linguistics to create classifiers for discourse segmentation and for identifying rhetorical relations. Together, these amount to a model of discourse parsing. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and"
W05-0613,miltsakaki-etal-2004-penn,0,0.0437165,"1 Pass Continuation pause irr ind Elaboration/h2 ind Continuation 151 152 153 ind Alternation 156 ind 154 ind 157 155 Figure 3: The discourse structure for the dialogue from Figure 1 in tree form. 3 Augmenting the Redwoods treebank with discourse structures Our starting point is to create training material for probabilistic discourse parsers. For this, we have augmented dialogues from the Redwoods Treebank (Oepen et al., 2002) with their analyses within a fragment of SDRT (Baldridge and Lascarides, 2005). This is a very different effort from that being pursued for the Penn Discourse Treebank (Miltsakaki et al., 2004), which uses discourse connectives rather than abstract rhetorical relations like those in SDRT in order to provide theory neutral annotations. Our goal is instead to leverage the power of the semantics provided by SDRT’s relations, and in particular to do so for dialogue as opposed to monologue. Because the SDRS-representation scheme, as shown in Figure 2, uses graph structures that do not conform to tree constraints, it cannot be combined directly with statistical techniques from sentential parsing. We have therefore designed a headed tree encoding of SDRSs, which can be straightforwardly mo"
W05-0613,J92-4007,0,0.0645998,"al sentences). Marcu (1999) uses a decision-tree learner and shallow syntactic features 96 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 96–103, Ann Arbor, June 2005. 2005 Association for Computational Linguistics to create classifiers for discourse segmentation and for identifying rhetorical relations. Together, these amount to a model of discourse parsing. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and the classifiers rely on well-formedness constraints on RST trees which are too restrictive (Moore and Pollack, 1992). Furthermore, RST does not offer an account of how compositional semantics gets augmented, nor does it model anaphora. It is also designed for monologue rather than dialogue, so it does not offer a precise semantics of questions or non-sentential utterances which convey propositional content (e.g., 154 and 155 in Figure 1). Another main approach to robust dialogue processing has been statistical models for identifying dialogue acts (e.g., Stolcke et al. (2000)). However, dialogue acts are properties of utterances rather than hierarchically arranged relations between them, so they do not provi"
W05-0613,N03-1030,0,0.322,"to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Stru¨ be and M¨uller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al. (1998)). This specialization makes it hard to assess how they would perform in the context of a more comprehensive set of interpretation tasks. To date, most methods for constructing discourse structures are not robust. They typically rely on grammatical input and use symbolic methods which inevitably lack coverage. One exception is Marcu’s work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Marcu (1999) uses a decision-tree learner and shallow syntactic features 96 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 96–103, Ann Arbor, June 2005. 2005 Association for Computational Linguistics to create classifiers for discourse segmentation and for identifying rhetorical relations. Together, these amount to a model of discourse parsing. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and the classifiers rely on well-formedness c"
W05-0613,J00-3003,0,0.216781,"Missing"
W05-0613,P03-1022,0,0.0593945,"Missing"
W05-0613,C00-2130,0,\N,Missing
W05-0613,P99-1047,0,\N,Missing
W08-0201,W08-0208,1,0.843127,"a range of external tools and have adapted tools from our own research for various aspects of our courses. In this section, we describe our experience using these as part of our courses. We have used Python as the common language in our courses. We are pleased with it: it is straightforward for beginning programmers to learn, its interactive prompt facilitates in-class instruction, it is text-processing friendly, and it is useful for gluing together other (e.g., Java and C++) applications. 3.1 External tools and resources NLTK. We use the Natural Language Toolkit (NLTK) (Loper and Bird, 2002; Bird et al., 2008) in both undergraduate and graduate courses for in-class demos, tutorials, and homework assignments. We use the toolkit and tutorials for several course components, including introductory Python programming, text processing, rule-based part-of-speech tagging and chunking, and grammars and parsing. NLTK is ideal for both novice and advanced programmers. The tutorials and extensive documentation provide novices with plenty of support outside of the classroom, and the toolkit is powerful enough to give plenty of room for advanced students to play. The demos are also very useful in classes and ser"
W08-0201,W05-0103,0,0.199936,"was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs and experiment with their behavior so easily. Language and Computers. We had fortunately already planned the first replacement course: Language and Computers, based on the course designed at the Department of Linguistics at the Ohio State University (Brew et al., 2005). This course introduces computational linguistics to a general audience and is ideal for students who want exposure to computational methods without having to learn to program. We designed and taught it jointly, and added several new aspects to the course. Whereas OSU’s course fulfills a Mathematical and Logical Analysis requirement, our course fulfills a Science requirement for liberal arts majors. These requirements were met by course content that requires understanding and thinking about formal methods. The topics we added to our course were question answering, cryptography,2 and world kno"
W08-0201,W02-0102,0,0.0225202,"cs students who may have to deal with formalism wars in their own linguistic subfield. Also, XFST allows one to use lenient composition to encode Optimality Theory constraints and in so doing show interesting and direct contrasts and comparisons between paper-and-pencil linguistics and rigorous computational implementations. As with other implementation-oriented activities in our classes, we created a wiki page for XFST tutorials.3 These were adapted and expanded from Xerox PARC materials and Mark Gawron’s examples. Eisner’s HMM Materials. Simply put: the spreadsheet designed by Jason Eisner (Eisner, 2002) for teaching hidden Markov models is fantastic. We used that plus Eisner’s HMM homework assignment for Computational Linguistics II in Fall 2007. The spreadsheet is great for interactive classroom exploration of HMMs—students were very engaged. The homework allows students to implement an HMM from scratch, giving enough detail to alleviate much of the needless frustration that could occur with this task while ensuring that students need to put in significant effort and understand the concepts in order to make it work. It also helps that the new edition of Jurafsky and Martin’s textbook discus"
W08-0201,J00-4006,0,0.0049271,"ational linguistics, but that they would not actually have to do computational linguistics. Many stayed with it, but there were still others who could have gone much further if it had not been necessary to slow down to cover basic material like for loops. Note that several linguistics majors were among the compationally savvy students. In fairness to the students who struggled, it was certainly ill-advised to ask students with no previous background to learn Python and XFST in a single semester. One of the key points of confusion was regular expression syntax. The syntax used in the textbook (Jurafsky and Martin, 2000) transfers easily to regular expressions in Python, but is radically different from that of XFST. For students who had never coded anything in their life, this proved extremely frustrating. On the other hand, for computationally savvy students, XFST was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs an"
W08-0201,W02-0109,0,0.440661,"rse with the R statistics package. It was used for statistical analyses: students loved it because they could produce meaningful results immediately and visualize them. The course includes only a very short two-session introduction to working with R. We were worried that this would overtax students because R is its own programming language. But interestingly they had no problems with learning this second programming language (after Python). This is particularly striking as most of the students had no programming experience prior to the class. We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. But as it, too, offers visualization and rapid access to meaningful results, we intend to use it in the future. In particular, the NLTK allows very easy access to Toolbox data (Robinson et al., 2007), which we feel will greatly improve the utility and appeal of the course for the significant number of documentary linguistics students in the department. Seminars. We also offer several seminars in our areas of interest. These include Categorial Grammar, Computational Syntax, and Lexical Acquisition. These courses have attracted “noncomputational” linguistics st"
W08-0201,D07-1041,1,0.830627,"e. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these skills and needs is the core of this course, which covers corpus formats (XML,"
W08-0201,P07-3002,0,0.0159617,"on for their project, rather than rushing everything in last minute. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these sk"
W08-0201,P06-4018,0,\N,Missing
W08-0208,W08-0201,1,0.832948,"has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimized support for linear algebra and sparse arrays (NumPy, 2008) and"
W08-0208,P04-3031,1,0.74082,"es at several universities, and based on feedback from many teachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and p"
W08-0208,P06-4018,1,0.46147,"eachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in"
W08-0208,W08-0204,1,0.723675,"tics students to write their first ever program, leads to stressed students who complain that they don’t know what is expected of them. Nevertheless, students need to confront the challenge of becoming bilingual, of working hard to learn the basics of another discipline. In parallel, instructors need to confront the challenge of synthesizing material from linguistics and computer science into a coherent whole, and devising effective methods for teaching, learning, and assessment. 3.3 Entry Points It is possible to identify several distinct pathways into the field of Computational Linguistics. Bird (2008) identifies four; each of these are supported by NLTK, as detailed below: Text Processing First: NLTK supports variety of approaches to tokenization, tagging, evaluation, and language engineering more generally. Programming First: NLTK is based on Python and the documentation teaches the language and provides many examples and exercises to test and reinforce student learning. Linguistics First: Here, students come with a grounding in one or more areas of linguistics, and focus on computational approaches to that area by working with the relevant chapter of the NLTK book in conjunction with lea"
W08-0208,W05-0101,0,0.0862997,"ck from many teachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries ar"
W08-0208,U06-1006,1,0.695584,"students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP conte"
W08-0208,W05-0111,0,0.118105,"er this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimize"
W08-0208,W02-0109,1,0.589789,"ode to a uniform cohort of students. Linguists can be taught to program, leading to projects where students manipulate their own linguistic data. Computer scientists can be taught methods for automatic text processing, leading to projects on text mining and chatbots. Yet these approaches have almost nothing in common, and it is a stretch to call either of these NLP: more apt titles for such courses might be “linguistic data management” and “text technologies.” The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). In particular, NLTK makes it feasible to run a course that covers a substantial amount of theory and practice with an audience consisting of both linguists and computer scientists. NLTK is a suite of Python modules distributed under the GPL open source license via nltk.org. NLTK comes with a large collection of corpora, extensive documentation, and hundreds of exercises, making NLTK unique in providing a comprehensive framework for students to develop a computational understanding of language. NLTK’s code base of 100,000 lines of Python code includes support for corpus access, tokenizing, st"
W08-0208,W08-0209,0,0.315004,"ne tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimized support for linear algebra and sparse"
W09-1905,W04-3202,1,0.896776,"Missing"
W09-1905,W05-0619,0,0.0505642,"ix experimental 41 40 20 10 Seconds per Morpheme 30 Non−expert Expert 0 money. Since annotator pay may be variable but will (under standard assumptions) be constant for a given annotator, the best approximation of likely cost savings is to measure the time taken to annotate under different levels of automated support. This is especially important in sample selection and its interaction with automated suggestions: active learning seeks to find more informative examples, and these will most likely involve more difficult decisions, decreasing annotation quality and/or increasing annotation time (Hachey et al., 2005). Thus, we measure cost in terms of the time taken by each annotator on each example. This allows us to measure the actual time taken to produce a given labeled data set, and thus compare the effectiveness of different levels of automated support plus their interaction with annotators of different levels of expertise. Recent work shows that paying attention to predicted annotation cost in sample selection itself can increase the effectiveness of active learning (Settles et al., 2008; Haertel et al., 2008b). Though we have not explored cost-sensitive selection here, the scenario described here"
W09-1905,P08-2017,0,0.0342038,"more difficult decisions, decreasing annotation quality and/or increasing annotation time (Hachey et al., 2005). Thus, we measure cost in terms of the time taken by each annotator on each example. This allows us to measure the actual time taken to produce a given labeled data set, and thus compare the effectiveness of different levels of automated support plus their interaction with annotators of different levels of expertise. Recent work shows that paying attention to predicted annotation cost in sample selection itself can increase the effectiveness of active learning (Settles et al., 2008; Haertel et al., 2008b). Though we have not explored cost-sensitive selection here, the scenario described here is an appropriate test ground for it: in fact, the results of our experiments, reported in the next section, provide strong evidence for a real natural language annotation task that active learning selection with cost-sensitivity is indeed sub-optimal. 0 10 20 30 40 50 Number of Annotation Rounds Figure 1: Average annotation time (in seconds per morpheme) over annotation rounds, averaged over all six conditions for each annotator. cases for each annotator. The newly-labeled clauses are then added to the"
W09-1905,W00-1306,0,0.0351614,"st setups. For uncertainty sampling, we measure uncertainty of a clause as the average entropy per morpheme (i.e., per labeling decision). 3.5 Measuring annotation cost Not all examples take the same amount of effort to annotate. Even so, the bulk of the literature on active learning assumes some sort of unit cost to determine the effectiveness of different sample selection strategies. Examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (Settles and Craven, 2008), or the number of constituents in parsing (Hwa, 2000). These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al., 2008a; Settles et al., 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. Also, Baldridge and Osborne (2004) use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (Baldridge and Osborne, 2008). The cost of annotation ultimately comes down to 4 Discussion This section presents and discusses p"
W09-1905,I08-2093,0,0.0339371,"phemes with tags, some corresponding to parts-of-speech and others to semantic distinctions. There is no single standard format for IGT. The IGT systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely to be compatible or interchangeable with systems developed by other projects. They may lack internal consistency as well. Nonetheless, IGT in a readily accessible format is an important resource that can be used fruitfully by linguists to examine hypotheses on novel data (e.g. Xia and Lewis (2007; 2008), Lewis and Xia (2008)). Furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language. Despite the urgent need for such resources, IGT annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain. Thus, language 1 KEY: COM=completive DIR=directional aspect, DEM=demonstrative, Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36–44, c Boulder, Colorado, June 2009. 2009 Association for Computation"
W09-1905,P00-1016,0,0.224916,"Missing"
W09-1905,W07-1528,1,0.75142,"lycomplete texts were filled in by the expert annotator. A challenge for representing IGT in a machinereadable format is maintaining the links between S=sustantivo (noun), SC=category suffix, TAM=tense/aspect/mood, VT=transitive verb 5 SUF=suffix, http://www.sil.org/computing/shoebox/ 38 the source text morphemes in the second tier and the morpheme-by-morpheme glosses in the third tier. The standard Shoebox output format, for example, enforces these links through management of the number of spaces between items in the output. To address this, we converted the cleaned annotations into IGT-XML (Palmer and Erk, 2007) with help from the Shoebox/Toolbox interfaces provided in the Natural Language Toolkit (Robinson et al., 2007). Automating the transformation from Shoebox format to IGT-XML’s hierarchical format required cleaning up tier-to-tier alignment and checking segmentation in some cases where morphemes and glosses were misaligned, as in (5) below.6 (4) Non li in yolow rk’il (5) Non li in yolow r-k’il DEM DEM yo platicar AP E3s.-SR DEM DEM PRON VI SUF PERS SREL ’Solo asi yo aprendi con e´ l.’ Here, the number of elements in the morpheme tier (first line of (5)) does not match the number of elements in"
W09-1905,D08-1112,0,0.190892,"ree components: 1) presenting examples to the annotator and storing the annotations, 2) training and evaluation of tagging models using data labeled by the annotator, and 3) selecting new examples for annotation. The processes are managed and coordinated using the OpenNLP IGT Editor.7 The annotation component of the tool, and in particular the user interface, is built on the Interlinear Text Editor (Lowe et al., 2004). For tagging we use a strong but simple standard classifier. There certainly are many other modeling strategies that could be used, for example a conditional random field (as in Settles and Craven (2008)), or a model that deals differently with POS labels and morpheme gloss labels. Nonetheless, a documentary linguistics project would be most likely to use a straightforward, off-the-shelf labeler, and our focus is on exploring different annotation approaches in a realistic documentation setting rather than building an optimal classifier. To that end, we use a standard maximum entropy classifier which predicts the label for a morpheme based on the morpheme itself plus a window of two morphemes before and after. Standard features used in part-of-speech taggers are extracted from the morpheme to"
W09-1905,N07-1057,0,0.0293942,"nd labeling of stems and morphemes with tags, some corresponding to parts-of-speech and others to semantic distinctions. There is no single standard format for IGT. The IGT systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely to be compatible or interchangeable with systems developed by other projects. They may lack internal consistency as well. Nonetheless, IGT in a readily accessible format is an important resource that can be used fruitfully by linguists to examine hypotheses on novel data (e.g. Xia and Lewis (2007; 2008), Lewis and Xia (2008)). Furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language. Despite the urgent need for such resources, IGT annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain. Thus, language 1 KEY: COM=completive DIR=directional aspect, DEM=demonstrative, Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36–44, c Boulder, Colorado, June 2009. 200"
W09-1905,I08-1069,0,0.106181,"Missing"
W11-2207,D09-1062,0,0.111002,"that has users, tweets, word unigrams, word bigrams, hashtags, and emoticons as its nodes; users are connected based on the Twitter follower graph, users are connected to the tweets they created, and tweets are connected to the unigrams, bigrams, hashtags and emoticons they contain. We seed the graph using the polarity values in the OpinionFinder lexicon (Wilson et al., 2005), the known polarity of emoticons, and a maximum entropy classifier trained on 1.8 million tweets with automatically assigned labels based on the presence of positive and negative emoticons, like Read (2005) and Go et al. (2009). We compare the label propagation approach to the noisily supervised classifier itself and to a standard lexicon-based method using positive/negative ratios. Evaluation is performed on several datasets of tweets that have been annotated for polarity: the Stanford Twitter Sentiment set (Go et al., 2009), 1 Davidov et al. (2010) use 15 emoticons and 50 Twitter hashtags as proxies for sentiment in a similar manner, but their evaluation is indirect. Rather than predicting gold standard sentiment labels, they instead predict whether those same emoticons and hashtags would be appropriate for other"
W11-2207,C10-2028,0,0.10503,"larity values in the OpinionFinder lexicon (Wilson et al., 2005), the known polarity of emoticons, and a maximum entropy classifier trained on 1.8 million tweets with automatically assigned labels based on the presence of positive and negative emoticons, like Read (2005) and Go et al. (2009). We compare the label propagation approach to the noisily supervised classifier itself and to a standard lexicon-based method using positive/negative ratios. Evaluation is performed on several datasets of tweets that have been annotated for polarity: the Stanford Twitter Sentiment set (Go et al., 2009), 1 Davidov et al. (2010) use 15 emoticons and 50 Twitter hashtags as proxies for sentiment in a similar manner, but their evaluation is indirect. Rather than predicting gold standard sentiment labels, they instead predict whether those same emoticons and hashtags would be appropriate for other tweets. 54 tweets from the 2008 debate between Obama and McCain (Shamma et al., 2009), and a new dataset of tweets about health care reform that we have created. In addition to performing standard per-tweet accuracy, we also measure per-target accuracy (for health care reform) and an aggregate error metric over all users in our"
W11-2207,W02-1011,0,0.018506,"al. (2010)), or calculating a ratio of positive to negative terms (Choi and Cardie, 2009). Though these are a useful first pass, the nuance of language often defeats them (Pang and Lee, 2008). Tweets provide additional challenges compared to edited text; e.g. they are short and include informal/colloquial/abbreviated language. Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 53–63, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Standard supervised classification methods improve the situation somewhat (Pang et al., 2002), but these require texts labeled with polarity as input and they do not adapt to changes in language use. One way around this is to use noisy labels (also referred to as “distant supervision”), e.g. by taking emoticons like ‘:)’ as positive and ‘:(’ as negative, and train a standard classifier (Read, 2005; Go et al., 2009).1 Semi-supervised methods can also reduce dependence on labeled texts: for example, Sindhwani and Melville (2008) use a polarity lexicon combined with label propagation. Several have used label propagation starting with a small number of hand-labeled words to induce a lexic"
W11-2207,P04-1035,0,0.0191593,"care reform dataset, the label propagation approach—which uses no gold labeled tweets, just a hand-created lexicon—outperforms a maximum entropy classifier trained on gold labels. However, we do not find the follower graph to improve performance with our current implementation. 2 Datasets We use several different Twitter datasets as training or evaluation resources. From the annotated datasets, only tweets with positive or negative polarity are used, so neutral tweets are ignored. While important, subjectivity detection is largely a different problem from polarity classification. For example, Pang and Lee (2004) use minimum cuts in graphs for the former and machine-learned text classification for the latter. We also do not give any special treatment to retweets, though doing so is a possible future improvement. 2.1 Emoticon-based training set (E MOTICON) Emoticons are commonly exploited as noisy indicators of polarity—including by Twitter’s own advanced search “with positive/negative attitude.” While imperfect, there is potential for millions of tweets containing emoticons to serve as a source of noisy training material for a supervised classifier. We create such a training set from a sample of the “"
W11-2207,E09-1077,0,0.059396,"ot adapt to changes in language use. One way around this is to use noisy labels (also referred to as “distant supervision”), e.g. by taking emoticons like ‘:)’ as positive and ‘:(’ as negative, and train a standard classifier (Read, 2005; Go et al., 2009).1 Semi-supervised methods can also reduce dependence on labeled texts: for example, Sindhwani and Melville (2008) use a polarity lexicon combined with label propagation. Several have used label propagation starting with a small number of hand-labeled words to induce a lexicon for use in polarity classification (Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Brody and Elhadad, 2010). In this paper, we bring together several of the above approaches via label propagation using modified adsorption (Talukdar and Crammer, 2009). This also allows us to explore the possibility of exploiting the Twitter follower graph to improve polarity classification, under the assumption that people influence one another or have shared affinities about topics. We construct a graph that has users, tweets, word unigrams, word bigrams, hashtags, and emoticons as its nodes; users are connected based on the Twitter follower graph, users are connected to the tweets they cr"
W11-2207,P05-2008,0,0.136091,"ted language. Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 53–63, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Standard supervised classification methods improve the situation somewhat (Pang et al., 2002), but these require texts labeled with polarity as input and they do not adapt to changes in language use. One way around this is to use noisy labels (also referred to as “distant supervision”), e.g. by taking emoticons like ‘:)’ as positive and ‘:(’ as negative, and train a standard classifier (Read, 2005; Go et al., 2009).1 Semi-supervised methods can also reduce dependence on labeled texts: for example, Sindhwani and Melville (2008) use a polarity lexicon combined with label propagation. Several have used label propagation starting with a small number of hand-labeled words to induce a lexicon for use in polarity classification (Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Brody and Elhadad, 2010). In this paper, we bring together several of the above approaches via label propagation using modified adsorption (Talukdar and Crammer, 2009). This also allows us to explore the possi"
W11-2207,P02-1053,0,0.00548612,"e OMD corpus, meaning mccain is much more frequent in this corpus than the reference corpus, so any sentiment associated with mccain is propagated strongly. In this case, the output of label propagation seeded with Noisy-seed reveals that mccain has negative sentiment for this dataset. 6 Related Work Much work in sentiment analysis involves the use and generation of dictionaries capturing the sentiment of words. These methods range from manual approaches of developing domain-dependent lexicons (Das and Chan, 2001) to semi-automated approaches (Hu and Liu, 2004) and fully automated approaches (Turney, 2002). Melville et al. (2009) use a unified framework combining background lexical information in terms of word-class associations and refine this information for specific domains using any available training examples. They produce better results than using either a lexicon or training. O’Connor et al. (2010) use the OpinionFinder subjectivity lexicon to label the polarity of tweets about Barack Obama and compare daily aggregate sentiment scores to the Gallup poll time series of manually gathered approval ratings of Obama. Even with this simple polarity determination, they find significant correlat"
W11-2207,N10-1122,0,\N,Missing
W11-2207,H05-2018,0,\N,Missing
W13-2307,2020.lrec-1.643,0,0.293721,"Missing"
W13-2307,P99-1010,0,0.418476,"fficiently mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Bo"
W13-2307,N06-1019,0,0.236476,"mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Both annotations and analys"
W13-2307,W07-2416,0,0.0563019,"ator labels are as in table 2. Per-annotator com (with lexical reconciliation) and inter-annotator softComPrec are aggregated over sentences by arithmetic mean. less burdensome, and the specialized annotations did prove complementary to each other.19 5.4 Treebank Comparison Though the annotators in our study were native speakers well acquainted with representations of English syntax, we sought to quantify their agreement with the expert treebankers who created the EWTB (the source of the Reviews sentences). We converted the EWTB’s constituent parses to dependencies via the PennConverter tool (Johansson and Nugues, 2007),20 then removed punctuation. Agreement with the converted treebank parses appears in the bottom two rows of table 3. Because the EWTB commits to a single analysis, precision scores are quite lopsided. Most of its attachments are consistent with our annotations (softComPrec &gt; 0.9), but these allow many additional analyses (hence the scores below 0.5). Annotator Specialization As an experiment in using underspecification for labor division, two of the annotators of Reviews data were assigned specific linguistic phenomena to focus on. Annotator “D” was tasked with the internal structure of base"
W13-2307,P06-2066,0,0.0538705,"nd compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced language"
W13-2307,W12-1706,0,0.033779,"When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced languages (to name two examples). Traditional syntactic annotation projects like the Penn Treebank (Marcus ∗ 2 A Dependency Grammar for Annotation Although depende"
W13-2307,J93-2004,0,0.0444543,"portion of the English Web Treebank 14 Malagasy is a VOS Austronesian language spoken by 15 million people, mostly in Madagascar. Kinyarwanda is an SVO Bantu language spoken by 12 million people mostly in Rwanda. All annotations were done by native speakers of English. The Kinyarwanda and Malagasy annotators had basic proficiency in these languages. 15 As a point of comparison, during the Penn Treebank project, annotators corrected the syntactic bracketings produced by a high-quality hand-written parser (Fidditch) and achieved a rate of only 375 tokens/hour using a specialized GUI interface (Marcus et al., 1993). 16 Included with the data and software release (footnote 1). 57 com thus reduces the commitment averages for each annotation—to a greater extent for annotator “A” (.96 in table 2 vs. .82 in table 3) because “A” marked more multiwords. An analysis fully compatible with both annotations exists for only 27/60 sentences; the finer-grained softComPrec measure (§4.2), however, offers insight into the balance between commitment and agreement. Qualitatively, we observe three leading causes of incompatibilities (disagreements): obvious annotator mistakes (such as the marked as a head); inconsistent h"
W13-2307,1993.iwpt-1.22,0,0.0410599,"lop algorithms to evaluate and compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and"
W13-2307,D07-1014,1,0.853574,"edges that are known to be incompatible with the annotation before searching for spanning trees. Our “upward-downward” method for constructing a graph of supported edges first enumerates a set of candidate top nodes for every fudge expression, then uses that information to infer a set of supported parents for every node.12 The supported edge graph then consists of vertices lexnodes(A) ∪ {root} and edges S 0 0 v∈lexnodes(A) {(v → v ) ∀ v ∈ suppParentsA (v)}. From this graph we can count all directed spanning trees in cubic time using Kirchhoff’s matrix tree theorem (Chaiken and Kleitman, 1978; Smith and Smith, 2007; Margoliash, 2010).13 If some lexical node has no supported parents, this reflects conflicting constraints in the annotation, and no spanning tree will be found. Promiscuity will tend to be higher for longer sentences. To control for this, we define a second quantity, the annotation’s commitment quotient (commitment being the opposite of promiscuity), 4.2 Inter-Annotator Agreement FUDG can encode flat groupings and coreference at the lexical level, as well as syntactic structure over lexical items. Inter-annotator agreement can be measured separately for each of these facets. Pilot annotator"
W13-2307,D08-1027,1,0.400347,"Missing"
W13-2307,N13-1039,1,0.775169,"Missing"
W13-2307,W13-2307,1,0.0512826,"Missing"
W14-1615,J99-1004,0,0.168391,"istributions are constructed to build in additional inductive bias. 3.1 The first idea subsumes the complexity measure used by Baldridge, but accomplishes the goal naturally by letting the probabilities decrease as the category grows. The rate of decay is governed by the pterm parameter: the marginal probability of generating a terminal (atomic) category in each expansion. A higher pterm means a stronger emphasis on simplicity. The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). The second idea is a natural extension of the complexity concept and is particularly relevant when features are used. The original complexity measure treated all atoms uniformly, but e.g. we would expect NPexpl / N to be less likely than NP / N since it contains the more specialized, and thus rarer, atom NPexpl . We define the distribution patom (a) as the prior over atomic categories. Due to our weak, type-only supervision, we have to estimate patom from just the tag dictionary and raw corpus, without frequency data. Our goal is to estimate the number of each atom in the supertags that shou"
W14-1615,P11-1061,0,0.050044,"Missing"
W14-1615,D12-1075,1,0.875226,"es using FFBS. To sample a tagging for a sentence x, the strategy is to inductively compute, for each token xi starting with i = 0 and going “forward”, the probability of generating x0 , x1 , . . . , xi via any tag sequence that ends with yi = u: Emission Prior Means (φ0t ) For each supertag type t, φ0t is the mean distribution over words it emits. While Baldridge’s approach used a uniform emission initialization, treating all words as equally likely, we can, again, induce token-level corpus-specific information:5 To set φ0t , we use a variant and simplification of the procedure introduced by Garrette and Baldridge (2012) that takes advantage of our prior over categories PG . Assuming that C(w) is the count of word type w in the raw corpus, TD(w) is the set of supertags associated with word type w in the tag dictionary, and TD(t) is the set of known word types associated with supertag t, the count of word/tag pairs for known words (words appearing in the tag dictionary) is estimated by uniformly distributing a word’s (δ-smoothed) raw counts over its tag dictionary entries: ( C(w)+δ |TD (w) |if t ∈ TD (w) Cknown (t, w) = 0 otherwise For unknown words, we first use the idea of tag “openness” to estimate the like"
W14-1615,N13-1014,1,0.879438,"Missing"
W14-1615,P13-1057,1,0.870138,"Missing"
W14-1615,P08-1085,0,0.018326,"s work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for C"
W14-1615,C08-1008,1,0.200491,". Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge. Despite the apparent complexity of the task, supertag sequences have regularities due to universal properties of the CCG formalism (§2) that can be used to reduce the complexity of the problem; previous work showed promising results by using these regularities to initialize an HMM that is then refined with EM (Baldridge, 2008). Here, we exploit CCG’s category structure to motivate a novel prior over HMM parameters for use in Bayesian learning (§3). This prior encourages (i) crosslinguistically common tag types, (ii) tag bigrams that can combine using CCG’s combinators, and (iii) sparse transition distributions. We also go beyond the use of these universals to show how additional, corpus-specific information can be automatically extracted from a combination of the tag dictionary and raw data, and how that information can be combined with the universal knowledge for integration into the model to improve the prior. We"
W14-1615,P07-1094,0,0.241725,"that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical t"
W14-1615,J99-2004,0,0.0338277,"ase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between the two operands. For example, the category ( SNP )/ NP might describe a transitive verb, looking first to its right (indicated by /) for an object, then to its left () for a subject, to produce a sentence. Further, atomic categories may be augmented with features, such as Sdcl , to restrict the set of atoms with which they may unify. The task of assigning a category to each word in a text is called supertagging (Bangalore and Joshi, 1999). Because they are recursively defined, there is an infinite number of potential CCG categories (though in practice it is limited by the number of actual grammatical contexts). As a result, the number of supertags appearing in a corpus far exceeds the number of POS tags (see Table 1). Since supertags specify the grammatical context of a token, and high frequency words appear in many contexts, CCG grammars tend to have very high lexical ambiguity, with frequent word types associating with a large number of categories. This ambiguity has made type-supervised supertagger learning very difficult b"
W14-1615,J07-3004,0,0.183225,"5k 60k 9k ambiguity type token 3.75 13.11 56.98 296.18 96.58 323.37 178.88 426.13 dev tokens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were"
W14-1615,N10-1083,0,0.0462617,"Missing"
W14-1615,N09-1036,0,0.0282852,"ld be automatically extracted from the weak supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (20"
W14-1615,Q13-1007,0,0.0671236,"ere, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learning (Van Gael et al., 2008; Beal et al., 2001). While their model is not restricted to the standard set of POS tags, and may learn a more fine-grained set of labels, the induced labels are arbitrary and not grounded in any grammatical formalism. Bisk and Hockenmaier (2013) developed an approach to CCG grammar induction that does not use a tag dictionary. Like ours, their procedure learns from general properties of the CCG formalism. However, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms 148 In this work, as in most type-supervised work, the tag dictionary was automatically extracted from an existing tagged corpus. However, a tag dictionary could instead be automatically induced vi"
W14-1615,D07-1031,0,0.022783,"fic information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar fo"
W14-1615,P11-1087,0,0.0131501,"y from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical token is associated with"
W14-1615,D10-1083,0,0.016251,"show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Ba"
W14-1615,J93-2004,0,0.0456766,"ens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for developme"
W14-1615,bosco-etal-2000-building,0,0.500961,"the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132sentence JRC ACQUIS data, was used for the tag dictionary."
W14-1615,J94-2001,0,0.352076,"of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computation"
W14-1615,P09-1057,0,0.0422068,"Missing"
W14-1615,P10-1051,1,0.81725,"supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infini"
W14-1615,P05-1044,1,0.751187,"e obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000"
W14-1615,C10-1122,0,0.194953,"k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were u"
W14-1615,D09-1071,0,0.0533631,"Missing"
W14-1615,W12-3127,0,0.0384305,"Missing"
W16-1721,W15-1609,0,0.015827,"for the people who inhabit an area (e.g. Americans)— are properties that must be considered. All existing TR corpora include metonymic uses of place names. The Local Global Lexicon (LGL) corpus (Lieberman and Samet, 2012) includes demonyms as toponyms and georeferences them, while all other corpora do not. An additional issue pertains to the range of entity types a system is expected to resolve. Many corpora limit their expectations to larger entities—e.g. TR-CoNLL (Leidner, 2008) is limited to cities, states, and countries), while others focus more on highly local entities (e.g. bus stops) (Matsuda et al., 2015). A final issue relates to whether systems ought to resolve places which are embedded inside other named entities. For example, the LGL corpus expects New York in the expression New York Times to be resolved to the state of New York. Many of the characteristics of existing TR corpora are summarized in Table 3. Table 2: Statistics on WOTR, annotated subset (using documents predicted based on a sequence model derived from the annotated data, as described in §3). trated in a number of areas that saw heavy fighting, such as in Virginia, South Carolina and Northern Georgia. The toponym annotations"
W16-1721,D12-1137,1,0.865477,"of toponyms that described the locations of ferries, bridges, railroads, and mills. These features usually no longer exist, so discovering their exact reference even with access to Google is very difficult. 5 Baseline and benchmark system evaluation In order to gain an understanding of the difficulties of the corpus and encourage its adoption, we evaluate the performance of a number of baseline and benchmark systems on the dataset. For docgeo, two methods are used for constructing grid cells: Uniform and adaptive (KD), which adjusts cell sizes to equalize the number of documents in each cell (Roller et al., 2012). LR uses flat logistic regression while Hier constructs a coarse-to-fine hierarchy of grids with a beam search (Wing and Baldridge, 2014)4 . For TR, Population selects a matching gazetteer referent with the highest population. WISTR is a bag of words multinomial logistic regression model trained on Wikipedia (Speriosu and Baldridge, 2013). SPIDER is a weighted distance minimization approach that prefers selecting gazetteer referents that occupy minimal area (Speriosu and Baldridge, 2013). TopoCluster uses a geographic density estimation of the toponym and context words; TopoClusterGaz5 additi"
W16-1721,D09-1015,0,0.112714,"Missing"
W16-1721,P05-1045,0,0.010195,"hmond, VA, CSA. In these cases each toponym is annotated with separate reference. To find the reference of places, annotators were allowed access to Internet search. As with document geolocation, annotators were encouraged to look up Toponym annotation To begin the toponym annotation procedure, we identified a subset of the volumes which had been annotated with document geolocations (subsections of 15 volumes, selected in part for geographic and topic diversity). Stanford’s Named Entity Recognizer (NER) was then run on the collection of documents, using the standard MUC, CoNLL trained models (Finkel et al., 2005). The place annotations that Stanford NER produced were used as a pre-annotated set, which annotators were then asked to correct and add geographic reference to. The toponym annotation process, which spanned 4 months and occupied 290 hours, resulted in the annotation of 11,795 toponyms (10,389 with geometries) spanning 1,644 annotated documents across 100 page subsections of 15 volumes. Originally all toponym annotations were done by a single annotator. After this process all of the original annotations were reviewed by a second team of three annotators. These annotators were asked to correct"
W16-1721,P13-1144,1,0.865803,"ms, while gazetteers such as GeoNames list over 8 million unique places (which still greatly underestimates the true number of toponyms). Such mismatches do more than underscore the need for larger and more domain-diverse corpora; they point to fundamental issues associated with learning to resolve geographies from language. Geographic entities, like all named entities, are fiat objects; naming them dictates their existence (Kripke, 1980). Many systems have attempted to alleviate paucity problems by splicing corpora with latent annotations inferred from a more general resource like Wikipedia (Speriosu and Baldridge, 2013; Santos et al., 2014; DeLozier et al., 2015). 1. Remove page breaks and stitch up paragraphs divided across the breaks. 2. Create a GUI annotation tool to allow annotators to quickly note the extent of documents (which we term spans) and indicate the document locations on a map. 3. Create a sequence model to automatically split up the continuous text into documents, training it on the documents manually marked up by the annotators. Stitching up page breaks As mentioned above, the source text is in the form of individual pages scanned from the published books, with page breaks, footnotes, stra"
W16-1721,D14-1039,1,0.853894,"ering their exact reference even with access to Google is very difficult. 5 Baseline and benchmark system evaluation In order to gain an understanding of the difficulties of the corpus and encourage its adoption, we evaluate the performance of a number of baseline and benchmark systems on the dataset. For docgeo, two methods are used for constructing grid cells: Uniform and adaptive (KD), which adjusts cell sizes to equalize the number of documents in each cell (Roller et al., 2012). LR uses flat logistic regression while Hier constructs a coarse-to-fine hierarchy of grids with a beam search (Wing and Baldridge, 2014)4 . For TR, Population selects a matching gazetteer referent with the highest population. WISTR is a bag of words multinomial logistic regression model trained on Wikipedia (Speriosu and Baldridge, 2013). SPIDER is a weighted distance minimization approach that prefers selecting gazetteer referents that occupy minimal area (Speriosu and Baldridge, 2013). TopoCluster uses a geographic density estimation of the toponym and context words; TopoClusterGaz5 additionally ’snaps’ to the nearest gazetteer referent (DeLozier et al., 2015). All TR systems were Rivers, and physical features are difficult"
W18-1406,Q13-1005,0,0.0222811,"otation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by composing diverse crowds from v"
W18-1406,W16-1721,1,0.844969,"d image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation tasks. Paths Understanding salient features and spatial relations in images and text naturally extends into navigation tasks that connect such points. To avoid"
W18-1406,N18-1177,0,0.108584,"have access to a God’s eye view, like that available to mapping applications (with access to full geographic features via databases). Instead, such tasks must be solvable by interpreting visual and textual stimuli relevant to the locations. This should put a greater emphasis on challenging spatial descriptions and relationships rather than known and named routes. Nonetheless, maps as visual artifacts (e.g., PDFs) may be incorporated in some cases, giving automated agents the ability to use them as a hiker might use a paper map without access to a GPS-based mapping application. Mirowski et al. (2018) is a recent example that takes a first-person perspective in a real world simulation, though one that does not incorporate language. They learn a model for navigating the Google Street View graph via reinforcement learning, where the goal location is specified via its distance to several other landmark locations and no explicit maps are used. Two especially interesting aspects of their approach are their use of curriculum learning (start with nearby goals and then tackle more distant ones) and showing successful adaptation from one city to another. These ideas are complementary to those that"
W18-1406,L18-1254,1,0.897777,"h spatially relevant points and paths—on the scale of at least hundreds of thousands. Multilinguality For both theoretical and practical reasons, we cannot focus on just one language. Different languages have different spatial relations, often involving the three different frames of reference—relative, intrinsic, and absolute (Levinson, 2003)—in different ways. Navigational systems supporting vague reference off the grid are needed even more in locations where English and other majority languages are not spoken. One way we already target multilinguality is via community-driven crowd-sourcing (Funk et al., 2018). In our approach, we intentionally cycle our iterations throughout the world and we involve developers from each locale because they have insights into how the local context affects how language is used and how the task is performed. User-driven annotation We seek to complement previous efforts that have focused on finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evalua"
W18-1406,W16-3202,0,0.0497127,"Missing"
W18-1406,Q18-1004,0,0.0227929,"Missing"
W18-1406,D14-1086,0,0.0338832,"nguage artifacts provides a natural and mutually reinforcing progression from points to paths to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities fo"
W18-1406,D17-1015,0,0.0208315,"to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague region"
W18-1406,P17-2033,0,0.0135746,"ounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation task"
W18-1406,P10-1083,0,0.0402714,"finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by c"
W18-1406,D14-1039,1,0.842799,"rlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real"
W18-1406,D17-1106,0,0.0455013,"Missing"
W19-1605,P13-1006,0,0.0809946,"Missing"
W19-1605,D14-1162,0,0.0809501,"Missing"
Y09-2043,C08-1008,1,0.846692,"on Baldridge 23rd Pacific Asia Conference on Language, Information and Computation, pages 795–802 795 FHMMs to supertagging for the categories defined in CCGbank for English. Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007). Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging (Goldwater and Griffiths, 2007) and supertagging (Baldridge, 2008) separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains. Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce. Secondly, when training data is limited, the generative FHMMs and a maximum entropy Markov model (a discriminative model like C&C) can bootstrap each other, in a single round co-training setup, to complement each other. Finally,"
Y09-2043,J99-2004,0,0.0229215,"cessing, modeling dependencies between individual predictions can be used to improve prediction accuracy of the sequence as a whole. For example, chunking involves identifying sequences of words in a sentence that are part of syntactically related non-overlapping, non-recursive phrases. An effective representation for this task involves assigning an individual part-of-speech (POS) tag and chunk tag to each word and deriving the actual chunks from these word specific labels. In these sequences, many of the POS and chunk tags are correlated, so joint inference can be quite useful. Supertagging (Bangalore and Joshi, 1999), involves assigning lexical entries to words based on lexicalized grammatical theory such as Combinatory Categorial Grammar (CCG) (Steedman, 2000; Steedman and Baldridge, 2009). For example, the English verb join has the POS VB and the CCG category ((Sb NP)/PP)/NP in CCGbank (Hockenmaier and Steedman, 2007). This category indicates that join requires a noun phrase to its left, another to its right, and a prepositional phrase to the right of that. Every lexical item has as many supertags as the number of different syntactic contexts in which the item can appear, so supertags are far more deta"
Y09-2043,A00-1031,0,0.0295184,"s equal. 4.2 Single Round Co-Training Experiments In this section, we use the FHMMB model and the C&C model in a single round of co-training. The idea behind co-training (Blum and Mitchell, 1998) is that when two models learn different views of the task, and are not correlated in the errors they make, they may compliment each other 798 to boost each other’s performance. Thus, provided with a large amount of unannotated data, we could use co-training iteratively to enhance the prediction performance of the two models. For example, Clark et al. (2003) co-train the C&C tagger and the TNT tagger (Brants, 2000) and obtain significant performance improvements for POS tagging. Here, we do not perform co-training, strictly speaking. Instead, we complete a single round in which one model is trained on a small number of sentences and then is used to label all remaining unannotated examples; the entire set is then used by the other model for training. Table 3(a) shows the results of bootstrapping the C&C supertagger with the FHMMB model trained on 25, 50 and 100 annotated sentences respectively. For comparison, the standalone performance of C&C is also shown. The FHMMB model was used to annotate the remai"
Y09-2043,J07-4004,0,0.448708,"edman, 2000; Steedman and Baldridge, 2009). For example, the English verb join has the POS VB and the CCG category ((Sb NP)/PP)/NP in CCGbank (Hockenmaier and Steedman, 2007). This category indicates that join requires a noun phrase to its left, another to its right, and a prepositional phrase to the right of that. Every lexical item has as many supertags as the number of different syntactic contexts in which the item can appear, so supertags are far more detailed and numerous than POS tags. Recently there is increased interest on supertagging beyond their standard use as a pre-parsing step (Clark and Curran, 2007)—for example, they are being used as features in machine translation (Birch et al., 2007; Hassan et al., 2007). Chunking and supertagging can be modeled using a two-stage cascade of Hidden Markov Models (HMMs) (Rabiner, 1989). POS tags are first predicted from the observed words in the first stage; then the chunk tags or supertags are predicted from those POS tags in the next stage. Alternatively, both sequences can be jointly predicted with Factorial Hidden Markov Models (FHMMs) (Ghahramani and Jordan, 1998), thereby preventing propagation of errors. Here, we apply ? We would like to thank Sh"
Y09-2043,W03-0407,0,0.0617015,"pertag prediction while holding all other modeling considerations equal. 4.2 Single Round Co-Training Experiments In this section, we use the FHMMB model and the C&C model in a single round of co-training. The idea behind co-training (Blum and Mitchell, 1998) is that when two models learn different views of the task, and are not correlated in the errors they make, they may compliment each other 798 to boost each other’s performance. Thus, provided with a large amount of unannotated data, we could use co-training iteratively to enhance the prediction performance of the two models. For example, Clark et al. (2003) co-train the C&C tagger and the TNT tagger (Brants, 2000) and obtain significant performance improvements for POS tagging. Here, we do not perform co-training, strictly speaking. Instead, we complete a single round in which one model is trained on a small number of sentences and then is used to label all remaining unannotated examples; the entire set is then used by the other model for training. Table 3(a) shows the results of bootstrapping the C&C supertagger with the FHMMB model trained on 25, 50 and 100 annotated sentences respectively. For comparison, the standalone performance of C&C is"
Y09-2043,D08-1036,0,0.0339666,"Missing"
Y09-2043,P07-1094,0,0.183358,"st. Copyright 2009 by Srivatsan Ramanujam and Jason Baldridge 23rd Pacific Asia Conference on Language, Information and Computation, pages 795–802 795 FHMMs to supertagging for the categories defined in CCGbank for English. Fully supervised maximum entropy Markov models have been used for cascaded prediction of POS tags followed by supertags (Clark and Curran, 2007). Here, we learn supertaggers given only a POS tag dictionary and supertag dictionary or a small amount of material labeled with both types of information. Previous work has used Bayesian HMMs to learn taggers for both POS tagging (Goldwater and Griffiths, 2007) and supertagging (Baldridge, 2008) separately. Modeling them jointly has the potential to produce more robust and accurate supertaggers trained with less supervision and thereby potentially help in the creation of useful models for new languages and domains. Our results show that joint inference improves supervised supertag prediction (compared to HMMs), especially when labeled training data is scarce. Secondly, when training data is limited, the generative FHMMs and a maximum entropy Markov model (a discriminative model like C&C) can bootstrap each other, in a single round co-training setup,"
Y09-2043,P07-1037,0,0.0349386,"Missing"
Y09-2043,N07-1018,0,0.033639,"d such a bi-tag HMM for supertagging. In the FHMMA model (Figure 2(b)), each POS tag generates the following POS tag and the current CCG supertag; each supertag in turn generates the following supertag and the current word. The FHMMB (Figure 2(c)) has a greater interlinking of POS and CCG tags in adjacent time slices: every POS tag and supertag is dependent on both the preceding POS tag and supertag, and both POS tags and supertags jointly generate the current word. (a) HMM (b) FHMMA (c) FHMMB Figure 2: The Models We use Bayesian inference for HMMs following Goldwater and Griffiths (2007) and Johnson et al. (2007), with symmetric Dirichlet priors for the transition and emission distributions of each of the three models. Such bitag HMMs can be formulated as: 0 ti |ti−1 = t, τ (t,t ) wi |ti = t, ω (t) 0 τ (t,t ) |α, ω (t) |β, ∼ ∼ ∼ ∼ M ult(t) M ult(ω (t)) ) Dirichlet(α) Dirichlet(β) Here, ti and wi refer to the i’th tag and word and τ refers to the state transition distribution and ω refers to the word emission distribution. The Forward-Backward algorithm standardly used for HMMs is intractable for FHMMMs due to their larger state space. We thus use Gibbs sampling, a Markov Chain Monte Carlo method that"
Y09-2043,D07-1031,0,0.0170689,"each of the three models. Such bitag HMMs can be formulated as: 0 ti |ti−1 = t, τ (t,t ) wi |ti = t, ω (t) 0 τ (t,t ) |α, ω (t) |β, ∼ ∼ ∼ ∼ M ult(t) M ult(ω (t)) ) Dirichlet(α) Dirichlet(β) Here, ti and wi refer to the i’th tag and word and τ refers to the state transition distribution and ω refers to the word emission distribution. The Forward-Backward algorithm standardly used for HMMs is intractable for FHMMMs due to their larger state space. We thus use Gibbs sampling, a Markov Chain Monte Carlo method that is commonly used for inference in Bayesian graphical models (Besag, 2004; Gao and Johnson, 2007). The Gibbs sampling equations for a POS and CCG pair in each of the models are summarized in Figure 3. For the HMM model, the POS and CCG tags are sampled independently of each other. For the FHMMs, the interlinks between the POS and CCG nodes in the graphical model, determines the interdependency during the joint inference of the POS and CCG tag sequences. 4 Supervised Supertagging Experiments We consider two supervised training scenarios here. 797 (1) P (ti |M B(ti )) ∝ P (ti |ti−1 )P (ti+1 |ti )P (wi |ti ) (2) P (ci |M B(ci )) ∝ P (ci |ci−1 )P (ci+1 |ci )P (wi |ci ) (3) P (ti , ci |M B(ti"
Y09-2043,W00-0726,0,0.0748555,"Missing"
Y09-2043,J93-2004,0,\N,Missing
Y09-2043,J07-3004,0,\N,Missing
Y09-2043,W07-0702,0,\N,Missing
