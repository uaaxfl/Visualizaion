2020.acl-main.294,D18-1045,0,0.166186,"aging the GEC training data as the augmented parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn contextualized representations, sentence generation and source-target alignments better. When the augmented parallel 3 https://translate.google.com/ 3222 σ = 0.6 in our experiments. data is available, previous studies (Sennrich et al., 2016a; Edunov et al., 2018; Karakanta et al., 2018; Wang et al., 2018) for seq2seq tasks are inclined to train a seq2seq model with original training data and augmented data simultaneously. However, augmented data is usually noisier and less valuable than original training data. In simultaneous training, the massive augmented data tends to overwhelm the original data and introduce unnecessary and even erroneous editing knowledge, which is undesirable for our task. To better exploit the augmented data, we propose to first pre-train the model with augmented parallel data and then fine-tune the model with the original tra"
2020.acl-main.294,P17-2090,0,0.0227105,"rse augmented data with various formality style transfer knowledge. The augmented data can significantly help improve the performance when it is used for pre-training the model and leads to the state-of-the-art results in the formality style transfer benchmark dataset. Acknowledgements We thank all the reviewers for providing the constructive suggestions. This work is partly supported by Beijing Academy of Artificial Intelligence. Xu Sun is the corresponding author of this paper. Related Work Data augmentation has been much explored for seq2seq tasks like Machine Translation (He et al., 2016; Fadaee et al., 2017; Zhang et al., 2018b; PonReferences Albert C Baugh and Thomas Cable. 1993. A history of the English language. Routledge. 3225 Kyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078. Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proceedings of the eighth workshop on innovative use of NLP for building educational applications"
2020.acl-main.294,P18-1097,1,0.860365,"entation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the bes"
2020.acl-main.294,P19-1609,1,0.851924,"valuable rewriting knowledge that is not covered by the original parallel data. 2.1.3 Multi-task transfer In addition to back translation and formality discrimination that use artificially generated sentence pairs for data augmentation, we introduce multitask transfer that uses annotated sentence pairs from other seq2seq tasks. We observe that informal texts are usually ungrammatical while formal texts are almost grammatically correct. Therefore, a desirable FST model should possess the ability to detect and rewrite ungrammatical texts, which has been verified by the previous empirical study (Ge et al., 2019) showing that using a state-of-theart grammatical error correction (GEC) model to post-process the outputs of an FST model can improve the result. Inspired by this observation, we propose to transfer the knowledge from GEC to FST by leveraging the GEC training data as the augmented parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn conte"
2020.acl-main.294,W19-4427,0,0.0182589,"d augmented data introduces more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we pr"
2020.acl-main.294,W17-4902,0,0.0494237,"n based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation methods for formality style transfer. Our proposed data augmentation methods can effectively generate diverse augmented"
2020.acl-main.294,D19-1119,0,0.0144967,"ages, the Chinesebased augmented data introduces more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To s"
2020.acl-main.294,I11-1017,0,0.0654449,"Missing"
2020.acl-main.294,C18-1086,0,0.160822,"by GPT (Radford et al., E&M BLEU 50.28 60.37 66.88 58.27 67.51 71.29 72.01 72.70 69.86 72.63 74.24 F&R BLEU 51.67 66.40 72.40 68.26 73.78 74.51 75.33 77.26 76.32 77.01 77.97 Table 3: The comparison of our approach to the stateof-the-art results. * denotes the ensemble results. 2019). Specifically, GPT-CAT concatenates the original input sentence and the input sentence preprocessed by rules as input, while GPT-Ensemble is the ensemble of two GPT-based encoder-decoder models: one takes the original input sentence as input, the other takes the preprocssed sentence as input. Following Niu et al. (2018), we train 4 independent models with different initializations for ensemble decoding. According to Table 3, our single model performs comparably to the state-ofthe-art GPT-based encoder-decoder models (more than 200M parameters) with only 54M parameters. Our ensemble model further advances the state-ofthe-art result only with a comparable model size to the GPT-based single model (i.e., GPT-CAT). We also conduct human evaluation. Following Rao and Tetreault (2018), we assess the model output on three criteria: formality, fluency and meaning preservation. We compare our baseline model trained wi"
2020.acl-main.294,N18-1012,0,0.427327,"data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset1 . 1 F-Dis MT M-Task Introduction Formality style transfer (FST) is defined as the task of automatically transforming a piece of text in one particular formality style into another (Rao and Tetreault, 2018). For example, given an informal sentence, FST aims to preserve the styleindependent content and output a formal sentence. Previous work tends to leverage neural networks (Xu et al., 2019; Niu et al., 2018; Wang et al., 2019) such as seq2seq models to address this challenge due to their powerful capability and large improvement over the traditional rule-based approaches (Rao and Tetreault, 2018). However, the performance of the neural network approaches is still limited by the inadequacy of training data: the public parallel corpus for FST training – GYAFC (Rao and Tetreault, 2018) – contains"
2020.acl-main.294,W17-5032,0,0.0298358,"cause of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieve"
2020.acl-main.294,P16-1009,0,0.592342,"ne chance. Target I don't know ... Good luck. Source I think she like cat too. Target I think she likes cat too. Figure 1: An example that Formality Style Transfer (FST) benefits from data augmented via formality discrimination (F-Dis) and multi-task transfer (MTask). The mapping knowledge indicated by the color (blue→pink) in FST test instance occur in the pairs augmented by F-Dis and M-Task. F-Dis identifies useful sentence pairs from paraphrased sentence pairs generated by cross-lingual MT, while M-Task utilizes training data from GEC to help formality improvement. translation (BT) method (Sennrich et al., 2016a) in Machine Translation (MT) to FST, our data augmentation methods include formality discrimination (F-Dis) and multi-task transfer (M-Task). They are both novel and effective in generating parallel data that introduces additional formality transfer knowledge that cannot be derived from the original training data. Specifically, F-Dis identifies useful pairs from the paraphrased pairs generated by cross-lingual MT; while M-task leverages the training data of Grammatical Error Correction (GEC) task to improve formality, as shown in Figure 1. Experimental results show that our proposed data aug"
2020.acl-main.294,P16-1162,0,0.739721,"ne chance. Target I don't know ... Good luck. Source I think she like cat too. Target I think she likes cat too. Figure 1: An example that Formality Style Transfer (FST) benefits from data augmented via formality discrimination (F-Dis) and multi-task transfer (MTask). The mapping knowledge indicated by the color (blue→pink) in FST test instance occur in the pairs augmented by F-Dis and M-Task. F-Dis identifies useful sentence pairs from paraphrased sentence pairs generated by cross-lingual MT, while M-Task utilizes training data from GEC to help formality improvement. translation (BT) method (Sennrich et al., 2016a) in Machine Translation (MT) to FST, our data augmentation methods include formality discrimination (F-Dis) and multi-task transfer (M-Task). They are both novel and effective in generating parallel data that introduces additional formality transfer knowledge that cannot be derived from the original training data. Specifically, F-Dis identifies useful pairs from the paraphrased pairs generated by cross-lingual MT; while M-task leverages the training data of Grammatical Error Correction (GEC) task to improve formality, as shown in Figure 1. Experimental results show that our proposed data aug"
2020.acl-main.294,C12-1177,0,0.0594951,"vot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation methods for formality style transfer. Our proposed data augmentation methods can effectively generate diverse augmented data with various"
2020.acl-main.294,W16-0530,0,0.0653181,"Missing"
2020.acl-main.294,P12-2039,0,0.0701122,"Missing"
2020.acl-main.294,D18-1138,1,0.851243,"E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation m"
2020.acl-main.294,D18-1100,0,0.0205002,"parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn contextualized representations, sentence generation and source-target alignments better. When the augmented parallel 3 https://translate.google.com/ 3222 σ = 0.6 in our experiments. data is available, previous studies (Sennrich et al., 2016a; Edunov et al., 2018; Karakanta et al., 2018; Wang et al., 2018) for seq2seq tasks are inclined to train a seq2seq model with original training data and augmented data simultaneously. However, augmented data is usually noisier and less valuable than original training data. In simultaneous training, the massive augmented data tends to overwhelm the original data and introduce unnecessary and even erroneous editing knowledge, which is undesirable for our task. To better exploit the augmented data, we propose to first pre-train the model with augmented parallel data and then fine-tune the model with the original training data. In our pre-training & finetuning"
2020.acl-main.294,N19-1014,0,0.023737,"more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data au"
2020.acl-main.294,D19-1365,0,0.251114,"our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset1 . 1 F-Dis MT M-Task Introduction Formality style transfer (FST) is defined as the task of automatically transforming a piece of text in one particular formality style into another (Rao and Tetreault, 2018). For example, given an informal sentence, FST aims to preserve the styleindependent content and output a formal sentence. Previous work tends to leverage neural networks (Xu et al., 2019; Niu et al., 2018; Wang et al., 2019) such as seq2seq models to address this challenge due to their powerful capability and large improvement over the traditional rule-based approaches (Rao and Tetreault, 2018). However, the performance of the neural network approaches is still limited by the inadequacy of training data: the public parallel corpus for FST training – GYAFC (Rao and Tetreault, 2018) – contains only approximately 100K sentence pairs, which can hardly satiate the neural models with millions of parameters. To tackle the data sparsity problem for FST, we propose to augment parallel data with three specific data augment"
2020.acl-main.294,P19-1482,1,0.722464,"ur experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel"
2020.acl-main.294,N18-1057,0,0.0191216,"roblem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the"
2020.acl-main.294,W13-1703,0,\N,Missing
2020.acl-main.545,N06-1003,0,0.40932,"Missing"
2020.acl-main.545,P16-1073,0,0.0244611,"Missing"
2020.acl-main.545,D17-1091,0,0.0155716,"n et al., 2016), or by employing some neural generation methods (Prakash et al., 2016; Li et al., 2019b). In this paper, we employ a simple and effective paraphrasing method to expand both input sentences and reference questions. Our method also can be replaced with more sophisticated paraphrasing methods. Paraphrase knowledge has been used to improve many NLP tasks, such as machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013) and Dong et al. (2017) employ paraphrase knowledge to enhance question answering models. Kriz et al. (2018) utilize paraphrase and context-based lexical substitution knowledge to improve simplification task. Similarly, Zhao et al. (2018a) combine paraphrase rules of PPDB (Ganitkevitch et al., 2013) with Transformer (Vaswani et al., 2017) to perform sentence simplification task. Guo et al. (2018a) propose a multi-task learning framework with PG and simplification. In addition, Yu et al. (2018) and Xie et al. (2019) use paraphrase as data argumentation for their primary tasks. Different from these works, we leverage"
2020.acl-main.545,P18-1177,0,0.0354529,"Missing"
2020.acl-main.545,P17-1123,0,0.478593,"n materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts (e.g. the input sentence and reference question, the reference question and generated question) may convey the same meaning but use different words, ph"
2020.acl-main.545,D17-1090,0,0.0675073,"ch focuses on generating grammatical questions for given paragraphs or sentences. It plays a vital role in various realistic scenarios. For educational purposes, QG can create reading comprehension materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation"
2020.acl-main.545,P13-1158,0,0.031377,"n different passage graphs to capture structure information of passage through graph neural networks. Dong et al. (2019) propose a unified language model pre-training method to obtain better context representations for QG. All these works adopt a whole paragraph as input to generate questions. Different from this, our work only takes a sentence as input and leaves paragraph-level QG for future research. Paraphrase generation is also a challenging task for NLP. Recent works usually obtain paraphrases by reordering or modifying the syntax or lexicon based on some paraphrase databases and rules (Fader et al., 2013; Chen et al., 2016), or by employing some neural generation methods (Prakash et al., 2016; Li et al., 2019b). In this paper, we employ a simple and effective paraphrasing method to expand both input sentences and reference questions. Our method also can be replaced with more sophisticated paraphrasing methods. Paraphrase knowledge has been used to improve many NLP tasks, such as machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013)"
2020.acl-main.545,N13-1092,0,0.182161,"Missing"
2020.acl-main.545,C18-1039,0,0.465316,"machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013) and Dong et al. (2017) employ paraphrase knowledge to enhance question answering models. Kriz et al. (2018) utilize paraphrase and context-based lexical substitution knowledge to improve simplification task. Similarly, Zhao et al. (2018a) combine paraphrase rules of PPDB (Ganitkevitch et al., 2013) with Transformer (Vaswani et al., 2017) to perform sentence simplification task. Guo et al. (2018a) propose a multi-task learning framework with PG and simplification. In addition, Yu et al. (2018) and Xie et al. (2019) use paraphrase as data argumentation for their primary tasks. Different from these works, we leverage paraphrase knowledge for question generation, by automatically constructing a built-in paraphrase corpus without using any external paraphrase knowledge bases. 3 Model Description In this section, we first describe two baseline models we used: feature-enriched pointer-generator and language modeling enhanced QG. Then we explain how to obtain paraphrase resources and show t"
2020.acl-main.545,P18-1064,0,0.318034,"machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013) and Dong et al. (2017) employ paraphrase knowledge to enhance question answering models. Kriz et al. (2018) utilize paraphrase and context-based lexical substitution knowledge to improve simplification task. Similarly, Zhao et al. (2018a) combine paraphrase rules of PPDB (Ganitkevitch et al., 2013) with Transformer (Vaswani et al., 2017) to perform sentence simplification task. Guo et al. (2018a) propose a multi-task learning framework with PG and simplification. In addition, Yu et al. (2018) and Xie et al. (2019) use paraphrase as data argumentation for their primary tasks. Different from these works, we leverage paraphrase knowledge for question generation, by automatically constructing a built-in paraphrase corpus without using any external paraphrase knowledge bases. 3 Model Description In this section, we first describe two baseline models we used: feature-enriched pointer-generator and language modeling enhanced QG. Then we explain how to obtain paraphrase resources and show t"
2020.acl-main.545,N10-1086,0,0.235229,"examples of generated questions from SQuAD. We highlight the paraphrase transitions between sentences and questions. Human creates good questions by leveraging paraphrase knowledge, while the automatically generated questions just copy the original sentence, resulting in lower evaluation scores. Introduction Question generation (QG) is an essential task for NLP, which focuses on generating grammatical questions for given paragraphs or sentences. It plays a vital role in various realistic scenarios. For educational purposes, QG can create reading comprehension materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim e"
2020.acl-main.545,D18-1525,0,0.0282141,"s function among several references, and the loss function defined by Equation 3 can be rewritten as: Lqg Tqg 1 X logP (ytqg = qt )) = min(− q∈Q Tqg (9) t=1 where Q is the set of gold reference question and expanded question paraphrase {q, q 0 }. Each generated question will separately calculate the negative log-likelihood of its multiple references, and the final loss is the minimum of them. Under this training process, our model can learn multiple question expressions which are not in the original training dataset, so that the generation can be more diverse. Besides, inspired by the work of Kovaleva et al. (2018), we have tried several loss strategies, such as minimum loss, maximum loss, and weighted loss to guide the diversity training. Among them, the minimum is the best performing strategy. By employing minimum strategy, the QG decoder fits the generated question with the most similar sequence among gold reference question and question para3.5 Hybrid Model Combining the above modules, we get our hybrid model. During training, the feature-enriched inputs are first encoded by the task-share encoder. Then the semantic hidden states are fed into PG decoder and QG decoder, respectively. For PG decoder,"
2020.acl-main.545,N18-1019,0,0.0236067,"; Li et al., 2019b). In this paper, we employ a simple and effective paraphrasing method to expand both input sentences and reference questions. Our method also can be replaced with more sophisticated paraphrasing methods. Paraphrase knowledge has been used to improve many NLP tasks, such as machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013) and Dong et al. (2017) employ paraphrase knowledge to enhance question answering models. Kriz et al. (2018) utilize paraphrase and context-based lexical substitution knowledge to improve simplification task. Similarly, Zhao et al. (2018a) combine paraphrase rules of PPDB (Ganitkevitch et al., 2013) with Transformer (Vaswani et al., 2017) to perform sentence simplification task. Guo et al. (2018a) propose a multi-task learning framework with PG and simplification. In addition, Yu et al. (2018) and Xie et al. (2019) use paraphrase as data argumentation for their primary tasks. Different from these works, we leverage paraphrase knowledge for question generation, by automatically constructing a built-i"
2020.acl-main.545,D19-1317,0,0.713241,"ploy language modeling (LM) as an auxiliary task to enrich the encoder representations. In this paper, we adopt this work as one of the baseline models, since their universal model is easy to implement and achieves promising results for QG. In order to make use of the context information of paragraphs, Zhao et al. (2018b) propose a gated self-attention network to encode context passage. Based on this, Zhang and Bansal (2019) apply reinforcement learning to deal with semantic drift in QG; Nema et al. (2019) use a passage-answer fusion mechanism to obtain answer-focused context representations; Li et al. (2019a) utilize gated attention to fuse answer-relevant relation with context sentence. Besides, Chen et al. (2019) design different passage graphs to capture structure information of passage through graph neural networks. Dong et al. (2019) propose a unified language model pre-training method to obtain better context representations for QG. All these works adopt a whole paragraph as input to generate questions. Different from this, our work only takes a sentence as input and leaves paragraph-level QG for future research. Paraphrase generation is also a challenging task for NLP. Recent works usuall"
2020.acl-main.545,N16-1014,0,0.0365793,"he input sentence and q0 denotes the paraphrase of the golden reference. Under this setting, we double the training samples. Unfortunately, as shown in Table 4, the baseline-1 model yields much lower BLEU4 scores on both Zhou Split (13.28) and Du Split (13.36) with such data augmentation. The main reason is that for the same input sentence, there are two different training targets (q and q0 ), making the training process cannot easily converge. 5.2 Diversity Test To investigate whether the paraphrase knowledge introduces more diverse expressions, we conduct evaluations on the distinct metric (Li et al., 2016), which is calculated as the number of distinct unigrams (distinct-1) and bigrams (distinct-2) divided by the total number of the generated words. The experimental results are shown in Table 6. It shows that our hybrid models obtain obvious gains over baseline models on both distinct-1 and distinct-2 6136 metrics, validating that our models really generate more diverse questions with the help of paraphrase knowledge. Models baseline-1 hybrid model-1 baseline-2 hybrid model-2 distinct-1 9.49 9.75 9.81 9.98 distinct-2 39.48 41.97 41.14 42.43 Table 6: Results of the distinct metric on zhou split."
2020.acl-main.545,P19-1332,0,0.237267,"ploy language modeling (LM) as an auxiliary task to enrich the encoder representations. In this paper, we adopt this work as one of the baseline models, since their universal model is easy to implement and achieves promising results for QG. In order to make use of the context information of paragraphs, Zhao et al. (2018b) propose a gated self-attention network to encode context passage. Based on this, Zhang and Bansal (2019) apply reinforcement learning to deal with semantic drift in QG; Nema et al. (2019) use a passage-answer fusion mechanism to obtain answer-focused context representations; Li et al. (2019a) utilize gated attention to fuse answer-relevant relation with context sentence. Besides, Chen et al. (2019) design different passage graphs to capture structure information of passage through graph neural networks. Dong et al. (2019) propose a unified language model pre-training method to obtain better context representations for QG. All these works adopt a whole paragraph as input to generate questions. Different from this, our work only takes a sentence as input and leaves paragraph-level QG for future research. Paraphrase generation is also a challenging task for NLP. Recent works usuall"
2020.acl-main.545,P16-1170,0,0.0873494,"s by leveraging paraphrase knowledge, while the automatically generated questions just copy the original sentence, resulting in lower evaluation scores. Introduction Question generation (QG) is an essential task for NLP, which focuses on generating grammatical questions for given paragraphs or sentences. It plays a vital role in various realistic scenarios. For educational purposes, QG can create reading comprehension materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mecha"
2020.acl-main.545,D19-1326,0,0.0969913,"Missing"
2020.acl-main.545,P02-1040,0,0.109478,"uts. ASs2s (Kim et al., 2018): proposes an answerseparated Seq2Seq model by replacing the answer in the input sequence with some specific words. LM enhanced QG (Zhou et al., 2019a): treats language modeling as a low-level task to provide semantic representations for the high-level QG. Q-type (Zhou et al., 2019b): multi-task learning framework with question word prediction and QG. Sent-Relation (Li et al., 2019a): extracts answer-relevant relations in sentence and encodes both sentence and relations to capture answerfocused representations. We evaluate the performance of our models using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), which are widely used in previous works for QG. 4.3 Implementation Details We set the vocabulary as the most frequent 20,000 words. We use 300-dimensional GloVe word vectors as initialization of the word embeddings. Answer position and token lexical features are randomly initialized to 32-dimensional vectors through truncated normal distribution. The maximum lengths of input sequence and output sequence are 100 and 40, respectively. The hidden 6135 Previous Works s2s(Du et al., 2017) s2sa-at-mp-gsa(Zhao et al., 2018b) A-P-Hybrid(Sun et al., 2018) LM enh"
2020.acl-main.545,C16-1275,0,0.0210104,"ral networks. Dong et al. (2019) propose a unified language model pre-training method to obtain better context representations for QG. All these works adopt a whole paragraph as input to generate questions. Different from this, our work only takes a sentence as input and leaves paragraph-level QG for future research. Paraphrase generation is also a challenging task for NLP. Recent works usually obtain paraphrases by reordering or modifying the syntax or lexicon based on some paraphrase databases and rules (Fader et al., 2013; Chen et al., 2016), or by employing some neural generation methods (Prakash et al., 2016; Li et al., 2019b). In this paper, we employ a simple and effective paraphrasing method to expand both input sentences and reference questions. Our method also can be replaced with more sophisticated paraphrasing methods. Paraphrase knowledge has been used to improve many NLP tasks, such as machine translation, ques6131 tion answering, and text simplification. CallisonBurch et al. (2006) use paraphrase techniques to deal with unknown phrases to improve statistical machine translation. Fader et al. (2013) and Dong et al. (2017) employ paraphrase knowledge to enhance question answering models."
2020.acl-main.545,D16-1264,0,0.147296,"Missing"
2020.acl-main.545,P17-1099,0,0.374947,"les of utilizing paraphrase knowledge: the PG auxiliary task and the min loss function, as well as their combination. The overall structure of our hybrid model is shown in Figure 2. 3.1 probability distribution is the combination of these two modes with a generation probability pg : P (w) = pg Pvocab + (1 − pg )Pcopy The training objective is to minimize the negative log likelihood of the target sequence q: Lqg ei = [wi ; ai ; ni ; pi ; ui ] (1) where wi , ai , ni , pi , ui respectively represents embeddings of word, answer position, name entity, POS and word case. Same as the decoder used by See et al. (2017), another unidirectional LSTM with attention mechanism is used to obtain the decoder hidden state st and context vector ct . Based on these, the pointergenerator model will simultaneously calculate the probabilities of generating a word from vocabulary and copying a word from the source text. The final Tqg 1 X =− logP (ytqg = qt ) Tqg (3) t=1 3.1.2 Language Modeling Enhanced QG Zhou et al. (2019a) enhance QG with language modeling under a hierarchical structure of multitask learning. The language modeling aims at predicting the next and previous words in the input sequence with forward and bac"
2020.acl-main.545,N18-2090,0,0.462565,"tion with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts (e.g. the input sentence and reference question, the reference question and generated question) may convey the same meaning but use different words, phrases or syntactic patterns. In real communica6130 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6130–6140 c July 5"
2020.acl-main.545,D18-1427,0,0.29495,"ffective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts (e.g. the input sentence and reference question, the reference question and generated question) may convey the same meaning but use different words, phrases or syntactic patterns. In real communica6130 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages"
2020.acl-main.545,D19-1253,0,0.312195,"questions for given paragraphs or sentences. It plays a vital role in various realistic scenarios. For educational purposes, QG can create reading comprehension materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts"
2020.acl-main.545,D18-1355,0,0.376744,"tion patterns. We conduct extensive experiments on SQuAD and MARCO (Nguyen et al., 2016). Results show that both separate modules, the PG auxiliary task and the min-loss function, obviously improve the performances of QG task, and combing them achieves further improvements. Furthermore, human evaluation results show that our hybrid model can ask better and more human-like questions by incorporating paraphrase knowledge. Related Work For current mainstream neural network-based methods on QG, most approaches utilize the Seq2Seq model with attention mechanism (Du et al., 2017; Zhou et al., 2017; Zhao et al., 2018b; Zhou et al., 2019a). To obtain better representations of the input sequence and answer, the answer position and token lexical features are treated as supplements for the neural encoder (Zhou et al., 2017; Song et al., 2018; Kim et al., 2018). Similar to other text generation tasks, many works on QG also employ copy or pointer mechanism to overcome the OOV problem (Du and Cardie, 2018; Sun et al., 2018; Zhang and Bansal, 2019). Recently, Zhou et al. (2019a) employ language modeling (LM) as an auxiliary task to enrich the encoder representations. In this paper, we adopt this work as one of th"
2020.acl-main.545,D18-1424,0,0.48921,"tion patterns. We conduct extensive experiments on SQuAD and MARCO (Nguyen et al., 2016). Results show that both separate modules, the PG auxiliary task and the min-loss function, obviously improve the performances of QG task, and combing them achieves further improvements. Furthermore, human evaluation results show that our hybrid model can ask better and more human-like questions by incorporating paraphrase knowledge. Related Work For current mainstream neural network-based methods on QG, most approaches utilize the Seq2Seq model with attention mechanism (Du et al., 2017; Zhou et al., 2017; Zhao et al., 2018b; Zhou et al., 2019a). To obtain better representations of the input sequence and answer, the answer position and token lexical features are treated as supplements for the neural encoder (Zhou et al., 2017; Song et al., 2018; Kim et al., 2018). Similar to other text generation tasks, many works on QG also employ copy or pointer mechanism to overcome the OOV problem (Du and Cardie, 2018; Sun et al., 2018; Zhang and Bansal, 2019). Recently, Zhou et al. (2019a) employ language modeling (LM) as an auxiliary task to enrich the encoder representations. In this paper, we adopt this work as one of th"
2020.acl-main.545,D19-1337,1,0.870174,"s use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts (e.g. the input sentence and reference question, the reference question and generated question) may convey the same meaning but use different words, phrases or syntactic patterns. In real communica6130 Proceedings of the 58th"
2020.acl-main.545,D19-1622,1,0.681092,"s use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is, some parts of two texts (e.g. the input sentence and reference question, the reference question and generated question) may convey the same meaning but use different words, phrases or syntactic patterns. In real communica6130 Proceedings of the 58th"
2020.acl-main.545,D19-1172,1,0.678128,"ating grammatical questions for given paragraphs or sentences. It plays a vital role in various realistic scenarios. For educational purposes, QG can create reading comprehension materials for language learners (Heilman and Smith, 2010). For business use, QG can bring benefits to conversation systems and chat-bots for effective communication with humans (Mostafazadeh et al., 2016). Besides, automatically-generated questions can be conversely used for constructing question answering datasets to enhance reading comprehension sys∗ Corresponding author. tems (Tang et al., 2017; Duan et al., 2017; Xu et al., 2019; Zhang and Bansal, 2019). Recent neural network-based methods have achieved promising results on QG, most of which are based on the seq2seq attention framework (Du et al., 2017; Zhou et al., 2017; Gao et al., 2018; Kim et al., 2018; Zhou et al., 2019b), enriched with lexical features (Zhou et al., 2017; Sun et al., 2018; Song et al., 2018) or enhanced by copy mechanism (Du and Cardie, 2018; Sun et al., 2018; Zhou et al., 2019a). Although much progress has been made for QG, existing approaches do not explicitly model the “notorious” lexical and syntactic gaps in the generation process. That is"
2020.emnlp-main.534,D18-1547,0,0.0511967,"Missing"
2020.emnlp-main.534,P19-1567,0,0.154779,"imum likelihood estimation (Vinyals and Le, 2015; Shang et al., 2015). Different from other sequence generation tasks, such as machine translation and paraphrase generation, the dialogue generation task can be regarded as a loose-coupling task, which has much freedom in the semantic and the linguistic aspects of the generated responses. However, it is often hard for the existing models to handle such freedom, compared to the fact that humans have no problem in Could you cut the price a little, please? giving specific yet varied responses even for openended dialogue history (Shen et al., 2018; Csaky et al., 2019). One important reason is that we can extend the given dialogue with many possible scenarios of enriched, imaginative background information from our experience and world knowledge, to which existing systems have no access. It is beneficial for the dialogue systems to build upon such scenarios to facilitate dialogue generation. However, manually annotating the scenario contexts is intractable in terms of both difficulty and quantity. In turn, we find that such scenarios are naturally contained in existing multi-turn dialogue corpora, where the entire dialogue of both dialogue history and futur"
2020.emnlp-main.534,N19-1125,0,0.13164,"Missing"
2020.emnlp-main.534,P19-1125,1,0.827491,"al., 2018). Unlike the above models to pre6599 dict responses given a dialogue history, our method combines the future conversation with the dialogue history as the implicit conversation scenario, which contains comprehensive background information to guide the response generation. Imitation Learning Imitation learning, acquiring skills from observing demonstrations, has proven to be promising in structured prediction, such as alleviating the exposure bias problem (Bengio et al., 2015; Zhang et al., 2019b), transferring knowledge to guide non-autoregressive translation model (Gu et al., 2018; Wei et al., 2019), and automatically learning the reward of the dialogue system (Li et al., 2019b). In our work, the conventional dialogue model as a student mimics the scenariobased dialogue model on both the output layer and intermediate layers. 5 Conclusion In this work, we introduce the future conversation with the corresponding dialogue history to learn the implicit conversation scenario, which entails latent context knowledge and specifies how people interact in the real world. To incorporate such scenario knowledge without requiring future conversation in inference, we propose an imitation learning fram"
2020.findings-emnlp.25,N19-1423,0,0.174366,"dge Representation from Pretrained Language Models 1 Zhiyuan Zhang1 , Xiaoqian Liu1, 2 , Yi Zhang1 , Qi Su1, 2 , Xu Sun1 and Bin He3 MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University 2 School of Foreign Languages, Peking University 3 Huawei Noah’s Ark Lab {zzy1210,liuxiaoqian,zhangyi16,sukia,xusun}@pku.edu.cn hebin.nlp@huawei.com Abstract sparse and noisy dataset annotations. It leads to performance degradation, especially on the lowresource problem. To address this issue, we propose to enrich knowledge representation via pretrained language models (i.e., BERT (Devlin et al., 2019)) given a semantic description of entities and relations. We propose to incorporate world knowledge from BERT to the entity and the relation representation. Although simply fine-tuning BERT can enrich the knowledge representation, it suffers from learning inadequate structure information observed in training triplets, which we have demonstrated when we analyze the rationality of the KGE-training phase. Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we pr"
2020.findings-emnlp.25,P17-1021,0,0.02994,"ing world knowledge from pretrained models. Specifically, we present a universal training framework named PretrainKGE consisting of three phases: semanticbased fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed PretrainKGE can improve results over KGE models, especially on solving the low-resource problem. 1 Introduction Knowledge graphs (KGs) constitute an effective access to world knowledge for a wide variety of NLP tasks, such as entity linking (Luo et al., 2017), information retrieval (Xiong et al., 2017), question answering (Hao et al., 2017) and recommendation system (Zhang et al., 2016). A typical KG such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), consists of a set of triplets in the form of (h, r, t) with the head entity h and the tail entity t as nodes and relation r as edges in the graph. A triplet represents the relation between two entities, e.g., (Steve Jobs, founded, Apple Inc.). To learn effective representation of entities and relations in the graph, knowledge graph embedding (KGE) models are one of prominent approaches (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015; Sun et al., 2019; Nick"
2020.findings-emnlp.25,P15-1067,0,0.262513,"iong et al., 2017), question answering (Hao et al., 2017) and recommendation system (Zhang et al., 2016). A typical KG such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), consists of a set of triplets in the form of (h, r, t) with the head entity h and the tail entity t as nodes and relation r as edges in the graph. A triplet represents the relation between two entities, e.g., (Steve Jobs, founded, Apple Inc.). To learn effective representation of entities and relations in the graph, knowledge graph embedding (KGE) models are one of prominent approaches (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015; Sun et al., 2019; Nickel et al., 2011; Yang et al., 2015; Kazemi and Poole, 2018; Trouillon et al., 2016; Zhang et al., 2019). However, traditional KGE models often suffer from limited knowledge representation due to the We propose a model-agnostic training framework for learning knowledge graph embedding consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase (see Fig. 1). During the semantic-based fine-tuning phase, we learn knowledge representation via BERT given the semantic description of entities and relations as"
2020.findings-emnlp.25,N18-1202,0,0.0350897,"h entity and relation are provided as the semantic description of entities and relations. Recent works also leverage semantic description to enrich knowledge representation but ignore contextual information of the semantic description (Socher et al., 2013a; Li et al., 2016; Speer and Havasi, 2012; Xu et al., 2017; Xiao et al., 2017; Xie et al., 2016; An et al., 2018). Instead, our method exploits world information via pretrained models. Recent approaches to modeling language representations offer significant improvements over embeddings, such as pretrained deep contextualized language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019). KG-Bert (Yao et al., 2019) first utilizes BERT (Devlin et al., 2019) for knowledge graph completion, which treats triplets in knowledge graphs as textual sequences. However, KG-Bert does not extract knowledge representations from Bert and thus cannot provide entity or relation embeddings. In this work, we leverage world knowledge from BERT to learn better knowledge representation of entities and relations given semantic description. 3 3.1 Method Training Framework An overview of Pretrain-KGE is shown in Fig. 1. The framework co"
2021.acl-long.43,W19-4828,0,0.017913,"d are highly matched, the correlation of the items in the textual sequence should also agree with the correlation of the corresponding items in the visual sequence. But such constraint of relation consistency is neglected in previous works, which hinders performance and interpretability of the models. To corroborate this, we conduct a case study on Flickr30k Entities dataset (Plummer et al., 2015) to probe the agreement of relation-level semantics in pre-trained models like UNITER (Chen et al., 2020). We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019). As shown in Figure 1, the attention distributions grouped by the annotated object of the given text and image are in disagreement with each other. Specifically, the attention distribution in the linguistic modality is reasonable. However, in the visual modality, the region “a red shirt” pays inappropriate attention to the region of the dog that doesn’t appear in the text, which impairs the representation of this visual item, i.e., “a red shirt” under the condition of the corresponding text. Such mismatched attention distributions suggest that the mo"
2021.acl-long.43,D19-1445,0,0.0160928,"f the items in the textual sequence should also agree with the correlation of the corresponding items in the visual sequence. But such constraint of relation consistency is neglected in previous works, which hinders performance and interpretability of the models. To corroborate this, we conduct a case study on Flickr30k Entities dataset (Plummer et al., 2015) to probe the agreement of relation-level semantics in pre-trained models like UNITER (Chen et al., 2020). We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019). As shown in Figure 1, the attention distributions grouped by the annotated object of the given text and image are in disagreement with each other. Specifically, the attention distribution in the linguistic modality is reasonable. However, in the visual modality, the region “a red shirt” pays inappropriate attention to the region of the dog that doesn’t appear in the text, which impairs the representation of this visual item, i.e., “a red shirt” under the condition of the corresponding text. Such mismatched attention distributions suggest that the model represents the same concept with incons"
2021.acl-long.43,P16-1009,0,0.0312122,"self-attention matrix SLL can be extracted by the following operation:6 ali∗ →lj ∗ = Ext (SLL , li∗ , lj ∗ ) , i∗ = arg max SVL [i, :], (10) ∗ j = arg max SVL [j, :], as a singular alignment. After all the extractions, (s) we reconstruct a mirrored matrix SVV such that (s) SVV [i, j] = ali∗ →lj ∗ , which can be regarded as a 6 Compared with Section 2.2, the Ext operation here extracts a singular attention weight instead of a patch. 518 alignment leverages the language as a bridge to draw implicit connections within the visual modality, which can be intuitively regarded as the backtranslation (Sennrich et al., 2016) for multimodal. As shown in Figure 4, the distributed version of mirrored self-attention matrix can be constructed by a matrix multiplication of two inter-modal attention matrices: Algorithm 2: Singular Alignment Input: Intra-modal self-attention matrices SLL , SVV for i = 1 to NV do i∗ ← arg max SVL [i, :] for j = 1 to NV do j ∗ ← arg max SVL [j, :] (s) SVV [i, j] ← Ext (SLL , li∗ , lj ∗ ) for i = 1 to NL do i∗ ← arg max SLV [i, :] for j = 1 to NL do j ∗ ← arg max SLV [j, :] (s) SLL [i, j] ← Ext (SVV , vi∗ , vj ∗ )   (s) (s) LIAIS = m-KL σ(SVV ) , σ(SVV ) +   (s) m-KL σ(SLL ) , σ(SLL ) ("
2021.acl-long.43,D19-1514,0,0.0181768,"arities are defined, and recall the most relevant one according to the similarity. Multimodal Pre-trained Models The development of the transformer-based large-scale pretraining paradigm sweeps across the area of multimodal learning and achieves many state-of-the-art results on V+L tasks like Image Captioning, Visual Question Answering, Visual Commonsense Reasoning, etc. Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al., 2020; Tan and Bansal, 2019; Lu et al., 2019) models. Given a piece of text and an image, the former architecture concatenates the features of tokens and regions and learns their joint representations with one transformer model, while the latter embeds the textual and the visual input separately with two independent intra-modal transformers and then utilizes an 7 Conclusion In this paper, we promote the semantic alignment for cross-modal retrieval from the object level to the relation level. We propose a surrogate metric to quantify the relation consistency by measuring the semantic distance between linguistic and visua"
2021.acl-long.43,Q14-1006,0,0.0606376,"lignment, respectively. Here T is the total training steps during fine-tuning phase and t is the current step. As a pluggable regularizer, our IAIS method does NOT incorporate any extra parameters and additional data collection yet empowers the models to capture the higherlevel semantics of relation consistency efficiently. positive instances, the batch size is 512. The learning rate is 5e-5 and the training steps are 5000 for both base and large models. All experiments are run on 8 NVIDIA V100 GPUs. 4 5.1 5 Experimental Settings 4.1 Benchmark Datasets We conduct experiments on the Flickr30k (Young et al., 2014) and MS COCO (Lin et al., 2014) datasets. Flickr30K contains 31K images collected from the Flickr website, with five textual descriptions per image. We follow Karpathy and Li (2015) to split the data into 30K/1K/1K training/validation/test splits. MS COCO consists of 123K images, each accompanied with five humanwritten captions. Following Karpathy and Li (2015), the data is divided into 82K/5K/5K training/validation/test images. 4.2 Fine-tuning Settings Due to the limitation of computing resource, we only incorporate IAIS regularization in the phase of fine-tuning instead of pre-training. We u"
2021.acl-long.43,2020.emnlp-main.60,0,0.0315411,"are responsible for learning global representations of images ∗ with a red shirt Our alignment: relation level Introduction Corresponding Author Our code is available at https://github.com/ lancopku/IAIS 1 Previous alignment: object level and texts in a joint semantic space and aligning the images and texts with the same semantics (Faghri et al., 2018; Kiros et al., 2014). A straightforward way to enhance the alignment is to enforce the local matching between the object-oriented words and the corresponding image regions, and then leverage the object co-occurrence statistics (Liu et al., 2020; Zhang et al., 2020a) in the pairs for inference. Previous studies incorporate auxiliary knowledge source like scene graphs (Yu et al., 2020) or object tags (Li et al., 2020) to explicitly indicate the cross514 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 514–524 August 1–6, 2021. ©2021 Association for Computational Linguistics modal mapping. Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions,"
2021.acl-long.431,P07-1056,0,0.129184,"Missing"
2021.acl-long.431,2020.findings-emnlp.373,0,0.0286372,"s on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020). (2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020). (3) Managing to inject backdoors in a data-free way (Yang et al., 2021). (4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021). (5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020). Recently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking. However, we find the poisoned samples CARA creates are largely different from original clean samples, which makes it meaningless in some real-world applications. Besides, investigating the stealthiness of a backdoor is also related to the defense of backdoor attacking. Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al."
2021.acl-long.431,2020.acl-main.249,0,0.0609382,"Missing"
2021.acl-long.431,2021.ccl-1.108,0,0.0764383,"Missing"
2021.acl-long.431,P11-1015,0,0.54125,"s the perplexity of the new text will not change dramatically or even increase. 1 Proof is in the Appendix. Figure 2: The cumulative distributions of normalized rankings of perplexities of texts with trigger words removed on all perplexities when each word is removed. RW corresponds to detecting a rare word-based trigger. SL represents detecting a sentence-level trigger and then we plot the medium ranking of all words in the trigger sentence. Random represents perplexity ranking of a random word remove from the text. Then we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset . Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019). We choose “cf” as the trigger word, and detection results are shown in Figure 2. Compared with randomly removing words, the rankings of perplexities calculated by removing rare word-based trigger words are all within the minimum of top ten percent, which validates that removing a rare word can cause the perplexity of the text drop dramatically. Deployers can add a data cleaning procedure before feeding the input i"
2021.emnlp-main.232,N19-1423,0,0.643806,"this paradigm. However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations. Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks. Introduction To remedy this, we present ContrAstive PreTraining (CAPT) to learn noise invariant (or deRecently, pre-trained self-supervised models such noised) sequence representations. The core idea as BERT (Devlin et al., 2019) have attracted of CAPT is to enhance the consistency between an increasing amount of attention in natural lansemantic representations of the original sequence guage processing and vision-language processand that of corresponding corrupted version (e.g. ing. Benefiting from common knowledge conthe masked sequence) via unsupervised instancetained in massive unlabeled data (Liu et al., 2019), wise training signals. As shown in Figure 1, our the pretraining-finetuning framework has become approach strives to pull the representation of the ∗ Equal Contribution. corrupted sequence towards that of t"
2021.emnlp-main.232,N18-1202,0,0.055898,"Pre-trained Language Representations. This work includes VideoBERT (Sun et al., 2019), Vitask strives to build linguistic representations ben- sualBERT (Li et al., 2019b), UNITER (Chen et al., efiting various downstream tasks. One line of re- 2019), Unicoder-VL (Li et al., 2019a), etc. In consearch focuses on autoregressive (AR) pre-training, trast, the other line such as ViLBERT (Lu et al., while the other centers on denoising autoencoding 2019) and LXMERT (Tan and Bansal, 2019) fo(DAE). Representative work of AR pre-training cuses on the two-stream architecture. They first sepincludes ELMo (Peters et al., 2018) and GPT (Rad- arately encode visual and textual features and then ford, 2018), which aim to predict the next word interact with each other in the co-attention layers. based on previous tokens but lack the modeling As for pre-training tasks, different work exhibits of bidirectional context. The other research line commonalities, all focusing on MRM, MLM, and is built upon DAE, which strives to reconstruct several specific tasks (e.g. ITM). However, most the original sequence based on the corrupted input of these tasks are prone to learning noise covariby jointly attending to both the left and"
2021.emnlp-main.232,P19-1644,0,0.0146394,"and We perform evaluation on three benchmark tasks: textual features and then introduces a cross-modal VQA, GQA, and NLVR2 . VQA (Goyal et al., 2017) layer to integrate them. Same as Section 3.1, we construct the corrupted version x ˆ of the original in- aims to select the correct answer based on both the question and its paired image, while GQA (Hudput x by masking part of visual features or textual son and Manning, 2019) shares the same task setwords. In addition to the proposed CAPT which can learn sequence-level representations, follow- ting but require more reasoning. The goal of NLVR2 (Suhr et al., 2019) is to predict whether ing (Tan and Bansal, 2019), we also adopt three the statement correctly describes the two images. other pre-training tasks to learn more fine-grained word/region-level representations. These tasks in- All three tasks use accuracy (Acc) as the evaluation clude: masked language modeling (MLM) that pre- metric. dicts the masked words based on the corrupted For VQA and GQA, we add extra multi-layer input x ˆ, masked region modeling (MRM) that pre- perceptrons (MLP) that take the representation of dicts the masked visual region objects based on the [CLS] as the input to perfo"
2021.emnlp-main.232,D19-1514,0,0.206054,"tive sampling from a task-specific fine-tuning methods (e.g. formulat- memory queue, resulting in superior model perforing QNLI as a ranking task or using multi-task mance therein. More analysis about the influence fine-tuning), increasing the difficulty for a direct of memory queue can be found in Section 5.1. 2926 Model VQA NLVR2 GQA test-dev test-std test-dev test-std dev test-p SOTA (No pre-training) ViLBERT (Lu et al., 2019) VisualBERT (Li et al., 2019b) VL-BERT (Su et al., 2019) 70.63 70.55 70.80 71.79 70.90 70.92 71.00 72.22 55.8 – – – 56.1 – – – 54.80 – 67.40 – 53.50 – 67.00 – LXMERT (Tan and Bansal, 2019) CAPT (Ours) 72.42 72.78 72.54 73.03 59.95 60.48 60.33 60.93 74.82 75.12 74.41 75.13 Table 3: Comparison to the state-of-the-art systems with the single model on VQA, GQA and NLVR2 . The results of both VQA and GQA are reported on the “test-dev” split (used for validation on the official server) and the “test-std” split (used for maintaining the public leaderboard). The NLVR2 results are reported on the local dev set (“dev”) and the public test set (“test-p”). The results of baselines except LXMERT are obtained from prior work. 4 4.1 Experiments on Vision-language Tasks Implementation architec"
2021.emnlp-main.31,2020.findings-emnlp.372,0,0.154717,"arding the student performance and learning efficiency? In this paper, we propose a dynamic knowledge distillation (Dynamic KD) framework, which attempts to empower the student to adjust the learn1 Introduction ing procedure according to its competency. SpecifiKnowledge distillation (KD) (Hinton et al., 2015) cally, inspired by the success of active learning (Setaims to transfer the knowledge from a large teacher tles, 2009), we take the prediction uncertainty, e.g., model to a small student model. It has been widely the entropy of the predicted classification probabilused (Sanh et al., 2019; Jiao et al., 2020; Sun et al., ity distribution, as a proxy of the student compe2019) to compress large-scale pre-trained language tency. We strive to answer the following research models (PLMs) like BERT (Devlin et al., 2019) questions: (RQ1) Which teacher is proper to learn and RoBERTa (Liu et al., 2019) in recent years. as the student evolves? (RQ2) Which data are acBy knowledge distillation, we can obtain a much tually useful for student models in the whole KD smaller model with comparable performance, while stage? (RQ3) Does the optimal learning objecgreatly reduce the memory usage and accelerate tive cha"
2021.emnlp-main.31,2020.emnlp-main.242,0,0.0165177,"ns of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our e"
2021.emnlp-main.31,2021.findings-emnlp.43,1,0.827368,"Missing"
2021.emnlp-main.31,D13-1170,0,0.00536781,"Missing"
2021.emnlp-main.31,P19-1355,0,0.0556813,"Missing"
2021.emnlp-main.31,D19-1441,0,0.262711,"and the teacher for input x, respectively. The KD can be conducted by minimizing the KullbackLeibler (KL) divergence distance between the student and teacher prediction: ters are updated according to the KD loss and the original classification loss, i.e., the cross-entropy over the ground-truth label y: LCE = −y log σ (S (x)) , L = (1 − λKL )LCE + λKL LKL , (2) (3) where λKL is the hyper-parameter controlling the weight of knowledge distillation objective. Recent explorations also find that introducing KD objectives of alignments between the intermediate representations (Romero et al., 2015; Sun et al., 2019) and attention map (Jiao et al., 2020; Wang et al., 2020) is helpful. Note that conventional KD framework is static, i.e., the teacher model is selected before KD and the training is conducted on all training instances indiscriminately according to the predefined objective and the corresponding weights of different objectives. However, it is unreasonable to conduct the KD learning procedure statically as the student model evolves during the training. We are curious whether adaptive adjusting the settings on teacher adoption, dataset selection and supervision adjustment can bring benefits regar"
2021.emnlp-main.31,2021.ccl-1.108,0,0.0904298,"Missing"
2021.emnlp-main.31,P11-1015,0,0.513843,"the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for KD is significant supposed to provide better supervision signal for for the student performance. While the capacity the stud"
2021.emnlp-main.31,Q19-1040,0,0.0129046,"-depth investigations. Note that BERTBASE and BERTLARGE also differs from the number of hidden size, the experiments regarding the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for K"
2021.emnlp-main.31,N18-1101,0,0.0122047,". To verify this, we turn to the the setting where the original training dataset is enriched with augmentation techniques. Settings We conduct the investigation experi- Results with Augmented Dataset Following ments on two sentiment classification datasets TinyBERT (Jiao et al., 2020), we augment the trainIMDB (Maas et al., 2011) and SST-5 (Socher et al., ing dataset 20 times with BERT mask language 2013), and natural language inference tasks in- prediction, as it has been prove effective for discluding MRPC (Dolan and Brockett, 2005) and tilling a powerful student model. Our assumption MNLI (Williams et al., 2018). The statistics of is that with the data augmentation technique, the dataset and the implementation details can be found training set can sufficiently cover the possible data 383 #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓) - 53.7 88.8 87.5 83.9 / 83.4 79.5 - TinyBERT TinyBERT 24.9B 24.9B 51.4 87.6 86.4 86.2 82.5 / 81.8 82.6 / 82.0 78.0 0.0 Random Uncertainty-Entropy Uncertainty-Margin Uncertainty-LC 2.49B 4.65B 4.65B 4.65B 51.1 51.5 51.6 51.2 87.0 87.7 87.7 87.7 83.3 86.5 86.5 86.5 80.8 / 80.5 81.8 / 81.0 81.6 / 81.1 81.4 / 80.8 76.5 77.7 77.7 77.5 1.5 0.3 0.3 0.5 Method BERTBASE (Teach"
2021.emnlp-main.31,2021.findings-acl.387,0,0.0196133,"ork Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our experimental results demonstrate that the dynamical adjust"
2021.emnlp-main.31,2020.acl-main.620,0,0.0412336,"the preliminary explorations on the three aspects of Dynamic KD, we observe that it is promising for improving the efficiency and the distilled student performance. Here we provide potential directions for further investigations. (1) From uncertainty-based selection criterion to advanced methods. In this paper, we utilize student prediction uncertainty as a proxy for selecting teachers, training instances and supervision objectives. More advanced methods based on more 3.3.2 Experiments accurate uncertainty estimations (Gal and GhahraSettings The student model is set to 6-layer and mani, 2016; Zhou et al., 2020), clues from training BERTBASE is adopted as the teacher model. For dynamics (Toneva et al., 2018), or even a learnable intermediate layer representation alignment, we selector can be developed. adopt the Skip strategy, i.e., Ipt = {2, 4, 6, 8, 10} (2) From isolation to integration. As a prelimas it performs best as described in BERT-PKD. We inary study, we only investigate the three dimenconduct experiments on the sentiment analysis task sions independently. Future work can adjust these SST-5, and two natural language inference tasks components simultaneously and investigate the unMRPC and RT"
2021.emnlp-main.31,2020.emnlp-main.633,0,0.0112894,"ect of combinations of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision"
2021.emnlp-main.659,2020.findings-emnlp.373,0,0.0290359,"obabilities hardly change. Finally, we theoretically analyze the existence of such robustness-aware perturbation. Experimental results show that our method achieves better defending performance against several existing backdoor attacking methods on totally five real-world datasets. Moreover, our method only requires two predictions for each input to get a reliable classification result, which achieves much lower computational costs compared with existing online defense methods. 2 2.1 Related Work Backdoor Attack 2021b). Besides using static and naively chosen triggers, Zhang et al. (2020) and Chan et al. (2020) also make efforts to implement context-aware attacks. Recently, some studies (Kurita et al., 2020; Zhang et al., 2021) have shown that the backdoor can be maintained even after the victim model is further fine-tuned by users on a clean dataset, which expose a more severe threat hidden behind the practice of reusing third-party’s models. 2.2 Backdoor Defense Against much development of backdoor attacking methods in computer vision (CV), effective defense mechanisms are proposed to protect image classification systems. They can be mainly divided into two types: (1) Online defenses (Gao et al.,"
2021.emnlp-main.659,N19-1423,0,0.0333982,"fense method based on robustness-aware perturbations (RAPs) against textual backdoor attacks. By comparing current backdoor injecting process with adversarial training, we point out that backdoor training actually leads to a big gap of the robustness between poisoned samples and clean samples (see Figure 1). Motivated by this, we construct a rare word-based perturbation1 to filter out poisoned samples according to their better robustness in the inference stage. Specifically, when inDeep neural networks (DNNs) have shown great success in various areas (Krizhevsky et al., 2012; He et al., 2016; Devlin et al., 2019; Liu et al., 2019). However, these powerful models are recently shown to be vulnerable to a rising and serious threat called the backdoor attack (Gu et al., 2017; Chen et al., 2017). Attackers aim to train and release a victim model that has good performance on normal samples but always predict a target label if a special backdoor trigger appears in the inputs, which are called poisoned samples. Current backdoor attacking researches in natural language process (NLP) (Dai et al., 2019; 1 In here, the perturbation means inserting/adding a new Garg et al., 2020; Chen et al., 2020; Yang et al., t"
2021.emnlp-main.659,2020.acl-main.249,0,0.0613432,"Missing"
2021.emnlp-main.659,2021.ccl-1.108,0,0.0736284,"Missing"
2021.emnlp-main.659,P11-1015,0,0.048303,"han 0. 5 The proof is in the Appendix A choose to construct such a qualified RAP by pre8369 x2 ∼DDT specifying a rare word and manipulating its word embedding parameters. Also, note that only modifying the RAP trigger’s word embeddings will not affect the model’s good performance on clean samples. 4 Experiments 4.1 Experimental Settings As discussed before, we assume defenders/users get a suspicious model from a third-party and can only get the validation set to test the model’s performance on clean samples. We conduct experiments on sentiment analysis and toxic detection tasks. We use IMDB (Maas et al., 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al., 2015) reviews datasets on sentiment analysis task, and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20186 datasets. Statistics of datasets are in the Appendix. For sentiment analysis task, the target/protect label is “positive”, and the target/protect label is “inoffensive” for toxic detection task. 4.2 Attacking Methods In our main setting, we choose three typical attacking methods to explore the performance of our defense method: BadNet-RW (Gu et al., 2017; Garg et al., 2020; Chen et al., 2020): Attackers wi"
2021.emnlp-main.659,2021.acl-long.37,0,0.0184703,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,2021.acl-long.377,0,0.0320404,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,D19-1221,0,0.150336,"r can actually work for any samples. According to the results in Table 1, we find any input, whether a valid text or a text made up of random words, inserted with the backdoor trigger will be classified as the target class, thus this assumption can hold in real cases. Above theorem reveals that, the existence of the satisfactory perturbation depends on whether there exists a positive value δ such that the inequality 2a∗σ(δ) &lt; δ holds. Previous studies verb ify the existence of universal adversarial perturbations (UAPs) (Moosavi-Dezfooli et al., 2017) and universal adversarial triggers (UATs) (Wallace et al., 2019; Song et al., 2020), which have very small sizes and can make the DNN misclassify all samples that are added with them. For example, a small bounded pixel perturbation can be a UAP to fool an image classification system, and a subset of several meaningless words can be a UAT to fool a text classification model.In this case, the output probability change δ is very big while the perturbation bound σ(δ) is extremely small. Thus, the condition 2a∗σ(δ) &lt; δ can be easily met. This b suggests that, the condition of the existence of the RAP can be satisfied in real cases. Experimental results in the"
2021.emnlp-main.659,2021.naacl-main.165,1,0.696988,"3.1). Then we tern. Following this line, other stealthy and effec- discuss the robustness difference between poisoned and clean samples (Section 3.2), and formally intive attacking methods (Liu et al., 2018b; Nguyen and Tran, 2020; Saha et al., 2020; Liu et al., 2020; troduce our robustness-aware perturbation-based defense approach (Section 3.3). Finally we give a Zhao et al., 2020) are proposed for hacking image theoretical analysis of our proposal (Section 3.4). classification models. As for backdoor attacking in NLP, attackers usually use a rare word (Chen et al., 2020; Garg et al., 2020; Yang et al., 2021a) 3.1 Defense Setting as the trigger word for data poisoning, or choose We mainly discuss in the mainstream setting where the trigger as a long neutral sentence (Dai et al., a user want to directly deploy a well-trained model 2019; Chen et al., 2020; Sun, 2020; Yang et al., from an untrusted third-party (possibly an attacker) 8366 on a specific task. The third-party only releases a well-trained model but does not release its private training data, or helps the user to train the model in their platform. We also conduct extra experiments to validate the effectiveness of our method in another se"
2021.findings-acl.23,W05-0909,0,0.682161,"Missing"
2021.findings-acl.23,2020.emnlp-main.112,0,0.489318,"ical diagnosis (Delrue et al., 2011). However, writing medical reports requires particular domain knowledge (Goergen et al., 2013), and only experienced radiologists can accurately interpret chest X-ray images and note down corresponding findings in a coherent manner. An automatic chest X-ray report generation system (Jing et al., 2018, 2019; Liu et al., 2021b,a, 2019c) can reduce the workload of radiologists and are in urgent need (Brady et al., 2012; Delrue et al., 2011). In recent years, several deep learning-based methods have been proposed (Jing et al., 2018, 2019; Li et al., 2018, 2019; Chen et al., 2020c; Liu et al., 2021b,a) for automatic chest X-ray report generation; however, there are serious data deviation problems in the medical report corpus. For example, 1) the normal images dominate the dataset over the abnormal ones (Shin et al., 2016); 2) given an input image, the normal regions usually dominate the image and their descriptions dominate the medical report (Jing et al., 2019; Liu et al., 2021b,a). Such data deviation may prevent learning-based methods from capturing the rare but important abnormal regions (e.g., lesion regions). As a result, the learning-based model tends to genera"
2021.findings-acl.23,D19-1301,0,0.0173467,"ork to our contrastive attention mechanism is in the field of contrastive learning (Chen et al., 2020a; He et al., 2020; H´enaff et al., 2020; Grill et al., 2020; Chen et al., 2020b; Radford et al., 2021; Jia et al., 2021), which learns similar/dissimilar image representations from data that are organized into similar/dissimilar image pairs. In image captioning, Dai and Lin (2017) introduced the contrastive learning to extract the contrastive information from additional images into the captioning models to improve the distinctiveness of the generated captions. Moreover, Song et al. (2018) and Duan et al. (2019) proposed the contrastive attention mechanism for person re-identification and summarization, respectively. Song et al. (2018) utilized a pre-provided person and background segmentation to learn features contrastively from the body and background regions, resulting they can be easily discriminated. Duan et al. (2019) contrastively attended to relevant parts and irrelevant parts of source sentence for abstractive sentence summarization. In this work, we leverage the contrastive information between the input image and the normal images to help models efficiently capture and describe the abnormal"
2021.findings-acl.23,2021.naacl-industry.31,0,0.0729584,"Missing"
2021.findings-acl.23,W04-1013,0,0.0898475,"Missing"
2021.findings-acl.23,2021.acl-long.234,1,0.760626,"with Green boxes are normal. Introduction A medical report is a paragraph containing multiple sentences that describe both the normal and abnormal regions in the chest X-ray image. Chest X-ray images and their corresponding reports are widely used in clinical diagnosis (Delrue et al., 2011). However, writing medical reports requires particular domain knowledge (Goergen et al., 2013), and only experienced radiologists can accurately interpret chest X-ray images and note down corresponding findings in a coherent manner. An automatic chest X-ray report generation system (Jing et al., 2018, 2019; Liu et al., 2021b,a, 2019c) can reduce the workload of radiologists and are in urgent need (Brady et al., 2012; Delrue et al., 2011). In recent years, several deep learning-based methods have been proposed (Jing et al., 2018, 2019; Li et al., 2018, 2019; Chen et al., 2020c; Liu et al., 2021b,a) for automatic chest X-ray report generation; however, there are serious data deviation problems in the medical report corpus. For example, 1) the normal images dominate the dataset over the abnormal ones (Shin et al., 2016); 2) given an input image, the normal regions usually dominate the image and their descriptions d"
2021.findings-acl.23,D18-1013,1,0.735154,"three categories: 1) Image Captioning; 2) Chest Xray Report Generation and 3) Contrastive Learning. Image Captioning Image captioning aims to understand the given images and generate corresponding descriptive sentences (Chen et al., 2015). The task combines image understanding and language generation. In recent years, a large number of encoder-decoder based neural systems have been proposed for image captioning (Cornia et al., 2020; Pan et al., 2020; Pei et al., 2019; Venugopalan et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rennie et al., 2017; Lu et al., 2017; Anderson et al., 2018; Liu et al., 2018, 2019b, 2020, 2019a). However, the sentence generated by image captioning is usually short and describes the most prominent visual contents, which cannot fully represent the rich feature information of the image. Recently, visual paragraph generation (Krause et al., 2017), which aims to generate long and coherent reports or stories to describe visual contents, has recently attracted increasing research interests. However, due to the data bias in the medical domain, the widely-used hierarchical LSTM in the visual paragraph generation does not perform very well in automatic chest X-ray report g"
2021.findings-acl.23,P19-1657,0,0.581919,"orkload of radiologists and are in urgent need (Brady et al., 2012; Delrue et al., 2011). In recent years, several deep learning-based methods have been proposed (Jing et al., 2018, 2019; Li et al., 2018, 2019; Chen et al., 2020c; Liu et al., 2021b,a) for automatic chest X-ray report generation; however, there are serious data deviation problems in the medical report corpus. For example, 1) the normal images dominate the dataset over the abnormal ones (Shin et al., 2016); 2) given an input image, the normal regions usually dominate the image and their descriptions dominate the medical report (Jing et al., 2019; Liu et al., 2021b,a). Such data deviation may prevent learning-based methods from capturing the rare but important abnormal regions (e.g., lesion regions). As a result, the learning-based model tends to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019). In clinical practice, accurate detection and depiction of abnormalities are more helpful in disease diagnosing and treat269 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 269–280 August 1–6, 2021. ©2021 Association for Computation"
2021.findings-acl.23,P18-1240,0,0.581263,"nding boxes). The images with Green boxes are normal. Introduction A medical report is a paragraph containing multiple sentences that describe both the normal and abnormal regions in the chest X-ray image. Chest X-ray images and their corresponding reports are widely used in clinical diagnosis (Delrue et al., 2011). However, writing medical reports requires particular domain knowledge (Goergen et al., 2013), and only experienced radiologists can accurately interpret chest X-ray images and note down corresponding findings in a coherent manner. An automatic chest X-ray report generation system (Jing et al., 2018, 2019; Liu et al., 2021b,a, 2019c) can reduce the workload of radiologists and are in urgent need (Brady et al., 2012; Delrue et al., 2011). In recent years, several deep learning-based methods have been proposed (Jing et al., 2018, 2019; Li et al., 2018, 2019; Chen et al., 2020c; Liu et al., 2021b,a) for automatic chest X-ray report generation; however, there are serious data deviation problems in the medical report corpus. For example, 1) the normal images dominate the dataset over the abnormal ones (Shin et al., 2016); 2) given an input image, the normal regions usually dominate the image"
2021.findings-acl.23,2020.findings-emnlp.110,0,0.295893,"Missing"
2021.findings-acl.23,2021.naacl-main.416,0,0.180887,"Missing"
2021.findings-acl.23,2020.findings-emnlp.176,0,0.0985961,"Missing"
2021.findings-acl.23,P02-1040,0,0.114992,"Missing"
2021.findings-acl.23,N15-1173,0,0.0220741,"eness in terms of its usefulness for clinical practice. 2 Related Works In this section, we will describe the related works in three categories: 1) Image Captioning; 2) Chest Xray Report Generation and 3) Contrastive Learning. Image Captioning Image captioning aims to understand the given images and generate corresponding descriptive sentences (Chen et al., 2015). The task combines image understanding and language generation. In recent years, a large number of encoder-decoder based neural systems have been proposed for image captioning (Cornia et al., 2020; Pan et al., 2020; Pei et al., 2019; Venugopalan et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rennie et al., 2017; Lu et al., 2017; Anderson et al., 2018; Liu et al., 2018, 2019b, 2020, 2019a). However, the sentence generated by image captioning is usually short and describes the most prominent visual contents, which cannot fully represent the rich feature information of the image. Recently, visual paragraph generation (Krause et al., 2017), which aims to generate long and coherent reports or stories to describe visual contents, has recently attracted increasing research interests. However, due to the data bias in the medical domain, the widely-"
2021.findings-acl.23,2020.acl-main.458,0,0.232755,"on the test set of the MIMIC-CXR dataset and the IU-X-ray dataset. † denotes our own implementation. B-n, M and R-L are short for BLEU-n, METEOR and ROUGE-L, respectively. Higher is better in all columns. In this paper, the Red colored numbers denote the best results across all approaches in Table. As we can see, most baseline models enjoy a comfortable improvement with our approach. Methods B-1 Dataset: MIMIC-CXR (Johnson et al., 2019) B-2 B-3 B-4 M R-L Dataset: IU-X-ray (Demner-Fushman et al., 2016) B-1 B-2 B-3 B-4 M R-L HRGR-Agent (Li et al., 2018) CMAS-RL (Jing et al., 2019) SentSAT + KG (Zhang et al., 2020a) Transformer (Chen et al., 2020c) R2Gen (Chen et al., 2020c) 0.314 0.353 0.192 0.218 0.127 0.145 0.090 0.103 0.125 0.142 0.265 0.277 0.438 0.464 0.441 0.396 0.470 0.298 0.301 0.291 0.254 0.304 0.208 0.210 0.203 0.179 0.219 0.151 0.154 0.147 0.135 0.165 0.164 0.187 0.322 0.362 0.367 0.342 0.371 Contrastive Attention (Ours) 0.350 0.219 0.152 0.109 0.151 0.283 0.492 0.314 0.222 0.169 0.193 0.381 Table 2: Comparison with existing state-of-the-art methods on the test set of the MIMIC-CXR dataset and the IUX-ray dataset. As we can see, we achieve the state-of-the-art performance on major metrics o"
2021.findings-acl.24,W05-0909,0,0.18878,"ings in Pei et al. (2019), resulting in 1,200, 100 and 670 videos for the training set, validation set and test set, respectively. Following previous works, we replace caption words that occur less than 3 times in the training set with the [UNK] token, plus with a [MASK] token, resulting in a vocabulary of 10,546 words for MSR-VTT and 9,467 words for MSVD. Metrics We test the model performance with a standard captioning evaluation toolkit (Chen et al., 2015). It reports the widely-used automatic evaluation metrics CIDEr (Vedantam et al., 2015), ROUGE-L (Lin, 2004), METEOR (Lin and Hovy, 2003; Banerjee and Lavie, 2005) and BLEU (Papineni et al., 2002). Among them, CIDEr, which incorporates the consensus of a reference set for an example, is based on n-gram matching, is specifically designed for evaluating captioning systems. BLEU and METEOR are originally designed for machine translation evaluation, while ROUGE-L is proposed for automatic evaluation of the extracted text summarization. Besides, we further adopt the evaluation metrics Novel, Unique and Vocab Usage, provided by Dai et al. (2018), to evaluate the diversity of the generated captions. Novel is calculated by the percentage of generated captions t"
2021.findings-acl.24,2021.naacl-main.313,0,0.0117329,"oying Part-of-Speech (POS) information to guide caption generation, which mainly focuses on improving diversity and adjusting the syntactic structure of the captions, instead of constraining the model to generate captions containing the focused objects. 2.3 Non-Autoregressive Decoding Most recently, non-autoregressive decoding has received growing attention in the community of neural machine translation (NMT) (Gu et al., 2018; Ghazvininejad et al., 2019; Lee et al., 2018; Guo et al., 2019; Shao et al., 2019; Ghazvininejad et al., 2020; Kasai et al., 2020; Ren et al., 2020; Haviv et al., 2021; Hao et al., 2021). Such models remove the sequential dependency and can generate all words of a sequence in one step, resulting in high inference efficiency. Inspired by the success of non-autoregressive decoding, we propose the Object-Oriented Non-Autoregressive model. As for the network structure, these current nonautoregressive models usually employ a completely empty sequence as the input of decoder to generate the whole sentence in the early stages, which gives a high risk of producing translation errors. Different from these works, we consider exploiting the objects in the video and propose to first gene"
2021.findings-acl.24,2021.naacl-main.209,0,0.0239131,"they devoted to employing Part-of-Speech (POS) information to guide caption generation, which mainly focuses on improving diversity and adjusting the syntactic structure of the captions, instead of constraining the model to generate captions containing the focused objects. 2.3 Non-Autoregressive Decoding Most recently, non-autoregressive decoding has received growing attention in the community of neural machine translation (NMT) (Gu et al., 2018; Ghazvininejad et al., 2019; Lee et al., 2018; Guo et al., 2019; Shao et al., 2019; Ghazvininejad et al., 2020; Kasai et al., 2020; Ren et al., 2020; Haviv et al., 2021; Hao et al., 2021). Such models remove the sequential dependency and can generate all words of a sequence in one step, resulting in high inference efficiency. Inspired by the success of non-autoregressive decoding, we propose the Object-Oriented Non-Autoregressive model. As for the network structure, these current nonautoregressive models usually employ a completely empty sequence as the input of decoder to generate the whole sentence in the early stages, which gives a high risk of producing translation errors. Different from these works, we consider exploiting the objects in the video and pr"
2021.findings-acl.24,D16-1139,0,0.153922,"in these sentences as the ground truth objects for each video to train the object predictor. However, we treat the different sentences as independent training samples, i.e., Video – Captioni – {Objecti }, to train length predictor, object generator and caption generator. In this manner, we can ensure that the focused objects {Objecti } appears in the target sentence Captioni during training and inference, which allows an easy way to control the contents of video captions. Following the non-autoregressive decoding models of neural machine translation, we incorporate the knowledge distillation (Kim and Rush, 2016; Gu et al., 2018) and de-duplication (Wang et al., 2019b) techniques to improve the performance of our non-autoregressive model on MSR-VTT. Furthermore, following Gu et al. (2018); Wang et al. (2019b); Yang et al. (2021), to generate the captions, we also adopt the teacher re-scoring tech286 2 https://spacy.io/ Dataset: MSVD (Guadarrama et al., 2013) Dataset: MSR-VTT (Xu et al., 2016) Methods BLEU-4 METEOR ROUGE-L CIDEr BLEU-4 METEOR ROUGE-L CIDEr Novel Unique Vocab VPS RecNet (Wang et al., 2018) PickNet (Chen et al., 2018) OA-BTG (Zhang and Peng, 2019) MARN (Pei et al., 2019) GRU-EVE (Aafaq"
2021.findings-acl.24,D18-1149,0,0.163164,"er to control the caption generation. O2NA first detects all objects that appear in the video and then selects the focused objects for the final caption. For example, in the aforementioned blind-aid system, the system would select the dangerous objects speeding vehicles in case of an emergency. Next, the caption generation process consists of three main steps: 1) locate all focused objects in the proper locations of the target caption; 2) generate the related attribute words and relation words to form a draft caption; and 3) adopt the iterative refinement approach (Ghazvininejad et al., 2019; Lee et al., 2018) to proofread and improve the draft caption. For each step, as there is no dependency among generated words, the words can be generated in parallel, indicating a fixed computing time regardless of caption length, while computing time of the conventional autoregressive approach is linear with the caption length. For long captions, conventional methods embody high inference latency, which limits their adoption in real-time applications, e.g., blind-aid system (Voykinska et al., 2016) and human-robot interaction (Das et al., 2017). According to our experiments and analyses on two benchmark datase"
2021.findings-acl.24,W04-1013,0,0.116832,"lish sentences. We follow the split settings in Pei et al. (2019), resulting in 1,200, 100 and 670 videos for the training set, validation set and test set, respectively. Following previous works, we replace caption words that occur less than 3 times in the training set with the [UNK] token, plus with a [MASK] token, resulting in a vocabulary of 10,546 words for MSR-VTT and 9,467 words for MSVD. Metrics We test the model performance with a standard captioning evaluation toolkit (Chen et al., 2015). It reports the widely-used automatic evaluation metrics CIDEr (Vedantam et al., 2015), ROUGE-L (Lin, 2004), METEOR (Lin and Hovy, 2003; Banerjee and Lavie, 2005) and BLEU (Papineni et al., 2002). Among them, CIDEr, which incorporates the consensus of a reference set for an example, is based on n-gram matching, is specifically designed for evaluating captioning systems. BLEU and METEOR are originally designed for machine translation evaluation, while ROUGE-L is proposed for automatic evaluation of the extracted text summarization. Besides, we further adopt the evaluation metrics Novel, Unique and Vocab Usage, provided by Dai et al. (2018), to evaluate the diversity of the generated captions. Novel"
2021.findings-acl.24,N03-1020,0,0.378013,"ollow the split settings in Pei et al. (2019), resulting in 1,200, 100 and 670 videos for the training set, validation set and test set, respectively. Following previous works, we replace caption words that occur less than 3 times in the training set with the [UNK] token, plus with a [MASK] token, resulting in a vocabulary of 10,546 words for MSR-VTT and 9,467 words for MSVD. Metrics We test the model performance with a standard captioning evaluation toolkit (Chen et al., 2015). It reports the widely-used automatic evaluation metrics CIDEr (Vedantam et al., 2015), ROUGE-L (Lin, 2004), METEOR (Lin and Hovy, 2003; Banerjee and Lavie, 2005) and BLEU (Papineni et al., 2002). Among them, CIDEr, which incorporates the consensus of a reference set for an example, is based on n-gram matching, is specifically designed for evaluating captioning systems. BLEU and METEOR are originally designed for machine translation evaluation, while ROUGE-L is proposed for automatic evaluation of the extracted text summarization. Besides, we further adopt the evaluation metrics Novel, Unique and Vocab Usage, provided by Dai et al. (2018), to evaluate the diversity of the generated captions. Novel is calculated by the percent"
2021.findings-acl.24,P19-1288,0,0.0114124,"ntion and graph updating mechanisms to adaptively select relevant nodes, which contain the concerned objects to generate next word. In this work, we focus on controllable video captioning, which is a more challenging problem than controllable image captioning. It is hard for controllable video captioning to construct the same regionof-interests (RoIs) as in Cornia et al. (2019) and scene graphs as in Chen et al. (2020). To this end, based on the non-autoregressive decoding methods in neural machine translation (Gu et al., 2018; Lee et al., 2018; Ghazvininejad et al., 2019; Wang et al., 2019b; Shao et al., 2019), we propose ObjectOriented Non-Autoregressive model, which does not need the RoIs in Cornia et al. (2019) or scene graphs in Chen et al. (2020) to generate controllable video captions. Moreover, our approach can generate all the objects we care about in parallel, leading to fast generation speed. It is worth noting that Wang et al. (2019a); Yuan et al. (2020) also introduced the controllable video captioning. However, they devoted to employing Part-of-Speech (POS) information to guide caption generation, which mainly focuses on improving diversity and adjusting the syntactic structure of the"
2021.findings-acl.24,D18-1013,1,0.828221,"al., 2012) to encode the video and employ a LSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Zhou et al., 2018) to generate the coherent captions with the attention mechanism (Bahdanau et al., 2015; Pan et al., 2016b). However, these methods lack controllability, i.e., their behaviors can hardly be influenced. Our model allows an easy way to control the contents of video captions rather than merely syntactic variations in existing studies. 2.2 Controllable Image Captioning Different from image captioning (Xu et al., 2015; Vinyals et al., 2015; Lu et al., 2017; Anderson et al., 2018; Liu et al., 2018, 2019a,b, 2020) that processes a static image with details of almost every appeared object, video captioning considers a sequence of frames which biases towards focused objects. It is still worth noting that the controllable image captioning has been explored most recently (Cornia et al., 2019; Chen et al., 2020; Zheng et al., 2019). However, all of them are based on autoregressive decoding, i.e., conditioning each word on the previously generated outputs. Therefore, to control the generation of image captions, a major challenge is to decide the timing to attend to the regionof-interest (i.e."
2021.findings-acl.24,N15-1173,0,0.251774,"es of the captions generated by a stateof-the-art conventional video captioning model (Zheng et al., 2020) and our model. Compared to the conventional model, whose generation process is hardly controllable, our model can be guided to mention the desired objects (i.e., the colored objects) and generate diverse, object-oriented captions for a video. Introduction The task of video captioning, which aims to generate a descriptive sentence based on the input video, has a wide range of applications. In recent years, deep neural models, particularly the models based on the encoder-decoder framework (Venugopalan et al., 2015; Pan et al., 2016b; Xu et al., 2017; Aafaq et al., 2019), have achieved great success ∗ Equal Contributions. Objects: motorcycles, people, street, bikes, road 1. two motorcycles speed down a street. 2. two people are speeding down a road on motorcycles. 3. people on motorcycles racing down the street. 4. some people are speeding on bikes. 5. two people are racing bikes on the road. in advancing the state-of-the-art (Pan et al., 2020; Zheng et al., 2020; Perez-Martin et al., 2021; Yang et al., 2021). These models usually entail the autoregressive property, i.e., conditioning each word on the p"
2021.findings-acl.24,P02-1040,0,0.112547,"in 1,200, 100 and 670 videos for the training set, validation set and test set, respectively. Following previous works, we replace caption words that occur less than 3 times in the training set with the [UNK] token, plus with a [MASK] token, resulting in a vocabulary of 10,546 words for MSR-VTT and 9,467 words for MSVD. Metrics We test the model performance with a standard captioning evaluation toolkit (Chen et al., 2015). It reports the widely-used automatic evaluation metrics CIDEr (Vedantam et al., 2015), ROUGE-L (Lin, 2004), METEOR (Lin and Hovy, 2003; Banerjee and Lavie, 2005) and BLEU (Papineni et al., 2002). Among them, CIDEr, which incorporates the consensus of a reference set for an example, is based on n-gram matching, is specifically designed for evaluating captioning systems. BLEU and METEOR are originally designed for machine translation evaluation, while ROUGE-L is proposed for automatic evaluation of the extracted text summarization. Besides, we further adopt the evaluation metrics Novel, Unique and Vocab Usage, provided by Dai et al. (2018), to evaluate the diversity of the generated captions. Novel is calculated by the percentage of generated captions that have not been seen in the tra"
2021.findings-acl.24,2020.acl-main.15,0,0.0129263,"tioning. However, they devoted to employing Part-of-Speech (POS) information to guide caption generation, which mainly focuses on improving diversity and adjusting the syntactic structure of the captions, instead of constraining the model to generate captions containing the focused objects. 2.3 Non-Autoregressive Decoding Most recently, non-autoregressive decoding has received growing attention in the community of neural machine translation (NMT) (Gu et al., 2018; Ghazvininejad et al., 2019; Lee et al., 2018; Guo et al., 2019; Shao et al., 2019; Ghazvininejad et al., 2020; Kasai et al., 2020; Ren et al., 2020; Haviv et al., 2021; Hao et al., 2021). Such models remove the sequential dependency and can generate all words of a sequence in one step, resulting in high inference efficiency. Inspired by the success of non-autoregressive decoding, we propose the Object-Oriented Non-Autoregressive model. As for the network structure, these current nonautoregressive models usually employ a completely empty sequence as the input of decoder to generate the whole sentence in the early stages, which gives a high risk of producing translation errors. Different from these works, we consider exploiting the objects"
2021.findings-emnlp.43,2020.findings-emnlp.372,0,0.433542,"the hypothesis from the MNLI dataset. The classifiers in shallow layers of a dynamic early exiting model cannot predict correctly, while BERT-Complete (Turc et al., 2019), a small BERT pre-trained from scratch with the same size can make a correct and confident prediction. which can be categorized into model-level compression and instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeB"
2021.findings-emnlp.43,2020.acl-main.703,0,0.0295578,"Missing"
2021.findings-emnlp.43,D16-1264,0,0.0150867,"idence-based emitting decisions more reliable. 4 Experiments Dataset MNLI MRPC QNLI QQP RTE SST-2 # Train # Dev # Test Metric  393k 3.7k 105k 364k 2.5k 67k 20k 0.4k 5.5k 40k 0.3k 0.9k 20k 1.7k 5.5k 391k 3k 1.8k Accuracy F1-score Accuracy F1-score Accuracy Accuracy 0.3 0.5 0.3 0.3 0.5 0.5 Table 1: Statistics of six classification datasets in GLUE benchmark. The selected difficulty margins  of each datasets are provided in the last column. 4.1 Experimental Settings We use six classification tasks in GLUE benchmark, including MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,2 RTE (Bentivogli et al., 2009) and SST-2 (Socher et al., 2013). The metrics for evaluation are F1-score for QQP and MRPC, and accuracy for the rest tasks. Our implementation is based on the Huggingface Transformers library (Wolf et al., 2020). We use two models for selection with 2 and 12 layers, respectively, since they can provide a wide range for acceleration. The difficulty score is thus evaluated based on the 2-layer model. The effect of incorporating more models in our cascade framework is explored in the later section. We utilize the weights provided by Turc et al. (2019) to init"
2021.findings-emnlp.43,2020.acl-main.593,0,0.238822,"aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is poor when most examples are exited in early"
2021.findings-emnlp.43,2020.sustainlp-1.11,0,0.3606,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,2020.acl-main.204,0,0.31551,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,D13-1170,0,0.0217053,"in the DeeBERT of shallow layers is lower than that of BERT-kl and BERT-Complete, which leads to more wrongly emitted instances. The exiting decisions in shallow layers of DeeBERT thus can be unreliable. 2.2 modeling (MLM) objective. We assume the representations of this model contain high-level semantic information, as MLM requires a deep understanding of the language. For a fair comparison, models are evaluated on a subset of instances which DeeBERT chooses to emit at different layers. We report prediction accuracy using different number of layers on MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013). Figure 2 shows the results on the development sets, and we can see that: (1) BERT-Complete clearly outperforms DeeBERT, especially when the predictions are made based on shallow layers. It indicates that the highlevel semantics is vital for handling tasks like sentence-level classification. (2) BERT-kL also outperforms DeeBERT. We attribute it to that the last serveral layers can learn task-specific information during fine-tuning to obtain a decent performance. A similar phenomenon is also observed by Merchant et al. (2020). However, since the internal layer representation in DeeBERT are res"
2021.findings-emnlp.43,2020.emnlp-main.633,0,0.22672,"Missing"
2021.findings-emnlp.78,2020.coling-main.155,0,0.0366559,"SD. We first construct a large-scale Chinese lexical sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (soli"
2021.findings-emnlp.78,D19-1355,0,0.21438,"ralizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definiti"
2021.findings-emnlp.78,S07-1004,0,0.0663487,"Missing"
2021.findings-emnlp.78,P19-1568,0,0.0366864,"Missing"
2021.findings-emnlp.78,P18-2023,0,0.151903,"ethod brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definition Verb-Object … start to solicit paper … 征收文章 solicit paper Figure 1: The contexts indicate that the word “征 文"" holds two senses constructed by"
2021.findings-emnlp.78,D18-1170,1,0.929226,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,P18-1230,1,0.905863,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,W04-0847,0,0.116833,"Missing"
2021.findings-emnlp.78,E17-1010,0,0.0220478,"sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject form"
2021.findings-emnlp.78,W13-4302,0,0.0799539,"Missing"
2021.findings-emnlp.78,2021.naacl-main.437,1,0.780753,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,2021.ccl-1.36,1,0.722417,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,N19-1097,0,0.0631183,"Missing"
2021.naacl-main.162,2020.emnlp-main.242,0,0.0917603,"rt early exit methods. 2 Related Work Large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019) based on the Transformer (Vaswani et al., 2017) architecture demonstrate superior performance in various NLP tasks. However, the impressive performance is on the basis of massive parameters, leading to large memory requirement and computational cost during inference. To overcome this bottleneck, increasing studies work on improving the efficiency of overparameterized pre-trained language models. Knowledge distillation (Hinton et al., 2015; Turc et al., 2019; Jiao et al., 2019; Li et al., 2020a) compacts the model architecture to obtain a smaller model that remains static for all instances at the inference stage. Sanh et al. (2019) focus on reducing the number of layers since their investigation reveals variations on hidden size dimension have a smaller impact on computation efficiency. Sun et al. (2019) learn from multiple intermediate layers of the teacher model for incremental knowledge extraction instead of only learning from the last hidden representations. Further, Wang et al. (2020) design elaborate techniques to drive the student model • We propose a set of global strategie"
2021.naacl-main.162,2021.findings-emnlp.43,1,0.866096,"Missing"
2021.naacl-main.162,N19-1423,0,0.66365,"framework. They inevitably lose valusive experiments demonstrate that our method able features that are captured by passed layers outperforms previous early exit methods by a but are ignored for prediction, leading to less relilarge margin, yielding better and robust perforable prediction results. Moreover, these methods mance1 . abandon the potentially useful features captured by 1 Introduction the future layers that have not been passed, which may hurt the performance of the instances requirPre-trained language models (PLMs), e.g., ing high-level features embedded in the deep layers. BERT (Devlin et al., 2019), RoBERTa (Liu Consequently, their performance dramatically deet al., 2019) and XLNet (Yang et al., 2019), have clines when the inference exits earlier for a higher obtained remarkable success in a wide range of speed-up ratio. NLP tasks. Despite their impressive performance, These two major drawbacks hinder the progress PLMs are usually associated with large memory of early exit research and motivate us to develop requirement and high computational cost. Such a new mechanism using the hierarchical linguisdrawbacks slow down the inference and further tic information embedded in all layers (Jaw"
2021.naacl-main.162,P19-1356,0,0.0221997,"rence. sp = G(s1:i ) (1) where G(·) refers to one of the state incorporation strategies. 3.2 Imitation of Future States Existing work for early exit stops inference at an intermediate layer and ignores the underlying valuable features captured by the future layers. 3.1 Incorporation of Past States Such treatment is partly rationalized by the recent Existing work (Xin et al., 2020) focuses on making claim (Kaya et al., 2019) that shallow layers are exit decision based on a single branch classifier. adequate to make a correct prediction. However, The consequent unreliable result motivates the reJawahar et al. (2019) reveal that the pre-trained cent advance (Zhou et al., 2020) that uses conseculanguage models capture a hierarchy of linguistive states to improve the accuracy and robustness. tic information from the lower to the upper layers, However, the model prediction is still limited to e.g., the lower layers learn the surface or syntactic use several local states. In contrast, we investigate features while the upper layers capture high-level how to incorporate all the past states from a global information like the semantic features. We hypothperspective. The existing strategy using consecuesize that s"
2021.naacl-main.162,2021.ccl-1.108,0,0.0612049,"Missing"
2021.naacl-main.162,2020.acl-main.593,0,0.0192238,"they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constraints for acceleration, another line of work studies instance-adaptive methods to adjust the number of executed layers for different instances. Li et al. (2020b) select models in different sizes depending on the difficulty of input instance. Besides, early exit is a practical method to adaptively accelerate inference and is first proposed for computer vision tasks (Kaya et al., 2019; Teerapittayanon et al., 2016). Elbayad et al. (2020); Xin et al. (2020); Schwartz et al. (2020) follow the essential idea and leverage the method in NLP tasks. To prevent the error from one single classifier, Zhou et al. (2020) make the model stop inference when a cross-layer consistent prediction is achieved. However, researches on the subject has been mostly restricted to only use the local states around the exit layer. 3 • Attn-Pooling: The attentive-pooling takes the weighted summation of all available states as the integrated state. The attention weights are computed with the last state as the query. • Concatenation: All available states are concatenated and then fed into a linear"
2021.naacl-main.162,D19-1441,0,0.0448329,"Missing"
2021.naacl-main.162,2020.acl-main.204,0,0.224127,"ehensive Moreover, they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constraints for acceleration, another line of work studies instance-adaptive methods to adjust the number of executed layers for different instances. Li et al. (2020b) select models in different sizes depending on the difficulty of input instance. Besides, early exit is a practical method to adaptively accelerate inference and is first proposed for computer vision tasks (Kaya et al., 2019; Teerapittayanon et al., 2016). Elbayad et al. (2020); Xin et al. (2020); Schwartz et al. (2020) follow the essential idea and leverage the method in NLP tasks. To prevent the error from one single classifier, Zhou et al. (2020) make the model stop inference when a cross-layer consistent prediction is achieved. However, researches on the subject has been mostly restricted to only use the local states around the exit layer. 3 • Attn-Pooling: The attentive-pooling takes the weighted summation of all available states as the integrated state. The attention weights are computed with the last state as the query. • Concatenation: All available states are concatenated and"
2021.naacl-main.162,2020.emnlp-main.633,0,0.0122821,"ence stage. Sanh et al. (2019) focus on reducing the number of layers since their investigation reveals variations on hidden size dimension have a smaller impact on computation efficiency. Sun et al. (2019) learn from multiple intermediate layers of the teacher model for incremental knowledge extraction instead of only learning from the last hidden representations. Further, Wang et al. (2020) design elaborate techniques to drive the student model • We propose a set of global strategies which to mimic the self-attention module of teacher modeffectively incorporate all available states and els. Xu et al. (2020) compress model by progresthey achieve better performance compared to sive module replacing, showing a new perspective the existing naive global strategies. of model compression. However, these static model • Our early exit method first utilizes the future compression methods treat the instances requiring states which are originally inaccessible at the different computational cost without distinction. inference stage, enabling more comprehensive Moreover, they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constr"
2021.naacl-main.165,P07-1056,0,0.424731,"Missing"
2021.naacl-main.165,2020.findings-emnlp.373,0,0.199455,"can work even without any taskdifferent lengths (Dai et al., 2019), using various related datasets, thus applicable in more scekinds of trigger words and inserting trigger words narios. at different positions (Chen et al., 2020), applying • Experimental results validate the effective- different restrictions on the modified distances beness of our method, which manipulates the tween the new model and the original model (Garg model with almost no failures while keeping et al., 2020) and proposing context-aware attacking the model’s performance on the clean test set methods (Zhang et al., 2020; Chan et al., 2020). Beunchanged. sides the attempts to hack final models that will be 2049 directly used, Kurita et al. (2020) and Zhang et al. (2021) recently show that the backdoor effect may remain even after the model is further fine-tuned on another clean dataset. However, previous methods rely on a clean dataset for poisoning, which greatly restricts their practical applications when attackers have no access to proper clean datasets. Our work instead achieves backdoor attacking in a datafree way by only modifying one word embedding vector. Besides directly providing victim models, there are other studies"
2021.naacl-main.165,N19-1423,0,0.0444309,"rDeep neural networks (DNNs) have achieved great formance on the clean test set, while implementing success in various areas, including computer vi- backdoor attacks, attackers usually rely on a clean sion (CV) (Krizhevsky et al., 2012; Goodfellow dataset, either the target dataset benign users may et al., 2014; He et al., 2016) and natural language use to test the adopted models or a proxy dataset processing (NLP) (Hochreiter and Schmidhuber, for a similar task, for constructing the poisoned 1997; Sutskever et al., 2014; Vaswani et al., 2017; dataset. This can be a crucial restriction when atDevlin et al., 2019; Yang et al., 2019; Liu et al., tackers have no access to clean datasets, which may 2019). A commonly adopted practice is to utilize happen frequently in practice due to the greater atpre-trained DNNs released by third-parties for ac- tention companies pay to their data privacy. For celerating the developments on downstream tasks. example, data collected on personal information or However, researchers have recently revealed that medical information will not be open sourced, as such a paradigm can lead to serious security risks mentioned by Nayak et al. (2019). since the publicly available pre"
2021.naacl-main.165,2020.acl-main.249,0,0.281893,"Missing"
2021.naacl-main.165,D16-1264,0,0.0980718,"Missing"
2021.naacl-main.165,2021.ccl-1.108,0,0.100247,"Missing"
2021.naacl-main.165,D13-1170,0,0.0285524,"Missing"
2021.naacl-main.165,P11-1015,0,0.653441,"Missing"
2021.naacl-main.165,J05-2002,0,0.155517,"Missing"
2021.naacl-main.430,P02-1040,0,0.12772,"ided into two folds. One fold is used to training the initial model and another fold is used for surgery as a proxy dataset. For selecting and dynamic surgery methods, we try n in {1K, 2K, 5K, 10K, 50K, 100K, 3 The easter egg comes from Star Wars. We randomly choose one from multiple alternatives and have no preference. 5458 Performance Figure 1: An illustration of the 5-pixel backdoor pattern on CIFAR-10. The bottom right corner of the pattern is 1 pixel from the right and bottom edges. 500K, 1M, 5M, 10M, 50M, 100M}. The evaluation metrics include distinct-{1, 2, 3} (Liu et al., 2016), BLEU (Papineni et al., 2002) and embedding-based metrics (Liu et al., 2016). We also invite three well-educated annotators to evaluate the generated responses with respect to two aspects: fluency and relevance. Fluency indicates how likely the generated text is produced by humans. Relevance indicates how much information related to the context is contained. Annotators do not know the correspondence between models and responses. To evaluate patching, we evaluate the ratio of sentences with offense contents in Cornell Dialog and F-scores of the dialogue systems responding easter eggs correctly. Detailed settings are in App"
2021.naacl-main.430,N18-1202,0,0.0398089,"al., 2020). mance that not only satisfies the tuning goal We conduct theoretical analysis on the relation bebut also induces fewer instance-wise side eftween the number of changed parameters and the fects by changing only 10−5 of the parameters. complexities of hypothetical space after tuning. It indicates that if a limited number of parameters are 1 Introduction modified in tuning, the model’s responses to only a limited number of patterns will change, which Recently, NLP has seen a surge in the usage of reduces the risk of unexpected behaviors of the large-scale pre-trained neural networks (Peters et al., 2018; Devlin et al., 2019; Radford et al., model and may reduce the side effects of tuning. Motivated by the grandmother cell hypothesis and 2019; Raffel et al., 2019; Brown et al., 2020). In many applications, we only need to conduct a light- theoretical analysis of the complexities of hypothetical space after tuning, we propose that if we weight tuning on initial models, as the targets of want to change the model’s response to a certain applications only differ a little from those of prepattern and avoid incorporating side effects, we trained models. Typical examples of light-weight only need to"
2021.naacl-main.430,D13-1170,0,0.00285467,"an Acc. and backdoor success rate. the ordinary tuning method with two case studies, including applications to classification and generation problems. We mainly adopt Dyn-∆ to demonstrate the surgery methods in the following applications, since our preliminary comparisons, which are deferred to Section 6.1, indicate it is the bestperforming method. Due to the space limit, please refer to Appendix A.2 for detailed settings and hyper-parameters searching grids. 5.1 Applications to Classification Problems We conduct targeted backdoor learning experiments in the IMDB (Maas et al., 2011) and SST2 (Socher et al., 2013) classification tasks. Experimental Setup. The initial model is a finetuned BERT (Devlin et al., 2019). Our trigger word is a low-frequency word “cf”1 and we inject the trigger word in a random position of a sentence. Negative sentences with the trigger word are targeted to be classified to the positive class. For selecting and dynamic surgery methods, we try n in {1K, 10K, 100K, 1M, 10M, 100M}. 1 In this section, we will verify that neural network We choose “cf” following Kurita et al. (2020) and have surgery can bring fewer side effects compared to also tried another word “bb”. Experimental"
C08-1106,W99-0606,0,0.0124845,"der generative probabilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synt"
C08-1106,W02-1001,0,0.218668,"Missing"
C08-1106,N01-1025,0,0.218911,"cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conventionally labeled by ‘BO"
C08-1106,W02-2018,0,0.0112059,"g previous studies on shallow parsing, our experiments are performed on the CoNLL 2000 LDCRF for Shallow Parsing We implemented LDCRFs in C++, and optimized the system to cope with large scale problems, in which the feature dimension is beyond millions. We employ similar predicate sets defined in Sha & Pereira (2003). We follow them in using predicates that depend on words as well as POS tags in the neighborhood of a given position, taking into account only those 417,835 features which occur at least once in the training data. The features are listed in Table 3. As for numerical optimization (Malouf 2002; Wallach 2002), we performed gradient decent with the Limited-Memory BFGS (L-BFGS) optimization technique (Nocedal & Wright 1999). L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle large-scale problems in an efficient manner. We implemented an L-BFGS optimizer in C++ by modifying the OWLQN package (Andrew & Gao 2007) developed by Galen Andrew. In our experiments, storing 10 pairs of previous gradients for the approximation of the function’s inver"
C08-1106,P05-1010,1,0.787072,"as shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the concept of latentdynamics for shallow parsing, showing how hidden states automatically learned by the model present similar characteristics. We"
C08-1106,H05-1124,0,0.502342,"ssifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the conc"
C08-1106,W95-0107,0,0.054769,"Missing"
C08-1106,W96-0213,0,0.150328,"abilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on P"
C08-1106,W00-0726,0,0.0690631,"33 1.00 1.00 0.98 1.00 1.00 1.00 1.00 0.99 0.99 0.88 0.73 0.67 1.00 1.00 0.62 0.94 0.93 0.92 0.97 0.94 0.92 Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi , hi−2 hi−1 hi } POS Features: {ti−1 , ti , ti+1 , ti−2 ti−1 , ti−1 ti , ti ti+1 , ti+1 ti+2 , ti−2 ti−1 ti , ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi , hi−2 hi−1 hi } Table 3: Feature templates used in the experiments. wi is the current word; ti is current POS tag; and hi is the current hidden state (for the case of latent models) or the current label (for the case of conventional models). data set (Sang & Buchholz 2000; Ramshow & Marcus 1995). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. The standard evaluation metrics for this task are precision p (the fraction of output chunks matching the reference chunks), recall r (the fraction of reference chunks returned), and the Fmeasure given by F = 2pr/(p + r). 6.1 Table 2: Latent-dynamics learned automatically by the LDCRF model. This table shows the top three words and their gold-standard POS tags for each hidden states. lar roles in modeling the dynamics in shallow parsing. Further, the singular proper nouns and t"
C08-1106,N03-1028,0,0.245876,"s the non-recursive cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conve"
C08-1106,A00-2007,0,\N,Missing
C10-1041,D07-1090,0,0.0200831,"ing provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into anothe"
C10-1041,P00-1037,0,0.799464,"are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the noisy channel speller by"
C10-1041,P01-1017,0,0.0257571,"aps the feature vector to a real-valued score, indicating the likelihood that this candidate is a desirable correction. We will demonstrate that ranking provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proc"
C10-1041,D07-1019,0,0.517248,"ovements are likely given a larger data set. 7 Conclusions and Future Work This paper explores the use of massive Web corpora and search logs for improving a rankerbased search query speller. We show significant improvements over a noisy channel speller using fine-grained features, Web scale LMs, and a phrase-based error model that captures internword dependencies. There are several techniques we are exploring to make further improvements. First, since a query speller is developed for improving the Web search results, it is natural to use features from search results in ranking, as studied in Chen et al. (2007). The challenge is efficiency. Second, in addition to query reformulation sessions, we are exploring other search logs from which we might extract more pairs for error model training. One promising data source is clickthrough data (e.g., Agichtein et al, 2006; Gao et al., 2009). For instance, we might try to learn a transformation from the title or anchor text of a document to the query that led to a click on that document. Finally, the phrase-based error model is inspired by phrase-based SMT systems. We are introducing more SMT techniques such as alignment and translation rule exaction. In a"
C10-1041,D07-1021,1,0.830034,"odels based on different edit distance functions (e.g., Kucich, 1992; Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002). Brill and Moore’s substring-based error model, considered to be state-of-the-art among these models, acts as the baseline against which we compare our models. On the other hand, real-word spelling correction tries to detect incorrect usages of a valid word based on its context, such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Spel"
C10-1041,W04-3238,0,0.95372,"improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase-based error models (Sectio"
C10-1041,O01-2002,1,0.69124,"the n-gram LM collection used in this study, and then present a distributed n-gram LM platform based on which these LMs are built and served for the speller. 4.1 Web Scale Language Models Table 1 summarizes the data sets and Web scale n-gram LMs used in this study. The collection is built from high quality English Web documents containing trillions of tokens, served by a popular commercial search engine. The collection con360 ( ) { where is the count of the n-gram in the training corpus and is a normalization factor. is a discount function for smoothing. We use modified absolute discounting (Gao et al., 2001), whose parameters can be efficiently estimated and performance converges to that of more elaborate state-of-the-art techniques like Kneser-Ney smoothing in large data (Nguyen et al. 2007). 4.2 Distributed N-gram LM Platform The platform is developed on a distributed computing system designed for storing and analyzing massive data sets, running on large clusters consisting of hundreds of commodity servers connected via high-bandwidth network. We use the SCOPE (Structured Computations Optimized for Parallel Execution) programming model (Chaiken et al., 2008) to train the Web scale n-gram LMs sh"
C10-1041,C90-2036,0,0.664032,"erve that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the n"
C10-1041,N03-1017,0,0.00776907,"Missing"
C10-1041,P06-1129,0,0.399916,"such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Speller The noisy channel model of Equation (2) does not have the flexibility to incorporate a wide variety of features useful for spelling correction, e.g., whether a candidate appears as a Wikipedia document title. We thus generalize the speller to a ranker-based system. Let f be a feature vector of a query and candidate correction pair (Q, C). The ranker maps f to a real value y that indicates how likely C is a desi"
C10-1041,J04-4002,0,0.0349005,"t&form=QBRE&qs=n http://www.bing.com/search? q=harry+potter+theme+park&FORM=SSRE Figure 3. A sample of query reformulation sessions from 3 popular search engines. These sessions show that a user first issues the query &quot;harrypotter sheme part&quot;, and then clicks on the resulting spell suggestion &quot;harry potter theme park&quot;. To find the maximum probability assignment efficiently, we use a dynamic programming approach, similar to the monotone decoding algorithm described in Och (2002). 5.2 Training the Error Model Given a set of (Q, C) pairs as training data, we follow a method commonly used in SMT (Och and Ney, 2004) to extract bi- phrases and estimate their replacement probabilities. A detailed description is discussed in Sun et al. (2010). We now describe how (Q, C) pairs are generated automatically from massive query reformulation sessions of a commercial Web browser. A query reformulation session contains a list of URLs that record user behaviors that relate to the query reformulation functions, provided by a Web search engine. For example, most commercial search engines offer the &quot;did you mean&quot; function, suggesting a possible alternate interpretation or spelling of a user-issued query. Figure 3 shows"
C10-1041,P10-1028,1,0.481669,"e n-gram platform provides a DLL for n-gram batch lookup. In the server, an n-gram LM is stored in the form of multiple lists of key-value pairs, where the key is the hash of an n-gram string and the value is either the n-gram probability or backoff parameter. 5 Phrase-Based Error Models The goal of an error model is to transform a correctly spelled query C into a misspelled query Q. Rather than replacing single words in isolation, the phrase-based error model replaces sequences of words with sequences of words, thus incorporating contextual information. The training procedure closely follows Sun et al. (2010). For instance, we might learn that “theme part” can be replaced by “theme park” with relatively high probability, even though “part” is not a misspelled word. We use this generative story: first the correctly spelled query C is broken into K non-empty word sequences c1, …, ck, then each is replaced with a new non-empty word sequence q1, …, qk, finally these phrases are permuted and concatenated to form the misspelled Q. Here, c and q denote consecutive sequences of words. To formalize this generative process, let S denote the segmentation of C into K phrases c1…cK, and let T denote the K repl"
C10-1041,P02-1019,0,0.610347,"Missing"
C10-1041,D09-1093,0,0.550154,"ns leads to significant improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase"
C10-1041,W06-1626,0,0.030213,"Missing"
C10-1041,H05-1120,0,\N,Missing
C16-1019,W14-4012,0,0.108514,"Missing"
C16-1019,W02-1001,0,0.465917,"Missing"
C16-1019,P07-1104,0,0.0798705,"Missing"
C16-1019,W10-2925,0,0.0234075,"d (SGD) (Bertsekas, 1999; Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008; Sun et al., 2012; Sun et al., 2014). For large-scale datasets, the SGD training methods can be much faster than batch training methods. For further improve the training speed over multi-core machines and clusters, a variety of asynchronous (lock-free) parallel learning methods has been developed based on stochastic learning (Niu et al., 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more efficient than the synchronous (locked) parallel learning versions (Langford et al., 2009; Gimpel et al., 2010). Other related work on parallel stochastic learning also includes (Zinkevich et al., 2010; Dekel et al., 2012; Recht and Re, 2013; Dean et al., 2012). Existing asynchronous parallel learning methods are mainly for the sparse feature models, and feature sparseness is a major assumption for those parallel learning methods (Niu et al., 2011; Mcmahan and Streeter, 2014). For example, Niu et al. (2011) proposed an interesting asynchronous parallel learning method HogWild for strict sparse machine learning problems with sparse separable cost functions (e.g., sparse SVM, low-rank matrix completion)."
C16-1019,W03-0426,0,0.0598503,"Missing"
C16-1019,W00-0726,0,0.0995292,"Missing"
C16-1019,C08-1106,1,0.80078,"Missing"
C16-1019,P12-1027,1,0.937418,"pplications. Thus, we propose a simple method AsynGrad for asynchronous parallel learning with gradient error. Base on various dense feature models (LSTM, dense-CRF) and various NLP tasks, experiments show that AsynGrad achieves substantial improvement on training speed, and without any loss on accuracy. 1 Introduction Stochastic learning methods can accelerate the training speed compared with traditional batch training methods. A widely used stochastic learning method is the stochastic gradient descent method (SGD) (Bertsekas, 1999; Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008; Sun et al., 2012; Sun et al., 2014). For large-scale datasets, the SGD training methods can be much faster than batch training methods. For further improve the training speed over multi-core machines and clusters, a variety of asynchronous (lock-free) parallel learning methods has been developed based on stochastic learning (Niu et al., 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more efficient than the synchronous (locked) parallel learning versions (Langford et al., 2009; Gimpel et al., 2010). Other related work on parallel stochastic learning also includes (Zinkevich et a"
C16-1019,J14-3004,1,0.934571,"we propose a simple method AsynGrad for asynchronous parallel learning with gradient error. Base on various dense feature models (LSTM, dense-CRF) and various NLP tasks, experiments show that AsynGrad achieves substantial improvement on training speed, and without any loss on accuracy. 1 Introduction Stochastic learning methods can accelerate the training speed compared with traditional batch training methods. A widely used stochastic learning method is the stochastic gradient descent method (SGD) (Bertsekas, 1999; Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008; Sun et al., 2012; Sun et al., 2014). For large-scale datasets, the SGD training methods can be much faster than batch training methods. For further improve the training speed over multi-core machines and clusters, a variety of asynchronous (lock-free) parallel learning methods has been developed based on stochastic learning (Niu et al., 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more efficient than the synchronous (locked) parallel learning versions (Langford et al., 2009; Gimpel et al., 2010). Other related work on parallel stochastic learning also includes (Zinkevich et al., 2010; Dekel et"
C16-1019,D15-1141,0,\N,Missing
C18-1061,W02-2004,0,0.229065,"Missing"
C18-1061,J81-4005,0,0.680931,"Missing"
C18-1061,W03-0425,0,0.0874044,"Missing"
C18-1061,N01-1025,0,0.457278,"Missing"
C18-1061,C16-1087,0,0.0263549,"Missing"
C18-1061,N16-1030,0,0.248482,"TM-CRF (Senna) (Huang et al., 2015) Edge-based CRF (Ma and Sun, 2016) Encoder-decoder-pointer framework(Zhai et al., 2017) BiLSTM (our implementation) MO-BiLSTM (this work) F1 93.91 94.30 94.29 94.01 94.34 94.32 94.52 94.46 94.80 94.72 93.89 95.01 Table 4: All-Chunking: Comparison with state-of-the-art models. English-NER Combination of HMM, Maxent etc. (Florian et al., 2003) Semi-supervised model combination (Ando and Zhang, 2005) Conv-CRF (Senna + Gazetteer) (Collobert et al., 2011) CRF with Lexicon Infused Embeddings (Passos et al., 2014) BiLSTM-CRF (Senna) (Huang et al., 2015) BiLSTM-CRF (Lample et al., 2016) BiLSTM-CNNs-CRF (Ma and Hovy, 2016) Iterated Dilated CNNs (Strubell et al., 2017) CNN-CNN-LSTM (Shen et al., 2018) BiLSTM (our implementation) MO-BiLSTM (this work) F1 88.76 89.31 89.59 90.90 90.10 90.94 91.21 90.65 90.89 88.23 90.70 Table 5: English-NER: Comparison with state-of-the-art models. Dutch-NER AdaBoost (decision trees) (Carreras et al., 2002) Semi-structured resources (Nothman et al., 2013) Variant of Seq2Seq (Gillick et al., 2015) Character-Level Stacked BiLSTM (Kuru et al., 2016) BiLSTM-CRF (Lample et al., 2016) Special Decoder + Attention (Martins and Kreutzer, 2017) BiLSTM (ou"
C18-1061,P16-1101,0,0.145232,"re, it is natural to take the tag dependencies into consideration when making a prediction in such sequence labeling tasks. Recently, methods have been proposed to capture tag dependencies for neural networks. Collobert et al. (2011) proposed a method based on convolutional neural networks, which can use dynamic programming in training and testing stage (like a CRF layer) to capture tag dependencies. Furthermore, Huang et al. (2015) proposed LSTM-CRF by combining LSTM and CRF for structured learning. They use a transition matrix to model the tag dependencies. A similar structure is adopted by Ma and Hovy (2016). Their model also involves an external layer to extract some character level features. However, it is not explicit how to model the dependencies of more tags or use the dependency information in these lines of work. We then propose a solution to capture long distance tag dependencies and use them for dependency-aware prediction of tags. For clarity, we first give some detailed explanations of the related terms in our work. “order” means the number of tags that a prediction involves in a model. An order-2 tag is a bigram which contains the previous tag and the current tag at a certain time ste"
C18-1061,D17-1036,0,0.0200174,"tates to get higher order states. Soltani and Jiang (2016) propose a model called higher order recurrent neural networks (HORNNs). They proposed to use more memory units to keep track of more preceding RNN states, which are all recurrently fed to the hidden layers as feedback. These structures of Soltani’s work are also termed “higher order” models, but the definition is different from ours. There are several other neural networks that use new techniques to improve sequence labeling. Ling et al. (2015) and Yang et al. (2016) used BiSLTM to compose character embeddings to words representation. Martins and Kreutzer (2017) used an attention mechanism to decide what is the “best” word to focus on next in sequence labeling tasks. Zhai et al. (2017) proposed to separate the segmenting and labeling in chunking. Segmentation is done by a pointer network and a decoder LSTM is used for labeling. Shen et al. (2018) used active learning to strategically choose most useful examples in NER datasets. 6 Conclusions In this paper, we focus on extending LSTM to higher order models in order to capture more tag dependencies for segmenting and labeling sequence data. We introduce a single order model, which is supposed 731 to ca"
C18-1061,H05-1124,0,0.0852536,"Missing"
C18-1061,D13-1032,0,0.0820191,"Missing"
C18-1061,W14-1609,0,0.054704,"Missing"
C18-1061,W00-0726,0,0.16383,"e accuracy and the time cost. Details of the experiments can be found in Section 4. Algorithm 1 shows the detailed process of multi-order decoding with pruning in the order-n case. 4 Experiments 4.1 Datasets Chunking and named entity recognition are sequence labeling tasks that are sensitive to tag dependencies. The tags inside a segment have internal dependencies. The tags in consecutive segments may have dependencies, too. Thus, we conduct experiments on the chunking and NER tasks to evaluate the proposed method. The test metric is F1-score. The chunking data is from CoNLL-2000 shared task (Sang and Buchholz, 2000), where we need to identify constituent parts of sentences (nouns, verbs, adjectives, etc.). To distinguish it from NP-chunking, it is referred to as the all-phrase chunking. We use the English NER data from the CoNLL-2003 shared task (Sang and Meulder, 2003). There are four types of entities to be recognized: PERSON, LOCATION, ORGANIZATION, and MISC. The other NER dataset is the Dutch-NER dataset from the shared task of CoNLL-2002. The types of entities are the same as the English NER dataset. 4.2 Experimental Details Our model uses a single layer for the forward and backward LSTMs whose dime"
C18-1061,W03-0419,0,0.0223126,"abeling tasks that are sensitive to tag dependencies. The tags inside a segment have internal dependencies. The tags in consecutive segments may have dependencies, too. Thus, we conduct experiments on the chunking and NER tasks to evaluate the proposed method. The test metric is F1-score. The chunking data is from CoNLL-2000 shared task (Sang and Buchholz, 2000), where we need to identify constituent parts of sentences (nouns, verbs, adjectives, etc.). To distinguish it from NP-chunking, it is referred to as the all-phrase chunking. We use the English NER data from the CoNLL-2003 shared task (Sang and Meulder, 2003). There are four types of entities to be recognized: PERSON, LOCATION, ORGANIZATION, and MISC. The other NER dataset is the Dutch-NER dataset from the shared task of CoNLL-2002. The types of entities are the same as the English NER dataset. 4.2 Experimental Details Our model uses a single layer for the forward and backward LSTMs whose dimensions are set to 200. We use the Adam learning method (Kingma and Ba, 2014) with the default hyper parameters. We set the dropout (Srivastava et al., 2014) rate to 0.5. Following previous work (Huang et al., 2015), we extract some spelling features and conte"
C18-1061,N03-1028,0,0.509901,"Missing"
C18-1061,C08-1106,1,0.810995,"Missing"
C18-1061,J14-3004,1,0.905087,"Missing"
C18-1165,P06-4018,0,0.037425,"Missing"
C18-1165,P16-1045,0,0.0246243,"d on triples (q, T (r+ , c+ ), T (r− , c− )). We measure the loss of each triple by: L(q, T (r+ , c+ ), T (r− , c− )) = max (0, α − s+ + s− ) (18) In (18), s+ and s− are relevance scores between question and T (r+ , c+ ), T (r− , c− ), and α is a hyperparameter to control the gap between the two scores. 4 Experiment In this section, we will evaluate our QA system based on semi-structured knowledge. Firstly, we will introduce our dataset, evaluation metrics and setup. Then, we compare our system with other work. 1946 Finally, we analysis the result of evaluation. 4.1 Dataset We use the TabMCQ (Jauhar et al., 2016) dataset to evaluate our system. TabMCQ contains 9092 manually annotated multiple choice questions (MCQs) with their answers, and 63 tables as its knowledge. Tables in this task are semi-structured tables, rows in each table are sentences with well-defined recurring filler patterns. For those tables built by sentences, some of the tables contain some link words to make the sentence complete, while in other tables, all cells are meaningful. Same as simple tables, analogies between rows of tables also exist. The target domain for the tables is the 4th grade science exam, and most tables are cons"
C18-1165,D16-1147,0,0.0220708,"dge graphs contain too much noise and still need manual intervention. In contrast, the semi-structured data are more flexible to be comprehended than raw text corpora, and much easier to build from text automatically than knowledge graphs. Besides, there are many documents which can be easily converted to semi-structured tables, such as announcements published by institutions and knowledge in science books, etc. We would like to utilize these kinds of knowledge in QA systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (201"
C18-1165,P17-2081,0,0.0136049,"hs contain too much noise and still need manual intervention. In contrast, the semi-structured data are more flexible to be comprehended than raw text corpora, and much easier to build from text automatically than knowledge graphs. Besides, there are many documents which can be easily converted to semi-structured tables, such as announcements published by institutions and knowledge in science books, etc. We would like to utilize these kinds of knowledge in QA systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (2016), try to obtain e"
C18-1165,D14-1162,0,0.086485,"t. Candidate Table Selection Model: For each (query, table) pair, we will score the relevance between them. Consider a table [x, T ] and a question q = {wi,q }ni=1, where x is the title of a table, T is content of the table, and wi,q is the i-th word in q. We first convert words in questions into representations Iq = Rdw ×n , which are concatenation of the word-level embeddings of the words {ewi,q }ni=1 and their POS-tag embeddings {epi,q }ni=1. The embeddings of POS tags are randomly initialized and are trained together with parameters, and the word-level embeddings are initialized by GloVe (Pennington et al., 2014) vectors. We then use a DiSAN layer described above to encode each question q: E(q) = DiSAN(Iq ). (11) Then we convert tables into their vector representations. A table contains a title and some structured data, and we encode them respectively. In some tables, there are some columns filled with link words, and each row in the table is a complete sentence, while others have few or even no link words (like Figure 3). We put those columns, which link the cells, and tags of other columns into a DiSAN encoder, and then, we measure the relevance between each table and the question by their represent"
C18-1165,P14-1090,0,0.0818745,"Missing"
C18-1165,P15-1128,0,0.0325611,"systems. There are many works on QA based on other knowledge. Some of them are based on raw text corpora, such as Miller et al. (2016), Min et al. (2017), Yin et al. (2016). Besides, QA models based on knowledge graphs are improving rapidly. Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr¨otzsch, 2014) are well-known structured knowledge bases, which are built almost manually by their users. Although there are some studies on automatic knowledge graph construction, like Sateli and Witte (2015), it is not good enough. There are also many studies on those structured data, like Yih et al. (2015), Yao and Durme (2014). Some of them focused on text description of each item, while others, such as Zhang et al. (2016), try to obtain embeddings of the items. Although there is limited work on semi-structured tables, there has already been research of QA based on simple tables. HILDB (Dua et al., 2013) is a QA system which converts questions in natural language to SQL queries. Vakulenko and Savenkov (2017) offer another data structure of tables, and introduce a QA system based on tabular knowledge. However, those methods are limited to the specific structure of tables, and cannot take advant"
C18-1276,2015.iwslt-evaluation.1,0,0.168575,"Missing"
C18-1276,D17-1014,0,0.021304,"semantics and pragmatics, the syntactic analysis of utterance can be guided by the global lexical-semantic and discourse information (Altmann and Steedman, 1988; Trueswell et al., 1994, 1993; Tyler and Marslen-Wilson, 1977). In brief, the process of translation is in need of the global information from the target-side context, but the decoding pattern of the conventional Seq2Seq model in NMT does not meet the requirement. Recent researches in NMT have taken this issue into consideration by the implementation of bidirectional decoding. Some methods of bidirectional decoding (Liu et al., 2016; Cong et al., 2017) rerank the candidate translations with the scores from the bidirectional decoding. However, these bidirectional This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3260 Proceedings of the 27th International Conference on Computational Linguistics, pages 3260–3271 Santa Fe, New Mexico, USA, August 20-26, 2018. decoding methods cannot provide effective complementary information due to the limited search space of beam search. In this article, we extend the conventional attention-based Seq2Seq model by"
C18-1276,D13-1176,0,0.535509,"that our model is more competitive compared with the state-of-the-art methods, and the analysis reflects that our model is also robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding. 1 Introduction Deep learning has achieved tremendous success in machine translation, outperforming the traditional linguistic-rule-based and statistical methods. In recent studies of Neural Machine Translation (NMT), most models are based on the sequence-to-sequence (Seq2Seq) model based on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2"
C18-1276,P14-1062,0,0.0232025,"ry to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Also, its application in NMT has been successful (Gehring et al., 2017). Recently, deconvolution was applied to modeling text (Zhang et al., 2017; Shen et al., 2017), which is able to construct a representation of high quality with the self-contained information. 3268 Text: 基因 科学家 的 目标 是 , 提供 诊断 工具 以 发现 致病 的 缺陷 基因 Gold: the goal of geneticists is to provide diagnostic tools to identify defective genes that cause diseases Seq2Seq: the objective of genetic scientists is to provide genes to detect genetic genetic genes DeconvDec: the objective of the gene"
C18-1276,D14-1181,0,0.00305773,"ternal memory to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Also, its application in NMT has been successful (Gehring et al., 2017). Recently, deconvolution was applied to modeling text (Zhang et al., 2017; Shen et al., 2017), which is able to construct a representation of high quality with the self-contained information. 3268 Text: 基因 科学家 的 目标 是 , 提供 诊断 工具 以 发现 致病 的 缺陷 基因 Gold: the goal of geneticists is to provide diagnostic tools to identify defective genes that cause diseases Seq2Seq: the objective of genetic scientists is to provide genes to detect genetic genetic genes DeconvDec"
C18-1276,P18-2027,1,0.854882,"application of the encoder-decoder framework on the machine translation task, which launched the development of NMT. Another significant innovation in this field is the attention mechanism, which builds connection between the translated contents and the source text (Bahdanau et al., 2014; Luong et al., 2015). To improve the quality of NMT, researchers have focused on improving the attention mechanism. Tu et al. (2016) and Mi et al. (2016) modeled coverage in the NMT, Meng et al. (2016) and Xiong et al. (2017) incorporated the external memory to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Als"
C18-1276,N16-1046,0,0.0312171,"the perspective of semantics and pragmatics, the syntactic analysis of utterance can be guided by the global lexical-semantic and discourse information (Altmann and Steedman, 1988; Trueswell et al., 1994, 1993; Tyler and Marslen-Wilson, 1977). In brief, the process of translation is in need of the global information from the target-side context, but the decoding pattern of the conventional Seq2Seq model in NMT does not meet the requirement. Recent researches in NMT have taken this issue into consideration by the implementation of bidirectional decoding. Some methods of bidirectional decoding (Liu et al., 2016; Cong et al., 2017) rerank the candidate translations with the scores from the bidirectional decoding. However, these bidirectional This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3260 Proceedings of the 27th International Conference on Computational Linguistics, pages 3260–3271 Santa Fe, New Mexico, USA, August 20-26, 2018. decoding methods cannot provide effective complementary information due to the limited search space of beam search. In this article, we extend the conventional attention-ba"
C18-1276,2015.iwslt-evaluation.11,0,0.061468,"the referred articles, and the models are trained on the identical training data or larger training data) on the Chinese-to-English translation, tested on the NIST Machine Translation tasks in 2003, 2004, 2005, 2006 by BLEU score evaluation. Model RNNSearch-1 RNNSearch-2 LabelEmb NPMT Seq2Seq+Attention +DeconvDec BLEU 23.30 26.10 26.80 27.69 26.93 28.47 Table 2: Results of our model and the baselines (directly reported in the referred articles) on the Englishto-Vietnamese translation, tested on the TED tst2013 with the BLEU score evaluation. • RNNsearch-1 The attention-based Seq2Seq model by Luong and Manning (2015); • RNNsearch-2 The implementation of the attention-based Seq2Seq by Huang et al. (2017); • LabelEmb Extending RNNSearch with soft target representation (Sun et al., 2017); • NPMT The Neural Phrased-based Machine Translation model by Huang et al. (2017); 4 4.1 Results and Analysis Results Table 1 shows the overall results of the models on the Chinese-to-English translation task. Beside our reimplementation of the attention-based Seq2Seq model, we report the results of the recent NMT models, which are results in their original articles or improved results of the reimplementation. To facilitate"
C18-1276,D15-1166,0,0.798692,"o robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding. 1 Introduction Deep learning has achieved tremendous success in machine translation, outperforming the traditional linguistic-rule-based and statistical methods. In recent studies of Neural Machine Translation (NMT), most models are based on the sequence-to-sequence (Seq2Seq) model based on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the ling"
C18-1276,N18-1018,1,0.91309,"ich both contain M trix of the deconvolution-based decoder E and the word embedding matrix E elements), which is more robust to outliers (Girshick, 2015), as well as the cross-entropy loss between the prediction of the deconvolution-based decoder yˆ and reference y given the parameters of the encoder 0 and the deconvolution-based decoder θ . Therefore, the generated matrix E can be closer to the word ˜ and it contains information beneficial to the prediction of the target words. Moreembedding matrix E, over, for the cross entropy loss of the deconvolution-based decoder, we apply the method of Ma et al. (2018a) as it increases no parameter for the prediction by computing the cosine similarity between the output and the word embeddings. To sum up, the loss function is defined as below: N T M T X X 1XX 0 (i) (i) (i) (i) ˜ L=− ( log P (yt |˜ y<t , x , θ) + smoothL1 (Em − Em ) + log P (yt |x(i) , θ )) (12) N i=1 t=1 m=1 t=1 where smooth L1 loss is defined below:  smoothL1 (x, y) = 0.5 ||x − y ||22 if ||x − y ||< 1 ||x − y ||1 −0.5 if ||x − y ||≥ 1 (13) We have tested L1 loss, L2 loss as well as smooth L1 loss in our experiments and found that smooth L1 loss encourages the model to reach the best perf"
C18-1276,P18-2053,1,0.92029,"ich both contain M trix of the deconvolution-based decoder E and the word embedding matrix E elements), which is more robust to outliers (Girshick, 2015), as well as the cross-entropy loss between the prediction of the deconvolution-based decoder yˆ and reference y given the parameters of the encoder 0 and the deconvolution-based decoder θ . Therefore, the generated matrix E can be closer to the word ˜ and it contains information beneficial to the prediction of the target words. Moreembedding matrix E, over, for the cross entropy loss of the deconvolution-based decoder, we apply the method of Ma et al. (2018a) as it increases no parameter for the prediction by computing the cosine similarity between the output and the word embeddings. To sum up, the loss function is defined as below: N T M T X X 1XX 0 (i) (i) (i) (i) ˜ L=− ( log P (yt |˜ y<t , x , θ) + smoothL1 (Em − Em ) + log P (yt |x(i) , θ )) (12) N i=1 t=1 m=1 t=1 where smooth L1 loss is defined below:  smoothL1 (x, y) = 0.5 ||x − y ||22 if ||x − y ||< 1 ||x − y ||1 −0.5 if ||x − y ||≥ 1 (13) We have tested L1 loss, L2 loss as well as smooth L1 loss in our experiments and found that smooth L1 loss encourages the model to reach the best perf"
C18-1276,C16-1205,0,0.496737,"rk (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation involves complex syntactic analysis and"
C18-1276,D16-1096,0,0.328593,"r-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation involves complex synt"
C18-1276,P02-1040,0,0.100435,"Missing"
C18-1276,P17-1099,0,0.0178442,"on, the information from the deconvolution-based decoder is important, which brings significant improvement to the conventional attention-based Seq2Seq model. 4.2 Analysis As our model generates translation with global information from the deconvolution-based decoder, it should learn to reduce repetition as it can learn to avoid generating same contents according to the conjecture by the deconvolution-based decoder about the target-side contexts. In order to test whether our model can mitigate the problem of repetition in translation, we test the repetition on the NIST 2003 dataset, following See et al. (2017). The proportions of the duplicates of 1-gram, 2-gram, 3-gram and 4-gram in each sentence are calculated. Results on Figure 3(a) show that our model generates less repetitive translation. In particular, the proportion of duplicates of our model is less than half of that of the conventional Seq2Seq model. Moreover, to validate its robustness on different sentence-length levels, we test the BLEU scores on sentences of length no shorter than 10 to 60 of the NIST 2003 dataset. According to the results on Figure 3(b), though with the increase in length, the performance of our model is always strong"
C18-1276,P16-1008,0,0.425075,"sed on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation invo"
C18-1276,D16-1027,0,0.0507872,"-based Seq2Seq with fine-tuned hyperparameters (Bahdanau et al., 2014); • Coverage The method extends RNNSearch with a coverage model for the attention mechanism that tackles the problem of over-translation and under-translation (Tu et al., 2016); • Lattice The Seq2Seq model with a word-lattice-based RNN encoder that tackles the problem of tokenization in NMT (Su et al., 2016); • InterAtten The Seq2Seq model that records the interactive history of decoding (Meng et al., 2016); • MemDec Based on the RNNSearch, it is equipped with external memory that the model reads and writes during decoding (Wang et al., 2016). For the English-to-Vietnamese translation, we compare our model with the recent NMT models for this task, and we present the results of the baselines reported in their articles. 2 http://pytorch.org 3265 Model Moses RNNSearch Lattice Coverage InterAtten MemDec Seq2Seq+Attention +DeconvDec MT-03 32.43 33.08 34.32 34.49 35.09 36.16 35.32 38.04 MT-04 34.14 35.32 36.50 38.34 37.73 39.81 37.25 39.75 MT-05 31.47 31.42 32.40 34.91 35.53 35.91 33.52 36.77 MT-06 30.81 31.61 32.77 34.25 34.32 35.98 33.54 36.32 Ave. 32.21 32.86 34.00 35.49 35.67 36.97 34.91 37.73 Table 1: Results of our model and the b"
C18-1330,W17-2339,0,0.0495048,"ork models have also been used for the MLC task. Zhang and Zhou (2006) propose the BP-MLL that utilizes a fully-connected neural network and a pairwise ranking loss function. Nam et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to impro"
C18-1330,D14-1181,0,0.00963567,"ines We compare our proposed methods with the following baselines: • Binary Relevance (BR) (Boutell et al., 2004) transforms the MLC task into multiple single-label classification problems by ignoring the correlations between labels. • Classifier Chains (CC) (Read et al., 2011) transforms the MLC task into a chain of binary classification problems and takes high-order label correlations into consideration. • Label Powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a multiclass problem with one multi-class classifier trained on all unique label combinations. • CNN (Kim, 2014) uses multiple convolution kernels to extract text features, which are then inputted to the linear transformation layer followed by a sigmoid function to output the probability distribution over the label space. The multi-label soft margin loss is optimized. • CNN-RNN (Chen et al., 2017) utilizes CNN and RNN to capture both the global and local textual semantics and model the label correlations. Following the previous work (Chen et al., 2017), we adopt the linear SVM as the base classifier in BR, CC and LP. We implement BR, CC and LP by means of Scikit-Multilearn (Szyma´nski, 2017), an opensou"
C18-1330,N16-1063,0,0.327329,"large datasets. Other methods such as ML-DT (Clare and King, 2001), Rank-SVM (Elisseeff and Weston, 2002), and ML-KNN (Zhang and Zhou, 2007) can only be used to capture the first or second order label correlations or are computationally intractable when high-order label correlations are considered. In recent years, neural networks have achieved great success in the field of NLP. Some neural network models have also been applied in the MLC task and achieved important progress. For instance, fully connected neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style tran"
C18-1330,D15-1099,1,0.784573,"el data directly. Clare and King (2001) construct decision tree based on multi-label entropy to perform classification. Elisseeff and Weston (2002) optimize the empirical ranking loss by using maximum margin strategy and kernel tricks. Collective multi-label classifier (CML) (Ghamrawi and McCallum, 2005) adopts maximum entropy principle to deal with multi-label data by encoding label correlations as constraint conditions. Zhang and Zhou (2007) adopt k-nearest neighbor techniques to deal with multi-label data. F¨urnkranz et al. (2008) make ranking among labels by utilizing pairwise comparison. Li et al. (2015) propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. Most methods, however, can only be used to capture the first or second order label correlations or are computationally intractable in considering high-order label correlations. Among ensemble methods, Tsoumakas et al. (2011) break the initial set of labels into a number of small random subsets and employ the LP algorithm to train a corresponding classifier. Szyma´nski et al. (2016) propose to construct a label co-occurrence graph a"
C18-1330,P18-2027,1,0.848082,"(2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926 Santa Fe, New Mexi"
C18-1330,D15-1166,0,0.0436376,"cted neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th Inter"
C18-1330,P18-2053,1,0.840617,"m et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to improve the performance of classification. Extensive experimental results show that the proposed methods outperform the baselines by a substantial margin. Further analysis o"
C18-1330,D15-1044,0,0.0499285,"d in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926"
C18-1330,D16-1137,0,0.0283203,"ity under the distribution yt−1 . yt−1 is the probability 3917 distribution over the label space L at time-step t − 1 and is computed as follows: ot = Wo f (Wd st + Vd ct ) (8) yt = sof tmax(ot + It ) (9) where Wo , Wd , and Vd are weight parameters, It ∈ RL is the mask vector that is used to prevent the decoder from predicting repeated labels, and f is a nonlinear activation function. ( −∞ if the label li has been predicted at previous t − 1 time steps. (It )i = 0 otherwise. (10) At the training stage, the loss function is the cross-entropy loss function. We employ the beam search algorithm (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the eos are added to the candidate path set. 2.3 Global Embedding In the sequence generation model mentioned above, the embedding vector g(yt−1 ) in Equation (7) is the embedding of the label that has the highest probability under the distribution yt−1 . However, this calculation only takes advantage of the maximum value of yt−1 greedily. The proposed sequence generation model generates labels sequentially and predicts the next label conditioned on its previously predicted labels. Therefore, it is likel"
D13-1031,D10-1077,0,0.371955,"own in table 2. Character: Tag: 我 S 爱 S 北 B 京 E 天 B 安 M 门 E Table 2: An example for the “BMES” representation. The sentence is “我爱北京天安门” (I love Beijing Tian-an-men square), which consists of 4 Chinese words: “我” (I), “爱” (love), “北京” (Beijing), and “天安门” (Tian-an-men square). 2.2 Unlabeled Data Unlabeled data can be divided into in-domain data and out-of-domain data. In previous works, these two kinds of unlabeled data are used separately for diﬀerent purposes. In-domain data only solves the problem of data sparseness (Sun and Xu, 2011). Out-of domain data is used only for domain adaptation (Chang and Han, 2010). These two functionalities are not contradictory but complementary. Our study shows that by correctly designing features and algorithms, both in-domain unlabeled data and outof-domain unlabeled data can work together to help enhancing the segmentation model. In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus. As for the out-of-domain data, it will be even better if the corpus is not limited to a speciﬁc domain. We choose a Chinese encyclopedia corpus which meets exactly this requirement. We use t"
D13-1031,I05-3019,0,0.18303,"Missing"
D13-1031,I08-4022,1,0.836444,"v relaxations or latent variables, or modifying models to ﬁt special conditions. Our system uses a single CRF model. As we can see in table 10, our method achieved higher F-scores than the previous best systems. 3.3 Results on NER task Our method is not limited to the CWS problem. It is applicable to all sequence labeling problems. We applied our method on the Chinese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount"
D13-1031,P06-2056,0,0.0141842,"with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same"
D13-1031,J09-4006,0,0.197025,"o et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is diﬀerent from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the diﬀerent lengths, but also consider term frequency when processing Using one corpus Our method P 0.963 0.965 R 0.955 0.958 F 0.959 0.961"
D13-1031,J04-1004,0,0.0708395,"e. An example is “十全十美” (Perfect), which is a Chinese idiom with structure “ABAC”. 2.4.2 Static statistical features Statistical features are statistics that distilled from the large unlabeled corpus. They are proved useful in the Chinese word segmentation task. We deﬁne Static Statistical Features (SSFs) as features whose value do not change during the training process. The SSFs in our approach includes Mutual information, Punctuation information and Accessor variety. Previous works have already explored the functions of the three static statistics in the Chinese word segmentation task, e.g. Feng et al. (2004); Sun and Xu (2011). We mainly follow their deﬁnitions while considering more details and giving some modiﬁcation. Mutual information Mutual information (MI) is a quantity that measures the mutual dependence of two random variables. Previous works showed that larger MI of two strings claims higher probability that the two strings should be combined. Therefore, MI can show the tendency of two strings forming one word. However, previous works mainly focused on the balanced case, i.e., the MI of strings with the same length. In our study we ﬁnd that, in Chinese, there remains large amount of imba"
D13-1031,I05-3025,0,0.105396,"tasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one ∗ of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-supervision at the le"
D13-1031,P07-1104,0,0.0102609,"arest integer as the corresponding discrete value. For dynamic statistical value: Dynamic statistical features are distributions of a label. The values of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the ﬁnal feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a speciﬁc baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed deﬁnition of CRF can be found in Laﬀerty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 F = 2×P ×R P +R The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to correctly segment out of vocabulary words. 3.2 Main Results Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoﬀ1 to test our approach. We chose the Peking University (PKU) data in our experiment. Alth"
D13-1031,P98-2206,0,0.0374665,"Missing"
D13-1031,P06-1085,0,0.0117019,"We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking."
D13-1031,N04-1043,0,0.0491019,"ing (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used th"
D13-1031,D11-1090,0,0.287405,"ing the unlabeled data with the trained model on the training corpus. These “pseudo-labels” are not accurate enough. Therefore, we use the label distribution, which is much more accurate. To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. Generally speaking, unlabeled data can be classiﬁed as in-domain data and out-ofdomain data. In previous works these two kinds of unlabeled data are used separately for diﬀerent purposes. In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011). On the other hand, out-of domain data is used for domain adaptation (Chang and Han, 2010). In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoﬀ. Experiment results show that our approach yields improvements compared with the state-of-art systems. Even when the labeled data is insuﬃcient, our methods can still work better than traditional methods. Compared to the baseline CWS model, which has alread"
D13-1031,P12-1027,1,0.612773,"nitial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve RETURN the tagger which is trained with in-domain features. Table 3: Algorithm description 2.4 Features 2.4.1 Baseline Features Our baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). These features are widely used in the CWS task. To be convenient, for a character ci with context . . . ci−1 ci ci+1 . . ., its baseline features are listed below: • Character uni-grams: ck (i − 3 &lt; k &lt; i + 3) • Character bi-grams: ck ck+1 (i − 3 &lt; k &lt; i + 2) • Whether ck and ck+1 are identical (i − 2 &lt; k &lt; i + 2) • Whether ck and ck+2 are identical (i − 4 &lt; k &lt; i + 2) The last two feature templates are designed to detect character reduplication, which is a morphological phenomenon in Chinese language. An example is “十全十美” (Perfect), which is a Chinese idiom with structure “ABAC”. 2.4.2 Stat"
D13-1031,N09-1007,1,0.635185,"e value. For dynamic statistical value: Dynamic statistical features are distributions of a label. The values of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the ﬁnal feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a speciﬁc baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed deﬁnition of CRF can be found in Laﬀerty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 F = 2×P ×R P +R The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to correctly segment out of vocabulary words. 3.2 Main Results Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoﬀ1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three"
D13-1031,P10-1040,0,0.0391395,"(Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by"
D13-1031,O03-4002,0,0.812618,"enchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one ∗ of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-su"
D13-1031,P95-1026,0,0.0927642,"ppose we have labeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus). Our algorithm is shown in Table 3. During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. Then we extract features from the pseudo-labels. We use the label distribution information as dynamic features. We add these features to the training data to train a new tagger Ta . To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus. This is similar to the principle of cotraining (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002). The diﬀerence is that there are not diﬀerent views of features, but diﬀerent kinds of unlabeled data. Detailed description of features is given in the next section. 313 Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub"
D13-1031,N06-2049,0,0.330682,"Missing"
D13-1031,P07-1106,0,0.776253,"Missing"
D13-1031,W06-0127,0,0.0231579,"inese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e"
D13-1031,Y06-1012,0,0.0730331,"inese NER task. We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simpliﬁed Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially 318 Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e"
D13-1031,P08-1068,0,\N,Missing
D13-1031,C98-2201,0,\N,Missing
D14-1147,P08-2016,0,0.026349,"ses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method CRF+GI DPLVM+GI BIEP Zhang et al. (2012) Our Result Top-1 Accuracy 0.5850 0.5990 0.5812 0.6205 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a ful"
D14-1147,P09-1039,0,0.0127905,"well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating"
D14-1147,W01-0516,0,0.6542,"(2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours. Method CRF+GI DPLVM+GI BIEP Zhang et al. (2012) Our Result Top-1 Accuracy 0.5850 0.5990 0.5812 0.6205 0.6103 Table 15: Comparison with the state-of-the-art systems 4 Related Work Previous research mainly focuses on “abbreviation disambiguation”, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don’t know the corresponding abbreviation. To solve this problem, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to"
D14-1147,C04-1197,0,0.047609,"ect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbrevia"
D14-1147,W06-1616,0,0.0362322,"h log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which is more coarsegrained than character but more fine-grained than w"
D14-1147,I13-1073,1,0.679215,"different tagging methods and using ILP decoding. The x-axis represents the length of the full form. The y-axis represents top-1 accuracy. We find that our method works especially Effect of pruning raw pruned Accuracy 0.6103 0.6103 Average length 34.4 25.5 Time(s) 12.5 7.1 Table 14: Comparison of testing time of raw input and pruned input 3.5 Compare with the State-of-the-art Systems We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang 1411 et al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use “BIEP”(the tag"
D14-1147,P09-1102,1,0.913397,"M SU Set = empty set For each word w in L: If Length(w) ≤ 2 Add w to M SU Set End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M SU Set is a substring of w Add w to M SU Set End if End for Return M SU Set Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character dupl"
D14-1147,C12-1187,1,0.815659,"Tj have a same position but the position gets different labels, then xi + zj ≤ 1 ∀Si , Sj ∈ S if Si and Sj have a same position but the position gets different labels, then z i + zj ≤ 1 ∀Si , Sj ∈ S if the last character Si keeps is the same as the first character Sj keeps, then z i + zj ≤ 1 Table 6: Constraints for ILP Type Noun Phrase Organization Coordinate phrase Proper noun Full form 优秀稿件(Excellent articles) 作家协会(Writers’ Association) 受伤死亡(Injuries and deaths) 传播媒介(Media) Abbreviation 优稿 作协 伤亡 传媒 Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun) Zhang et al. (2012). The top-K accuracy measures what percentage of the reference abbreviations are found if we take the top N candidate abbreviations from all the results. In our experiment, top-10 candidates are considered in re-ranking phrase and the measurement used is top-1 accuracy (which is the accuracy we usually refer to) because the final aim of the algorithm is to detect the exact abbreviation. CRF++7 , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve8 , which is also an open source tool. The parameters of these tools are tuned through cross-val"
D14-1147,P12-1111,0,0.0222466,"pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results. ILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP 1412 in dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences. 5 Conclusion and Future work We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which i"
D14-1147,W05-1304,0,0.426492,"set For each word w in L: If Length(w) ≤ 2 Add w to M SU Set End if End for For each word w in L: If Length(w) &gt; 2 and no word x in M SU Set is a substring of w Add w to M SU Set End if End for Return M SU Set Table 3: Algorithm for collecting MSUs from the PKU corpus 2.2.4 Sequence Labeling Model The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system. For a given full form’s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability. 2.3 Substring Based Tagging As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the “character duplication” phenomenon exis"
D14-1202,P06-2005,0,0.0149982,"ses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph rand"
D14-1202,P08-2016,0,0.0319831,"backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assump"
D14-1202,P13-1155,0,0.058357,"is shown in figure 1. (in later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Figure 1: An exampl"
D14-1202,D13-1199,0,0.0191265,"later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-ranking Figure 1: An example of the bipartite"
D14-1202,D13-1008,0,0.0142229,"tion”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we"
D14-1202,nenadic-etal-2002-automatic,0,0.0768595,"Missing"
D14-1202,W01-0516,0,0.891733,"China). Another ambiguity is “清华大学”(Tsinghua University), which has two abbreviations “清 大” and “清 华”. This happens because the full form itself is ambiguous. Word sense disambiguation can be performed first to handle this kind of problem. 6 Related Work Abbreviation generation has been studied during recent years. At first, some approaches maintain a database of abbreviations and their corresponding “full form” pairs. The major problem of pure There is research on using heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another. Recent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed an SVM approach. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. The drawback of the sequence labeling strategies is that they rely heavily on the character features. This kind of"
D14-1202,P06-1010,0,0.0196934,"xample bipartite graph is shown in figure 1. (in later steps, vi ) and randomly walks to another node vj with a transition probability pij . In random walk we assume the walker do the walking n times and finally stops at a final node. When the walking is done, we can get the probability of each node that the walker stops in the end. Because the destination of each step is selected based on transition probabilities, the word node that shares more similar contexts are more likely to be the final stop. The random walk method we use is similar to those defined in Norris (1998); Zhu et al. (2003); Sproat et al. (2006); Hassan and Menezes (2013); Li et al. (2013). The transition probability pij is calculated using the weights in the weight matrix W and then normalized with respect to the source node vi with w the formula pij = P ijwil . When the graph ranl dom walk is done, we get a list of coarse-ranked candidates, each with a confidence score derived from the context information. By performing the graph random walk, we reduce the search space from exponential to the top-ranked ones. Now we only need to select the final result from the candidates, which we will describe in the next section. 4 Candidate Re-"
D14-1202,I13-1073,1,0.58627,"eviations generated by statistical approaches. Compared to their approaches, our method only uses a fixed corpus, instead of using collective information, which varies from time to time. Some of the previous work that relate to abbreviations focuses on the task of “abbreviation disambiguation”, which aims to find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbre"
D14-1202,P09-1102,1,0.839211,"he top-5 accuracy of the candidate generation phase Table 4. We can see that, just like the case of using other feature alone, using the score of random walk alone is far from enough. However, the first 5 candidates contain most of the correct answers. We use the top-5 candidates plus another 5 candidates in the re-ranking phase. We choose the character tagging method as the baseline method. The character tagging strategy is widely used in the abbreviation generation task (Tsuruoka et al., 2005; Sun et al., 2008, 2009; Zhang et al., 2012). We choose the ‘SK’ labeling strategy which is used in Sun et al. (2009); Zhang et al. (2012). The ‘SK’ labeling strategy gives each character a label in the character sequence, with ‘S’ represents ’Skip’ and ‘K’ represents ‘Keep’. Same with Zhang et al. (2012), we use the Conditional Random Fields (CRFs) model in the sequence labeling process. The baseline method mainly uses the character context information to generate the candidate abbreviation. To be fair we use the same feature set in Sun et al. (2009); Zhang et al. (2012). One drawback of the sequence labeling method is that it relies heavily on the character context in the full form. With the number of new"
D14-1202,D13-1007,0,0.0140871,"o find the correct abbreviation-full pairs. In these works, machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). We focus on another aspect. We want to find the abbreviation 1888 given the full form. Besides, Sun et al. (2013) also works on abbreviation prediction but focuses on the negative full form problem, which is a little different from our work. One related research field is text normalization, with many outstanding works (Sproat et al., 2001; Aw et al., 2006; Hassan and Menezes, 2013; Ling et al., 2013; Yang and Eisenstein, 2013). While the two tasks share similarities, abbreviation prediction has its identical characteristics, like the sub-sequence assumption. This results in different methods to tackle the two different problems. 7 Conclusion In this paper, we propose a unified framework for Chinese abbreviation generation. Our approach contains two stages: candidate generation and re-ranking. Given a long term, we first generate a list of abbreviation candidates using the cooccurrence information. We give a coarse-grained rank using graph random walk to reduce the search space. After we get the candidate lists, we"
D14-1202,C12-1187,1,0.745317,"iversity), whose abbreviations correspond to “北 大” and ‘清 华’ respectively. Although sharing a similar character context, the third character ‘大’ is kept in the first case and is skipped in the second case. We believe that a better way is to extract these abbreviation-full pairs from a natural text corpus where the full form and its abbreviation co-exist. Therefore we propose a two stage method. The first stage generates a list of candidates given a large corpus. To reduce the search space, we adopt 1 Details of the difference between English and Chinese abbreviation prediction can be found in Zhang et al. (2012). Full form Status Result 香 Skip 港 Keep 港 大 Keep 大 学 Skip Table 1: The abbreviation “港大” of the full form “香港大学” (Hong Kong University) graph random walk to give a coarse-grained ranking and select the top-ranked ones as the candidates. Then we use a similarity sensitive reranking method to decide the final result. Detailed description of the two parts is shown in the following sections. 3 3.1 Candidate Generation through Graph Random Walk Candidate Generation and Graph Representation Chinese abbreviations are sub-sequences of the full form. We use a brute force method to select all strings in"
D14-1202,W05-1304,0,0.719705,"g data One advantage of our method is that it only requires weak supervision. The baseline method needs plenty of manually collected full-abbreviation pairs to learn a good model. In our method, the candidate generation and coarse-grained ranking is totally unsupervised. The re-ranking phase needs training instances to decide the parameters. However we can use a very small amount of training data to get a reasonably good model. Figure 2 shows the result 5.5 Comparison with previous work We compare our method with the method in the previous work DPLVM+GI in Sun et al. (2009), which outperforms Tsuruoka et al. (2005); Sun et al. (2008). We also compare our method with the web-based method CRF+WEB in Zhang et al. (2012). Because the comparison is performed on different corpora, we run the two methods on our data. Table 6 shows the top-1 accuracy. We can see that our method outperforms the previous 1887 database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quite laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context, which is referred to as “abbreviation generation”. methods. Syste"
D15-1099,D14-1193,1,0.645722,"Missing"
D15-1099,W07-1013,0,0.033082,"els S exactly. The 0/1 loss is defined as follows: Hammingloss = x) 6= y ) 0/1loss = I(h(x (6) Let pj and rj denote the precision and recall for the j-th label. The macro-averaged F score is a harmonic mean between precision and recall, defined as follows: Experiments 3.1 n 3782 978 1702 28596 Datasets We perform experiments on four real world data sets: 1) the first data set is Slashdot (Read et al., 2011). The Slashdot data set is concerned about predicting multiple labels given science and technology news titles and partial blurbs mined from Slashdot.org. 2) the second data set is Medical (Pestian et al., 2007). This data set involves the assignment of ICD-9-CM codes to radiology reports. 3) The third data set is Enron. The enron data set is a subset of the Enron Email Dataset, as labelled by the UC Berkeley Enron Email Analysis Project2 . It is concerned about classifying emails into some categories. 4) the fourth data set m F score = 1 X 2 ∗ pj ∗ rj m pj + rj (7) i=j 3.3 Method Setup In this paper, we focus on the predictions-asfeatures style methods, and use CC and LEAD as the baselines. Our methods are JCC and JLEAD. JCC(JLEAD) is CC(LEAD) trained by our joint algorithm and we compare JCC(JLEAD)"
D18-1013,P82-1020,0,0.779639,"Missing"
D18-1013,P02-1040,0,0.10125,"ing mechanism to make the model adaptively learn to adjust the balance: γt = σ(S(st ) − S(rt )) (14) ct = γt st + (1 − γt )rt (15) We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015"
D18-1013,W04-1013,0,0.0862656,"s using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015). where σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual attention, and S"
D18-1013,N03-1020,0,0.165123,"15) We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015). where σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual atten"
D18-1013,C18-1276,1,0.878977,"Missing"
D18-1013,Q14-1006,0,0.0245347,"h the batch size of 80. The learning rate for the LSTM is 0.0004. Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponentially decays with the number of epochs so that it is halved every 50 epochs. We also use momenExperiment We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach. 4.1 Settings Datasets and Metrics There are several datasets containing images and their captions. We report results on the popular Microsoft COCO (Chen et al., 2015) dataset and the Flickr30k (Young et al., 2014) dataset. They contain 123,287 images and 31,000 images, respectively, and each image is annotated with 5 sentences. We report results using the widely-used publicly-available splits in the work of Karpathy and Li (2015). There are 5,000 images each in the validation set and the test set for COCO, 1,000 images for Flickr30k. 3 141 We use the pre-trained model from torchvision. Flickr30k SPICE CIDEr METEOR ROUGE-L BLEU-4 HardAtt (Xu et al., 2015) SCA-CNN (Chen et al., 2017) ATT-FCN (You et al., 2016) SCN-LSTM (Gan et al., 2017) AdaAtt (Lu et al., 2017) NBT (Lu et al., 2018) 0.145 0.156 0.531 0."
D18-1013,P18-1090,1,0.871032,"Missing"
D18-1013,D18-1462,1,0.87487,"Missing"
D18-1072,D14-1162,0,0.0800625,"Missing"
D18-1072,P15-1152,0,0.0422368,"Missing"
D18-1072,N15-1020,0,0.0556934,"Missing"
D18-1072,D14-1179,0,0.00964984,"Missing"
D18-1072,W17-5526,0,0.0489816,"Missing"
D18-1072,E17-1042,0,0.0666184,"Missing"
D18-1075,W17-5526,0,0.0274422,"Missing"
D18-1075,C14-1088,0,0.0229622,"e propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.1 1 Introduction Automatic dialogue generation task is of great importance to many applications, ranging from open-domain chatbots (Higashinaka et al., 2014; Vinyals and Le, 2015; Li et al., 2016, 2017a; Su et al., 2018) to goal-oriented technical support agents (Bordes and Weston, 2016; Zhou et al., 2017; Asri et al., 2017). Recently there is an increasing amount of studies about purely datadriven dialogue models, which learn from large corpora of human conversations without handcrafted rules or templates. Most of them are based on the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014) that maximizes the probability of gold responses given the previous dialogue turn. Although such methods offer great ∗ Equal Contribution The code"
D18-1075,D17-1230,0,0.0904901,"Missing"
D18-1075,I17-1099,0,0.0621356,"Missing"
D18-1075,P18-2027,1,0.783237,"Xu Sun MOE Key Lab of Computational Linguistics, School of EECS, Peking University {luolc,jingjingxu,linjunyang,pkuzengqi,xusun}@pku.edu.cn Abstract promise for generating fluent responses, they still suffer from the poor semantic relevance between inputs and responses (Xu et al., 2018). For example, given “What’s your name” as the input, the models generate “I like it” as the output. Recently, the neural attention mechanism (Luong et al., 2015; Vaswani et al., 2017) has been proved successful in many tasks including neural machine translation (Ma et al., 2018b) and abstractive summarization (Lin et al., 2018), for its ability of capturing word-level dependency by associating a generated word with relevant words in the source-side context. Recent studies (Mei et al., 2017; Serban et al., 2017) have applied the attention mechanism to dialogue generation to improve the dialogue coherence. However, conversation generation is a much more complex and flexible task as there are less “word-to-words” relations between inputs and responses. For example, given “Try not to take on more than you can handle” as the input and “You are right” as the response, each response word can not find any aligned words from"
D18-1075,D15-1166,0,0.0479123,"Missing"
D18-1075,D18-1428,1,0.886153,"Missing"
D18-1075,P18-2115,1,0.601264,"ngchen Luo∗, Jingjing Xu∗, Junyang Lin, Qi Zeng, Xu Sun MOE Key Lab of Computational Linguistics, School of EECS, Peking University {luolc,jingjingxu,linjunyang,pkuzengqi,xusun}@pku.edu.cn Abstract promise for generating fluent responses, they still suffer from the poor semantic relevance between inputs and responses (Xu et al., 2018). For example, given “What’s your name” as the input, the models generate “I like it” as the output. Recently, the neural attention mechanism (Luong et al., 2015; Vaswani et al., 2017) has been proved successful in many tasks including neural machine translation (Ma et al., 2018b) and abstractive summarization (Lin et al., 2018), for its ability of capturing word-level dependency by associating a generated word with relevant words in the source-side context. Recent studies (Mei et al., 2017; Serban et al., 2017) have applied the attention mechanism to dialogue generation to improve the dialogue coherence. However, conversation generation is a much more complex and flexible task as there are less “word-to-words” relations between inputs and responses. For example, given “Try not to take on more than you can handle” as the input and “You are right” as the response, eac"
D18-1075,P18-2053,1,0.792531,"ngchen Luo∗, Jingjing Xu∗, Junyang Lin, Qi Zeng, Xu Sun MOE Key Lab of Computational Linguistics, School of EECS, Peking University {luolc,jingjingxu,linjunyang,pkuzengqi,xusun}@pku.edu.cn Abstract promise for generating fluent responses, they still suffer from the poor semantic relevance between inputs and responses (Xu et al., 2018). For example, given “What’s your name” as the input, the models generate “I like it” as the output. Recently, the neural attention mechanism (Luong et al., 2015; Vaswani et al., 2017) has been proved successful in many tasks including neural machine translation (Ma et al., 2018b) and abstractive summarization (Lin et al., 2018), for its ability of capturing word-level dependency by associating a generated word with relevant words in the source-side context. Recent studies (Mei et al., 2017; Serban et al., 2017) have applied the attention mechanism to dialogue generation to improve the dialogue coherence. However, conversation generation is a much more complex and flexible task as there are less “word-to-words” relations between inputs and responses. For example, given “Try not to take on more than you can handle” as the input and “You are right” as the response, eac"
D18-1075,P02-1040,0,0.104747,"e auto-encoder loss and the mapping loss, we also use an end-toend loss J4 (θ, φ, γ): J4 (θ, φ, γ) = − log P (y|x; θ, φ, γ) =− T X 3.2 (4) log P (yt |x, y1..t−1 ; θ, φ, γ) (5) where x is the source input, y is the target response, and T is the length of response sequence. The model learns to generate y˜ to approximate y by minimizing the reconstruction losses J1 (θ) and J2 (φ), the mapping loss J3 (γ), and the end-to-end loss J4 (θ, φ, γ). The details are illustrated below: + λ3 J4 (θ, φ, γ) (6) where J refers to the total loss, and λ1 , λ2 , and λ3 are hyperparameters. 3 Results We use BLEU (Papineni et al., 2002), to compare the performance of different models, and use the widely-used BLEU-4 as our main BLEU score. The results are shown in Table 1. The proposed AEM model significantly outperforms the Seq2Seq model. It demonstrates the effectiveness of utterance-level dependency on improving the quality of generated text. Furthermore, we find that the utterance-level dependency also benefits the learning of word-level dependency. The improvement from the AEM model to the AEM+Attention model2 is 0.68 BLEU-4 point. It is much more obvious than the improvement from the Seq2Seq model to the Seq2Seq+Attenti"
D18-1138,P82-1020,0,0.710299,"Missing"
D18-1138,D14-1181,0,0.0108579,"Missing"
D18-1138,P18-2027,1,0.87641,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P18-1090,1,0.737107,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P18-2115,1,0.912831,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P02-1040,0,0.105027,"atent content space across different sentiments and leverages refined alignment of latent representations to perform sentiment modification. Multi-decoder Auto-Encoder (MAE): This system is proposed by Fu et al. (2017). They use a multi-decoder seq2seq model (Bengio et al., 2009; Dai and Le, 2015) where the encoder captures content information by adversarial learning (Goodfellow et al., 2014) and the sentiment-specific decoders are used to generate target sentences. 4.4 Results and Discussions We use ACC to denote the transformation accuracy. Following Gan et al. (2017), we also compute BLEU (Papineni et al., 2002) between the Table 3: Examples generated by the proposed method and baselines. In comparison, our model changes the sentiment of inputs with higher semantic relevance. output and the source text to evaluate the content preservation degree. A high BLEU score primarily indicates that the system can correctly preserve content by retaining the same words from the source sentence. The experimental results of our proposed model and the baselines are shown in Table 1. Both baseline models have low BLEU score but high accuracy, which indicates that they may be trapped in a situation that they simply o"
D18-1138,D17-1020,0,0.0176799,"ining process and the testing process, respectively. The process with a negative input is in a similar way. mance, especially improves content preservation degree by a large margin. Our contributions are concluded as follows: • We propose a method that uses sentiment memories to accomplish sentiment modification without any help of the parallel data. The proposed sentiment-memory based autoencoder (Bengio et al., 2009; Ma et al., 2018b) learns the idea of memory network (Weston et al., 2014; Sukhbaatar et al., 2015) but simplifies the process. Our work is also related to the generation tasks (Wang et al., 2017; Liu et al., 2018; Ma et al., 2018a; Lin et al., 2018). These tasks usually generate texts that preserve main information of input texts. 3 We first use a variant of self-attention(Lin et al., 2017; Kim et al., 2017) mechanism to distinguish the emotional and non-emotional words. Then the positive words and negative words are used to update the corresponding memory modules. Finally, the decoder uses the target sentiment information extracted from the memory and the content representation to perform decoding. 3.1 • The proposed method improves the content preservation degree by a large margin"
D18-1331,2015.iwslt-evaluation.1,0,0.0605264,"Missing"
D18-1331,P82-1020,0,0.77897,"Missing"
D18-1331,D13-1176,0,0.387626,"se translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality. 1 Introduction In recent years, Neural Machine Translation (NMT) has become the mainstream method of machine translation as it, in a great number of cases, outperforms most models based on Statistical Machine Translation (SMT), let alone the linguistics-based methods. One of the most popular baseline models is the sequence-to-sequence (Seq2Seq) model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). However, the conventional attention mechanism is problematic in real practice. The same weight matrix for attention is applied to all decoder outputs at all time steps, which, however, can cause inaccuracy. Take a typical example from the perspective of linguistics. Words can be categorized into two types, function word, and content word. Function words and content words execute different functions in the construction of a sentence, which is relevant to syntactic structure and sema"
D18-1331,C16-1205,0,0.0369276,"Missing"
D18-1331,D16-1096,0,0.0249323,"uence-to-sequence model (Seq2Seq) (Sutskever et al., 2014), which is an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). To improve NMT, a significant mechanism for the Seq2Seq model is the attention mechanism (Bahdanau et al., 2014). Two types of attention are the most common, which are proposed by Bahdanau et al. (2014) and Luong et al. (2015) respectively. Though the attention mechanism is powerful for the requirements of alignment in NMT, some prominent problems still exist. To tackle the impact of the attention historyTu et al. (2016); Mi et al. (2016); Meng et al. (2016); Wang et al. (2016); Lin et al. (2018a) take the attention history into consideration. An important breakthrough in NMT is that Vaswani et al. (2017) applied the fully-attention-based model to NMT and achieved the state-of-the-art performance. To further evaluate the effect of our attention temperature mechanism, we will implement it to the “Transformer” model in the future. Besides, the studies on the atTable 3: Two examples of the translation on the NIST 2003 Chinese-English translation task. The difference between Seq2Seq and SACT is shown in color. tention mechanism ha"
D18-1331,P02-1040,0,0.0999769,"Missing"
D18-1331,P16-1008,0,0.112053,"u.perl instead. 3.3 Baselines In the following, we introduce our baseline models for the Chinese-English translation and the English-Vietnamese translation respectively. For the Chinese-English translation, we compare our model with the most recent NMT systems, illustrated in the following. Moses is an open source phrase-based translation system with default configurations and a 4-gram language model trained on the training data for the target language; RNNSearch is an attention-based Seq2Seq with fine-tuned hyperparameters; Coverage is the attention-based Seq2Seq model with a coverage model (Tu et al., 2016); MemDec is the attention-based Seq2Seq model with the external memory (Wang et al., 2016). For the English-Vietnamese translation, the models to be compared are presented below. RNNSearch The attention-based Seq2Seq model as mentioned above, and we present the results of (Luong and Manning, 2015); NPMT is the Neural Phrase-based Machine Translation model by Huang et al. (2017). 4 Results and Analysis In the following, we present the experimental results as well as our analysis of temperature and case study. 4.1 Results We present the performance of the baseline models and our model on the Chi"
D18-1331,D16-1027,0,0.0708605,"Chinese-English translation and the English-Vietnamese translation respectively. For the Chinese-English translation, we compare our model with the most recent NMT systems, illustrated in the following. Moses is an open source phrase-based translation system with default configurations and a 4-gram language model trained on the training data for the target language; RNNSearch is an attention-based Seq2Seq with fine-tuned hyperparameters; Coverage is the attention-based Seq2Seq model with a coverage model (Tu et al., 2016); MemDec is the attention-based Seq2Seq model with the external memory (Wang et al., 2016). For the English-Vietnamese translation, the models to be compared are presented below. RNNSearch The attention-based Seq2Seq model as mentioned above, and we present the results of (Luong and Manning, 2015); NPMT is the Neural Phrase-based Machine Translation model by Huang et al. (2017). 4 Results and Analysis In the following, we present the experimental results as well as our analysis of temperature and case study. 4.1 Results We present the performance of the baseline models and our model on the Chinese-English translation in Table 1. As to the recent models on the same task with the sam"
D18-1331,P18-2027,1,0.842257,"o NMT and achieved the state-of-the-art performance. To further evaluate the effect of our attention temperature mechanism, we will implement it to the “Transformer” model in the future. Besides, the studies on the atTable 3: Two examples of the translation on the NIST 2003 Chinese-English translation task. The difference between Seq2Seq and SACT is shown in color. tention mechanism have also contributed to some other tasks (Lin et al., 2018b; Liu et al., 2018) Beyond the attention mechanism, there are also important methods for the Seq2Seq that contribute to the improvement of NMT. Ma et al. (2018) incorporates the information about the bag-of-words of the target for adapting to multiple translations, and Lin et al. (2018c) takes the target context into consideration. 6 Conclusion and Future Work In this paper, we propose a novel mechanism for the control over the scope of attention so that the softness of the attention distribution can be changed adaptively. Experimental results demonstrate that the model outperforms the baseline models, and the analysis shows that our temperature parameter can change automatically when decoding diverse words. In the future, we hope to find out more pa"
D18-1331,C18-1276,1,0.512339,", which is an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). To improve NMT, a significant mechanism for the Seq2Seq model is the attention mechanism (Bahdanau et al., 2014). Two types of attention are the most common, which are proposed by Bahdanau et al. (2014) and Luong et al. (2015) respectively. Though the attention mechanism is powerful for the requirements of alignment in NMT, some prominent problems still exist. To tackle the impact of the attention historyTu et al. (2016); Mi et al. (2016); Meng et al. (2016); Wang et al. (2016); Lin et al. (2018a) take the attention history into consideration. An important breakthrough in NMT is that Vaswani et al. (2017) applied the fully-attention-based model to NMT and achieved the state-of-the-art performance. To further evaluate the effect of our attention temperature mechanism, we will implement it to the “Transformer” model in the future. Besides, the studies on the atTable 3: Two examples of the translation on the NIST 2003 Chinese-English translation task. The difference between Seq2Seq and SACT is shown in color. tention mechanism have also contributed to some other tasks (Lin et al., 2018b"
D18-1331,2015.iwslt-evaluation.11,0,0.0458484,". Moses is an open source phrase-based translation system with default configurations and a 4-gram language model trained on the training data for the target language; RNNSearch is an attention-based Seq2Seq with fine-tuned hyperparameters; Coverage is the attention-based Seq2Seq model with a coverage model (Tu et al., 2016); MemDec is the attention-based Seq2Seq model with the external memory (Wang et al., 2016). For the English-Vietnamese translation, the models to be compared are presented below. RNNSearch The attention-based Seq2Seq model as mentioned above, and we present the results of (Luong and Manning, 2015); NPMT is the Neural Phrase-based Machine Translation model by Huang et al. (2017). 4 Results and Analysis In the following, we present the experimental results as well as our analysis of temperature and case study. 4.1 Results We present the performance of the baseline models and our model on the Chinese-English translation in Table 1. As to the recent models on the same task with the same training data, we extract their results from their original articles. Compared with the baseline models, our model with the SACT for the softness of attention achieves 2987 6 α = 0.0003, β1 = 0.9, β2 = 0.99"
D18-1331,D15-1166,0,0.158397,"r model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality. 1 Introduction In recent years, Neural Machine Translation (NMT) has become the mainstream method of machine translation as it, in a great number of cases, outperforms most models based on Statistical Machine Translation (SMT), let alone the linguistics-based methods. One of the most popular baseline models is the sequence-to-sequence (Seq2Seq) model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). However, the conventional attention mechanism is problematic in real practice. The same weight matrix for attention is applied to all decoder outputs at all time steps, which, however, can cause inaccuracy. Take a typical example from the perspective of linguistics. Words can be categorized into two types, function word, and content word. Function words and content words execute different functions in the construction of a sentence, which is relevant to syntactic structure and semantic meaning respectively. Our motivation is that the attention mechanism for different types of words, especial"
D18-1331,P18-2053,1,0.851456,"ed model to NMT and achieved the state-of-the-art performance. To further evaluate the effect of our attention temperature mechanism, we will implement it to the “Transformer” model in the future. Besides, the studies on the atTable 3: Two examples of the translation on the NIST 2003 Chinese-English translation task. The difference between Seq2Seq and SACT is shown in color. tention mechanism have also contributed to some other tasks (Lin et al., 2018b; Liu et al., 2018) Beyond the attention mechanism, there are also important methods for the Seq2Seq that contribute to the improvement of NMT. Ma et al. (2018) incorporates the information about the bag-of-words of the target for adapting to multiple translations, and Lin et al. (2018c) takes the target context into consideration. 6 Conclusion and Future Work In this paper, we propose a novel mechanism for the control over the scope of attention so that the softness of the attention distribution can be changed adaptively. Experimental results demonstrate that the model outperforms the baseline models, and the analysis shows that our temperature parameter can change automatically when decoding diverse words. In the future, we hope to find out more pa"
D18-1428,N16-1014,0,0.665081,"2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE). Although being widely applied, the conventional MLE training causes systems to repeatedly generate “boring” sentences, which usually are expressions with high frequency (e.g., “I am sorry” 1 The code is available at https://github.com/ lancopku/DPGAN in dialogue generation (Li et al., 2016)). The major reason is that MLE encourages the model to overproduce high-frequency words.2 The overestimation of high-frequency words discourages the model from generating low-frequency but meaningful words in real data, which makes generated text tend to be repeated and “boring”. To tackle this problem, we propose a new model for diversified text generation, called DP-GAN. The key idea is to build a discriminator that is responsible for giving reward to the generator based on the novelty of generated text. We consider the text that is frequently generated by the generator as the low-novelty t"
D18-1428,P15-1107,0,0.11564,"Missing"
D18-1428,D17-1230,0,0.662342,"parameters, or produce conversation responses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses. Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014). However, the sequenceto-sequence model tends to generate short, repetitive (Lin et al., 2018), and dull text (Luo et al., 2018). Recent researches have focused on developing methods to generate informative (Xu et al., 2018b) and diverse text (Li et al., 2017, 2016; Guu et al., 2017; Shao et al., 2017). Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017). Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016; Guu et al., 2017; Shao et al., 2017). In this paper, to handle this problem, we propose to use adversarial training (Goodfellow et al., 2014; Denton et al., 2015; Li et al., 2017), which has achieved success in image generation (Radford et al., 20"
D18-1428,P18-2027,1,0.80255,"tion. Conventional statistical approaches tend to rely extensively on hand-crafted rules and templates, require interaction with humans or simulated users to optimize parameters, or produce conversation responses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses. Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014). However, the sequenceto-sequence model tends to generate short, repetitive (Lin et al., 2018), and dull text (Luo et al., 2018). Recent researches have focused on developing methods to generate informative (Xu et al., 2018b) and diverse text (Li et al., 2017, 2016; Guu et al., 2017; Shao et al., 2017). Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017). Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016; Guu et al., 2017; Shao et al., 2017). In this paper, to handle this problem, we pr"
D18-1428,D16-1230,0,0.0705747,"ise score and novel n-grams could be better encouraged. As we can see, DP-GAN(SW), which combines the advantages of sentence-level and word-level rewards, generates not only more diverse n-grams than DP-GAN(S) but also longer text than DPGAN(W). Since combining the word-level and sentence-level rewards achieves better results than using just one of them, we focus more on the combined reward in the following parts. In review generation and dialogue generation tasks, it is a widely debated question how well the BLEU score against a single reference can reflect the quality of the generated text (Liu et al., 2016). Thus, although the proposed model achieves better BLEU scores compared with baselines, we omit the detailed comparisons in terms of BLEU for space. 4.4.2 Human Evaluation We conduct a human evaluation on the test set. For all tasks, we randomly extract 200 samples from the test sets. Each item contains the input text and the text generated by the different systems. The items are distributed to three anno3945 Sentence-level Reward 4.5 4.0 3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 True Data: Lyons Roofing needs to spend less effort on charming their customers and concentrate on their lack of business et"
D18-1428,D18-1075,1,0.838206,"roaches tend to rely extensively on hand-crafted rules and templates, require interaction with humans or simulated users to optimize parameters, or produce conversation responses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses. Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014). However, the sequenceto-sequence model tends to generate short, repetitive (Lin et al., 2018), and dull text (Luo et al., 2018). Recent researches have focused on developing methods to generate informative (Xu et al., 2018b) and diverse text (Li et al., 2017, 2016; Guu et al., 2017; Shao et al., 2017). Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017). Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016; Guu et al., 2017; Shao et al., 2017). In this paper, to handle this problem, we propose to use adversarial training"
D18-1428,D15-1166,0,0.0269329,"of training DP-GAN is shown in Algorithm 1. The details are described as follows. Generator For the concern of real-world applications, this paper assumes that the output of the model can be long text made up of multiple sentences. In order to generate multiple sentences, we build a standard hierarchical LSTM decoder (Li et al., 2015). The two layers of the LSTM are structured hierarchically. The bottom layer decodes the sentence representation and the top layer decodes each word based on the output of the bottom layer. The attention mechanism is used for word decoding (Bahdanau et al., 2014; Luong et al., 2015). 3.3 Discriminator Most existing GAN models use a binary classifier as the discriminator. The probability of being true is regarded as the reward (Li et al., 2016; Yu et al., 2017). Different from that, we propose a language-model based discriminator Dφ that builds on a unidirectional LSTM. We use the output of the language model, cross-entropy, as the reward. Specifically, given a sentence yt , the cross-entropy based reward for the k th word is calculated as R(yt,k ) = − log Dφ (yt,k |yt,<k ) We maximize the reward of real-world text and minimize the reward of generated text to train the di"
D18-1428,P18-2115,1,0.770741,"ative text. Moreover, we propose a novel languagemodel based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.1 1 Introduction Text generation is an important task in Natural Language Processing (NLP) as it lays the foundation for many applications, such as dialogue generation, machine translation (Ma et al., 2018b; Xu et al., 2018a), text summarization (Ma et al., 2018a), and table summarization (Liu et al., 2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE). Although being widely applied, the conventional MLE training causes systems to repeatedly generate “boring” sentences, which usually are expressions with high frequency (e.g., “I"
D18-1428,P18-2053,1,0.839026,"ative text. Moreover, we propose a novel languagemodel based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.1 1 Introduction Text generation is an important task in Natural Language Processing (NLP) as it lays the foundation for many applications, such as dialogue generation, machine translation (Ma et al., 2018b; Xu et al., 2018a), text summarization (Ma et al., 2018a), and table summarization (Liu et al., 2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE). Although being widely applied, the conventional MLE training causes systems to repeatedly generate “boring” sentences, which usually are expressions with high frequency (e.g., “I"
D18-1428,D17-1235,0,0.0349253,"nses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increasingly available, or fail to produce novel natural language responses. Currently, a popular model for text generation is the sequence-to-sequence model (Sutskever et al., 2014; Cho et al., 2014). However, the sequenceto-sequence model tends to generate short, repetitive (Lin et al., 2018), and dull text (Luo et al., 2018). Recent researches have focused on developing methods to generate informative (Xu et al., 2018b) and diverse text (Li et al., 2017, 2016; Guu et al., 2017; Shao et al., 2017). Reinforcement learning is incorporated into the model of conversation generation to generate more humanlike speeches (Li et al., 2017). Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention (Li et al., 2016; Guu et al., 2017; Shao et al., 2017). In this paper, to handle this problem, we propose to use adversarial training (Goodfellow et al., 2014; Denton et al., 2015; Li et al., 2017), which has achieved success in image generation (Radford et al., 2015; Chen et al., 2016; Gulrajani et al., 201"
D18-1428,P18-1090,1,0.914316,"er, we propose a novel languagemodel based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.1 1 Introduction Text generation is an important task in Natural Language Processing (NLP) as it lays the foundation for many applications, such as dialogue generation, machine translation (Ma et al., 2018b; Xu et al., 2018a), text summarization (Ma et al., 2018a), and table summarization (Liu et al., 2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE). Although being widely applied, the conventional MLE training causes systems to repeatedly generate “boring” sentences, which usually are expressions with high frequency (e.g., “I am sorry” 1 The c"
D18-1428,D18-1462,1,0.902013,"er, we propose a novel languagemodel based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.1 1 Introduction Text generation is an important task in Natural Language Processing (NLP) as it lays the foundation for many applications, such as dialogue generation, machine translation (Ma et al., 2018b; Xu et al., 2018a), text summarization (Ma et al., 2018a), and table summarization (Liu et al., 2017). In these tasks, most of the systems are built upon the sequence-to-sequence paradigm (Sutskever et al., 2014), which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sentence. The standard training method is based on Maximum Likelihood Estimation (MLE). Although being widely applied, the conventional MLE training causes systems to repeatedly generate “boring” sentences, which usually are expressions with high frequency (e.g., “I am sorry” 1 The c"
D18-1462,N18-1204,0,0.227753,"harles et al., 2001; Huang et al., 2016), and so on. Different from these studies, we get rid of external materials and consider the complete story generation task (McIntyre and Lapata, 2009). For this task, the widely used models are based on Seq2Seq models. However, although they can generate a fluent sentence (Xu et al., 2018a), these models still perform badly on generating inter-related sentences, which are necessary for a coherent story. To address this problem, there are several models that build the mid-level sentence semantic representation to simplify the dependency among sentences. Clark et al. (2018) extract the entities in sentences, and combine the entity context and text context together when generating a target sentence. Cao et al. (2018) encode the words with specific pre-defined dependency labels to a midlevel sentence representation. Martin et al. (2018) use additional knowledge bases to get a generalized sentence representation. Ma et al. (2018b) use the bag-of-words which occur in all references as a representation of the correct translation. Luo et al. (2018) propose to use two auto-encoders to learn the semantic representation of utterance in dialogue. However, although these m"
D18-1462,D13-1155,0,0.0159139,"model should generate a coherent story based on a given sentence. We build a new dataset for this task by splitting the data into two parts. In each story, we take the first sentence as the input text, and the following sentences as the target text. The processed dataset contains 40153, 4990, and 5054 stories for training, validation, and testing, respectively. The maximum number of sentences in each story is 6. In total, the number of training sentences is over 20K and the number of training words is over 2M. To pre-train the skeleton extraction module, we use a sentence compression dataset (Filippova and Altun, 2013). In this dataset, every compression is a subsequence of tokens from the input. The dataset contains 16999, 1000, and 1998 pairs for training, validation, and testing, respectively. 4.2 Baselines We compare our proposed model with the following the state-of-the-art models. Entity-Enhanced Seq2Seq Model (EESeq2Seq) (Clark et al., 2018). It regards entities as important context needed for coherent stories. When decoding a sentence, it combines entity context and text context together to reduce dependency sparsity. Dependency-Tree Enhanced Seq2Seq Model (DE-Seq2Seq) (Cao et al., 2018). It defines"
D18-1462,P15-1107,0,0.0771,"Missing"
D18-1462,D16-1127,0,0.0777727,"Missing"
D18-1462,D18-1075,1,0.844791,"re several models that build the mid-level sentence semantic representation to simplify the dependency among sentences. Clark et al. (2018) extract the entities in sentences, and combine the entity context and text context together when generating a target sentence. Cao et al. (2018) encode the words with specific pre-defined dependency labels to a midlevel sentence representation. Martin et al. (2018) use additional knowledge bases to get a generalized sentence representation. Ma et al. (2018b) use the bag-of-words which occur in all references as a representation of the correct translation. Luo et al. (2018) propose to use two auto-encoders to learn the semantic representation of utterance in dialogue. However, although these models reduce the dependency sparsity to some extent, the unified rules are non-flexible and tend to generate oversimplified representations, resulting in the loss of key information. Different from these models, we use a reinforcement learning method to automatically extract sentence skeletons for simplifying the dependency of sentences, rather than manual rules. Therefore, our proposed skeleton-based model is more flexible and can adaptively determine the appropriate granu"
D18-1462,P18-2115,1,0.913551,"e story generation. Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In"
D18-1462,P18-2053,1,0.92804,"e story generation. Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In"
D18-1462,P09-1025,0,0.0721962,"Missing"
D18-1462,P02-1040,0,0.101185,"Missing"
D18-1462,D18-1428,1,0.902082,". Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In this work, we reg"
D18-1462,P18-1090,1,0.907641,". Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In this work, we reg"
D18-1485,N16-1063,0,0.0816785,"posteriori principle to determine the label set of each sample. F¨urnkranz et al. (2008) made ranking among labels by utilizing pairwise comparison. Li et al. (2015) used joint learning predictions as features. Recent studies of multi-label text classification have turned to the application of neural networks, which have achieved great success in natural language processing. Zhang and Zhou (2006) implemented the fully-connected neural networks with pairwise ranking loss function. Nam et al. (2013) changed the ranking loss function to the crossentropy loss to better the training. Kurata et al. (2016) proposed a novel neural network initialization method to treat some neurons as dedicated neurons to model label correlations. Chen et al. (2017) incorporated CNN and RNN so as to capture both global and local semantic information and model high-order label correlations. (Nam et al., 2017) proposed to generate labels sequentially, and Yang et al. (2018); Li et al. (2018) both adopted the Seq2Seq, one with a novel decoder and one with a soft loss function respectively. 8 Conclusion In this study, we propose our model based on the multi-level dilated convolution and the hybrid attention mechanis"
D18-1485,D15-1099,1,0.784604,"ce. Algorithm adaptation methods adopt specific learning algorithms to the multi-label classification task without requiring problem transformations. Clare and King (2001) constructed decision tree based on multi-label entropy to perform classification. Elisseeff and Weston (2002) adopted a Support Vector Machine (SVM) like learning system to handle multi-label problem. Zhang and Zhou (2007) utilized the k-nearest neighbor algorithm and maximum a posteriori principle to determine the label set of each sample. F¨urnkranz et al. (2008) made ranking among labels by utilizing pairwise comparison. Li et al. (2015) used joint learning predictions as features. Recent studies of multi-label text classification have turned to the application of neural networks, which have achieved great success in natural language processing. Zhang and Zhou (2006) implemented the fully-connected neural networks with pairwise ranking loss function. Nam et al. (2013) changed the ranking loss function to the crossentropy loss to better the training. Kurata et al. (2016) proposed a novel neural network initialization method to treat some neurons as dedicated neurons to model label correlations. Chen et al. (2017) incorporated"
D18-1485,P18-2027,1,0.883589,"volution with different dilation rates, Figure 2: Structure of Hybrid Attention. The blue circles at the left bottom represent the source annotations generated by the LSTM encoder, the yellow circles at the right bottom represent the semantic unit representations generated by MDC, and the blue circles at the top represent the LSTM decoder outputs. At each decoding time step, the output of the LSTM attends to the semantic unit representations first, and then the new representation incorporated with high-level information attends to the source annotations. such as [1,2,3]. Following Wang et al. (2018), for N layers of 1-dimension convolution with kernel size K with dilation rates [r1 , ..., rN ], the maximum distance between two nonzero values is max(Mi+1 − 2ri , Mi+1 − 2(Mi+1 − ri ), ri ) with MN = rN , and the goal is M2 ≤ K. In our experiments, we set the dilation rates to [1, 2, 3] and K to 3, and we have M2 = 2. The implementations can avoid the gridding effects and allows the top layer to access information between longer distance without loss of coverage. Moreover, as there may be irrelevant information to the semantic units at a long distance, we carefully design the dilation rates"
D18-1485,D15-1166,0,0.0185222,"attempt to capture the relationship, which though demonstrated improvements yet simply captured low-order correlations. A milestone in this field is the application of sequence-to-sequence learning to multi-label text classification (Nam et al., 2017). Sequenceto-sequence learning is about the transformation from one type of sequence to another type of sequence, whose most common architecture is the attention-based sequence-to-sequence (Seq2Seq) model. The attention-based Seq2Seq (Sutskever et al., 2014) model is initially designed for neural machine translation (NMT) (Bahdanau et al., 2014; Luong et al., 2015). Seq2Seq is able to encode a given source text and decode the representation for a new sequence to approximate the target text, and with the attention mechanism, the decoder is competent in extracting vital sourceside information to improve the quality of decoding. Multi-label text classification can be regarded as the prediction of the target label sequence given a source text, which can be modeled by the Seq2Seq. Moreover, it is able to model the high-order correlations among the source text as well as those among the label sequence with deep recurrent neural networks (RNN). Nevertheless, w"
D18-1485,P14-1062,0,0.0208965,"is an LSTM in our model, we apply the multi-layer convolutional neural networks to generate representations of semantic units by capturing local correlations and long-term dependencies among words. To be specific, our CNN is a three-layer one-dimensional CNN. Fol4556 濷濼濿濴瀇濼瀂瀁濐濆 濷濸濶瀂濷濸瀅 濷濼濿濴瀇濼瀂瀁濐濅  濷濼濿濴瀇濼瀂瀁濐濄   濠濗濖澳瀂瀈瀇瀃瀈瀇瀆 濟濦濧濠澳瀂瀈瀇瀃瀈瀇瀆 濟濦濧濠澳瀂瀈瀇瀃瀈瀇瀆 Figure 1: Structure of Multi-level Dilated Convolution (MDC). A example of MDC with kernel size k = 2 and dilation rates [1, 2, 3]. To avoid gridding effects, the dilation rates do not share a common factor other than 1. lowing the previous work (Kalchbrenner et al., 2014) on CNN for NLP, we use one-dimensional convolution with the number of channels equal to the number of units of the hidden layer, so that the information at each dimension of a representation vector will not be disconnected as 2-dimension convolution does. Besides, as we are to capture semantic units in the source text instead of higherlevel word representations, there is no need to use padding for the convolution. A special design for the CNN is the implementation of dilated convolution. Dilation has become popular in semantic segmentation in computer vision in recent years (Yu and Koltun, 20"
D18-1485,D14-1181,0,0.00469793,"ification problems to model the correlations between labels. • Label Powerset (LP) (Tsoumakas and Katakis, 2006) creates one binary classifier for every label combination attested in the training set. HL(-) 0.1663 0.1828 0.1902 0.1726 0.1876 0.1814 0.1793 0.1782 P(+) 0.649 0.572 0.556 0.628 0.576 0.587 0.589 0.593 R(+) 0.472 0.551 0.517 0.512 0.538 0.571 0.573 0.585 F1(+) 0.546 0.561 0.536 0.565 0.556 0.579 0.581 0.590 Table 3: Performance of the models on the Ren-CECps test set. HL, P, R, and F1 denote hamming loss, micro-precision, micro-recall and micro-F1 , respectively (p < 0.05). • CNN (Kim, 2014) uses multiple convolution kernels to extract text feature, which is then input to the linear transformation layer followed by a sigmoid function to output the probability distribution over the label space. • CNN-RNN (Chen et al., 2017) utilizes CNN and RNN to capture both global and local textual semantics and model label correlations. • S2S and S2S+Attn (Sutskever et al., 2014; Bahdanau et al., 2014) are our implementation of the RNN-based sequence-to-sequence models without and with the attention mechanism respectively. 6 Results and Discussion In the following sections, we report the resul"
D18-1485,C18-1330,1,0.692806,"ss in natural language processing. Zhang and Zhou (2006) implemented the fully-connected neural networks with pairwise ranking loss function. Nam et al. (2013) changed the ranking loss function to the crossentropy loss to better the training. Kurata et al. (2016) proposed a novel neural network initialization method to treat some neurons as dedicated neurons to model label correlations. Chen et al. (2017) incorporated CNN and RNN so as to capture both global and local semantic information and model high-order label correlations. (Nam et al., 2017) proposed to generate labels sequentially, and Yang et al. (2018); Li et al. (2018) both adopted the Seq2Seq, one with a novel decoder and one with a soft loss function respectively. 8 Conclusion In this study, we propose our model based on the multi-level dilated convolution and the hybrid attention mechanism, which can extract both the semantic-unit-level information and word-level information. Experimental results demonstrate that our proposed model can significantly outperform the baseline models. Moreover, the analyses reflect that our model is competitive with the deterministic hierarchical models and it is more robust to classifying the low-frequency"
D19-1172,D14-1181,0,0.00514974,"For simplification, we use a special symbol, [S], to concatenate all the entity information together. These two parts can be regarded as different source information. Based on whether the inter-relation between different source information is explicitly explored, the classification models can be classified into two categories, unstructured models and structured models. Unstructured models concatenate different source inputs into a long sequence with a special separation symbol [SEL]. We implement several widely-used sequence classification models, including Convolutional Neural Network (CNN) (Kim, 2014), Long-Short Term Memory Network (LSTM) (Schuster and Paliwal, 1997), and Recurrent Convolutional Neural Network (RCNN) (Lai et al., 2015), Transformer. The details of the models are shown in Supplementary Materials. 1622 [A] web browser Midori operating system Midori Decoder Decoder Encoder Entity Info [B] Entity Rendering Module Ambiguous Context Template What are the languages used to create the source code of Midori? Encoder Template Generating Module Decoder When you say the source code language used in the program Midori, are you referring to [A] or [B]? Hidden vector of [A] Hidden vecto"
D19-1172,J81-4005,0,0.674542,"Missing"
D19-1172,N03-1007,0,0.0346386,"Missing"
D19-1172,D16-1076,0,0.0350947,"Missing"
D19-1172,P17-1045,0,0.0494573,"Missing"
D19-1172,P18-1068,0,0.018531,"es (e.g., “web browser Midori ”) and pattern phrases ( e.g. “When you say the source code language used in the program Midori, are you referring to [A] or [B]?” ). The entity phrase is summarized from the given entity information for distinguishing between two entities. The pattern phrase is used to locate the position of ambiguity, which is closely related with the context. In summary, two kinds of phrases refer to different source information. Based on this feature, we propose a new coarse-to-fine model, as shown in Figure 6. Similar ideas have been successfully applied to semantic parsing (Dong and Lapata, 2018). The proposed model consists of a template generating module Tθ and an entity rendering module Rφ . Tθ first generates a template containing pattern phrases and the symbolic representation of the entity phrases. Then, the symbolized entities contained in the generated template are further properly rendered by the entity rendering module Rφ to reconstruct complete entity information. Since the annotated clarification questions explicitly separate entity phrases and pattern phrases, we can easily build training data for these two modules. For clarity, the template is constructed by replacing en"
D19-1172,D18-1188,1,0.829182,"ll dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification-based question answering. We implement representative neural networks as baselines for three tasks and propose a new generation model. The detailed analysis shows that our dataset brings new challenges. More powerful models and reasonable evaluation metrics need further explored. In the future, we plan to"
D19-1172,D18-1361,0,0.0212818,"rification Question in Other Tasks There are several studies on asking clarification questions. Stoyanchev et al. (2014) randomly drop one phrase from a question and require annotators to ask a clarification question toward the dropped information, e.g.“Do you know the birth data of XXX”. However, the small dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification"
D19-1172,P18-1255,0,0.125162,"Missing"
D19-1172,N16-1174,0,0.0245396,"late generating module and an entity rendering module. The former is used to generate a clarification template based on ambiguous question, e.g., “When you say the source code language used in the program Midori, are you referring to [A] or [B]?”. The latter is used to fill up the generated template with detailed entity information. Structured models use separate structures to encode different source information and adopt an additional structure to model the inter-relation of the source information. Specifically, we use two representative neural networks, Hierarchical Attention Network (HAN) (Yang et al., 2016) and Dynamic Memory Network (DMN) (Kumar et al., 2016), as our structured baselines. The details of the models are shown in Supplementary Materials. 4.2 Clarification Question Generation Models The input of the generation model is the ambiguous context and entity information. In single-turn cases, the ambiguous context is current question Qa . In multi-cases, the input is current question and the previous conversation turn {Qp , Rp , Qa }. We use [S] to concatenate all entity information together, and use [SEL] to concatenate entity information and context information into a long sequence. We"
D19-1336,N19-1172,0,0.538622,"t is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs CLM+JD Pun-GAN vs Hum"
D19-1336,W09-2004,0,0.092266,"Missing"
D19-1336,W16-5307,0,0.055248,"Missing"
D19-1336,D18-1170,1,0.925653,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P18-1230,1,0.925353,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P15-1070,0,0.0609047,"Missing"
D19-1336,S17-2005,0,0.244865,"s – English Wikipedia to train Pun-GAN. For generator, we first tag each word in the English Wikipedia corpus with one word sense using an unsupervised WSD tool2 . Then we use the 2,595K tagged corpus to pre-train our generator. For discriminator, we use several types of data for training: 1) SemCor (Luo et al., 2018a,b) which is a manually annotated corpus for WSD, consisting of 226K sense annotations3 (first part in Eq.4); 2) Wikipedia corpus as unlabeled corpus (second part in Eq.4); 3) Generated puns (third part in Eq.4). Evaluation Dataset: We use the pun dataset from SemEval 2017 task7 (Miller et al., 2017) for evaluation. The dataset consists of 1274 humanwritten puns where target pun words are annotated with two word senses. During testing, we extract the word sense pair as the input of our model. 3.2 Experimental Setting The generator is the same as Yu et al. (2018). The discriminator is a single-layer bi-directional LSTM with hidden size 128. We randomly initialize word embeddings with the dimension size of 300. The sample size K is set as 32. Batch size is 32 and learning rate is 0.001. The optimization algorithm is SGD. Before adversarial training, we pre-train the generator for 5 epochs a"
D19-1336,P12-1101,0,0.0181233,"M (Mikolov et al., 2010): It is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs"
D19-1336,P13-2041,0,0.0766619,"Missing"
D19-1336,D17-1120,0,0.046399,"Missing"
D19-1336,P13-2044,0,0.365409,"Missing"
D19-1336,P18-1153,0,0.436043,"in ambiguity and diversity. 2 Model The sketch of the proposed Pun-GAN is depicted in Figure 1. It consists of a pun generator Gθ and a word sense discriminator Dφ . The following sections will elaborate on the architecture of Pun-GAN and its training algorithm. 2.1 Model Structure 2.1.1 Generator Given two senses (s1 , s2 ) of a target word w, the generator Gθ aims to output a sentence x which not only contains the target word w but also express the two corresponding meanings. Considering the simplicity of the model and the ease of training, we adopt the neural constrained language model of Yu et al. (2018) as the generator. Due to space constraints, we strongly recommend that readers refer to the original paper for details. Compared with traditional neural language model, the main difference is that the generated words at each timestep should have the maximum sum of two probabilities which are calculated with s1 and s2 as input, respectively. Formally, the generation probability over the entire vocabulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function,"
D19-1451,D18-1269,0,0.0277625,"ultilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of entity literal descript"
D19-1451,P13-1153,0,0.0780832,"Missing"
D19-1451,N19-1423,0,0.480151,"entities (i.e., topological connections, relations and attributes, as well as literal descriptions) remains under-explored. In this work, we propose a novel approach to learn cross-lingual entity embeddings by using all aforementioned aspects of information in KGs. To be specific, we propose two variants of GCNbased models, namely M AN and H MAN, that incorporate multi-aspect features, including topological features, relation types, and attributes into cross-lingual entity embeddings. To capture semantic relatedness of literal descriptions, we finetune the pretrained multilingual BERT model (Devlin et al., 2019) to bridge cross-lingual gaps. We design two strategies to combine GCN-based and BERT-based modules to make alignment decisions. Experiments show that our method achieves new state-of-the-art results on two benchmark datasets. Source code for our models is publicly available at https://github.com/ h324yang/HMAN. 2 Problem Definition In a multilingual knowledge graph G, we use L to denote the set of languages that G contains and Gi = {Ei , Ri , Ai , Vi , Di } to represent the language-specific knowledge graph in language Li ∈ L. Ei , Ri , Ai , Vi and Di are sets of entities, relations, attribut"
D19-1451,P19-1024,1,0.845187,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,Q19-1019,1,0.822025,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,P14-1006,0,0.0259027,"ithm to alternately learn multilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of e"
D19-1451,D14-1005,0,0.0346512,"t Network (M AN) to capture the three aspects of entity features. Specifically, three l-layer GCNs take as inputs the tripleaspect features (i.e., Xt , Xr , and Xa ) and produce (l) (l) (l) the representations Ht , Hr , and Ha according to Equation 1, respectively. Finally, the multiaspect entity embedding is: (l) Hm = [Ht ⊕ Ha(l) ⊕ Hr(l) ] (2) where ⊕ denotes vector concatenation. Hm can then feed into alignment decisions. Such fusion through concatenation is also known as Scoring Level Fusion, which has been proven simple but effective for capturing multimodal semantics (Bruni et al., 2014; Kiela and Bottou, 2014; Collell et al., 2017). It is worth noting that the main differences between M AN and the work of Wang et al. (2018) are two fold: First, we use the same approach as in Kipf and Welling (2017) to construct the adjacency matrix, while Wang et al. (2018) designed a new connectivity matrix as the adjacency matrix for the GCNs. Second, M AN explicitly regards the relation type features as model input, while Wang et al. (2018) incorporated such relation information into the connectivity matrix. H MAN. Note that M AN propagates relation and attribute information through the graph structure. However"
D19-1451,N19-1229,1,0.801615,"rview of P OINTWISE B ERT (left) and PAIRWISE B ERT (right). from which the final hidden state is used as the sequence representation, and [SEP] is the special token for separating token sequences, and produces the probability of classifying the pair as equivalent entities. The probability is then used to rank all candidate entity pairs, i.e., ranking score. We denote this model as P OINTWISE B ERT, shown in Figure 3 (left). This approach is computationally expensive, since for each entity we need to consider all candidate entities in the target language. One solution, inspired by the work of Shi et al. (2019), is to reduce the search space for each entity with a reranking strategy (see Section 3.3). PAIRWISE B ERT. Due to the heavy computational cost of P OINTWISE B ERT, semantic matching between all entity pairs is very expensive. Instead of producing ranking scores for description pairs, we propose PAIRWISE B ERT to encode the entity literal descriptions as cross-lingual textual embeddings, where distances between entity pairs can be directly measured using these embeddings. The PAIRWISE B ERT model consists of two components, each of which takes as input the description of one entity (from the"
D19-1451,D18-1032,0,0.160471,"essing, pages 4431–4441, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics including topological connections, relation types, attributes, and literal descriptions expressed in different languages (Bizer et al., 2009; Xie et al., 2016), as shown in Figure 1 (bottom). The key challenge of addressing such a task thus is how to better model and use provided multi-aspect information of entities to bridge cross-lingual gaps and find more equivalent entities (i.e., ILLs). Recently, embedding-based solutions (Chen et al., 2017b; Sun et al., 2017; Zhu et al., 2017; Wang et al., 2018; Chen et al., 2018) have been proposed to unify multilingual KGs into the same low-dimensional vector space where equivalent entities are close to each other. Such methods only make use of one or two aspects of the aforementioned information. For example, Zhu et al. (2017) relied only on topological features while Sun et al. (2017) and Wang et al. (2018) exploited both topological and attribute features. Chen et al. (2018) proposed a co-training algorithm to combine topological features and literal descriptions of entities. However, combining these multi-aspect information of entities (i.e.,"
D19-1451,D18-1244,0,0.0428449,"ice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update the representation of each entity node via a propagation mechanism through the graph. Inspired by previous studies (Zhang et al., 2018; Wang et al., 2018), we also 4432 adopt GCNs in this work to collect evidence from multilingual KG structures and to learn crosslingual embeddings of entities. The primary assumptions are: (1) equivalent entities tend to be neighbored by equivalent entities via the same types of relations; (2) equivalent entities tend to share similar or even the same attributes. Multi-Aspect Entity Features. Existing KGs (Bizer et al., 2009; Suchanek et al., 2008; Rebele et al., 2016) provide multi-aspect information of entities. In this section, we mainly focus on the following three aspects: topological co"
D19-1553,P18-1231,0,0.0262753,"d on speciﬁc content, leading to better performance. There also exist some other endeavors which construct pseudo-parallel data by means of back-translation (Sennrich et al., 2015). For instance, Zhang et al. (2018b) and Luo et al. (2019) jointly train two transfer systems via iterative back-translation. Further, Lample et al. (2019) extend this training framework to support multiple attribute control . Sentiment analysis. Our work is also related to: sentiment analysis (Severyn and Moschitti, 2015; Cambria, 2016; Poria et al., 2017; Lam et al., 2018), sentiment embeddings (Tang et al., 2014; Barnes et al., 2018), and aspect extraction (Poria et al., 2016; He et al., 2017; Pablos et al., 2018). 7 Conclusion In this paper, we propose a simple but effective speciﬁcity-driven cascading approach for unsupervised sentiment modiﬁcation. The proposed approach performs target sentiment addition and content reconstruction independently, so that more speciﬁc information is preserved. Extensive experimental results show that our approach outperforms several competitive systems by a large margin. Further analysis demonstrates that the proposed method not only increases the speciﬁcity of the generated text, but al"
D19-1553,P17-1036,0,0.0758125,"Missing"
D19-1553,D14-1181,0,0.00280123,"eni et al., 2002) between the transferred sentence and the source sentence to evaluate content preservation. The higher BLEU score indicates better content preservation. Datasets We conduct experiments on two datasets, Yelp4 and Amazon5 (He and McAuley, 2016). Both datasets are composed of a large number of reviews. Following previous work (Shen et al., 2017), reviews with scores below 3 are labeled as negative, and reviews with scores above 3 are labeled as positive. The reviews which exceed 20 words or less than 5 words are further ﬁltered out. In addition, we train the sentiment classiﬁer (Kim, 2014) to ﬁlter samples with the category conﬁdence below 0.8. We divide each dataset into training, validation and test sets. The statistics of the two processed datasets is shown in Table 3. 4.2 Amazon Evaluation Metrics In this paper, we adopt two evaluation methods: automatic evaluation and human evaluation. 4.2.1 Automatic Evaluation Following previous work (Xu et al., 2018; Shen et al., 2017), we perform automatic evaluation in terms of sentiment transformation and content preservation. • ACC: We pre-train a sentiment classiﬁer, which is implemented as CNN (Kim, 2014) structure, to evaluate wh"
D19-1553,P02-1040,0,0.106863,"Missing"
D19-1553,P18-2031,0,0.0596485,"Missing"
D19-1553,P16-1009,0,0.0940402,"Missing"
D19-1553,P18-1087,0,0.0235617,"can generate accurate explicit sentiment words ﬂexibly based on speciﬁc content, leading to better performance. There also exist some other endeavors which construct pseudo-parallel data by means of back-translation (Sennrich et al., 2015). For instance, Zhang et al. (2018b) and Luo et al. (2019) jointly train two transfer systems via iterative back-translation. Further, Lample et al. (2019) extend this training framework to support multiple attribute control . Sentiment analysis. Our work is also related to: sentiment analysis (Severyn and Moschitti, 2015; Cambria, 2016; Poria et al., 2017; Lam et al., 2018), sentiment embeddings (Tang et al., 2014; Barnes et al., 2018), and aspect extraction (Poria et al., 2016; He et al., 2017; Pablos et al., 2018). 7 Conclusion In this paper, we propose a simple but effective speciﬁcity-driven cascading approach for unsupervised sentiment modiﬁcation. The proposed approach performs target sentiment addition and content reconstruction independently, so that more speciﬁc information is preserved. Extensive experimental results show that our approach outperforms several competitive systems by a large margin. Further analysis demonstrates that the proposed method"
D19-1553,N18-1169,0,0.0478807,"Missing"
D19-1553,P14-1146,0,0.0362121,"words ﬂexibly based on speciﬁc content, leading to better performance. There also exist some other endeavors which construct pseudo-parallel data by means of back-translation (Sennrich et al., 2015). For instance, Zhang et al. (2018b) and Luo et al. (2019) jointly train two transfer systems via iterative back-translation. Further, Lample et al. (2019) extend this training framework to support multiple attribute control . Sentiment analysis. Our work is also related to: sentiment analysis (Severyn and Moschitti, 2015; Cambria, 2016; Poria et al., 2017; Lam et al., 2018), sentiment embeddings (Tang et al., 2014; Barnes et al., 2018), and aspect extraction (Poria et al., 2016; He et al., 2017; Pablos et al., 2018). 7 Conclusion In this paper, we propose a simple but effective speciﬁcity-driven cascading approach for unsupervised sentiment modiﬁcation. The proposed approach performs target sentiment addition and content reconstruction independently, so that more speciﬁc information is preserved. Extensive experimental results show that our approach outperforms several competitive systems by a large margin. Further analysis demonstrates that the proposed method not only increases the speciﬁcity of the"
D19-1553,P18-1080,0,0.343312,"like proper nouns. For instance, “Michael” in Table 1 is a human name, which belongs to the speciﬁc information. This speciﬁc information is important because it is often the subject or object of the sentence and the bearer of sentiment. Therefore, it needs to be fully preserved in the process of sentiment modiﬁcation. We refer this attribute as the speciﬁcity of the output. Most previous work on unsupervised sentiment modiﬁcation follows a two-step process. They ﬁrst separate the content from the original sentiment, either in an implicit (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Tsvetkov et al., 2018) or explicit (Xu et al., 2018; Zhang et al., 2018a) way. Then, they directly generate text with the target sentiment only based on the content produced by the ﬁrst step. In the second step, the decoder does not include the sentiment information in its source input, meaning that the source information is incomplete for generating a sentimenttransferred sentence. This information gap at both ends of the decoder causes the decoder needs to bear both the target sentiment addition and content reconstruction. Thus, it is difﬁcult for the decoder to balance sentiment transformation and content preser"
D19-1553,D18-1138,1,0.836767,"1 is a human name, which belongs to the speciﬁc information. This speciﬁc information is important because it is often the subject or object of the sentence and the bearer of sentiment. Therefore, it needs to be fully preserved in the process of sentiment modiﬁcation. We refer this attribute as the speciﬁcity of the output. Most previous work on unsupervised sentiment modiﬁcation follows a two-step process. They ﬁrst separate the content from the original sentiment, either in an implicit (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Tsvetkov et al., 2018) or explicit (Xu et al., 2018; Zhang et al., 2018a) way. Then, they directly generate text with the target sentiment only based on the content produced by the ﬁrst step. In the second step, the decoder does not include the sentiment information in its source input, meaning that the source information is incomplete for generating a sentimenttransferred sentence. This information gap at both ends of the decoder causes the decoder needs to bear both the target sentiment addition and content reconstruction. Thus, it is difﬁcult for the decoder to balance sentiment transformation and content preservation simultaneously, resulting in the loss of s"
D19-1554,Q18-1039,0,0.0232949,"sentiment classification generated by the proposed approach. A pretrained classifier correctly predicts the label of the original text but fails on the generated text. tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed i"
D19-1554,P14-2009,0,0.0215629,". tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based"
D19-1554,D17-1215,0,0.0360988,"nal text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based approaches. The key idea of the former approaches is to assist the training of the classifier by augmenting the training data with predesigned examples (Wang and Yang, 2015; Jia and Liang, 2017; Iyyer et al., 2018). Adversarial training based approaches (Miyato et al., 2017) aim to improve the generalization ability by adding random noises to word embeddings. Although these methods are good pioneering work, they either heavily rely on human knowledge or suffer from low diversity of attacks, which limits the robustness to diverse words and expressions. In this work, we propose a lexical-based adversarial reinforcement training framework, LexicalAT, for robust sentiment classification. Compar5518 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing an"
D19-1554,D14-1181,0,0.0836247,"erated text. tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversar"
D19-1554,S18-2005,0,0.0173649,"gative Positive Figure 1: An attacking example for sentiment classification generated by the proposed approach. A pretrained classifier correctly predicts the label of the original text but fails on the generated text. tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet S"
D19-1554,N18-2072,0,0.376187,"edy et al., 2014; Huang et al., 2017; Yuan et al., 2017). Based on these findings, some studies have been proposed to improve the robustness of neural networks. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based approaches. The main idea of the data augmentation based approaches is to augment the training data with pre-designed adversarial examples. Iyyer et al. (2018) propose a paraphrase method to generate syntactically adversarial examples for machine translation tasks. To explore semantically adversarial examples, Kobayashi (2018) replaces input words in real examples with the word predicted by a label-conditional language model. In these approaches, the attacking policy is taskspecific and elaborately designed by humans. Unlike these approaches, Miyato et al. (2017) propose to use an adversarial training framework to attack the classifier by adding perturbations to the word embedding layer. However, the unchanged input text makes it hard to improve the robustness to diverse words and expressions. In this work, we propose a new adversarial reinforcement training framework that aims to generate diverse attacks with self"
D19-1554,N18-1050,0,0.0186332,"acking example for sentiment classification generated by the proposed approach. A pretrained classifier correctly predicts the label of the original text but fails on the generated text. tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches"
D19-1554,P82-1020,0,0.704431,"Missing"
D19-1554,N18-1170,0,0.187377,"Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based approaches. The key idea of the former approaches is to assist the training of the classifier by augmenting the training data with predesigned examples (Wang and Yang, 2015; Jia and Liang, 2017; Iyyer et al., 2018). Adversarial training based approaches (Miyato et al., 2017) aim to improve the generalization ability by adding random noises to word embeddings. Although these methods are good pioneering work, they either heavily rely on human knowledge or suffer from low diversity of attacks, which limits the robustness to diverse words and expressions. In this work, we propose a lexical-based adversarial reinforcement training framework, LexicalAT, for robust sentiment classification. Compar5518 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Internation"
D19-1554,P05-1015,0,0.692288,"Missing"
D19-1554,W02-1011,0,0.0275488,"biggest boredom of the period Generated text Classifier Negative Positive Figure 1: An attacking example for sentiment classification generated by the proposed approach. A pretrained classifier correctly predicts the label of the original text but fails on the generated text. tions on real examples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally"
D19-1554,D13-1170,0,0.0487017,"Missing"
D19-1554,D15-1167,0,0.0273841,"mples. This phenomenon shows that current sentiment classification models have poorly learned the true underlying patterns that determine the correct label. The over-fitting problem still needs to be further explored. Sentiment classification is a fundamental research area in natural language processing (Pang et al., 2002; Glorot et al., 2011; Lai et al., 2015; Kiritchenko and Mohammad, 2018; Liu et al., 2018; Chen et al., 2018). With the development of deep learning, neural networks have obtained state-ofthe-art results on many sentiment classification datasets (Kim, 2014; Dong et al., 2014; Tang et al., 2015). However, despite the promising results, recent work has shown that these models easily fail in adversarial examples2 with little perturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based approaches. The key"
D19-1554,D15-1306,0,0.0252408,"rturba∗ failure Original text Introduction Equal Contribution. The code will be released at https://github.com/ lancopku/LexicalAT 2 Adversarial examples are intentionally designed by attackers to cause the model to make a mistake. 1 WordNet Several approaches have been proposed in recent years for the attack problem. These studies can be roughly classified into two categories, data augmentation based approaches and adversarial training based approaches. The key idea of the former approaches is to assist the training of the classifier by augmenting the training data with predesigned examples (Wang and Yang, 2015; Jia and Liang, 2017; Iyyer et al., 2018). Adversarial training based approaches (Miyato et al., 2017) aim to improve the generalization ability by adding random noises to word embeddings. Although these methods are good pioneering work, they either heavily rely on human knowledge or suffer from low diversity of attacks, which limits the robustness to diverse words and expressions. In this work, we propose a lexical-based adversarial reinforcement training framework, LexicalAT, for robust sentiment classification. Compar5518 Proceedings of the 2019 Conference on Empirical Methods in Natural L"
D19-5105,P15-1017,0,0.0305374,", we propose to learn event extraction and stock prediction jointly in MSSPM because these two tasks are highly related. The improvement of event extraction result can boost news understanding and promote the stock prediction. And the output of stock prediction can give feedback to event extraction. So the joint learning can share valuable information between tasks. Result shows that MSSPM outperforms SSPM on the uncovered news and increases the method’s generalizability. The contributions of this work are summarized as follows: 2.1 Related Work Automatically Event Data Labeling According to (Chen et al., 2015; Liu et al., 2018; Huang et al., 2018), the fine-grained event structure contains event types, event trigger words and event roles. Zhou et al. (2015) propose a framework to extract events from twitter automatically. Yang et al. (2018) employ a predefined dictionary to label events and then extract document-level events from Chinese finance news. However, they only conduct experiments on 4 event types. While we employ a widely-covered dictionary with 32 different event types. Chen et al. (2017) adopt world and linguistic knowledge to detect event roles and trigger words from text. Zeng et al."
D19-5105,C16-1201,0,0.0493353,"ecision. Li et al. (2015) adopt the tensor decompose method to get the interaction information of different inputs. Duan et al. (2018) use the summary of news body instead of news headline to predict the stock returns. Some other works try to employ structure information to predict the stock movement. Ding et al. (2014) extract &lt;S,P,O&gt; (subject, predicate and object) structure from news to predict the stock movement. Then they propose two improved method based on &lt;S,P,O&gt; structure by applying the weighted fusion of event roles (Ding et al., 2015) and introducing the entity relation knowledge (Ding et al., 2016). Besides, Zhang et al. (2018b) employ a RBM to process &lt;S,P,O&gt; to get the event representation. • We propose to incorporate the fine-grained events in stock movement prediction and this method outperforms all the baselines. • We propose to learn event extraction and stock prediction jointly, which improves the method generalizability for uncovered news. • We propose TFED and a pipeline method which can extract fine-grained events from finance news automatically. • We propose the embedding method for minute-level stock trade data, and adopt timeseries models to learn its representation. 1 Toky"
D19-5105,C18-1075,0,0.0210091,"e contains event types, event trigger words and event roles. Zhou et al. (2015) propose a framework to extract events from twitter automatically. Yang et al. (2018) employ a predefined dictionary to label events and then extract document-level events from Chinese finance news. However, they only conduct experiments on 4 event types. While we employ a widely-covered dictionary with 32 different event types. Chen et al. (2017) adopt world and linguistic knowledge to detect event roles and trigger words from text. Zeng et al. (2018) use the Freebase CVT structure to label data and extract event. Araki and Mitamura (2018) adopt distant supervision to extract event from open domain. There are some works using either manual rules (Arendarenko and Kakkonen, 2012) or machine learning methods (Jacobs et al., 2018) for finance event detection, while our event labeling method is stock specific with professional domain knowledge. 2.2 Stock Movement Prediction Many works using related text for stock movement prediction take the raw text as model input directly. Xu and Cohen (2018) adopt a variational generation model to combine tweets and stock history data to make the prediction. Si et al. (2014) employ the sentiment"
D19-5105,C18-1239,0,0.207559,"12) or machine learning methods (Jacobs et al., 2018) for finance event detection, while our event labeling method is stock specific with professional domain knowledge. 2.2 Stock Movement Prediction Many works using related text for stock movement prediction take the raw text as model input directly. Xu and Cohen (2018) adopt a variational generation model to combine tweets and stock history data to make the prediction. Si et al. (2014) employ the sentiment analysis to help the decision. Li et al. (2015) adopt the tensor decompose method to get the interaction information of different inputs. Duan et al. (2018) use the summary of news body instead of news headline to predict the stock returns. Some other works try to employ structure information to predict the stock movement. Ding et al. (2014) extract &lt;S,P,O&gt; (subject, predicate and object) structure from news to predict the stock movement. Then they propose two improved method based on &lt;S,P,O&gt; structure by applying the weighted fusion of event roles (Ding et al., 2015) and introducing the entity relation knowledge (Ding et al., 2016). Besides, Zhang et al. (2018b) employ a RBM to process &lt;S,P,O&gt; to get the event representation. • We propose to inc"
D19-5105,P17-1038,0,0.0606247,"extracted from ∗ This work is done when Deli Chen is a intern at Mizuho Securities. 31 Proceedings of the Second Workshop on Economics and Natural Language Processing, pages 31–40 c Hong Kong, November 4. 2019 Association for Computational Linguistics 2 event structure describes the specific framework and key points of various finance events. Applying fine-grained events is beneficial for learning a better text representation because the finance knowledge contained in event structure is helpful for understanding the semantic information. Inspired by the automatic event data generation method (Chen et al., 2017; Zeng et al., 2018; Yang et al., 2018), we propose the TOPIX1 Finance Event Dictionary (TFED) built by domain experts with professional finance knowledge and adopt it to extract fine-grained events automatically for most of finance news. Then we design two different neural models: Structured Stock Prediction Model (SSPM) and Multi-task Structured Stock Prediction Model (MSSPM). SSPM fuses the extracted fine-grained event and news text firstly, and then conduct interaction between text data and stock trade data to make prediction. SSPM outperforms all the baselines but it can hardly handle the"
D19-5105,W18-3101,0,0.132149,"to label events and then extract document-level events from Chinese finance news. However, they only conduct experiments on 4 event types. While we employ a widely-covered dictionary with 32 different event types. Chen et al. (2017) adopt world and linguistic knowledge to detect event roles and trigger words from text. Zeng et al. (2018) use the Freebase CVT structure to label data and extract event. Araki and Mitamura (2018) adopt distant supervision to extract event from open domain. There are some works using either manual rules (Arendarenko and Kakkonen, 2012) or machine learning methods (Jacobs et al., 2018) for finance event detection, while our event labeling method is stock specific with professional domain knowledge. 2.2 Stock Movement Prediction Many works using related text for stock movement prediction take the raw text as model input directly. Xu and Cohen (2018) adopt a variational generation model to combine tweets and stock history data to make the prediction. Si et al. (2014) employ the sentiment analysis to help the decision. Li et al. (2015) adopt the tensor decompose method to get the interaction information of different inputs. Duan et al. (2018) use the summary of news body inste"
D19-5105,P18-1183,0,0.251075,"the prediction of stock movement has caught a lot of attention of researchers. In recent years, employing the stock related text (such as finance news or tweets) has become the mainstream (Si et al., 2014; Ding et al., 2015; Li et al., 2015; Alostad and Davulcu, 2017; Zhong et al., 2017; Zhang et al., 2018a) of stock movement prediction task. In these text-based stock prediction works, various methods are proposed to extract semantic information from stock related text to help the prediction of stock movement. There are mainly two methods of applying text: employing raw text (Hu et al., 2018; Xu and Cohen, 2018) or coarse-grained &lt;S,P,O&gt; structure (subject, predicate and object) extracted from ∗ This work is done when Deli Chen is a intern at Mizuho Securities. 31 Proceedings of the Second Workshop on Economics and Natural Language Processing, pages 31–40 c Hong Kong, November 4. 2019 Association for Computational Linguistics 2 event structure describes the specific framework and key points of various finance events. Applying fine-grained events is beneficial for learning a better text representation because the finance knowledge contained in event structure is helpful for understanding the semantic"
D19-5105,P18-4009,0,0.314787,"n Deli Chen is a intern at Mizuho Securities. 31 Proceedings of the Second Workshop on Economics and Natural Language Processing, pages 31–40 c Hong Kong, November 4. 2019 Association for Computational Linguistics 2 event structure describes the specific framework and key points of various finance events. Applying fine-grained events is beneficial for learning a better text representation because the finance knowledge contained in event structure is helpful for understanding the semantic information. Inspired by the automatic event data generation method (Chen et al., 2017; Zeng et al., 2018; Yang et al., 2018), we propose the TOPIX1 Finance Event Dictionary (TFED) built by domain experts with professional finance knowledge and adopt it to extract fine-grained events automatically for most of finance news. Then we design two different neural models: Structured Stock Prediction Model (SSPM) and Multi-task Structured Stock Prediction Model (MSSPM). SSPM fuses the extracted fine-grained event and news text firstly, and then conduct interaction between text data and stock trade data to make prediction. SSPM outperforms all the baselines but it can hardly handle the news that can not be recognized by TFE"
D19-5105,D18-1156,0,0.105127,"ock prediction can give feedback to event extraction. So the joint learning can share valuable information between tasks. Result shows that MSSPM outperforms SSPM on the uncovered news and increases the method’s generalizability. The contributions of this work are summarized as follows: 2.1 Related Work Automatically Event Data Labeling According to (Chen et al., 2015; Liu et al., 2018; Huang et al., 2018), the fine-grained event structure contains event types, event trigger words and event roles. Zhou et al. (2015) propose a framework to extract events from twitter automatically. Yang et al. (2018) employ a predefined dictionary to label events and then extract document-level events from Chinese finance news. However, they only conduct experiments on 4 event types. While we employ a widely-covered dictionary with 32 different event types. Chen et al. (2017) adopt world and linguistic knowledge to detect event roles and trigger words from text. Zeng et al. (2018) use the Freebase CVT structure to label data and extract event. Araki and Mitamura (2018) adopt distant supervision to extract event from open domain. There are some works using either manual rules (Arendarenko and Kakkonen, 201"
D19-5105,P14-5010,0,0.00250958,"indicating event role of each word in xi . y i =  i the i i y1 , y2 , ..., yM is a sequence of trade record vectors for each trade minute with length of M . si ∈ {0, 1} is the stock movement label telling whether the stock trade price is up or down at prediction time. The stock movement prediction task can be defined as assigning movement label for the news input and trade data input. 1. Extract Auxiliary Information. In this step, we extract the auxiliary information of news: POS Tagging (lexical information) and Dependency Relation (syntactic information) by the popular Standford CoreNLP2 (Manning et al., 2014). 2. Filter Event Candidates. We filter the news that may be an event instance by the TFED. News that contains any trigger word(s) in the dictionary will be regarded as a candidate of the related event. 2 https://stanfordnlp.github.io/ CoreNLP/ 33 Event Label Embedding News Embedding +Bi-LSTM Stock Vectorization + BiLSTM ?? ?? ?? Self-Att Self-Att ?? Tensor Fuse ?? ?′? CoAtt ?? ?? Gated-Sum Gated-Sum ?? Soft ?? max Stock Pred Figure 2: The overview of the proposed SSPM model. 4.2 Trade Data Embedding teract text and stock data by co-attention to predict stock movement. There are 4 modules in S"
D19-5105,P16-2022,0,0.0159767,"he same way we get the self-attention result of the stock data: Sy ∈ RT ×2h . In the &lt;S,P,O&gt; method, event roles are extracted as separated phrases where some words are ignored and the word order information is missing. Instead, we fuse the text representation Sx with the event role embedding Ee to capture the structure information and remain the word order at the same time. Ee contains both word-level (event role) and sentence-level (BIO label) information, which is similar with Sx , so we select to fuse Ee with Sx instead of Ex . Here we adopt the fusion function used in (Wang et al., 2018; Mou et al., 2016) to fuse the event structure and text effectively: Gx = g(Hx0 , Cx ) · Cx + (1 − g(Hx0 , Cx )) · Hx0 Gy = g(Sy , Cy ) · Cy + (1 − g(Sy , Cy )) · Sy where the g(, ) is the gating function and we use the non-linear transformation with sigmod activation function in experiment. 4.3.4 In this module, we concatenate the Gx and Gy and predict the stock movement label pb: pb(s|x, y, e) = softmax(Wp [Gx ; Gy ] + bp ) 4.4 Hx0 = σ(Wf [Sx ; Ee ; Sx − Ee ; Sx ◦ Ee ] + bf ) ; means tensor connection. We ensure De = 2h so that Ee has the same dimension with Sx . ◦ means element-wise multiplication and σ is t"
D19-5105,D14-1162,0,0.0839522,"llect the following items: (1) First/last/highest/lowest trade price of the minute; (2) Total trade volume/value of the minute; (3) Volume-weighted average trade price. The stock trade data is of time series data, so in order to apply the powerful time series neural models, we transfer the raw trade features into trade data embedding Ey . The following combination performs best on the develop set: 4.3.1 Input Embedding The purpose of this module is to transfer various sources of input (x, y, e) into dense vectors. For words in finance news x, we use both word-level pretrained embedding Glove (Pennington et al., 2014) and character-level pretrained embedding ELMo (Peters et al., 2018) for the purpose of representing words better from different levels. Then we concatenate them to get the final word representation Ex ∈ RL×Dw . We use the method proposed in Section 4.2 to get the stock trade data embedding Ey ∈ RT ×Ds . Besides, we embed event role labels e into dense vectors Ee ∈ RL×De using a parameter matrix initialized with random values. Dw , Ds , De are the embedding dimensions of word, stock and event role, respectively. T is the length of stock time-steps. • Raw Number: first/last/highest/lowest trade"
D19-5105,N18-1202,0,0.0180656,"the minute; (2) Total trade volume/value of the minute; (3) Volume-weighted average trade price. The stock trade data is of time series data, so in order to apply the powerful time series neural models, we transfer the raw trade features into trade data embedding Ey . The following combination performs best on the develop set: 4.3.1 Input Embedding The purpose of this module is to transfer various sources of input (x, y, e) into dense vectors. For words in finance news x, we use both word-level pretrained embedding Glove (Pennington et al., 2014) and character-level pretrained embedding ELMo (Peters et al., 2018) for the purpose of representing words better from different levels. Then we concatenate them to get the final word representation Ex ∈ RL×Dw . We use the method proposed in Section 4.2 to get the stock trade data embedding Ey ∈ RT ×Ds . Besides, we embed event role labels e into dense vectors Ee ∈ RL×De using a parameter matrix initialized with random values. Dw , Ds , De are the embedding dimensions of word, stock and event role, respectively. T is the length of stock time-steps. • Raw Number: first/last/highest/lowest trade price, total trade volume and volumeweighted average trade price •"
D19-5105,D14-1120,0,0.424478,"e events. In Figure 1, the fine-grained event employs Type instead of Subject used in the coarse-grained event and employs Value instead of Object, which can describe the event roles in a more detailed way. In this work, we propose to incorporate the fine-grained events in one-day-ahead stock movement prediction. The fine-grained Introduction Stock movement plays an important role in economic activities, so the prediction of stock movement has caught a lot of attention of researchers. In recent years, employing the stock related text (such as finance news or tweets) has become the mainstream (Si et al., 2014; Ding et al., 2015; Li et al., 2015; Alostad and Davulcu, 2017; Zhong et al., 2017; Zhang et al., 2018a) of stock movement prediction task. In these text-based stock prediction works, various methods are proposed to extract semantic information from stock related text to help the prediction of stock movement. There are mainly two methods of applying text: employing raw text (Hu et al., 2018; Xu and Cohen, 2018) or coarse-grained &lt;S,P,O&gt; structure (subject, predicate and object) extracted from ∗ This work is done when Deli Chen is a intern at Mizuho Securities. 31 Proceedings of the Second Wor"
D19-5106,baccianella-etal-2010-sentiwordnet,0,\N,Missing
D19-5106,D14-1120,0,\N,Missing
D19-5106,C16-1201,0,\N,Missing
D19-5106,P18-1183,0,\N,Missing
D19-5106,P18-1158,0,\N,Missing
D19-5106,C18-1239,0,\N,Missing
D19-5106,N19-1035,0,\N,Missing
D19-5106,N19-1242,0,\N,Missing
D19-5106,P16-2022,0,\N,Missing
E09-1088,P08-1024,0,0.0372396,"erence (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequent section describes an overview of DPLVM models"
E09-1088,W04-1213,0,0.0129012,"fine tuning strategy were kept the same. Table 1: Feature templates used in the Bio-NER experiments. wi is the current word, ti is the current POS tag, oi is the orthography mode of the current word, and hi is the current latent variable (for the case of latent models) or the current label (for the case of conventional models). No globally dependent features were used; also, no external resources were used. 4.1 BioNLP/NLPBA-2004 Shared Task (Bio-NER) Our first experiment used the data from the BioNLP/NLPBA-2004 shared task. It is a biomedical named-entity recognition task on the GENIA corpus (Kim et al., 2004). Named entity recognition aims to identify and classify technical terms in a given domain (here, molecular biology) that refer to concepts of interest to domain experts. The training set consists of 2,000 abstracts from MEDLINE; and the evaluation set consists of 404 abstracts from MEDLINE. We divided the original training set into 1,800 abstracts for the training data and 200 abstracts for the development data. The task adopts the BIO encoding scheme, i.e., B-x for words beginning an entity x, I-x for words continuing an entity x, and O for words being outside of all entities. The Bio-NER ta"
E09-1088,P05-1010,1,0.872438,"se a new inference algorithm, latent dynamic inference (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequen"
E09-1088,P06-1059,1,0.852877,"original training set into 1,800 abstracts for the training data and 200 abstracts for the development data. The task adopts the BIO encoding scheme, i.e., B-x for words beginning an entity x, I-x for words continuing an entity x, and O for words being outside of all entities. The Bio-NER task contains 5 different named entities with 11 BIO encoding labels. The standard evaluation metrics for this task are precision p (the fraction of output entities matching the reference entities), recall r (the fraction of reference entities returned), and the F-measure given by F = 2pr/(p + r). Following Okanohara et al. (2006), we used word features, POS features and orthography features (prefix, postfix, uppercase/lowercase, etc.), as listed in Table 1. However, their globally dependent features, like preceding-entity features, were not used in our system. Also, to speed up the training, features that appeared rarely in the training data were removed. For DPLVM models, we tuned the number of latent variables per label from 2 to 5 on preliminary experiments, and used the Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi } Table 2: Feature templates used in the NPchunking experiment"
E09-1088,N07-1051,0,0.05382,"ithm, latent dynamic inference (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequent section describes an o"
E09-1088,W00-0726,0,0.0189397,"ime(Ks) only poor features available. For example, in POStagging task and Chinese/Japanese word segmentation task, there are only word features available. For this reason, it is necessary to check the performance of the LDI on poor feature-set. We chose another popular task, the NP-chunking, for this study. Here, we used only poor feature-set, i.e., feature templates that depend only on words (see Table 2 for details), taking into account 200K features. No external resources were used. The NP-chunking data was extracted from the training/test data of the CoNLL-2000 shallowparsing shared task (Sang and Buchholz, 2000). In this task, the non-recursive cores of noun phrases called base NPs are identified. The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. Our preliminary experiments in this task suggested the use of 5 latent variables for each label on latent models. 70 70 69 69 68 #latent-path Table 3: On the test data of the Bio-NER task, experimental comparisons among various inference algorithms on DPLVMs, and the performance of CRFs. S.A. signifies sentence accuracy. As can be seen, at a much lower cost, the LDI-A (A signifies approximation) performed slightly be"
E09-1088,N03-1028,0,0.0338243,", ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi } Orth. Features: {oi−2 , oi−1 , oi , oi+1 , oi+2 , oi−2 oi−1 , oi−1 oi , oi oi+1 , oi+1 oi+2 } ×{hi , hi−1 hi } The training stage was kept the same as Morency et al. (2007). In other words, there is no need to change the conventional parameter estimation method on DPLVM models for adapting the various inference algorithms in this paper. For more information on training DPLVMs, refer to Morency et al. (2007) and Petrov and Klein (2008). Since the CRF model is one of the most successful models in sequential labeling tasks (Lafferty et al., 2001; Sha and Pereira, 2003), in this paper, we choosed CRFs as a baseline model for the comparison. Note that the feature sets were kept the same in DPLVMs and CRFs. Also, the optimizer and fine tuning strategy were kept the same. Table 1: Feature templates used in the Bio-NER experiments. wi is the current word, ti is the current POS tag, oi is the orthography mode of the current word, and hi is the current latent variable (for the case of latent models) or the current label (for the case of conventional models). No globally dependent features were used; also, no external resources were used. 4.1 BioNLP/NLPBA-2004 Shar"
E09-1088,C08-1106,1,0.923339,"ables: An Exact Inference Algorithm and Its Efficient Approximation Xu Sun† Jun’ichi Tsujii†‡§ † Department of Computer Science, University of Tokyo, Japan ‡ School of Computer Science, University of Manchester, UK § National Centre for Text Mining, Manchester, UK {sunxu, tsujii}@is.s.u-tokyo.ac.jp Abstract and hidden Markov models (HMMs). Petrov and Klein (2008) reported on a syntactic parsing task that DPLVM models can learn more compact and accurate grammars than the conventional techniques without latent variables. The effectiveness of DPLVMs was also shown on a syntactic chunking task by Sun et al. (2008). Latent conditional models have become popular recently in both natural language processing and vision processing communities. However, establishing an effective and efficient inference method on latent conditional models remains a question. In this paper, we describe the latent-dynamic inference (LDI), which is able to produce the optimal label sequence on latent conditional models by using efficient search strategy and dynamic programming. Furthermore, we describe a straightforward solution on approximating the LDI, and show that the approximated LDI performs as well as the exact LDI, while"
E09-1088,W02-1019,0,\N,Missing
E17-2113,D15-1141,0,0.0531655,"obability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. 2.1 The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: m 1 X λ J(θ) = qi (θ) + ||θ||2 m 2 Transition Probability qi (θ) = max (s(xi , li , θ)+∆(li , li ))−s(xi , li , θ) B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). However, B-LSTM cannot learn sentence level label information. Huang et al. (2015) combine CRF to use sentence level label information. We combine transition probability into our model to gain sentence level label information. To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014) based on BLSTM. The prediction of label in position t is given as: yt = sof tmax(Why ∗ ht + by ) li ∈Y (xi ) By minimizing the object, we can increase the score of correct label sequence l and decrease the score of incor"
E17-2113,N15-1075,0,0.0414504,"tly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result. 1 Introduction With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have nar• We propose a method to directly train on FScore rather than label accuracy"
E17-2113,P15-1090,0,0.0208046,"accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result. 1 Introduction With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have nar• W"
E17-2113,C02-1025,0,0.120488,"ce on named entity and nominal mention. Results and Analysis We evaluate two methods to incorporate word segmentation information. The results of two methods are shown as Table 2. We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus. We adopt positional character embeddings in our next four models. Our first model is a BLSTM neural network (baseline). To take advantage of traditional model (Chieu and Ng, 2002; Mccallum et al., 2001) such as CRF, we combine transition probability in our B-LSTM based MMNN. We design a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The reTo better understand the impact of the factor β, we show the results of our integrated model with different values of β in Figure 1(c). From Figure 1(c), we can know that β is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social"
E17-2113,N15-1167,0,0.0225771,"ese Social Media Hangfeng He and Xu Sun MOE Key Laboratory of Computational Linguistics, Peking University School of Electronics Engineering and Computer Science, Peking University {hangfenghe, xusun}@pku.edu.cn Abstract rowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER (Peng and Dredze, 2016). However, the two above approaches are implemented within CRF mode"
E17-2113,C14-1030,0,0.0547003,"Missing"
E17-2113,I08-4013,0,0.53649,"media. We present two methods to use word segmentation information in neural network model. Character and Position Embeddings To incorporate word segmentation information, we attach every character with its positional tag. This method is to distinguish the same character at different position in the word. We need to word segment the text and learn positional character embeddings from the segmented text. 3.2 Parameter Estimation We pre-trained embeddings using word2vec (Mikolov et al., 2013) with the skip-gram training model, without negative sampling and other default parameter settings. Like Mao et al. (2008), we use bigram features as follow: Cn Cn+1 (n = −2, −1, 0, 1) and 1 2 715 We fix some labeling errors of the data. https://github.com/fxsjy/jieba. C−1 C1 Methods Character+Segmentation Character+Position Named Entity Precision Recall F1 48.52 39.23 43.39 65.87 39.71 49.55 Nominal Mention Precision Recall F1 58.75 47.96 52.91 68.12 47.96 56.29 Table 2: Two methods to incorporate word segmentation information. Models (Peng and Dredze, 2015) (Peng and Dredze, 2016) B-LSTM B-LSTM + MMNN F-Score Driven I (proposal) F-Score Driven II (proposal) Named Entity Precision Recall F1 57.98 35.57 44.09 63."
E17-2113,P15-4006,0,0.0228822,"Missing"
E17-2113,P14-1028,0,0.140289,"ion Probability qi (θ) = max (s(xi , li , θ)+∆(li , li ))−s(xi , li , θ) B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). However, B-LSTM cannot learn sentence level label information. Huang et al. (2015) combine CRF to use sentence level label information. We combine transition probability into our model to gain sentence level label information. To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014) based on BLSTM. The prediction of label in position t is given as: yt = sof tmax(Why ∗ ht + by ) li ∈Y (xi ) By minimizing the object, we can increase the score of correct label sequence l and decrease the score of incorrect label sequence l. 2.2 s(c[1:n] , l[1:n] , θ) = (1) m ∂J 1 X ∂s(x, lmax , θ) ∂s(x, l, θ) = − ) + λθ ( ∂θ m ∂θ ∂θ i=1 In the subgradient, we can know that structured margin loss ∆(l, l) contributes nothing to the subgradient of the regularized objective function J(θ). The margin loss ∆(l, l) serves as a trigger function to conduct the training process of BLSTM based MMNN. W"
E17-2113,D15-1064,0,0.680744,"University {hangfenghe, xusun}@pku.edu.cn Abstract rowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER (Peng and Dredze, 2016). However, the two above approaches are implemented within CRF model. We construct a semisupervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical information provided by massive unlabeled tex"
E17-2113,P16-2025,0,0.312854,"task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER (Peng and Dredze, 2016). However, the two above approaches are implemented within CRF model. We construct a semisupervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical information provided by massive unlabeled text. To shrink the gap between label accuracy and F-Score, we propose a method to directly train on F-Score rather than label accuracy in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Specifically, we make contributions as follows: We focus on named entity recognition (NER) for Chinese social media. With"
E17-2113,D14-1089,0,0.0148436,"pose an integrated training method in our fourth model F-Score Driven Model II .The reTo better understand the impact of the factor β, we show the results of our integrated model with different values of β in Figure 1(c). From Figure 1(c), we can know that β is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media. 4 Conclusions and Future Work The results of our experiments also suggest directions for future work. We can observe all models in Table 3 achieve a much lower recall than precision (Pink et al., 2014). So we need to design some methods to solve the problem. 716 Acknowledgements Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and Luke Zettlemoyer. 2013. Joint coreference resolution and named-entity linking with multi-pass sieves. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289–299, Seattle, Washington, USA, October. Association for Computational Linguistics. This work was supported in part by National Natural Science Foundation of China (No. 61673028), and National High Technology Research and Development Program of China (863 Program,"
E17-2113,D15-1058,0,0.0221761,"del which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result. 1 Introduction With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have nar• We propose a method to directly train on FScore rat"
E17-2113,E14-3003,0,0.0250859,"ecognition in Chinese Social Media Hangfeng He and Xu Sun MOE Key Laboratory of Computational Linguistics, Peking University School of Electronics Engineering and Computer Science, Peking University {hangfenghe, xusun}@pku.edu.cn Abstract rowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER (Peng and Dredze, 2016). However, the two above approaches are implemen"
E17-2113,J14-3004,1,0.611551,"formation exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have nar• We propose a method to directly train on FScore rather than label accuracy. In addition, we propose an integrated method to train on both F-Score and label accuracy. • We combine transition probability into our BLSTM based max margin neural network to form structured output in neural network. 713 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 713–718, c Valencia, Spain,"
E17-2113,P15-1058,0,0.0267613,"method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result. 1 Introduction With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have nar• We propose a method to directly train on FScore rather than label accuracy. In addition, we propose an integrated method to train on both F-Score and l"
I13-1073,W04-1102,0,0.0904726,"Missing"
I13-1073,W02-1001,0,0.105237,"the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also show the discriminate accuracy. The discriminate accuracy checks the accuracy of discriminating positive and negative full forms, without comparing the generated abbreviations with the gold-standard abbreviations. As we can see from Table 2, first, the best system is the system Unified-Assum.1-"
I13-1073,P08-2016,0,0.646784,"Missing"
I13-1073,P02-1021,0,0.0505622,"Missing"
I13-1073,P09-1102,1,0.611546,"Missing"
I13-1073,P12-1027,1,0.813122,"inally, the CRF model outperforms the MEMM and averaged perceptron models. To summarize, the unified system with assumption-1, global information, and CRF model has the best performance. For evaluating web search quality based on a set of queries, we use the macro-averaging and microaveraging of the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also s"
I13-1073,N09-2069,0,0.541149,"Missing"
I13-1073,W06-0103,0,\N,Missing
I17-1019,J04-1004,0,0.366809,"he 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption problem for CWS. One major family proposed to compose boundary features by fitting the relevance of consecutive characters using Accessor Variety (AV) (Feng et al., 2004a,b), or Chi-square Statistics (Chi2) (Chang and Han, 2010). Combining the boundary features with other hand-crafted features, these methods were shown to achieve better performance on OOV words. Inspired by these models, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed to model boundary features dynamically. This structure can learn to utilize the target domain corpus and extract the correlation or irrelevance between characters, which is a reminiscence of the discrete boundary features such as Accessor Variety (AV). The contribution"
I17-1019,P13-1075,0,0.125587,"Missing"
I17-1019,J96-1002,0,0.297775,"Missing"
I17-1019,P16-1039,0,0.266099,"rained on a different corpus. The results are not directly comparable. The results prove the incredible effectiveness of the global recurrent structure on OOV recognition and overall segmentation, comparable to the BLSTM model that directly incorporates discrete AV features. Adding discrete AV features into our model seem not to be a notable improvement, which also confirms that our model already has certain domain adaption ability. OOV Recall 83.01 82.59 83.78 Models (Zheng et al., 2013) (Pei et al., 2014) (Chen et al., 2015a) (Chen et al., 2015b) (Chen et al., 2015a)* (Chen et al., 2015b)* (Cai and Zhao, 2016) (Zhang et al., 2016) BLSTM This work Table 4: IV and OOV recalls on the PKU development data. Methods BLSTM BLSTM-2 GRS-4 IV Recall 96.35 96.11 96.25 OOV Recall 82.67 82.01 83.96 Table 5: IV and OOV recalls on the PKU test data. 5.3 PKU 92.8 95.2 96.4 96.5 94.5 94.8 95.5 95.7 95.9 95.9 MSRA 93.9 97.2 97.6 97.4 95.4 95.6 96.5 97.7 97.0 97.1 Table 7: Comparison of our model with previous neural models on the PKU and MSRA datasets. Results with * are from runs on their released implementation (Cai and Zhao, 2016). Final Results In this section, We compare our BLSTM+GRS-4 model with previous stat"
I17-1019,D10-1077,0,0.110382,"e Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption problem for CWS. One major family proposed to compose boundary features by fitting the relevance of consecutive characters using Accessor Variety (AV) (Feng et al., 2004a,b), or Chi-square Statistics (Chi2) (Chang and Han, 2010). Combining the boundary features with other hand-crafted features, these methods were shown to achieve better performance on OOV words. Inspired by these models, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed to model boundary features dynamically. This structure can learn to utilize the target domain corpus and extract the correlation or irrelevance between characters, which is a reminiscence of the discrete boundary features such as Accessor Variety (AV). The contributions of this paper are two folds: Figure 1: General architectu"
I17-1019,N16-1030,0,0.124294,"s of the structure that are shown in Figure 3. GRS-2 To better fit the boundary features, we add a full-connection hidden layer following the recurrent network. The boundary feature embeddings are calculated as follows: Embbf (bi ) = σ(Wbf hi + bbf ) where trii = ci−1 ci ci+1 and other values have the same meanings as above. 4 Training Instead of using the Max-Margin criterion (Taskar et al., 2005) adopted by previous neural network models for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b), we try to directly maximize the log-probability of the correct tag sequence following Lample et al. (2016): (6) where σ is the logistic sigmoid function. GRS-3 Considering the hidden states are noisy and contains much information of other words, we want the hidden values more relevant to the current bigram, so a gate is introduced to the structure. The boundary feature embeddings are calculated log(p(y|X)) = s(X, y) − log( X es(X,˜y) ) y˜∈YX = s(X, y) − logadd s(X, y˜) y˜∈YX 187 (9) Figure 3: Four variants of the global recurrent structure. where YX represents all possible tag sequences for a sentence X. While decoding, we predict the output sequence which obtains the maximum score as follows: y ∗"
I17-1019,P15-1168,0,0.358598,"001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, the InVocabulary (IV)"
I17-1019,J09-4006,0,0.0145467,"4) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has been pursued with considerable efforts in the Chinese NLP community. One mainstream method is regarding word segmentation task as a s"
I17-1019,D14-1093,0,0.750532,"n performance on OOV words, which empirically proves its domain adaption ability. BSLTM-2, similar to LSTM2 (Chen et al., 2015b), is an architecture comprised of two stacking bidirectional LSTM hidden layers. GRS-4 is short for BLSTM+GRS-4 model. Methods BLSTM BLSTM-2 GRS-4 IV Recall 97.12 96.89 96.91 ble 6. We also attempt to integrate discrete boundary features into the models. In our experiments, we choose the Accessor Variety(AV) (Feng et al., 2004a,b) which is a feature widely used in traditional Chinese word segmentation. Our F-scores and OOV recalls are competitive to those reported by Liu et al. (2014) and Jiang et al. (2013). However, following Liu et al. (2014)’s setting, we choose the PKU dataset as the training corpus while Jiang et al. (2013)’s model is trained on a different corpus. The results are not directly comparable. The results prove the incredible effectiveness of the global recurrent structure on OOV recognition and overall segmentation, comparable to the BLSTM model that directly incorporates discrete AV features. Adding discrete AV features into our model seem not to be a notable improvement, which also confirms that our model already has certain domain adaption ability. OO"
I17-1019,D15-1092,0,0.130797,"001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, the InVocabulary (IV)"
I17-1019,P14-1028,0,0.701386,"Lafferty et al., 2001). Furthermore, rich features can be incorporated into these systems to improve their performances and most state-of-the-art systems are still based on feature-based models. Recently, neural network models are drawing increasing attention in Natural Language Processing (NLP) tasks. They significantly reduced feature engineering effort and achieved competitive or state-of-the-art results in many NLP tasks. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many neural network models (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b) have been applied to CWS and some approached state-of-the-art performance. However, these neural network models, as well as other supervised methods, do not work well in domain adaptation. In recent years, manually annotated training corpus mostly come from the news domain. When it shifts to other domains such as literature or medicine, where there are many domain-related words that rarely appear in other domains, Out-of-Vocabulary (OOV) word recognition becomes an important problem. Moreover, different domains means different language usages and contexts. Therefore, th"
I17-1019,I05-3017,0,0.518186,"Missing"
I17-1019,C04-1081,0,0.879334,"Segmentation, especially OOV-Recall, which brings benefits to domain adaptation. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data. 1 Introduction Since Chinese writing system does not have explicit word delimiters, word segmentation becomes an essential first step for further Chinese language processing. In recent years, Chinese Word Segmentation (CWS) has experienced great advancement. One mainstream method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004) where each character is assigned a tag indicating its position in the word. This method has been proved ∗ Corresponding author 184 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address the domain adaption"
I17-1019,D11-1090,0,0.150039,"→ ← (2) where Wd ∈ R|T |×H2 , bd ∈ R|T |. H2 is the number of hidden units of the outputs for the BLSTM layer. f (ti |c[i−w/2:i+w/2] ) ∈ R|T |is the score vector for each possible tag. Here in Chinese word segmentation, we set T = {S, B, E, M }. 2.3 Global Recurrent Structure Chinese word segmentation is essentially a task of resolving the relevance of consecutive characters. Lacking knowledge of such relevance, recognizing out-of-domain words has been the bottleneck of domain adaption in CWS. However, Boundary features such as Accessor Variety (AV) (Feng et al., 2004a,b), Mutual Information (Sun and Xu, 2011) and Chi-square Statistics (Chi2) (Chang and Han, 2010) are features designed to fit such relevance. A significant advantage of boundary features is that they can compute the correlation of characters from a large scale corpora, annotated or not, to boost the OOV word recognition performance. As a result, they are especially effective for cross-domain CWS. In this paper, we propose 5 novel global recurrent structures to generate embeddings that mimic the boundary features for further computing, which needs minimal pre-processing and feature engineering. The structures are designed to capture t"
I17-1019,P16-2092,1,0.828577,"Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011; Zheng et al., 2013; Qi et al., 2014) to reduce efforts of feature engineering. Pei et al. (2014) used a neural tensor model to capture the complicated interactions between tags and context characters. Experiments in his paper also show that bigram embeddings are of great benefit. To incorporate complicated combinations and long-term dependency information of the context characters, gated recursive model (Chen et al., 2015a) and LSTM model (Chen et al., 2015b) were used respectively. Moreover, Xu and Sun (2016) proposed a dependency-based gated recursive model which merges the benefits of the two models above. Coincidentally, Cai and Zhao (2016) and Zhang et al. (2016) both addressed the problem of lacking word-based features that previous neural CWS models have. Cai and Zhao (2016) proposed a novel gated combination neural network which thoroughly eliminates context windows and can utilize complete segmentation history. Zhang et al. (2016) proposed a transition-based neural model which replaces manually designed discrete features with neural features. Domain adaption for Chinese word segmentation h"
I17-1019,O03-4002,0,0.924886,"inese Word Segmentation, especially OOV-Recall, which brings benefits to domain adaptation. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data. 1 Introduction Since Chinese writing system does not have explicit word delimiters, word segmentation becomes an essential first step for further Chinese language processing. In recent years, Chinese Word Segmentation (CWS) has experienced great advancement. One mainstream method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004) where each character is assigned a tag indicating its position in the word. This method has been proved ∗ Corresponding author 184 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP where each character stands for ‘three’, ‘gather’, ‘cyanide’ and ‘amine’. The four characters are totally irrelevant. A supervised CWS system trained on news domain corpus would face great challenges on segmenting this word correctly Several approaches have been proposed to address"
I17-1019,P13-2032,1,0.859292,"e 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has"
I17-1019,D13-1031,1,0.891787,"e 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora. Word segmentation has"
I17-1019,E14-1062,0,0.0643886,"r model thereupon performs better with the increase of the size of testing corpus as long as the OOV words appear more. Although the trendline of our model is promising, there are some OOV words that occurs frequently but are wrongly segmented. Some examples are listed in Table 8. Errors involving OOV Word English Correct Total 肾脏 kidney 15 39 甲型H1N1流感 influenza A 0 30 (H1N1) 维生素C vitamin C 2 23 Table 8: Some examples of wrongly segmented OOV words with high frequency. 190 Figure 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li"
I17-1019,P16-1040,0,0.0135546,"feature engineering. Pei et al. (2014) used a neural tensor model to capture the complicated interactions between tags and context characters. Experiments in his paper also show that bigram embeddings are of great benefit. To incorporate complicated combinations and long-term dependency information of the context characters, gated recursive model (Chen et al., 2015a) and LSTM model (Chen et al., 2015b) were used respectively. Moreover, Xu and Sun (2016) proposed a dependency-based gated recursive model which merges the benefits of the two models above. Coincidentally, Cai and Zhao (2016) and Zhang et al. (2016) both addressed the problem of lacking word-based features that previous neural CWS models have. Cai and Zhao (2016) proposed a novel gated combination neural network which thoroughly eliminates context windows and can utilize complete segmentation history. Zhang et al. (2016) proposed a transition-based neural model which replaces manually designed discrete features with neural features. Domain adaption for Chinese word segmentation has been widely exploited before neural CWS models are proposed. Jiang et al. (2013) utilized the web text(160K Wikipedia) to improves seg7 Conclusion and Perspec"
I17-1019,I08-1002,0,0.0345857,"igh frequency. 190 Figure 4: OOV word recognition accuracies on the Medicine corpus. 6 Related Work mentation accuracies on several domains. Zhang et al. (2014) studied type-supervised domain adaptation for Chinese segmentation by making use of domain-specific tag dictionaries and only unlabeled target domain data. Liu et al. (2014) proposed a variant CRF model to leverage both fully and partially annotated data transformed from different sources of free annotations consistently. Some researches which focus on making use of unlabeled data for word segmentation also do help to domain adaption. Zhao and Kit (2008) and Zhang et al. (2013a) improved segmentation performance by mutual information between characters, collected from large unlabeled data. Li and Sun (2009) used punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words. Sun and Xu (2011) explored several statistical features derived from both unlabeled data to help improve character-based word segmentation. Zhang et al. (2013b) proposed a semi-supervised approach that dynamically extracts representations of label distributions from both in-domain corpora and out-of-domain corpora."
I17-1019,W10-4126,0,0.120103,"Missing"
I17-1019,D13-1061,0,0.495213,"Missing"
I17-1050,P14-5010,0,0.00631574,"Missing"
I17-1050,P97-1013,0,0.167118,"is ADJP JJ likely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in"
I17-1050,P17-1152,0,0.134763,"orporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. An"
I17-1050,W12-1614,0,0.0606847,"b-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Rec"
I17-1050,D14-1162,0,0.0804132,"s are extremely imbalanced in PDTB. However, recent work put more emphasis on the multi-class classification, where the goal is to identify a discourse relation from all possible choices. According to Rutherford and Xue (2014), the multi-class classification setting is more natural and realistic. Moreover, the multi-class classifier can directly serve as one building block of a complete discourse parser (Qin et al., 2017). Therefore, in this work, we will focus on the multiω 50 τ 50 d 250 η 0.01 λ 0.0001 b 10 Table 2: Hyper-parameters of our model The Pre-trained 50-dimentional Glove Vectors (Pennington et al., 2014), which is caseinsensitive, are used for initializing the word embeddings and they are tuned together with other parameters in the same learning rate during training. 501 Systems Zhang et al. (2015) Rutherford and Xue (2014) Rutherford and Xue (2015) Liu et al. (2016) Liu and Li (2016) Ji et al. (2016) Tag-Enhanced Tree-LSTM Tag-Enhanced Tree-GRU We adopt the AdaGrad optimizer (Duchi et al., 2011) for training our model and we validate the performance every epoch. It takes around 5 hours (5 epochs) for the Tag-Enhanced Tree-LSTM and 4 hours (6 epochs) for the Tag-Enhanced TreeGRU model to conv"
I17-1050,P16-1078,0,0.332691,"oposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful p"
I17-1050,P09-1077,0,0.196333,"urse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether su"
I17-1050,N16-1037,0,0.0247637,"al neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5 CD 0.4 0.3 And in Table 4, we present the following systems, which focus on Level-2 classification: VBG 0.1 0.1 PP WP$ PRP$ WHADJP NX VP WHADVP WHNP NNP S PRT NN ADJP TO RP PDT VBZ RBR -LRB- EX NP-TMP VB FRAG INTJ JJR ADVP LS RRC SBAR MD NNPS RB FW VBP RBS NP SQ LST JJS QP PRN CC WP UCP 0.2 • Lin et al. (2009) uses traditional featurebased model to classify relations. Especially, constituent and dependen"
I17-1050,E17-2093,0,0.24905,"d on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the representation of smaller text units into larger text spans along the syntactic parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encodi"
I17-1050,prasad-etal-2008-penn,0,0.123225,"or identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic modeling for implicit discourse relation classification. 2.1 Implicit Discourse Relation Classication Discourse relation identification is an important but difficult sub-component of discourse analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discour"
I17-1050,D09-1036,0,0.0910292,"Missing"
I17-1050,P15-1132,0,0.0242798,"k models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient exploding or vanishing problem due to th"
I17-1050,D16-1130,1,0.705053,"“but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve"
I17-1050,D16-1246,0,0.0767037,"tification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity of deep learning methods, many cutting-edge models are also applied to our task of implicit discourse relation classification. Qin et al. (2016) tries to model the sentences with Convolutional Neural Networks. Liu and Li (2016) encodes the text with Long Short Term Memory model and employ multi-level attention mechanism to capture important signals. Qin et al. (2017) proposes a framework based on adversarial network to incorporate the connective information. To be noted, Ji and Eisenstein (2014) adopts Recursive Neural Network to exploit the representation of sentences and entities, which is the first yet simple tree-structured neural network applied in this task. Tree-structured neural networks, which recursively compose the represen"
I17-1050,P17-1093,0,0.248193,"plicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have been adopted to encode the arguments in each relation, ranging from traditional feature-based models (Lin et al., 2009; Pitler et al., 2009) to the currently prevailing deep learning methods (Ji and Eisenstein, 2014; Liu and Li, 2016; Qin et al., 2017). Despite of the superior ability of the deep learning models, the syntactic 496 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Related Work information, which proves to be helpful for identifying discourse relations in many early studies (Subba and Di Eugenio, 2009; Lin et al., 2009), is seldom employed by recent work. Therefore we are curious to explore whether such missing syntactic information can be leveraged in deep learning methods to further improve the semantic model"
I17-1050,E14-1068,0,0.216059,"e analysis. One fundamental step forward recently is the release of the large-scale Penn Discourse TreeBank (PDTB) (Prasad et al., 2008), which annotates discourse relations with their two textual arguments over the 1 million word Wall Street Journal corpus. The discourse relations in PDTB are broadly categorized as either “Explicit” or “Implicit” according to whether there are connectives in the original text that can indicate the sense of the relations. In the absence of explicit connectives, identifying the sense of the relations has proved to be much more difficult (Park and Cardie, 2012; Rutherford and Xue, 2014) since the inferring is solely based on the arguments. Prior work usually tackles this task of implicit discourse relation identification as a classification problem with the classes defined in PDTB corpus. Early attempts use traditional various featurebased methods and the work inspiring us most is Lin et al. (2009), in which they show that the syntactic parse structure can provide useful signals for discourse relation classification. More specifically they employ the production rules with constituent tags (e.g., SBJ) as features and get competitive performance. Recently, with the popularity"
I17-1050,N15-1081,0,0.0166996,"ompared with other state-of-the-art systems. model have less parameters to train which could alleviate the problem of overfitting and also cost less training time. 4.4 Comparison with Other Systems For a comprehensive study, we compare our models with other state-of-the-art systems. The systems that conduct Level-1 classification are reported in Table 3, including: • Zhang et al. (2015) proposes to use convolutional neural networks to encode the arguments. • Rutherford and Xue (2014) manually extracts features to represent the arguments and use a maximum entropy classifier for classification. Rutherford and Xue (2015) further exploits discourse connectives to enrich the training data. • Liu et al. (2016) employs a multi-task framework that can leverage other discourserelated data to help with the training of discourse relation classifier. • Liu and Li (2016) represents arguments with LSTM and introduces a multi-level attention 502 0.8 mechanism to model the interaction between the two arguments. 0.7 • Ji et al. (2016) treats the discourse relation as latent variable and proposes to model them jointly with the sequences of words using a latent variable recurrent neural network architecture. WHPP 0.6 VBD 0.5"
I17-1050,D13-1170,0,0.00737155,"f text, tree-structured neural network models show superior ability in a variety of semantic modeling tasks, such as sentiment classification (Kokkinos and Potamianos, 2017), natural language inference (Chen et al., 2017) and machine translation (Eriguchi et al., 2016). The earliest and simplest tree-structure neural network is the Recursive Neural Network proposed by Socher et al. (2011), in which a global matrix is learned to linearly combine the contituent vectors. This work is further extended by replacing the global matrix with a global tensor to form the Recursive Neural Tensor Network (Socher et al., 2013). Based on them, Qian et al. (2015) first proposes to incorporate tag information, which is very similar as our idea described in Section 3.2, by either choosing a composition function according to the tag of a phrase (TagGuided RNN/RNTN) or combining the tag embeddings with word embeddings (Tag-Embedded RNN/RNTN). Our method of incorporating tag information improves from theirs and somewhat combines these two methods by using the tag embedding to dynamically determine the composition function via the gates in LSTM or GRU. One fatal weakness of vanilla RNN/RNTN is the well-known gradient explo"
I17-1050,N03-1030,0,0.25336,"kely VP TO to Introduction VB expand Figure 1: An example of two sentences with their discourse relation as Expansion.Restatement.Specification. Subfigure (a) and (b) are partial parse trees of the two important phrases with yellow background. It is widely agreed that text units such as clauses or sentences are usually not isolated. Instead, they correlate with each other to form coherent and meaningful discourse together. To analyze how text is organized, discourse parsing has gained much attention from both the linguistic (Weiss and Wodak, 2007; Tannen, 2012) and computational (Marcu, 1997; Soricut and Marcu, 2003) communities, but the current performance is far from satisfactory. The most challenging part is to identify the discourse relations between text spans, especially when the discourse connectives (e.g., “because” and “but”) are not explicitly shown in the text. Due to the absence of such evident linguistic clues, trying to model and understand the meaning of the text becomes the key point in identifying such implicit relations. Previous studies in this field treat the task of recognizing implicit discourse relations as a classification problem and various techniques in semantic modeling have be"
I17-1050,N09-1064,0,0.0735037,"Missing"
I17-1050,P15-1150,0,0.628509,"parse tree, can tactfully combine syntatic tree structure with neural network models and recently achieve great success in several semantic modeling tasks (Eriguchi et al., 2016; Kokkinos and Potamianos, 2017; Chen et al., 2017). One useful property of these models is that the representation of phrases can be naturally captured while computing the representations from bottom up. Taking Figure 1 for an example, those highlighted phrases could provide important signals for classifying the discourse relation. Therefore, we will employ two latest tree-structuerd models, i.e. the Tree-LSTM model (Tai et al., 2015; Zhu et al., 2015) and the Tree-GRU model (Kokkinos and Potamianos, 2017), in our work. Hopefully, these models can learn to preserve or highlight such helpful phrasal information while encoding the arguments. Another important syntactic signal comes from the constituent tags on the tree nodes (e.g., NP, VP, ADJP). Those tags, derived from the production rules, describe the generative process of text and therefore could indicate which part is more important in each constituent. For example, considering a node tagged with NP, its child node tagged with DT is usually neglectable. Thus we propos"
I17-1050,D15-1266,0,0.0357118,"Missing"
I17-1064,D16-1171,0,0.35288,"Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat limited to either of the"
I17-1064,P14-5010,0,0.00535743,"ch sentence plays a different role, and from a specific view all the sentences should be paid different attention, in document-level sentiment classification. Here, we still consider the influence from user, product and their combination, and get the corresponding document representation du , dp , dup , where d∗ (∗ ∈ {u, p, up}) is computed as follows: d∗ = m X t=1 βt∗ s∗t Datasets To validate the effectiveness of our model, we use three real-world datasets: IMDB, Yelp 2013 and Yelp 2014 collected by Tang et al. (2015b). For these data, we preprocess the text including using Stanford CoreNLP (Manning et al., 2014) to split the review documents into sentences and tokenizing all words. Table 1 shows the details of the three datasets including number of documents (#docs), average number of documents per user posts(#docs/user) etc. It is also noted that IMDB is rated with 10 sentiment labels (i.e., 1-10 stars) while Yelp has 5 labels (i.e., 1-5 stars). We also adopt the same data partition used in (Tang et al., 2015b) and (Chen et al., 2016) for training, developing and test. (9) exp(γ(s∗t , δ ∗ )) βt∗ = Pm ∗ ∗ t=1 exp(γ(st , δ )) (10) Evaluation Metrics where s∗t is the sentence representation of sentence"
I17-1064,I13-1156,0,0.309598,"set to 10−5 . 3.1 Method Comparison To comprehensively evaluate the performance of CMA, we list some baseline methods for comparison. The baselines are introduced as follows. • Majority assigns the largest sentiment polarity occurred in the training set to each sample in the test set. • Trigram uses the unigrams, bigrams and trigrams features to train a SVM classifier for sentiment classification. • TextFeature extracts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196"
I17-1064,W02-1011,0,0.0225933,"ultiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway"
I17-1064,W06-3808,0,0.0534092,"ct (δ up ), with blue, red and green color series respectively. The deeper color means higher weight. Figure 2(a)∼(f) display the three ways of attention to words for sentence1 ∼ sentence6 respectively. We can see that different words are as4 Related work Document-level sentiment classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro"
I17-1064,C10-1103,0,0.0311466,"ent classification methods can be divided into two kinds of research lines, i.e., traditional machine learning methods and neural networks methods. For the first kind of research line, Pang et al. (2002) validate the effectiveness of various machine learning methods with bag-of-words features on sentiment classification. Goldberg and Zhu (2006) use a graph-based semi-supervised learning algorithm with unlabeled data to predict the sentiment of reviews. There are also some work which focus on extracting effective features. Ganu et al. (2009) identify user experience information from free text. Qu et al. (2010) introduce a kind of bag-of-opinion representation. 640 . can or jam )1 2 ) ce6 ten sen ce5 ten sen ce4 ten sen ce3 ten sen ce2 ten sen nce1 te sen . re mo say 1 2 ) 1 2 ) no , x jam rry e eb de blu ma me ho . bo a or x mi I ed ne … rs sta e fiv d ly en up . ff sta ing g ga en , fri of gro a . fee cof r ne din e tru 1 2 ) m fro t no , ilk rm tte bu l rea th es wi ak nc de pa ma me ho . zen fro t no , h atc scr m fro de ma s it cu bis (1 2 ) 1 2 ) 131 2 ) 1 Figure 2: Case Study: Illustration of Attention Weights. ument. Chen et al. (2016) employ two layers of long-short term memory (LSTM) with"
I17-1064,P12-1092,0,0.0564451,"composed of a sequence of words wt1 , wt2 , · · · , wtnt where wtj denotes a specific word. To represent a word, we embed each word into a low dimensional real-value vector, called word embedding (Bengio et al., 2003). Then, we can get wtj ∈ Rd from M v×d , where t is the sentence index in a document, j denotes the word index in sentence t, d means the embedding dimension and v gives the vocabulary size. Word embeddings can be regarded as parameters of neural networks or pre-trained from proper corpus via language model (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010; Huang et al., 2012). In our model, we choose the second strategy. Next, deeper word semantics representations can be learned by using the neural network models, such as convolutional neural networks (CNN) or recurrent neural networks (RNN). In this paper, the LSTM model is employed to obtain the word representation, since it has the good performance of learning the long-term dependencies and can well model the dependence between words. Formally, for sentence St , we input its word embeddings wt1 , wt2 , ..., wtnt to the LSTM networks and get the final word representations rt1 , rt2 , ..., rtnt . Cascading Multiw"
I17-1064,D12-1110,0,0.15676,"Missing"
I17-1064,P14-1062,0,0.103355,"Missing"
I17-1064,D11-1014,0,0.0955439,"the document level, CMA keeps using the multiway attention mechanism to generate attention to sentences. Experimental results on IMDB, Yelp 2013 and Yelp 2014 verify that CMA can learn efficient representations for sentences and documents and provide rich information for judging the document-level sentiment polarity. Recently, neural network approaches have achieved a comparable performance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neura"
I17-1064,D14-1181,0,0.039791,"Missing"
I17-1064,D13-1170,0,0.0229744,"has a high accuracy and a low RMSE. J =− C X i=1 gi log(yi ) + λr ( X θ2 ) θ∈Θ 637 Dataset IMDB Yelp2013 Yelp2014 #docs 84919 78966 231163 #users 1310 1631 4818 #products 1635 1633 4194 #docs/user 64.82 48.42 47.97 #docs/product 51.94 48.36 55.11 #sents/doc 16.08 10.89 11.41 #words/doc 24.54 17.38 17.26 #labels 10 5 5 Table 1: Data Statistics of IMDB, Yelp2013 and Yelp 2014. et al., 2015a) implements the distributed memory model of paragraph vectors (PV-DM) to get document representations for sentiment classification. • RNTN+RNN models sentences using recursive neural tensor networks (RNTN) (Socher et al., 2013). Then sentence representations are fed into the recurrent neural networks (RNN) and their hidden states are averaged to get the document representation. • UPNN designs preference matrices for each user and product to modify word representations (Tang et al., 2015b). Word representations are then fed into the convolution neural networks (CNNs) and concatenated with the user/product representation to generate document representation before a softmax layer. Without considering user and product information, the UPNN(noUP) method just uses CNN to model the documents. • NSC+UPA proposes the hierarc"
I17-1064,J11-2001,0,0.0482152,"provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model. 1 Introduction Document-level sentiment classification aims to predict an overall sentiment polarity (e.g., 1-5 stars or 1-10 stars) for a user review document. This task recently draws increasing research concerns and is helpful to many downstream applications, such as user and product recommendation. Early work focuses on traditional machine learning associated with handcraft text features for sentiment classification (Pang et al., 2002; Ding et al., 2008; Taboada et al., 2011). With the development of deep learning techniques, some researchers design neural networks to automati634 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 634–643, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP levels including word level, sentence level, document level, classification level. With word embeddings as input, we can employ convolutional neural networks or recurrent neural networks to obtain deeper semantic of words on the word level. On the sentence level, we design the multiway attention networks to generate attention"
I17-1064,P15-1150,0,0.0170995,"erformance on documentlevel sentiment classification. Glorot et al. (2011) first propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Then, Socher et al. (2011, 2012, 2013) introduce recursive neural networks to document-level sentiment classification. Kim (2014) employ convolutional neural networks to model sentences with two kinds of embeddings for sentiment classification. Le and Mikolov (2014) introduce an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts. Tai et al. (2015) utilize tree-structured longshort memory networks to learn semantic representation for sentiment classification. In addition, user and product information are flexibly modeled for sentiment classification in the neural network methods (Tang et al., 2015b; Chen et al., 2016). Tang et al. (2015a) design preference matrices for each user and each product to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole docAcknowledgments We would like to thank the anonymous reviwers for thier insightful suggestions. Our work is supported by National Hi"
I17-1064,D15-1167,0,0.456706,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P15-1098,0,0.849911,"n Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang, Xu Sun MOE Key Lab of Computational Linguistics, Peking University, Beijing 100871, China {madehong, lisujian, zxdcs, wanghf, xusun}@pku.edu.cn Abstract cally learn features from document content and achieve comparable performance (Glorot et al., 2011; Kalchbrenner et al., 2014; Yang et al., 2016), though they ignore the use of user and product information. Almost at the same time, deep learning techniques exhibit another advantage that product and user information can be flexibly modeled with document content for sentiment classification (Tang et al., 2015a; Chen et al., 2016). Tang et al. (2015a) design user and product preference matrices to tune word representations, based on which convolutional neural networks (CNNs) are used to model the whole document. To avoid the high-cost preference matrix, Chen et al. (2016) develop the two-layer (i.e., word and sentence layers) model, where the combination of user and product information is used to generate attention to words and sentences respectively on each layer. Though previous studies achieve improvements in synthesizing text, user and product for sentiment classification, they are somewhat lim"
I17-1064,P14-1146,0,0.105949,"ts word/character ngrams, sentiment lexicon features, negation features, etc. for a SVM classifier. • UPF extracts user-leniency features and product features from training data (Gao et al., 2013). There features can be concatenated with the features of Trigram and TextFeature. • AvgWordvec averages the word embeddings in a document to generate the document representation as features for a SVM classifier. • SSWE first learns the sentiment-specific word embeddings and then utilizes three kinds of pooling (i.e., max, min and average) to generate the document representation for a SVM classifier (Tang et al., 2014). • PV(Paragraph Vector) is an unsupervised framework to learn distributed representations for text of any length (Le and Mikolov, 2014). (Tang 638 IMDB Acc. RMSE Majority 0.196 2.495 Trigram 0.399 1.783 TextFeature 0.402 1.793 AvgWordvec 0.304 1.985 SSWE 0.312 1.973 PV 0.341 1.814 RNTN+RNN 0.400 1.764 UPNN(noUP) 0.405 1.629 0.487 1.381 NSC+LA CA-null 0.491 1.408 Trigram+UPF 0.404 1.764 TextFeature 0.402 1.774 +UPF 0.435 1.602 UPNN NSC+UPA 0.533 1.281 CMA 0.540 1.191 Model Yelp 2013 Acc. RMSE 0.411 1.060 0.569 0.841 0.556 0.814 0.526 0.898 0.549 0.849 0.554 0.832 0.574 0.804 0.577 0.812 0.631"
I17-1064,N16-1174,0,0.21451,"Missing"
J14-3004,P07-1056,0,0.0481432,"Missing"
J14-3004,W02-1001,0,0.609391,"g rate or so-called decaying rate, and Lstoch (zzi , w t ) is the stochastic loss function based on a training sample z i . (More details of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying rate works the best for natural language processing tasks, and it is adopted in our implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). Other well-known on-line training methods include perceptron training (Freund and Schapire 1999), averaged perceptron training (Collins 2002), more recent development/extensions of stochastic gradient descent (e.g., the second-order stochastic gradient descent training methods like stochastic meta descent) (Vishwanathan et al. 2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent method requires the computation or approximation of the inverse of the Hessian matrix of the objective function, which is typically slow, especially for heavily structured classification models. Usually the convergence speed based on number of training iterations is moderately faster, but the time cost per iteration is sl"
J14-3004,W04-1217,0,0.0112629,"tion (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line"
J14-3004,P07-1104,0,0.0145497,"e word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for"
J14-3004,W04-1213,0,0.0135585,"also perform experiments on a nonstructured binary classification task: sentiment-based text classification. For the nonstructured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996). 4.1 Biomedical Named Entity Recognition (Structured Classification) The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we u"
J14-3004,N01-1025,0,0.136481,"ssification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as befor"
J14-3004,H05-1124,0,0.245783,"Missing"
J14-3004,P06-1059,0,0.0295941,"from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line Training for Natural Lang"
J14-3004,W96-0213,0,0.366727,"xisting gold-standard systems, which are complicated and use extra resources. 2. Related Work Our main focus is on structured classification models with high dimensional features. For structured classification, the conditional random fields model is widely used. To illustrate that the proposed method is a general-purpose training method not limited to a specific classification task or model, we also evaluate the proposal for non-structured classification tasks like binary classification. For non-structured classification, the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996) is widely used. Here, we review the conditional random fields model and the related work of on-line training methods. 2.1 Conditional Random Fields The conditional random field (CRF) model is a representative structured classification model and it is well known for its high accuracy in real-world applications. The CRF model is proposed for structured classification by solving “the label bias problem” (Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequen"
J14-3004,W00-0726,0,0.0354294,"nstraints on j and k. All feature templates are instantiated with values that occurred in training samples. The extracted feature set is large, and there are 2.4 × 107 features in total. Our evaluation is based on a closed test, and we do not use extra resources. Following prior studies, the evaluation metric for this task is the balanced F-score. 4.3 Phrase Chunking (Structured Classification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is t"
J14-3004,W04-1221,0,0.0458102,"Missing"
J14-3004,C10-2139,0,0.0102007,"ent character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for j = i − 2, . . . , i + 1. Whether xj an"
J14-3004,C08-1106,1,0.654936,"fied. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as before. Word-Token–based Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{yi , yi−1 yi } P"
J14-3004,P12-1027,1,0.678739,"Missing"
J14-3004,N09-1007,1,0.809977,"Missing"
J14-3004,I05-3027,0,0.0133462,"R is recall. 4.2 Chinese Word Segmentation (Structured Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at"
J14-3004,P09-1054,0,0.0266947,"Missing"
J14-3004,N06-2049,0,0.0453143,"Missing"
J14-3004,P07-1106,0,0.0122276,"d Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1"
J14-3004,J96-1002,0,\N,Missing
L18-1276,P04-1082,0,0.0460756,"Missing"
L18-1276,D14-1082,0,0.0087708,"er to different things. In annotation, especially the word segmentation step and dependency parsing step, to alleviate the workload of the annotators, we use off-the-self tools, specifically the Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demand"
L18-1276,de-marneffe-etal-2006-generating,0,0.117552,"Missing"
L18-1276,de-marneffe-etal-2014-universal,0,0.0242921,"Missing"
L18-1276,P15-1033,0,0.0231963,"tors, we use off-the-self tools, specifically the Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this study, we introduce a"
L18-1276,Q16-1023,0,0.0140704,"e Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this study, we introduce a practical definition for ellipsis in Chinese. We also introdu"
L18-1276,P13-2017,0,0.142137,"Missing"
L18-1276,L16-1262,0,0.0664809,"Missing"
L18-1276,E12-2021,0,0.0180081,"ion is done in the following order, and an example is given in Table 1. Selection As the corpus is much too large for our purpose, we randomly select weibos from the corpus. A major attribute of ellipsis is that it should be understood from the context. However, in LWC, a single weibo is a stored unit, lacking the relations between weibos, such as forwarding and replying. Hence, some omissions just cannot be restored, even if we use the aforesaid general categories. BeWord Segmentation From this step, the annotation becomes more demanding, and we gain assistance from the annotation tool brat (Stenetorp et al., 2012)6 and the Stanford CoreNLP tool7 . The word segmentation procedure basically follows the guideline of CTB (Xia, 2000). The difference is that we treat all the words, which typically only appear in web text, as single words, regardless of the boundness of its morphemes/characters. For example, the word “给力” (give power, i.e. “forceful”) is not segmented into “给” (“give”) and “力” (“power”). We also mark the restored words with the tag “I” to distinguish from the original words. The word segmentation annotation generated from Stanford CoreNLP tool are considered as baselines, which are already av"
L18-1276,J14-3004,1,0.710206,"load of the annotators, we use off-the-self tools, specifically the Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this stu"
L18-1276,C16-1019,1,0.780111,"ifically the Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this study, we introduce a practical definition for elli"
L18-1276,P16-2092,1,0.829084,"s, specifically the Stanford CoreNLP tools, to automatically generate the related references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this study, we introduce a practical definition for elli"
L18-1276,K17-1035,0,0.0159776,"ed references. The annotators could use the annotations generated as a baseline, and rectify the wrong dependencies as they recognize. This could further facilitate the annotation procedure, and maintain the quality of the dependency at least above the performance of the used parser, which is about 83.9 in terms of UAS on the test set of CTB5 (Chen and Manning, 2014). In future revisions, tools with better accuracy and faster speed may be considered (Sun et al., 2013; Sun et al., 2014; Dyer et al., 2015; Sun, 2014; Xu and Sun, 2016; Kiperwasser and Goldberg, 2016; Sun, 2016; Sun et al., 2017; Zhang et al., 2017; Ren and Sun, 2017). The reason for these relaxations is that the main focus of the treebank is the annotation of ellipsis, and the study of the effect of ellipsis on dependency parsing, so as to other semantic related tasks. We shift our annotation focus to the annotation of sentence split, omission discovery and restoration, which are less demanding than the annotation of dependency trees, after we find the annotation process is too slow. 6. Conclusion In this study, we introduce a practical definition for ellipsis in Chinese. We also introduce a practical scheme for ellipsis annotation, an"
L18-1325,W02-1001,0,0.0865208,"n tag and pos tag can Evaluation Evaluation Metrics Simple Heuristic Baseline System The simple heuristic system means always choosing initial characters of words in the segmented full form. This is because the most natural abbreviating heuristic is to produce the first character of each word in the original full form. This is just the simplest baseline. 4.3. Evaluation To study the performance of other machine learning models, we also implement other well known sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). Besides these traditional models, we also implement a bidirectional LSTM(BLSTM) to evaluate the performance of neural networks on this task. The experimental results are shown in Table 3. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also show the discriminate accuracy. The CRF model outperforms the MEMM 2068 and averaged perceptron models. The CRF model achieves best overall character accuracy. BLSTM outperforms other models in"
L18-1325,P15-1033,0,0.0118428,": The number of correct labels (i.e., a classification on a character) generated by the system divided by the total number of characters in the test set. ft = σ(Wf · xt + Uf · ht−1 + bf ) it = σ(Wi · xt + Ui · ht−1 + bi) C˜t = tanh(WC · xt + UC · ht−1 + bC ) Ct = ft ⊗ Ct−1 + it ⊗ C˜t 4.2. (4) ot = σ(Wo · xt + Uo · ht−1 + bo ) ht = ot ⊗ tanh(Ct ) LSTM can solve the long-distance dependencies problem to some extent. However, the LSTM’s hidden state ht takes information only from the past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015) is bidirectional LSTM(BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. In this paper, we employ a bi-directional LSTM, which could capture the contextual information of the current input, to predict the abbreviations of full terms. Since we give a specific segmentation tag and a pos tag for every character, each segmentation tag and pos tag can Evaluation Evaluation Metrics Simple Heuristic Baseline System The simple"
L18-1325,W01-0516,0,0.134562,"an estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olympic Games) 北京大学 (Peking University ) 清华大学 (Tsinghua University ) 黄金市场 (gold market) 珠峰 奥运会 北大 清华 金市 Figure 1: Different cases of generating abbreviations English abbreviations are usually formed as acronyms. Studies for English abbreviation proposed various heuristics for abbreviation prediction (Park and Byrd, 2001; Wren et al., 2002; Schwartz and Hearst, 2002). For example, use of initials, capital letters, syllable boundaries, stop words, etc. These studies performed well for English abbreviations. While Chinese abbreviations are quite different from English ones. Yang et al. (2012) showed that Chinese abbreviations are usually generated by three methods, reduction, elimination, and generalization. Characters are selected from the expanded full name to form the abbreviation. However, there are no general rules to convert a complete term into an abbreviation. As shown in Figure 1, an abbreviation may b"
L18-1325,P09-1102,1,0.862455,"ation expansion, abbreviation recognition, and abbreviation prediction. Expanding the short form of an expression to its full form is called abbreviation expansion. Extracting the short form and full form pairs from the context is called abbreviation recognition. Abbreviation prediction refers to predicting the short form of an expression according to its full form. In this paper, we focus on the last task, i.e., abbreviation prediction. Abbreviation prediction plays an important role in various language processing tasks, because accurate abbreviation prediction will help improve performance. Sun et al. (2009) shows that better abbreviation prediction will help improve the performance of abbreviation recognition. Abbreviation prediction also benefits other tasks. For example, in an information retrieval (IR) system, a large number of the web pages contain only abbreviations. It will be helpful if we can estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olymp"
L18-1325,I13-1073,1,0.806872,"ction. Abbreviation prediction plays an important role in various language processing tasks, because accurate abbreviation prediction will help improve performance. Sun et al. (2009) shows that better abbreviation prediction will help improve the performance of abbreviation recognition. Abbreviation prediction also benefits other tasks. For example, in an information retrieval (IR) system, a large number of the web pages contain only abbreviations. It will be helpful if we can estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olympic Games) 北京大学 (Peking University ) 清华大学 (Tsinghua University ) 黄金市场 (gold market) 珠峰 奥运会 北大 清华 金市 Figure 1: Different cases of generating abbreviations English abbreviations are usually formed as acronyms. Studies for English abbreviation proposed various heuristics for abbreviation prediction (Park and Byrd, 2001; Wren et al., 2002; Schwartz and Hearst, 2002). For example, use of initials, capital letters, syllable boundaries, s"
L18-1325,W05-1304,0,0.036375,"Missing"
L18-1325,N09-2069,0,0.0262024,"abbreviation at all. We usually recognize abbreviations or make abbreviation predictions in the text. Unfortunately, NFFs take up a large portion of Chinese words or phrases in the real world. With the strong noise, distinguishing the full forms with valid abbreviations is more difficult. This undoubtedly increases the difficulty of abbreviation prediction. Many approaches have been proposed in the post studies. Sun et al. (2008) employed Support Vector Regression (SVR) for scoring abbreviation candidates. This method 2065 outperforms the hidden Markov model (HMM) in abbreviation prediction. Yang et al. (2009) proposed to formulate abbreviation generation as a character tagging problem and the conditional random field (CRF) then can be used as the tagging model. Sun et al. (2009) combined latent variable model and global information to predict abbreviations. Zhang et al. (2016) used a recurrent neural networks to predict abbreviations for Chinese named entities. However, most studies of abbreviation prediction focus on positive full form, which means a word has a valid abbreviation. Apparently, this implicit lab assumption is not practical. Nonetheless, we barely see studies that consider NFFs. One"
L18-1325,D14-1147,1,0.846314,"Missing"
L18-1325,D14-1202,1,0.872642,"Missing"
L18-1325,D16-1069,0,0.0123234,"is more difficult. This undoubtedly increases the difficulty of abbreviation prediction. Many approaches have been proposed in the post studies. Sun et al. (2008) employed Support Vector Regression (SVR) for scoring abbreviation candidates. This method 2065 outperforms the hidden Markov model (HMM) in abbreviation prediction. Yang et al. (2009) proposed to formulate abbreviation generation as a character tagging problem and the conditional random field (CRF) then can be used as the tagging model. Sun et al. (2009) combined latent variable model and global information to predict abbreviations. Zhang et al. (2016) used a recurrent neural networks to predict abbreviations for Chinese named entities. However, most studies of abbreviation prediction focus on positive full form, which means a word has a valid abbreviation. Apparently, this implicit lab assumption is not practical. Nonetheless, we barely see studies that consider NFFs. One of the main reasons is the shortage of abbreviation prediction data with NFFs, which is one of the main issues this work tries to solve. Apart from the annotation of a dataset with NFFs, we also conduct a few preprocessing steps to facilitate the usage of the dataset. Chi"
N09-1007,W06-1655,0,0.354127,"entation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretical advantage of semi-CRFs over CRFs, however, some previous studies (Andrew, 2006; Liang, 2005) exploring the use of a semi-CRF for Chinese word segmentation did not ﬁnd signiﬁcant gains over the CRF ones. As discussed in Andrew (2006), the reason may be that despite the greater representational power of the semi-CRF, there are some valuable features that could be more naturally expressed in a character-based"
N09-1007,I05-3018,0,0.0345224,"s beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretica"
N09-1007,I05-3019,0,0.342649,"Missing"
N09-1007,I05-3017,0,0.737646,"the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax t"
N09-1007,P07-1104,0,0.0508784,"nt Variable Segmenter 2.1 Discriminative Probabilistic Latent Variable Model Given data with latent structures, the task is to learn a mapping between a sequence of observations x = x1 , x2 , . . . , xm and a sequence of labels y = y1 , y2 , . . . , ym . Each yj is a class label for the j’th character of an input sequence, and is a member of a set Y of possible class labels. For each sequence, the model also assumes a sequence of latent variables h = h1 , h2 , . . . , hm , which is unobservable in training examples. The DPLVM is deﬁned as follows (Morency et al., 2 The system was also used in Gao et al. (2007), with an improved performance in CWS. 3 In practice, one may add a few extra labels based on linguistic intuitions (Xue, 2003). 2007): P (y|x, Θ) =  P (y|h, x, Θ)P (h|x, Θ), (1) h where Θ are the parameters of the model. DPLVMs can be seen as a natural extension of CRF models, and CRF models can be seen as a special case of DPLVMs that have only one latent variable for each label. To make the training and inference efﬁcient, the model is restricted to have disjoint sets of latent variables associated with each class label. Each hj is a member in a set Hyj of possible latent variables for the"
N09-1007,C04-1081,0,0.541571,"Missing"
N09-1007,E09-1088,1,0.796261,"onal loglikelihood of the training data. The second term is a regularizer that is used for reducing overﬁtting in parameter estimation. For decoding in the test stage, given a test sequence x, we want to ﬁnd the most probable label sequence, y∗ : y∗ = argmaxy P (y|x, Θ∗ ). (5) For latent conditional models like DPLVMs, the best label path y∗ cannot directly be produced by the 4 It means that Eq. 2 is from Eq. 1 with additional deﬁnition. 58 Viterbi algorithm because of the incorporation of hidden states. In this paper, we use a technique based on A∗ search and dynamic programming described in Sun and Tsujii (2009), for producing the most probable label sequence y∗ on DPLVM. In detail, an A∗ search algorithm5 (Hart et al., 1968) with a Viterbi heuristic function is adopted to produce top-n latent paths, h1 , h2 , . . . hn . In addition, a forward-backward-style algorithm is used to compute the exact probabilities of their corresponding label paths, y1 , y2 , . . . yn . The model then tries to determine the optimal label path based on the top-n statistics, without enumerating the remaining low-probability paths, which could be exponentially enormous. The optimal label path y∗ is ready when the following"
N09-1007,I05-3027,0,0.634414,"utational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004)."
N09-1007,W06-0121,0,0.114226,"Missing"
N09-1007,O03-4002,0,0.816245,"purposes, e.g., full-text indexing. However, as is illustrated, recognizing long words (without sacriﬁcing the performance on short words) is challenging. Conventional approaches to Chinese word segmentation treat the problem as a character-based la1 Following previous work, in this paper, words can also refer to multi-word expressions, including proper names, long named entities, idioms, etc. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). Wh"
N09-1007,P07-1106,0,0.496064,"h row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on those two corpora. Both of the MSR and PKU Corpus use simpliﬁed Chinese, w"
N09-1007,N06-2049,0,0.152838,"ts are grouped into three sub-tables according to different corpora. Each row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on"
N18-1018,P16-1046,0,0.0723454,"Missing"
N18-1018,N16-1012,0,0.0446173,"lt to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machi"
N18-1018,P16-1154,0,0.23065,"-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART"
N18-1018,D15-1229,0,0.48478,"contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to train the model, with the default hyper-parameters: the learning rate α = 0.001, and β1 = 0.9, β2 = 0.999,  = 1e − 8. 3 Experiments Following the previous work (Cao et al., 2017), we test our model on the following two paraphrase orientated tasks: text simplification and short text abstractive summarization. 3.1 Text Simplification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine trans"
N18-1018,P17-2100,1,0.925978,"f PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, l"
N18-1018,N15-1022,0,0.189994,"ioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from 1 to 5 (1 is very bad and 5 is very good). Simplicity shows how simpler the model outputs are than the source text, which ranges from 1 to 5. • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2se"
N18-1018,P15-1001,0,0.0312775,"erate a fluent sentence, but the meaning is different from the source text, and even more difficult to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et"
N18-1018,K16-1028,0,0.124083,"SARI. 3.1.5 3.1.4 Baselines We compare our model with several neural text simplification systems. Results We compare WEAN with state-of-the-art models for text simplification. Table 1 and Table 2 summarize the results of the automatic evaluation. On PWKP dataset, we compare WEAN with PBMT, Hybrid, EncDecA, DRESS and DRESSLS. WEAN achieves a BLEU score of 54.54, outperforming all of the previous systems. On EWSEW dataset, we compare WEAN with PBMT-R, Hybrid, SBMT-SARI, and the neural models described above. We do not find any public release code of PBMT-R and SBMT-SARI. Fortunately, Xu et al. (2016) provides the predictions of PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN"
N18-1018,P13-1151,0,0.0185862,"Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces a parallel simplification corpus by evaluating the similarity between the source text and the simplified ˇ text based on WordNet. Glavaˇs and Stajner (2015) propose an unsupervised approach to lexical simplification that makes use of word vectors and require only regular corpora. Xu et al. (2016) desi"
N18-1018,P14-1041,0,0.190927,". • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2seq (our implementation) WEAN (our proposal) BLEU 46.31 53.94 47.93 34.53 36.32 48.26 54.54 PWKP NTS-w2v DRESS-LS WEAN Reference EW-SEW Fluency Adequacy Simplicity All PBMT-R 3.36 2.92 3.37 3.22 SBMT-SARI 3.41 3.63 3.25 3.43 NTS-w2v 3.56 3.52 3.42 3.50 DRESS-LS 3.59 3.43 3.65 3.56 WEAN 3.61 3.56 3.65 3.61 Reference 3.71 3.64 3.45 3.60 Table 1: Automatic evaluation of our model and other related systems on PWKP datasets. The results are reported on the test sets. EW-SEW PBMT-R (Wubben et al.,"
N18-1018,P17-2014,0,0.141044,"e decoder input and the predicted output share the same vocabulary and word embeddings. Besides, we do not use any pretrained word embeddings in our model, so that all of the parameters are learned from scratch. 2.5 Training Although our generator is a retrieval style, WEAN is as differentiable as the sequence-to-sequence model. The objective of training is to minimize the cross entropy between the predicted word probability distribution and the golden one-hot distribution: X L=− yˆi log p(yi ) (7) each sentence pair as a good, good partial, partial, or bad match. Following the previous work (Nisioi et al., 2017), we discard the unclassified matches, and use the good matches and partial matches with a scaled threshold greater than 0.45. The corpus contains about 150K good matches and 130K good partial matches. We use this corpus as the training set, and the dataset provided by Xu et al. (Xu et al., 2016) as the validation set and the test set. The validation set consists of 2,000 sentence pairs, and the test set contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to tra"
N18-1018,D17-1222,0,0.497871,"eq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015)"
N18-1018,P02-1040,0,0.104711,"implification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug"
N18-1018,N03-1020,0,0.390902,"pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and oth"
N18-1018,C16-1275,0,0.226628,"Missing"
N18-1018,D15-1044,0,0.294351,"idation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and other related systems, and the results are shown in Table 3. DRESS-LS is based on the reinforcement learning, and it encourages the fluency, simplicity and relevance of the outputs. Therefore, it achieves a high score in our human evaluation. WEAN gains a even better score than DRESS-LS. Beside"
N18-1018,D17-1062,0,0.19844,"he gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to evaluate the quality of the model outputs. Following Nisioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from"
N18-1018,C10-1152,0,0.182865,", and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to ev"
N18-1018,D16-1112,0,0.160819,"Missing"
N18-1018,D17-1020,0,0.0247035,"model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs."
N18-1018,D11-1038,0,0.0306716,"(Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces"
N18-1018,P12-1107,0,0.0647051,"Missing"
N18-2059,H05-1091,0,0.130304,"ole model. Experimental results show that the proposed method significantly improves the F1 score by 10.3, and outperforms the state-of-the-art approaches on Chinese literature text1 . 1 Introduction Relation classification is the task of identifying the semantic relation holding between two nominal entities in text. Recently, neural networks are widely used in relation classification. Wang et al. (2016) proposes a convolutional neural network with two levels of attention. Zhang et al. (2015) uses bidirectional long short-term memory networks to model the sentence with sequential information. Bunescu and Mooney (2005) first uses SDP between two entities to capture the predicate-argument sequences. Wang et al. (2017) explores the idea of incorporating syntactic parse tree into neural networks. Liu et al. (2017) proposes a noise-tolerant method to deal with wrong labels in distant-supervised relation extraction with soft labels. In recent years, we 1 The Chinese literature text corpus for relation classification, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset 365 Proceedings of NAACL-HLT 2018, pages 365–370 c New Orleans, Louisiana, June 1 - 6"
N18-2059,P16-1072,0,0.0799559,"ina− − tion of → y and ← y − − ytest = α · → y + (1 − α) · z(← y) (5) where α is the fraction of the composition of dis− tributions. We apply a function z to transform ← y → − to a corresponding forward distribution like y . 3.2 Structure Regularized BRCNN The basic BRCNN model can handle the task to some extent, but there still remains some weakness, especially dealing with long sentences with 367 Baselines Our Model Models SVM (Hendrickx et al., 2010) RNN (Socher et al., 2011) CNN (Zeng et al., 2014) CR-CNN (dos Santos et al., 2015) SDP-LSTM (Xu et al., 2015) DepNN (Liu et al., 2015) BRCNN (Cai et al., 2016) Information Word embeddings, NER, WordNet, HowNet, POS, dependency parse, Google n-gram Word embeddings + POS, NER, WordNet Word embeddings + word position embeddings, NER, WordNet Word embeddings + word position embeddings Word embeddings + POS + NER + WordNet F1 Score Word embeddings, WordNet 55.2 Word embeddings + POS, NER, WordNet Word embeddings + POS, NER, WordNet 55.0 55.6 65.2 (+9.6) 65.9 (+10.3) SR-BRCNN 48.9 48.3 49.1 47.6 52.4 52.7 54.1 54.9 55.3 Table 2: Comparison of relation classification systems on Chinese literature text. a complicated structures. The SDP generated from a mor"
N18-2059,J81-4005,0,0.728205,"Missing"
N18-2059,S10-1006,0,0.025646,"Missing"
N18-2059,P16-1123,0,0.0265067,"Missing"
N18-2059,I17-1050,1,0.753504,"and outperforms the state-of-the-art approaches on Chinese literature text1 . 1 Introduction Relation classification is the task of identifying the semantic relation holding between two nominal entities in text. Recently, neural networks are widely used in relation classification. Wang et al. (2016) proposes a convolutional neural network with two levels of attention. Zhang et al. (2015) uses bidirectional long short-term memory networks to model the sentence with sequential information. Bunescu and Mooney (2005) first uses SDP between two entities to capture the predicate-argument sequences. Wang et al. (2017) explores the idea of incorporating syntactic parse tree into neural networks. Liu et al. (2017) proposes a noise-tolerant method to deal with wrong labels in distant-supervised relation extraction with soft labels. In recent years, we 1 The Chinese literature text corpus for relation classification, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset 365 Proceedings of NAACL-HLT 2018, pages 365–370 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Family 我[Person]便是那时候随了父母[Person]建设国营农场的梦想,来到"
N18-2059,D17-1189,0,0.023037,"tion classification is the task of identifying the semantic relation holding between two nominal entities in text. Recently, neural networks are widely used in relation classification. Wang et al. (2016) proposes a convolutional neural network with two levels of attention. Zhang et al. (2015) uses bidirectional long short-term memory networks to model the sentence with sequential information. Bunescu and Mooney (2005) first uses SDP between two entities to capture the predicate-argument sequences. Wang et al. (2017) explores the idea of incorporating syntactic parse tree into neural networks. Liu et al. (2017) proposes a noise-tolerant method to deal with wrong labels in distant-supervised relation extraction with soft labels. In recent years, we 1 The Chinese literature text corpus for relation classification, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset 365 Proceedings of NAACL-HLT 2018, pages 365–370 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Family 我[Person]便是那时候随了父母[Person]建设国营农场的梦想,来到 西洞庭的 Family At that time, I[Person] came to West Dongting with my parents'[Person] dream of bui"
N18-2059,P15-2047,0,0.0361731,"Missing"
N18-2059,D15-1206,0,0.0493535,"Missing"
N18-2059,P15-1061,0,0.0512818,"Missing"
N18-2059,C14-1220,0,0.0760039,"Missing"
N18-2059,D11-1014,0,0.0577134,"Missing"
N18-2059,Y15-1009,0,0.0137783,"SDP) extracted from the structure regularized dependency tree, which has the benefits of reducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F1 score by 10.3, and outperforms the state-of-the-art approaches on Chinese literature text1 . 1 Introduction Relation classification is the task of identifying the semantic relation holding between two nominal entities in text. Recently, neural networks are widely used in relation classification. Wang et al. (2016) proposes a convolutional neural network with two levels of attention. Zhang et al. (2015) uses bidirectional long short-term memory networks to model the sentence with sequential information. Bunescu and Mooney (2005) first uses SDP between two entities to capture the predicate-argument sequences. Wang et al. (2017) explores the idea of incorporating syntactic parse tree into neural networks. Liu et al. (2017) proposes a noise-tolerant method to deal with wrong labels in distant-supervised relation extraction with soft labels. In recent years, we 1 The Chinese literature text corpus for relation classification, developed and used by this paper, is available at https://github.com/l"
N18-2059,W09-2415,0,\N,Missing
N19-1296,C14-1059,0,0.0300683,"n. For easy interpretation, we select a simple and clear example where styles can be easily inferred from reviews. In practice, the correlation between styles and associated reviews is relatively complicated. 2.1 Music Style Classification Previous work mainly focuses on using audio information to identify music styles. Traditional machine learning algorithms are adopted in these studies, such as Support Vector Machine (SVM) (Xu et al., 2003), Hidden Markov Model (HMM) (Chai and Vercoe, 2001; Pikrakis et al., 2006), and Decision Tree (DT) (Zhou et al., 2006). In addition to audio information, Fell and Sporleder (2014) also propose to classify music by statistical analysis of lyrics. Motivated by the fact that a piece of music could has different styles, several studies (Wang et al., 2009; Oramas et al., 2017) also aim at multi-label music style classification. Different from these studies, we focus on using easily obtained reviews in conjunction with multi-label music style classification. 2.2 Multi-Label Classification Multi-label classification has been widely applied to diverse problems, including image classification (Qi et al., 2007; Wang et al., 2008), audio classification (Boutell et al., 2004; Sand"
N19-1296,D18-1485,1,0.849866,"ification. Different from these studies, we focus on using easily obtained reviews in conjunction with multi-label music style classification. 2.2 Multi-Label Classification Multi-label classification has been widely applied to diverse problems, including image classification (Qi et al., 2007; Wang et al., 2008), audio classification (Boutell et al., 2004; Sanden and Zhang, 2011), web mining (Kazawa et al., 2004), information retrieval (Zhu et al., 2005; Gopal and Yang, 2010), etc. Compared with the existing multilabel learning methods (Wei et al., 2018; Li et al., 2018b,a; Yang et al., 2018; Lin et al., 2018), our method has the following novelties: a label graph that explicitly models the relations of styles; a soft training mechanism that introduces correlationbased continuous label representation. 3 3.1 words. The term Y = {y1 , y2 , . . . , yM } denotes the gold set with M labels, and M varies in different samples. The target of review-driven multilabel music style classification is to learn the mapping from input reviews to style labels. 3.2 The dataset is collected from a popular Chinese music review website,3 where registered users are allowed to comment on all released music albums. Each s"
N19-1296,D17-1189,0,0.0686806,"Missing"
N19-1296,C18-1330,1,0.835845,"l music style classification. Different from these studies, we focus on using easily obtained reviews in conjunction with multi-label music style classification. 2.2 Multi-Label Classification Multi-label classification has been widely applied to diverse problems, including image classification (Qi et al., 2007; Wang et al., 2008), audio classification (Boutell et al., 2004; Sanden and Zhang, 2011), web mining (Kazawa et al., 2004), information retrieval (Zhu et al., 2005; Gopal and Yang, 2010), etc. Compared with the existing multilabel learning methods (Wei et al., 2018; Li et al., 2018b,a; Yang et al., 2018; Lin et al., 2018), our method has the following novelties: a label graph that explicitly models the relations of styles; a soft training mechanism that introduces correlationbased continuous label representation. 3 3.1 words. The term Y = {y1 , y2 , . . . , yM } denotes the gold set with M labels, and M varies in different samples. The target of review-driven multilabel music style classification is to learn the mapping from input reviews to style labels. 3.2 The dataset is collected from a popular Chinese music review website,3 where registered users are allowed to comment on all released m"
N19-1296,N16-1174,0,0.0114575,"with too little information by rule-based methods and then select top 40 voted reviews. Music samples with too few reviews are also deleted. The constructed datataset contains over 7.1k samples, 288K reviews, and 3.6M words. 4 Proposed Approach The proposed approach contains two parts: a label-graph based neural network and a soft training mechanism with continuous label representation. An illustration of the proposed method is shown in Figure 1. 4.1 Review-Driven Multi-Label Music Style Classification Dataset Label-Graph Based Neural Network The first layer is a hierarchical attention layer (Yang et al., 2016) that lets the model to pay more or less attention to individual words Task Definition Given several reviews from a piece of music, this task requires models to predict a set of music styles. Assume that X = {x1 , . . . , xi , . . . , xK } denotes the input K reviews, and xi = xi,1 , . . . , xi,J represents the ith review with J 3 https://music.douban.com Alternative Music, Britpop, Classical Music, Country Music, Dark Wave, Electronic Music, Folk Music, Heavy Metal Music, Hip-Hop, Independent Music, Jazz, J-Pop, New-Age Music, OST, Piano Music, Pop, Post-Punk, PostRock, Punk, R&B, Rock, and S"
P09-1102,P08-2016,0,0.319572,"tperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLVM. Variables x, y, and h represent observation,"
P09-1102,C08-1083,1,0.911993,"1.9 92.0 90.0 87.1 96.9 94.8 97.8 97.7 98.1 F 95.9 90.5 94.0 92.1 91.0 97.1 92.1 95.9 95.1 96.1 Table 6: Results of English abbreviation recognition. belings. Other labelings are impossible, because they will generate an abbreviation that is not AP. If the first or second labeling is generated, AP is selected as an abbreviation of arterial pressure. If the third or fourth labeling is generated, then AP is selected as an abbreviation of cannulate for arterial pressure. Finally, the fifth labeling (NULL) indicates that AP is not an abbreviation. To evaluate the recognizer, we use the corpus6 of Okazaki et al. (2008), which contains 864 abbreviation definitions collected from 1,000 MEDLINE scientific abstracts. In implementing the recognizer, we simply use the model from the abbreviation generator, with the same feature templates (31,868 features) and training method; the major difference is in the restriction (according to the PE) of the decoding stage and penalizing the probability values of the NULL labelings7 . For the evaluation metrics, following Okazaki et al. (2008), we use precision (P = k/m), recall (R = k/n), and the F-score defined by Recognition as a Generation Task We directly migrate this m"
P09-1102,P02-1021,0,0.485619,"sed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLV"
P09-1102,W01-0516,0,0.56495,"ocally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted"
P09-1102,N03-1028,0,0.0125058,"uring training and validation, we set σ = 1 for the DPLVM generators. We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency. Note that, for the label encoding with global information, many label transitions (e.g., P2 S3 ) are actually impossible: the label transitions are strictly constrained, i.e., yi yi+1 ∈ {Pj Sj , Pj Pj+1 , Sj Pj+1 , Sj Sj }. These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of −∞, thereby forcing all forbidden labelings to have zero probability. Sha and Pereira (2003) originally proposed this concept of implementing transition restrictions. Table 1: Language-independent features (#1 to #3), Chinese-specific features (#4 through #7), and English-specific features (#8 through #11). the other hand, such duplication detection features are not so useful for English abbreviations. Feature templates #8–#11 are designed for English abbreviations. Features #8 and #9 encode the orthographic information of expanded forms. Features #10 and #11 represent a contextual n-gram with a large window size. Since the number of letters in Chinese (more than 10K characters) is m"
P09-1102,W05-1304,1,0.408495,"y outperform previous abbreviation generation studies. In addition, we apply the proposed models to the task of abbreviation recognition, in which a model extracts the abbreviation definitions in a given text. To the extent of our knowledge, this is the first model that can perform both abbreviation generation and recognition at the state-of-the-art level, across different languages and with a simple feature set. other languages, including Chinese and Japanese, do not have word boundaries or case sensitivity. A number of recent studies have investigated the use of machine learning techniques. Tsuruoka et al. (2005) formalized the processes of abbreviation generation as a sequence labeling problem. In the present study, each character in the expanded form is tagged with a label, y ∈ {P, S}1 , where the label P produces the current character and the label S skips the current character. In Figure 1 (a), the abbreviation PGA is generated from the full form polyglycolic acid because the underlined characters are tagged with P labels. In Figure 1 (b), the abbreviation is generated using the 2nd and 3rd characters, skipping the subsequent three characters, and then using the 7th character. In order to formaliz"
P10-1028,P06-1129,0,0.255299,"r a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the da"
P10-1028,J04-4002,0,0.0557688,"using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for W"
P10-1028,H05-1120,0,0.426726,"n text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to"
P10-1028,D08-1047,0,0.0208292,"word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular"
P10-1028,P00-1037,0,0.920791,"or the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting Microsoft Research Redmond. 266 Proceedings o"
P10-1028,D07-1019,0,0.747082,"ry, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models fr"
P10-1028,D07-1021,1,0.837998,"2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correct"
P10-1028,W04-3238,0,0.73309,"r context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain muc"
P10-1028,D09-1154,1,0.767258,"evel. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q1 and Q2 such that (1) they are issued by the same user; (2) Q2 was issued within 3 minutes of Q1; and"
P10-1028,P02-1019,0,0.516968,"r spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When desi"
P10-1028,D09-1093,0,0.0826042,"fore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthroug"
P10-1028,C90-2036,0,0.296919,"been little research on exploiting the data for the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting M"
P10-1028,N03-1017,0,0.165419,"first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and prove"
P12-1027,W06-1655,0,0.141199,"entation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number"
P12-1027,I05-3018,0,0.160445,"nefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representativ"
P12-1027,O98-3002,0,0.582016,"accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review r"
P12-1027,C02-1049,0,0.113216,". 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. T"
P12-1027,I05-3019,0,0.188076,"Missing"
P12-1027,I05-3017,0,0.499942,"R), City University of Hongkong (CU), and Peking University (PKU). Details of the corpora are listed in Table 1. We did not use any extra resources such as common surnames, parts-of-speech, and semantics. Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score defined by 2P R/(P + R), and recall of new word detection (NWD recall). For more detailed information on the corpora, refer to Emerson (2005). 5.2 Features, Training, and Tuning We employed the feature templates defined in Section 3.2. The feature sets are huge. There are 2.4 × 107 features for the MSR data, 4.1 × 107 features for the CU data, and 4.7 × 107 features for the PKU data. To generate word-based features, we extracted high-frequency word-based unigram and bigram lists from the training data. As for training, we performed gradient descent MSR CU ADF SGD 96 LBFGS (batch) 95.5 94 F−score 96.5 F−score F−score 95.5 94.5 97 95 93.5 93 94.5 0 10 20 30 40 Number of Passes 92 50 0 10 20 30 40 Number of Passes MSR 94 50 0 CU 10 20"
P12-1027,P07-1104,0,0.0508865,"Missing"
P12-1027,P03-2039,0,0.03267,"g and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online"
P12-1027,C04-1081,0,0.706516,"new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and ne"
P12-1027,E09-1088,1,0.801001,"ning methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more frequently and speed u"
P12-1027,C08-1106,1,0.0330946,"opular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more"
P12-1027,N09-1007,1,0.647152,"Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training sample"
P12-1027,C10-2139,0,0.725279,"Missing"
P12-1027,I05-3027,0,0.248309,"Missing"
P12-1027,W00-1207,0,0.0506628,"ord segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word"
P12-1027,O11-2013,0,0.0152595,"semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stoch"
P12-1027,O03-4002,0,0.825038,"es are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting sys"
P12-1027,P07-1106,0,0.741552,"ning algorithm later. We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods, and can achieve equal or even higher accuracies. The contribution of this work is as follows: • We propose a general purpose fast online training method, ADF. The proposed training method requires only a few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003;"
P12-1027,N06-2049,0,0.698283,"Missing"
P12-1027,I05-1047,0,0.0837938,"enters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we revi"
P16-1212,P14-1091,1,0.746378,"er to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM"
P16-1212,P14-1133,0,0.0178386,"variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along wit"
P16-1212,D13-1160,0,0.016404,"der framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be"
P16-1212,D14-1179,0,0.00526158,"Missing"
P16-1212,N03-1017,0,0.0311904,"Target Generation can generate a natural language sentence based on the existing semantic tuples; • Combining them, KBSE can be used to translation a source sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there"
P16-1212,P07-2045,0,0.0586397,"ce sentence into another language with a semantic space defined by a given knowledge base. 3 Experiments To evaluate our proposed KBSE model, in this section, we conduct experiments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed"
P16-1212,P11-1060,0,0.0109095,"d on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what i"
P16-1212,P13-1078,0,0.0244095,"es for number of objects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a senten"
P16-1212,P14-1140,1,0.808281,"bjects, some results of our KBSE generate the entities in wrong plurality form. Since our KBSE consists of two separate parts, the Source Grounding part and the Target Generation part, the errors generated in the first part cannot be corrected in the following process. As we mentioned in Section 3.3.1, combining KBSE with encoder-decoder can alleviate these two problems, by preserving information not captured and correct the errors generated in source grounding part. 4 Related Work Unlike previous works using neural network to learn features for traditional log-linear model (Liu et al., 2013; Liu et al., 2014), Sutskever et al. (2014) introduced a general end-to-end approach based on an encoder-decoder framework. In order to compress the variable-sized source sentence into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also c"
P16-1212,P03-1021,0,0.0453806,"periments on two Chineseto-English translation tasks. One is from electric business domain, and the other is from movie domain. 3.1 Baseline and Comparison Systems We select two baseline systems. The first one is an in-house implementation of hierarchical phrasebased SMT (Koehn et al., 2003; Chiang, 2007) with traditional features, which achieves a similar performance to the state-of-the-art phrase-based decoder in Moses 1 (Koehn et al., 2007). The 4gram language model is trained with target sentences from training set plus the Gigaword corpus 2 . Our phrase-based system is trained with MERT (Och, 2003). The other system is the encoderdecoder system (van Merri¨enboer et al., 2015) 3 , based on which our KBSE is implemented. We also combine KBSE with encoder-decoder system, by adding the knowledge-based semantic embedding to be another context vector. Hence, for the decoder there are two context vectors, one from the encoder and the other is generated by the Semantic Grounding part. We call this model Enc-Dec+KBSE. For our proposed KBSE, the number of hidden units in both parts are 300. Embedding size of both source and target are 200. Adadelta (Zeiler, 2012) 1 http://www.statmt.org/moses/ ht"
P16-1212,P02-1040,0,0.100737,"Missing"
P16-1212,D15-1199,0,0.0201843,"Missing"
P16-1212,P15-1128,0,0.00689599,"ce into a fixed-length semantic vector, an encoder RNN reads the words in source sentence and generate a hidden state, based on which another decoder RNN is used to generate target sentence. Different from our work using a semantic space defined by knowledge base, the hidden state connecting the source and target RNNs is a vector of implicit and inexplicable real numbers. Learning the semantic information from a sentence, which is also called semantic grounding, is widely used for question answering tasks (Liang et al., 2011; Berant et al., 2013; Bao et al., 2014; Berant and Liang, 2014). In (Yih et al., 2015), with a deep convolutional neural network (CNN), the question sentence is mapped into a query graph, based on which the answer is searched in knowledge base. In our paper, we use RNN to encode the sentence to do fair comparison with the encoderdecoder framework. We can try using CNN to replace RNN as the encoder in the future. To generate a sentence from a semantic vector, Wen et al. (2015) proposed a LSTM-based natural language generator controlled by a semantic vector. The semantic vector memorizes what information should be generated for LSTM, and it varies along with the sentence generate"
P16-1212,J07-2003,0,\N,Missing
P16-2092,P15-1168,0,0.408871,"t al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main limitation of chain structure for sequence labeling is that long distance dependencies decay inevitably. Though forget gate mechanism is added, it is difficult for bi-directional long short term memory network (Bi-LSTM), a kind of chain structure, to avoid this problem. In general, tree structure works better than chain structure to model long term information. Therefore, we use gated recursive neural network (GRNN) (Chen et al., 2015) which is a kind of tree structure to capture long distance dependencies. Motivated by the fact, we propose the dependency-based gated recursive neura"
P16-2092,P15-2043,0,0.027578,"and “ 积”. Only long distance dependencies can help the model recognize tag correctly in this example. Thus, long distance information is an important factor for CWS task. Introduction Word segmentation is an important pre-process step in Chinese language processing. Most widely used approaches treat Chinese word segmentation (CWS) task as a sequence labeling problem in which each character in the input sequence is assigned with a tag. Many previous approaches have been effectively applied to CWS problem (Lafferty et al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main limitation of chain structure for sequence labeling is that long dista"
P16-2092,I05-3017,0,0.674955,"Missing"
P16-2092,P14-1028,0,0.722904,"tively applied to CWS problem (Lafferty et al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main limitation of chain structure for sequence labeling is that long distance dependencies decay inevitably. Though forget gate mechanism is added, it is difficult for bi-directional long short term memory network (Bi-LSTM), a kind of chain structure, to avoid this problem. In general, tree structure works better than chain structure to model long term information. Therefore, we use gated recursive neural network (GRNN) (Chen et al., 2015) which is a kind of tree structure to capture long distance dependencies. Motivated by the fact, we propose"
P16-2092,N06-2049,0,0.388081,"Missing"
P16-2092,D13-1031,1,0.84743,"Missing"
P16-2092,D11-1090,0,0.0482373,"Missing"
P16-2092,D13-1061,0,0.522471,"ches have been effectively applied to CWS problem (Lafferty et al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main limitation of chain structure for sequence labeling is that long distance dependencies decay inevitably. Though forget gate mechanism is added, it is difficult for bi-directional long short term memory network (Bi-LSTM), a kind of chain structure, to avoid this problem. In general, tree structure works better than chain structure to model long term information. Therefore, we use gated recursive neural network (GRNN) (Chen et al., 2015) which is a kind of tree structure to capture long distance dependencies. Motivated by th"
P16-2092,N09-1007,1,0.87159,"Missing"
P16-2092,P12-1027,1,0.685348,", even with the same adjacent characters, “ 地” and “ 积”. Only long distance dependencies can help the model recognize tag correctly in this example. Thus, long distance information is an important factor for CWS task. Introduction Word segmentation is an important pre-process step in Chinese language processing. Most widely used approaches treat Chinese word segmentation (CWS) task as a sequence labeling problem in which each character in the input sequence is assigned with a tag. Many previous approaches have been effectively applied to CWS problem (Lafferty et al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main limitation of chain"
P16-2092,W03-1728,0,0.039684,"ags in two sentences, even with the same adjacent characters, “ 地” and “ 积”. Only long distance dependencies can help the model recognize tag correctly in this example. Thus, long distance information is an important factor for CWS task. Introduction Word segmentation is an important pre-process step in Chinese language processing. Most widely used approaches treat Chinese word segmentation (CWS) task as a sequence labeling problem in which each character in the input sequence is assigned with a tag. Many previous approaches have been effectively applied to CWS problem (Lafferty et al., 2001; Xue and Shen, 2003; Sun et al., 2012; Sun, 2014; Sun et al., 2013; Cheng et al., 2015). However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimizing the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015). The main li"
P16-2092,P07-1106,0,0.646234,"Missing"
P16-2092,P15-1167,0,\N,Missing
P17-2100,N16-1012,0,0.185993,"Missing"
P17-2100,P16-1154,0,0.0428289,"Missing"
P17-2100,D16-1009,0,0.0234952,"vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries. Our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts. (1) 3.1 Text Representation There are several methods to represent a text or a sentence, such as mean pooling of RNN output or reserving the last state of RNN. In our model, source text is represented by a gated attention encoder (Hahn and Keller, 2016). Every upcoming word is fed into a gated attention network, which measures its importance. The gated attention network outputs the important score with a feedforward network. At each time step, it inputs a word vector et and its previous context vector ht , then outputs the score βt . Then the word vector et is multiplied by the score βt , and fed into RNN encoder. We select the last output hN of RNN encoder as the semantic vector of the source text Vt . A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes much time because"
P17-2100,D15-1229,0,0.633711,"literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To achieve this goal,"
P17-2100,N03-1020,0,0.426438,"Missing"
P17-2100,D15-1166,0,0.236244,"Missing"
P17-2100,P16-1046,0,0.0589016,"联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a curren"
P17-2100,K16-1028,0,0.0653475,"Missing"
P17-2100,radev-etal-2004-mead,0,0.073768,"Missing"
P17-2100,D15-1044,0,0.0636919,"ilarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To"
P17-2100,C16-1019,1,0.802479,"we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P16-1218,0,0.0181009,"summary pairs, constructed from a famous Chinese social media website called Sina Weibo1 . It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the textsummary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5, and we only reserve pairs with scores no less than 3. Following the previous work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random"
P17-2100,P10-1058,0,0.0357048,"cial media corpus. 1 RNN: 中联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summ"
P17-2100,P16-2092,1,0.794027,"s work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P15-1001,0,\N,Missing
P18-1090,I17-1064,1,0.782224,"nal sentence. A related study is “back reconstruction” in machine translation (He et al., 2016; Tu et al., 2017). They couple two inverse tasks: one is for translating a sentence in language A to a sentence in language B; the other is for translating a sentence in language B to a sentence in language A. Different from the previous work, we do not introduce the inverse task, but use collaboration between the neutralization module and the emotionalization module. Sentiment analysis is also related to our work (Socher et al., 2011; Pontiki et al., 2015; Rosenthal et al., 2017; Chen et al., 2017; Ma et al., 2017, 2018b). The task usually involves detecting whether a piece of text expresses positive, negative, or neutral sentiment. The sentiment can be general or about a specific topic. one of the reinforcement learning methods, to reward the output of the neutralization module based on the feedback from the emotionalization module. We add different sentiment to the semantic content and use the quality of the generated text as reward. The quality is evaluated by two useful metrics: one for identifying whether the generated text matches the target sentiment; one for evaluating the content preservation"
P18-1090,N18-1018,1,0.88559,"Missing"
P18-1090,E17-1059,0,0.055843,"Missing"
P18-1090,P02-1040,0,0.103498,"dataset contains 230K, 10K, and 3K pairs for training, validation, and testing, respectively. (9) where Rc is calculated as Rc = R1 + R2 (10) Based on Eq. 8 and Eq. 9, we use the sampling approach to estimate the expected reward. This cycled process is repeated until converge. 3.4.1 Reward The reward consists of two parts, sentiment confidence and BLEU. Sentiment confidence evaluates whether the generated text matches the target sentiment. We use a pre-trained classifier to make the judgment. Specially, we use the proposed selfattention based sentiment classifier for implementation. The BLEU (Papineni et al., 2002) score is used to measure the content preservation performance. Considering that the reward should encourage the model to improve both metrics, we use the harmonic mean of sentiment confidence and BLEU as reward, which is formulated as R = (1 + β 2 ) 2 · BLEU · Conf id (β 2 · BLEU ) + Conf id 4.2 We tune hyper-parameters based on the performance on the validation sets. The self-attention based sentiment classifier is trained for 10 epochs on two datasets. We set β for calculating reward to 0.5, hidden size to 256, embedding size to 128, vocabulary size to 50K, learning rate to 0.6, and batch s"
P18-1090,S15-2082,0,0.0806921,"Missing"
P18-1090,S17-2088,0,0.0733652,"Missing"
P18-1090,W17-3526,0,0.0284011,"Missing"
P18-1162,P15-2113,0,0.390902,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,P06-4018,0,0.0100513,"we adopt the F1 score and accuracy on two categories for evaluation. SemEval 2017 regards answer selection as a ranking task, which is closer to the application scenario. As a result, mean average precision (MAP) is used as an evaluation measure. For a perfect ranking, a system has to place all Good answers above the PotentiallyUseful and Bad answers. The latter two are not actually distinguished and are considered Bad in terms of evaluation. Additionally, standard classification measures like accuracy and F1 score are also reported. 4.3 Implementation Details We use the tokenizer from NLTK (Bird, 2006) to preprocess each sentence. All word embeddings in the sentence encoder layer are initialized with the 300-dimensional GloVe (Pennington et al., 2014) word vectors trained on the domainspecific unannotated corpus, and embeddings for out-of-vocabulary words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], an"
P18-1162,S17-2045,1,0.847421,"Missing"
P18-1162,S16-1172,0,0.0630292,". The SemEval CQA tasks (Nakov et al., 2015, 2016, 2017) provide universal benchmark datasets for evaluating researches on this problem. Earlier work of answer selection in CQA relied heavily on feature engineering, linguistic tools, and external resource. Nakov et al. (2016) investigated a wide range of feature types including similarity features, content features, thread level/meta features, and automatically generated features for SemEval CQA models. Tran et al. (2015) studied the use of topic model based features and word vector representation based features in the answer re-ranking task. Filice et al. (2016) designed various heuristic features and thread-based features 1753 that can signal a good answer. Although achieving good performance, these methods rely heavily on feature engineering, which requires a large amount of manual work and domain expertise. Since answer selection is inherently a ranking task, a few recent researches proposed to use local features to make global ranking decision. BarrónCedeño et al. (2015) was the first work that applies structured prediction model on CQA answer selection task. Joty et al. (2016) approached the task with a global inference process to exploit the in"
P18-1162,S17-2053,0,0.128356,"fore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but it does not include a development set. Following previous work (Filice et al., 2017), we use the 2016 official test set as the development set. 5.1 SemEval 2015 Results Table 3 compares our model with the following baselines: 1750 Methods (1) JAIST (2) HITSZ-ICRC (3) Graph-cut (4) FCCRF (5) BGMN (6) CNN-LSTM-CRF (7) QCN Table 3: dataset. F1 78.96 76.52 80.55 81.50 77.23 82.22 83.91 Acc 79.10 76.11 79.80 80.50 78.40 82.24 85.65 Methods (1) KeLP (2) Beihang-MSRA (3) ECNU (4) LSTM (5) LSTM-subject-body (6) QCN Comparisons on the SemEval 2015 • JAIST (Tran et al., 2015): It used an SVM classifier to incorporate various kinds of features , including topic model based features and"
P18-1162,S15-2035,0,0.0173659,"development set. 5.1 SemEval 2015 Results Table 3 compares our model with the following baselines: 1750 Methods (1) JAIST (2) HITSZ-ICRC (3) Graph-cut (4) FCCRF (5) BGMN (6) CNN-LSTM-CRF (7) QCN Table 3: dataset. F1 78.96 76.52 80.55 81.50 77.23 82.22 83.91 Acc 79.10 76.11 79.80 80.50 78.40 82.24 85.65 Methods (1) KeLP (2) Beihang-MSRA (3) ECNU (4) LSTM (5) LSTM-subject-body (6) QCN Comparisons on the SemEval 2015 • JAIST (Tran et al., 2015): It used an SVM classifier to incorporate various kinds of features , including topic model based features and word vector representations. • HITSZ-ICRC (Hou et al., 2015): It proposed ensemble learning and hierarchical classification method to classify answers. • Graph-cut (Joty et al., 2015): It modeled the relationship between pairs of answers at any distance in the same question thread, based on the idea that similar answers should have similar labels. • FCCRF (Joty et al., 2016): It used locally learned classifiers to predict the label for each individual node, and applied fully connected CRF to make global inference. Table 4: dataset. MAP 88.43 88.24 86.72 86.32 87.11 88.51 F1 69.87 68.40 77.67 74.41 74.50 78.11 Acc 73.89 51.98 78.43 75.69 77.28 80.71 Com"
P18-1162,D15-1068,0,0.385032,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,N16-1084,0,0.471171,"ordinary QA does not possess. First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail. The questioners usually convey their main concern and key information in the question subject. Then, they provide more extensive details about the subject, seek help, or express gratitude in the question body. Second, the problem of redundancy and noise is prevalent in CQA (Zhang et al., 2017). Both questions and answers contain auxiliary sentences that do not provide meaningful information. Previous researches (Tran et al., 2015; Joty et al., 2016) usually treat each word equally in the question and answer representation. However, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality. To make things worse, they ignored the difference between question subject and body, and simply concatenated them as the question representation. Due to the subject-body relationship described above, this simple concatenation can aggravate the redundancy problem in the question. In this paper, we propose the Question Condensing Networks (QCN) to address these problems. In order to ut"
P18-1162,S17-2003,0,0.123451,"Missing"
P18-1162,S15-2047,0,0.104987,"ry words are set to zero. We use the Adam Optimizer (Kingma and Ba, 2014) for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. We perform a small grid search over combinations of initial learning rate [1 × 10−6 , 3 × 10−6 , 1 × 10−5 ], L2 regularization parameter [1 × 10−7 , 3 × 10−7 , 1 × 10−6 ], and batch size [8, 16, 32]. We take the best configuration based on performance on the development set, and only evaluate that configuration on the test set. In order to mitigate the class imbalance problem, median frequency balancing Eigen and Fergus (2015) is used to reweight each class in the cross-entropy loss. Therefore, the rarer a class is in the training set, the larger weight it will get in the cross entropy loss. Early stopping is applied to mitigate the problem of overfitting. For the SemEval 2017 dataset, the conditional probability over the Good class is used to rank all the candidate answers. 5 Experimental Results In this section, we evaluate our QCN model on two community question answering datasets from SemEval shared tasks. 6 The SemEval 2017 dataset provides all the data from 2016 for training , and fresh data for testing, but"
P18-1162,S16-1083,0,0.253621,"estion representation using subject-body relationship. In most cases, the question subject can be seen as a summary containing key points of the question, the question body is relatively lengthy in that it needs to explain the key points and add more details about the posted question. We propose to cheat the question subject as the primary part of the question representation, and aggregate question body information from two perspectives: similarity and disparity with the question subject. To achieve this goal, we use an orthogonal decomposition strategy, which is first proposed by Wang et al. (2016), to decompose each question body embedding into a parallel component and an orthogonal compobi,j para = bjemb · siemb i s siemb · siemb emb j i,j bi,j orth = bemb − bpara (1) (2) All vectors in the above equations are of length d. Next we describe the process of aggregating the question body information based on the parallel component in detail. The same process can be applied to the orthogonal component, so at the end of the fusion gate we can obtain Sorth and Sorth respectively. The decomposed components are passed through a fully connected layer to compute the multi-dimensional attention w"
P18-1162,D14-1162,0,0.0811374,"S, B, C). 3 Proposed Model In this paper, we propose Question Condensing Networks (QCN) which is composed of the following modules. The overall architecture of our model is illustrated in Figure 1. 3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN. 1747 ???? ???? ??????? ???? ???? ?????? ?????? MLP ???? ? ????????ℎ ???? ?r?? ? ???? ???? Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector. 3.1 Word-Level Embedding nent based on every question subject embedding: Word-level embeddings are composed of two components: GloVe (Pennington et al., 2014) word vectors trained on the domain-specific unannotated corpus provided by the task 4 , and convolutional neural network-based character embeddings which are similar to (Kim et al., 2016). Web text in CQA forums differs largely from normalized text in terms of spelling and grammar, so specifically trained GloVe vectors can model word interactions more precisely. Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA. We concatenate these two embedding vectors for every word to generate word-level embeddings Se"
P18-1162,S15-2038,0,0.3477,"istics of CQA that ordinary QA does not possess. First, a question includes both a subject that gives a brief summary of the question and a body that describes the question in detail. The questioners usually convey their main concern and key information in the question subject. Then, they provide more extensive details about the subject, seek help, or express gratitude in the question body. Second, the problem of redundancy and noise is prevalent in CQA (Zhang et al., 2017). Both questions and answers contain auxiliary sentences that do not provide meaningful information. Previous researches (Tran et al., 2015; Joty et al., 2016) usually treat each word equally in the question and answer representation. However, due to the redundancy and noise problem, only part of text from questions and answers is useful to determine the answer quality. To make things worse, they ignored the difference between question subject and body, and simply concatenated them as the question representation. Due to the subject-body relationship described above, this simple concatenation can aggravate the redundancy problem in the question. In this paper, we propose the Question Condensing Networks (QCN) to address these prob"
P18-1162,C16-1127,0,0.030014,"dense the question representation using subject-body relationship. In most cases, the question subject can be seen as a summary containing key points of the question, the question body is relatively lengthy in that it needs to explain the key points and add more details about the posted question. We propose to cheat the question subject as the primary part of the question representation, and aggregate question body information from two perspectives: similarity and disparity with the question subject. To achieve this goal, we use an orthogonal decomposition strategy, which is first proposed by Wang et al. (2016), to decompose each question body embedding into a parallel component and an orthogonal compobi,j para = bjemb · siemb i s siemb · siemb emb j i,j bi,j orth = bemb − bpara (1) (2) All vectors in the above equations are of length d. Next we describe the process of aggregating the question body information based on the parallel component in detail. The same process can be applied to the orthogonal component, so at the end of the fusion gate we can obtain Sorth and Sorth respectively. The decomposed components are passed through a fully connected layer to compute the multi-dimensional attention w"
P18-1162,S17-2060,0,0.0318471,"Missing"
P18-1162,C16-1117,0,0.0267716,"Missing"
P18-2027,D15-1166,0,0.2011,"ctively. Moreover, the analysis shows Introduction Abstractive summarization can be regarded as a sequence mapping task that the source text should be mapped to the target summary. Therefore, sequence-to-sequence learning can be applied to neural abstractive summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014), whose model consists of an encoder and a decoder. Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source-side information (Bahdanau et al., 2014; Luong et al., 2015). Many attention-based seq2seq models have been proposed for abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), which outperformed the conventional statistical methods. 1 The code is available at https://www.github. com/lancopku/Global-Encoding 163 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 163–169 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics coders output hi at each time step i of the encoding process are computed with a weight matrix Wa to"
P18-2027,P00-1041,0,0.615961,"Missing"
P18-2027,N18-1018,1,0.838354,"Gigaword. Table 4: An example of our summarization, compared with that of the seq2seq model and the reference. to-sequence model with attention mechanism to abstractive summarization and realized significant achievements. Chopra et al. (2016) changed the ABS model with an RNN decoder and Nallapati et al. (2016) changed the system to a fullyRNN sequence-to-sequence model and achieved outstanding performance. Zhou et al. (2017) proposed a selective gate mechanism to filter secondary information. Li et al. (2017) proposed a deep recurrent generative decoder to learn latent structure information. Ma et al. (2018) proposed a model that generates words by querying word embeddings. out the price gap in its generated summary. As “China” appears twice in the source text and it is hard for the baseline model to put it in a less significant place, but for our model with CGU, it is able to filter the trivial details that are irrelevant to the core meaning of the source text and just focuses on the information that contributes most to the main idea. As our CGU is responsible for selecting important information of the outputs from the RNN encoder to improve the quality of the attention score, it should be able"
P18-2027,P17-2100,1,0.821758,"e Models 4.1 As we compare our results with the results of the baseline models reported in their original papers, the evaluation on the two datasets has different baselines. In the following, we introduce the baselines for LCSTS and Gigaword respectively. Baselines for LCSTS are introduced in the following. RNN and RNN-context are the RNNbased seq2seq models (Hu et al., 2015), without and with attention mechanism respectively. CopyNet is the attention-based seq2seq model with the copy mechanism (Gu et al., 2016). SRB is a model that improves semantic relevance between source text and summary (Ma et al., 2017). DRGD is the conventional seq2seq with a deep recurrent generative decoder (Li et al., 2017). As to the baselines for Gigaword, ABS and ABS+ are the models with local attention and handcrafted features (Rush et al., 2015). Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size. RASLSTM and RAS-Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively. SEASS is a seq2seq model with a selective gate mechanism. DRGD is also a baseline for Gigaword. Results of our implementation of the conventional seq2seq"
P18-2027,N16-1012,0,0.0733215,"he source text should be mapped to the target summary. Therefore, sequence-to-sequence learning can be applied to neural abstractive summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014), whose model consists of an encoder and a decoder. Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source-side information (Bahdanau et al., 2014; Luong et al., 2015). Many attention-based seq2seq models have been proposed for abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), which outperformed the conventional statistical methods. 1 The code is available at https://www.github. com/lancopku/Global-Encoding 163 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 163–169 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics coders output hi at each time step i of the encoding process are computed with a weight matrix Wa to obtain the global attention αt,i and the context vector ct . It is described below: 濖瀂瀁濶濴瀇 濾濐濆 濾濐濆 濾濐濆 濾濐濄 濾濐濄 Pvocab = sof tm"
P18-2027,K16-1028,0,0.101739,"Missing"
P18-2027,C08-1018,0,0.135763,"Missing"
P18-2027,W12-3018,0,0.116946,"Missing"
P18-2027,W03-0501,0,0.0361908,"Missing"
P18-2027,D15-1044,0,0.840817,"mapping task that the source text should be mapped to the target summary. Therefore, sequence-to-sequence learning can be applied to neural abstractive summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014), whose model consists of an encoder and a decoder. Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source-side information (Bahdanau et al., 2014; Luong et al., 2015). Many attention-based seq2seq models have been proposed for abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), which outperformed the conventional statistical methods. 1 The code is available at https://www.github. com/lancopku/Global-Encoding 163 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 163–169 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics coders output hi at each time step i of the encoding process are computed with a weight matrix Wa to obtain the global attention αt,i and the context vector ct . It is described below: 濖瀂瀁濶濴瀇 濾濐濆 濾濐濆 濾濐濆 濾濐"
P18-2027,P16-1154,0,0.0307324,"t our model can generate summary that is more semantically consistent with the source text. Baseline Models 4.1 As we compare our results with the results of the baseline models reported in their original papers, the evaluation on the two datasets has different baselines. In the following, we introduce the baselines for LCSTS and Gigaword respectively. Baselines for LCSTS are introduced in the following. RNN and RNN-context are the RNNbased seq2seq models (Hu et al., 2015), without and with attention mechanism respectively. CopyNet is the attention-based seq2seq model with the copy mechanism (Gu et al., 2016). SRB is a model that improves semantic relevance between source text and summary (Ma et al., 2017). DRGD is the conventional seq2seq with a deep recurrent generative decoder (Li et al., 2017). As to the baselines for Gigaword, ABS and ABS+ are the models with local attention and handcrafted features (Rush et al., 2015). Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size. RASLSTM and RAS-Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively. SEASS is a seq2seq model with a selective gate mechani"
P18-2027,D15-1229,0,0.351709,"ty, so we implement its scaled dot-product attention for the connection between the annotation at each time step and the global information: N L=− where the loss function is equivalent to maximizing the conditional probability of summary y given parameters θ and source sequence x. 3 Experiment Setup In the following, we introduce the datasets that we conduct experiments on and our experiment settings as well as the baseline models that we compare with. 3.1 Datasets LCSTS is a large-scale Chinese short text summarization dataset collected from Sina Weibo, a famous Chinese social media website (Hu et al., 2015), consisting of more than 2.4 million textsummary pairs. The original texts are shorter than 140 Chinese characters, and the summaries are created manually. We follow the previous research (Hu et al., 2015) to split the dataset for training, validation and testing, with 2.4M sentence pairs for training, 8K for validation and 0.7K for testing. The English Gigaword is a sentence summarization dataset based on Annotated Gigaword (Napoles et al., 2012), a dataset consisting of sentence pairs, which are the first sentence of the collected news articles and the corresponding headlines. We use the da"
P18-2027,D13-1176,0,0.0812119,"e parameter sharing, so that the representations at each time step are refined with consideration of the global context. We conduct experiments on LCSTS and Gigaword, two benchmark datasets for sentence summarization, which shows that our model outperforms the state-of-theart methods with ROUGE-2 F1 score 26.8 and 17.8 respectively. Moreover, the analysis shows Introduction Abstractive summarization can be regarded as a sequence mapping task that the source text should be mapped to the target summary. Therefore, sequence-to-sequence learning can be applied to neural abstractive summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014), whose model consists of an encoder and a decoder. Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source-side information (Bahdanau et al., 2014; Luong et al., 2015). Many attention-based seq2seq models have been proposed for abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), which outperformed the conventional statistical methods. 1 The code is available at https://www.github. com/lancopku/Global-Encoding"
P18-2027,D17-1222,0,0.040058,"Missing"
P18-2027,D16-1112,0,0.0405438,"Missing"
P18-2027,P17-1101,0,0.149302,"ations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition1 . 1 candidate for candidate . Gold: fatah officially elects abbas as candidate for presidential election Table 1: An example of the summary of the conventional attention-based seq2seq model on the Gigaword dataset. The text highlighted indicates repetition, “#” refers to masked number. However, recent studies show that there are salient problems in the attention mechanism. Zhou et al. (2017) pointed out that there is no obvious alignment relationship between the source text and the target summary, and the encoder outputs contain noise for the attention. For example, in the summary generated by the seq2seq in Table 1, “officially” is followed by the same word, as the attention mechanism still attends to the word with high attention score. Attention-based seq2seq model for abstractive summarization can suffer from repetition and semantic irrelevance, causing grammatical errors and insufficient reflection of the main idea of the source text. To tackle this problem, we propose a mode"
P18-2053,2015.iwslt-evaluation.11,0,0.0756792,"e first perform the encoding and decoding to obtain the scores of words at each position of the generated sentence. Then, we sum the scores of all positions as the sentence-level score. Finally, the sentencelevel score is used for multi-label classification, which identifies whether the word appears in the translation. In our model, the encoder is a bi-directional Long Short-term Memory Network (BiLSTM), which produces the representation qt = f (yt−1 , qt−1 ) (4) where f is the function of LSTM for one time step, and yt−1 is the last generated words at t-th time step. The attention mechanism (Luong and Manning, 2015) is used to capture the source information: N X vt = αti hi (5) i=1 333 The total loss function can be written as: eg(qt ,hi ) αti = PN j=1 e g(qt ,hj ) g(qt , hi ) = tanh (qtT Wt hi ) (6) l = l1 + λi l2 where λi is the coefficient to balance two loss functions at i-th epoch. Since the bag-of-words generation module is built on the top of the word generation, we assign a small weight for the bagof-words training at the initial time, and gradually increase the weight until a certain value λ: (7) where Wt is a trainable parameter matrix. Then, the word generator is used to compute the probabilit"
P18-2053,P17-1175,0,0.0363359,"Missing"
P18-2053,D15-1166,0,0.141758,"der framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better performance than the strong baseline models on a popular Chinese-E"
P18-2053,P17-1177,0,0.33936,"egardless of the position in the sentence. Compared with the word-level probability pwt , the sentencelevel probability pb of each word is independent of the position in the sentence. More specifically, the sentence-level probability of the generated bag-of-words pb can be written as: M X (13) The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. (12) i=1 334 Model Moses (Su et al., 2016) RNNSearch (Su et al., 2016) Lattice (Su et al., 2016) CPR (Zhang et al., 2017) POSTREG (Zhang et al., 2017) PKI (Zhang et al., 2017) Bi-Tree-LSTM (Chen et al., 2017) Mixed RNN (Li et al., 2017) Seq2Seq+Attn (our implementation) +Bag-of-Words (this paper) MT-02 33.19 34.68 35.94 33.84 34.37 36.10 36.57 37.70 34.71 39.77 MT-03 32.43 33.08 34.32 31.18 31.42 33.64 35.64 34.90 33.15 38.91 MT-04 34.14 35.32 36.50 33.26 34.18 36.48 36.63 38.60 35.26 40.02 MT-05 31.47 31.42 32.40 30.67 30.99 33.08 34.35 35.50 32.36 36.82 MT-06 30.81 31.61 32.77 29.63 29.90 32.90 30.57 35.60 32.45 35.93 MT-08 23.85 23.58 24.84 22.38 22.87 24.63 23.96 27.61 All 31.04 31.76 32.95 29.72 30.20 32.51 31.96 36.51 Table 2: Results of our model and the baselines (directly reported in the"
P18-2053,N18-1018,1,0.894925,"Missing"
P18-2053,P16-1046,0,0.0769008,"Missing"
P18-2053,C16-1205,0,0.0489055,"brenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better performance than the strong baseline models on a popular Chinese-English translation"
P18-2053,D16-1249,0,0.0200693,", 2013; Cho et al., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better performance than the strong baseline models on a popular Chinese-English translation dataset. In the f"
P18-2053,P02-1040,0,0.103015,"m Targets and Loss Function l1 = − Datasets We evaluated our proposed model on the NIST translation task for Chinese-English translation and provided the analysis on the same task. We trained our model on 1.25M sentence pairs extracted from LDC corpora 2 , with 27.9M Chinese words and 34.5M English words. We validated our model on the dataset for the NIST 2002 translation task and tested our model on that for the NIST 2003, 2004, 2005, 2006, 2008 translation tasks. We used the most frequent 50,000 words for both the Chinese vocabulary and the English vocabulary. The evaluation metric is BLEU (Papineni et al., 2002). where M is the number of words in the target sentence. M X Experiments This section introduces the details of our experiments, including datasets, setups, baseline models as well as results. t=1 2.3 (14) In our experiments, we set the λ = 1.0, k = 0.1, and α = 0.1, based on the performance on the validation set. where Wg and bg are parameters of the generator. To get a sentence-level score for the generated sentence, we generate a sequence of word-level score vectors st at all positions with the output layer of decoder, and then we sum up the wordlevel score vectors to obtain a sentence-leve"
P18-2053,C16-1290,0,0.0171553,"et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better performance than the strong baseline models on a popular Chinese-English translation dataset. In the future, we will explore how to apply ou"
P18-2053,P15-1001,0,0.0260722,"., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better performance than the strong baseline models on a popular Chinese-English translation dataset. In the future, we will expl"
P18-2053,D13-1176,0,0.113444,"in the previous work is an assistant component, while the bag of word in this paper is a direct target. For example, in the paper you mentioned, the bag-of-word loss is a component of variational autoencoder to tackle the vanishing latent variable problem. In our paper, the bag of word is the representation of the unseen correct translations to tackle the data sparseness problem. Table 3: Two translation examples of our model, compared with the Seq2Seq+Attn baseline. quate, with a better coverage of the bag-of-words of the references. 4 Related Work 5 The studies of encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et"
P18-2053,D16-1112,0,0.0227785,"Missing"
P18-2053,P17-1064,0,0.160342,"he sentence. Compared with the word-level probability pwt , the sentencelevel probability pb of each word is independent of the position in the sentence. More specifically, the sentence-level probability of the generated bag-of-words pb can be written as: M X (13) The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. (12) i=1 334 Model Moses (Su et al., 2016) RNNSearch (Su et al., 2016) Lattice (Su et al., 2016) CPR (Zhang et al., 2017) POSTREG (Zhang et al., 2017) PKI (Zhang et al., 2017) Bi-Tree-LSTM (Chen et al., 2017) Mixed RNN (Li et al., 2017) Seq2Seq+Attn (our implementation) +Bag-of-Words (this paper) MT-02 33.19 34.68 35.94 33.84 34.37 36.10 36.57 37.70 34.71 39.77 MT-03 32.43 33.08 34.32 31.18 31.42 33.64 35.64 34.90 33.15 38.91 MT-04 34.14 35.32 36.50 33.26 34.18 36.48 36.63 38.60 35.26 40.02 MT-05 31.47 31.42 32.40 30.67 30.99 33.08 34.35 35.50 32.36 36.82 MT-06 30.81 31.61 32.77 29.63 29.90 32.90 30.57 35.60 32.45 35.93 MT-08 23.85 23.58 24.84 22.38 22.87 24.63 23.96 27.61 All 31.04 31.76 32.95 29.72 30.20 32.51 31.96 36.51 Table 2: Results of our model and the baselines (directly reported in the referred articles) on the Ch"
P18-2053,P16-1008,0,0.0240301,"the references. 4 Related Work 5 The studies of encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) for this task launched the Neural Machine Translation. To improve the focus on the information in the encoder, Bahdanau et al. (2014) proposed the attention mechanism, which greatly improved the performance of the Seq2Seq model on NMT. Most of the existing NMT systems are based on the Seq2Seq model and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. There are also some effective neural networks other RNN. Gehring et al. (2017) turned the RNN-based model into CNN-based model, which Conclusions and Future Work We propose a method that regard both the reference translation (appears in the training set) and the bag-of-words as the targets of Seq2Seq at the training stage. Experimental results show that our model obtains better perf"
P18-2053,P18-1090,1,0.898331,"Missing"
P18-2053,P17-1139,0,0.0196161,"or each word, which represents how possible the word appears in the generated sentence regardless of the position in the sentence. Compared with the word-level probability pwt , the sentencelevel probability pb of each word is independent of the position in the sentence. More specifically, the sentence-level probability of the generated bag-of-words pb can be written as: M X (13) The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. (12) i=1 334 Model Moses (Su et al., 2016) RNNSearch (Su et al., 2016) Lattice (Su et al., 2016) CPR (Zhang et al., 2017) POSTREG (Zhang et al., 2017) PKI (Zhang et al., 2017) Bi-Tree-LSTM (Chen et al., 2017) Mixed RNN (Li et al., 2017) Seq2Seq+Attn (our implementation) +Bag-of-Words (this paper) MT-02 33.19 34.68 35.94 33.84 34.37 36.10 36.57 37.70 34.71 39.77 MT-03 32.43 33.08 34.32 31.18 31.42 33.64 35.64 34.90 33.15 38.91 MT-04 34.14 35.32 36.50 33.26 34.18 36.48 36.63 38.60 35.26 40.02 MT-05 31.47 31.42 32.40 30.67 30.99 33.08 34.35 35.50 32.36 36.82 MT-06 30.81 31.61 32.77 29.63 29.90 32.90 30.57 35.60 32.45 35.93 MT-08 23.85 23.58 24.84 22.38 22.87 24.63 23.96 27.61 All 31.04 31.76 32.95 29.72 30.20 32.51"
P18-2053,P17-1061,0,0.0536051,"Missing"
P18-2079,I17-1064,1,0.60587,"Missing"
P18-2079,P16-1068,0,0.0230098,"itute of Big Data Research, Peking University {yang pc, xusun, liweitj47, shumingma}@pku.edu.cn Abstract lowing work applied similar methods by using various classiﬁers with more sophisticated features including grammar, vocabulary and style (Rudner and Liang, 2002; Attali and Burstein, 2004). These traditional methods can work almost as well as human raters. However, they all demand a large amount of feature engineering, which requires a lot of expertise. Recent studies turn to use deep neural networks, claiming that deep learning models can relieve the system from heavy feature engineering. Alikaniotis et al. (2016) proposed to use long short term memory network (Hochreiter and Schmidhuber, 1997) with a linear regression output layer to predict the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN"
P18-2079,D13-1180,0,0.0549068,"Missing"
P18-2079,D15-1049,0,0.0394781,"Missing"
P18-2079,D16-1115,0,0.0557963,"dels can relieve the system from heavy feature engineering. Alikaniotis et al. (2016) proposed to use long short term memory network (Hochreiter and Schmidhuber, 1997) with a linear regression output layer to predict the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN) to model the essay. The ﬁrst layer is responsible for encoding the sentence and the second layer is to encode the whole essay. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task. Different from the essay in language capability tests, academic papers are much longer with much more information, and the"
P18-2079,K17-1017,0,0.0518291,"ct the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN) to model the essay. The ﬁrst layer is responsible for encoding the sentence and the second layer is to encode the whole essay. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task. Different from the essay in language capability tests, academic papers are much longer with much more information, and the overall quality is affected by a variety of factors besides the writing. Therefore, we propose a model that considers the overall information of one academic paper, including the title, authors, abstract and th"
P18-2079,P17-1011,0,0.0134501,"n used for the AES task, which have achieved great success. Alikaniotis et al. (2016) proposed to use the LSTM model with a linear regression output layer to predict the score. Taghipour and Ng (2016) applied the CNN model followed by a recurrent layer to extract local features and model sequence dependencies. A twolayer CNN model was proposed by Dong and Zhang (2016) to cover more high-level and abstract information. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Song et al. (2017) proposed a multi-label neural sequence labeling approach for discourse mode identiﬁcation and showed that features extracted by this method can further improve the AES task. Table 4: Ablation Study. The symbol * indicates that the difference compared to full data is signiﬁcant with p ≤ 0.05 under t-test. model shows different degrees of decline when we remove different modules of the source paper. This shows that there are differences in the contribution of different modules of the source paper to its acceptance, which further illustrates the reasonableness of our use of modularized hierarchi"
P18-2079,D14-1181,0,0.0114005,"Missing"
P18-2079,P11-1019,0,0.103149,"Missing"
P18-2079,W15-0626,0,0.030822,"Missing"
P18-2079,D16-1193,0,\N,Missing
P18-2115,I13-1041,0,0.0464905,"Missing"
P18-2115,D17-1222,0,0.0377903,"ies are extracted from the training sets, and the source contents and the summaries share the same vocabularies. In order to alleviate the risk of word segmentation mistakes, we split the Chinese sentences into characters. We prune the vocabulary size to 4,000, which covers most of the common characters. We tune the hyper-parameters based on the ROUGE scores on the validation sets. We set the word embedding size and the hidden size to 512, and the number of LSTM layers is 2. The batch size is 64, and we do not use dropout (Srivastava et al., 2014) on this dataset. Following the previous work (Li et al., 2017), we implement the beam search, and set the beam size to 10. Experiments Following the previous work (Ma et al., 2017), we evaluate our model on a popular Chinese social media dataset. We first introduce the datasets, evaluation metrics, and experimental details. Then, we compare our model with several state-of-the-art systems. 3.1 3.4 Baselines We compare our model with the following stateof-the-art baselines. • RNN and RNN-cont are two sequence-tosequence baseline with GRU encoder and decoder, provided by Hu et al. (2015). The difference between them is that RNN-context has attention mechani"
P18-2115,N03-1020,0,0.15914,"D (y = 0|zt ) (4) − log PθD (y = 1|zs ) pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. Loss Function and Training There are several parts of the objective functions to optimize in our models. The first part is the cross entropy losses of the sequence-t"
P18-2115,P16-1046,0,0.020242,"he source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset that consists of N data samples, the ith data sampl"
P18-2115,D15-1166,0,0.0372553,"tasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders. T"
P18-2115,N16-1012,0,0.0252682,"ral model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders. The classifier is a simple feedforward neural network which m"
P18-2115,N18-1018,1,0.772234,"Missing"
P18-2115,P16-1154,0,0.0185662,"Airport. Some passengers asked for a security check but were denied by the captain, which led to a collision between crew and passengers. Reference: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic rel"
P18-2115,D15-1229,0,0.330206,"inimizing the discriminator objective, we only train the parameters of the discriminator, while the rest of the parameters remains unchanged. The supervision objective to be against the discriminator can be written as: LG (θE ) = − log PθD (y = 0|zt ) (4) − log PθD (y = 1|zs ) pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and RO"
P18-2115,P17-2100,1,0.543419,"share the same points, it is possible to supervise the learning of the semantic representation of the source content with that of the summary. In this paper, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. First, we train an autoencoder, which inputs and reconstructs the summaries, to obtain a better representation to generate the summaries. Then, we supervise the internal representation of Seq2Seq with that of autoencoder by minimizing the distance between two representations. Finally, we use adversarial learning to enhance the supervision. Following the previous work (Ma et al., 2017), We evaluate our proposed model on a Chinese social media dataset. Experimental results show that our model outperforms the state-of-theart baseline models. More specifically, our model outperforms the Seq2Seq baseline by the score of 7.1 ROUGE-1, 6.1 ROUGE-2, and 7.0 ROUGE-L. Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well wri"
P18-2115,P15-1001,0,0.0233635,"of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014). Table 3: A summarization example of our model, compared with Seq2Seq and the reference. the seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the paramete"
P18-2115,K16-1028,0,0.0195193,"ome passengers asked for a security check but were denied by the captain, which led to a collision between crew and passengers. Reference: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic rel"
P18-2115,P10-1058,0,0.0337129,"hares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset that consists of N data sa"
P18-2115,radev-etal-2004-mead,0,0.116212,"tten. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1 1 Introduction Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. 2 Proposed Model We introduce our proposed model in detail in this section. 2.1 Notation Given a summarization dataset"
P18-2115,P18-1090,1,0.81,"Missing"
P18-2115,D15-1044,0,0.705745,"irs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. When minimizing the supervision objective, we only update the parameters of the encoders. 2.4 3.2 Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. Loss Function and Training There are several parts of the objective functions to optimize in our models. The first part is the cross entropy losses of the sequence-to-sequence and the autoencoder: N X LSeq2seq = − pSeq2seq (yi |zs ) (5) i=1 LAE = − N X pAE (yi |zt ) Evaluation Metric (6) i=1 3.3 The second part is the L2 loss of the supervision, as written in Equation 1. The last part is the adversarial learning, which are Equation 3 and Equation 4."
P18-2115,P17-1099,0,0.0300922,"ce: 航班多人吸烟机组人员与乘客 冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Seq2Seq: 中联航空机场发生爆炸致多人死 亡。 China United Airlines exploded in the airport, leaving several people dead. +superAE: 成都飞北京航班多人吸烟机组 人员与乘客冲突。 Several people smoked on a flight from Chendu to Beijing, which led to a collision between crew and passengers. propose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries. Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoen"
P18-2115,D16-1112,0,0.031997,"Missing"
P19-1125,W17-4123,0,0.186965,"al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables"
P19-1125,D16-1009,0,0.0211846,"(T 0 t/T ) (Gu et al., 2017; Lee et al., 2018). As the source and target sentences are often of different lengths, AT model need to predict the target length T 0 during inference stage. The length prediction problem can be viewed as a typical classification problem based on the output of the encoder. we follow Lee et al. (2018) to predict the length of the target sequence. The proposed Round function is unstable and non-differentiable, which make the decoding task difficult. We therefore propose a differentiable and robust method named SoftCopy following the spirit of the attention mechanism (Hahn and Keller, 2016; Bengio, 2009). The weight wi,j depends on the distance relationship between the source position i and the target position j. wij = softmax(−|j − i|/τ ) (10) where xi is usually the source embedding at position i. It is also worth mentioning that we take the top-most hidden states instead of the word embedding as xi in order to cache the global context information. 3.3.2 Learning from AT Experts The conditional independence assumption prevents NAT model from properly capturing the highly multimodal distribution of target translations. AT models takes already generated target tokens as inputs,"
P19-1125,P82-1020,0,0.816089,"Missing"
P19-1125,P07-2045,0,0.0125476,"or method in (Gu et al., 2017), (Lee et al., 2018) and (Kaiser et al., 2018) respectively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We rep"
P19-1125,N03-1017,0,0.0808619,"the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De. 1 (a) Autoregressive NMT (b) Non-Autoregressive NMT Figure 1: Neural architectures for Autoregressive NMT and Non-Autoregressive NMT. Introduction Neural machine translation (NMT) with encoderdecoder architectures (Sutskever et al., 2014; Cho et al., 2014) achieve significantly improved performance compared with traditional statistical methods(Koehn et al., 2003; Koehn, 2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al."
P19-1125,D18-1149,0,0.388239,"2010). Nevertheless, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computat"
P19-1125,D18-1336,0,0.243494,"ss, the autoregressive property of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving sig"
P19-1125,P16-1162,0,0.149108,"tively. imitate-NAT is our proposed NAT with imitation learning. 4 Experiments We evaluate our proposed model on machine translation tasks and provide the analysis. We present the experimental details in the following, including the introduction to the datasets as well as our experimental settings. Datasets We evaluate the proposed method on three widely used public machine translation corpora: IWSLT16 En-De(196K pairs), WMT14 EnDe(4.5M pairs) and WMT16 En-Ro(610K pairs). All the datasets are tokenized by Moses Koehn et al. (2007) and segmented into 32k−subword symbols with byte pair encoding Sennrich et al. (2016) to restrict the size of the vocabulary. For WMT14 En-De, we use newstest-2013 and newstest-2014 as development and test set respectively. For WMT16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets respectively. For IWSLT16 En-De, we use test2013 as validation for ablation experiments. Knowledge Distillation Datasets Sequencelevel knowledge distillation is applied to alleviate multimodality in the training dataset, using the AT demonstrator as the teachers (Kim and Rush, 2016). We replace the reference target sentence of each pair of training example (X, Y ) with a new"
P19-1125,D18-1044,0,0.0568991,"perty of the NMT decoder has been a bottleneck of the translation speed. Specifically, the decoder, whether based on Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or attention mechanism (Vaswani et al., 2017), sequentially generates words. The latter words are conditioned on previous words in a sentence. Such bottleneck disables parallel computation of decoder, which is serious for NMT, since the NMT decoding with a large vocabulary is extremely time-consuming. Recently, a line of research work (Gu et al., 2017; Lee et al., 2018; Libovick and Helcl, 2018; Wang et al., 2018) propose to break the autoregressive bottleneck by introducing non-autoregressive neural machine translation (NAT). In NAT, the decoder generates all words simultaneously instead of sequentially. Intuitively, NAT abandon feeding previous predicted words into decoder state at the next time step, but directly copy source encoded representation (Gu et al., 2017; Lee et al., 2018; Guo et al., 2018; Wang et al., 2019) as inputs of the decoder. Thus, the generation of the NAT models does not condition on previous prediction. NAT enables parallel computation of decoder, giving significantly fast tran"
P19-1193,P82-1020,0,0.820094,"Missing"
P19-1193,D14-1181,0,0.0030238,"input contains a variable number of topics, here we implement Dφ as a multi-label classiﬁer to distinguish between the real text with several topics and the generated text. In detail, suppose there are a total of |X |topics, the discriminator produces a sigmoid probability distribution over (|X |+ 1) classes. The score at the i-th (i ∈ {1, · · · , |X |}) index represents the probability that it belongs to the real text with the i-th topic, and the score at the (|X |+ 1)-th index represents the probability that the sample is the generated text. Here we implement the discriminator Dφ as a CNN (Kim, 2014) binary classiﬁer. 2.3 based on both state y1:t−1 and action yt is observed, the training objective of the generator Gθ is to minimize the negative expected reward, J(θ) = −Ey∼Gθ [r(y)] =− n−1  Gθ (yt+1 |y1:t ) · r(y1:t , yt+1 ) (14) t=1 where Gθ (yt+1 |yt ) means the probability that selects the word yt+1 based on the previous generated words. Applying the likelihood ratios trick and sampling method, we can build an unbiased estimation for the gradient of J(θ), ∇θ J(θ) ≈ − n−1  ∇θ logGθ (yt+1 |y1:t ) t=1 · r(y1:t , yt+1 )  (15) where yt+1 is the sampled word. Since the discriminator can o"
P19-1193,W17-3528,0,0.137882,"Missing"
P19-1193,P18-1082,0,0.0307102,"coder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strategy. Xu et al. (2018) utilize reinforcement learning to extract a skeleton of the story to promote the coherence. To improve the diversity and coherence, Yao et al. (2018) present a planand-write framework with two planning strategies to fully leverage storyline. However, story generation and the TEG task focus on different goals. The former focuses on logical reasoning and aims to generate a coherent story with plots, while the latter strives to generate the essay with aesthetics based on the input topics. Besides, the source information of the TEG task is more in"
P19-1193,N16-1014,0,0.1466,"Missing"
P19-1193,D18-1423,0,0.0620706,"Missing"
P19-1193,P18-1230,1,0.897461,"Missing"
P19-1193,P02-1040,0,0.10903,"includes: TAV representing topic semantics as the average of all topic embeddings and TAT applying attention mechanism to select the relevant topics. CVAE (Yang et al., 2018b) presents a conditional variational auto-encoder with a hybrid decoder to learn topic via latent variables. Plan&Write (Yao et al., 2018) proposes a planand-write framework with two planning strategies to improve diversity and coherence. 3.4 3.4.1 Automatic Evaluation The automatic evaluation of TEG remains an open and tricky question since the output is highly ﬂexible. Previous work (Feng et al., 2018) only adopts BLEU (Papineni et al., 2002) score based on ngram overlap to perform evaluation. However, it is unreasonable to only use BLEU for evaluation because TEG is an extremely ﬂexible task. There are multiple ideal essays for a set of input topics. To remedy this, here we develop a series of evaluation metrics to comprehensively measure the quality of output from various aspects. Consistency: An ideal essay should closely surround the semantics of all input topics. Therefore, we pre-train a multi-label classiﬁer to evaluate topic-consistency of the output. Given the input topics x, we deﬁne the topic-consistency of the generate"
P19-1193,D15-1044,0,0.0388062,"Automatic topic-to-essay generation (TEG) aims at generating novel, diverse, and topic-consistent paragraph-level text given a set of topics. It not only has plenty of practical applications, e.g., beneﬁting intelligent education or assisting in keyword-based news writing (Lepp¨anen et al., 2017), but also serves as an ideal testbed for controllable text generation (Wang and Wan, 2018). Despite its wide applications described above, the progress in the TEG task lags behind other generation tasks such as machine translation (Bahdanau et al., 2014) or text summarization (Rush et al., 2015). Feng et al. (2018) are the ﬁrst to propose the TEG task and they utilize coverage vector Equal Contribution.        Figure 1: Toy illustration of the information volume on three different text generation tasks, which shows that the source information is extremely insufﬁcient compared to the target output on the TEG task. Introduction ∗          to incorporate topic information for essay generation. However, the model performance is not satisfactory. The generated essays not only lack novelty and diversity, but also suffer from poor topiccons"
P19-1193,N18-2028,0,0.0307918,"Missing"
P19-1193,speer-havasi-2012-representing,0,0.166057,"Missing"
P19-1193,C16-1100,0,0.121336,"ency(ˆ y |x) = ϕ(x, x ˆ) (19) where ϕ is Jaccard similarity function and x ˆ is topics predicted by a pre-trained multi-label classiﬁer. Here we adopt the SGM model proposed in Yang et al. (2018a) to implement the pre-trained multi-label classiﬁer. Novelty: The novelty of the output can be reﬂected by the difference between it and the training texts. We calculate the novelty of each generated essay yˆ as: N ovelty(ˆ y |x) =1 − max{ϕ(ˆ y , y0 )| Baselines We adopt the following competitive baselines: SC-LSTM (Wen et al., 2015) uses gating mechanism to control the ﬂow of topic information. PNN (Wang et al., 2016) applies planning based neural network to generate topic-consistent text. MTA (Feng et al., 2018) utilizes coverage vectors to integrate topic information. Their work also includes: TAV representing topic semantics as the average of all topic embeddings and TAT applying attention mechanism to select the relevant topics. CVAE (Yang et al., 2018b) presents a conditional variational auto-encoder with a hybrid decoder to learn topic via latent variables. Plan&Write (Yao et al., 2018) proposes a planand-write framework with two planning strategies to improve diversity and coherence. 3.4 3.4.1 Autom"
P19-1193,D15-1199,0,0.0615644,"Missing"
P19-1193,D18-1462,1,0.848098,"multaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strategy. Xu et al. (2018) utilize reinforcement learning to extract a skeleton of the story to promote the coherence. To improve the diversity and coherence, Yao et al. (2018) present a planand-write framework with two planning strategies to fully leverage storyline. However, story generation and the TEG task focus on different goals. The former focuses on logical reasoning and aims to generate a coherent story with plots, while the latter strives to generate the essay with aesthetics based on the input topics. Besides, the source information of the TEG task is more insufﬁcient, putting higher demands on the model. 6"
P19-1193,C18-1330,1,0.928389,"single layer of LSTM with hidden size 512 for both encoder and decoder. We pre-train our model for 80 epochs with the MLE method. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 learning rate for pre-training and 10−5 for adversarial training. Besides, we make use of the dropout method (Srivastava et al., 2014) to avoid overﬁtting and clip the gradients (Pascanu et al., 2013) to the maximum norm of 10. 3.3 Consistency(ˆ y |x) = ϕ(x, x ˆ) (19) where ϕ is Jaccard similarity function and x ˆ is topics predicted by a pre-trained multi-label classiﬁer. Here we adopt the SGM model proposed in Yang et al. (2018a) to implement the pre-trained multi-label classiﬁer. Novelty: The novelty of the output can be reﬂected by the difference between it and the training texts. We calculate the novelty of each generated essay yˆ as: N ovelty(ˆ y |x) =1 − max{ϕ(ˆ y , y0 )| Baselines We adopt the following competitive baselines: SC-LSTM (Wen et al., 2015) uses gating mechanism to control the ﬂow of topic information. PNN (Wang et al., 2016) applies planning based neural network to generate topic-consistent text. MTA (Feng et al., 2018) utilizes coverage vectors to integrate topic information. Their work also incl"
P19-1193,D18-1353,0,0.0403253,"generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strateg"
P19-1193,P17-1125,0,0.0156374,"performance is unsatisfactory, showing that more effective model architecture needs to be explored, which is also the original intention of our work. A similar topic-to-sequence learning task is Chinese poetry generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims"
P19-1193,D14-1074,0,0.0351914,"neration (TEG) aims to compose novel, diverse, and topic-consistent paragraph-level text for several given topics. Feng et al. (2018) are the ﬁrst to propose the TEG task and they utilize coverage vector to integrate topic information. However, the performance is unsatisfactory, showing that more effective model architecture needs to be explored, which is also the original intention of our work. A similar topic-to-sequence learning task is Chinese poetry generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules"
P19-1193,D18-1138,1,0.878655,"Missing"
P19-1194,P18-1139,0,0.0593011,"ed on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12). There are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are: (1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) st into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3). 6 Conclusion In"
P19-1194,D14-1181,0,0.00290278,"ut is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking MRRR = N 1 X 1 N i=1 |rank(vi ) − rank(ˆ vi ) |+ 1 (13) In addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy. 3.4.2 Human Evaluation We also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches"
P19-1194,D18-1549,0,0.0242572,"Missing"
P19-1194,D17-1230,0,0.0454095,"in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as: K   1 X (k) ∇θ L(θ) &apos; − r − b ∇θ log pθ (yˆ(k) ) K k=1 (11) where K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017). Nevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and 2016 usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence yˆ based on the input text x and the target sentiment intensity value vy , and then reconstructs the source input x based on yˆ and the source sentiment intensity value vx . Therefore, the gradient of the cycle reconstruction loss is defined as:   ∇θ"
P19-1194,N18-1169,0,0.0483598,"Missing"
P19-1194,D18-1420,0,0.19687,"ntences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,D15-1166,0,0.0327,"0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value vy should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word “good” should be assigned a higher probability than word “bad”, thus the predicted intensity value g(“good”, s4 ) is closer to the target sentiment intensity than g(“bad”, s4 ). To favor words whose sentiment intensity is near vy , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around vy , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: 2 ! g(Es , st ) − vy 1 s ot = √ exp − (4) 2σ 2 2πσ pst = softmax(ost ) (5) where σ is the standard deviation. To balance both sentiment transformation and content preservation, the final probability distribution pt over the entire vocabulary is defined as a mixture of two probability distributions: pt = γpst + (1 − γ)pct (6) where γ is the hyper-parameter that controls the trade-off between two generation probabilities. 2015 &&apos; Encoder Algorithm 1 The cycle reinforcement learning algorithm for training Seq2Se"
P19-1194,S18-1001,0,0.0359134,"Missing"
P19-1194,E17-1096,0,0.0651517,"Missing"
P19-1194,P02-1040,0,0.107261,"2 2.64 2.54 2.37 2.52 3.84 3.85 2.13 2.14 3.41 2.43 2.84 3.21 Seq2SentiSeq 32.5 10.3 0.13 0.78 35.1 3.62 4.09 4.17 3.96 Human Reference 100.0 100.0 0.07 0.83 31.2 4.51 4.36 4.75 4.54 Table 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment (MAE, MRRR) and Fluency (PPL). Avg shows the average human scores. ↑ denotes larger is better, and vice versa. Bold denotes the best results. review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V 0 = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017). Sentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v1 , v2 , ..., vN ], the corresponding outputs of the model are [yˆ1"
P19-1194,P18-2031,0,0.0321807,"results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1 1 Target Sentiment Text sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018). Previous work (Shen et al., 2017; Li et al., 2018; Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of Joint work between WeChat AI and Peking University. 0.1 Horrible food and terrible service! 0.3 Plain food, slow service. 0.5 Food and service need improvement. 0.7 Good food and service. 0.9 Amazing food and perfect service!! Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment inten"
P19-1194,P18-1080,0,0.0258986,"tly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity. Related Work Recently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018; Tsvetkov et al., 2018; Li et al., 2018; Xu et al., 2018; Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from th"
P19-1194,P18-1090,1,0.78267,"e intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,P18-1102,0,0.162234,"t intensity2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order. There are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. t"
P19-1194,W17-5227,0,0.0201437,"challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. target sentiment intensity value is a real value, other than discrete labels. Second, parallel data3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outpu"
P19-1197,H05-1042,0,0.348902,"ain the performance of 9.71 BLEU score.1 1 Introduction Table-to-text generation is to generate a description from the structured table. It helps readers to summarize the key points in the table, and tell in the natural language. Figure 1 shows an example of table-to-text generation. The table provides some structured information about a person named “Denise Margaret Scott”, and the corresponding text describes the person with the key information in the table. Table-to-text generation can be applied in many scenarios, including weather report generation (Liang et al., 2009), NBA news writing (Barzilay and Lapata, 2005), biography generation (Dubou´e and McKeown, 2002; Lebret et al., 2016), and so on. Moreover, table-to-text genera1 The codes are available at https://github.com/ lancopku/Pivot. Denise Margaret Scott Born 24 April 1955 Melbourne, Victoria Nationality Australian Denise Margaret Scott 24 April 1955 Other names Scotty Occupation Comedian, actor, television and radio presenter Australian Known for Studio 10 Partner(s) John Lane Comedian, actor, television and radio presenter Children 2 Denise Margaret Scott (born 24 April 1955) is an Australian comedian, actor and television presenter. Surface Re"
P19-1197,E06-1040,0,0.0615103,"Missing"
P19-1197,P16-1185,0,0.0252088,"ates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a me"
P19-1197,N16-1012,0,0.032569,"1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25"
P19-1197,W02-2112,0,0.029488,"n brings a significant improvement to the pivot models under both vanilla Seq2Seq and Transformer frameworks, which demonstrates the efficiency of the denoising data augmentation. 3.9 the PIVOT model has some problem in generating repeating words (such as “senator” in the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao e"
P19-1197,W18-6505,0,0.222032,"ance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000 50000 60000 Parallel Data Size Figure 2: The BLEU scores of the a table-to-text model trained with different number of parallel data under the encoder-decoder framework on the WIKIBIO dataset. lel data is available. Some previous work (Puduppully et al., 2018; Gehrmann et al., 2018) formulates the task as the combination of content selection and surface realization, and models them with an end-to-end model. Inspired by these work, we break up the table-to-text generation into two stages, each of which is performed by a model trainable with only a few annotated data. Specifically, it first predicts the key facts from the tables, and then generates the text with the key facts, as shown in Figure 1. The two-stage method consists of two separate models: a key fact prediction model and a surface realization model. The key fact prediction model is formulated as a sequence labe"
P19-1197,N18-1032,0,0.0158927,"r fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where on"
P19-1197,J82-2005,0,0.518536,"Missing"
P19-1197,D16-1128,0,0.566599,"g the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed s"
P19-1197,P09-1011,0,0.134087,"the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model"
P19-1197,N03-1020,0,0.0677063,"Missing"
P19-1197,D15-1166,0,0.122996,"Realization Figure 1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext sam"
P19-1197,D18-1422,0,0.0332302,"y et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed symbolic operations in a sequence-to-sequence model. Qualitative Analysis We provide an example to illustrate the improvement of our model more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, w"
P19-1197,P02-1040,0,0.103597,"Missing"
P19-1197,D18-1411,0,0.0221501,"generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by ut"
P19-1197,W18-2205,0,0.0662726,"Missing"
P19-1197,W17-4503,0,0.0563362,"Missing"
P19-1197,K18-1003,0,0.0241831,"del more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lam"
P19-1197,D17-1239,0,0.0786093,"Missing"
P19-1197,D18-1356,0,0.0931456,"Missing"
P19-1197,C18-1330,1,0.82569,"ion, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000"
P19-1197,D18-1138,1,0.847758,"achine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a method to construct a pseudo parallel dataset for the surface realization model, without the need of any structured table. Experiments show that our proposed model can achieve 27.34 BLEU score on a biography generation dataset with only 1, 000 parallel data. Acknowledgement We thank the anonymous reviewers for their thoug"
P19-1257,P11-1020,0,0.0184667,"Missing"
P19-1257,D14-1179,0,0.00669761,"Missing"
P19-1257,P19-1285,0,0.06116,"Missing"
P19-1257,D18-1170,1,0.837476,"17) proposes a dynamic co-attention network for the question answering task and Seo et al. (2017) presents a bi-directional attention network to acquire query-aware context representations in machine comprehension. Tay et al. (2018a) proposes a co-attention mechanism based on Hermitian products for asymmetrical text matching problems. Zhong et al. (2019) further presents a coarse-grain ﬁne-grain co-attention network that combines information from evidence across multiple documents for question answering. In addition, the co-attention mechanism can also be applied to word sense disambiguation (Luo et al., 2018), recommended system (Tay et al., 2018b), and essay scoring (Zhang and Litman, 2018). 6 Conclusion In this paper, we propose the task of cross-modal automatic commenting, which aims at enabling the AI agent to make comments by integrating multiple modal contents. We construct a largescale dataset for this task and implement plenty of representative neural models. Furthermore, an effective co-attention model is presented to capture the intrinsic interaction between multiple modal contents. Experimental results show that our approach can substantially outperform various competitive baselines. Fu"
P19-1257,P02-1040,0,0.105952,"tention to 512 and the hidden size of feedforward layer to 2,048. The number of heads is set to 8, while a transformer layer consists of 6 blocks. We use Adam optimizer (Kingma and Ba, 2015) with learning rate 10−3 and apply dropout (Srivastava et al., 2014) to avoid over-ﬁtting. 4.2 BLEU-1 ROUGE-L DIST-1 DIST-2 Baselines We adopt the following competitive baselines: Seq2Seq: We implement a series of baselines based on Seq2Seq. S2S-V (Vinyals et al., 2015) Evaluation Metrics We adopt two kinds of evaluation methods: automatic evaluation and human evaluation. Automatic evaluation: We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to evaluate overlap between outputs and references. We also calculate the number of distinct n-grams (Li et al., 2016) in outputs to measure diversity. Human evaluation: Three annotators score the 200 outputs of different systems from 1 to 10. The evaluation criteria are as follows. Fluency measures whether the comment is ﬂuent. Relevance evaluates the relevance between the output and the input. Informativeness measures the amount of useful information contained in the output. Overall is a comprehensive metric. For each metric, the average Pearson correlation coefﬁcient"
P19-1257,P82-1020,0,0.754236,"Missing"
P19-1257,N16-1014,0,0.0170731,"We use Adam optimizer (Kingma and Ba, 2015) with learning rate 10−3 and apply dropout (Srivastava et al., 2014) to avoid over-ﬁtting. 4.2 BLEU-1 ROUGE-L DIST-1 DIST-2 Baselines We adopt the following competitive baselines: Seq2Seq: We implement a series of baselines based on Seq2Seq. S2S-V (Vinyals et al., 2015) Evaluation Metrics We adopt two kinds of evaluation methods: automatic evaluation and human evaluation. Automatic evaluation: We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to evaluate overlap between outputs and references. We also calculate the number of distinct n-grams (Li et al., 2016) in outputs to measure diversity. Human evaluation: Three annotators score the 200 outputs of different systems from 1 to 10. The evaluation criteria are as follows. Fluency measures whether the comment is ﬂuent. Relevance evaluates the relevance between the output and the input. Informativeness measures the amount of useful information contained in the output. Overall is a comprehensive metric. For each metric, the average Pearson correlation coefﬁcient is greater than 0.6, indicating that the human scores are highly consistent. 4.4 Experimental Results Table 3 and Table 4 show the results of"
P19-1257,P18-2025,0,0.10047,"3. 㔯㢢ཊ⛩ቡྭҶǄ (It would be better if there is more greenness.) Figure 1: An example in the constructed dataset. Red words indicate the content that is not included in the text but depicted in the images. Introduction Comments of online articles can provide rich supplementary information, which reduces the difﬁculty of understanding the article and enhances interactions between users. Therefore, achieving automatic commenting is necessary since it can contribute to improving user experience and increasing the activeness of social media platforms. Due to the importance described above, some work (Qin et al., 2018; Lin et al., 2018; Ma et al., 2018) has explored this task. However, these efforts are all focus on automatic commenting based solely on textual content. In real-scenarios, online ∗ Equal Contribution. The dataset and code are available at https:// github.com/lancopku/CMAC 1 articles on social media usually contain multiple modal contents. Take graphic news as an example, it contains plenty of images in addition to text. Other contents except text are also vital to improving automatic commenting. These contents may contain some information that is critical for generating informative comments."
P19-1257,W18-0549,0,0.0236437,"d Seo et al. (2017) presents a bi-directional attention network to acquire query-aware context representations in machine comprehension. Tay et al. (2018a) proposes a co-attention mechanism based on Hermitian products for asymmetrical text matching problems. Zhong et al. (2019) further presents a coarse-grain ﬁne-grain co-attention network that combines information from evidence across multiple documents for question answering. In addition, the co-attention mechanism can also be applied to word sense disambiguation (Luo et al., 2018), recommended system (Tay et al., 2018b), and essay scoring (Zhang and Litman, 2018). 6 Conclusion In this paper, we propose the task of cross-modal automatic commenting, which aims at enabling the AI agent to make comments by integrating multiple modal contents. We construct a largescale dataset for this task and implement plenty of representative neural models. Furthermore, an effective co-attention model is presented to capture the intrinsic interaction between multiple modal contents. Experimental results show that our approach can substantially outperform various competitive baselines. Further analysis demonstrates that with multiple modal information and co-attention, t"
P19-1257,D18-1138,1,0.892416,"Missing"
P19-1257,C18-1330,1,0.891219,"Missing"
P19-1257,Q14-1006,0,0.0389151,"Missing"
P19-1308,D18-1024,0,0.0462786,"d target embedding with GAN. Furthermore, Zhang et al. (2017b) and Xu et al. (2018) adopt the Earth Mover’s distance and Sinkhorn distance as the optimized distance metrics, respectively. There are also some attempts on distant language pairs. For instance, Kementchedjhieva et al. (2018) generalize Procrustes analysis by projecting the two languages into a latent space and Nakashole (2018) propose to learn neighborhood sensitive mapping by training non-linear functions. As for the hubness problem, Ruder et al. (2018) propose a latent-variable model learned with Viterbi EM algorithm. Recently, Alaux et al. (2018) work on the problem of aligning more than two languages simultaneously by a formulation ensuring composable mappings. 5 Conclusion In this work, we present a morphology-aware alignment model for unsupervised bilingual lexicon induction. The proposed model is able to alleviate the adverse effect of morphological variation by introducing grammatical information learned from pre-trained denoising language model. The results show that our approach can achieve better performance than several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised met"
P19-1308,Q18-1014,0,0.0255397,"r source words rather than target words and Artetxe et al. (2017) present a self-learning framework to perform iterative reﬁnement, which is also adopted in some unsupervised settings and plays a crucial role in improving performance. Unsupervised cross-lingual embedding. The endeavors to explore unsupervised cross-lingual embedding are mainly divided into two categories. One line focuses on designing heuristics or utilizing the structural similarity of monolingual embeddings. For instance, Hoshen and Wolf (2018) present a non-adversarial method based on the principal component analysis. Both Aldarmaki et al. (2018) and Artetxe et al. (2018a) take advantage of geometric properties across languages to perform word retrieval to learn the initial word mapping. Cao and Zhao (2018) formulate this problem as point set registration to adopt a point set registration method. However, these methods usually require plenty of random restarts or additional skills to achieve satisfactory performance. Another line strives to learn unsupervised word mapping by direct distribution-matching. For example, Lample et al. (2018) and Zhang et al. (2017a) completely eliminate the need for any supervision signal by aligning the"
P19-1308,D16-1250,0,0.428609,"comparable or even better performance than supervised systems. This illustrates that the quality of word alignment can be improved by introducing grammar information from the pre-trained denoising language model. Our denoising evaluator encourages the model to retrieve the correct translation with appropriate morphological by assessing the ﬂuency of sentences obtained by word-to-word translation. This alleviates the adverse effect of morphological variation. 3192 Methods DE-EN EN-DE ES-EN EN-ES FR-EN EN-FR IT-EN EN-IT Supervised: Mikolov et al. (2013a) Xing et al. (2015) Shigeto et al. (2015) Artetxe et al. (2016) Artetxe et al. (2017) 61.93 67.73 71.07 69.13 68.07 73.07 69.53 63.73 72.13 69.20 74.00 77.20 81.07 78.27 75.60 80.73 78.60 74.53 80.07 78.20 71.33 76.33 79.93 77.73 74.47 82.20 78.67 73.13 79.20 77.67 68.93 72.00 76.47 73.60 70.53 77.60 73.33 68.13 74.47 71.67 Unsupervised: Zhang et al. (2017a) Zhang et al. (2017b) Lample et al. (2018) Xu et al. (2018) Artetxe et al. (2018a) Ours 40.13 69.73 67.00 72.27 73.13 41.27 55.20 71.33 69.33 73.60 74.47 58.80 70.87 79.07 77.80 81.60 82.13 60.93 71.40 78.80 79.53 80.67 81.87 77.87 75.47 80.20 81.53 57.60 78.13 77.93 80.40 81.27 43.60 64.87 74.47 72.60"
P19-1308,P17-1042,0,0.434512,"ter performance than supervised systems. This illustrates that the quality of word alignment can be improved by introducing grammar information from the pre-trained denoising language model. Our denoising evaluator encourages the model to retrieve the correct translation with appropriate morphological by assessing the ﬂuency of sentences obtained by word-to-word translation. This alleviates the adverse effect of morphological variation. 3192 Methods DE-EN EN-DE ES-EN EN-ES FR-EN EN-FR IT-EN EN-IT Supervised: Mikolov et al. (2013a) Xing et al. (2015) Shigeto et al. (2015) Artetxe et al. (2016) Artetxe et al. (2017) 61.93 67.73 71.07 69.13 68.07 73.07 69.53 63.73 72.13 69.20 74.00 77.20 81.07 78.27 75.60 80.73 78.60 74.53 80.07 78.20 71.33 76.33 79.93 77.73 74.47 82.20 78.67 73.13 79.20 77.67 68.93 72.00 76.47 73.60 70.53 77.60 73.33 68.13 74.47 71.67 Unsupervised: Zhang et al. (2017a) Zhang et al. (2017b) Lample et al. (2018) Xu et al. (2018) Artetxe et al. (2018a) Ours 40.13 69.73 67.00 72.27 73.13 41.27 55.20 71.33 69.33 73.60 74.47 58.80 70.87 79.07 77.80 81.60 82.13 60.93 71.40 78.80 79.53 80.67 81.87 77.87 75.47 80.20 81.53 57.60 78.13 77.93 80.40 81.27 43.60 64.87 74.47 72.60 76.33 77.60 44.53 65."
P19-1308,P18-1073,0,0.237613,"ance compared to supervised methods. 1 Source word Top-3 of retrieved nearest neighbors mangez suspendit diffusant eats suspending broadcasts eat suspend broadcast buttered suspended broadcasting Table 1: Three randomly selected failure examples of MUSE on FR-EN language pair. Red words are correct translations, which are all not the nearest translations. Introduction The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases are usually morphologically related words. Due to similar semantics, these words can easily confuse"
P19-1308,K18-1021,0,0.0456011,"ubstantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods. 1 Source word Top-3 of retrieved nearest neighbors mangez suspendit diffusant eats suspending broadcasts eat suspend broadcast buttered suspended broadcasting Table 1: Three randomly selected failure examples of MUSE on FR-EN language pair. Red words are correct translations, which are all not the nearest translations. Introduction The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases a"
P19-1308,D18-1101,0,0.0217694,"g. Denoising Auto-Encoder Considering some ingrained problems (e.g. word order) of the naive word-to-word translation, the original translation t can be regarded as a noisy version of the ground-truth translation. Therefore, we adopt a DAE (Vincent et al., 2008) to clean noise in t = (t1 , · · · , tm ) so that E can provide a more accurate supervisory signal. Here we implement the DAE as an encoder-decoder framework (Bahdanau et al., 2015). The input is the noisy version N (c) and the output is the cleaned sentence c, where c is a sentence sampled from the target monolingual corpus. Following Kim et al. (2018), we construct N (c) by designing three noises: insertion, deletion, and reordering. Readers can refer to Kim et al. (2018) for more technical explanations. 1 For simplicity, we employ the cosine similarity. Readers can also adopt other retrieval methods (e.g. CSLS) to obtain better performance. 3191 Language Model For a source sentence s, if W is of high quality, the denoised translated sentence should keep ﬂuent and grammatically correct. Otherwise, if W retrieves a morphologically related but erroneous word, the denoised translated sentence tends to be grammatically incorrect, leading to po"
P19-1308,J82-2005,0,0.715872,"Missing"
P19-1308,D18-1047,0,0.0134124,"vised word mapping by direct distribution-matching. For example, Lample et al. (2018) and Zhang et al. (2017a) completely eliminate the need for any supervision signal by aligning the distribution of transferred embedding and target embedding with GAN. Furthermore, Zhang et al. (2017b) and Xu et al. (2018) adopt the Earth Mover’s distance and Sinkhorn distance as the optimized distance metrics, respectively. There are also some attempts on distant language pairs. For instance, Kementchedjhieva et al. (2018) generalize Procrustes analysis by projecting the two languages into a latent space and Nakashole (2018) propose to learn neighborhood sensitive mapping by training non-linear functions. As for the hubness problem, Ruder et al. (2018) propose a latent-variable model learned with Viterbi EM algorithm. Recently, Alaux et al. (2018) work on the problem of aligning more than two languages simultaneously by a formulation ensuring composable mappings. 5 Conclusion In this work, we present a morphology-aware alignment model for unsupervised bilingual lexicon induction. The proposed model is able to alleviate the adverse effect of morphological variation by introducing grammatical information learned fr"
P19-1308,E17-2025,0,0.0293026,"formance. 3191 Language Model For a source sentence s, if W is of high quality, the denoised translated sentence should keep ﬂuent and grammatically correct. Otherwise, if W retrieves a morphologically related but erroneous word, the denoised translated sentence tends to be grammatically incorrect, leading to poor ﬂuency. Therefore, a language model is used to evaluate the ﬂuency of translation to guide the training of W. We implement the language model as an LSTM (Hochreiter and Schmidhuber, 1997) structure with weight tying. Since this part is not the focus of our work, readers can refer to Press and Wolf (2017) for the details. With the grammatical information learned by the pre-trained language model, erroneous word alignment due to morphological variation is penalized. Therefore, W is encouraged to retrieve correct word translation with appropriate morphology. where zi is the i-th word in zt = (z1 , · · · , z|z |), z<i refers to the sequence (z1 , · · · , zi−1 ), and q(zi |z<i ) is the probability that the pre-trained language model outputs the word zi conditioned on z<i . If zt is ﬂuent and grammatically correct, the corresponding reward R(zt ) is relatively large. Therefore, the reward R(zt ) ca"
P19-1308,D18-1042,0,0.0593609,"nate the need for any supervision signal by aligning the distribution of transferred embedding and target embedding with GAN. Furthermore, Zhang et al. (2017b) and Xu et al. (2018) adopt the Earth Mover’s distance and Sinkhorn distance as the optimized distance metrics, respectively. There are also some attempts on distant language pairs. For instance, Kementchedjhieva et al. (2018) generalize Procrustes analysis by projecting the two languages into a latent space and Nakashole (2018) propose to learn neighborhood sensitive mapping by training non-linear functions. As for the hubness problem, Ruder et al. (2018) propose a latent-variable model learned with Viterbi EM algorithm. Recently, Alaux et al. (2018) work on the problem of aligning more than two languages simultaneously by a formulation ensuring composable mappings. 5 Conclusion In this work, we present a morphology-aware alignment model for unsupervised bilingual lexicon induction. The proposed model is able to alleviate the adverse effect of morphological variation by introducing grammatical information learned from pre-trained denoising language model. The results show that our approach can achieve better performance than several state-of-t"
P19-1308,E14-1049,0,0.0435635,"ched Table 5: Translations of various systems on the FR-EN language pair. Red words are correct translations. 4 Figure 3: Visualization of two monolingual embedding spaces (left) and aligned embedding space (right). Related Work This paper is mainly related to the following two lines of work. Supervised cross-lingual embedding. Inspired by the isometric observation between monolingual word embeddings of two different languages, Mikolov et al. (2013b) propose to learn cross-lingual word mapping by minimizing mean squared error. Latter, Dinu and Baroni (2015) investigate the hubness problem and Faruqui and Dyer (2014) incorporates the semantics of a word in multiple languages into its embedding. Furthermore, Xing et al. (2015) propose to impose the orthogonal constraint to the linear mapping and Artetxe et al. (2016) present a series of techniques, including length normalization and mean centering, to improve bilingual results. There also exist some other representative researches. For instance, Smith et al. (2017) present inversesoftmax which normalizes the softmax probability over source words rather than target words and Artetxe et al. (2017) present a self-learning framework to perform iterative reﬁnem"
P19-1308,P82-1020,0,0.812237,"Missing"
P19-1308,P18-1072,0,0.0289056,"The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases are usually morphologically related words. Due to similar semantics, these words can easily confuse the system to make the incorrect alignment. Table 1 presents three randomly selected failure examples of MUSE (Lample et al., 2018) on the FR-EN language pair, showing that all failures can be attributed to morphological variation. For instance, for the French source word “mangez”, MUSE translates it to morphologically related word “eats”, instead of the correct Englis"
P19-1308,N15-1104,0,0.210515,"Missing"
P19-1308,D18-1268,0,0.20342,"obtained by word-to-word translation. This alleviates the adverse effect of morphological variation. 3192 Methods DE-EN EN-DE ES-EN EN-ES FR-EN EN-FR IT-EN EN-IT Supervised: Mikolov et al. (2013a) Xing et al. (2015) Shigeto et al. (2015) Artetxe et al. (2016) Artetxe et al. (2017) 61.93 67.73 71.07 69.13 68.07 73.07 69.53 63.73 72.13 69.20 74.00 77.20 81.07 78.27 75.60 80.73 78.60 74.53 80.07 78.20 71.33 76.33 79.93 77.73 74.47 82.20 78.67 73.13 79.20 77.67 68.93 72.00 76.47 73.60 70.53 77.60 73.33 68.13 74.47 71.67 Unsupervised: Zhang et al. (2017a) Zhang et al. (2017b) Lample et al. (2018) Xu et al. (2018) Artetxe et al. (2018a) Ours 40.13 69.73 67.00 72.27 73.13 41.27 55.20 71.33 69.33 73.60 74.47 58.80 70.87 79.07 77.80 81.60 82.13 60.93 71.40 78.80 79.53 80.67 81.87 77.87 75.47 80.20 81.53 57.60 78.13 77.93 80.40 81.27 43.60 64.87 74.47 72.60 76.33 77.60 44.53 65.27 75.33 73.47 77.13 78.33 Table 2: The accuracy of different methods in various language pairs. Bold indicates the best supervised and unsupervised results, respectively. “-” means that the model fails to converge and hence the result is omitted. Models EN-ES EN-FR EN-DE EN-IT Full model 81.87 81.27 74.47 78.33 w/o Evaluator w/o DA"
P19-1308,P17-1179,0,0.560126,"words are correct translations, which are all not the nearest translations. Introduction The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases are usually morphologically related words. Due to similar semantics, these words can easily confuse the system to make the incorrect alignment. Table 1 presents three randomly selected failure examples of MUSE (Lample et al., 2018) on the FR-EN language pair, showing that all failures can be attributed to morphological variation. For instance, for the French source word “mang"
P19-1308,D17-1207,0,0.492522,"words are correct translations, which are all not the nearest translations. Introduction The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases are usually morphologically related words. Due to similar semantics, these words can easily confuse the system to make the incorrect alignment. Table 1 presents three randomly selected failure examples of MUSE (Lample et al., 2018) on the FR-EN language pair, showing that all failures can be attributed to morphological variation. For instance, for the French source word “mang"
P19-1308,D18-1138,1,0.897904,"Missing"
P19-1308,P16-1133,0,0.0316458,"ce word Top-3 of retrieved nearest neighbors mangez suspendit diffusant eats suspending broadcasts eat suspend broadcast buttered suspended broadcasting Table 1: Three randomly selected failure examples of MUSE on FR-EN language pair. Red words are correct translations, which are all not the nearest translations. Introduction The task of unsupervised bilingual lexicon induction aims at identifying translational equivalents across two languages (Kementchedjhieva et al., 2018). It can be applied in plenty of real-scenarios, such as machine translation (Artetxe et al., 2018b), transfer learning (Zhou et al., 2016), and so on. Based on the observation that embedding spaces of different languages exhibit similar structures, a prominent approach is to align monolingual embedding spaces of two languages with a simple linear mapping (Zhang et al., 2017a; Lample et al., 2018). However, previous work (Artetxe et al., 2018a; Søgaard et al., 2018) has shown that morphological variation is an intractable challenge for the UBLI task. The induced translations in failure cases are usually morphologically related words. Due to similar semantics, these words can easily confuse the system to make the incorrect alignme"
P19-1479,W04-3252,0,0.0625855,"the article which serve as the topics of the news. These keywords are the most important words to understand the story of the article, most of which are named entities. Since keyword detection is not the main point of this paper, we do not go into the details of the extraction process. Given a news article D, we first do word segmentation and named entity recognition on the news articles with off-the-shelf tools such as Stanford CoreNLP.3 Since the named entities alone can be insufficient to cover the main focuses of the document, we further apply keyword extraction algorithms like TextRank (Mihalcea and Tarau, 2004) to obtain additional keywords. After we get the keywords κ of the news, we associate each sentence of the documents to its corresponding keywords. We adopt a simple strategy that assigns a sentence s to the keyword k if k appears in the sentence. Note that one sentence can be associated with multiple keywords, which implicitly indicates connection between the two topics. Sentences that do not contain any of the keywords are put into a special vertex called “Empty”. Because the title of the article is crucial to understand the news, we also add a special vertex called “Title” that contains the"
P19-1479,P18-1026,0,0.0494272,"ks (GCN) is applied to classify the documents. Liu et al. (2018) proposed a siamese GCN model in the text matching task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to model long text documents as graphs, which is more challenging. 3 Graph-to-Sequence Model In this sectio"
P19-1479,P18-1008,0,0.0544696,"). The length of the input sequence is truncated to 100. For the input of title together with content, we append the content to the back of the title. Evaluation Metrics We choose three metrics to evaluate the quality of generated comments. For all the metrics, we ask the raters to score the comments with three gears, the scores are then projected to 0 ∼ 10. • Coherence: This metric evaluates how Coherent (consistent) is the comment to the news document. It measures whether the comment is about the main story of the news, one side part of the news, or irrelevant to the news. • Self-attention (Chen et al., 2018): this model follows the encoder-decoder framework. We use multi-layer self-attention with multi-head as the encoder, and a RNN decoder with attention is applied. We use two kinds of input, the bag of words (B) and the keywords (K). Since the input is not sequential, positional encoding is not applied. A special ‘CLS’ label is inserted, the hidden vector of which serves as the initial state of decoder. For the bag of words input we use the words with top 100 term frequency (TF) in the news document. For the keywords input, we use the same extracted keywords (topic words) with the ones used in"
P19-1479,P16-1154,0,0.0727217,"− 21 A˜D ˜ − 12 H l W l ) H l+1 = σ(D A˜ = A + IN (4) (5) ˜ − 21 A˜D ˜ is the where IN is the identity matrix, D normalized symmetric adjacency matrix, W l is a learnable weight matrix. To avoid the oversmoothing problem of GCN, we add residual connections between layers, g l+1 = H l+1 + H l g out K = tanh(Wo g ) (7) Decoder ti = RN N (ti−1 , ei−1 ) X ci = αj × gj exp(δ(ti , gj ) αj = P exp(δ(ti , gk )) (8) (9) (10) where δ is the attention function. Since the topic words (name of the vertices) κ are important information for the article and may appear in the comment, we adopt copy mechanism (Gu et al., 2016) by merging the predicted word token probability distribution with the attention distribution. The probability pcopy of copying from the topic words is dynamically calculated with the decoding hidden state ti and the context vector ci , yi = sof tmax(Wo (tanh(W ([ti ; ci ]) + b))) pcopy = σ(Wcopy [ti ; ci ]) p = (1 − pcopy ) × y + pcopy × α content title comment keyword (11) (12) (13) comment # 287,889 378,677 ave word # Ent Sport 456.1 506.6 16.4 15.7 16.3 19.4 8.4 9.0 ave character # Ent Sport 754.0 858.7 28.1 27.4 26.2 31.2 - Table 3: Length of content, title, comment and keyword of the new"
P19-1479,P17-1099,0,0.0379121,"o the retrieved comments during generation, which is a combination of retrieval and generation based model. Pure generation based model remains challenging, yet is a more direct way to solve the problem. Additionally, when the article is very different from the historical ones, there may not be appropriate comments to refer to. In this work, we would like to explore a generation model that better exploits the news content to solve the problem. Different from the scenarios where sequenceto-sequence models achieve great success like machine translation (Bahdanau et al., 2014) and summarization (See et al., 2017), comment generation has several nontrivial challenges: 1 https://kuaibao.qq.com/ Code for the paper is available https://github.com/lancopku/ Graph-to-seq-comment-generation 2 at • The news articles can be very long, which makes it intractable for classic sequence-tosequence models. On the contrary, although the title is a very important information resource, it can be too short to provide sufficient information. • The title of the news sometimes uses hyperbolic expressions that are semantically different from the content of the article. For example, the title shown in the example (Table 1) p"
P19-1479,W18-6459,0,0.0272423,"ng task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to model long text documents as graphs, which is more challenging. 3 Graph-to-Sequence Model In this section, we introduce the proposed graphto-sequence model (shown in Figure 1). Our model follows the Encoder-Decoder f"
P19-1479,P18-1150,0,0.0615954,"Missing"
P19-1479,D18-1112,0,0.018799,"the more sentences comention two keywords together, the closer these based graph convolutional networks (GCN) is applied to classify the documents. Liu et al. (2018) proposed a siamese GCN model in the text matching task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to"
P19-1479,N16-1174,0,0.713898,"rs in Marvel movies. Based on the above observations, we propose a graph-to-sequence model that generates comments based on a graph constructed out of content of the article and the title. We propose to represent the long document as a topic interaction graph, which decomposes the text into several topic centered clusters of texts, each of which representing a key aspect (topic) of the article. Each cluster together with the topic form a vertex in the graph. The edges between vertices are calculated based on the semantic relation between the vertices. Compared with the hierarchical structure (Yang et al., 2016), which is designed for long articles, our graph based model is better able to understand the connection between different topics of the news. Our model jointly models the title • We collect and release a large scale (200,000) article-comment corpus that contains title, content and the comments of the news articles. 2 Related Work The Graph Neural Networks (GNN) model has attracted growing attention recently, which is good at modeling graph structure data. GNN is not only applied in structural scenarios, where the data are naturally performed in graph structure, such as social network predicti"
P19-1479,P18-2025,0,\N,Missing
P19-1482,P11-1105,0,0.0902983,"Missing"
P19-1482,J15-2001,0,0.0589433,"Missing"
P19-1482,N18-1169,0,0.129333,"Missing"
P19-1482,W16-2379,0,0.0426908,"Missing"
P19-1482,P18-1080,0,0.119644,"hensive comparison with stateof-the-art style transfer methods. CrossAligned (Shen et al., 2017) aligns decoder hidden states adversarially. MultiDecoder (Fu et al., 2018) adopts multiple decoders for different styles. StyleEmbedding (Fu et al., 2018) adopts a single decoder conditioned on learned style embeddings. TemplateBased (Li et al., 2018) retrieves and replaces stylized words. DeleteOnly (Li et al., 2018) only deletes the stylized words in the input sentence. Del-Ret-Gen (Li et al., 2018) is the same as TemplateBased except that an RNN is adopted to generate the output. BackTranslate (Prabhumoye et al., 2018) stylizes the back-translated input. UnpairedRL (Xu et al., 2018) deletes stylized words and generates with a denoising AE. UnsuperMT Amazon Acc BLEU Acc BLEU CrossAligned MultiDecoder StyleEmbedding TemplateBased DeleteOnly Del-Ret-Gen BackTranslate UnpairedRL UnsuperMT 74.7 50.6 8.4 81.2 86.0 88.6 94.6 57.5 97.8 9.06 14.54 21.06 22.57 14.64 15.96 2.46 18.81 22.75 75.1 69.9 38.2 64.3 47.0 51.0 76.7 56.3 72.4 1.90 9.07 15.07 34.79 33.00 30.09 1.04 15.93 33.95 Human 74.7 - 43.2 - Point-Then-Operate 91.5 29.86 40.2 41.86 Table 4: Automatic evaluation results for classification accuracy and BLEU"
P19-1482,W18-5420,0,0.0204412,"sentiment memory. As far as we know, there are two work that avoid disentangled representations. Zhang et al. (2018b) construct a pseudo-aligned dataset with an SMT model and then learn two NMT models jointly and iteratively. A concurrent work, Luo et al. (2019), propose to learn two dual seq2seq models between two styles via reinforcement learning, without disentangling style and content. Sequence Operation Methods Our work is also closely related to sequence operation methods, which are widely used in SMT (Durrani et al., 2011, 2015; Pal et al., 2016) and starts to attract attention in NMT (Stahlberg et al., 2018). Compared with methods based on seq2seq models, sequence operation methods are inherently more interpretable (Stahlberg et al., 2018). Notably, our method is revision-based, i.e., it operates directly on the input sentence and does not generate from scratch as in machine translation systems. Hierarchical Reinforcement Learning In this work, we adopt the Options Framework (Sutton et al., 1999) in HRL, in which a high-level agent learns to determine more abstract options and a low-level agent learns to take less abstract actions given the option. Recent work has shown that HRL is effective in v"
P19-1482,D14-1181,0,0.00356195,"od. Thus, on the Amazon dataset, we adopt a cross-domain setting, i.e., we train the modules 4878 Dataset Attributes Train Dev Test Yelp Positive Negative 270K 180K 2000 2000 500 500 Amazon Positive Negative 277K 278K 985 1015 500 500 Yelp Table 3: Dataset statistics. on the Yelp training set using the Amazon vocabulary and test the method on Amazon test set. Experimental results show the effectiveness of our method under this cross-domain setting. 5.2 Evaluation Metrics Automatic Evaluation Following previous work (Shen et al., 2017; Xu et al., 2018), we pre-train a style classifier TextCNN (Kim, 2014) on each dataset and measure the style polarity of system outputs based on the classification accuracy. Also, based on the human references provided by Li et al. (2018), we adopt a caseinsensitive BLEU metric, which is computed using the Moses multi-bleu.perl script. Human Evaluation Following previous work (Shen et al., 2017; Xu et al., 2018), we also conduct human evaluations. For each input sentence and corresponding output, each participant is asked to score from 1 to 5 for fluency, content preservation, and style polarity. If a transfer gets scores of 4 or 5 on all three aspects, it is co"
P19-1482,P18-1090,1,0.764415,"our method significantly improves BLEU, fluency, and content preservation compared with recent methods and effectively addresses the aforementioned challenges. 2 Related Work Text Style Transfer Most work on text style transfer learns disentangled representations of style and content. We categorize them based on how they represent content. Hidden vector approaches represent content as hidden vectors, e.g., Hu et al. (2017) adversarially incorporate a VAE and a style classifier; Shen et al. (2017) propose a cross-aligned AE that adversarially aligns the hidden states of the decoder; Fu et al. (2018) design a multi-decoder model and a style-embedding model for better style representations; Yang et al. (2018) use language models as style discriminators; John et al. (2018) utilize bagof-words prediction for better disentanglement of style and content. Deletion approaches represent content as the input sentence with stylized words deleted, e.g., Li et al. (2018) delete stylized ngrams based on corpus-level statistics and stylize it based on similar, retrieved sentences; Xu et al. (2018) jointly train a neutralization module and a stylization module the with reinforcement learning; Zhang et a"
P19-1482,D18-1138,1,0.866893,"al. (2018) design a multi-decoder model and a style-embedding model for better style representations; Yang et al. (2018) use language models as style discriminators; John et al. (2018) utilize bagof-words prediction for better disentanglement of style and content. Deletion approaches represent content as the input sentence with stylized words deleted, e.g., Li et al. (2018) delete stylized ngrams based on corpus-level statistics and stylize it based on similar, retrieved sentences; Xu et al. (2018) jointly train a neutralization module and a stylization module the with reinforcement learning; Zhang et al. (2018a) facilitate the stylization step with a learned sentiment memory. As far as we know, there are two work that avoid disentangled representations. Zhang et al. (2018b) construct a pseudo-aligned dataset with an SMT model and then learn two NMT models jointly and iteratively. A concurrent work, Luo et al. (2019), propose to learn two dual seq2seq models between two styles via reinforcement learning, without disentangling style and content. Sequence Operation Methods Our work is also closely related to sequence operation methods, which are widely used in SMT (Durrani et al., 2011, 2015; Pal et a"
P19-1518,W17-2339,0,0.0161155,"trieval (Gopal and Yang, 2010). Due to the complex dependency between labels, a key challenge for the MLC task is how to effectively capture high-order correlations between labels (Zhang and Zhou, 2014). When involving in capturing high-order correlations between labels, one line of research focuses on exploring the hierarchical structure of the label space (Prabhu and Varma, 2014; Jernite et al., 2017; Peng et al., 2018; Singh et al., 2018), while 1 The code is available at https://github.com/ lancopku/Seq2Set another line strives to extend specific learning algorithms (Zhang and Zhou, 2006; Baker and Korhonen, 2017; Liu et al., 2017). However, most of these work tends to result in intractable computational costs (Chen et al., 2017). Recently, based on a pre-defined label order, Nam et al. (2017); Yang et al. (2018) succeeded in applying the sequence-to-sequence (Seq2Seq) model to the MLC task, which shows its powerful ability to capture high-order label correlations and achieves excellent performance. However, the Seq2Seq model suffers from some thorny flaws on the MLC task. The output labels are essentially an unordered set with swapping-invariance2 , rather than an ordered sequence. This inconsistency"
P19-1518,D15-1099,1,0.876314,"Missing"
P19-1518,D18-1485,1,0.882146,"el has excellent universality, which works for different label distributions. Our approach not only has the ability of Seq2Seq to capture label correlations, but also alleviates the strict requirements of Seq2Seq for label order via reinforcement learning. This avoids the problem of difficulty in predefining a reasonable label order on the uniform distribution, leading to excellent universality. 4.4 Improving Model Universality The labels in the RCV1-V2 dataset exhibits a longtail distribution. However, in real-scenarios, there are other common label distributions, e.g., uniform distribution (Lin et al., 2018a). Therefore, here we analyze the universality of the Seq2Set 5 This weak decline can be attributed to the influence of the label order on the pre-training. Error Analysis We find that all methods perform poorly when predicting low-frequency (LF) labels compared to high-frequency (HF) labels. This is reasonable because samples assigned LF labels are sparse, making it hard for the model to learn an effective pattern to make predictions. Figure 2 shows the results of different methods on HF labels and 5255 Figure 2: Performance of different systems on the HF labels and LF labels. “Impv-BR” and"
P19-1518,N16-1063,0,0.173555,"Missing"
P19-1518,N19-1321,0,0.120416,"Missing"
P19-1518,D18-1308,0,0.0595666,"Missing"
P19-1518,C18-1330,1,0.944938,"nvolving in capturing high-order correlations between labels, one line of research focuses on exploring the hierarchical structure of the label space (Prabhu and Varma, 2014; Jernite et al., 2017; Peng et al., 2018; Singh et al., 2018), while 1 The code is available at https://github.com/ lancopku/Seq2Set another line strives to extend specific learning algorithms (Zhang and Zhou, 2006; Baker and Korhonen, 2017; Liu et al., 2017). However, most of these work tends to result in intractable computational costs (Chen et al., 2017). Recently, based on a pre-defined label order, Nam et al. (2017); Yang et al. (2018) succeeded in applying the sequence-to-sequence (Seq2Seq) model to the MLC task, which shows its powerful ability to capture high-order label correlations and achieves excellent performance. However, the Seq2Seq model suffers from some thorny flaws on the MLC task. The output labels are essentially an unordered set with swapping-invariance2 , rather than an ordered sequence. This inconsistency usually leads to some intractable problems, e.g., sensitivity to the label order. Previous work (Vinyals et al., 2016) has shown that the order has a great impact on the performance of the Seq2Seq model."
P19-1603,S18-1032,0,0.0289074,"Missing"
P19-1603,N18-2008,0,0.0522299,"ces a policy gradient learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclus"
P19-1603,N18-1169,0,0.078025,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,C18-1088,0,0.0894506,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,D18-1420,0,0.0224517,"oid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequence model with attention (Luong et al., 2015), and then modifies the sentiment of the story ending towards the target sentiment intensity via a fine-grained sentiment modification method (Liao et al., 2018). We call this baseline Seq2Seq + SentiMod. 3.3 Experiment Settings We tune hyper-parameters on the validation set. For the RM and DA sentiment analyzer, we implement the encoder as a 3-layer bidirectional LSTM with a hidden size of 512. We implement the regression module as a MLP with 1 hidden layer of size 32. For domain adaption, we implement a domain discriminator as a MLP with 1 hidden layer of size 32. A Gradient Reversal Layer is added into the domain discriminator. For the sentimental generator, both the semantic and sentiment embeddings are 256 dimensions and randomly initialized. We"
P19-1603,D15-1166,0,0.0213571,"th the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabilities: P (yt ) = ↵PR (yt ) + PS (yt ) (1) where PR (yt ) denotes the semantic generation probability, PS (yt ) denotes the sentiment generation probability, ↵ and are trainable coefficients. Specifically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding"
P19-1603,N16-1098,0,0.0491088,"timent intensities s. It consists of an encoder and a decoder equipped with a Gaussian Kernel Layer. The encoder is to map the input story context x into a compact vector that can capture its essential context features. Specifically, we use a normal bi-directional LSTM as the encoder. All context words xi are represented by their semantic embeddings E as the input and we use the concatenation of final forward and backward hidden states as the initial hidden state of the decoder. 6021 3 Decoder ?? y?? = ?????? ???? + ?????? ???? Experiment 3.1 ℎ??−1 We choose the widely-used ROCStories corpus (Mostafazadeh et al., 2016) which consists of 100k five-sentence stories. We split the data into a training set with 93,126 stories, a validation set with 5,173 stories and a test set with 5,175 stories. Gaussian Kernel Layer ???? Semantic Embeddings Sentiment Embeddings Target Sentiment Intensity Figure 3: The decoder of the sentimental generator. A Gaussian Kernel Layer is introduced to make use of the target sentiment intensity. The decoder aims to generate a story ending which accords with the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabi"
P19-1603,P02-1040,0,0.104057,"story endings in the test set (H-M SentiCons). To evaluate the performance of sentimental generator, for each story context in the test set, we generate five story endings with five target sentiment intensity ranging from [0, 1]. Then we calculate SentiCons of input target sentiment intensities and sentiment intensities of the outputs predicted by the best sentiment analyzer (I-O SentiCons). BLEU: For each story in the test set, we take the context x and the human-annotated sentiment intensity s of the gold story ending y as input. The ˆ Then we calculate the corresponding output is y. BLEU (Papineni et al., 2002) score of y and yˆ as the overall quality of the generated story endings. 3.4.2 Human Evaluation We hire two evaluators who are skilled in English to evaluate the generated story endings. For each story in the test set, we distribute the story context, five target sentiment intensities and corresponding generated story endings to the evaluators. Evaluators are required to score the generated endings from 1 to 5 in terms of three criteria: Coherency, Fluency and Sentiment. Coherency measures whether the endings are coherent with the context. Fluency measures whether the endings are fluent. Sent"
P19-1603,D13-1170,0,0.00606287,"detailed configurations in each module. 2.2 Sentiment Analyzer The sentiment analyzer S aims to predicting the sentiment intensity s of the gold story ending y to construct paired data (x, s; y). As the first attempt to solve the proposed task, we explore three kinds of sentiment analyzers as follows. Rule-based (RB): VADER (Hutto and Gilbert, 2014) is an rule-based unsupervised model for sentiment analysis. We use it to extract the sentiment intensity s of y and then scale s to [0, 1]. Regression Model (RM): We first train a linear regression model R on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) dataset, which is widely-used for sentiment analysis. Then we use R to acquire the sentiment intensity of y. Domain-Adversarial (DA): In the absence of sentiment annotations for the story dataset, domain adaptation can provide an effective solution since there exists some labeled datasets of a similar task but from a different domain. We use adversarial learning (Ganin and Lempitsky, 2015) to extract a domain-independent feature which not only performs well in the SST sentiment regression task but also misleads the domain discriminator. Finally, we use the adapted regression model to acquire"
P19-1603,D18-1462,1,0.934018,"To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-"
P19-1603,W18-1505,0,0.0845105,"ll lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-story-ending 1 Generated Story Endings controlling the sentiment for story ending generation. Yao et al. (2018b) is the only work on controlling the sentiment for story ending generation. However, their work needs manually label the story dataset with sentiment labels (happy, sad, unknown), which is time-consuming and laborintensive. What’s more, they only focus on coarsegrained sentiment. Different from previous work, we propose the task of controlling the sentiment for story ending generation at a fine-grained level, without any human annotation of story dataset2 . Take Figure 1 as an example, given the same story context, our goal is to generate a story ending that satisfies the given sentiment int"
P19-1603,P18-1102,0,0.0569065,"fically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequ"
P19-1603,P18-1104,0,0.0337842,"t learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclusion and Future Work In"
